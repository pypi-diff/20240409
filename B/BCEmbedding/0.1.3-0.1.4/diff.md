# Comparing `tmp/BCEmbedding-0.1.3-py3-none-any.whl.zip` & `tmp/BCEmbedding-0.1.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,25 +1,25 @@
-Zip file size: 29205 bytes, number of entries: 23
--rw-r--r--  2.0 unx      146 b- defN 24-Jan-15 16:28 BCEmbedding/__init__.py
--rw-r--r--  2.0 unx      146 b- defN 23-Dec-28 10:20 BCEmbedding/evaluation/__init__.py
--rw-r--r--  2.0 unx     8786 b- defN 24-Jan-11 03:38 BCEmbedding/evaluation/c_mteb/Reranking.py
--rw-r--r--  2.0 unx    10905 b- defN 23-Dec-31 17:15 BCEmbedding/evaluation/c_mteb/Retrieval.py
--rw-r--r--  2.0 unx      234 b- defN 23-Dec-27 08:32 BCEmbedding/evaluation/c_mteb/__init__.py
--rw-r--r--  2.0 unx     3503 b- defN 24-Jan-06 16:17 BCEmbedding/evaluation/c_mteb/yd_dres_model.py
--rw-r--r--  2.0 unx      250 b- defN 24-Jan-15 16:20 BCEmbedding/models/__init__.py
--rw-r--r--  2.0 unx     4404 b- defN 24-Jan-19 04:14 BCEmbedding/models/embedding.py
--rw-r--r--  2.0 unx     5722 b- defN 24-Jan-19 04:14 BCEmbedding/models/reranker.py
--rw-r--r--  2.0 unx     2473 b- defN 24-Jan-15 05:08 BCEmbedding/models/utils.py
--rw-r--r--  2.0 unx      124 b- defN 24-Jan-15 16:27 BCEmbedding/tools/__init__.py
--rw-r--r--  2.0 unx      158 b- defN 24-Jan-15 16:26 BCEmbedding/tools/langchain/__init__.py
--rw-r--r--  2.0 unx     3424 b- defN 24-Jan-15 15:18 BCEmbedding/tools/langchain/bce_rerank.py
--rw-r--r--  2.0 unx      158 b- defN 24-Jan-15 16:27 BCEmbedding/tools/llama_index/__init__.py
--rw-r--r--  2.0 unx     3167 b- defN 24-Jan-15 15:16 BCEmbedding/tools/llama_index/bce_rerank.py
--rw-r--r--  2.0 unx      192 b- defN 24-Jan-05 08:04 BCEmbedding/utils/__init__.py
--rw-r--r--  2.0 unx      401 b- defN 23-Dec-28 10:09 BCEmbedding/utils/logger.py
--rw-r--r--  2.0 unx     1671 b- defN 24-Jan-06 06:38 BCEmbedding/utils/query_instructions.py
--rw-r--r--  2.0 unx    11357 b- defN 24-Jan-19 04:23 BCEmbedding-0.1.3.dist-info/LICENSE
--rw-r--r--  2.0 unx    29477 b- defN 24-Jan-19 04:23 BCEmbedding-0.1.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Jan-19 04:23 BCEmbedding-0.1.3.dist-info/WHEEL
--rw-r--r--  2.0 unx       12 b- defN 24-Jan-19 04:23 BCEmbedding-0.1.3.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2060 b- defN 24-Jan-19 04:23 BCEmbedding-0.1.3.dist-info/RECORD
-23 files, 88862 bytes uncompressed, 25809 bytes compressed:  71.0%
+Zip file size: 30136 bytes, number of entries: 23
+-rw-------  2.0 unx      145 b- defN 24-Jan-02 10:43 BCEmbedding/__init__.py
+-rw-------  2.0 unx      146 b- defN 24-Jan-02 10:43 BCEmbedding/evaluation/__init__.py
+-rw-------  2.0 unx     8786 b- defN 24-Jan-11 04:18 BCEmbedding/evaluation/c_mteb/Reranking.py
+-rw-------  2.0 unx    10905 b- defN 24-Jan-02 10:43 BCEmbedding/evaluation/c_mteb/Retrieval.py
+-rw-------  2.0 unx      234 b- defN 24-Jan-02 10:43 BCEmbedding/evaluation/c_mteb/__init__.py
+-rw-------  2.0 unx     3503 b- defN 24-Jan-06 16:17 BCEmbedding/evaluation/c_mteb/yd_dres_model.py
+-rw-------  2.0 unx      250 b- defN 24-Jan-15 16:50 BCEmbedding/models/__init__.py
+-rw-------  2.0 unx     4404 b- defN 24-Jan-19 04:10 BCEmbedding/models/embedding.py
+-rw-------  2.0 unx     5722 b- defN 24-Jan-19 04:11 BCEmbedding/models/reranker.py
+-rw-------  2.0 unx     2588 b- defN 24-Apr-09 09:33 BCEmbedding/models/utils.py
+-rw-------  2.0 unx      124 b- defN 24-Jan-15 16:53 BCEmbedding/tools/__init__.py
+-rw-------  2.0 unx      158 b- defN 24-Jan-15 16:53 BCEmbedding/tools/langchain/__init__.py
+-rw-------  2.0 unx     3338 b- defN 24-Feb-24 15:19 BCEmbedding/tools/langchain/bce_rerank.py
+-rw-------  2.0 unx      158 b- defN 24-Jan-15 16:53 BCEmbedding/tools/llama_index/__init__.py
+-rw-------  2.0 unx     3151 b- defN 24-Mar-04 08:00 BCEmbedding/tools/llama_index/bce_rerank.py
+-rw-------  2.0 unx      192 b- defN 24-Jan-02 10:43 BCEmbedding/utils/__init__.py
+-rw-------  2.0 unx      401 b- defN 24-Jan-02 10:43 BCEmbedding/utils/logger.py
+-rw-------  2.0 unx     1671 b- defN 24-Jan-06 16:22 BCEmbedding/utils/query_instructions.py
+-rw-r--r--  2.0 unx    11357 b- defN 24-Apr-09 09:39 BCEmbedding-0.1.4.dist-info/LICENSE
+-rw-------  2.0 unx    32615 b- defN 24-Apr-09 09:39 BCEmbedding-0.1.4.dist-info/METADATA
+-rw-------  2.0 unx       92 b- defN 24-Apr-09 09:39 BCEmbedding-0.1.4.dist-info/WHEEL
+-rw-r--r--  2.0 unx       12 b- defN 24-Apr-09 09:39 BCEmbedding-0.1.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2060 b- defN 24-Apr-09 09:39 BCEmbedding-0.1.4.dist-info/RECORD
+23 files, 92012 bytes uncompressed, 26740 bytes compressed:  70.9%
```

## zipnote {}

```diff
@@ -48,23 +48,23 @@
 
 Filename: BCEmbedding/utils/logger.py
 Comment: 
 
 Filename: BCEmbedding/utils/query_instructions.py
 Comment: 
 
-Filename: BCEmbedding-0.1.3.dist-info/LICENSE
+Filename: BCEmbedding-0.1.4.dist-info/LICENSE
 Comment: 
 
-Filename: BCEmbedding-0.1.3.dist-info/METADATA
+Filename: BCEmbedding-0.1.4.dist-info/METADATA
 Comment: 
 
-Filename: BCEmbedding-0.1.3.dist-info/WHEEL
+Filename: BCEmbedding-0.1.4.dist-info/WHEEL
 Comment: 
 
-Filename: BCEmbedding-0.1.3.dist-info/top_level.txt
+Filename: BCEmbedding-0.1.4.dist-info/top_level.txt
 Comment: 
 
-Filename: BCEmbedding-0.1.3.dist-info/RECORD
+Filename: BCEmbedding-0.1.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## BCEmbedding/__init__.py

```diff
@@ -1,8 +1,8 @@
 '''
 @Description: 
 @Author: shenlei
 @Date: 2023-11-28 17:16:10
-@LastEditTime: 2024-01-16 00:28:03
+@LastEditTime: 2023-12-18 15:08:31
 @LastEditors: shenlei
 '''
-from .models import *
+from .models import *
```

## BCEmbedding/evaluation/c_mteb/yd_dres_model.py

```diff
@@ -1,12 +1,12 @@
 '''
 @Description: 
 @Author: shenlei
 @Date: 2023-11-29 18:52:01
-@LastEditTime: 2024-01-07 00:17:08
+@LastEditTime: 2024-01-07 00:17:13
 @LastEditors: shenlei
 '''
 from typing import cast, List, Dict, Union
 
 import numpy as np
 from torch import nn
 from BCEmbedding import EmbeddingModel
```

## BCEmbedding/models/__init__.py

```diff
@@ -1,12 +1,12 @@
 '''
 @Description: 
 @Author: shenlei
 @Date: 2023-11-29 12:34:35
-@LastEditTime: 2024-01-16 00:20:50
+@LastEditTime: 2024-01-16 00:50:14
 @LastEditors: shenlei
 '''
 from .embedding import EmbeddingModel
 from .reranker import RerankerModel
 
 __all__ = [
     'EmbeddingModel', 'RerankerModel'
```

## BCEmbedding/models/embedding.py

```diff
@@ -1,12 +1,12 @@
 '''
 @Description: 
 @Author: shenlei
 @Date: 2023-11-28 14:04:27
-@LastEditTime: 2024-01-19 12:14:24
+@LastEditTime: 2024-01-19 12:10:58
 @LastEditors: shenlei
 '''
 import logging
 import torch
 
 from tqdm import tqdm
 from numpy import ndarray
```

## BCEmbedding/models/reranker.py

```diff
@@ -1,12 +1,12 @@
 '''
 @Description: 
 @Author: shenlei
 @Date: 2023-11-28 14:04:27
-@LastEditTime: 2024-01-19 12:14:40
+@LastEditTime: 2024-01-19 12:11:05
 @LastEditors: shenlei
 '''
 import logging
 import torch
 
 import numpy as np
```

## BCEmbedding/models/utils.py

```diff
@@ -1,12 +1,12 @@
 '''
 @Description: 
 @Author: shenlei
 @Date: 2024-01-15 13:06:56
-@LastEditTime: 2024-01-15 13:08:02
+@LastEditTime: 2024-04-09 17:33:35
 @LastEditors: shenlei
 '''
 from typing import List, Dict, Tuple, Type, Union
 from copy import deepcopy
 
 def reranker_tokenize_preproc(
     query: str, 
@@ -16,25 +16,30 @@
     overlap_tokens: int=80,
     ):
     assert tokenizer is not None, "Please provide a valid tokenizer for tokenization!"
     sep_id = tokenizer.sep_token_id
 
     def _merge_inputs(chunk1_raw, chunk2):
         chunk1 = deepcopy(chunk1_raw)
+
+        chunk1['input_ids'].append(sep_id)
         chunk1['input_ids'].extend(chunk2['input_ids'])
         chunk1['input_ids'].append(sep_id)
+
+        chunk1['attention_mask'].append(chunk2['attention_mask'][0])
         chunk1['attention_mask'].extend(chunk2['attention_mask'])
         chunk1['attention_mask'].append(chunk2['attention_mask'][0])
+
         if 'token_type_ids' in chunk1:
-            token_type_ids = [1 for _ in range(len(chunk2['token_type_ids'])+1)]
+            token_type_ids = [1 for _ in range(len(chunk2['token_type_ids'])+2)]
             chunk1['token_type_ids'].extend(token_type_ids)
         return chunk1
 
     query_inputs = tokenizer.encode_plus(query, truncation=False, padding=False)
-    max_passage_inputs_length = max_length - len(query_inputs['input_ids']) - 1
+    max_passage_inputs_length = max_length - len(query_inputs['input_ids']) - 2
     assert max_passage_inputs_length > 100, "Your query is too long! Please make sure your query less than 400 tokens!"
     overlap_tokens_implt = min(overlap_tokens, max_passage_inputs_length//4)
     
     res_merge_inputs = []
     res_merge_inputs_pids = []
     for pid, passage in enumerate(passages):
         passage_inputs = tokenizer.encode_plus(passage, truncation=False, padding=False, add_special_tokens=False)
```

## BCEmbedding/tools/langchain/bce_rerank.py

```diff
@@ -1,39 +1,34 @@
 '''
 @Description: 
 @Author: shenlei
 @Date: 2024-01-15 18:56:59
-@LastEditTime: 2024-01-15 23:18:38
+@LastEditTime: 2024-02-24 23:19:02
 @LastEditors: shenlei
 '''
 from __future__ import annotations
 
-import torch
-
 from typing import Dict, Optional, Sequence, Any
 
 from langchain_core.documents import Document
 from langchain_core.pydantic_v1 import Extra, root_validator
 
 from langchain.callbacks.manager import Callbacks
 from langchain.retrievers.document_compressors.base import BaseDocumentCompressor
 
 from pydantic.v1 import PrivateAttr
 
 
-def infer_torch_device():
-    return "cuda" if torch.cuda.is_available() else "cpu"
-
 class BCERerank(BaseDocumentCompressor):
     """Document compressor that uses `BCEmbedding RerankerModel API`."""
 
     client: str = 'BCEmbedding'
     top_n: int = 3
     """Number of documents to return."""
-    model: str = "bce-reranker-base_v1"
+    model: str = "maidalun1020/bce-reranker-base_v1"
     """Model to use for reranking."""
     _model: Any = PrivateAttr()
 
     class Config:
         """Configuration for this pydantic object."""
         extra = Extra.forbid
         arbitrary_types_allowed = True
```

## BCEmbedding/tools/llama_index/bce_rerank.py

```diff
@@ -1,17 +1,17 @@
 '''
 @Description: 
 @Author: shenlei
 @Date: 2024-01-15 14:15:30
-@LastEditTime: 2024-01-15 23:16:24
+@LastEditTime: 2024-03-04 16:00:52
 @LastEditors: shenlei
 '''
 from typing import Any, List, Optional
 
-from llama_index.bridge.pydantic import Field, PrivateAttr
+from pydantic.v1 import Field, PrivateAttr
 from llama_index.callbacks import CBEventType, EventPayload
 from llama_index.postprocessor.types import BaseNodePostprocessor
 from llama_index.schema import MetadataMode, NodeWithScore, QueryBundle
 from llama_index.utils import infer_torch_device
 
 
 class BCERerank(BaseNodePostprocessor):
```

## BCEmbedding/utils/__init__.py

```diff
@@ -1,9 +1,9 @@
 '''
 @Description: 
 @Author: shenlei
 @Date: 2023-11-28 17:32:38
-@LastEditTime: 2024-01-05 16:03:13
+@LastEditTime: 2023-12-18 16:54:43
 @LastEditors: shenlei
 '''
 from .logger import logger_wrapper
 from .query_instructions import *
```

## BCEmbedding/utils/query_instructions.py

```diff
@@ -1,12 +1,12 @@
 '''
 @Description: 
 @Author: shenlei
 @Date: 2023-12-18 16:54:50
-@LastEditTime: 2024-01-06 14:38:15
+@LastEditTime: 2024-01-07 00:22:50
 @LastEditors: shenlei
 '''
 
 query_instruction_for_retrieval_dict = {
     "BAAI/bge-large-en": "Represent this sentence for searching relevant passages: ",
     "BAAI/bge-base-en": "Represent this sentence for searching relevant passages: ",
     "BAAI/bge-small-en": "Represent this sentence for searching relevant passages: ",
```

## Comparing `BCEmbedding-0.1.3.dist-info/LICENSE` & `BCEmbedding-0.1.4.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `BCEmbedding-0.1.3.dist-info/METADATA` & `BCEmbedding-0.1.4.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,28 +1,28 @@
 Metadata-Version: 2.1
 Name: BCEmbedding
-Version: 0.1.3
+Version: 0.1.4
 Summary: A text embedding model and reranking model produced by Netease Youdao Inc., which can be use for dense embedding retrieval and reranking in RAG workflow.
 Home-page: https://gitlab.corp.youdao.com/ai/BCEmbedding
 Author: Netease Youdao, Inc.
 Author-email: shenlei02@corp.netease.com
 License: apache-2.0
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: torch (>=1.6.0)
-Requires-Dist: transformers (>=4.35.0)
+Requires-Dist: torch >=1.6.0
+Requires-Dist: transformers <4.37.0,>=4.35.0
 Requires-Dist: datasets
 Requires-Dist: sentence-transformers
 
 <!--
  * @Description: 
  * @Author: shenlei
  * @Modified: linhui
  * @Date: 2023-12-19 10:31:41
- * @LastEditTime: 2024-01-19 12:20:52
+ * @LastEditTime: 2024-04-09 17:34:35
  * @LastEditors: shenlei
 -->
 
 <h1 align="center">BCEmbedding: Bilingual and Crosslingual Embedding for RAG</h1>
 
 <div align="center">
     <a href="./LICENSE">
@@ -66,21 +66,40 @@
 - <a href="#-related-links" target="_Self">🔗 Related Links</a>
 
 </details>
 <br>
 
 **B**ilingual and **C**rosslingual **Embedding** (`BCEmbedding`) in English and Chinese, developed by NetEase Youdao, encompasses `EmbeddingModel` and `RerankerModel`. The `EmbeddingModel` specializes in generating semantic vectors, playing a crucial role in semantic search and question-answering, and the `RerankerModel` excels at refining search results and ranking tasks.
 
-`BCEmbedding` serves as the cornerstone of Youdao's Retrieval Augmented Generation (RAG) implmentation, notably [QAnything](http://qanything.ai) [[github](https://github.com/netease-youdao/qanything)], an open-source implementation widely integrated in various Youdao products like [Youdao Speed Reading](https://read.youdao.com/#/home) and [Youdao Translation](https://fanyi.youdao.com/download-Mac?keyfrom=fanyiweb_navigation).
+`BCEmbedding` serves as the cornerstone of Youdao's Retrieval Augmented Generation (RAG) implementation, notably [QAnything](http://qanything.ai) [[github](https://github.com/netease-youdao/qanything)], an open-source implementation widely integrated in various Youdao products like [Youdao Speed Reading](https://read.youdao.com/#/home) and [Youdao Translation](https://fanyi.youdao.com/download-Mac?keyfrom=fanyiweb_navigation).
 
 Distinguished for its bilingual and crosslingual proficiency, `BCEmbedding` excels in bridging Chinese and English linguistic gaps, which achieves
 
-- **A high performence on <a href="#semantic-representation-evaluations-in-mteb" target="_Self">Semantic Representation Evaluations in MTEB</a>**;
+- **A high performance on <a href="#semantic-representation-evaluations-in-mteb" target="_Self">Semantic Representation Evaluations in MTEB</a>**;
 - **A new benchmark in the realm of <a href="#rag-evaluations-in-llamaindex" target="_Self">RAG Evaluations in LlamaIndex</a>**.
 
+<img src="./Docs/assets/rag_eval_multiple_domains_summary.jpg">
+
+### Our Goals
+
+Provide a bilingual and crosslingual two-stage retrieval model repository for the RAG community, which can be used directly without finetuning, including `EmbeddingModel` and `RerankerModel`:
+
+- One Model: `EmbeddingModel` handle **bilingual and crosslingual** retrieval task in English and Chinese. `RerankerModel` supports **English, Chinese, Japanese and Korean**.
+- One Model: **Cover common business application scenarios with RAG optimization**. e.g. Education, Medical Scenario, Law, Finance, Literature, FAQ, Textbook, Wikipedia, General Conversation.
+- Easy to Integrate: We provide **API** in `BCEmbedding` for LlamaIndex and LangChain integrations.
+- Others Points:
+  - `RerankerModel` supports **long passages (more than 512 tokens, less than 32k tokens) reranking**;
+  - `RerankerModel` provides **meaningful relevance score** that helps to remove passages with low quality.
+  - `EmbeddingModel` **does not need specific instructions**.
+
+### Third-party Examples
+
+- RAG applications: [QAnything](https://github.com/netease-youdao/qanything), [HuixiangDou](https://github.com/InternLM/HuixiangDou), [ChatPDF](https://github.com/shibing624/ChatPDF).
+- Efficient inference: [ChatLLM.cpp](https://github.com/foldl/chatllm.cpp), [Xinference](https://github.com/xorbitsai/inference), [mindnlp (Huawei GPU)](https://github.com/mindspore-lab/mindnlp/tree/master/llm/inference/bce).
+
 ## 🌐 Bilingual and Crosslingual Superiority
 
 Existing embedding models often encounter performance challenges in bilingual and crosslingual scenarios, particularly in Chinese, English and their crosslingual tasks. `BCEmbedding`, leveraging the strength of Youdao's translation engine, excels in delivering superior performance across monolingual, bilingual, and crosslingual settings.
 
 `EmbeddingModel` supports ***Chinese (ch) and English (en)*** (more languages support will come soon), while `RerankerModel` supports ***Chinese (ch), English (en), Japanese (ja) and Korean (ko)***.
 
 ## 💡 Key Features
@@ -91,25 +110,26 @@
 - **Broad Domain Adaptability**: Trained on diverse datasets for superior performance across various fields.
 - **User-Friendly Design**: Instruction-free, versatile use for multiple tasks without specifying query instruction for each task.
 - **Meaningful Reranking Scores**: `RerankerModel` provides relevant scores to improve result quality and optimize large language model performance.
 - **Proven in Production**: Successfully implemented and validated in Youdao's products.
 
 ## 🚀 Latest Updates
 
+- ***2024-02-04***: **Technical Blog** - See <a href="https://zhuanlan.zhihu.com/p/681370855">为RAG而生-BCEmbedding技术报告</a>.
 - ***2024-01-16***: **LangChain and LlamaIndex Integrations** - See <a href="#embedding-and-reranker-integrations-for-rag-frameworks" target="_Self">more</a>.
 - ***2024-01-03***: **Model Releases** - [bce-embedding-base_v1](https://huggingface.co/maidalun1020/bce-embedding-base_v1) and [bce-reranker-base_v1](https://huggingface.co/maidalun1020/bce-reranker-base_v1) are available.
-- ***2024-01-03***: **Eval Datasets** [[CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset)] - Evaluate the performence of RAG, using [LlamaIndex](https://github.com/run-llama/llama_index).
-- ***2024-01-03***: **Eval Datasets** [[Details](./BCEmbedding/evaluation/c_mteb/Retrieval.py)] - Evaluate the performence of crosslingual semantic representation, using [MTEB](https://github.com/embeddings-benchmark/mteb).
+- ***2024-01-03***: **Eval Datasets** [[CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset)] - Evaluate the performance of RAG, using [LlamaIndex](https://github.com/run-llama/llama_index).
+- ***2024-01-03***: **Eval Datasets** [[Details](./BCEmbedding/evaluation/c_mteb/Retrieval.py)] - Evaluate the performance of crosslingual semantic representation, using [MTEB](https://github.com/embeddings-benchmark/mteb).
 
 ## 🍎 Model List
 
 | Model Name            |     Model Type     |   Languages   | Parameters |                                                                          Weights                                                                          |
 | :-------------------- | :----------------: | :------------: | :--------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------: |
-| bce-embedding-base_v1 | `EmbeddingModel` |     ch, en     |    279M    | [Huggingface](https://huggingface.co/maidalun1020/bce-embedding-base_v1) |
-| bce-reranker-base_v1  | `RerankerModel` | ch, en, ja, ko |    279M    |  [Huggingface](https://huggingface.co/maidalun1020/bce-reranker-base_v1)  |
+| bce-embedding-base_v1 | `EmbeddingModel` |     ch, en     |    279M    | [Huggingface](https://huggingface.co/maidalun1020/bce-embedding-base_v1), [国内通道](https://hf-mirror.com/maidalun1020/bce-embedding-base_v1) |
+| bce-reranker-base_v1  | `RerankerModel` | ch, en, ja, ko |    279M    |  [Huggingface](https://huggingface.co/maidalun1020/bce-reranker-base_v1), [国内通道](https://hf-mirror.com/maidalun1020/bce-reranker-base_v1)  |
 
 ## 📖 Manual
 
 ### Installation
 
 First, create a conda environment and activate it.
 
@@ -117,18 +137,18 @@
 conda create --name bce python=3.10 -y
 conda activate bce
 ```
 
 Then install `BCEmbedding` for minimal installation (To avoid cuda version conflicting, you should install [`torch`](https://pytorch.org/get-started/previous-versions/) that is compatible to your system cuda version manually first):
 
 ```bash
-pip install BCEmbedding==0.1.3
+pip install BCEmbedding==0.1.4
 ```
 
-Or install from source:
+Or install from source (**recommended**):
 
 ```bash
 git clone git@github.com:netease-youdao/BCEmbedding.git
 cd BCEmbedding
 pip install -v -e .
 ```
 
@@ -138,15 +158,15 @@
 
 Use `EmbeddingModel`, and `cls` [pooler](./BCEmbedding/models/embedding.py#L24) is default.
 
 ```python
 from BCEmbedding import EmbeddingModel
 
 # list of sentences
-sentences = ['sentence_0', 'sentence_1', ...]
+sentences = ['sentence_0', 'sentence_1']
 
 # init embedding model
 model = EmbeddingModel(model_name_or_path="maidalun1020/bce-embedding-base_v1")
 
 # extract embeddings
 embeddings = model.encode(sentences)
 ```
@@ -154,15 +174,15 @@
 Use `RerankerModel` to calculate relevant scores and rerank:
 
 ```python
 from BCEmbedding import RerankerModel
 
 # your query and corresponding passages
 query = 'input_query'
-passages = ['passage_0', 'passage_1', ...]
+passages = ['passage_0', 'passage_1']
 
 # construct sentence pairs
 sentence_pairs = [[query, passage] for passage in passages]
 
 # init reranker model
 model = RerankerModel(model_name_or_path="maidalun1020/bce-reranker-base_v1")
 
@@ -181,26 +201,26 @@
 
 For `EmbeddingModel`:
 
 ```python
 from transformers import AutoModel, AutoTokenizer
 
 # list of sentences
-sentences = ['sentence_0', 'sentence_1', ...]
+sentences = ['sentence_0', 'sentence_1']
 
 # init model and tokenizer
 tokenizer = AutoTokenizer.from_pretrained('maidalun1020/bce-embedding-base_v1')
 model = AutoModel.from_pretrained('maidalun1020/bce-embedding-base_v1')
 
 device = 'cuda'  # if no GPU, set "cpu"
 model.to(device)
 
 # get inputs
 inputs = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors="pt")
-inputs_on_device = {k: v.to(self.device) for k, v in inputs.items()}
+inputs_on_device = {k: v.to(device) for k, v in inputs.items()}
 
 # get embeddings
 outputs = model(**inputs_on_device, return_dict=True)
 embeddings = outputs.last_hidden_state[:, 0]  # cls pooler
 embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)  # normalize
 ```
 
@@ -258,14 +278,23 @@
 
 ### Embedding and Reranker Integrations for RAG Frameworks
 
 #### 1. Used in `langchain`
 
 We provide `BCERerank` in `BCEmbedding.tools.langchain` that inherits the advanced preproc tokenization of `RerankerModel`.
 
+- Install langchain first
+```bash
+pip install langchain==0.1.0
+pip install langchain-community==0.0.9
+pip install langchain-core==0.1.7
+pip install langsmith==0.0.77
+```
+
+- Demo
 ```python
 # We provide the advanced preproc tokenization for reranking.
 from BCEmbedding.tools.langchain import BCERerank
 
 from langchain.text_splitter import RecursiveCharacterTextSplitter
 from langchain_community.document_loaders import PyPDFLoader
 from langchain_community.vectorstores import FAISS
@@ -304,14 +333,21 @@
 response = compression_retriever.get_relevant_documents("What is Llama 2?")
 ```
 
 #### 2. Used in `llama_index`
 
 We provide `BCERerank` in `BCEmbedding.tools.llama_index` that inherits the advanced preproc tokenization of `RerankerModel`.
 
+- Install llama_index first
+
+```bash
+pip install llama-index==0.9.42.post2
+```
+
+- Demo
 ```python
 # We provide the advanced preproc tokenization for reranking.
 from BCEmbedding.tools.llama_index import BCERerank
 
 import os
 from llama_index.embeddings import HuggingFaceEmbedding
 from llama_index import VectorStoreIndex, ServiceContext, SimpleDirectoryReader
@@ -373,15 +409,15 @@
 
 Just run following cmd to evaluate `your_embedding_model` (e.g. `maidalun1020/bce-embedding-base_v1`) in **bilingual and crosslingual settings** (e.g. `["en", "zh", "en-zh", "zh-en"]`).
 
 ```bash
 python BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path maidalun1020/bce-embedding-base_v1 --pooler cls
 ```
 
-The total evaluation tasks contain ***114 datastes*** of **"Retrieval", "STS", "PairClassification", "Classification", "Reranking" and "Clustering"**.
+The total evaluation tasks contain ***114 datasets*** of **"Retrieval", "STS", "PairClassification", "Classification", "Reranking" and "Clustering"**.
 
 ***NOTE:***
 
 - **All models are evaluated in their recommended pooling method (`pooler`)**.
   - `mean` pooler: "jina-embeddings-v2-base-en", "m3e-base", "m3e-large", "e5-large-v2", "multilingual-e5-base", "multilingual-e5-large" and "gte-large".
   - `cls` pooler: Other models.
 - "jina-embeddings-v2-base-en" model should be loaded with `trust_remote_code`.
@@ -396,19 +432,19 @@
 
 Run following cmd to evaluate `your_reranker_model` (e.g. "maidalun1020/bce-reranker-base_v1") in **bilingual and crosslingual settings** (e.g. `["en", "zh", "en-zh", "zh-en"]`).
 
 ```bash
 python BCEmbedding/tools/eval_mteb/eval_reranker_mteb.py --model_name_or_path maidalun1020/bce-reranker-base_v1
 ```
 
-The evaluation tasks contain ***12 datastes*** of **"Reranking"**.
+The evaluation tasks contain ***12 datasets*** of **"Reranking"**.
 
 #### 3. Metrics Visualization Tool
 
-We proveide a one-click script to sumarize evaluation results of `embedding` and `reranker` models as [Embedding Models Evaluation Summary](./Docs/EvaluationSummary/embedding_eval_summary.md) and [Reranker Models Evaluation Summary](./Docs/EvaluationSummary/reranker_eval_summary.md).
+We provide a one-click script to summarize evaluation results of `embedding` and `reranker` models as [Embedding Models Evaluation Summary](./Docs/EvaluationSummary/embedding_eval_summary.md) and [Reranker Models Evaluation Summary](./Docs/EvaluationSummary/reranker_eval_summary.md).
 
 ```bash
 python BCEmbedding/evaluation/mteb/summarize_eval_results.py --results_dir {your_embedding_results_dir | your_reranker_results_dir}
 ```
 
 ### Evaluate RAG by LlamaIndex
 
@@ -444,38 +480,41 @@
 In order to compare our `BCEmbedding` with other embedding and reranker models fairly, we provide a one-click script to reproduce results of the LlamaIndex Blog, including our `BCEmbedding`:
 
 ```bash
 # There should be two GPUs available at least.
 CUDA_VISIBLE_DEVICES=0,1 python BCEmbedding/tools/eval_rag/eval_llamaindex_reproduce.py
 ```
 
-Then, sumarize the evaluation results by:
+Then, summarize the evaluation results by:
 
 ```bash
 python BCEmbedding/tools/eval_rag/summarize_eval_results.py --results_dir BCEmbedding/results/rag_reproduce_results
 ```
 
-Results Reproduced from the LlamaIndex Blog can be checked in ***[Reproduced Summary of RAG Evaluation](./Docs/EvaluationSummary/rag_eval_reproduced_summary.md)***, with some obvious ***conclusions***:
+Results reproduced from the LlamaIndex Blog can be checked in ***[Reproduced Summary of RAG Evaluation](./Docs/EvaluationSummary/rag_eval_reproduced_summary.md)***, with some obvious ***conclusions***:
 
 - In `WithoutReranker` setting, our `bce-embedding-base_v1` outperforms all the other embedding models.
-- With fixing the embedding model, our `bce-reranker-base_v1` achieves the best performence.
+- With fixing the embedding model, our `bce-reranker-base_v1` achieves the best performance.
 - ***The combination of `bce-embedding-base_v1` and `bce-reranker-base_v1` is SOTA.***
 
 #### 3. Broad Domain Adaptability
 
-The evaluation of [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83) is **monolingual, small amount of data, and specific domain** (just including "llama2" paper). In order to evaluate the **broad domain adaptability, bilingual and crosslingual capability**, we follow the blog to build a multiple domains evaluation dataset (includding "Computer Science", "Physics", "Biology", "Economics", "Math", and "Quantitative Finance". [Details](./BCEmbedding/tools/eval_rag/eval_pdfs/)), named [CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset), **by OpenAI `gpt-4-1106-preview` for high quality**.
+The evaluation of [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83) is **monolingual, small amount of data, and specific domain** (just including "llama2" paper). In order to evaluate the **broad domain adaptability, bilingual and crosslingual capability**, we follow the blog to build a multiple domains evaluation dataset (includding "Computer Science", "Physics", "Biology", "Economics", "Math", and "Quantitative Finance". [Details](./BCEmbedding/tools/eval_rag/eval_pdfs/)), named [CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset):
+
+- To prevent test data leakage, English eval data is selected from the latest English articles in various fields on ArXiv, up to date December 30, 2023. Chinese eval data is selected from high-quality, as recent as possible, Chinese articles in the corresponding fields on Semantic Scholar.
+- Use OpenAI `gpt-4-1106-preview` to produce eval data for high quality.
 
 First, run following cmd to evaluate the most popular and powerful embedding and reranker models:
 
 ```bash
 # There should be two GPUs available at least.
 CUDA_VISIBLE_DEVICES=0,1 python BCEmbedding/tools/eval_rag/eval_llamaindex_multiple_domains.py
 ```
 
-Then, run the following script to sumarize the evaluation results:
+Then, run the following script to summarize the evaluation results:
 
 ```bash
 python BCEmbedding/tools/eval_rag/summarize_eval_results.py --results_dir BCEmbedding/results/rag_results
 ```
 
 The summary of multiple domains evaluations can be seen in <a href="#1-multiple-domains-scenarios" target="_Self">Multiple Domains Scenarios</a>.
 
@@ -499,44 +538,48 @@
 | e5-large-v2                         |    1024    | `mean` |     Need     |      35.98      |      55.23      |         75.28         |        59.53        |      42.12      |      36.51      |          46.52          |
 | multilingual-e5-base                |    768    | `mean` |     Need     |      54.73      |      65.49      |         76.97         |        69.72        |      55.01      |      38.44      |          58.34          |
 | multilingual-e5-large               |    1024    | `mean` |     Need     |      56.76      | **66.79** |         78.80         |   **71.61**   |      56.49      | **43.09** |     **60.50**     |
 | ***bce-embedding-base_v1*** |    768    | `cls` |     Free     | **57.60** |      65.73      |         74.96         |        69.00        | **57.29** |      38.95      |          59.43          |
 
 ***NOTE:***
 
-- Our ***bce-embedding-base_v1*** outperforms other opensource embedding models with comparable model sizes.
-- ***114 datastes including 119 eval results*** (some dataset contains multiple languages) of "Retrieval", "STS", "PairClassification", "Classification", "Reranking" and "Clustering" in ***`["en", "zh", "en-zh", "zh-en"]` setting***, including **MTEB and CMTEB**.
+- Our ***bce-embedding-base_v1*** outperforms other open-source embedding models with comparable model sizes.
+- ***114 datasets including 119 eval results*** (some dataset contains multiple languages) of "Retrieval", "STS", "PairClassification", "Classification", "Reranking" and "Clustering" in ***`["en", "zh", "en-zh", "zh-en"]` setting***, including **MTEB and CMTEB**.
 - The [crosslingual evaluation datasets](./BCEmbedding/evaluation/c_mteb/Retrieval.py) we released belong to `Retrieval` task.
-- More evaluation details should be checked [Embedding Models Evaluations](./Docs/EvaluationSummary/embedding_eval_summary.md).
+- More evaluation details should be checked in [Embedding Models Evaluations](./Docs/EvaluationSummary/embedding_eval_summary.md).
 
 #### 2. Reranker Models
 
 | Model                              | Reranking (12) | ***AVG*** (12) |
 | :--------------------------------- | :-------------: | :--------------------: |
 | bge-reranker-base                  |      59.04      |         59.04         |
 | bge-reranker-large                 |      60.86      |         60.86         |
 | ***bce-reranker-base_v1*** | **61.29** |  ***61.29***  |
 
 ***NOTE:***
 
-- Our ***bce-reranker-base_v1*** outperforms other opensource reranker models.
-- ***12 datastes*** of "Reranking" in ***`["en", "zh", "en-zh", "zh-en"]` setting***.
-- More evaluation details should be checked [Reranker Models Evaluations](./Docs/EvaluationSummary/reranker_eval_summary.md).
+- Our ***bce-reranker-base_v1*** outperforms other open-source reranker models.
+- ***12 datasets*** of "Reranking" in ***`["en", "zh", "en-zh", "zh-en"]` setting***.
+- More evaluation details should be checked in [Reranker Models Evaluations](./Docs/EvaluationSummary/reranker_eval_summary.md).
 
 ### RAG Evaluations in LlamaIndex
 
 #### 1. Multiple Domains Scenarios
 
 <img src="./Docs/assets/rag_eval_multiple_domains_summary.jpg">
 
 ***NOTE:***
 
+- Data Quality: 
+  - To prevent test data leakage, English eval data is selected from the latest English articles in various fields on ArXiv, up to date December 30, 2023. Chinese eval data is selected from high-quality, as recent as possible, Chinese articles in the corresponding fields on Semantic Scholar. 
+  - Use OpenAI `gpt-4-1106-preview` to produce eval data for high quality.
+- Evaluated in ***`["en", "zh", "en-zh", "zh-en"]` setting***. If you are interested in monolingual setting, please check in [Chinese RAG evaluations with ["zh"] setting](./Docs/EvaluationSummary/rag_eval_multiple_domains_summary_zh.md), and [English RAG evaluations with ["en"] setting](./Docs/EvaluationSummary/rag_eval_multiple_domains_summary_en.md).
 - Consistent with our ***[Reproduced Results](./Docs/EvaluationSummary/rag_eval_reproduced_summary.md)*** of [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83).
 - In `WithoutReranker` setting, our `bce-embedding-base_v1` outperforms all the other embedding models.
-- With fixing the embedding model, our `bce-reranker-base_v1` achieves the best performence.
+- With fixing the embedding model, our `bce-reranker-base_v1` achieves the best performance.
 - **The combination of `bce-embedding-base_v1` and `bce-reranker-base_v1` is SOTA**.
 
 ## 🛠 Youdao's BCEmbedding API
 
 For users who prefer a hassle-free experience without the need to download and configure the model on their own systems, `BCEmbedding` is readily accessible through Youdao's API. This option offers a streamlined and efficient way to integrate BCEmbedding into your projects, bypassing the complexities of manual setup and maintenance. Detailed instructions and comprehensive API documentation are available at [Youdao BCEmbedding API](https://ai.youdao.com/DOCSIRMA/html/aigc/api/embedding/index.html). Here, you'll find all the necessary guidance to easily implement `BCEmbedding` across a variety of use cases, ensuring a smooth and effective integration for optimal results.
 
 ## 🧲 WeChat Group
@@ -569,7 +612,9 @@
 [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)
 
 [MTEB](https://github.com/embeddings-benchmark/mteb)
 
 [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)
 
 [LLama Index](https://github.com/run-llama/llama_index) | [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)
+
+[HuixiangDou](https://github.com/internlm/huixiangdou)
```

### html2text {}

```diff
@@ -1,15 +1,15 @@
-Metadata-Version: 2.1 Name: BCEmbedding Version: 0.1.3 Summary: A text
+Metadata-Version: 2.1 Name: BCEmbedding Version: 0.1.4 Summary: A text
 embedding model and reranking model produced by Netease Youdao Inc., which can
 be use for dense embedding retrieval and reranking in RAG workflow. Home-page:
 https://gitlab.corp.youdao.com/ai/BCEmbedding Author: Netease Youdao, Inc.
 Author-email: shenlei02@corp.netease.com License: apache-2.0 Description-
-Content-Type: text/markdown License-File: LICENSE Requires-Dist: torch
-(>=1.6.0) Requires-Dist: transformers (>=4.35.0) Requires-Dist: datasets
-Requires-Dist: sentence-transformers
+Content-Type: text/markdown License-File: LICENSE Requires-Dist: torch >=1.6.0
+Requires-Dist: transformers <4.37.0,>=4.35.0 Requires-Dist: datasets Requires-
+Dist: sentence-transformers
     ************ BBCCEEmmbbeeddddiinngg:: BBiilliinngguuaall aanndd CCrroosssslliinngguuaall EEmmbbeeddddiinngg ffoorr RRAAGG ************
   _[_h_t_t_p_s_:_/_/_i_m_g_._s_h_i_e_l_d_s_._i_o_/_b_a_d_g_e_/_l_i_c_e_n_s_e_-_A_p_a_c_h_e_-_-_2_._0_-_y_e_l_l_o_w_]Â Â Â Â  _[_h_t_t_p_s_:_/_/
     _i_m_g_._s_h_i_e_l_d_s_._i_o_/_b_a_d_g_e_/_f_o_l_l_o_w_-_%_4_0_Y_D_O_p_e_n_S_o_u_r_c_e_-_1_D_A_1_F_2_?_l_o_g_o_=_t_w_i_t_t_e_r_&_s_t_y_l_e_=
                                _{_s_t_y_l_e_}_]Â Â Â Â 
 
                             EEnngglliisshh | _ç_®__ä_½__ä_¸_­_æ__
 Click to Open Contents - _ð____ _B_i_l_i_n_g_u_a_l_ _a_n_d_ _C_r_o_s_s_l_i_n_g_u_a_l_ _S_u_p_e_r_i_o_r_i_t_y - _ð___¡_ _K_e_y
@@ -22,95 +22,118 @@
 _G_r_o_u_p - _â___ï_¸__ _C_i_t_a_t_i_o_n - _ð____ _L_i_c_e_n_s_e - _ð____ _R_e_l_a_t_e_d_ _L_i_n_k_s
 **B**ilingual and **C**rosslingual **Embedding** (`BCEmbedding`) in English and
 Chinese, developed by NetEase Youdao, encompasses `EmbeddingModel` and
 `RerankerModel`. The `EmbeddingModel` specializes in generating semantic
 vectors, playing a crucial role in semantic search and question-answering, and
 the `RerankerModel` excels at refining search results and ranking tasks.
 `BCEmbedding` serves as the cornerstone of Youdao's Retrieval Augmented
-Generation (RAG) implmentation, notably [QAnything](http://qanything.ai) [
+Generation (RAG) implementation, notably [QAnything](http://qanything.ai) [
 [github](https://github.com/netease-youdao/qanything)], an open-source
 implementation widely integrated in various Youdao products like [Youdao Speed
 Reading](https://read.youdao.com/#/home) and [Youdao Translation](https://
 fanyi.youdao.com/download-Mac?keyfrom=fanyiweb_navigation). Distinguished for
 its bilingual and crosslingual proficiency, `BCEmbedding` excels in bridging
-Chinese and English linguistic gaps, which achieves - **A high performence on
+Chinese and English linguistic gaps, which achieves - **A high performance on
 _S_e_m_a_n_t_i_c_ _R_e_p_r_e_s_e_n_t_a_t_i_o_n_ _E_v_a_l_u_a_t_i_o_n_s_ _i_n_ _M_T_E_B**; - **A new benchmark in the realm
-of _R_A_G_ _E_v_a_l_u_a_t_i_o_n_s_ _i_n_ _L_l_a_m_a_I_n_d_e_x**. ## ð Bilingual and Crosslingual
-Superiority Existing embedding models often encounter performance challenges in
-bilingual and crosslingual scenarios, particularly in Chinese, English and
-their crosslingual tasks. `BCEmbedding`, leveraging the strength of Youdao's
-translation engine, excels in delivering superior performance across
-monolingual, bilingual, and crosslingual settings. `EmbeddingModel` supports
-***Chinese (ch) and English (en)*** (more languages support will come soon),
-while `RerankerModel` supports ***Chinese (ch), English (en), Japanese (ja) and
-Korean (ko)***. ## ð¡ Key Features - **Bilingual and Crosslingual
-Proficiency**: Powered by Youdao's translation engine, excelling in Chinese,
-English and their crosslingual retrieval task, with upcoming support for
-additional languages. - **RAG-Optimized**: Tailored for diverse RAG tasks
-including **translation, summarization, and question answering**, ensuring
-accurate **query understanding**. See _R_A_G_ _E_v_a_l_u_a_t_i_o_n_s_ _i_n_ _L_l_a_m_a_I_n_d_e_x. -
-**Efficient and Precise Retrieval**: Dual-encoder for efficient retrieval of
-`EmbeddingModel` in first stage, and cross-encoder of `RerankerModel` for
-enhanced precision and deeper semantic analysis in second stage. - **Broad
-Domain Adaptability**: Trained on diverse datasets for superior performance
-across various fields. - **User-Friendly Design**: Instruction-free, versatile
-use for multiple tasks without specifying query instruction for each task. -
-**Meaningful Reranking Scores**: `RerankerModel` provides relevant scores to
-improve result quality and optimize large language model performance. -
-**Proven in Production**: Successfully implemented and validated in Youdao's
-products. ## ð Latest Updates - ***2024-01-16***: **LangChain and LlamaIndex
+of _R_A_G_ _E_v_a_l_u_a_t_i_o_n_s_ _i_n_ _L_l_a_m_a_I_n_d_e_x**. [./Docs/assets/
+rag_eval_multiple_domains_summary.jpg]### Our Goals Provide a bilingual and
+crosslingual two-stage retrieval model repository for the RAG community, which
+can be used directly without finetuning, including `EmbeddingModel` and
+`RerankerModel`: - One Model: `EmbeddingModel` handle **bilingual and
+crosslingual** retrieval task in English and Chinese. `RerankerModel` supports
+**English, Chinese, Japanese and Korean**. - One Model: **Cover common business
+application scenarios with RAG optimization**. e.g. Education, Medical
+Scenario, Law, Finance, Literature, FAQ, Textbook, Wikipedia, General
+Conversation. - Easy to Integrate: We provide **API** in `BCEmbedding` for
+LlamaIndex and LangChain integrations. - Others Points: - `RerankerModel`
+supports **long passages (more than 512 tokens, less than 32k tokens)
+reranking**; - `RerankerModel` provides **meaningful relevance score** that
+helps to remove passages with low quality. - `EmbeddingModel` **does not need
+specific instructions**. ### Third-party Examples - RAG applications:
+[QAnything](https://github.com/netease-youdao/qanything), [HuixiangDou](https:/
+/github.com/InternLM/HuixiangDou), [ChatPDF](https://github.com/shibing624/
+ChatPDF). - Efficient inference: [ChatLLM.cpp](https://github.com/foldl/
+chatllm.cpp), [Xinference](https://github.com/xorbitsai/inference), [mindnlp
+(Huawei GPU)](https://github.com/mindspore-lab/mindnlp/tree/master/llm/
+inference/bce). ## ð Bilingual and Crosslingual Superiority Existing
+embedding models often encounter performance challenges in bilingual and
+crosslingual scenarios, particularly in Chinese, English and their crosslingual
+tasks. `BCEmbedding`, leveraging the strength of Youdao's translation engine,
+excels in delivering superior performance across monolingual, bilingual, and
+crosslingual settings. `EmbeddingModel` supports ***Chinese (ch) and English
+(en)*** (more languages support will come soon), while `RerankerModel` supports
+***Chinese (ch), English (en), Japanese (ja) and Korean (ko)***. ## ð¡ Key
+Features - **Bilingual and Crosslingual Proficiency**: Powered by Youdao's
+translation engine, excelling in Chinese, English and their crosslingual
+retrieval task, with upcoming support for additional languages. - **RAG-
+Optimized**: Tailored for diverse RAG tasks including **translation,
+summarization, and question answering**, ensuring accurate **query
+understanding**. See _R_A_G_ _E_v_a_l_u_a_t_i_o_n_s_ _i_n_ _L_l_a_m_a_I_n_d_e_x. - **Efficient and Precise
+Retrieval**: Dual-encoder for efficient retrieval of `EmbeddingModel` in first
+stage, and cross-encoder of `RerankerModel` for enhanced precision and deeper
+semantic analysis in second stage. - **Broad Domain Adaptability**: Trained on
+diverse datasets for superior performance across various fields. - **User-
+Friendly Design**: Instruction-free, versatile use for multiple tasks without
+specifying query instruction for each task. - **Meaningful Reranking Scores**:
+`RerankerModel` provides relevant scores to improve result quality and optimize
+large language model performance. - **Proven in Production**: Successfully
+implemented and validated in Youdao's products. ## ð Latest Updates -
+***2024-02-04***: **Technical Blog** - See _ä_¸_º_R_A_G_è___ç___-
+_B_C_E_m_b_e_d_d_i_n_g_æ___æ__¯_æ__¥_å__. - ***2024-01-16***: **LangChain and LlamaIndex
 Integrations** - See _m_o_r_e. - ***2024-01-03***: **Model Releases** - [bce-
 embedding-base_v1](https://huggingface.co/maidalun1020/bce-embedding-base_v1)
 and [bce-reranker-base_v1](https://huggingface.co/maidalun1020/bce-reranker-
 base_v1) are available. - ***2024-01-03***: **Eval Datasets** [
 [CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/
-CrosslingualMultiDomainsDataset)] - Evaluate the performence of RAG, using
+CrosslingualMultiDomainsDataset)] - Evaluate the performance of RAG, using
 [LlamaIndex](https://github.com/run-llama/llama_index). - ***2024-01-03***:
 **Eval Datasets** [[Details](./BCEmbedding/evaluation/c_mteb/Retrieval.py)] -
-Evaluate the performence of crosslingual semantic representation, using [MTEB]
+Evaluate the performance of crosslingual semantic representation, using [MTEB]
 (https://github.com/embeddings-benchmark/mteb). ## ð Model List | Model Name
 | Model Type | Languages | Parameters | Weights | | :-------------------- | :--
 --------------: | :------------: | :--------: | :------------------------------
 -------------------------------------------------------------------------------
 ------------------------------------------: | | bce-embedding-base_v1 |
 `EmbeddingModel` | ch, en | 279M | [Huggingface](https://huggingface.co/
+maidalun1020/bce-embedding-base_v1), [å½åéé](https://hf-mirror.com/
 maidalun1020/bce-embedding-base_v1) | | bce-reranker-base_v1 | `RerankerModel`
 | ch, en, ja, ko | 279M | [Huggingface](https://huggingface.co/maidalun1020/
-bce-reranker-base_v1) | ## ð Manual ### Installation First, create a conda
+bce-reranker-base_v1), [å½åéé](https://hf-mirror.com/maidalun1020/bce-
+reranker-base_v1) | ## ð Manual ### Installation First, create a conda
 environment and activate it. ```bash conda create --name bce python=3.10 -
 y conda activate bce ``` Then install `BCEmbedding` for minimal installation
 (To avoid cuda version conflicting, you should install [`torch`](https://
 pytorch.org/get-started/previous-versions/) that is compatible to your system
-cuda version manually first): ```bash pip install BCEmbedding==0.1.3 ``` Or
-install from source: ```bash git clone git@github.com:netease-youdao/
-BCEmbedding.git cd BCEmbedding pip install -v -e . ``` ### Quick Start #### 1.
-Based on `BCEmbedding` Use `EmbeddingModel`, and `cls` [pooler](./BCEmbedding/
-models/embedding.py#L24) is default. ```python from BCEmbedding import
-EmbeddingModel # list of sentences sentences = ['sentence_0', 'sentence_1',
-...] # init embedding model model = EmbeddingModel
+cuda version manually first): ```bash pip install BCEmbedding==0.1.4 ``` Or
+install from source (**recommended**): ```bash git clone git@github.com:
+netease-youdao/BCEmbedding.git cd BCEmbedding pip install -v -e . ``` ### Quick
+Start #### 1. Based on `BCEmbedding` Use `EmbeddingModel`, and `cls` [pooler]
+(./BCEmbedding/models/embedding.py#L24) is default. ```python from BCEmbedding
+import EmbeddingModel # list of sentences sentences = ['sentence_0',
+'sentence_1'] # init embedding model model = EmbeddingModel
 (model_name_or_path="maidalun1020/bce-embedding-base_v1") # extract embeddings
 embeddings = model.encode(sentences) ``` Use `RerankerModel` to calculate
 relevant scores and rerank: ```python from BCEmbedding import RerankerModel #
 your query and corresponding passages query = 'input_query' passages =
-['passage_0', 'passage_1', ...] # construct sentence pairs sentence_pairs = [
-[query, passage] for passage in passages] # init reranker model model =
-RerankerModel(model_name_or_path="maidalun1020/bce-reranker-base_v1") # method
-0: calculate scores of sentence pairs scores = model.compute_score
-(sentence_pairs) # method 1: rerank passages rerank_results = model.rerank
-(query, passages) ``` NOTE: - In [`RerankerModel.rerank`](./BCEmbedding/models/
-reranker.py#L137) method, we provide an advanced preproccess that we use in
-production for making `sentence_pairs`, when "passages" are very long. #### 2.
-Based on `transformers` For `EmbeddingModel`: ```python from transformers
-import AutoModel, AutoTokenizer # list of sentences sentences = ['sentence_0',
-'sentence_1', ...] # init model and tokenizer tokenizer =
+['passage_0', 'passage_1'] # construct sentence pairs sentence_pairs = [[query,
+passage] for passage in passages] # init reranker model model = RerankerModel
+(model_name_or_path="maidalun1020/bce-reranker-base_v1") # method 0: calculate
+scores of sentence pairs scores = model.compute_score(sentence_pairs) # method
+1: rerank passages rerank_results = model.rerank(query, passages) ``` NOTE: -
+In [`RerankerModel.rerank`](./BCEmbedding/models/reranker.py#L137) method, we
+provide an advanced preproccess that we use in production for making
+`sentence_pairs`, when "passages" are very long. #### 2. Based on
+`transformers` For `EmbeddingModel`: ```python from transformers import
+AutoModel, AutoTokenizer # list of sentences sentences = ['sentence_0',
+'sentence_1'] # init model and tokenizer tokenizer =
 AutoTokenizer.from_pretrained('maidalun1020/bce-embedding-base_v1') model =
 AutoModel.from_pretrained('maidalun1020/bce-embedding-base_v1') device = 'cuda'
 # if no GPU, set "cpu" model.to(device) # get inputs inputs = tokenizer
 (sentences, padding=True, truncation=True, max_length=512, return_tensors="pt")
-inputs_on_device = {k: v.to(self.device) for k, v in inputs.items()} # get
+inputs_on_device = {k: v.to(device) for k, v in inputs.items()} # get
 embeddings outputs = model(**inputs_on_device, return_dict=True) embeddings =
 outputs.last_hidden_state[:, 0] # cls pooler embeddings = embeddings /
 embeddings.norm(dim=1, keepdim=True) # normalize ``` For `RerankerModel`:
 ```python import torch from transformers import AutoTokenizer,
 AutoModelForSequenceClassification # init model and tokenizer tokenizer =
 AutoTokenizer.from_pretrained('maidalun1020/bce-reranker-base_v1') model =
 AutoModelForSequenceClassification.from_pretrained('maidalun1020/bce-reranker-
@@ -129,42 +152,45 @@
 embeddings = model.encode(sentences, normalize_embeddings=True) ``` For
 `RerankerModel`: ```python from sentence_transformers import CrossEncoder #
 init reranker model model = CrossEncoder('maidalun1020/bce-reranker-base_v1',
 max_length=512) # calculate scores of sentence pairs scores = model.predict
 (sentence_pairs) ``` ### Embedding and Reranker Integrations for RAG Frameworks
 #### 1. Used in `langchain` We provide `BCERerank` in
 `BCEmbedding.tools.langchain` that inherits the advanced preproc tokenization
-of `RerankerModel`. ```python # We provide the advanced preproc tokenization
-for reranking. from BCEmbedding.tools.langchain import BCERerank from
-langchain.text_splitter import RecursiveCharacterTextSplitter from
-langchain_community.document_loaders import PyPDFLoader from
-langchain_community.vectorstores import FAISS from langchain.embeddings import
-HuggingFaceEmbeddings from langchain_community.vectorstores.utils import
-DistanceStrategy from langchain.retrievers import
-ContextualCompressionRetriever # init embedding model embedding_model_name =
-'maidalun1020/bce-embedding-base_v1' embedding_model_kwargs = {'device': 'cuda:
-0'} embedding_encode_kwargs = {'batch_size': 32, 'normalize_embeddings': True,
-'show_progress_bar': False} embed_model = HuggingFaceEmbeddings
-( model_name=embedding_model_name, model_kwargs=embedding_model_kwargs,
-encode_kwargs=embedding_encode_kwargs ) reranker_args = {'model':
-'maidalun1020/bce-reranker-base_v1', 'top_n': 5, 'device': 'cuda:1'} reranker =
-BCERerank(**reranker_args) # init documents documents = PyPDFLoader
-("BCEmbedding/tools/eval_rag/eval_pdfs/Comp_en_llama2.pdf").load()
-text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,
-chunk_overlap=200) texts = text_splitter.split_documents(documents) # example
-1. retrieval with embedding and reranker retriever = FAISS.from_documents
-(texts, embed_model,
+of `RerankerModel`. - Install langchain first ```bash pip install
+langchain==0.1.0 pip install langchain-community==0.0.9 pip install langchain-
+core==0.1.7 pip install langsmith==0.0.77 ``` - Demo ```python # We provide the
+advanced preproc tokenization for reranking. from BCEmbedding.tools.langchain
+import BCERerank from langchain.text_splitter import
+RecursiveCharacterTextSplitter from langchain_community.document_loaders import
+PyPDFLoader from langchain_community.vectorstores import FAISS from
+langchain.embeddings import HuggingFaceEmbeddings from
+langchain_community.vectorstores.utils import DistanceStrategy from
+langchain.retrievers import ContextualCompressionRetriever # init embedding
+model embedding_model_name = 'maidalun1020/bce-embedding-base_v1'
+embedding_model_kwargs = {'device': 'cuda:0'} embedding_encode_kwargs =
+{'batch_size': 32, 'normalize_embeddings': True, 'show_progress_bar': False}
+embed_model = HuggingFaceEmbeddings( model_name=embedding_model_name,
+model_kwargs=embedding_model_kwargs, encode_kwargs=embedding_encode_kwargs )
+reranker_args = {'model': 'maidalun1020/bce-reranker-base_v1', 'top_n': 5,
+'device': 'cuda:1'} reranker = BCERerank(**reranker_args) # init documents
+documents = PyPDFLoader("BCEmbedding/tools/eval_rag/eval_pdfs/
+Comp_en_llama2.pdf").load() text_splitter = RecursiveCharacterTextSplitter
+(chunk_size=1500, chunk_overlap=200) texts = text_splitter.split_documents
+(documents) # example 1. retrieval with embedding and reranker retriever =
+FAISS.from_documents(texts, embed_model,
 distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT).as_retriever
 (search_type="similarity", search_kwargs={"score_threshold": 0.3, "k": 10})
 compression_retriever = ContextualCompressionRetriever
 ( base_compressor=reranker, base_retriever=retriever ) response =
 compression_retriever.get_relevant_documents("What is Llama 2?") ``` #### 2.
 Used in `llama_index` We provide `BCERerank` in `BCEmbedding.tools.llama_index`
-that inherits the advanced preproc tokenization of `RerankerModel`. ```python #
-We provide the advanced preproc tokenization for reranking. from
+that inherits the advanced preproc tokenization of `RerankerModel`. - Install
+llama_index first ```bash pip install llama-index==0.9.42.post2 ``` - Demo
+```python # We provide the advanced preproc tokenization for reranking. from
 BCEmbedding.tools.llama_index import BCERerank import os from
 llama_index.embeddings import HuggingFaceEmbedding from llama_index import
 VectorStoreIndex, ServiceContext, SimpleDirectoryReader from
 llama_index.node_parser import SimpleNodeParser from llama_index.llms import
 OpenAI from llama_index.retrievers import VectorIndexRetriever # init embedding
 model and reranker model embed_args = {'model_name': 'maidalun1020/bce-
 embedding-base_v1', 'max_length': 512, 'embed_batch_size': 32, 'device': 'cuda:
@@ -195,30 +221,30 @@
 mteb) and [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/
 C_MTEB). First, install `MTEB`: ``` pip install mteb==1.1.1 ``` #### 1.
 Embedding Models Just run following cmd to evaluate `your_embedding_model`
 (e.g. `maidalun1020/bce-embedding-base_v1`) in **bilingual and crosslingual
 settings** (e.g. `["en", "zh", "en-zh", "zh-en"]`). ```bash python BCEmbedding/
 tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path maidalun1020/bce-
 embedding-base_v1 --pooler cls ``` The total evaluation tasks contain ***114
-datastes*** of **"Retrieval", "STS", "PairClassification", "Classification",
+datasets*** of **"Retrieval", "STS", "PairClassification", "Classification",
 "Reranking" and "Clustering"**. ***NOTE:*** - **All models are evaluated in
 their recommended pooling method (`pooler`)**. - `mean` pooler: "jina-
 embeddings-v2-base-en", "m3e-base", "m3e-large", "e5-large-v2", "multilingual-
 e5-base", "multilingual-e5-large" and "gte-large". - `cls` pooler: Other
 models. - "jina-embeddings-v2-base-en" model should be loaded with
 `trust_remote_code`. ```bash python BCEmbedding/tools/eval_mteb/
 eval_embedding_mteb.py --model_name_or_path {mean_pooler_models} --pooler mean
 python BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path
 jinaai/jina-embeddings-v2-base-en --pooler mean --trust_remote_code ``` #### 2.
 Reranker Models Run following cmd to evaluate `your_reranker_model` (e.g.
 "maidalun1020/bce-reranker-base_v1") in **bilingual and crosslingual settings**
 (e.g. `["en", "zh", "en-zh", "zh-en"]`). ```bash python BCEmbedding/tools/
 eval_mteb/eval_reranker_mteb.py --model_name_or_path maidalun1020/bce-reranker-
-base_v1 ``` The evaluation tasks contain ***12 datastes*** of **"Reranking"**.
-#### 3. Metrics Visualization Tool We proveide a one-click script to sumarize
+base_v1 ``` The evaluation tasks contain ***12 datasets*** of **"Reranking"**.
+#### 3. Metrics Visualization Tool We provide a one-click script to summarize
 evaluation results of `embedding` and `reranker` models as [Embedding Models
 Evaluation Summary](./Docs/EvaluationSummary/embedding_eval_summary.md) and
 [Reranker Models Evaluation Summary](./Docs/EvaluationSummary/
 reranker_eval_summary.md). ```bash python BCEmbedding/evaluation/mteb/
 summarize_eval_results.py --results_dir {your_embedding_results_dir |
 your_reranker_results_dir} ``` ### Evaluate RAG by LlamaIndex [LlamaIndex]
 (https://github.com/run-llama/llama_index) is a famous data framework for LLM-
@@ -243,38 +269,42 @@
 reciprocal rank is 1/2, and so on. ***The larger, the better.*** #### 2.
 Reproduce [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-
 the-best-embedding-reranker-models-42d079022e83) In order to compare our
 `BCEmbedding` with other embedding and reranker models fairly, we provide a
 one-click script to reproduce results of the LlamaIndex Blog, including our
 `BCEmbedding`: ```bash # There should be two GPUs available at least.
 CUDA_VISIBLE_DEVICES=0,1 python BCEmbedding/tools/eval_rag/
-eval_llamaindex_reproduce.py ``` Then, sumarize the evaluation results by:
+eval_llamaindex_reproduce.py ``` Then, summarize the evaluation results by:
 ```bash python BCEmbedding/tools/eval_rag/summarize_eval_results.py --
-results_dir BCEmbedding/results/rag_reproduce_results ``` Results Reproduced
+results_dir BCEmbedding/results/rag_reproduce_results ``` Results reproduced
 from the LlamaIndex Blog can be checked in ***[Reproduced Summary of RAG
 Evaluation](./Docs/EvaluationSummary/rag_eval_reproduced_summary.md)***, with
 some obvious ***conclusions***: - In `WithoutReranker` setting, our `bce-
 embedding-base_v1` outperforms all the other embedding models. - With fixing
-the embedding model, our `bce-reranker-base_v1` achieves the best performence.
+the embedding model, our `bce-reranker-base_v1` achieves the best performance.
 - ***The combination of `bce-embedding-base_v1` and `bce-reranker-base_v1` is
 SOTA.*** #### 3. Broad Domain Adaptability The evaluation of [LlamaIndex Blog]
 (https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-
 models-42d079022e83) is **monolingual, small amount of data, and specific
 domain** (just including "llama2" paper). In order to evaluate the **broad
 domain adaptability, bilingual and crosslingual capability**, we follow the
 blog to build a multiple domains evaluation dataset (includding "Computer
 Science", "Physics", "Biology", "Economics", "Math", and "Quantitative
 Finance". [Details](./BCEmbedding/tools/eval_rag/eval_pdfs/)), named
 [CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/
-CrosslingualMultiDomainsDataset), **by OpenAI `gpt-4-1106-preview` for high
-quality**. First, run following cmd to evaluate the most popular and powerful
+CrosslingualMultiDomainsDataset): - To prevent test data leakage, English eval
+data is selected from the latest English articles in various fields on ArXiv,
+up to date December 30, 2023. Chinese eval data is selected from high-quality,
+as recent as possible, Chinese articles in the corresponding fields on Semantic
+Scholar. - Use OpenAI `gpt-4-1106-preview` to produce eval data for high
+quality. First, run following cmd to evaluate the most popular and powerful
 embedding and reranker models: ```bash # There should be two GPUs available at
 least. CUDA_VISIBLE_DEVICES=0,1 python BCEmbedding/tools/eval_rag/
 eval_llamaindex_multiple_domains.py ``` Then, run the following script to
-sumarize the evaluation results: ```bash python BCEmbedding/tools/eval_rag/
+summarize the evaluation results: ```bash python BCEmbedding/tools/eval_rag/
 summarize_eval_results.py --results_dir BCEmbedding/results/rag_results ``` The
 summary of multiple domains evaluations can be seen in _M_u_l_t_i_p_l_e_ _D_o_m_a_i_n_s
 _S_c_e_n_a_r_i_o_s. ## ð Leaderboard ### Semantic Representation Evaluations in MTEB
 #### 1. Embedding Models | Model | Dimensions | Pooler | Instructions |
 Retrieval (47) | STS (19) | PairClassification (5) | Classification (21) |
 Reranking (12) | Clustering (15) | ***AVG*** (119) | | :-----------------------
 ----------- | :--------: | :------: | :----------: | :-------------: | :-------
@@ -293,55 +323,65 @@
 Free | 34.85 | 59.74 | 67.69 | 60.07 | 48.99 | 31.62 | 46.78 | | e5-large-v2 |
 1024 | `mean` | Need | 35.98 | 55.23 | 75.28 | 59.53 | 42.12 | 36.51 | 46.52 |
 | multilingual-e5-base | 768 | `mean` | Need | 54.73 | 65.49 | 76.97 | 69.72 |
 55.01 | 38.44 | 58.34 | | multilingual-e5-large | 1024 | `mean` | Need | 56.76
 | **66.79** | 78.80 | **71.61** | 56.49 | **43.09** | **60.50** | | ***bce-
 embedding-base_v1*** | 768 | `cls` | Free | **57.60** | 65.73 | 74.96 | 69.00 |
 **57.29** | 38.95 | 59.43 | ***NOTE:*** - Our ***bce-embedding-base_v1***
-outperforms other opensource embedding models with comparable model sizes. -
-***114 datastes including 119 eval results*** (some dataset contains multiple
+outperforms other open-source embedding models with comparable model sizes. -
+***114 datasets including 119 eval results*** (some dataset contains multiple
 languages) of "Retrieval", "STS", "PairClassification", "Classification",
 "Reranking" and "Clustering" in ***`["en", "zh", "en-zh", "zh-en"]` setting***,
 including **MTEB and CMTEB**. - The [crosslingual evaluation datasets](./
 BCEmbedding/evaluation/c_mteb/Retrieval.py) we released belong to `Retrieval`
-task. - More evaluation details should be checked [Embedding Models
+task. - More evaluation details should be checked in [Embedding Models
 Evaluations](./Docs/EvaluationSummary/embedding_eval_summary.md). #### 2.
 Reranker Models | Model | Reranking (12) | ***AVG*** (12) | | :----------------
 ----------------- | :-------------: | :--------------------: | | bge-reranker-
 base | 59.04 | 59.04 | | bge-reranker-large | 60.86 | 60.86 | | ***bce-
 reranker-base_v1*** | **61.29** | ***61.29*** | ***NOTE:*** - Our ***bce-
-reranker-base_v1*** outperforms other opensource reranker models. - ***12
-datastes*** of "Reranking" in ***`["en", "zh", "en-zh", "zh-en"]` setting***. -
-More evaluation details should be checked [Reranker Models Evaluations](./Docs/
-EvaluationSummary/reranker_eval_summary.md). ### RAG Evaluations in LlamaIndex
-#### 1. Multiple Domains Scenarios [./Docs/assets/
-rag_eval_multiple_domains_summary.jpg]***NOTE:*** - Consistent with our ***
-[Reproduced Results](./Docs/EvaluationSummary/
-rag_eval_reproduced_summary.md)*** of [LlamaIndex Blog](https://
-blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-
-42d079022e83). - In `WithoutReranker` setting, our `bce-embedding-base_v1`
-outperforms all the other embedding models. - With fixing the embedding model,
-our `bce-reranker-base_v1` achieves the best performence. - **The combination
-of `bce-embedding-base_v1` and `bce-reranker-base_v1` is SOTA**. ## ð 
-Youdao's BCEmbedding API For users who prefer a hassle-free experience without
-the need to download and configure the model on their own systems,
-`BCEmbedding` is readily accessible through Youdao's API. This option offers a
-streamlined and efficient way to integrate BCEmbedding into your projects,
-bypassing the complexities of manual setup and maintenance. Detailed
-instructions and comprehensive API documentation are available at [Youdao
-BCEmbedding API](https://ai.youdao.com/DOCSIRMA/html/aigc/api/embedding/
-index.html). Here, you'll find all the necessary guidance to easily implement
-`BCEmbedding` across a variety of use cases, ensuring a smooth and effective
-integration for optimal results. ## ð§² WeChat Group Welcome to scan the QR
-code below and join the WeChat group. [./Docs/assets/Wechat.jpg]## âï¸
-Citation If you use `BCEmbedding` in your research or project, please feel free
-to cite and star it: ``` @misc{youdao_bcembedding_2023, title={BCEmbedding:
-Bilingual and Crosslingual Embedding for RAG}, author={NetEase Youdao, Inc.},
-year={2023}, howpublished={\url{https://github.com/netease-youdao/BCEmbedding}}
-} ``` ## ð License `BCEmbedding` is licensed under [Apache 2.0 License](./
-LICENSE) ## ð Related Links [Netease Youdao - QAnything](https://github.com/
-netease-youdao/qanything) [FlagEmbedding](https://github.com/FlagOpen/
-FlagEmbedding) [MTEB](https://github.com/embeddings-benchmark/mteb) [C_MTEB]
-(https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) [LLama Index]
-(https://github.com/run-llama/llama_index) | [LlamaIndex Blog](https://
-blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-
-42d079022e83)
+reranker-base_v1*** outperforms other open-source reranker models. - ***12
+datasets*** of "Reranking" in ***`["en", "zh", "en-zh", "zh-en"]` setting***. -
+More evaluation details should be checked in [Reranker Models Evaluations](./
+Docs/EvaluationSummary/reranker_eval_summary.md). ### RAG Evaluations in
+LlamaIndex #### 1. Multiple Domains Scenarios [./Docs/assets/
+rag_eval_multiple_domains_summary.jpg]***NOTE:*** - Data Quality: - To prevent
+test data leakage, English eval data is selected from the latest English
+articles in various fields on ArXiv, up to date December 30, 2023. Chinese eval
+data is selected from high-quality, as recent as possible, Chinese articles in
+the corresponding fields on Semantic Scholar. - Use OpenAI `gpt-4-1106-preview`
+to produce eval data for high quality. - Evaluated in ***`["en", "zh", "en-zh",
+"zh-en"]` setting***. If you are interested in monolingual setting, please
+check in [Chinese RAG evaluations with ["zh"] setting](./Docs/
+EvaluationSummary/rag_eval_multiple_domains_summary_zh.md), and [English RAG
+evaluations with ["en"] setting](./Docs/EvaluationSummary/
+rag_eval_multiple_domains_summary_en.md). - Consistent with our ***[Reproduced
+Results](./Docs/EvaluationSummary/rag_eval_reproduced_summary.md)*** of
+[LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-
+embedding-reranker-models-42d079022e83). - In `WithoutReranker` setting, our
+`bce-embedding-base_v1` outperforms all the other embedding models. - With
+fixing the embedding model, our `bce-reranker-base_v1` achieves the best
+performance. - **The combination of `bce-embedding-base_v1` and `bce-reranker-
+base_v1` is SOTA**. ## ð  Youdao's BCEmbedding API For users who prefer a
+hassle-free experience without the need to download and configure the model on
+their own systems, `BCEmbedding` is readily accessible through Youdao's API.
+This option offers a streamlined and efficient way to integrate BCEmbedding
+into your projects, bypassing the complexities of manual setup and maintenance.
+Detailed instructions and comprehensive API documentation are available at
+[Youdao BCEmbedding API](https://ai.youdao.com/DOCSIRMA/html/aigc/api/
+embedding/index.html). Here, you'll find all the necessary guidance to easily
+implement `BCEmbedding` across a variety of use cases, ensuring a smooth and
+effective integration for optimal results. ## ð§² WeChat Group Welcome to scan
+the QR code below and join the WeChat group. [./Docs/assets/Wechat.jpg]##
+âï¸ Citation If you use `BCEmbedding` in your research or project, please
+feel free to cite and star it: ``` @misc{youdao_bcembedding_2023, title=
+{BCEmbedding: Bilingual and Crosslingual Embedding for RAG}, author={NetEase
+Youdao, Inc.}, year={2023}, howpublished={\url{https://github.com/netease-
+youdao/BCEmbedding}} } ``` ## ð License `BCEmbedding` is licensed under
+[Apache 2.0 License](./LICENSE) ## ð Related Links [Netease Youdao -
+QAnything](https://github.com/netease-youdao/qanything) [FlagEmbedding](https:/
+/github.com/FlagOpen/FlagEmbedding) [MTEB](https://github.com/embeddings-
+benchmark/mteb) [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/
+C_MTEB) [LLama Index](https://github.com/run-llama/llama_index) | [LlamaIndex
+Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-
+reranker-models-42d079022e83) [HuixiangDou](https://github.com/internlm/
+huixiangdou)
```

## Comparing `BCEmbedding-0.1.3.dist-info/RECORD` & `BCEmbedding-0.1.4.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-BCEmbedding/__init__.py,sha256=JxGfmNqFE1m7OIzQvBxlz4fywCMyEXEaoniXg0MVgQA,146
+BCEmbedding/__init__.py,sha256=H35SvW466Df1SHiCaOBfTNFHPnoSSeEKI5dssG1z8hM,145
 BCEmbedding/evaluation/__init__.py,sha256=YefkjYpadq-v3iB41EYw1g7JhPEwx2vC_eWYPehWR3c,146
 BCEmbedding/evaluation/c_mteb/Reranking.py,sha256=pxUVonkWvdIqsX7hg9QN4SY4qSMGY_UWQEZ67n4iV_A,8786
 BCEmbedding/evaluation/c_mteb/Retrieval.py,sha256=S3HsoGJDp5ix9Om8BPp9bgpyZEvCrCwqZMjnkAy7W8U,10905
 BCEmbedding/evaluation/c_mteb/__init__.py,sha256=t4jvOIUMjWwETNvPbxqGPx5UA-719tEmIDoWcEBjwsM,234
-BCEmbedding/evaluation/c_mteb/yd_dres_model.py,sha256=7N7ljNrO8nNxSZkbVE8ndPis17E4xASgM3vapQdMaRo,3503
-BCEmbedding/models/__init__.py,sha256=sXUsjMTPoZ6oTHkOmvACLrkNj98a2BB2YE08P6TmSc0,250
-BCEmbedding/models/embedding.py,sha256=Vjx3oVxNDjKfUQ8mz9ws3d1eUNQ449aPjXMsmfT3Nrk,4404
-BCEmbedding/models/reranker.py,sha256=N_hBxoKfdRBym3Csrn2_4OBi1ilyEi5sgXaXZnfI7vQ,5722
-BCEmbedding/models/utils.py,sha256=jIrgsGIc8rqWLz6tNANCRnC0Izx7zuNQFh8t38VJzCQ,2473
+BCEmbedding/evaluation/c_mteb/yd_dres_model.py,sha256=-fo-p-Pz9DTOdDQu9PMVsBDy4hHvmnD3Qda8IoKFRzA,3503
+BCEmbedding/models/__init__.py,sha256=Sm9_on6NLokJ3fjkYHsLW-xLypIdz5ihNVwDvUZ2b9o,250
+BCEmbedding/models/embedding.py,sha256=hqWAuaQsOeNy944QjGlFEaO_69hB__54w8Ef2gqLmBY,4404
+BCEmbedding/models/reranker.py,sha256=8ZP11I1TlI4lxD3QHujh-_N3KGaowHN0oIncJemwXig,5722
+BCEmbedding/models/utils.py,sha256=W55zmT9yPqCikitF0t9Qw6OvKCfWH45zgbIT9FDj50o,2588
 BCEmbedding/tools/__init__.py,sha256=N6oz0MICVTEAWdhBgOzq1GVHpgihhcdF3p-_lDDedlk,124
 BCEmbedding/tools/langchain/__init__.py,sha256=CtQePyCxAaLUm3TXiZQGUeSkIN7f7aPRs2eizDpqiB0,158
-BCEmbedding/tools/langchain/bce_rerank.py,sha256=d8w10S4TR3efmZ_bJ7KG-SO1kgfu4Z0qjZBdWb2llX8,3424
+BCEmbedding/tools/langchain/bce_rerank.py,sha256=vMZD-L3jb5bzStxfcrm6NkiUxRFvsb_OHyxozb3KjjI,3338
 BCEmbedding/tools/llama_index/__init__.py,sha256=eZ3as37D-xAnf4tquFVNjCHQT16bj2KV2RxLtU2rZos,158
-BCEmbedding/tools/llama_index/bce_rerank.py,sha256=wbqVZ-Q4Gm6RHBbT_ipujHrVSgzQC7ih37uODXxk6ls,3167
-BCEmbedding/utils/__init__.py,sha256=-A4YuVirKfjXS73tDXC1M05Xnwy2-5H8h6pS5zGwYHQ,192
+BCEmbedding/tools/llama_index/bce_rerank.py,sha256=5jqJ8b17q_kiYP1mf0NSWaQLfMWkRVs45TM5iM38SeU,3151
+BCEmbedding/utils/__init__.py,sha256=Xt5C2GkMMaRuV6qbnmnu2OElq0FR0S_2f5Xn9hA7NRs,192
 BCEmbedding/utils/logger.py,sha256=k8d60HuFcaqnUCOwlgJ33n7gyqmXjMgC47tYHj77HLM,401
-BCEmbedding/utils/query_instructions.py,sha256=YDp2O_eVOa0XHFY7miBUSG5Ka8y53Xu325x2dBcgP0Y,1671
-BCEmbedding-0.1.3.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-BCEmbedding-0.1.3.dist-info/METADATA,sha256=dN2-Zus69IsNG32qMNvxCiDeo0kcw3blko3NVrKlRHI,29477
-BCEmbedding-0.1.3.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-BCEmbedding-0.1.3.dist-info/top_level.txt,sha256=h6kdwVU7f8_ae0K_sK5wG7m4kdEIpO3DQ-Eb2q-piGA,12
-BCEmbedding-0.1.3.dist-info/RECORD,,
+BCEmbedding/utils/query_instructions.py,sha256=tywpweF2y3Hf3Uf6meMGK2QqZG6smNeQbbx5mY-ZpQ8,1671
+BCEmbedding-0.1.4.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+BCEmbedding-0.1.4.dist-info/METADATA,sha256=qgje9Dq71bJRB2ZWaz-yvLm00xW8vf77YH0YqNnrALM,32615
+BCEmbedding-0.1.4.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
+BCEmbedding-0.1.4.dist-info/top_level.txt,sha256=h6kdwVU7f8_ae0K_sK5wG7m4kdEIpO3DQ-Eb2q-piGA,12
+BCEmbedding-0.1.4.dist-info/RECORD,,
```

