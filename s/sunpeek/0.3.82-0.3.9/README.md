# Comparing `tmp/sunpeek-0.3.82-py3-none-any.whl.zip` & `tmp/sunpeek-0.3.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,79 +1,76 @@
-Zip file size: 266301 bytes, number of entries: 77
--rw-r--r--  2.0 unx      705 b- defN 80-Jan-01 00:00 sunpeek/__init__.py
+Zip file size: 195605 bytes, number of entries: 74
+-rw-r--r--  2.0 unx     1035 b- defN 80-Jan-01 00:00 sunpeek/__init__.py
 -rw-r--r--  2.0 unx      264 b- defN 80-Jan-01 00:00 sunpeek/api/__init__.py
 -rw-r--r--  2.0 unx      581 b- defN 80-Jan-01 00:00 sunpeek/api/dependencies.py
--rw-r--r--  2.0 unx     7981 b- defN 80-Jan-01 00:00 sunpeek/api/main.py
+-rw-r--r--  2.0 unx     7932 b- defN 80-Jan-01 00:00 sunpeek/api/main.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 sunpeek/api/routers/__init__.py
--rw-r--r--  2.0 unx     1712 b- defN 80-Jan-01 00:00 sunpeek/api/routers/api_jobs.py
--rw-r--r--  2.0 unx     7924 b- defN 80-Jan-01 00:00 sunpeek/api/routers/config.py
--rw-r--r--  2.0 unx    11261 b- defN 80-Jan-01 00:00 sunpeek/api/routers/evaluations.py
--rw-r--r--  2.0 unx     9314 b- defN 80-Jan-01 00:00 sunpeek/api/routers/files.py
--rw-r--r--  2.0 unx     1402 b- defN 80-Jan-01 00:00 sunpeek/api/routers/helper.py
--rw-r--r--  2.0 unx    25685 b- defN 80-Jan-01 00:00 sunpeek/api/routers/plant.py
--rw-r--r--  2.0 unx     1456 b- defN 80-Jan-01 00:00 sunpeek/base_model.py
+-rw-r--r--  2.0 unx     1718 b- defN 80-Jan-01 00:00 sunpeek/api/routers/api_jobs.py
+-rw-r--r--  2.0 unx     6312 b- defN 80-Jan-01 00:00 sunpeek/api/routers/config.py
+-rw-r--r--  2.0 unx     6090 b- defN 80-Jan-01 00:00 sunpeek/api/routers/evaluations.py
+-rw-r--r--  2.0 unx     6582 b- defN 80-Jan-01 00:00 sunpeek/api/routers/files.py
+-rw-r--r--  2.0 unx      245 b- defN 80-Jan-01 00:00 sunpeek/api/routers/helper.py
+-rw-r--r--  2.0 unx    19263 b- defN 80-Jan-01 00:00 sunpeek/api/routers/plant.py
+-rw-r--r--  2.0 unx      780 b- defN 80-Jan-01 00:00 sunpeek/base_model.py
 -rw-r--r--  2.0 unx      100 b- defN 80-Jan-01 00:00 sunpeek/common/__init__.py
--rw-r--r--  2.0 unx     1316 b- defN 80-Jan-01 00:00 sunpeek/common/common_units.py
--rw-r--r--  2.0 unx     1843 b- defN 80-Jan-01 00:00 sunpeek/common/config_parser.py
--rw-r--r--  2.0 unx     1893 b- defN 80-Jan-01 00:00 sunpeek/common/errors.py
--rw-r--r--  2.0 unx    21050 b- defN 80-Jan-01 00:00 sunpeek/common/logos/Icon_Transparent.png
--rw-r--r--  2.0 unx    23452 b- defN 80-Jan-01 00:00 sunpeek/common/logos/Logo_wide.png
--rw-r--r--  2.0 unx    29201 b- defN 80-Jan-01 00:00 sunpeek/common/plot_utils.py
--rw-r--r--  2.0 unx     6731 b- defN 80-Jan-01 00:00 sunpeek/common/time_zone.py
--rw-r--r--  2.0 unx    18481 b- defN 80-Jan-01 00:00 sunpeek/common/unit_uncertainty.py
--rw-r--r--  2.0 unx     6710 b- defN 80-Jan-01 00:00 sunpeek/common/utils.py
--rw-r--r--  2.0 unx      985 b- defN 80-Jan-01 00:00 sunpeek/components/__init__.py
--rw-r--r--  2.0 unx    13537 b- defN 80-Jan-01 00:00 sunpeek/components/base.py
--rw-r--r--  2.0 unx     3182 b- defN 80-Jan-01 00:00 sunpeek/components/components_factories.py
--rw-r--r--  2.0 unx    26722 b- defN 80-Jan-01 00:00 sunpeek/components/fluids.py
--rw-r--r--  2.0 unx    15692 b- defN 80-Jan-01 00:00 sunpeek/components/fluids_wpd_models.py
--rw-r--r--  2.0 unx    16645 b- defN 80-Jan-01 00:00 sunpeek/components/helpers.py
--rw-r--r--  2.0 unx    15768 b- defN 80-Jan-01 00:00 sunpeek/components/iam_methods.py
+-rw-r--r--  2.0 unx     1248 b- defN 80-Jan-01 00:00 sunpeek/common/common_units.py
+-rw-r--r--  2.0 unx     1968 b- defN 80-Jan-01 00:00 sunpeek/common/config_parser.py
+-rw-r--r--  2.0 unx     1802 b- defN 80-Jan-01 00:00 sunpeek/common/errors.py
+-rw-r--r--  2.0 unx     6639 b- defN 80-Jan-01 00:00 sunpeek/common/time_zone.py
+-rw-r--r--  2.0 unx    17863 b- defN 80-Jan-01 00:00 sunpeek/common/unit_uncertainty.py
+-rw-r--r--  2.0 unx     5134 b- defN 80-Jan-01 00:00 sunpeek/common/utils.py
+-rw-r--r--  2.0 unx      986 b- defN 80-Jan-01 00:00 sunpeek/components/__init__.py
+-rw-r--r--  2.0 unx    13697 b- defN 80-Jan-01 00:00 sunpeek/components/base.py
+-rw-r--r--  2.0 unx     2800 b- defN 80-Jan-01 00:00 sunpeek/components/components_factories.py
+-rw-r--r--  2.0 unx    26948 b- defN 80-Jan-01 00:00 sunpeek/components/fluids.py
+-rw-r--r--  2.0 unx    14800 b- defN 80-Jan-01 00:00 sunpeek/components/fluids_wpd_models.py
+-rw-r--r--  2.0 unx    14886 b- defN 80-Jan-01 00:00 sunpeek/components/helpers.py
+-rw-r--r--  2.0 unx    15784 b- defN 80-Jan-01 00:00 sunpeek/components/iam_methods.py
 -rw-r--r--  2.0 unx      674 b- defN 80-Jan-01 00:00 sunpeek/components/jobs.py
 -rw-r--r--  2.0 unx     3804 b- defN 80-Jan-01 00:00 sunpeek/components/operational_events.py
--rw-r--r--  2.0 unx     4201 b- defN 80-Jan-01 00:00 sunpeek/components/outputs_pc_method.py
--rw-r--r--  2.0 unx    49950 b- defN 80-Jan-01 00:00 sunpeek/components/physical.py
--rw-r--r--  2.0 unx    25262 b- defN 80-Jan-01 00:00 sunpeek/components/sensor.py
--rw-r--r--  2.0 unx    11598 b- defN 80-Jan-01 00:00 sunpeek/components/sensor_types.py
--rw-r--r--  2.0 unx    24175 b- defN 80-Jan-01 00:00 sunpeek/components/types.py
+-rw-r--r--  2.0 unx    49813 b- defN 80-Jan-01 00:00 sunpeek/components/physical.py
+-rw-r--r--  2.0 unx     3075 b- defN 80-Jan-01 00:00 sunpeek/components/results.py
+-rw-r--r--  2.0 unx    23308 b- defN 80-Jan-01 00:00 sunpeek/components/sensor.py
+-rw-r--r--  2.0 unx    11480 b- defN 80-Jan-01 00:00 sunpeek/components/sensor_types.py
+-rw-r--r--  2.0 unx    20317 b- defN 80-Jan-01 00:00 sunpeek/components/types.py
 -rw-r--r--  2.0 unx       87 b- defN 80-Jan-01 00:00 sunpeek/core_methods/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 sunpeek/core_methods/common/__init__.py
--rw-r--r--  2.0 unx    15831 b- defN 80-Jan-01 00:00 sunpeek/core_methods/common/main.py
--rw-r--r--  2.0 unx     4717 b- defN 80-Jan-01 00:00 sunpeek/core_methods/pc_method/__init__.py
--rw-r--r--  2.0 unx    20409 b- defN 80-Jan-01 00:00 sunpeek/core_methods/pc_method/formula.py
--rw-r--r--  2.0 unx    31722 b- defN 80-Jan-01 00:00 sunpeek/core_methods/pc_method/main.py
--rw-r--r--  2.0 unx    66092 b- defN 80-Jan-01 00:00 sunpeek/core_methods/pc_method/plotting.py
--rw-r--r--  2.0 unx     8517 b- defN 80-Jan-01 00:00 sunpeek/core_methods/pc_method/wrapper.py
+-rw-r--r--  2.0 unx    13751 b- defN 80-Jan-01 00:00 sunpeek/core_methods/common/main.py
+-rw-r--r--  2.0 unx     4570 b- defN 80-Jan-01 00:00 sunpeek/core_methods/pc_method/__init__.py
+-rw-r--r--  2.0 unx    14730 b- defN 80-Jan-01 00:00 sunpeek/core_methods/pc_method/equation.py
+-rw-r--r--  2.0 unx    31591 b- defN 80-Jan-01 00:00 sunpeek/core_methods/pc_method/main.py
+-rw-r--r--  2.0 unx    22617 b- defN 80-Jan-01 00:00 sunpeek/core_methods/pc_method/plotting.py
+-rw-r--r--  2.0 unx    10738 b- defN 80-Jan-01 00:00 sunpeek/core_methods/pc_method/wrapper.py
 -rw-r--r--  2.0 unx       83 b- defN 80-Jan-01 00:00 sunpeek/core_methods/virtuals/__init__.py
--rw-r--r--  2.0 unx    39734 b- defN 80-Jan-01 00:00 sunpeek/core_methods/virtuals/calculations.py
--rw-r--r--  2.0 unx     4220 b- defN 80-Jan-01 00:00 sunpeek/core_methods/virtuals/main.py
--rw-r--r--  2.0 unx    34383 b- defN 80-Jan-01 00:00 sunpeek/core_methods/virtuals/radiation.py
--rw-r--r--  2.0 unx     3452 b- defN 80-Jan-01 00:00 sunpeek/core_methods/virtuals/virtuals_array.py
--rw-r--r--  2.0 unx     3809 b- defN 80-Jan-01 00:00 sunpeek/core_methods/virtuals/virtuals_plant.py
+-rw-r--r--  2.0 unx    39360 b- defN 80-Jan-01 00:00 sunpeek/core_methods/virtuals/calculations.py
+-rw-r--r--  2.0 unx     4202 b- defN 80-Jan-01 00:00 sunpeek/core_methods/virtuals/main.py
+-rw-r--r--  2.0 unx    35090 b- defN 80-Jan-01 00:00 sunpeek/core_methods/virtuals/radiation.py
+-rw-r--r--  2.0 unx     3446 b- defN 80-Jan-01 00:00 sunpeek/core_methods/virtuals/virtuals_array.py
+-rw-r--r--  2.0 unx     3773 b- defN 80-Jan-01 00:00 sunpeek/core_methods/virtuals/virtuals_plant.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 sunpeek/data_handling/__init__.py
--rw-r--r--  2.0 unx    35243 b- defN 80-Jan-01 00:00 sunpeek/data_handling/context.py
--rw-r--r--  2.0 unx    21521 b- defN 80-Jan-01 00:00 sunpeek/data_handling/data_uploader.py
+-rw-r--r--  2.0 unx    23667 b- defN 80-Jan-01 00:00 sunpeek/data_handling/context.py
+-rw-r--r--  2.0 unx    24335 b- defN 80-Jan-01 00:00 sunpeek/data_handling/data_uploader.py
 -rw-r--r--  2.0 unx     1849 b- defN 80-Jan-01 00:00 sunpeek/data_handling/wrapper.py
--rw-r--r--  2.0 unx       61 b- defN 80-Jan-01 00:00 sunpeek/db_utils/__init__.py
--rw-r--r--  2.0 unx     5898 b- defN 80-Jan-01 00:00 sunpeek/db_utils/crud.py
+-rw-r--r--  2.0 unx       25 b- defN 80-Jan-01 00:00 sunpeek/db_utils/__init__.py
+-rw-r--r--  2.0 unx     5593 b- defN 80-Jan-01 00:00 sunpeek/db_utils/crud.py
 -rw-r--r--  2.0 unx    16575 b- defN 80-Jan-01 00:00 sunpeek/db_utils/db_data_operations.py
--rw-r--r--  2.0 unx     3371 b- defN 80-Jan-01 00:00 sunpeek/db_utils/init_db.py
--rw-r--r--  2.0 unx      351 b- defN 80-Jan-01 00:00 sunpeek/definitions/__init__.py
--rw-r--r--  2.0 unx    28733 b- defN 80-Jan-01 00:00 sunpeek/definitions/collectors.py
+-rw-r--r--  2.0 unx     1921 b- defN 80-Jan-01 00:00 sunpeek/db_utils/init_db.py
+-rw-r--r--  2.0 unx      454 b- defN 80-Jan-01 00:00 sunpeek/definitions/__init__.py
+-rw-r--r--  2.0 unx    24370 b- defN 80-Jan-01 00:00 sunpeek/definitions/collector_types.py
 -rw-r--r--  2.0 unx      334 b- defN 80-Jan-01 00:00 sunpeek/definitions/fluid_data/Gasokol, corroStar mixture/density.csv
 -rw-r--r--  2.0 unx      337 b- defN 80-Jan-01 00:00 sunpeek/definitions/fluid_data/Gasokol, corroStar mixture/heat capacity.csv
 -rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 sunpeek/definitions/fluid_data/Pekasolar_FHW/density.csv
 -rw-r--r--  2.0 unx      241 b- defN 80-Jan-01 00:00 sunpeek/definitions/fluid_data/Pekasolar_FHW/heat capacity.csv
 -rw-r--r--  2.0 unx     3450 b- defN 80-Jan-01 00:00 sunpeek/definitions/fluid_data/Wocklum Thermum P/density.csv
 -rw-r--r--  2.0 unx     2532 b- defN 80-Jan-01 00:00 sunpeek/definitions/fluid_data/Wocklum Thermum P/heat capacity.csv
 -rw-r--r--  2.0 unx    33003 b- defN 80-Jan-01 00:00 sunpeek/definitions/fluid_definitions.py
 -rw-r--r--  2.0 unx      528 b- defN 80-Jan-01 00:00 sunpeek/demo/__init__.py
--rw-r--r--  2.0 unx     1860 b- defN 80-Jan-01 00:00 sunpeek/demo/demo_plant.py
--rw-r--r--  2.0 unx     5216 b- defN 80-Jan-01 00:00 sunpeek/demo/demo_plant_script.py
--rw-r--r--  2.0 unx     6116 b- defN 80-Jan-01 00:00 sunpeek/exporter.py
--rw-r--r--  2.0 unx    38773 b- defN 80-Jan-01 00:00 sunpeek/serializable_models.py
--rw-r--r--  2.0 unx     7652 b- defN 80-Jan-01 00:00 sunpeek-0.3.82.dist-info/COPYING.LESSER
--rw-r--r--  2.0 unx    15279 b- defN 80-Jan-01 00:00 sunpeek-0.3.82.dist-info/METADATA
--rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 sunpeek-0.3.82.dist-info/WHEEL
--rw-r--r--  2.0 unx    35149 b- defN 80-Jan-01 00:00 sunpeek-0.3.82.dist-info/COPYING
-?rw-r--r--  2.0 unx     7016 b- defN 16-Jan-01 00:00 sunpeek-0.3.82.dist-info/RECORD
-77 files, 895577 bytes uncompressed, 255071 bytes compressed:  71.5%
+-rw-r--r--  2.0 unx     1902 b- defN 80-Jan-01 00:00 sunpeek/demo/demo_plant.py
+-rw-r--r--  2.0 unx     5168 b- defN 80-Jan-01 00:00 sunpeek/demo/demo_plant_script.py
+-rw-r--r--  2.0 unx     6191 b- defN 80-Jan-01 00:00 sunpeek/exporter.py
+-rw-r--r--  2.0 unx    31326 b- defN 80-Jan-01 00:00 sunpeek/serializable_models.py
+-rw-r--r--  2.0 unx     7652 b- defN 80-Jan-01 00:00 sunpeek-0.3.9.dist-info/COPYING.LESSER
+-rw-r--r--  2.0 unx    15186 b- defN 80-Jan-01 00:00 sunpeek-0.3.9.dist-info/METADATA
+-rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 sunpeek-0.3.9.dist-info/WHEEL
+-rw-r--r--  2.0 unx    35149 b- defN 80-Jan-01 00:00 sunpeek-0.3.9.dist-info/COPYING
+?rw-r--r--  2.0 unx     6729 b- defN 16-Jan-01 00:00 sunpeek-0.3.9.dist-info/RECORD
+74 files, 720659 bytes uncompressed, 184827 bytes compressed:  74.4%
```

## zipnote {}

```diff
@@ -42,23 +42,14 @@
 
 Filename: sunpeek/common/config_parser.py
 Comment: 
 
 Filename: sunpeek/common/errors.py
 Comment: 
 
-Filename: sunpeek/common/logos/Icon_Transparent.png
-Comment: 
-
-Filename: sunpeek/common/logos/Logo_wide.png
-Comment: 
-
-Filename: sunpeek/common/plot_utils.py
-Comment: 
-
 Filename: sunpeek/common/time_zone.py
 Comment: 
 
 Filename: sunpeek/common/unit_uncertainty.py
 Comment: 
 
 Filename: sunpeek/common/utils.py
@@ -87,18 +78,18 @@
 
 Filename: sunpeek/components/jobs.py
 Comment: 
 
 Filename: sunpeek/components/operational_events.py
 Comment: 
 
-Filename: sunpeek/components/outputs_pc_method.py
+Filename: sunpeek/components/physical.py
 Comment: 
 
-Filename: sunpeek/components/physical.py
+Filename: sunpeek/components/results.py
 Comment: 
 
 Filename: sunpeek/components/sensor.py
 Comment: 
 
 Filename: sunpeek/components/sensor_types.py
 Comment: 
@@ -114,15 +105,15 @@
 
 Filename: sunpeek/core_methods/common/main.py
 Comment: 
 
 Filename: sunpeek/core_methods/pc_method/__init__.py
 Comment: 
 
-Filename: sunpeek/core_methods/pc_method/formula.py
+Filename: sunpeek/core_methods/pc_method/equation.py
 Comment: 
 
 Filename: sunpeek/core_methods/pc_method/main.py
 Comment: 
 
 Filename: sunpeek/core_methods/pc_method/plotting.py
 Comment: 
@@ -171,15 +162,15 @@
 
 Filename: sunpeek/db_utils/init_db.py
 Comment: 
 
 Filename: sunpeek/definitions/__init__.py
 Comment: 
 
-Filename: sunpeek/definitions/collectors.py
+Filename: sunpeek/definitions/collector_types.py
 Comment: 
 
 Filename: sunpeek/definitions/fluid_data/Gasokol, corroStar mixture/density.csv
 Comment: 
 
 Filename: sunpeek/definitions/fluid_data/Gasokol, corroStar mixture/heat capacity.csv
 Comment: 
@@ -210,23 +201,23 @@
 
 Filename: sunpeek/exporter.py
 Comment: 
 
 Filename: sunpeek/serializable_models.py
 Comment: 
 
-Filename: sunpeek-0.3.82.dist-info/COPYING.LESSER
+Filename: sunpeek-0.3.9.dist-info/COPYING.LESSER
 Comment: 
 
-Filename: sunpeek-0.3.82.dist-info/METADATA
+Filename: sunpeek-0.3.9.dist-info/METADATA
 Comment: 
 
-Filename: sunpeek-0.3.82.dist-info/WHEEL
+Filename: sunpeek-0.3.9.dist-info/WHEEL
 Comment: 
 
-Filename: sunpeek-0.3.82.dist-info/COPYING
+Filename: sunpeek-0.3.9.dist-info/COPYING
 Comment: 
 
-Filename: sunpeek-0.3.82.dist-info/RECORD
+Filename: sunpeek-0.3.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## sunpeek/__init__.py

```diff
@@ -11,12 +11,18 @@
     try:
         with open(Path(__file__).parent.with_name('pyproject.toml'), 'rb') as f:
             t = tomli.load(f)
         __version__ = t['tool']['poetry']['version']
     except FileNotFoundError:    # Package is in a context where pyproject not available (e.g. pip installed)
         __version__ = os.environ['SUNPEEK_VERSION']
 
-pd.set_option('display.max_rows', 50)
-pd.set_option('display.max_columns', 15)
-pd.set_option('display.width', 500)
-pd.set_option('plotting.backend', 'matplotlib')
+# Some calculations return Inf values (eg. CoolProp fluids when temperature exceeds allowed range). With this
+# setting, all calculations can assume everything not a valid number is encoded as NaN and use pd.isna()
+pd.set_option('mode.use_inf_as_na', True)
 
+
+try:
+    import plotly.io as pio
+    pd.set_option("plotting.backend", "plotly")
+    pio.renderers.default = 'browser'
+except ModuleNotFoundError:
+    print("WARNING: module 'plotly' not found. This is only important for development")
```

## sunpeek/api/main.py

```diff
@@ -14,24 +14,24 @@
 from sunpeek.common import errors
 import sunpeek.common.time_zone as tz
 from sunpeek.api.routers import files, evaluations, config, plant, api_jobs
 from sunpeek.api.dependencies import get_query_token, session
 import sunpeek.serializable_models as smodels
 import sunpeek.exporter
 
-root_path = os.environ.get('SUNPEEK_API_ROOT_PATH', None)
+root_path = os.environ.get('HIT_API_ROOT_PATH', None)
 sp_logger.info(f"HarvestIT API started with root_path set to {root_path}")
 
 app = FastAPI(dependencies=[Depends(get_query_token)], title="SunPeek API", root_path=root_path,
               version=sunpeek.__version__, default_response_class=ORJSONResponse,
               responses={400: {"description": "Bad Request", "model": smodels.Error}})
 
 app.include_router(files.files_router)
 app.include_router(evaluations.evaluations_router)
-# app.include_router(evaluations.stored_evaluations_router)
+app.include_router(evaluations.stored_evaluations_router)
 app.include_router(config.config_router)
 app.include_router(plant.plants_router)
 app.include_router(plant.plant_router)
 app.include_router(plant.any_plant_router)
 app.include_router(api_jobs.jobs_router)
 
 
@@ -114,15 +114,14 @@
 
 app.add_middleware(
     CORSMiddleware,
     allow_origins=origins,
     allow_credentials=True,
     allow_methods=["*"],
     allow_headers=["*"],
-    expose_headers=["content-disposition"]
 )
 
 
 @app.exception_handler(500)
 def internal_server_error(request, exc):
     tb = traceback.format_tb(exc.__traceback__)
     sp_logger.error(exc, exc_info=exc)
```

## sunpeek/api/routers/api_jobs.py

```diff
@@ -1,10 +1,10 @@
 import uuid
 import os
-import datetime as dt
+import datetime
 from fastapi import APIRouter, Depends, Request, BackgroundTasks
 from fastapi.responses import FileResponse
 from typing import List
 from sqlalchemy.orm import Session
 
 from sunpeek.api.dependencies import session, crud
 import sunpeek.serializable_models as smodels
@@ -33,13 +33,13 @@
     return jobs
 
 
 @jobs_router.get("/{id}/result_file", response_class=FileResponse, summary="Get a job result by id")
 def result_file(id: uuid.UUID, background_tasks: BackgroundTasks, session: Session=Depends(session), crud=Depends(crud)):
     job = crud.get_components(session, "Job", id)
     if job.plant is not None:
-        filename = f'export_{job.plant.name}_{dt.datetime.now().__format__("%Y-%m-%d_%H%M")}.tar.gz'
+        filename = f'export_{job.plant.name}_{datetime.datetime.now().__format__("%Y-%m-%d_%H%M")}.tar.gz'
     else:
-        filename = f'export_{dt.datetime.now().__format__("%Y-%m-%d_%H%M")}.tar.gz'
+        filename = f'export_{datetime.datetime.now().__format__("%Y-%m-%d_%H%M")}.tar.gz'
     background_tasks.add_task(os.remove, job.result_path)
     background_tasks.add_task(crud.delete_component, session, job)
     return FileResponse(job.result_path, filename=filename)
```

## sunpeek/api/routers/config.py

```diff
@@ -1,193 +1,134 @@
 from fastapi import APIRouter, Depends
-from starlette.responses import JSONResponse
-from typing import List
+from typing import List, Union
 import enum
 from sqlalchemy.orm import Session
 
 import sunpeek.components as cmp
 from sunpeek.api.dependencies import session, crud
 import sunpeek.serializable_models as smodels
 from sunpeek.common.errors import ConfigurationError
 import sunpeek.components.sensor_types as st
-from sunpeek.api.routers.helper import update_obj, update_plant
-from sunpeek.components.types import CollectorTestTypes
 
 config_router = APIRouter(
     prefix="/config",
     tags=["config"],
     # dependencies=[Depends(get_token_header)],
     responses={404: {"description": "Not found"}},
 )
 
 
 @config_router.get("/ping")
-def ping_harvestIT_old():
-    """old version for backward compatibility"""
-    return "success"
-
-
-@config_router.get("/ping_backend")
 def ping_harvestIT():
     return "success"
 
 
 @config_router.get("/ping_database")
-def ping_database(sess: Session = Depends(session)):
-    sess.execute('SELECT 1')
+def ping_database(session: Session = Depends(session)):
+    session.execute('SELECT 1')
     return True
 
 
 # @config_router.post("/sensors", response_model=Union[smodels.Sensor, List[smodels.Sensor]], tags=["sensors"],
 #                     responses= {409: {"description": "Conflict, most likely because the plant name or name of a child object already exists",
 #                     "model": smodels.Error}}))
 # def sensors(id: int = None, raw_name: str=None, plant_id: int = None, plant_name: str = None,
-#             sess: Session = Depends(session), crud = Depends(crud)):
-#     sensors = crud.get_sensors(sess, id, raw_name, plant_id, plant_name)
+#             session: Session = Depends(session), crud = Depends(crud)):
+#     sensors = crud.get_sensors(session, id, raw_name, plant_id, plant_name)
 #     return sensors
 
 
 @config_router.get("/sensor_types",
-                   response_model=smodels.SensorTypeValidator | List[smodels.SensorTypeValidator],
+                   response_model=Union[smodels.SensorTypeValidator, List[smodels.SensorTypeValidator]],
                    tags=["sensors"], summary="Get a list of sensor types, or select by id or name and plant")
 def sensor_types(name: str = None):
     if name is not None:
         return getattr(st, name)
     return st.all_sensor_types
 
 
 @config_router.get("/fluid_definitions",
-                   response_model=List[smodels.FluidDefinition] | smodels.FluidDefinition,
+                   response_model=Union[List[smodels.FluidDefintion], smodels.FluidDefintion],
                    tags=["fluids"], summary="Get a single list of fluid_definitions, or select by id or name and plant")
-def fluids(id: int = None,
-           name: str = None,
-           plant_id: int = None,
-           plant_name: str = None,
-           sess: Session = Depends(session),
-           crd=Depends(crud)):
-    return crd.get_components(sess, cmp.FluidDefinition, id, name, plant_id, plant_name)
+def fluids(id: int = None, name: str = None, plant_id: int = None, plant_name: str = None,
+           session: Session = Depends(session), crud=Depends(crud)):
+    return crud.get_components(session, cmp.FluidDefinition, id, name, plant_id, plant_name)
 
 
-@config_router.get("/fluid_definitions/{id}", response_model=smodels.FluidDefinition, tags=["fluids"],
+@config_router.get("/fluid_definitions/{id}", response_model=smodels.FluidDefintion, tags=["fluids"],
                    summary="Get a single fluid definition by id")
-def fluids(id: int = None,
-           name: str = None,
-           plant_id: int = None,
-           plant_name: str = None,
-           sess: Session = Depends(session),
-           crd=Depends(crud)):
-    return crd.get_components(sess, cmp.FluidDefinition, id, name, plant_id, plant_name)
-
-
-@config_router.get("/collectors",
-                   response_model=List[smodels.Collector] | smodels.Collector,
-                   tags=["collectors"],
-                   summary="Get a list of collectors, or select by id or name, or filter by collectors used "
-                           "in a specific plant.")
-def collectors(id: int = None,
-               name: str = None,
-               plant_id: int = None,
-               plant_name: str = None,
-               sess: Session = Depends(session),
-               crd=Depends(crud)):
+def fluids(id: int = None, name: str = None, plant_id: int = None, plant_name: str = None,
+           session: Session = Depends(session), crud=Depends(crud)):
+    return crud.get_components(session, cmp.FluidDefinition, id, name, plant_id, plant_name)
+
+
+@config_router.get("/collectors", response_model=Union[List[smodels.CollectorType], smodels.CollectorType],
+                   tags=["collectors"], summary="Get a list of collector types, or select by id or name, or filter by "
+                                                "types used in a certain plant")
+def collector_types(id: int = None, name: str = None, plant_id: int = None, plant_name: str = None,
+                    session: Session = Depends(session), crud=Depends(crud)):
     if plant_id is not None or plant_name is not None:
-        plant = crd.get_plants(sess, plant_id, plant_name)
-        return [array.collector for array in plant.arrays]
+        plant = crud.get_plants(session, plant_id, plant_name)
+        return [array.collector_type for array in plant.arrays]
 
-    return crd.get_components(sess, cmp.Collector, id, name)
+    return crud.get_components(session, cmp.CollectorType, id, name)
 
 
-@config_router.post("/collectors/new",
-                    response_model=smodels.Collector | List[smodels.Collector],
-                    tags=["collectors"],
-                    status_code=201,
-                    summary="Create a new collector or collectors")
-def create_collector(collector:
-smodels.CollectorSST | smodels.CollectorQDT | List[smodels.CollectorSST] | List[smodels.CollectorQDT],
-                     sess: Session = Depends(session),
-                     crd=Depends(crud)):
-    collectors_ = collector if isinstance(collector, list) else [collector]
-
-    for i, collector in enumerate(collectors_):
-        coll_dict = collector.dict(exclude_unset=True)
-        test_type = coll_dict.pop('test_type')
-
-        if test_type == CollectorTestTypes.SST.value:
-            coll = cmp.CollectorSST(**coll_dict)
-        elif test_type == CollectorTestTypes.QDT.value:
-            coll = cmp.CollectorQDT(**coll_dict)
+@config_router.post("/collectors/new", response_model=Union[smodels.CollectorType, List[smodels.CollectorType]],
+                    tags=["collectors"], status_code=201, summary="Create a new collector type or types")
+def create_collector_type(collector: Union[smodels.CollectorTypeSST, smodels.CollectorTypeQDT,
+                                           List[smodels.CollectorTypeSST], List[smodels.CollectorTypeQDT]],
+                          sess: Session = Depends(session), crd=Depends(crud)):
+    if not isinstance(collector, list):
+        collectors = [collector]
+    else:
+        collectors = collector
+
+    for i, collector in enumerate(collectors):
+        type_dict = collector.dict(exclude_unset=True)
+        test_type = type_dict.pop('test_type')
+        if test_type in ['SST', "static"]:
+            col_type = cmp.CollectorTypeSST(**type_dict)
+        elif test_type in ['QDT', "dynamic"]:
+            col_type = cmp.CollectorTypeQDT(**type_dict)
         else:
-            raise ConfigurationError(f'Collector test_type must be one of {", ".join(CollectorTestTypes)}.')
+            raise ConfigurationError(
+                "CollectorType test_type parameter must be one of 'SST', 'static', 'QDT' or 'dynamic'")
+        collectors[i] = crd.create_component(sess, col_type)
 
-        collectors_[i] = crd.create_component(sess, coll)
     sess.commit()
+    return collectors
+
+@config_router.get("/collectors/{id}", response_model=smodels.CollectorType, tags=["collectors"],
+                   summary="Get a single collector type by id")
+def get_collector_type(id: int, sess: Session = Depends(session), crd=Depends(crud)):
+    return crd.get_components(sess, cmp.CollectorType, id=id)
+
+
+from sunpeek.api.routers.helper import update_obj
+
+@config_router.post("/collectors/{id}", response_model=smodels.CollectorType, tags=["collectors"],
+                   summary="Update a collector")
+def update_collector(id: int, collector: smodels.CollectorType, sess: Session = Depends(session), crd=Depends(crud)):
+    coll = crd.get_components(sess, cmp.CollectorType, id=id)
+    coll = update_obj(coll, collector)
+    coll = crd.update_component(sess, coll)
+    return coll
 
-    return collectors_
 
+@config_router.delete("/collectors/{id}", status_code=204, tags=["collectors"],
+                   summary="Delete a single collector type by id")
+def delete_collector_type(id: int, sess: Session = Depends(session), crd=Depends(crud)):
+    component = crd.get_components(sess, cmp.CollectorType, id=id)
+    crd.delete_component(sess, component)
 
-@config_router.get("/collectors/{id}",
-                   response_model=smodels.Collector,
-                   tags=["collectors"],
-                   summary="Get a single collector by id")
-def get_collector(id: int,
-                  sess: Session = Depends(session),
-                  crd=Depends(crud)):
-    return crd.get_components(sess, cmp.Collector, id=id)
-
-
-@config_router.post("/collectors/{id}",
-                    response_model=smodels.CollectorUpdate,
-                    tags=["collectors"],
-                    summary="Update a collector")
-def update_collector(id: int,
-                     collector_update: smodels.Collector,
-                     sess: Session = Depends(session),
-                     crd=Depends(crud)):
-    collector = crd.get_components(sess, cmp.Collector, id=id)
-    collector = update_obj(collector, collector_update)
-    collector.update_parameters()
-
-    all_arrays = crd.get_components(sess, cmp.Array)
-    all_arrays = all_arrays if isinstance(all_arrays, list) else [all_arrays]
-    unique_plants = {a.plant for a in all_arrays if a.collector == collector and a.plant is not None}
-
-    for plant in unique_plants:
-        plant = update_plant(plant)
-        crd.update_component(sess, plant)
-
-    return crd.update_component(sess, collector)
-
-
-@config_router.delete("/collectors/{id}",
-                      status_code=204,
-                      tags=["collectors"],
-                      summary="Delete a single collector by id")
-def delete_collector(id: int,
-                     sess: Session = Depends(session),
-                     crd=Depends(crud)):
-    collector = crd.get_components(sess, cmp.Collector, id=id)
-
-    # Find out if collector is being used by any array
-    all_arrays = crd.get_components(sess, cmp.Array)
-    all_arrays = all_arrays if isinstance(all_arrays, list) else [all_arrays]
-    arrays_using_collector = [a for a in all_arrays if a.collector == collector]
-    if arrays_using_collector:
-        plant_names = {a.plant.name for a in arrays_using_collector}
-        return JSONResponse(
-            status_code=409,
-            content={'error': f'Cannot delete collector.',
-                     'message': f'Cannot delete collector because it is configured to be used by arrays. '
-                                f'Collector is used in {len(plant_names)} plants: {", ".join(plant_names)}'}
-        )
-    crd.delete_component(sess, collector)
 
 
-@config_router.get("/sensor_slots",
-                   response_model=List[smodels.SensorSlotValidator],
+@config_router.get("/sensor_slots", response_model=List[smodels.SensorSlotValidator],
                    tags=["arrays", "plants", "sensor_slots"],
                    summary="Get a list of slot names to which sensors can be assigned for the given component type")
-def slot_names(component_type: enum.Enum('cmp_types', {"Plant": "plant", "Array": "array"}),
+def slot_names(component_type: enum.Enum('col_types', {"Plant": "plant", "Array": "array"}),
                include_virtuals: bool = False):
     if include_virtuals:
         return cmp.__dict__[component_type.name].sensor_slots.values()
     else:
         return cmp.__dict__[component_type.name].get_real_slots()
```

## sunpeek/api/routers/evaluations.py

```diff
@@ -1,197 +1,102 @@
+import datetime
 from typing import Union, List
-import datetime as dt
 from fastapi import APIRouter, Depends, HTTPException
-from starlette.responses import JSONResponse, FileResponse
+from starlette.responses import JSONResponse
 
-import sunpeek.components.helpers
-from sunpeek.api.routers.helper import update_obj
 from sunpeek.api.dependencies import session, crud
 from sunpeek.api.routers.plant import plant_router
-# from sunpeek.api.routers.config import config_router
-from sunpeek.core_methods.common.main import CoreMethodFeedback
-from sunpeek.core_methods.pc_method import PCFormulae, PCMethods
-import sunpeek.core_methods.pc_method.wrapper as pc
-import sunpeek.core_methods.pc_method.plotting as pcp
-import sunpeek.common.plot_utils as pu
+from sunpeek.api.routers.config import config_router
+from sunpeek.core_methods.pc_method import AvailablePCEquations, AvailablePCMethods
+from sunpeek.core_methods.pc_method.wrapper import run_performance_check, list_pc_problems
 import sunpeek.serializable_models as smodels
-import sunpeek.common.errors as errors
 
 evaluations_router = APIRouter(
     prefix=plant_router.prefix + "/evaluations",
     tags=["methods", "evaluations"]
 )
 
+stored_evaluations_router = APIRouter(
+    prefix=config_router.prefix + "/stored_evaluations/{stored_eval_id}",
+    tags=["methods", "evaluations"]
+)
+
 
-# stored_evaluations_router = APIRouter(
-#     prefix=config_router.prefix + "/stored_evaluations/{stored_eval_id}",
-#     tags=["methods", "evaluations"]
-# )
+@evaluations_router.get("/run")
+@stored_evaluations_router.get("/run", tags=["methods", "evaluations"])
+def run(plant_id: int, stored_eval_id: int, method: str = None,
+        eval_start: str = "1900-01-01 00:00:00", eval_end: str = "2021-01-01 00:00:00",
+        sess=Depends(session), crd=Depends(crud)):
+    crd.get_plants(sess, plant_id=plant_id)
+    raise HTTPException(status_code=501,
+                        detail="Stored evaluations are not yet implemented in HarvesIT", headers=
+                        {"Retry-After": "Wed, 30 Nov 2022 23:59 GMT", "Cache-Control": "no-cache"})
 
 
 @evaluations_router.get("/pc_method", summary="Run the PC method", response_model=smodels.PCMethodOutput)
 def run_pc_method(plant_id: int,
-                  method: Union[PCMethods, None] = None,
-                  formula: Union[PCFormulae, None] = None,
-                  eval_start: Union[dt.datetime, None] = None,
-                  eval_end: Union[dt.datetime, None] = None,
+                  method: Union[AvailablePCMethods, None] = None,
+                  equation: Union[AvailablePCEquations, None] = None,
+                  eval_start: Union[datetime.datetime, None] = None,
+                  eval_end: Union[datetime.datetime, None] = None,
                   ignore_wind: Union[bool, None] = None,
                   safety_pipes: Union[float, None] = None,
                   safety_uncertainty: Union[float, None] = None,
                   safety_others: Union[float, None] = None,
                   sess=Depends(session), crd=Depends(crud)):
     """Runs the PC Method for the specified dates range"""
     plant = crd.get_plants(sess, plant_id=plant_id)
+    plant.context.set_eval_interval(eval_start=eval_start, eval_end=eval_end)
 
-    try:
-        pc_algo_result = pc.run_performance_check(
-            plant=plant,
-            method=[method],
-            formula=[formula],
-            use_wind=[None] if ignore_wind is None else [not ignore_wind],
-            eval_start=eval_start,
-            eval_end=eval_end,
-            safety_pipes=safety_pipes,
-            safety_uncertainty=safety_uncertainty,
-            safety_others=safety_others,
-        )
-    except errors.NoDataError as e:
-        # No data uploaded?
-        return JSONResponse(
-            status_code=400,
-            content={'error': 'Cannot run Performance Check.',
-                     'message': str(e)}
-        )
-
+    pc_algo_result = run_performance_check(
+        plant=plant,
+        method=[method],
+        equation=[equation],
+        use_wind=None if ignore_wind is None else [not ignore_wind],
+        safety_pipes=safety_pipes,
+        safety_uncertainty=safety_uncertainty,
+        safety_others=safety_others,
+    )
     pc_output = pc_algo_result.output
     if pc_output is not None:
         return pc_output
 
     # pc_output None -> None of the PC strategies was successful -> Return problem report
     return JSONResponse(
         status_code=400,
         content={'error': 'Could not calculate Performance Check.',
                  'message': f'None of the chosen Performance Check strategies '
-                            f'({len(pc_algo_result.feedback.sub_feedback)}) was successful.',
-                 'detail': pc_algo_result.feedback.parse()}
+                            f'({len(pc_algo_result.problems.sub_reports)}) was successful.',
+                 'detail': pc_algo_result.problems.parse_with_virtuals()}
     )
 
 
-@evaluations_router.get("/pc_method_report", summary="Run the PC method and create a pdf report")
-def get_pc_method_pdf_report(plant_id: int,
-                             method: Union[PCMethods, None] = None,
-                             formula: Union[PCFormulae, None] = None,
-                             eval_start: Union[dt.datetime, None] = None,
-                             eval_end: Union[dt.datetime, None] = None,
-                             ignore_wind: Union[bool, None] = None,
-                             safety_pipes: Union[float, None] = None,
-                             safety_uncertainty: Union[float, None] = None,
-                             safety_others: Union[float, None] = None,
-                             with_interval_plots: Union[bool, None] = None,
-                             include_creation_date: Union[bool, None] = None,
-                             anonymize: Union[bool, None] = None,
-                             sess=Depends(session), crd=Depends(crud)):
-    """Run the PC Method for the specified dates range and return a pdf report.
-    """
-    pc_output = run_pc_method(plant_id=plant_id,
-                              method=method,
-                              formula=formula,
-                              eval_start=eval_start,
-                              eval_end=eval_end,
-                              ignore_wind=ignore_wind,
-                              safety_pipes=safety_pipes,
-                              safety_uncertainty=safety_uncertainty,
-                              safety_others=safety_others,
-                              sess=sess, crd=crd)
-
-    if pc_output.plant_output.n_intervals == 0:
-        return JSONResponse(
-            status_code=400,
-            content={'error': 'Performance Check found no intervals.',
-                     'message': 'Performance Check found no intervals.',
-                     'detail': 'The Performance Check analysis completed successfully, '
-                               'but found no valid intervals in the specific time range.'}
-        )
-
-    # Create pdf report
-    settings = pu.PlotSettings(with_interval_plots=with_interval_plots,
-                               include_creation_date=include_creation_date,
-                               anonymize=anonymize)
-    pdf_path = pcp.create_pdf_report(pc_output=pc_output, settings=settings)
-    response = FileResponse(pdf_path, media_type="application/pdf", filename=pdf_path.name)
-
-    return response
-
-
-@evaluations_router.get("/pc_method_feedback",
-                        summary="Feedback about which PC method variants can be run with the given plant configuration",
-                        response_model=List[smodels.PCMethodFeedback])
-def list_pc_feedback_api(plant_id: int,
-                         method: Union[PCMethods, None] = None,
-                         formula: Union[PCFormulae, None] = None,
+@evaluations_router.get("/pc_method_problems", summary="Report which PC method variants can be run",
+                        response_model=List[smodels.PCMethodProblem])
+def list_pc_problems_api(plant_id: int,
+                         method: Union[AvailablePCMethods, None] = None,
+                         equation: Union[AvailablePCEquations, None] = None,
                          ignore_wind: Union[bool, None] = None,
-                         sess=Depends(session), crd=Depends(crud)) -> List[smodels.PCMethodFeedback]:
-    """List problems for the PC Method for the specified dates range"""
+                         sess=Depends(session), crd=Depends(crud)) -> List[smodels.PCMethodProblem]:
+    """Runs the PC Method for the specified dates range"""
     plant = crd.get_plants(sess, plant_id=plant_id)
-    pc_feedback = pc.list_feedback(
+    pc_problems = list_pc_problems(
         plant=plant,
         method=[method],
-        formula=[formula],
+        equation=[equation],
         use_wind=None if ignore_wind is None else [not ignore_wind],
     )
 
-    return pc_feedback
-
-
-@evaluations_router.get("/pc_method_settings",
-                        summary="Get Settings for the PC method",
-                        response_model=smodels.PCMethodSettings)
-def get_pc_method_settings(plant_id: int,
-                           sess=Depends(session), crd=Depends(crud)):
-    """Get PC Method settings for given plant.
-    """
-    settings = crd.get_components(sess, sunpeek.components.helpers.PCSettingsDefaults, plant_id=plant_id)
-    if len(settings) == 1:
-        return settings[0]
-    return JSONResponse(
-        status_code=400,
-        content={'error': 'PC Method Settings not found.',
-                 'message': 'No PC-Method setting seems to be directly assigned to this plant.',
-                 'detail': 'This might happen if the submitted plant_id is incorrect or missing, or in case '
-                           'multiple/no Settings are assigned to the plant due to a inconsistency in the database.'}
-    )
-
-
-@evaluations_router.post("/pc_method_settings",
-                         summary="Update Settings for the PC method",
-                         response_model=smodels.PCMethodSettings)
-def update_pc_method_settings(plant_id: int,
-                              setting_update: smodels.PCMethodSettings,
-                              sess=Depends(session), crd=Depends(crud)):
-    """Update PC Method settings for given plant.
-    """
-    setting = crd.get_components(sess, component=sunpeek.components.helpers.PCSettingsDefaults, plant_id=plant_id)
-    if len(setting) != 1:
-        return JSONResponse(
-            status_code=400,
-            content={'error': 'PC Method Settings not found.',
-                     'message': 'Settings for the PC-Method not found.',
-                     'detail': 'This might happen if the submitted plant_id is incorrect or missing, or in case '
-                               'multiple/no settings are assigned to the plant due to a inconsistency in the database.'}
-        )
-    setting = setting[0]
-    setting = update_obj(setting, setting_update)
-    setting = crd.update_component(sess, setting)
-    return setting
+    return pc_problems
 
 # @evaluations_router.get("/pc_method", summary="Run the PC method", response_model=smodels.PCMethodOutput)
 # def quick_run_pc_method(plant_id: int, method: AvailablePCMethods,
 #                         equation: Union[AvailablePCEquations, None],
-#                         eval_start: Union[dt.datetime, None] = None,
-#                         eval_end: Union[dt.datetime, None] = None,
+#                         eval_start: Union[datetime.datetime, None] = None,
+#                         eval_end: Union[datetime.datetime, None] = None,
 #                         sess=Depends(session), crd=Depends(crud)):
 #     """Runs the PC Method for the specified dates range"""
 #     plant = crd.get_plants(sess, plant_id=plant_id)
 #     plant.context.set_eval_interval(eval_start=eval_start, eval_end=eval_end)
 #     pc_obj = PCMethod.create(method=method.name, plant=plant, equation=equation)
 #     pc_output = pc_obj.run()
 #     return pc_output
@@ -216,20 +121,7 @@
 # @methods_router.get("/run-dcat-method")
 # async def run_dcat_method(plant_id: str, start_date: str = "2021-05-20 13:00:00", end_date: str = "2021-05-21 13:00:00"):
 #     """Runs the DCAT method on the clean data stored between the specified dates range"""
 #
 #     results_dict = {"plant_id": plant_id,"start_date":start_date, "end_date":end_date, "results_array": [.35,.39,1.69,4.86,6.23,.51,5.25] }
 #
 #     return results_dict
-
-
-## Stale - not planned to be supported
-
-# @evaluations_router.get("/run")
-# @stored_evaluations_router.get("/run", tags=["methods", "evaluations"])
-# def run(plant_id: int, stored_eval_id: int, method: str = None,
-#         eval_start: str = "1900-01-01 00:00:00", eval_end: str = "2021-01-01 00:00:00",
-#         sess=Depends(session), crd=Depends(crud)):
-#     crd.get_plants(sess, plant_id=plant_id)
-#     raise HTTPException(status_code=501,
-#                         detail="Stored evaluations are not yet implemented in SunPeek", headers=
-#                         {"Retry-After": "Wed, 30 Nov 2022 23:59 GMT", "Cache-Control": "no-cache"})
```

## sunpeek/api/routers/files.py

```diff
@@ -1,56 +1,47 @@
-from typing import List, Union
+from typing import List
 import warnings
-import datetime as dt
-
-import pandas as pd
 from fastapi import APIRouter, Depends, HTTPException, Response
 from fastapi.datastructures import UploadFile
 from sqlalchemy.orm import Session
-import parquet_datastore_utils as pu
 
 from sunpeek.common.utils import sp_logger
-from sunpeek.db_utils import PARTITION_COLS
 from sunpeek.common.time_zone import process_timezone
-from sunpeek.data_handling.data_uploader import DataUploader_df, DataUploader_pq, DataUploadResponse, \
-    DataUploadResponseFile, DataInspectionResponse
+from sunpeek.data_handling.data_uploader import DataUploader_df, DataUploader_pq, DataUploadResponse, DataColumnsResponse
 from sunpeek.api.dependencies import session, crud
 from sunpeek.api.routers.plant import plant_router
 from sunpeek.common.utils import DatetimeTemplates
-from sunpeek.components.helpers import UploadHistory
-from sunpeek.common.errors import SunPeekError
 
 files_router = APIRouter(
     prefix="/data",
     tags=["data"],
     # dependencies=[Depends(get_token_header)],
     responses={404: {"description": "Not found"}},
 )
 
 
 @files_router.post("/div-zero")
 def zero_div():
-    """This is a usage example only for the log class and the HTTP exception raising. It must be deleted for release.
-    """
+    """This method server as a usage example only for the log class and the HTTP exception raising. It must be deleted for release"""
+
     try:
         x = 1 / 0
-    except Exception as err:
+    except (Exception) as err:
+
         # how to use the logger to report an exception
         sp_logger.exception(err)
         # how to manually report using the INFO level
         sp_logger.info("This won't print in file because of loggers level")
         # str(err) gives the message related to the exception
         error_dict = {"message": "UPS something went wrong", "error": str(err)}
         # raise HTTPException so the API returns an error code instead of freezing
         raise HTTPException(status_code=500, detail=error_dict)
 
 
-@plant_router.post("/data", tags=["data"],
-                   summary='Upload measurement data to plant',
-                   response_model=DataUploadResponse, status_code=201)
+@plant_router.post("/data", tags=["data"], response_model=DataUploadResponse, status_code=201)
 def upload_measure_data(
         plant_id: int,
         files: List[UploadFile],
         datetime_template: DatetimeTemplates = None,
         datetime_format: str = None,
         timezone: str = None,
         csv_separator: str = ';',
@@ -77,83 +68,58 @@
         Used in pd.read_csv as 'sep' kwarg
     csv_decimal : str
         Used in pd.read_csv as 'decimal' kwarg
     csv_encoding : str
         Used in pd.read_csv as 'encoding' kwarg
     index_col : int
         DataUploader will try to parse timestamps from this column.
-    response : fastapi.Response
     sess : sqlalchemy.orm.Session
     crd : api.dependencies.crud
 
     Returns
     -------
     upload_response : DataUploadResponse
 
     Raises
     ------
     ConnectionError
     HTTPException
     """
 
     with warnings.catch_warnings(record=True) as wrngs:
-        plant = crd.get_plants(sess, plant_id)
-        up = DataUploader_pq(plant=plant,
+        up = DataUploader_pq(plant=crd.get_plants(sess, plant_id),
                              datetime_template=datetime_template,
                              datetime_format=datetime_format,
                              timezone=timezone,
                              csv_separator=csv_separator,
                              csv_decimal=csv_decimal,
                              csv_encoding=csv_encoding,
                              index_col=index_col)
         out = up.do_upload(files=files)
-        sess.commit()
 
     if wrngs is not None:
         response.headers['x-sunpeek-warnings'] = str([str(w.message) for w in wrngs])
 
     return out  # DataUploadResponse
 
 
-@plant_router.delete("/data", tags=["data"],
-                     summary='Delete measurement data from plant in given interval')
-def delete_measure_data(plant_id: int,
-                        start: dt.datetime,  # Timezone-aware timestamp, or will be interpreted as UTC
-                        end: dt.datetime,  # Timezone-aware timestamp, or will be interpreted as UTC
-                        sess: Session = Depends(session),
-                        crd: crud = Depends(crud)) -> None:
-    plant = crd.get_plants(sess, plant_id)
-    plant.context.delete_sensor_data(start, end)
-
-
-@plant_router.delete("/data/all", tags=["data"],
-                     summary='Delete all data from plant')
-def delete_all_data(plant_id: int,
-                    sess: Session = Depends(session),
-                    crd: crud = Depends(crud)) -> None:
-    plant = crd.get_plants(sess, plant_id)
-    plant.delete_all_data()
-
-
-@plant_router.post("/data/columns", tags=["data"], response_model=DataInspectionResponse,
-                   status_code=201)  # for backwards compatibility only
-@plant_router.post("/data/inspection", tags=["data"], response_model=DataInspectionResponse, status_code=201)
-def get_data_inspection(
+@plant_router.post("/data/columns", tags=["data"], response_model=DataColumnsResponse, status_code=201)
+def get_sensor_names_and_index(
         plant_id: int,
         files: List[UploadFile],
         csv_separator: str = ';',
         csv_decimal: str = '.',
         csv_encoding: str = 'utf-8',
         index_col: int = 0,
         datetime_template: DatetimeTemplates = None,
         datetime_format: str = None,
         timezone: str = None,
         response: Response = Response(),
         sess: Session = Depends(session),
-        crd: crud = Depends(crud)) -> DataInspectionResponse:
+        crd: crud = Depends(crud)) -> DataColumnsResponse:
     """Ingests csv files to database. For details, see docstring of the `data_uploader` module.
 
     Parameters
     ----------
     plant_id : A pre-configured plant with this name must exist in the database.
     files : list
         List of csv files that are batch ingested.
@@ -164,18 +130,17 @@
     csv_encoding : str
         Used in pd.read_csv as 'encoding' kwarg
     index_col : int
         DataUploader will try to parse timestamps from this column.
     datetime_template : DatetimeTemplates
         Templates to simplify the definition of a datetime format. Overridden by datetime_format (if not None).
     datetime_format : str
-        Used to parse timestamps from csv file. Leave to None infers the format.
+        Used to parse datetimes from csv file. Leave to None infers the format.
     timezone : str or pytz.timezone.
         Optional. To be provided if timestamps in the data have no time zone information.
-    response : fastapi.Response
     sess : sqlalchemy.orm.Session
     crd : api.dependencies.crud
 
     Returns
     -------
     upload_response : DataColumnsResponse
 
@@ -192,44 +157,16 @@
                              datetime_template=datetime_template,
                              datetime_format=datetime_format,
                              timezone=timezone,
                              csv_separator=csv_separator,
                              csv_decimal=csv_decimal,
                              csv_encoding=csv_encoding,
                              index_col=index_col)
-        df = up.do_inspection(files=files)
 
-        out = DataInspectionResponse(sensors=df.columns.tolist(),
-                                     index=df.index.name,
-                                     data=df.to_dict(orient='split'),
-                                     dtypes=df.dtypes.astype(str).tolist(),
-                                     settings=up.get_settings())
+        sensor_names = up.get_sensor_names(files=files)
+        index_name = up.get_index_name(files=files)
+        out = {"sensors": list(sensor_names), "index": index_name}
 
     if wrngs is not None:
         response.headers['x-sunpeek-warnings'] = str([str(w.message) for w in wrngs])
 
-    return out
-
-
-@plant_router.get("/data/history",
-                  tags=["data"],
-                  response_model=List[DataUploadResponseFile],
-                  status_code=201,
-                  summary="Get historic data uploads of the plant"
-                  )
-def get_data_history(plant_id: int, sess: Session = Depends(session), crd: crud = Depends(crud)):
-    plant = crd.get_plants(sess, plant_id)
-    uploads = plant.upload_history
-    return uploads
-
-
-@plant_router.delete("/data/history/{history_id}",
-                     tags=["data"],
-                     status_code=201,
-                     summary="Delete a single entry of the data history by id"
-                     )
-def delete_data_history(plant_id: int, history_id: int, sess: Session = Depends(session), crd: crud = Depends(crud)):
-    history_entry = sess.query(UploadHistory).get(history_id)
-    if history_entry.plant_id != plant_id:
-        raise SunPeekError("No entry found with matching id and plant_id")
-    crd.delete_component(sess, history_entry)
-    return
+    return out  # DataColumnsResponse
```

## sunpeek/api/routers/helper.py

```diff
@@ -1,53 +1,9 @@
-from sunpeek.core_methods import virtuals
-from sunpeek.components.physical import convert_to_concrete_components
-
-
+# -*- coding: utf-8 -*-
 def update_obj(obj, update_model):
     update_dict = update_model.dict(exclude_unset=True)
 
     for key, val in update_dict.items():
         if val != getattr(obj, key):
             setattr(obj, key, val)
 
     return obj
-
-
-def update_plant(plant, update_model=None, session=None):
-    """For a plant, update the plant including config-dependent virtual sensors state.
-
-    Parameters
-    ----------
-    plant : A cmp.Plant instance
-    update_model : updated component model
-
-    Returns
-    -------
-    The updated plant.
-    """
-    if update_model is not None:
-        update_obj(plant, update_model)
-
-    if session is not None:
-        convert_to_concrete_components(session, plant)
-    virtuals.config_virtuals(plant)
-
-    return plant
-
-
-def recalculate_plant(plant, eval_start, eval_end):
-    """For a plant, recalculate virtual sensors, set context and flush results to parquet.
-
-    Parameters
-    ----------
-    plant : A cmp.Plant instance
-    eval_start, eval_end : Recalculation can be limited to this interval. Can be datetime or None.
-
-    Returns
-    -------
-    The updated plant.
-    """
-    plant.context.set_eval_interval(eval_start=eval_start, eval_end=eval_end)
-    virtuals.calculate_virtuals(plant)
-    plant.context.flush_virtuals_to_parquet()
-
-    return plant
```

## sunpeek/api/routers/plant.py

```diff
@@ -1,19 +1,19 @@
-from typing import List
-import datetime as dt
+from typing import List, Union
+import datetime
 import pytz
 from fastapi import APIRouter, Depends, BackgroundTasks, Request
 from fastapi.responses import JSONResponse, Response
 
 from sqlalchemy.orm import Session
-from sqlalchemy.exc import NoResultFound
 from sunpeek.api.dependencies import session, crud
 import sunpeek.serializable_models as smodels
 import sunpeek.demo.demo_plant as demo_plant_function
-from sunpeek.api.routers.helper import update_obj, update_plant, recalculate_plant
+import sunpeek.core_methods.virtuals as virtuals
+from sunpeek.api.routers.helper import update_obj
 from sunpeek.common import config_parser
 import sunpeek.components as cmp
 from sunpeek.common.errors import TimeZoneError
 from sunpeek.data_handling.context import NanReportResponse
 import sunpeek.exporter
 
 plants_router = APIRouter(
@@ -34,577 +34,358 @@
     prefix="/plants/-",
     tags=["plant"],
     # dependencies=[Depends(get_token_header)],
     responses={404: {"description": "Not found"}},
 )
 
 
-@plants_router.get("",
-                   summary="List all plants",
-                   response_model=List[smodels.Plant])
-def plants(name: str = None,
-           sess: Session = Depends(session),
-           crd=Depends(crud)):
-    plant = crd.get_plants(sess, plant_name=name)
-    if not isinstance(plant, list):
-        plant = [plant]
+@plants_router.get("", response_model=List[smodels.Plant],
+                   summary="List all plants")
+def plants(name: str = None, session: Session = Depends(session), crud=Depends(crud)):
+    plants = crud.get_plants(session, plant_name=name)
+    if not isinstance(plants, list):
+        plants = [plants]
+
+    return plants
 
-    return plant
 
+@plants_router.get("/summary", response_model=List[smodels.PlantSummary],
+                   summary="Get a list of all plants, with only minimal information")
+def plants(name: str = None, session: Session = Depends(session), crud=Depends(crud)):
+    p = crud.get_plants(session, plant_name=name)
+    return p
 
-@plants_router.get("/summary",
-                   summary="Get a list of all plants, with only minimal information",
-                   response_model=List[smodels.PlantSummary])
-def plants(name: str = None,
-           sess: Session = Depends(session),
-           crd=Depends(crud)):
-    return crd.get_plants(sess, plant_name=name)
-
-
-@plant_router.get("/summary",
-                  summary="Get a plant summary, with only minimal information",
-                  response_model=smodels.PlantSummary)
-def plants(plant_id: int = None,
-           sess: Session = Depends(session),
-           crd=Depends(crud)):
-    return crd.get_plants(sess, plant_id=plant_id)
-
-
-@plants_router.post("/new",
-                    summary="Create plants",
-                    response_model=smodels.Plant,
+
+@plants_router.post("/new", response_model=smodels.Plant,
+                    summary="create plants",
                     status_code=201,
-                    responses={409: {"description": "Conflict, most likely because the plant name or name of a child "
-                                                    "object already exists",
-                                     "model": smodels.Error}})
-def create_plant(new_plant: smodels.NewPlant,
-                 sess: Session = Depends(session),
-                 crd=Depends(crud)):
+                    responses={
+                        409: {"description": "Conflict, most likely because the plant name or name of a child "
+                                             "object already exists",
+                              "model": smodels.Error}})
+def create_plant(plant: smodels.NewPlant, session: Session = Depends(session), crud=Depends(crud)):
     """ Create a new plant. `name`, `latitude`, `longitude` are required. sensors can be mapped by passing a list of sensor
     structures to `sensors`
     """
-    plant = config_parser.make_full_plant(new_plant.dict(exclude_unset=True), sess)
-    plant = crd.create_component(sess, plant)
-
+    plant = config_parser.make_full_plant(plant.dict(exclude_unset=True), session)
+    plant = crud.create_component(session, plant)
     return plant
 
 
-@plants_router.post("/import",
-                    summary="Import a plant from JSON configuration, such as that "
-                            "returned by `plants/{plant_id}/export_config`",
-                    response_model=List[
-                        smodels.Collector | smodels.SensorTypeValidator | smodels.FluidDefinition | smodels.Plant],
-                    status_code=201,
-                    responses={409: {"description": "Conflict, most likely because the plant name or name of a child "
-                                                    "object already exists",
-                                     "model": smodels.Error}},
-                    tags=["plants", "export/import"])
-def import_conf(import_config: smodels.ConfigImport,
-                new_plant_name: str | None = None,
-                sess: Session = Depends(session),
-                crd=Depends(crud)):
-    conf = import_config.plant.dict(exclude_unset=True)
-    if new_plant_name is not None:
-        conf['name'] = new_plant_name
-    plant = config_parser.make_full_plant(conf, sess)
-    # plant = config_parser.make_full_plant(import_config.plant.dict(exclude_unset=True), sess)
-
-    imported_collectors = [cmp.Collector(**coll.dict(exclude_unset=True)) for coll in import_config.collectors]
-    existing_collectors = [col[0] for col in crd.get_components(sess, cmp.Collector.name)]
-    new_collectors = [col for col in imported_collectors if col.name not in existing_collectors]
-
-    imported_fluids = [cmp.FluidDefinition(**fluid.dict(exclude_unset=True)) for fluid in
-                       import_config.fluid_definitions]
-    existing_fluids = [fluid[0] for fluid in crd.get_components(sess, cmp.FluidDefinition.name)]
-    new_fluids = [fluid for fluid in imported_fluids if fluid.name not in existing_fluids]
-
-    created = []
-    for item in new_collectors + new_fluids:
-        crd.get_components(sess, type(item), name=item.name)
-        crd.create_component(sess, item, commit=False)
-        created.append(item)
-
-    plant = crd.create_component(sess, plant)
-
-    return [plant] + created
-
-
-@plants_router.get("/create_demo_plant",
-                   response_model=smodels.Plant,
+@plants_router.get("/create_demo_plant", response_model=smodels.Plant,
                    summary="Create demo plant config, optionally including data, if data is to be included, "
                            "accept_license must also be set to true")
-def demo_plant(name: str = None,
-               include_data: bool = False,
-               accept_license: bool = False,
-               sess: Session = Depends(session)):
-    plant = demo_plant_function.create_demoplant(sess, name)
+def demo_plant(name: str = None, include_data: bool = False, accept_license: bool = False,
+               session: Session = Depends(session)):
+    plant = demo_plant_function.create_demoplant(session, name)
     if include_data and accept_license:
-        demo_plant_function.add_demo_data(plant, sess)
+        demo_plant_function.add_demo_data(plant, session)
     return plant
 
 
-@plant_router.get("",
-                  response_model=smodels.Plant,
-                  summary="Get a single plant by id",
-                  tags=["plants"])
-def plants(plant_id: int,
-           sess: Session = Depends(session),
-           crd=Depends(crud)):
-    plant = crd.get_plants(sess, plant_id)
-
-    return plant
+@plant_router.get("", response_model=smodels.Plant,
+                  tags=["plants"],
+                  summary="Get a single plant by id")
+def plants(plant_id: int, session: Session = Depends(session), crud=Depends(crud)):
+    p = crud.get_plants(session, plant_id)
+    return p
 
 
 @plant_router.get("/export_config",
                   response_model=smodels.ConfigExport,
+                  tags=["plants", "export"],
                   summary="Export a plant configuration, optionally with data",
-                  description="Export a plant with the sensor types, collector types, and fluid definitions it uses.",
-                  tags=["plants", "export/import"])
-def export_conf(plant_id: int,
-                sess: Session = Depends(session),
-                crd=Depends(crud)):
-    plant = crd.get_plants(sess, plant_id=plant_id)
-
+                  description="Export a plant with the sensor types, collector types, and fluid definitions it uses.")
+def export_conf(plant_id: int, session: Session = Depends(session), crud=Depends(crud)):
+    plant = crud.get_plants(session, plant_id=plant_id)
     return smodels.ConfigExport(**sunpeek.exporter.create_export_config(plant))
 
 
-@plant_router.post("/export_complete",
-                   response_model=smodels.JobReference,
-                   summary="Export a plant with configuration and data",
+@plant_router.post("/export_complete", response_model=smodels.JobReference,
+                   tags=["plants", "export"], summary="Export a plant with configuration and data",
                    description="""Create an export job for a complete plant with sensor types, collector types, 
                    fluid definitions, and data. When the job completes a tar package containing a json file, 
                    and data 1 CSV file per calender year, is available for download""",
-                   tags=["plants", "export/import"],
                    status_code=202)
-def create_complete_export(request: Request,
-                           background_tasks: BackgroundTasks,
-                           plant_id: int,
+def create_complete_export(request: Request, background_tasks: BackgroundTasks, plant_id: int,
                            include_virtuals: bool = True,
-                           sess: Session = Depends(session),
-                           crd=Depends(crud),
-                           ):
-    plant = crd.get_plants(sess, plant_id=plant_id)
+                           session: Session = Depends(session), crud=Depends(crud)):
+    plant = crud.get_plants(session, plant_id=plant_id)
     job = cmp.Job(status=cmp.helpers.ResultStatus.pending, plant=plant)
-    crd.create_component(sess, job)
+    crud.create_component(session, job)
     background_tasks.add_task(sunpeek.exporter.create_export_package, plant, include_virtuals, job)
-
     return smodels.JobReference(job_id=job.id, href=str(request.url_for('jobs')) + str(job.id))
 
 
-@plant_router.post("",
-                   response_model=smodels.Plant | List[smodels.Plant],
+@plant_router.post("", response_model=Union[smodels.Plant, List[smodels.Plant]],
                    summary="Update a plant",
                    responses={409: {"description": "Conflict, most likely because the plant name or name of a child "
                                                    "object already exists",
                                     "model": smodels.Error}})
-def plants(plant_id: int,
-           plant_update: smodels.UpdatePlant,
-           sess: Session = Depends(session),
-           crd=Depends(crud)):
-    plant = crd.get_plants(sess, plant_id=plant_id)
-    plant = update_plant(plant, plant_update, sess)
-    return crd.update_component(sess, plant)
+def plants(plant_id: int, plant: smodels.UpdatePlant, session: Session = Depends(session), crud=Depends(crud)):
+    plant_cmp = crud.get_plants(session, plant_id=plant_id)
+    plant_cmp = update_obj(plant_cmp, plant)
+    plant_cmp = crud.update_component(session, plant_cmp)
+    return plant_cmp
 
 
-@plant_router.post("/summary",
-                   response_model=smodels.PlantSummary | List[smodels.PlantSummary],
+@plant_router.post("/summary", response_model=Union[smodels.PlantSummary, List[smodels.PlantSummary]],
                    summary="Update a plant",
                    responses={409: {"description": "Conflict, most likely because the plant name or name of a child "
                                                    "object already exists",
                                     "model": smodels.Error}})
-def plants(plant_id: int,
-           plant_update: smodels.PlantSummaryBase,
-           sess: Session = Depends(session),
-           crd=Depends(crud)):
-    plant = crd.get_plants(sess, plant_id=plant_id)
-    # This is necessary because a change in a PlantSummaryBase may involve changes to latitude, longitude etc.
-    # which are used by virtual sensors / solar position etc.
-    plant = update_plant(plant, plant_update)
-
-    return crd.update_component(sess, plant)
+def plants(plant_id: int, plant: smodels.PlantSummaryBase, session: Session = Depends(session), crud=Depends(crud)):
+    plant_cmp = crud.get_plants(session, plant_id=plant_id)
+    plant_cmp = update_obj(plant_cmp, plant)
+    plant_cmp = crud.update_component(session, plant_cmp)
+    return plant_cmp
 
 
-@plant_router.delete("",
-                     summary="Delete a plant by id")
-def plants(plant_id: int,
-           sess: Session = Depends(session),
-           crd=Depends(crud)):
-    # Delete parquet data folders, and delete plant from database
-    plant = crd.get_plants(sess, plant_id=plant_id)
-    plant.delete_all_data()
-    plant_name = plant.name
-    sess.delete(plant)
-    sess.commit()
+@plant_router.delete("", summary="Delete a plant by id")
+def plants(plant_id: int, session: Session = Depends(session), crud=Depends(crud)):
+    p = crud.get_plants(session, plant_id=plant_id)
+    name = p.name
+    session.delete(p)
+    session.commit()
 
-    return str(f'plant {plant_name} was deleted')
+    return str(f'plant {name} was deleted')
 
 
-@plant_router.get("/data_start_end",
-                  response_model=smodels.PlantDataStartEnd,
-                  summary="Get timestamps when data associated with the plant start and end.",
-                  tags=["interval", "data"])
-def data_start_end(plant_id: int,
-                   sess: Session = Depends(session), crd=Depends(crud)):
-    plant = crd.get_plants(sess, plant_id=plant_id)
-
-    start_end = plant.context.get_data_start_end()
-    start, end = start_end if start_end is not None else (None, None)
-    return smodels.PlantDataStartEnd(start=start, end=end)
-
-
-@plant_router.get("/sensors/nan_report",
-                  summary="Triggers calculation of the daily-summarized NaN report for all sensors.",
-                  tags=["sensors", "data"])
+@plant_router.get("/sensors/nan_report", tags=["sensors", "data"],
+                  summary="Triggers calculation of the daily-summarized NaN report for all sensors.")
 def nan_report(plant_id: int,
-               eval_start: dt.datetime | None = None,
-               eval_end: dt.datetime | None = None,
+               eval_start: Union[datetime.datetime, None] = None,
+               eval_end: Union[datetime.datetime, None] = None,
                sess: Session = Depends(session), crd=Depends(crud)) -> NanReportResponse:
     plant = crd.get_plants(sess, plant_id=plant_id)
     plant.context.set_eval_interval(eval_start=eval_start, eval_end=eval_end)
+    nan_report = plant.context.get_nan_report(include_virtuals=True)
 
-    return plant.context.get_nan_report(include_virtuals=True)
+    return nan_report
 
 
-@plant_router.get("/sensors/recalculate_virtuals",
-                  summary="Triggers the recalculation of all virtual sensors of that plant",
-                  tags=["sensors, virtual"])
-def recalculate_virtuals(plant_id: int,
-                         eval_start: dt.datetime | None = None,
-                         eval_end: dt.datetime | None = None,
-                         sess: Session = Depends(session),
-                         crd=Depends(crud),
-                         ):
+@plant_router.get("/sensors/recalculate_virtuals", tags=["sensors, virtual"],
+                  summary="Triggers the recalculation of all virtual sensors of that plant")
+def recalculate_virtuals(plant_id: int, sess: Session = Depends(session), crd=Depends(crud)):
     plant = crd.get_plants(sess, plant_id=plant_id)
-    recalculate_plant(plant, eval_start, eval_end)
-    sess.commit()
-
+    virtuals.calculate_virtuals(plant)
     return JSONResponse(status_code=200,
-                        content={"description": "Recalculation done!",
-                                 "message": "Recalculation of virtual sensors successfully finished."})
+                        content={"description": "Recalculation done!", "message": "Recalculation done!"})
 
 
-@plant_router.get("/sensors",
-                  response_model=List[smodels.Sensor] | smodels.Sensor,
+@plant_router.get("/sensors", response_model=Union[List[smodels.Sensor], smodels.Sensor],
                   tags=["sensors"],
                   summary="Get a list of sensors, or select by id or raw name")
-@plant_router.get("/sensors/{id}",
-                  response_model=smodels.Sensor,
-                  tags=["sensors"],
+@plant_router.get("/sensors/{id}", response_model=smodels.Sensor, tags=["sensors"],
                   summary="Get a single sensor by id")
-@any_plant_router.get("/sensors/{id}",
-                      response_model=smodels.Sensor,
-                      tags=["sensors"],
+@any_plant_router.get("/sensors/{id}", response_model=smodels.Sensor, tags=["sensors"],
                       summary="Get a single sensor by id")
-def sensors(id: int = None,
-            raw_name: str = None,
-            plant_id: int | str = None,
-            sess: Session = Depends(session),
-            crd=Depends(crud)):
+def sensors(id: int = None, raw_name: str = None, plant_id: Union[int, str] = None,
+            session: Session = Depends(session), crud=Depends(crud)):
     plant_id = None if plant_id == '-' else plant_id
-    sensor = crd.get_sensors(sess, id, raw_name, plant_id)
-
-    return sensor
+    sensors = crud.get_sensors(session, id, raw_name, plant_id)
+    return sensors
 
 
 @plant_router.get("/sensors/{id}/data", tags=["sensors", "data"],
                   summary="Get measurement data of a single sensor by id")
 @any_plant_router.get("/sensors/{id}/data", response_model=smodels.Sensor, tags=["sensors"],
                       summary="Get measurement data of a single sensor by id")
-def sensor_data(id: int = None,
-                plant_id: int | str = None,
-                eval_start: dt.datetime | None = None,
-                eval_end: dt.datetime | None = None,
-                sess: Session = Depends(session),
-                crd=Depends(crud)):
+def sensor_data(id: int = None, raw_name: str = None, plant_id: Union[int, str] = None,
+                eval_start: Union[datetime.datetime, None] = None,
+                eval_end: Union[datetime.datetime, None] = None,
+                session: Session = Depends(session), crud=Depends(crud)):
     plant_id = None if plant_id == '-' else plant_id
-    plant = crd.get_plants(sess, plant_id=plant_id)
+    plant = crud.get_plants(session, plant_id=plant_id)
     plant.context.set_eval_interval(eval_start=eval_start, eval_end=eval_end)
-    sensor = crd.get_sensors(sess, plant_id=plant_id, id=id)
-    data = sensor.data
+    sensor = crud.get_sensors(session, plant_id=plant_id, id=id)
+    data = sensor.data.pint.to(sensor.native_unit)
     df = data.astype(float)  # to_json does not work with dtype pint.
-
     return Response(df.to_json(), media_type="application/json")
 
 
-@any_plant_router.post("/sensors",
-                       response_model=List[smodels.Sensor],
-                       summary="Batch update a list of sensors, each passed sensor object must contain an id",
-                       tags=["sensors"])
-def update_sensors(sensor_updates: List[smodels.BulkUpdateSensor],
-                   sess: Session = Depends(session),
-                   crd=Depends(crud)):
-    return_sensors = []
-    for sensor_update in sensor_updates:
-        sensor = crd.get_sensors(sess, sensor_update.id)
-        sensor = update_obj(sensor, sensor_update)
-        crd.update_component(sess, sensor, commit=False)
-        return_sensors.append(sensor)
-
-    plant_ids = {s.plant.id for s in return_sensors if s.plant is not None}
-    for plant_id in plant_ids:
-        plant = crd.get_plants(sess, plant_id=plant_id)
-        plant = update_plant(plant)
-        crd.update_component(sess, plant, commit=False)
+@plant_router.get("/sensors/recalculate_virtuals", tags=["sensors, virtual"],
+                  summary="Triggers the recalculation of all virtual sensors of that plant")
+def recalculate_virtuals(plant_id: int, sess: Session = Depends(session), crd=Depends(crud)):
+    plant = crd.get_plants(sess, plant_id=plant_id)
+    plant.calculate_virtuals()
+    return JSONResponse(status_code=200, content={"description": "Recalculation done!", "message": "Recalculation done!"})
 
+
+@any_plant_router.post("/sensors", response_model=List[smodels.Sensor], tags=["sensors"],
+                       summary="Batch update a list of sensors, each passed sensor object must contain an id")
+def update_sensors(sensors: List[smodels.BulkUpdateSensor], sess: Session = Depends(session), crd=Depends(crud)):
+    for sensor in sensors:
+        sensor_obj = crd.get_sensors(sess, sensor.id)
+        crd.update_component(sess, update_obj(sensor_obj, sensor), commit=False)
     sess.commit()
+    return sensors
+
 
-    return return_sensors
+@any_plant_router.post("/sensors/{id}", response_model=smodels.Sensor, tags=["sensors"],
+                       summary="Update a single sensor by id")
+def update_sensor(id: int, sensor_update: smodels.Sensor, sess: Session = Depends(session), crd=Depends(crud)):
+    sensor_obj = crd.get_sensors(sess, id)
+    sensor_obj = crd.update_component(sess, update_obj(sensor_obj, sensor_update))
+    return sensor_obj
 
 
-@any_plant_router.post("/sensors/{id}",
-                       response_model=smodels.Sensor,
-                       summary="Update a single sensor by id",
-                       tags=["sensors"])
-def update_sensor(id: int,
-                  sensor_update: smodels.Sensor,
-                  sess: Session = Depends(session),
-                  crd=Depends(crud)):
-    sensor = crd.get_sensors(sess, id)
-    sensor = update_obj(sensor, sensor_update)
-    sensor = crd.update_component(sess, sensor)
-
-    if sensor.plant is not None:
-        plant = update_plant(sensor.plant)
-        crd.update_component(sess, plant, commit=True)
-
-    return sensor
-
-
-@plant_router.post("/sensors/new",
-                   response_model=List[smodels.Sensor],
-                   summary="Create a new `Sensor` object or objects",
-                   tags=["sensors"],
-                   status_code=201,
+@plant_router.post("/sensors/new", response_model=List[smodels.Sensor],
+                   summary="Create a new `Sensor` object or objects", tags=["sensors"], status_code=201,
                    responses={
                        409: {
                            "description": "Conflict, most likely because the sensor raw name already exists in this plant",
                            "model": smodels.Error}})
-def create_sensors(plant_id: int,
-                   new_sensor: smodels.NewSensor | List[smodels.NewSensor],
-                   sess: Session = Depends(session),
-                   crd=Depends(crud)):
-    """Create a new sensor or sensors. `raw_name` is required.
+def create_sensors(plant_id: int, sensor: Union[smodels.NewSensor, List[smodels.NewSensor]],
+                   session: Session = Depends(session), crud=Depends(crud)):
+    """
+    Create a new sensor or sensors. `raw_name` is required.
     To create multiple sensors at once, pass a list of sensor structures
     """
-    sensor_list = new_sensor if isinstance(new_sensor, list) else [new_sensor]
-
-    return_sensors = []
-    plant = crd.get_plants(sess, plant_id=plant_id)
-    for sensor in sensor_list:
+    if not isinstance(sensor, list):
+        sensors = [sensor]
+    else:
+        sensors = sensor
+
+    rets = []
+    plant = crud.get_plants(session, plant_id=plant_id)
+    for sensor in sensors:
         sensor = cmp.Sensor(**sensor.dict(), plant=plant)
-        sensor = crd.create_component(sess, sensor, commit=False)
-        return_sensors.append(sensor)
-    sess.commit()
-
-    return return_sensors
-
-
-from fastapi import Query
-from typing import Annotated
+        sensor = crud.create_component(session, sensor, commit=False)
+        rets.append(sensor)
 
+    session.commit()
+    return rets
 
-@plant_router.delete("/sensors", summary="Delete multiple sensors by id", tags=["sensors"])
-def delete_sensors(ids: Annotated[list[str] | None, Query()] = None,
-                   plant_id: int | str | None = None,
-                   sess: Session = Depends(session),
-                   crd=Depends(crud),
-                   ):
-    sensors_to_delete = sess.query(cmp.Sensor).filter(cmp.Sensor.id.in_(ids)).filter(
-        cmp.Sensor.plant_id == plant_id).all()
-
-    # For faster updates, disable synchronization with post_config_changed_callbacks
-    # which would call updates for each sensor to be dropped
-    plant = sess.query(cmp.Plant).get(plant_id)
-    plant.defer_post_config_changed_actions = True
-
-    # Delete sensors
-    for sensor in sensors_to_delete:
-        print(sensor.id)
-        s = crd.get_sensors(sess, sensor.id)
-        with sess.no_autoflush:
-            s.remove_references()
-        crd.delete_component(sess, sensor)
-
-    # As the deletion is done now, we call the post_config_changed_callbacks and reset to the default
-    plant.defer_post_config_changed_actions = False
-    plant = update_plant(plant)
-    crd.update_component(sess, plant, commit=True)
-
-
-@any_plant_router.delete("/sensors/{id}",
-                         summary="Delete a single sensor by id",
-                         tags=["sensors"])
-def delete_sensor(id: int,
-                  sess: Session = Depends(session),
-                  crd=Depends(crud),
-                  ):
-    sensor = crd.get_sensors(sess, id)
-    sensor_plant = sensor.plant if sensor.plant is not None else None
 
+@any_plant_router.delete("/sensors/{id}", tags=["sensors"], summary="Delete a single sensor by id")
+def delete_sensor(id: int, sess: Session = Depends(session), crd=Depends(crud)):
+    sensor_obj = crd.get_sensors(sess, id)
+    # if sensor_obj.plant is not None:
+    #     plant = sensor_obj.plant
+    #     plant.defer_configure_virtuals = True
+    # sensor_obj.remove_references(include_plant=False)
+    # crd.delete_component(sess, sensor_obj)
+    # plant.defer_configure_virtuals = False
+    # plant.arrays
     with sess.no_autoflush:
-        sensor.remove_references()
-    crd.delete_component(sess, sensor)
+        sensor_obj.remove_references()
+    crd.delete_component(sess, sensor_obj)
+    # plant.config_virtuals()
 
-    if sensor_plant is not None:
-        plant = update_plant(sensor_plant)
-        crd.update_component(sess, plant, commit=False)
 
-    sess.commit()
-
-
-@plant_router.get("/arrays", response_model=List[smodels.Array] | smodels.Array,
+@plant_router.get("/arrays", response_model=Union[List[smodels.Array], smodels.Array],
                   tags=["arrays"],
                   summary="Get a list of arrays, or select by id or name and plant")
 @any_plant_router.get("/arrays/{id}", response_model=smodels.Array, tags=["arrays"],
                       summary="Get a single array by id")
-def arrays(id: int = None,
-           name: str = None,
-           plant_id: int | str = None,
-           plant_name: str = None,
-           sess: Session = Depends(session),
-           crd=Depends(crud)):
+def arrays(id: int = None, name: str = None, plant_id: Union[int, str] = None, plant_name: str = None,
+           session: Session = Depends(session), crud=Depends(crud)):
     plant_id = None if plant_id == '-' else plant_id
-
-    return crd.get_components(sess, cmp.Array, id, name, plant_id, plant_name)
+    return crud.get_components(session, cmp.Array, id, name, plant_id, plant_name)
 
 
 @any_plant_router.post("/arrays/{id}", response_model=smodels.Array,
                        tags=["arrays"],
                        summary="Update an array by id")
-def update_array(id: int,
-                 array_update: smodels.ArrayUpdate,
-                 sess: Session = Depends(session),
-                 crd=Depends(crud)):
-    array = crd.get_components(sess, component=cmp.Array, id=id)
-    array = update_obj(array, array_update)
-
-    # This update needed: New arrays might for example trigger new plant.tp value if sum of array.tp powers.
-    if array.plant is not None:
-        plant = update_plant(array.plant)
-        crd.update_component(sess, plant)
-
-    return crd.update_component(sess, array)
+def update_array(id: int, array: smodels.Array, session: Session = Depends(session), crud=Depends(crud)):
+    array_cmp = crud.get_components(session, component=cmp.Array, id=id)
+    array_cmp = update_obj(array_cmp, array)
+    array_cmp = crud.update_component(session, array_cmp)
+    return array_cmp
 
 
 @any_plant_router.delete("/arrays/{id}", tags=["arrays"],
                          summary="Delete an array by id")
-def arrays(id: int,
-           sess: Session = Depends(session),
-           crd=Depends(crud)):
-    array = crd.get_components(sess, component=cmp.Array, id=id)
+def arrays(id: int, session: Session = Depends(session), crud=Depends(crud)):
+    array = crud.get_components(session, component=cmp.Array, id=id)
     if array.plant is not None:
         array.plant.arrays.pop(array.plant.arrays.index(array))
-    sess.delete(array)
-    sess.commit()
+    session.delete(array)
+    session.commit()
 
 
 @plant_router.post("/arrays/new",
-                   response_model=List[smodels.Array] | smodels.Array,
+                   response_model=Union[List[smodels.Array], smodels.Array],
                    tags=["arrays"], status_code=201,
                    summary="Get a list of arrays, or select by id or name and plant",
                    responses={
                        409: {"description": "Conflict, most likely because the array name or a child object already "
-                                            "exists in this plant", "model": smodels.Error}}
-                   )
-def create_array(new_array: smodels.NewArray,
-                 plant_id: int,
-                 sess: Session = Depends(session),
-                 crd=Depends(crud)):
-    """Create a new array or arrays. `name` and `collector` are required.
+                                            "exists in this plant", "model": smodels.Error}
+                   })
+def create_array(array: smodels.NewArray, plant_id: int, session: Session = Depends(session), crud=Depends(crud)):
+    """
+    Create a new array or arrays. `name` and `collector_type` are required.
     To create multiple arrays at once, pass a list of array structures.
     sensors can be mapped by passing a dict of sensor structures to `sensors` (**NOTE** not actually tested, may not work yet.
     """
-    array_list = new_array if isinstance(new_array, list) else [new_array]
-
-    return_arrays = []
-    for array in array_list:
-        plant = crd.get_plants(sess, plant_id)
-        array = cmp.Array(**array.dict(exclude_unset=True), plant=plant)
-        array = crd.create_component(sess, array)
-        return_arrays.append(array)
-
-    return return_arrays
-
-
-@plant_router.get("/fluids",
-                  response_model=List[smodels.Fluid] | smodels.Fluid,
-                  summary="Get a list of fluids, or select by name",
-                  tags=["fluids"])
-def fluids(id: int = None,
-           name: str = None,
-           plant_id: int = None,
-           plant_name: str = None,
-           sess: Session = Depends(session),
-           crd=Depends(crud)):
-    return crd.get_components(sess, cmp.Fluid, id, name, plant_id, plant_name)
+    if not isinstance(array, list):
+        arrays = [array]
+    else:
+        arrays = array
+
+    rets = []
+    for array in arrays:
+        plant = crud.get_plants(session, plant_id)
+        array_cmp = cmp.Array(**array.dict(exclude_unset=True), plant=plant)
+        array_cmp = crud.create_component(session, array_cmp)
+        rets.append(array_cmp)
+
+    return rets
+
+
+@plant_router.get("/fluids", response_model=Union[List[smodels.Fluid], smodels.Fluid],
+                  tags=["fluids"],
+                  summary="Get a list of fluids, or select by name")
+def fluids(id: int = None, name: str = None, plant_id: int = None, plant_name: str = None,
+           session: Session = Depends(session), crud=Depends(crud)):
+    return crud.get_components(session, cmp.Fluid, id, name, plant_id, plant_name)
 
 
 @plant_router.get("/fluids/{id}", response_model=smodels.Fluid,
-                  summary="Get a single fluid by id",
-                  tags=["fluids"])
-def fluids(id: int,
-           sess: Session = Depends(session),
-           crd=Depends(crud)):
-    return crd.get_components(sess, cmp.Fluid, id=id)
-
-
-@plant_router.get("/operational_events",
-                  response_model=smodels.OperationalEvent | List[smodels.OperationalEvent],
-                  summary="Get a list of operational_events for a plant, or select by date range, or id",
-                  tags=["operational events"])
-def get_operational_events(plant_id: int,
-                           id: int = None,
-                           search_start: dt.datetime = None,
-                           search_end: dt.datetime = None,
-                           search_timezone: str = None,
-                           sess: Session = Depends(session),
-                           crd=Depends(crud)):
+                  tags=["fluids"],
+                  summary="Get a single fluid by id")
+def fluids(id: int, session: Session = Depends(session), crud=Depends(crud)):
+    return crud.get_components(session, cmp.Fluid, id=id)
+
+
+@plant_router.get("/operational_events", response_model=Union[smodels.OperationalEvent, List[smodels.OperationalEvent]],
+                  tags=["operational events"],
+                  summary="Get a list of operational_events for a plant, or select by date range, or id")
+def get_operational_events(plant_id: int, id: int = None, search_start: datetime.datetime = None,
+                           search_end: datetime.datetime = None, search_timezone: str = None,
+                           sess: Session = Depends(session), crd=Depends(crud)):
     if ((search_start is not None) or (search_end is not None)) and (search_timezone is None):
         raise TimeZoneError(
-            'The parameter "timezone" must be specified to interpret search start and search end timestamps correctly.')
+            "The parameter 'timezone' must be specified in order to interpret search start and search end timestamps correctly")
     if search_start is not None:
         search_start = pytz.timezone(search_timezone).localize(search_start)
         search_start = pytz.timezone('UTC').normalize(search_start)
     if search_end is not None:
         search_end = pytz.timezone(search_timezone).localize(search_end)
         search_end = pytz.timezone('UTC').normalize(search_end)
 
     return crd.get_operational_events(sess, id, plant_id, search_start=search_start, search_end=search_end)
 
 
-@any_plant_router.get("/operational_events/{id}",
-                      response_model=smodels.OperationalEvent,
-                      summary="an operational event by id",
-                      tags=["operational events"])
-def get_operational_event(id: int = None,
-                          sess: Session = Depends(session),
-                          crd=Depends(crud)):
+@any_plant_router.get("/operational_events/{id}", response_model=smodels.OperationalEvent,
+                      tags=["operational events"], summary="an operational event by id")
+def get_operational_event(id: int = None, sess: Session = Depends(session), crd=Depends(crud)):
     return crd.get_operational_events(sess, id)
 
 
-@plant_router.post("/operational_events",
-                   response_model=smodels.OperationalEvent,
-                   summary="Create an operational event",
-                   tags=["operational events"])
-def create_operational_event(plant_id: int,
-                             event_start: dt.datetime,
-                             timezone: str,
-                             event_end: dt.datetime = None,
-                             description: str = None,
-                             ignored_range: bool = False,
-                             sess: Session = Depends(session),
-                             crd=Depends(crud)):
+@plant_router.post("/operational_events", response_model=smodels.OperationalEvent, tags=["operational events"],
+                   summary="Create an operational event")
+def create_operational_event(plant_id: int, event_start: datetime.datetime, timezone: str, description: str = None,
+                             event_end: datetime.datetime = None, ignored_range: bool = False,
+                             sess: Session = Depends(session), crd=Depends(crud)):
     plant = crd.get_plants(sess, plant_id)
     event = cmp.OperationalEvent(plant, event_start, tz=timezone, event_end=event_end, description=description,
                                  ignored_range=ignored_range)
-
     return crd.create_component(sess, event)
 
 
-@any_plant_router.delete("/operational_events/{id}",
-                         summary="Delete an operational event by id",
-                         tags=["operational events"])
-def delete_operational_event(id: int,
-                             sess: Session = Depends(session),
-                             crd=Depends(crud)):
+@any_plant_router.delete("/operational_events/{id}", tags=["operational events"],
+                         summary="Delete an operational event by id")
+def delete_operational_event(id: int, sess: Session = Depends(session), crd=Depends(crud)):
     event = crd.get_operational_events(sess, id)
     crd.delete_component(sess, event)
```

## sunpeek/base_model.py

```diff
@@ -1,12 +1,11 @@
 from pydantic import BaseModel as PydanticBaseModel
 from pydantic import validator
 import pint
-from typing import List
-import numpy as np
+
 
 class BaseModel(PydanticBaseModel):
     class Config:
         orm_mode = True
         arbitrary_types_allowed = True
         fields = {
             'class_': 'class'
@@ -15,42 +14,16 @@
     @validator('*', pre=True)
     def make_strings(cls, v):
         if isinstance(v, pint.Unit):
             v = str(v)
             return v
         # elif isinstance(v, pint.Quantity):
         #     return str(v.units)
-        # elif isinstance(v, cmp.Collector):
+        # elif isinstance(v, cmp.CollectorType):
         #     return v.name
         return v
 
     @validator('units', 'native_unit', pre=True, check_fields=False)
     def validate_units(cls, v):
         if isinstance(v, pint.Quantity):
             return str(v.units)
         return v
-
-
-def np_to_list(val):
-    if isinstance(val, np.ndarray) and val.ndim == 1:
-        return list(val)
-    elif isinstance(val, np.ndarray) and val.ndim > 1:
-        out = []
-        for array in list(val):
-            out.append(np_to_list(array))
-        return out
-    return val
-
-
-class Quantity(BaseModel):
-    magnitude: float | List[float] | List[List[float]]
-    units: str
-
-    @validator('magnitude', pre=True)
-    def convert_numpy(cls, val):
-        return np_to_list(val)
-
-    @validator('units', pre=True)
-    def pretty_unit(cls, val):
-        if isinstance(val, pint.Unit):
-            return f"{val:~P}"
-        return val
```

## sunpeek/common/common_units.py

```diff
@@ -30,9 +30,7 @@
 length = Dimensionality(['cm', 'm', 'km', 'ft', 'yd'])
 energy = Dimensionality(['Wh', 'kWh', 'MWh', 'GWh', 'J', 'kJ', 'MJ', 'BTU'])
 power = Dimensionality(['W', 'kW', 'MW', 'GW', 'BTU/h'])
 temperature = Dimensionality(['C', 'K', 'F'])
 area = Dimensionality(['m', 'ft'])
 pressure = Dimensionality(['bar', 'Pa', 'kPa', 'MPa', 'psi'])
 angle = Dimensionality(['', 'rad'])
-bool = Dimensionality([""])
-float = Dimensionality(["", "percent"])
```

## sunpeek/common/config_parser.py

```diff
@@ -1,47 +1,47 @@
 import copy
 from sqlalchemy import exc
 
 import sunpeek.components as cmp
 from sunpeek.common.errors import ConfigurationError, DuplicateNameError
 
 
-def _check_collector_in_db(session, coll_name):
+def _check_colltype_in_db(session, col_type_name):
     if session is not None:
         import sqlalchemy.exc
         try:
-            session.query(cmp.Collector).filter(cmp.Collector.name == coll_name).one()
+            session.query(cmp.CollectorType).filter(cmp.CollectorType.name == col_type_name).one()
             return True
         except sqlalchemy.exc.NoResultFound:
             return False
     return False
 
 
 def make_full_plant(conf, session=None):
     conf = copy.deepcopy(conf)
-    collectors = {}
-    if 'collectors' in conf:
-        colls = conf['collectors']
-        for coll in colls:
-            test_type = coll.pop('test_type')
-            if _check_collector_in_db(session, coll['name']):
-                coll_obj = coll['name']
+    collector_types = {}
+    if 'collector_types' in conf:
+        col_types = conf['collector_types']
+        for col_type in col_types:
+            test_type = col_type.pop('test_type')
+            if _check_colltype_in_db(session, col_type['name']):
+                type_obj = col_type['name']
             elif test_type in ['SST', "static"]:
-                coll_obj = cmp.CollectorSST(**coll)
+                type_obj = cmp.CollectorTypeSST(**col_type)
             elif test_type in ['QDT', "dynamic"]:
-                coll_obj = cmp.CollectorQDT(**coll)
+                type_obj = cmp.CollectorTypeQDT(**col_type)
             else:
                 raise ConfigurationError(
-                    "Collector test_type parameter must be 'SST' or 'QDT'.")
-            collectors[coll_obj.name] = coll_obj
+                    "CollectorType test_type parameter must be one of 'SST', 'static', 'QDT' or 'dynamic'")
+            collector_types[type_obj.name] = type_obj
     if 'plant' in conf:
         conf = conf['plant']
         for array in conf['arrays']:
-            if array['collector'] in collectors.keys():
-                array['collector'] = collectors[array['collector']]
+            if array['collector_type'] in collector_types.keys():
+                array['collector_type'] = collector_types[array['collector_type']]
 
     plant = cmp.Plant(**conf)
 
     if session is not None:
         session.add(plant)
         # session.rollback()
 
@@ -50,12 +50,12 @@
 
 def make_and_store_plant(conf, session):
     plant = make_full_plant(conf, session)
     session.add(plant)
 
     try:
         session.flush()
-    except exc.IntegrityError as e:
+    except exc.IntegrityError:
         session.rollback()
         raise DuplicateNameError(f'Plant with name "{plant.name}" already exists.')
 
     return plant
```

## sunpeek/common/errors.py

```diff
@@ -3,17 +3,17 @@
 
 
 class ConfigurationError(SunPeekError):
     pass
 
 
 class CollectorDefinitionError(SunPeekError):
-    """Error in Collector definition.
-    E.g. if supplied information is contradictory or not sufficient for full Collector definition.
-    See #70 for valid Collector definitions.
+    """Error in CollectorType definition.
+    E.g. if supplied information is contradictory or not sufficient for full CollectorType definition.
+    See #70 for valid CollectorType definitions.
     """
     pass
 
 
 class IncompatibleUnitError(SunPeekError):
     """Supplied unit (of raw sensor) is not compatible with the expected unit, e.g. as defined in SensorType.
     """
@@ -55,19 +55,14 @@
 
 
 class SensorDataNotFoundError(SunPeekError):
     """Error due to not finding a data column for a sensor in the current data store"""
     pass
 
 
-class NoDataError(SunPeekError):
-    """No data are available in the selected data range"""
-    pass
-
-
 class TimeIndexError(SunPeekError):
     """Error handling or retrieving plant.time_index."""
     pass
 
 
 class TimeZoneError(SunPeekError):
     """Error related to time zone"""
```

## sunpeek/common/time_zone.py

```diff
@@ -5,15 +5,15 @@
 
 from sunpeek.common.unit_uncertainty import Q
 from sunpeek.common.utils import sp_logger
 from sunpeek.common.errors import TimeZoneError
 
 tf = TimezoneFinder()
 
-TIMEZONE_NONE = "UTC offset included in data"   # Example: 2017-05-01 00:01:00+02
+TIMEZONE_NONE = "Timezone included in data"
 TIMEZONE_LOCAL_NO_DST = "Plant local timezone without DST"
 TIMEZONE_LOCAL_WITH_DST = "Plant local timezone with DST"
 
 available_timezones = \
     [TIMEZONE_NONE, TIMEZONE_LOCAL_NO_DST, TIMEZONE_LOCAL_WITH_DST] + \
     list(pytz.common_timezones) + ['UTC+' + str(i) for i in range(1, 13)] + ['UTC-' + str(i) for i in range(1, 13)]
 
@@ -130,32 +130,30 @@
 
     See also
     --------
     `pvlib documentation on timezones <https://pvlib-python.readthedocs.io/en/v0.3.0/timetimezones.html>`_
     """
 
     sp_logger.debug("[validate_timezone] Checking for timezone consistency.")
-    original_timezone = timezone
 
     if isinstance(timezone, str):
         timezone = process_timezone(timezone, plant)
     if (timezone is not None) and not (
             isinstance(timezone, pytz.BaseTzInfo) or isinstance(timezone, pytz._FixedOffset)):
         raise TimeZoneError(f'Expected time zone to be a pytz.timezone object, but got "{type(timezone)}" instead.')
 
     assert isinstance(idx, pd.DatetimeIndex), \
         'Input index expected to be a pandas DatetimeIndex.'
     tz_idx_none = idx.tzinfo is None
 
     if (timezone is None) and tz_idx_none:
         # No timezone information available at all -> error
         raise TimeZoneError(
-            f"No timezone information found: 'timezone' is set to '{original_timezone}' but no timezone information "
-            f"was found in the provided data."
-        )
+            'No timezone information found: Timezone string is None, '
+            'and no timezone information was found in the provided pandas index.')
 
     if (timezone is not None) and not tz_idx_none:
         # Both timezone information available -> error (see docstring)
         raise TimeZoneError(
             'Ambiguous timezone information found: A timezone string (resulting in a dynamic offset) is given, '
             'and timezone information is also given in the pandas index (fixed offset).'
             'These two pieces of timezone information are incompatible. Use only one of them, string or pandas index.')
```

## sunpeek/common/unit_uncertainty.py

```diff
@@ -231,30 +231,14 @@
     #     unc_m = unc.to(unit).magnitude
     # uarr = unumpy.uarray(x, unc_m)
     # pobj = units.Quantity(uarr, unit)
     pobj = units.Quantity(x, unit)
     return pobj
 
 
-def to_dict(q: Q) -> dict | None:
-    """"Return dictionary with keys ['magnitude', 'units'], parsable by parse_quantity.
-    """
-    if q is None:
-        return None
-    if isinstance(q, dict):
-        if set(q.keys()) != {'magnitude', 'units'}:
-            raise ValueError(f'Expected dictionary with keys ["magnitude", "units"]. '
-                             f'Got keys {", ".join(list(q.keys()))}.')
-        return q
-    if isinstance(q, Q):
-        return {'magnitude': q.magnitude,
-                'units': str(q.units)}
-    raise TypeError(f'Expected input of type Quantity. Got {q.__class__.__name__}.')
-
-
 # def to_series(pobj, output_unit=None, direction='plus', k=1, n=1, index=None, name=None):
 #     """
 #     Converts a (vector) pint Quantity object to a pandas Series by stripping uncertainty and unit from pobj.
 #
 #     Notes
 #     -----
 #     Treatment of uncertainty can be controlled by parameters direction and k. This function returns the expanded
@@ -475,15 +459,15 @@
     max_limit = Q(max_limit, unit)
     if any(np.array(q < min_limit).ravel()) or any(np.array(q > max_limit).ravel()):
         raise ValueError(f'Input `q` must be within limits {min_limit:~} and {max_limit:~}, but is {q:~}')
     return q
 
 
 def parse_quantity(value: Union[dict, list, Q, pd.Series]) -> Q:
-    if (value is None) or isinstance(value, Q):
+    if isinstance(value, Q) or value is None:
         return value
     elif 'magnitude' and 'units' in value:
         return Q(value['magnitude'], value['units'])
     elif isinstance(value, pd.Series):
         return Q(value.astype('float64').to_numpy(), value.pint.units.__str__())
     elif isinstance(value, list):
         try:
```

## sunpeek/common/utils.py

```diff
@@ -1,53 +1,35 @@
-import datetime as dt
 import logging
 import os
 import enum
 import pathlib
 import sys
-from typing import Union
-
 import dotenv
-import pandas as pd
-import pytz
 import sqlalchemy.orm
 import sqlalchemy.event
 import sqlalchemy.exc
-import sqlalchemy.ext.declarative
-from sqlalchemy import MetaData
 from pydantic import BaseModel
 from logging.config import dictConfig
 
 try:
     import uvicorn
-
     api_modules_available = True
 except ModuleNotFoundError:
     # API dependecies are not installed, log only to standard output, no file.
     api_modules_available = False
 
 dotenv.load_dotenv()
 
 ROOT_DIR = os.path.abspath(pathlib.Path(__file__).parent.parent)
 log_dir = os.path.join(ROOT_DIR, 'logs')
 log_fname = os.path.join(log_dir, 'server.log')
 
 API_LOCAL_BASE_URL = "http://127.0.0.1:8000/"
 API_TOKEN = "harvestIT"
 
-ORMBase = sqlalchemy.ext.declarative.declarative_base()
-
-ORMBase.metadata = MetaData(naming_convention={
-    "ix": "ix_%(column_0_label)s",
-    "uq": "uq_%(table_name)s_%(column_0_name)s",
-    "ck": "ck_%(table_name)s_`%(constraint_name)s`",
-    "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
-    "pk": "pk_%(table_name)s"
-})
-
 
 async def handle_api_exceptions(caller: str, error_message: str, exception):
     print(f"[handle_api_exceptions] /!\\ An exception ocurred in {caller}. Preparing API and LOG entries...")
 
     err_type, err_obj, traceback = sys.exc_info()
 
     # details_dict = {"error_message": error_message, "exception_info": exception.}
@@ -106,24 +88,24 @@
     handlers = {
         "default": {
             "formatter": "std_out",
             "class": "logging.StreamHandler",
             "stream": "ext://sys.stderr",
         }}
 
-    loggers = {"sp_logger": {"handlers": ["default"], "level": LOG_LEVEL}, }
+    loggers = {"sp_logger": {"handlers": ["default"], "level": LOG_LEVEL},}
     if api_modules_available:
         handlers["file"] = \
             {
-                "formatter": "file_out",
-                "class": "logging.FileHandler",
-                "level": "WARNING",
-                "filename": log_fname
+            "formatter": "file_out",
+            "class": "logging.FileHandler",
+            "level": "WARNING",
+            "filename": log_fname
             }
-        loggers = {"sp_logger": {"handlers": ["default", "file"], "level": LOG_LEVEL}, }
+        loggers = {"sp_logger": {"handlers": ["default", "file"], "level": LOG_LEVEL},}
 
 
 def get_env(name):
     try:
         value = os.environ[name]
     except KeyError:
         raise MissingEnvVar(name)
@@ -142,15 +124,14 @@
     if user is not None and db_type != 'sqlite':
         db_str = db_str + user
     if pw is not None and db_type != 'sqlite':
         db_str = db_str + ':{}@'.format(pw)
     db_str = '{}{}'.format(db_str, host)
     if db_type != 'sqlite':
         db_str = '{}/{}'.format(db_str, db_name)
-
     return db_str
 
 
 S = None
 db_engine = None
 
 
@@ -187,41 +168,8 @@
     verify = 'verify'
 
 
 class DatetimeTemplates(enum.Enum):
     year_month_day = "year_month_day"
     day_month_year = "day_month_year"
     month_day_year = "month_day_year"
-
-
-# Timestamp-related utilities
-
-def to_utc(ds: str) -> dt.datetime:
-    """Return timezone-aware datetime in UTC from ISO format datetime string
-    """
-    return pytz.utc.localize(dt.datetime.fromisoformat(ds))
-
-
-def to_unix_str(ds: str) -> str:
-    """Return UNIX ms timestamp from ISO format datetime string.
-    """
-    return str(int(1000 * to_utc(ds).timestamp()))
-
-
-def utc_str(x: Union[str, dt.datetime, pd.Timestamp]) -> str:
-    """Return ISO string from datetime or UNIX ms timestamp (given as string).
-    """
-    fmt = "%Y-%m-%d %H:%M:%S"
-    if isinstance(x, (dt.datetime, pd.Timestamp)):
-        return x.strftime(fmt)
-    return dt.datetime.utcfromtimestamp(int(x) / 1000).strftime(fmt)
-
-
-def json_to_df(j: dict) -> pd.DataFrame:
-    """Convert json data returned by get-sensor-data API endpoint to DataFrame with UTC index.
-    """
-    df = pd.DataFrame(
-        list(j.items()), columns=["unix_timestamps_ms", "values"], dtype=float
-    )
-    df.index = pd.to_datetime(df["unix_timestamps_ms"], unit="ms")
-    df.index.name = "utc"
-    return df
+
```

## sunpeek/components/__init__.py

```diff
@@ -1,17 +1,17 @@
 from sunpeek.components import helpers
 from sunpeek.components.base import Component
-from sunpeek.components.components_factories import CollectorQDT, CollectorSST, CollectorTypes
+from sunpeek.components.components_factories import CollectorTypeQDT, CollectorTypeSST
 from sunpeek.components.fluids import Fluid, WPDFluid, CoolPropFluid, FluidFactory, \
     FluidDefinition, CoolPropFluidDefinition, WPDFluidDefinition
 from sunpeek.components.fluids_wpd_models import ModelFactory
 from sunpeek.components.iam_methods import IAM_K50, IAM_ASHRAE, IAM_Interpolated, IAM_Ambrosetti
 from sunpeek.components.operational_events import OperationalEvent
-from sunpeek.components.physical import Plant, Array
-from sunpeek.components.outputs_pc_method import PCMethodOutput, PCMethodOutputPlant, PCMethodOutputArray
+from sunpeek.components.physical import Plant, Array, HeatExchanger
+from sunpeek.components.results import PCMethodOutput, PCMethodOutputPlant, PCMethodOutputArray
 from sunpeek.components.sensor import Sensor, SensorInfo
-from sunpeek.components.types import SensorType, Collector
+from sunpeek.components.types import SensorType, CollectorType
 
 from sunpeek.components.jobs import Job
 from sunpeek.components.helpers import make_tables, SensorMap
 
 helpers.AttrSetterMixin.define_component_attrs()
```

## sunpeek/components/base.py

```diff
@@ -1,23 +1,29 @@
+
 import warnings
-from typing import Union, Dict
+from typing import Union, Dict, Optional
 import enum
 import dataclasses
 from sqlalchemy.ext.associationproxy import association_proxy
 from sqlalchemy.orm import relationship, declared_attr
 from sqlalchemy import Column, Integer, Identity, String
 from sqlalchemy.orm.collections import attribute_mapped_collection
 
 from sunpeek.common.errors import ConfigurationError
 from sunpeek.common.unit_uncertainty import Q
 from sunpeek.components import types
-from sunpeek.components.helpers import IsVirtual, AttrSetterMixin, SensorMap, ORMBase, AlgoCheckMode
+from sunpeek.components.helpers import IsVirtual, AttrSetterMixin, SensorMap, ORMBase
 from sunpeek.components.sensor import Sensor
 
 
+class AlgoCheckMode(str, enum.Enum):
+    config_only = 'config_only'
+    config_and_data = 'config_and_data'
+
+
 @dataclasses.dataclass
 class SensorSlot:
     """
     A pydantic class used to hold and validate information on a component sensor slot.
     
     Parameters
     ----------
@@ -141,55 +147,55 @@
             # If slot is not empty and sensor currently in slot is not virtual
             self.sensor_map[slot_name].unmap()
         elif self.sensors[slot_name].is_virtual:
             warnings.warn('You cannot set a virtual sensor directly. Virtual sensors are calculated automatically.')
 
         return
 
-    def map_vsensor(self, slot_name: str, feedback: 'sunpeek.serializable_models.CoreMethodFeedback'):
+    def map_vsensor(self, slot_name: str, problems: 'sunpeek.serializable_models.ProblemReport'):
         """Create virtual Sensor and map it to component.slot_name, or map None if it cannot be calculated.
 
         Parameters
         ----------
         slot_name : str, slot / channel name of the component self to which the virtual sensor will be mapped.
-        feedback : CoreMethodFeedback, problems reported at config time, prior to vsensor calculation.
+        problems : ProblemReport, problems reported at config time, prior to vsensor calculation.
         """
         if not self.has_virtual_slot_named(slot_name):
             raise ConfigurationError(f'Cannot map virtual sensor because slot {slot_name} of {self} '
                                      f'does not accept virtual sensors.')
         # print(f'map_vsensor: component={self}, slot_name={slot_name}')
         try:
             sensor = self.sensors[slot_name]
         except KeyError:
             sensor = None
 
-        can_calculate = feedback.success and (slot_name not in feedback.problem_slots)
+        can_calculate = problems.success and (slot_name not in problems.problem_slots)
 
         # Sensor already mapped? Update only, don't create new Sensor
         if sensor is not None:
             if sensor.is_virtual:
-                sensor.problems = feedback
+                sensor.problems = problems
                 sensor.can_calculate = can_calculate
             # Not virtual? Do nothing / Do not overwrite existing real sensor
             return
 
         # Create new sensor
         vsensor_name = f"{slot_name}__virtual__{self.__class__.__name__}_{self.name}".replace(' ', '_').lower()
         if (self.plant is not None) and (self.plant.get_raw_sensor(vsensor_name) is not None):
             # vsensor with matching name already exists in Plant
             sensor = self.plant.get_raw_sensor(vsensor_name)
-            sensor.feedback = feedback
+            sensor.problems = problems
             sensor.can_calculate = can_calculate
         else:
             # Create virtual sensor
             # Needs to store compatible_unit of sensor_type, so it can later check if vsensor calc result units are ok.
             sensor = Sensor(is_virtual=True,
                             plant=self.plant,
                             raw_name=vsensor_name,
-                            problems=feedback,
+                            problems=problems,
                             can_calculate=can_calculate,
                             native_unit=self.sensor_slots[slot_name].sensor_type.compatible_unit_str
                             )
 
         SensorMap(slot_name, sensor, component=self, sensor_type=self.sensor_slots[slot_name].sensor_type)
 
     def has_virtual_slot_named(self, slot_name):
@@ -200,39 +206,40 @@
 
     @classmethod
     def get_real_slots(cls):
         """Get component's slot names for (possibly or always) real (not virtual) sensors
         """
         return [slot for slot in cls.sensor_slots.values() if slot.virtual != IsVirtual.always]
 
-    def is_sensor_missing(self, slot_name: str, check_mode) -> bool:
+    def is_slot_missing(self, slot_name: str, check_mode) -> bool:
         """Return True if component slot has no sensor mapped or sensor is not ready for calculations.
         """
         # May raise AttributeError
         sensor = getattr(self, slot_name)
         if sensor is None:
             return True
         if isinstance(sensor, Q):
             raise TypeError(f'In {self.name}, {self.__class__.__name__}.{slot_name} exists, '
                             f'but is a ComponentParam, not a sensor. Check the calling code.')
-        if not sensor.is_virtual:
-            return False
-
-        # Virtual sensor "missing" status depends on check mode (config / config + data)
-        if check_mode == AlgoCheckMode.config_only:
-            is_slot_ok = sensor.can_calculate
-        elif check_mode == AlgoCheckMode.config_and_data:
-            is_slot_ok = sensor.data.notna().any()
-        else:
-            raise ValueError(f'Unexpected check_mode "{check_mode}". '
-                             f'Expected: {", ".join(list(AlgoCheckMode))}')
-        return not is_slot_ok
+        if sensor.is_virtual:
+            if check_mode == AlgoCheckMode.config_only:
+                is_slot_ok = sensor.can_calculate
+                # return not is_slot_ok
+            elif check_mode == AlgoCheckMode.config_and_data:
+                is_slot_ok = sensor.data.notna().any()
+                # return not is_slot_ok
+                # return not sensor.data.notna().any()
+            else:
+                raise ValueError(f'Unexpected check_mode "{check_mode}". '
+                                 f'Expected: {", ".join(list(AlgoCheckMode))}')
+            return not is_slot_ok
+        return False
 
-    def is_real_sensor_missing(self, slot_name: str, check_mode) -> bool:
-        """Like is_sensor_missing, but additionally checks if sensor in named slot is real (not virtual).
+    def is_real_slot_missing(self, slot_name: str, check_mode) -> bool:
+        """Like is_slot_missing, but additionally checks if sensor in named slot is real (not virtual).
         """
         sensor = getattr(self, slot_name)
         if sensor is None:
             return True
         if isinstance(sensor, Q):
             raise TypeError(f'In {self.name}, {self.__class__.__name__}.{slot_name} exists, '
                             f'but is a ComponentParam, not a sensor. Check the calling code.')
```

## sunpeek/components/components_factories.py

```diff
@@ -1,51 +1,45 @@
 # -*- coding: utf-8 -*-
 
-import datetime as dt
+from datetime import datetime
 from sunpeek.common.unit_uncertainty import Q
 from pint import Quantity
-from sunpeek.components.types import Collector, CollectorTypes
+from sunpeek.components.types import CollectorType
 from sunpeek.components.iam_methods import IAM_Method
 
 
-class CollectorSST:
+class CollectorTypeSST:
     def __new__(cls, eta0hem: Q, a1: Q, a2: Q, ceff: Q, test_reference_area: Q, gross_length: Q,
-                iam_method: IAM_Method, collector_type: CollectorTypes,
-                kd=None,
+                iam_method: IAM_Method, kd=None,
                 name: str = None, manufacturer_name: str = None, product_name: str = None, licence_number: str = None,
                 area_gr: Q = None, area_ap: Q = None, gross_width: Q = None, gross_height: Q = None,
-                description: str = None, certificate_date_issued: dt.datetime = None, certificate_lab: str = None,
-                certificate_details: str = None, test_report_id: str = None,
-                a8=None, f_prime: Q = None, concentration_ratio: Q = None,
+                description: str = None, certificate_date_issued: datetime = None, certificate_lab: str = None,
+                certificate_details: str = None, test_report_id: str = None, f_prime: Q = None
                 ):
         test_type = "SST"
         a5 = ceff
-        return Collector(
+        return CollectorType(
             test_reference_area=test_reference_area, test_type=test_type, gross_length=gross_length, name=name,
             manufacturer_name=manufacturer_name, product_name=product_name,
             test_report_id=test_report_id, licence_number=licence_number,
             certificate_date_issued=certificate_date_issued, certificate_lab=certificate_lab,
-            certificate_details=certificate_details, collector_type=collector_type,
-            area_gr=area_gr, area_ap=area_ap, gross_width=gross_width,
-            gross_height=gross_height, a1=a1, a2=a2, a5=a5, a8=a8, kd=kd, eta0hem=eta0hem,
-            iam_method=iam_method, f_prime=f_prime, concentration_ratio=concentration_ratio)
+            certificate_details=certificate_details, area_gr=area_gr, area_ap=area_ap, gross_width=gross_width,
+            gross_height=gross_height, a1=a1, a2=a2, a5=a5, kd=kd, eta0hem=eta0hem, iam_method=iam_method, f_prime=f_prime)
 
 
-class CollectorQDT:
+class CollectorTypeQDT:
     def __new__(cls, eta0b: Quantity, a1: Quantity, a2: Q, a5: Q, kd: Q, test_reference_area: str, gross_length: Q,
-                iam_method: IAM_Method, collector_type: CollectorTypes,
+                iam_method: IAM_Method,
                 name: str = None, manufacturer_name: str = None, product_name: str = None, licence_number: str = None,
                 area_gr: Q = None, area_ap: Q = None, gross_width: Q = None, gross_height: Q = None,
-                description: str = None, certificate_date_issued: dt.datetime = None, certificate_lab: str = None,
-                certificate_details: str = None, test_report_id: str = None,
-                a8=None, f_prime: Q = None, concentration_ratio: Q = None,
+                description: str = None, certificate_date_issued: datetime = None, certificate_lab: str = None,
+                certificate_details: str = None, test_report_id: str = None, f_prime: Q = None
                 ):
         test_type = "QDT"
-        return Collector(
+        return CollectorType(
             test_reference_area=test_reference_area, test_type=test_type, gross_length=gross_length, name=name,
             manufacturer_name=manufacturer_name, product_name=product_name,
-            test_report_id=test_report_id, licence_number=licence_number,
-            certificate_date_issued=certificate_date_issued,
-            certificate_lab=certificate_lab, certificate_details=certificate_details, collector_type=collector_type,
+            test_report_id=test_report_id, licence_number=licence_number, certificate_date_issued=certificate_date_issued,
+            certificate_lab=certificate_lab, certificate_details=certificate_details,
             area_gr=area_gr, area_ap=area_ap, gross_width=gross_width, gross_height=gross_height,
-            a1=a1, a2=a2, a5=a5, a8=a8, kd=kd, eta0b=eta0b,
-            iam_method=iam_method, f_prime=f_prime, concentration_ratio=concentration_ratio)
+            a1=a1, a2=a2, a5=a5, kd=kd, eta0b=eta0b,
+            iam_method=iam_method, f_prime=f_prime)
```

## sunpeek/components/fluids.py

```diff
@@ -120,14 +120,15 @@
 .. _IAPWS-95:
     http://www.iapws.org/relguide/IAPWS-95.html
 
 """
 
 import numpy as np
 import pandas as pd
+import re
 import warnings
 import sqlalchemy
 import CoolProp.CoolProp as Cp
 from sqlalchemy import Column, String, Integer, ForeignKey, Float, Boolean, Enum, Identity, or_, \
     CheckConstraint, JSON, func
 from sqlalchemy.orm import relationship, declared_attr
 import sqlalchemy.event
@@ -453,17 +454,17 @@
     }
 
     def __init__(self, fluid_def_name, stored_args):
         self.fluid_def_name = fluid_def_name
         self.stored_args = stored_args
 
 
-# def assert_valid_fluid(fluid: Fluid):
-#     assert fluid is not None
-#     assert not isinstance(fluid, UninitialisedFluid)
+def assert_valid_fluid(fluid: Fluid):
+    assert fluid is not None
+    assert not isinstance(fluid, UninitialisedFluid)
 
 
 class CoolPropFluid(Fluid):
     """High level class for interface with CoolProp incompressible fluids.
 
     Input and output units to CoolProp are standardized and thus don't need to be specified.
 
@@ -530,20 +531,22 @@
             return 'HEOS::water'
         query_str = 'INCOMP::' + self.fluid.name
         if self.concentration is not None:
             query_str = f"{query_str}[{self.concentration.to('').m}]"
         return query_str
 
     def _get_density(self, te):
+        # val = Cp.PropsSI("D", "P", self.P_DEFAULT, "T", uu.to_numpy(te, 'K'), self.query_str)
         val = self._get_coolprop('density', te)
-        return uu.to_s(val.flatten().astype('float64'), 'kg m**-3')
+        return uu.to_s(val, 'kg m**-3')
 
     def _get_heat_capacity(self, te):
+        # val = Cp.PropsSI("C", "P", self.P_DEFAULT, "T", uu.to_numpy(te, 'K'), self.query_str)
         val = self._get_coolprop('heat capacity', te)
-        return uu.to_s(val.flatten().astype('float64'), 'J kg**-1 K**-1')
+        return uu.to_s(val, 'J kg**-1 K**-1')
 
     def _get_coolprop(self, prop, te):
         """Return density or heat capacity, with common error handling / CoolProp allowed temperature ranges.
         """
         coolprop_prop = "D" if prop == "density" else "C"
         try:
             val = Cp.PropsSI(coolprop_prop, "P", self.P_DEFAULT, "T", uu.to_numpy(te, 'K'), self.query_str)
@@ -626,20 +629,21 @@
 
     __mapper_args__ = {
         'polymorphic_identity': 'WPDMixed',
     }
 
     @declared_attr
     def _concentration(cls):
-        """Concentration column, shared with WPDMixedFluid class.
-        """
+        "concentration column, shared with WPDMixedFluid class"
         return Fluid.__table__.c.get('_concentration', Column(Integer))
 
     def __init__(self, concentration: Union[Q, dict], **kwargs):
         self.concentration = concentration
+        # self.density_model = density_model
+        # self.heat_capacity_model = heat_capacity_model
         super().__init__(**kwargs)
 
     @property
     def concentration(self):
         return Q(self._concentration, 'dimensionless').to('percent')
 
     @concentration.setter
@@ -649,9 +653,7 @@
 
     def _get_property(self, model, te):
         te_ = te.pint.m_as(model.unit['te'])
         c_ = self.concentration.m_as(model.unit['c'])
         output = model.predict(te_, c_)
 
         return uu.to_s(output, model.unit['out'])
-
-
```

## sunpeek/components/fluids_wpd_models.py

```diff
@@ -1,24 +1,20 @@
-from typing import Union, Optional
+from typing import Union
+import warnings
 from pathlib import Path
 from abc import ABC, abstractmethod
 import numpy as np
 import pandas as pd
 import numbers
 
-import matplotlib
-from matplotlib import pyplot as plt
-from matplotlib.offsetbox import TextArea, AnnotationBbox, VPacker
 from sklearn.preprocessing import PolynomialFeatures, StandardScaler
 from sklearn.linear_model import Ridge
 from sklearn.pipeline import make_pipeline
 from sklearn.model_selection import GridSearchCV, ShuffleSplit
 from sunpeek.definitions import FluidProps, fluid_data_dir
-import sunpeek.common.plot_utils as pu
-from sunpeek.common.unit_uncertainty import units
 
 
 class ModelFactory:
 
     @classmethod
     def from_info_and_property(cls, fluid_info: 'sunpeek.definitions.fluid_definitions.WPDFluidInfo', prop: FluidProps):
         fn = fluid_data_dir / fluid_info.name / prop.value
@@ -228,72 +224,54 @@
         """Read WebPlotDigitizer csv with single dataset into dataframe.
         """
         df = pd.read_csv(csv_file, header=0, sep=',')
         df = df.rename(columns={'X': 'te', 'Y': 'out'})
 
         return df
 
-    def plot_fit(self,
-                 prop_to_plot: FluidProps,
-                 fluid_name: str,
-                 settings: Optional[pu.PlotSettings] = None,
-                 n_points: Optional[int] = 50,
-                 ) -> matplotlib.figure.Figure:
-        # ) -> Optional[pu.PlotResult]:
-        """Plot model fit and original / ground truth data from the WebPlotDigitizer csv dataset.
-        This is useful to check quality of model fit, after calling self.train().
+    def _plot_fit(self, plot_type: FluidProps = None):  # pragma: no cover
+        """Check quality of model fit, after calling self.train().
+        Plots model fit and original / ground truth data from WebPlotDigitizer csv dataset.
         """
-        if prop_to_plot is None or prop_to_plot not in list(FluidProps):
-            raise ValueError(f'Invalid "prop_to_plot": {prop_to_plot}')
-
-        if settings is None:
-            settings = pu.PlotSettings()
-
-        # Data to plot
-        te = np.linspace(self.df['te'].min() - 20, self.df['te'].max() + 20, n_points)
+        try:
+            import plotly.graph_objects as go
+        except ModuleNotFoundError:
+            warnings.warn(
+                'This function requires the plotly package, which is not installed. Install it with `pip install plotly`')
+            return
+
+        N_POINTS = 50
+        fig = go.Figure()
+        te = np.linspace(self.df['te'].min() - 20, self.df['te'].max() + 20, N_POINTS)
         out = self.predict(te)
-
-        if prop_to_plot == 'density':
-            title_str = 'Fluid Density'
-            y_label = 'Density'
-            legend_loc = 'upper right'
-        else:
-            title_str = 'Fluid Heat Capacity'
-            y_label = 'Heat Capacity'
-            legend_loc = 'lower right'
-
-        # Plot Title
-        fig, params = pu.prepare_figure(settings=settings)
-        box = VPacker(children=[pu.box_title(f'{title_str} of fluid "{fluid_name}"'),
-                                ], pad=0, sep=pu.Defaults.sep_major.value)
-        artist = pu.annotation_bbox(box, xy=pu.Defaults.xy_topleft.value)
-        fig.add_artist(artist)
-
-        # Data plot
-        rect = pu.get_rectangle_below(artist, vsep=pu.Defaults.sep_huge.value, bottom=0.45)
-        ax = fig.add_axes(rect)
-
-        ax.plot(te, out, '-', label='Model prediction', zorder=2.5)
+        fig.add_trace(go.Scatter(x=te, y=out,
+                                 mode='lines',
+                                 name='Model prediction'))
         # Measured values as scatter
-        ax.scatter(self.df['te'], self.df['out'],
-                   s=pu.Defaults.marker_size_scatter.value,
-                   alpha=pu.Defaults.marker_alpha.value,
-                   facecolors=pu.Defaults.marker_facecolor.value,
-                   edgecolors=pu.Defaults.marker_edgecolor.value,
-                   zorder=2.6,
-                   label='WebPlotDigitizer measurements',
-                   )
-
-        ax.set_xlabel(f'Temperature [{units(self.unit["te"]).units:~P}]')
-        ax.set_ylabel(f'{y_label} [{units(self.unit["out"]).units:~P}]')
-        ax.grid()
-        ax.set_axisbelow('line')
-        ax.legend(loc=legend_loc).set_zorder(3)
-
-        return fig
+        fig.add_trace(go.Scatter(x=self.df['te'], y=self.df['out'],
+                                 mode='markers',
+                                 marker=dict(
+                                     color='Black',
+                                     size=10,
+                                     opacity=0.4,
+                                     line=dict(
+                                         color='Black',
+                                         width=1
+                                     )),
+                                 name='WPD measurements'))
+
+        if plot_type == FluidProps.density:
+            fig.update_layout(title='Density', width=1600, height=1200,
+                              xaxis_title="Temperature [degC]", yaxis_title="Density [{self.unit['out']}]",
+                              legend_traceorder="reversed")
+        elif plot_type == FluidProps.heat_capacity:
+            fig.update_layout(title='Heat capacity', width=1600, height=1200,
+                              xaxis_title="Temperature [degC]", yaxis_title=f"Heat capacity [{self.unit['out']}]",
+                              legend_traceorder="reversed")
+        fig.show()
 
 
 class WPDModelMixed(WPDModel):
     n_inputs = 2
     predictor_cols_in = ['te', 'c']
 
     def csv2df(self, csv_file):
@@ -309,68 +287,52 @@
             y = df_csv.iloc[:, i + 1].dropna()
             c = pd.Series(float(n), index=x.index)
             df2 = pd.concat([x.rename('te'), y.rename('out'), c.rename('c')], axis=1)
             df = pd.concat([df, df2], ignore_index=True)
 
         return df
 
-    def plot_fit(self,
-                 prop_to_plot: FluidProps,
-                 fluid_name: str,
-                 settings: Optional[pu.PlotSettings] = None,
-                 n_points: Optional[int] = 50,
-                 ) -> matplotlib.figure.Figure:
-        """Plot model fit and original / ground truth data from the WebPlotDigitizer csv dataset.
-        This is useful to check quality of model fit, after calling self.train().
+    def _plot_fit(self, plot_type: FluidProps = None):  # pragma: no cover
+        """Check quality of model fit, after calling self.train().
+        Plots model fit and original / ground truth data from WebPlotDigitizer csv dataset.
         """
-        if prop_to_plot is None or prop_to_plot not in list(FluidProps):
-            raise ValueError(f'Invalid "prop_to_plot": {prop_to_plot}')
-
-        if settings is None:
-            settings = pu.PlotSettings()
-
-        # Data to plot
-        if prop_to_plot == 'density':
-            title_str = 'Fluid Density'
-            y_label = 'Density'
-            legend_loc = 'upper right'
-        else:
-            title_str = 'Fluid Heat Capacity'
-            y_label = 'Heat Capacity'
-            legend_loc = 'lower right'
-
-        # Plot Title
-        fig, params = pu.prepare_figure(settings=settings)
-        box = VPacker(children=[pu.box_title(f'{title_str} of fluid "{fluid_name}"'),
-                                ], pad=0, sep=pu.Defaults.sep_major.value)
-        artist = pu.annotation_bbox(box, xy=pu.Defaults.xy_topleft.value)
-        fig.add_artist(artist)
-
-        # Data plot
-        rect = pu.get_rectangle_below(artist, vsep=pu.Defaults.sep_huge.value, bottom=0.45)
-        ax = fig.add_axes(rect)
+        try:
+            import plotly.graph_objects as go
+        except ModuleNotFoundError:
+            warnings.warn(
+                'This function requires the plotly package, which is not installed. Install it with `pip install plotly`')
+            return
 
+        N_POINTS = 50
         # For all concentration levels, create output curves within measured temperature limits
         df = self.df.groupby('c').te.agg(['min', 'max'])
+        fig = go.Figure()
         for i in np.arange(df.shape[0]):
-            te = np.linspace(df.iloc[i, 0], df.iloc[i, 1], n_points)
+            te = np.linspace(df.iloc[i, 0], df.iloc[i, 1], N_POINTS)
             c = df.index[i]
             out = self.predict(te, c)
-            ax.plot(te, out, '-', label=f'Model prediction, c={df.index[i]}%', zorder=2.5)
-
+            fig.add_trace(go.Scatter(x=te, y=out,
+                                     mode='lines',
+                                     name=f"Model prediction, c={df.index[i]}%"))
         # Measured values as scatter
-        ax.scatter(self.df['te'], self.df['out'],
-                   s=pu.Defaults.marker_size_scatter.value,
-                   alpha=pu.Defaults.marker_alpha.value,
-                   facecolors=pu.Defaults.marker_facecolor.value,
-                   edgecolors=pu.Defaults.marker_edgecolor.value,
-                   zorder=2.6,
-                   label='WebPlotDigitizer measurements',
-                   )
-
-        ax.set_xlabel(f'Temperature [{units(self.unit["te"]).units:~P}]')
-        ax.set_ylabel(f'{y_label} [{units(self.unit["out"]).units:~P}]')
-        ax.grid()
-        ax.set_axisbelow('line')
-        ax.legend(loc=legend_loc).set_zorder(3)
+        fig.add_trace(go.Scatter(x=self.df['te'], y=self.df['out'],
+                                 mode='markers',
+                                 marker=dict(
+                                     color='Black',
+                                     size=10,
+                                     opacity=0.4,
+                                     line=dict(
+                                         color='Black',
+                                         width=1
+                                     )),
+                                 name='WPD measurements'))
+
+        if plot_type == FluidProps.density:
+            fig.update_layout(title='Density', width=1600, height=1200,
+                              xaxis_title="Temperature [degC]", yaxis_title="Density [{self.unit['out']}]",
+                              legend_traceorder="reversed")
+        elif plot_type == FluidProps.heat_capacity:
+            fig.update_layout(title='Heat capacity', width=1600, height=1200,
+                              xaxis_title="Temperature [degC]", yaxis_title=f"Heat capacity [{self.unit['out']}]",
+                              legend_traceorder="reversed")
 
-        return fig
+        fig.show()
```

## sunpeek/components/helpers.py

```diff
@@ -1,57 +1,63 @@
 """
 .. codeauthor:: Marnoch Hamilton-Jones <m.hamilton-jones@aee.at>
 .. codeauthor:: Philip Ohnewein <p.ohnewein@aee.at>
 """
 
 import numpy as np
 import sqlalchemy
-from sqlalchemy import Column, String, Integer, ForeignKey, Float, Identity, ARRAY, Enum, UniqueConstraint, JSON, \
-    Boolean, DateTime
+from sqlalchemy import Column, String, Integer, ForeignKey, Float, Identity, ARRAY, Enum, UniqueConstraint, JSON
+from sqlalchemy.ext.declarative import declarative_base
 import sqlalchemy.orm
 import sqlalchemy.exc
 from sqlalchemy.orm import relationship
 import sqlalchemy.event
 import enum
 
 from sunpeek.common.unit_uncertainty import Q
 import sunpeek.common.unit_uncertainty as uu
 from sunpeek.common.errors import ConfigurationError
 from sunpeek.common.utils import DatetimeTemplates
-from sunpeek.common.utils import ORMBase
+
+ORMBase = declarative_base()
 
 
 class ComponentParam:
     """Used to define parameters which are represented by Quantities, with optional limit checking.  
     
     Attributes
     ----------
     unit: compatible unit
-    minimum: value of the parameters should not be below this
-    maximum: value of the parameters should not be above this
+    minimum: value of the paramters should not be below this
+    maximum: value of the paramters should not be above this
     array_type: either "scalar" or "array", defaults to used to create the correct database column types and apply checks correctly.
     """
 
     def __init__(self, unit: str = None, minimum: float = -np.Inf, maximum: float = np.Inf, param_type: str = 'scalar'):
+        # self.name = name
         self.unit = unit
         self.minimum = minimum
         self.maximum = maximum
         self.array_type = param_type
+        # Idea: Implement ComponentParam availability, also see ComponentParam.available
+        #  -> make this an ORM class, add "name" attrib, dataclass
+        # available: Optional[bool] = None
 
 
 class AttrSetterMixin:
     name = None
     defer_post_config_changed_actions = False
 
     @classmethod
     def define_component_attrs(cls):
         for sub_cls in cls.all_subclasses():
-            # Get all ComponentParam from all component / subclass attributes
+            # get all ComponentParam from all component / subclass attributes
             params = {attr: obj for attr, obj in sub_cls.__dict__.items() if isinstance(obj, ComponentParam)}
             for attr, obj in params.items():
+                # delattr(sub_cls, attr)
                 sub_cls.add_component_attr(attr, obj.unit, obj.minimum, obj.maximum, obj.array_type)
 
     @classmethod
     def add_component_attr(cls, name, unit=None, minimum=-np.Inf, maximum=np.Inf, array_type='scalar'):
         if array_type == 'scalar':
             setattr(cls, f'_{name}_mag', Column(Float))
         elif 'array':
@@ -75,34 +81,34 @@
     def register_callback(cls, callback_type, func):
         print(cls)
         try:
             getattr(cls, callback_type).append(func)
         except AttributeError:
             cls.post_config_changed_callbacks = [func]
 
-    def _check_value(self, name, value, param_type):
+    def _check_value(self, name, value, type):
         unit = self._attr_props[name]['unit']
 
         uu.assert_compatible(value.units, unit)
 
         val = value.to(unit).magnitude
         min = self._attr_props[name]['minimum']
         max = self._attr_props[name]['maximum']
 
-        if param_type == 'scalar' or param_type is None:
+        if type == 'scalar' or type is None:
             if isinstance(value.magnitude, np.ndarray):
                 raise ConfigurationError("Attempting to assign an array quantity to a scalar type parameter")
             if val < min:
                 raise ConfigurationError(
                     f"attempting to set a value for attribute {name} that is less than the minimum of {min}{unit}")
             if val > max:
                 raise ConfigurationError(
                     f"attempting to set a value for attribute {name} that is greater than the maximum of {max}{unit}")
 
-        if param_type == 'array':
+        if type == 'array':
             if not isinstance(value.magnitude, np.ndarray):
                 raise ConfigurationError("Attempting to assign an array quantity to a scalar type parameter")
             if (val < min).any():
                 raise ConfigurationError(
                     f"attempting to set a value for attribute {name} that is less than the minimum of {min}{unit}")
             if (val > max).any():
                 raise ConfigurationError(
@@ -150,20 +156,14 @@
             if item in self.sensor_slots.keys():
                 return None
             else:
                 raise AttributeError(f"{item} not found in component {self}.")
         except (AttributeError, AssertionError, sqlalchemy.exc.InvalidRequestError) as ex:
             raise AttributeError(f"{item} not found in component {self}. Original exception: {ex}")
 
-    @classmethod
-    def get_default_unit(cls, name: str) -> str:
-        """Return default unit of a class attribute defined as ComponentParam.
-        """
-        return cls._attr_props[name]['unit']
-
     def __str__(self):
         try:
             if self.name is not None:
                 return f'HarvestIT {self.__class__.__name__} component called {self.name}'
             else:
                 return f'HarvestIT {self.__class__.__name__} object'
         except sqlalchemy.exc.InvalidRequestError:
@@ -308,64 +308,14 @@
     timezone = Column(String)
     csv_separator = Column(String)
     csv_decimal = Column(String)
     csv_encoding = Column(String)
     index_col = Column(Integer)
 
 
-class PCSettingsDefaults(ORMBase):
-    __tablename__ = 'pc_settings_defaults'
-
-    id = Column(Integer, Identity(0), primary_key=True)
-    plant_id = Column(Integer, ForeignKey('plant.id', ondelete="CASCADE"), unique=True, nullable=False)
-    plant = relationship("Plant", back_populates="pc_settings_defaults")
-
-    evaluation_mode = Column(Enum('ISO', 'extended', name='pc_evaluation_modes'))
-    formula = Column(Integer)
-    wind_used = Column(Boolean)
-
-    safety_uncertainty = Column(Float)
-    safety_pipes = Column(Float)
-    safety_others = Column(Float)
-
-    def __init__(self, *args, **kwargs):
-        self.evaluation_mode = "ISO"
-        super().__init__(*args, **kwargs)
-
-
-class UploadHistory(ORMBase):
-    __tablename__ = 'upload_history'
-
-    id = Column(Integer, Identity(0), primary_key=True)
-    plant_id = Column(Integer, ForeignKey('plant.id', ondelete="CASCADE"), nullable=False)
-    plant = relationship("Plant", back_populates="upload_history")
-
-    name = Column(String)
-    size_bytes = Column(Float)
-    status = Column(String)
-    error_cause = Column(String)
-    n_rows = Column(Integer)
-    date_of_upload = Column(DateTime())
-    start = Column(DateTime())
-    end = Column(DateTime())
-    _missing_columns = Column(String)       # list of string -> converted to string with ; as separator
-
-    SEPARATOR = ";"
-
-    @property
-    def missing_columns(self):
-        return [] if (self._missing_columns == "") else self._missing_columns.split(self.SEPARATOR)
-
-    @missing_columns.setter
-    def missing_columns(self, value):
-        if isinstance(value, str):
-            value = [value]
-        self._missing_columns = "" if (value is None) else self.SEPARATOR.join(value)
-
-
 # class Row(AttrSetterMixin, ORMBase):
 #     """
 #     Implements a single collector row with its inlet and outlet temperatures, as part of an Array.
 #
 #     Note
 #     ----
 #     An array may consist of several rows. Row components are useful if inlet or outlet temperatures (`te_in`,
@@ -410,12 +360,7 @@
 
 # AttrSetterMixin.define_component_attrs()
 
 
 def make_tables(engine):
     """Uses the table definitions contained in the component classes to create DB tables"""
     ORMBase.metadata.create_all(engine)
-
-
-class AlgoCheckMode(str, enum.Enum):
-    config_only = 'config_only'
-    config_and_data = 'config_and_data'
```

## sunpeek/components/iam_methods.py

```diff
@@ -159,15 +159,15 @@
     aoi_reference = aoi_reference.m_as('deg')
     iam_reference = iam_reference.m_as('')
     aoi = to_numpy(aoi, 'deg')
     azimuth_diff = to_numpy(azimuth_diff, 'deg')
 
     aoi_reference, iam_reference = _normalize_iam_reference(aoi_reference=aoi_reference, iam_reference=iam_reference)
 
-    # get components in transversal and longitudinal plane
+    # get components in transveral and longitudinal plane
     aoi_transversal = _get_aoi_transversal(aoi=aoi, azimuth_diff=azimuth_diff)
     aoi_longitudinal = _get_aoi_longitudinal(aoi=aoi, azimuth_diff=azimuth_diff)
 
     # interpolate IAM
     iam_transversal = np.interp(np.abs(aoi_transversal), aoi_reference[0], iam_reference[0])
     iam_longitudinal = np.interp(np.abs(aoi_longitudinal), aoi_reference[1], iam_reference[1])
     iam = np.multiply(iam_transversal, iam_longitudinal)
@@ -301,16 +301,16 @@
 
 class IAM_Method(ORMBase, AttrSetterMixin):
     __tablename__ = 'iam_methods'
     """ Abstract class for determining the incidence angle modifier (IAM) with different methods.
     """
     id = Column(Integer, Identity(0), primary_key=True)
     method_type = Column(Enum('IAM_Ambrosetti', 'IAM_ASHRAE', 'IAM_Interpolated', 'IAM_K50', name='iam_method_type'))
-    collector_id = Column(Integer, ForeignKey('collectors.id', ondelete="CASCADE"))
-    collector = relationship("Collector", back_populates="iam_method")
+    collector_type_id = Column(Integer, ForeignKey('collector_types.id', ondelete="CASCADE"))
+    collector_type = relationship("CollectorType", back_populates="iam_method")
 
     __mapper_args__ = {
         'polymorphic_identity': 'iam_method',
         'polymorphic_on': method_type,
         'with_polymorphic': '*'
     }
 
@@ -327,16 +327,15 @@
         azimuth_diff : pd.Series
             The difference between solar and collector azimuth angle, as pint (unit aware) pandas Series.
         """
         raise NotImplementedError("")
 
 
 class IAM_ASHRAE(IAM_Method):
-    """
-    Determine the IAM for given incidence angles ``aoi`` [degrees], using the ASHRAE formula.
+    """ Determines the IAM for given incidence angles ``aoi`` [degrees], using the ASHRAE formula.
 
     Parameters
     ----------
     b : float, default 0.05
         A parameter to adjust the incidence angle modifier as a function of
         angle of incidence. Typical values are on the order of 0.05 [3].
 
@@ -355,17 +354,16 @@
         self.b = b
 
     def get_iam(self, aoi: pd.Series, **kwargs):
         return get_iam_ASHRAE(aoi, b=self.b)
 
 
 class IAM_K50(IAM_Method):
-    """
-    Determine the IAM for given incidence angles ``aoi`` [degrees], using the ASHRAE formula, if only `k50`,
-    the IAM value at an incidence angle of 50, is given.
+    """ Determines the IAM for given incidence angles ``aoi`` [degrees], using the ASHRAE formula, if only `k50`, the
+        IAM value at an incidence angle of 50, is given.
 
     Parameters
     ----------
     k50 : float
         IAM value at an incidence angle of 50
 
     See Also
@@ -379,20 +377,21 @@
     b = ComponentParam('', 0, 1)
     k50 = ComponentParam('', 0, 2)
 
     def __init__(self, k50: Q, plant=None):
         self.k50 = k50
         self.b = -(self.k50 - 1) / (1 / np.cos(np.deg2rad(50)) - 1)
 
+
     def get_iam(self, aoi: pd.Series, **kwargs):
         return get_iam_ASHRAE(aoi, b=self.b)
 
 
 class IAM_Ambrosetti(IAM_Method):
-    """Determine the IAM for given incidence angles ``aoi`` [degrees], using the Ambrosetti formula.
+    """ Determines the IAM for given incidence angles ``aoi`` [degrees], using the Ambrosetti formula.
 
     Parameters
     ----------
     kappa : float
         exponent used for the Ambrosetti function
 
     See Also
@@ -410,16 +409,15 @@
         self.kappa = kappa
 
     def get_iam(self, aoi: pd.Series, **kwargs):
         return get_iam_ambrosetti(aoi, kappa=self.kappa)
 
 
 class IAM_Interpolated(IAM_Method):
-    """
-    Determine the incidence angle modifier by interpolating over a set of given reference values.
+    """ Determines the incidence angle modifier by interpolating over a set of given reference values.
 
     Parameters
     ----------
     iam_reference: list
         Reference values for IAM values at certain incidence angles. (must match with `aoi_reference`)
     aoi_reference: list
         Reference values for IAM values at certain incidence angles. (must match with `iam_reference`)
```

## sunpeek/components/physical.py

```diff
@@ -1,27 +1,28 @@
-import os
-from typing import List
-import uuid
 import numpy as np
 import pandas as pd
-import pytz
+from sqlalchemy.orm import relationship, Session, backref
 import sqlalchemy
+from sqlalchemy import Column, String, Integer, Float, Enum, ForeignKey, UniqueConstraint
 import sqlalchemy.event
-from sqlalchemy.orm import relationship, Session, backref
-from sqlalchemy import Column, String, Integer, Float, Boolean, Enum, ForeignKey, UniqueConstraint
+import os
+from typing import List
+import uuid
 
 from sunpeek.common.unit_uncertainty import Q, parse_quantity
-import sunpeek.common.errors as err
+from sunpeek.common.errors import ConfigurationError, DuplicateNameError, SensorNotFoundError
 import sunpeek.common.time_zone as tz
-from sunpeek.components.helpers import AccuracyClass, ComponentParam, IsVirtual, DataUploadDefaults, PCSettingsDefaults
+# import sunpeek.common.data_uploader
+
+from sunpeek.components.helpers import AccuracyClass, ComponentParam, IsVirtual, DataUploadDefaults
 from sunpeek.components.operational_events import OperationalEvent
 from sunpeek.components.base import Component, SensorSlot
-from sunpeek.components.fluids import FluidFactory, FluidDefinition, UninitialisedFluid, Fluid
+from sunpeek.components.fluids import FluidFactory, FluidDefinition, UninitialisedFluid
 from sunpeek.components.sensor import Sensor
-from sunpeek.components.types import Collector, UninitialisedCollector
+from sunpeek.components.types import CollectorType, UninitialisedCollectorType
 from sunpeek.components import sensor_types as st
 import sunpeek.db_utils.crud
 
 
 class Plant(Component):
     """
     Implements large solar thermal plant as the overarching component on which the kernel methods are applied.
@@ -32,23 +33,23 @@
     name : str
         Plant name. Must be unique within HarvestIT 'plant' database.
     owner : str, optional
         Name of plant owner.
     operator : str, optional
         Name of plant operator.
     description : str, optional
-        Description of the plant, its components, hydraulic setup and other relevant information.
+        Description of the plant, its comopnents, hydraulic setup and other relevant information.
     location_name : str, optional
         Name of the location. Example: 'Graz, Austria'
     latitude : pint Quantity
         Geographical latitude. Positive is north of the equator. See `pvlib Location`_.
     longitude : pint Quantity
         Geographical longitude. Positive is east of the prime meridian. See `pvlib Location`_.
-    elevation : pint Quantity, optional
-        Location elevation, e.g. Q(440, 'm'). If available, used to improve pvlib's solar position calculation.
+    altitude : pint Quantity, optional
+        Location altitude, e.g. Q(440, 'm'). If available, used to improve pvlib's solar position calculation.
     data_upload_defaults : DataUploadDefaults,
         Defaults for parsing raw data files for this plant. Defaults to an all null DataUploadDefaults
 
     fluid_solar : Fluid object
         Fluid in the solar circuit. Optional for the PC (Performance Check)
         method (but stated in the standard report, see Annex A1 in `ISO standard 24194`_),
         required for the D-CAT (Dynamic Collector Array Test) method.
@@ -148,212 +149,183 @@
     name = Column(String, unique=True)
     owner = Column(String)
     operator = Column(String)
     description = Column(String)
 
     raw_data_path = Column(String)
     calc_data_path = Column(String)
-    virtuals_calculation_uptodate = Column(Boolean)
 
     operational_events = relationship("OperationalEvent", back_populates="plant", cascade="all, delete-orphan")
 
     _latitude = ComponentParam('deg', -90, 90)
     _longitude = ComponentParam('deg', -180, 180)
-    tz_data_offset = Column(Float)
-    elevation = ComponentParam('m', -430.5, 8848.86)  # Anything between the Dead Sea and Everest
+    _tz_data_offset = Column(Float)
+    altitude = ComponentParam('m', -430.5, 8848.86)  # Anything between the Dead Sea and Everest
     location_name = Column(String)
 
     fluid_solar = relationship("Fluid", back_populates='plant', uselist=False, cascade="all, delete")
-    fluidvol_total = ComponentParam('m**3', 0, np.Inf)
+    fluid_vol = ComponentParam('m**3', 0, np.Inf)
 
     plant_measurement_accuracy = Column(Enum(AccuracyClass))
     raw_sensors = relationship("Sensor", back_populates="plant", cascade="all, delete-orphan")
-
-    upload_history = relationship("UploadHistory", back_populates="plant", cascade="all, delete-orphan",
-                                  passive_deletes=True)
     data_upload_defaults = relationship("DataUploadDefaults", back_populates="plant", cascade="all, delete-orphan",
                                         passive_deletes=True, uselist=False)
-    pc_settings_defaults = relationship("PCSettingsDefaults", back_populates="plant", cascade="all, delete-orphan",
-                                        passive_deletes=True, uselist=False)
 
     raw_names = {}
 
     sensor_slots = {
-        'tp':
-            SensorSlot('tp', st.thermal_power,
-                       'Thermal power', IsVirtual.possible,
-                       description='Total thermal power of the plant, including all its collector arrays.'),
-        'vf':
-            SensorSlot('vf', st.volume_flow,
-                       'Volume flow', IsVirtual.never,
-                       description='Total volume flow in the solar circuit of the plant, for all collector arrays.'),
-        'mf':
-            SensorSlot('mf', st.mass_flow,
-                       'Mass flow', IsVirtual.possible,
-                       description='Total mass flow in the solar circuit of the plant, for all collector arrays.'),
-        'te_in':
-            SensorSlot('te_in', st.fluid_temperature,
-                       'Inlet temperature', IsVirtual.never,
-                       description='Inlet / return temperature of the plant; this is the temperature after the '
-                                   'heat exchanger, sent back to the collector arrays.'),
-        'te_out':
-            SensorSlot('te_out', st.fluid_temperature,
-                       'Outlet temperature', IsVirtual.possible,
-                       description='Inlet / return temperature of the plant; this is the temperature received by '
-                                   'all collector arrays together, before the fluid enters the heat exchanger.'),
-        'te_amb':
-            SensorSlot('te_amb', st.ambient_temperature,
-                       'Ambient temperature', IsVirtual.never,
-                       description='Ambient air temperature representative for the plant.'),
-        've_wind':
-            SensorSlot('ve_wind', st.wind_speed,
-                       'Wind speed', IsVirtual.never,
-                       description='Wind speed / wind velocity representative for the plant.'),
-        'rh_amb':
-            SensorSlot('rh_amb', st.float_0_100,
-                       'Relative humidity', IsVirtual.never,
-                       description='Ambient relative humidity representative for the plant.'),
-        'pr_amb':
-            SensorSlot('pr_amb', st.pressure,
-                       'Air pressure', IsVirtual.never,
-                       description='Ambient air pressure representative for the plant.'),
-        'te_dew_amb':
-            SensorSlot('te_dew_amb', st.ambient_temperature,
-                       'Dew point temperature', IsVirtual.possible,
-                       'Dew point temperature representative for the plant. Is calculated as a virtual '
-                       'sensor if both te_amb and rh_amb have data (are not None).'),
-        'in_global':
-            SensorSlot('in_global', st.global_radiation,
-                       'Global radiation input', IsVirtual.never,
-                       description='Global radiation sensor to be used to calculate horizontal radiation '
-                                   'components for the plant. The sensor may be installed at a non-zero '
-                                   'tilt angle, in that case the horizontal radiation components will be '
-                                   'calculated by a radiation model.'),
-        'in_beam':
-            SensorSlot('in_beam', st.direct_radiation,
-                       'Direct radiation input', IsVirtual.never,
-                       description='Direct / beam radiation sensor to be used to calculate horizontal '
-                                   'radiation components for the plant. The sensor may be installed at a '
-                                   'non-zero tilt angle, in that case the horizontal radiation components '
-                                   'will be calculated by a radiation model.'),
-        'in_diffuse':
-            SensorSlot('in_diffuse', st.diffuse_radiation,
-                       'Diffuse radiation input', IsVirtual.never,
-                       description='Diffuse radiation sensor to be used to calculate horizontal radiation '
-                                   'components for the plant. The sensor may be installed at a non-zero '
-                                   'tilt angle, in that case the horizontal radiation components will be '
-                                   'calculated by a radiation model.'),
-        'in_dni':
-            SensorSlot('in_dni', st.dni_radiation,
-                       'DNI radiation input', IsVirtual.never,
-                       description='Direct normal irradiance (DNI) sensor to be used to calculate horizontal '
-                                   'radiation components for the plant.'),
-        'rd_ghi':
-            SensorSlot('rd_ghi', st.global_radiation,
-                       'Global radiation', IsVirtual.always,
-                       description='Global horizontal irradiance. Calculated by a radiation conversion model '
-                                   'from in_global, in_beam, in_diffuse, in_dni.'),
-        'rd_bhi':
-            SensorSlot('rd_bhi', st.direct_radiation,
-                       'Direct radiation', IsVirtual.always,
-                       description='Direct / beam horizontal irradiance. Calculated by a radiation conversion '
-                                   'model from in_global, in_beam, in_diffuse, in_dni.'),
-        'rd_dhi':
-            SensorSlot('rd_dhi', st.diffuse_radiation,
-                       'Diffuse radiation', IsVirtual.always,
-                       description='Diffuse horizontal irradiance. Calculated by a radiation conversion model '
-                                   'from in_global, in_beam, in_diffuse, in_dni.'),
-        'rd_dni':
-            SensorSlot('rd_dni', st.dni_radiation,
-                       'DNI (direct normal) radiation', IsVirtual.always,
-                       description='Direct normal irradiance. Calculated by a radiation model from '
-                                   'in_global, in_beam, in_diffuse, in_dni.'),
-        'sun_azimuth':
-            SensorSlot('sun_azimuth', st.angle_0_360,
-                       'Solar azimuth angle', IsVirtual.always,
-                       description='Solar azimuth angle.'),
-        'sun_zenith':
-            SensorSlot('sun_zenith', st.angle_0_180,
-                       'Solar zenith angle', IsVirtual.always,
-                       description='Solar zenith angle'),
-        'sun_apparent_zenith':
-            SensorSlot('sun_apparent_zenith', st.angle_0_180,
-                       'Apparent solar zenith angle', IsVirtual.always,
-                       description='Apparent solar zenith angle'),
-        'sun_elevation':
-            SensorSlot('sun_elevation', st.angle__90_90,
-                       'Solar elevation angle', IsVirtual.always,
-                       description='Solar elevation / altitude angle.'),
-        'sun_apparent_elevation':
-            SensorSlot('sun_apparent_elevation', st.angle__90_90,
-                       'Apparent solar elevation angle', IsVirtual.always,
-                       description='Apparent solar elevation angle'),
-        'rd_dni_extra':
-            SensorSlot('rd_dni_extra', st.dni_radiation,
-                       'Extraterrestrial solar radiation', IsVirtual.always,
-                       description='Extraterrestrial solar radiation.'),
-        'rel_airmass':
-            SensorSlot('rel_airmass', st.float_0_100,
-                       'Relative airmass', IsVirtual.always,
-                       description='Relative airmass.'),
-        'abs_airmass':
-            SensorSlot('abs_airmass', st.float_0_100,
-                       'Absolute airmass', IsVirtual.always,
-                       description='Absolute airmass.'),
-        'linke_turbidity':
-            SensorSlot('linke_turbidity', st.float_0_100,
-                       'Linke turbidity', IsVirtual.always,
-                       description='Linke turbidity calculated for specific location and date.'),
-        'rd_ghi_clearsky':
-            SensorSlot('rd_ghi_clearsky', st.global_radiation,
-                       'Clear sky global horizontal irradiance', IsVirtual.always,
-                       description='Clear sky global horizontal irradiance based on Linke turbidity, '
-                                   'calculated with pvlib.clearsky.ineichen'),
-        'rd_dni_clearsky':
-            SensorSlot('rd_dni_clearsky', st.dni_radiation,
-                       'Clear sky direct normal irradiance', IsVirtual.always,
-                       description='Clear sky direct normal irradiance (DNI) based on Linke turbidity, '
-                                   'calculated with pvlib.clearsky.ineichen')
+        'tp': SensorSlot('tp', st.thermal_power,
+                         'Thermal power', IsVirtual.possible,
+                         description='Total thermal power of the plant, including all its collector arrays.'),
+        'vf': SensorSlot('vf', st.volume_flow,
+                         'Volume flow', IsVirtual.never,
+                         description='Total volume flow in the solar circuit of the plant, for all collector arrays.'),
+        'mf': SensorSlot('mf', st.mass_flow,
+                         'Mass flow', IsVirtual.possible,
+                         description='Total mass flow in the solar circuit of the plant, for all collector arrays.'),
+        'te_in': SensorSlot('te_in', st.fluid_temperature,
+                            'Inlet temperature', IsVirtual.never,
+                            description='Inlet / return temperature of the plant; this is the temperature after the '
+                                        'heat exchanger, sent back to the collector arrays.'),
+        'te_out': SensorSlot('te_out', st.fluid_temperature,
+                             'Outlet temperature', IsVirtual.possible,
+                             description='Inlet / return temperature of the plant; this is the temperature received by '
+                                         'all collector arrays together, before the fluid enters the heat exchanger.'),
+        'te_amb': SensorSlot('te_amb', st.ambient_temperature,
+                             'Ambient temperature', IsVirtual.never,
+                             description='Ambient air temperature representative for the plant.'),
+        've_wind': SensorSlot('ve_wind', st.wind_speed,
+                              'Wind speed', IsVirtual.never,
+                              description='Wind speed / wind velocity representative for the plant.'),
+        'rh_amb': SensorSlot('rh_amb', st.float_0_1,
+                             'Relative humidity', IsVirtual.never,
+                             description='Ambient relative humidity representative for the plant.'),
+        'pr_amb': SensorSlot('pr_amb', st.pressure,
+                             'Air pressure', IsVirtual.never,
+                             description='Ambient air pressure representative for the plant.'),
+        'te_dew_amb': SensorSlot('te_dew_amb', st.ambient_temperature,
+                                 'Dew point temperature', IsVirtual.possible,
+                                 'Dew point temperature representative for the plant. Is calculated as a virtual '
+                                 'sensor if both te_amb and rh_amb have data (are not None).'),
+        'in_global': SensorSlot('in_global', st.global_radiation,
+                                'Global radiation input', IsVirtual.never,
+                                description='Global radiation sensor to be used to calculate horizontal radiation '
+                                            'components for the plant. The sensor may be installed at a non-zero '
+                                            'tilt angle, in that case the horizontal radiation components will be '
+                                            'calculated by a radiation model.'),
+        'in_beam': SensorSlot('in_beam', st.direct_radiation,
+                              'Direct radiation input', IsVirtual.never,
+                              description='Direct / beam radiation sensor to be used to calculate horizontal '
+                                          'radiation components for the plant. The sensor may be installed at a '
+                                          'non-zero tilt angle, in that case the horizontal radiation components '
+                                          'will be calculated by a radiation model.'),
+        'in_diffuse': SensorSlot('in_diffuse', st.diffuse_radiation,
+                                 'Diffuse radiation input', IsVirtual.never,
+                                 description='Diffuse radiation sensor to be used to calculate horizontal radiation '
+                                             'components for the plant. The sensor may be installed at a non-zero '
+                                             'tilt angle, in that case the horizontal radiation components will be '
+                                             'calculated by a radiation model.'),
+        'in_dni': SensorSlot('in_dni', st.dni_radiation,
+                             'DNI radiation input', IsVirtual.never,
+                             description='Direct normal irradiance (DNI) sensor to be used to calculate horizontal '
+                                         'radiation components for the plant.'),
+        'rd_ghi': SensorSlot('rd_ghi', st.global_radiation,
+                             'Global radiation', IsVirtual.always,
+                             description='Global horizontal irradiance. Calculated by a radiation conversion model '
+                                         'from in_global, in_beam, in_diffuse, in_dni.'),
+        'rd_bhi': SensorSlot('rd_bhi', st.direct_radiation,
+                             'Direct radiation', IsVirtual.always,
+                             description='Direct / beam horizontal irradiance. Calculated by a radiation conversion '
+                                         'model from in_global, in_beam, in_diffuse, in_dni.'),
+        'rd_dhi': SensorSlot('rd_dhi', st.diffuse_radiation,
+                             'Diffuse radiation', IsVirtual.always,
+                             description='Diffuse horizontal irradiance. Calculated by a radiation conversion model '
+                                         'from in_global, in_beam, in_diffuse, in_dni.'),
+        'rd_dni': SensorSlot('rd_dni', st.dni_radiation,
+                             'DNI (direct normal) radiation', IsVirtual.always,
+                             description='Direct normal irradiance. Calculated by a radiation model from '
+                                         'in_global, in_beam, in_diffuse, in_dni.'),
+        'sun_azimuth': SensorSlot('sun_azimuth', st.angle_0_360,
+                                  'Solar azimuth angle', IsVirtual.always,
+                                  description='Solar azimuth angle.'),
+        'sun_zenith': SensorSlot('sun_zenith', st.angle_0_180,
+                                 'Solar zenith angle', IsVirtual.always,
+                                 description='Solar zenith angle'),
+        'sun_apparent_zenith': SensorSlot('sun_apparent_zenith', st.angle_0_180,
+                                          'Apparent solar zenith angle', IsVirtual.always,
+                                          description='Apparent solar zenith angle'),
+        'sun_elevation': SensorSlot('sun_elevation', st.angle__90_90,
+                                    'Solar elevation angle', IsVirtual.always,
+                                    description='Solar elevation / altitude angle.'),
+        'sun_apparent_elevation': SensorSlot('sun_apparent_elevation', st.angle__90_90,
+                                             'Apparent solar elevation angle', IsVirtual.always,
+                                             description='Apparent solar elevation angle'),
+        'rd_dni_extra': SensorSlot('rd_dni_extra', st.dni_radiation,
+                                   'Extraterrestrial solar radiation', IsVirtual.always,
+                                   description='Extraterrestrial solar radiation.'),
+        'rel_airmass': SensorSlot('rel_airmass', st.float_0_100,
+                                  'Relative airmass', IsVirtual.always,
+                                  description='Relative airmass.'),
+        'abs_airmass': SensorSlot('abs_airmass', st.float_0_100,
+                                  'Absolute airmass', IsVirtual.always,
+                                  description='Absolute airmass.'),
+        'linke_turbidity': SensorSlot('linke_turbidity', st.float_0_100,
+                                      'Linke turbidity', IsVirtual.always,
+                                      description='Linke turbidity calculated for specific location and date.'),
+        'rd_ghi_clearsky': SensorSlot('rd_ghi_clearsky', st.global_radiation,
+                                      'Clear sky global horizontal irradiance', IsVirtual.always,
+                                      description='Clear sky global horizontal irradiance based on Linke turbidity, '
+                                                  'calculated with pvlib.clearsky.ineichen'),
+        'rd_dni_clearsky': SensorSlot('rd_dni_clearsky', st.dni_radiation,
+                                      'Clear sky direct normal irradiance', IsVirtual.always,
+                                      description='Clear sky direct normal irradiance (DNI) based on Linke turbidity, '
+                                                  'calculated with pvlib.clearsky.ineichen')
     }
 
     def __init__(self, name=None, owner=None, operator=None, description=None, plant_measurement_accuracy=None,
-                 location_name=None, latitude=None, longitude=None, elevation=None,
-                 fluid_solar=None, fluidvol_total=None, arrays=None, sensor_map=None, raw_sensors=None,
+                 location_name=None, latitude=None, longitude=None, altitude=Q(100, "m"),
+                 fluid_solar=None, fluidvol_total=None, arrays=[], sensor_map={}, raw_sensors=[],
                  **kwargs):
 
-        # To change plant context, explicitly attach a different Context object to plant in the calling code
+        # to change plant context, explicitly attach a different Context object to plant in the calling code
         from sunpeek.data_handling.context import Context
         self.context = Context(plant=self)
+
         self.defer_post_config_changed_actions = True
 
-        self.name = name or str(uuid.uuid4().hex[0:12])
+        if name is not None:
+            self.name = name
+        else:
+            self.name = str(uuid.uuid4().hex[0:12])
         self.owner = owner
         self.operator = operator
         self.description = description
         self.plant_measurement_accuracy = plant_measurement_accuracy
         self.location_name = location_name
-        self.tz_data_offset = None
+        self._tz_data_offset = None
         self.latitude = latitude
         self.longitude = longitude
-        self.elevation = elevation or Q(100, 'm')
+        self.altitude = altitude
 
-        self.raw_sensors = raw_sensors or []
+        self.raw_sensors = raw_sensors
         self.fluid_solar = fluid_solar
         self.fluidvol_total = fluidvol_total
-        self.arrays = arrays or []
+        self.arrays = arrays
 
         self.raw_data_path = os.environ.get('SUNPEEK_RAW_DATA_PATH', './raw_data') + '/' + self.name
         self.calc_data_path = os.environ.get('SUNPEEK_CALC_DATA_PATH', './calc_data') + '/' + self.name
-        self.virtuals_calculation_uptodate = False
 
         self.data_upload_defaults = DataUploadDefaults()
-        self.pc_settings_defaults = PCSettingsDefaults()
 
-        self.sensor_map = sensor_map or {}
-        self.set_sensors(**kwargs)
+        self.sensor_map = sensor_map
+        if len(kwargs) > 0:
+            self.set_sensors(**kwargs)
+        else:
+            self.defer_post_config_changed_actions = False
+            [func(self.plant) for func in self.plant.post_config_changed_callbacks]
 
     def add_array(self, arrays):
         """
         Convenience method for adding items to plant.arrays. Equivalent to plant.arrays += array or plant.arrays.append(array).
 
         Parameters
         ----------
@@ -367,44 +339,46 @@
             arrays = [arrays]
         for array in arrays:
             self.arrays.append(array)
         return self.arrays
 
     @sqlalchemy.orm.reconstructor
     def _init_on_load(self):
-        self.set_default_context(datasource='pq')
+        # create default Context object
+        from sunpeek.data_handling.context import Context
+        self.context = Context(plant=self, datasource='pq')
 
     @property
     def latitude(self):
         return self._latitude
 
     @property
     def longitude(self):
         return self._longitude
 
     @latitude.setter
     def latitude(self, val):
         val = parse_quantity(val)
         if (val is not None) and (self.longitude is not None):
-            self.tz_data_offset = tz.get_timezone_offset_minutes(latitude=val, longitude=self.longitude)
+            self._tz_data_offset = tz.get_timezone_offset_minutes(latitude=val, longitude=self.longitude)
         self._latitude = val
 
     @longitude.setter
     def longitude(self, val):
         val = parse_quantity(val)
         if (val is not None) and (self.latitude is not None):
-            self.tz_data_offset = tz.get_timezone_offset_minutes(latitude=self.latitude, longitude=val)
+            self._tz_data_offset = tz.get_timezone_offset_minutes(latitude=self.latitude, longitude=val)
         self._longitude = val
 
     @property
-    def tz_data(self) -> pytz.FixedOffset:
-        return tz.get_data_timezone(self.tz_data_offset)
+    def tz_data(self):
+        return tz.get_data_timezone(self._tz_data_offset)
 
     @property
-    def local_tz_string_with_DST(self) -> str | None:
+    def local_tz_string_with_DST(self):
         if (self.latitude is None) or (self.longitude is None):
             return None
         return tz.get_timezone_string(latitude=self.latitude, longitude=self.longitude)
 
     @sqlalchemy.orm.validates('arrays', include_removes=True)
     def _validate_arrays(self, attr_name, component, is_remove):
         """ Used to automatically convert array dict representations to components and
@@ -429,22 +403,14 @@
     def _validate_data_upload_defaults(self, _, component):
         """ Used to automatically convert dict representation to DataUploadDefaults object.
         """
         if isinstance(component, dict):
             return DataUploadDefaults(**component)
         return component
 
-    @sqlalchemy.orm.validates('pc_settings_defaults')
-    def _validate_pc_settings_defaults(self, _, component):
-        """ Used to automatically convert dict representation to PCSettingsDefaults object.
-        """
-        if isinstance(component, dict):
-            return PCSettingsDefaults(**component)
-        return component
-
     @property
     def plant(self):
         return self
 
     @property
     def ignored_ranges(self) -> List[pd.Interval]:
         """Gets a list of time ranges to be ignored from the plant's `operational_events`
@@ -453,33 +419,37 @@
         for event in self.operational_events:
             if event.ignored_range:
                 intervals.append(
                     pd.Interval(pd.to_datetime(event.event_start), pd.to_datetime(event.event_end), closed='both'))
 
         return list(set(intervals))
 
-    def is_ignored(self, timestamp) -> bool:
+    def is_ignored(self, timestamp):
         """
         Checks if a timestamp is in an ignored range
 
         Parameters
         ----------
         timestamp : datetime.datetime or pandas.Timestamp or str
+
+        Returns
+        -------
+        bool
         """
 
         if isinstance(timestamp, str):
             timestamp = pd.to_datetime(timestamp)
 
-        for r in self.ignored_ranges:
-            if timestamp in r:
+        for range in self.ignored_ranges:
+            if timestamp in range:
                 return True
 
         return False
 
-    def add_operational_event(self, start, end=None, tz=None, description=None, ignored_range=False) -> None:
+    def add_operational_event(self, start, end=None, tz=None, description=None, ignored_range=False):
         """
         Parameters
         ----------
         start : A datetime object, or a string. If the string does not contain a df_timezone like '2022-1-1 00:00+1',
             then the tz argument must also be specified.
         end : A datetime object, or a string. If the string does not contain a df_timezone like '2022-1-2 00:00+1',
             then the tz argument must also be specified.
@@ -488,32 +458,32 @@
             A description of the event or reason for ignored range.
         ignored_range : bool
             If data in the period specified in the event should be ignored
         """
 
         OperationalEvent(event_start=start, event_end=end, tz=tz, ignored_range=ignored_range, description=description,
                          plant=self)
-        if ignored_range and self.context is not None:
-            self.reset_cache()
+        if ignored_range:
+            self.context.reset_cache()
 
-    # @property
-    # def radiation_input_slots(self):
-    #     return self.in_global, self.in_beam, self.in_diffuse, self.in_dni
+    @property
+    def radiation_input_slots(self):
+        return self.in_global, self.in_beam, self.in_diffuse, self.in_dni
 
     @property
     def area_gr(self):
         return sum([a.area_gr for a in self.arrays])
 
     @property
     def area_ap(self):
         return sum([a.area_ap for a in self.arrays])
 
     @property
     def time_index(self):
-        return self.context.time_index if self.context is not None else None
+        return self.context.time_index
 
     @sqlalchemy.orm.validates('raw_sensors', include_removes=True)
     def _validate_raw_sensors(self, _, val, is_remove):
         # assert isinstance(val, list), "raw_sensors must be a list of Sensor objects or dicts"
         if is_remove:
             val.remove_references(include_plant=False)
         if isinstance(val, dict):
@@ -530,51 +500,37 @@
                     return sensor
         else:
             try:
                 return sunpeek.db_utils.crud.get_sensors(session, plant_id=self.id, raw_name=raw_name)
             except (sqlalchemy.exc.NoResultFound, sqlalchemy.exc.MultipleResultsFound):
                 pass
         if raise_if_not_found:
-            raise err.SensorNotFoundError(f"Either no sensor with raw_name '{raw_name}' was found, "
-                                          f"or more than one such sensor was")
+            raise SensorNotFoundError(f"Either no sensor with raw_name '{raw_name}' was found, "
+                                      f"or more than one such sensor was")
 
     def get_raw_names(self, include_virtuals=False, only_virtuals=False):
         if include_virtuals:
             return [sensor.raw_name for sensor in self.raw_sensors]
         if only_virtuals:
             return [sensor.raw_name for sensor in self.raw_sensors if (sensor.is_virtual and sensor.can_calculate)]
         return [sensor.raw_name for sensor in self.raw_sensors if not sensor.is_virtual]
 
-    def set_default_context(self, datasource=None):
-        """Create and set default context as Context with parquet datasource. Does not upload or affect any data.
-        """
-        from sunpeek.data_handling.context import Context
-        self.context = Context(plant=self, datasource=datasource)
-
-    def reset_cache(self) -> None:
-        if self.context is not None:
-            self.context.cache.reset()
-
-    def delete_all_data(self) -> None:
-        if self.context is not None:
-            self.context.delete_all_data()
-
 
 class Array(Component):
     """
-    Implements collector array with given area, homogeneous tilt and azimuth angles and exactly 1 collector.
+    Implements collector array with given area, homogeneous tilt and azimuth angles and exactly 1 collector_type.
 
     Attributes
     ----------
     name : str
         Name of array. Must be unique within parent plant.
     plant : Plant object
         Plant to which the array belongs.
-    collector
-        Collector used in this array. An array has exactly 1 `collector`.
+    collector_type
+        Collector type used in this array. An array has exactly 1 `collector_type`.
     area_gr : pint Quantity
         Total gross collector area of the collector array.
     area_ap : pint Quantity, optional
         Total aperture collector area of the collector array.
 
     azim : pint Quantity
         Azimuth angle of the array surface. An array has exactly 1 scalar `azim`. North=0, East=90,
@@ -617,16 +573,16 @@
         Thermal power of collector array.
     vf : Sensor, optional
         Total volume flow of collector array.
     mf : Sensor, optional
         Total mass flow of collector array.
 
     is_shadowed : Sensor, optional, or virtual Sensor
-        Boolean variable that describes whether at a particular timestamp the array is considered
-        partly or completely shadowed (shadowed: value 1 or True, not shadowed: value 0 or False).
+        Boolean variable that describes whether at a particular timestamp the array is considered shadowed,
+        so either partly or completely shadowed.
         A user can set `is_shadowed` as a real sensor to provide shadow information from external sources,
         e.g. from a calculation that takes horizon or the 3D surroundings of the array into account.
         If not provided by user, `is_shadowed` is calculated as a virtual sensor taking into account
         maximum angle of incidence, minimum sun elevation, no internal (row-to-row) shading.
     aoi : virtual Sensor
         Angle of incidence of sun on plane of array, i.e. the angle between the solar vector and the array surface
         normal.
@@ -683,16 +639,16 @@
     }
 
     id = Column(Integer, ForeignKey('components.component_id'), primary_key=True)
 
     plant_id = Column(Integer, ForeignKey('plant.id', ondelete="CASCADE"))
     plant = relationship("Plant", foreign_keys=[plant_id], backref=backref("arrays", cascade="all, delete"))
     name = Column(String)
-    collector_id = Column(Integer, ForeignKey('collectors.id'))
-    _collector = relationship("Collector", passive_deletes='all')
+    collector_type_id = Column(Integer, ForeignKey('collector_types.id'))
+    _collector_type = relationship("CollectorType", passive_deletes='all')
 
     area_gr = ComponentParam('m**2', 1, np.Inf)
     area_ap = ComponentParam('m**2', 1, np.Inf)
     azim = ComponentParam('deg', 0, 360)
     tilt = ComponentParam('deg', 0, 90)
     row_spacing = ComponentParam('m', 0, np.Inf)
     n_rows = ComponentParam('', 0, np.Inf)
@@ -704,184 +660,167 @@
     rho_colsurface = ComponentParam('', 0, 1)
     max_aoi_shadow = ComponentParam('deg', 30, 90)
     min_elevation_shadow = ComponentParam('deg', 0, 90)
 
     __table_args__ = (UniqueConstraint('name', 'plant_id', name='_unique_array_names_per_plant'),)
 
     sensor_slots = {
-        'tp':
-            SensorSlot('tp', st.thermal_power,
-                       'Thermal power', IsVirtual.possible,
-                       description='Thermal power of collector array.'),
-        'vf':
-            SensorSlot('vf', st.volume_flow,
-                       'Volume flow', IsVirtual.never,
-                       description='Total volume flow of collector array.'),
-        'mf':
-            SensorSlot('mf', st.mass_flow,
-                       'Mass flow', IsVirtual.possible,
-                       description='Total mass flow of collector array.'),
-        'te_in':
-            SensorSlot('te_in', st.fluid_temperature,
-                       'Inlet temperature', IsVirtual.never,
-                       description='Inlet / return temperature characteristic for this array.'),
-        'te_out':
-            SensorSlot('te_out', st.fluid_temperature,
-                       'Outlet temperature', IsVirtual.possible,
-                       description='Outlet / flow / supply temperature characteristic for this array.'),
-        'is_shadowed':
-            SensorSlot('is_shadowed', st.bool,
-                       'Array is shadowed', IsVirtual.possible,
-                       description='Boolean variable that describes whether at a particular timestamp '
-                                   'the array is considered partly or completely shadowed '
-                                   '(shadowed: value 1 or True, not shadowed: value 0 or False).'
-                                   'A user can set `is_shadowed` as a real sensor to provide shadow '
-                                   'information from external sources, e.g. from a calculation that takes '
-                                   'horizon or the 3D surroundings of the array into account. If not '
-                                   'provided by user, `is_shadowed` is calculated as a virtual sensor '
-                                   'taking into account maximum angle of incidence, minimum sun elevation, '
-                                   'no internal (row-to-row) shading.'),
-        'in_global':
-            SensorSlot('in_global', st.global_radiation,
-                       'Global radiation input', IsVirtual.never,
-                       description='Global radiation sensor to be used to calculate tilted radiation '
-                                   'components for the array. The sensor may be installed at a non-zero '
-                                   'tilt angle, in that case the horizontal radiation components will be '
-                                   'calculated by a radiation model.'),
-        'in_beam':
-            SensorSlot('in_beam', st.direct_radiation,
-                       'Direct radiation input', IsVirtual.never,
-                       description='Direct / beam radiation sensor to be used to calculate tilted radiation '
-                                   'components for the array. The sensor may be installed at a '
-                                   'non-zero tilt angle, in that case the horizontal radiation components '
-                                   'will be calculated by a radiation model.'),
-        'in_diffuse':
-            SensorSlot('in_diffuse', st.diffuse_radiation,
-                       'Diffuse radiation input', IsVirtual.never,
-                       description='Diffuse radiation sensor to be used to calculate tilted radiation '
-                                   'components for the array. The sensor may be installed at a non-zero '
-                                   'tilt angle, in that case the horizontal radiation components will be '
-                                   'calculated by a radiation model.'),
-        'in_dni':
-            SensorSlot('in_dni', st.dni_radiation,
-                       'DNI radiation input', IsVirtual.never,
-                       description='Direct normal irradiance (DNI) sensor to be used to calculate tilted '
-                                   'radiation components for the array.'),
-        'rd_gti':
-            SensorSlot('rd_gti', st.global_radiation,
-                       'Global radiation', IsVirtual.always,
-                       description='Global horizontal irradiance. Calculated by a radiation conversion model '
-                                   'from in_global, in_beam, in_diffuse, in_dni.'),
-        'rd_bti':
-            SensorSlot('rd_bti', st.direct_radiation,
-                       'Direct radiation', IsVirtual.always,
-                       description='Direct / beam horizontal irradiance. Calculated by a radiation conversion '
-                                   'model from in_global, in_beam, in_diffuse, in_dni.'),
-        'rd_dti':
-            SensorSlot('rd_dti', st.diffuse_radiation,
-                       'Diffuse radiation', IsVirtual.always,
-                       description='Diffuse horizontal irradiance. Calculated by a radiation conversion model '
-                                   'from in_global, in_beam, in_diffuse, in_dni.'),
-        'aoi':
-            SensorSlot('aoi', st.angle__90_90,
-                       'Angle of incidence', IsVirtual.possible,
-                       description='Angle of incidence of sun on plane of array, i.e. the angle between the solar '
-                                   'vector and the array surface normal.'),
-        'shadow_angle':
-            SensorSlot('shadow_angle', st.angle_0_90,
-                       'Shadow angle between collector rows', IsVirtual.always,
-                       description='Shadow angle between collector rows: Required minimum sun elevation '
-                                   'in order not to have beam shading.'),
-        'shadow_angle_midpoint':
-            SensorSlot('shadow_angle_midpoint', st.angle_0_90,
-                       'Shadow angle between collector rows, at half slant height',
-                       IsVirtual.always,
-                       description="Shadow angle between collector rows, at half of the "
-                                   "collector's slant height (the 'midpoint'): Sun elevation that "
-                                   "corresponds to a internal_shading_ratio of 0.5. This can be "
-                                   "used as a typical angle for diffuse masking."),
-        'internal_shading_ratio':
-            SensorSlot('internal_shading_ratio', st.float_0_1,
-                       'Internal shading of the array', IsVirtual.always,
-                       description='Internal shading (row-to-row shading) of the array, a numeric'
-                                   ' value between 0 (no shading) and 1 (completely shaded).'),
-        'te_op':
-            SensorSlot('te_op', st.fluid_temperature,
-                       'Mean fluid temperature', IsVirtual.always,
-                       description='Mean fluid temperature, arithmetic mean of inlet and outlet temperatures.'),
-        'te_op_deriv':
-            SensorSlot('te_op_deriv', st.temperature_derivative,
-                       'Derivative of mean fluid temperature', IsVirtual.always,
-                       description='Derivative of the mean operating temperature te_op.'),
-        'iam':
-            SensorSlot('iam', st.float,
-                       'Incidence angle modifier of direct radiation', IsVirtual.always,
-                       description='Incidence angle modifier of direct radiation.'),
+        'tp': SensorSlot('tp', st.thermal_power,
+                         'Thermal power', IsVirtual.possible,
+                         description='Thermal power of collector array.'),
+        'vf': SensorSlot('vf', st.volume_flow,
+                         'Volume flow', IsVirtual.never,
+                         description='Total volume flow of collector array.'),
+        'mf': SensorSlot('mf', st.mass_flow,
+                         'Mass flow', IsVirtual.possible,
+                         description='Total mass flow of collector array.'),
+        'te_in': SensorSlot('te_in', st.fluid_temperature,
+                            'Inlet temperature', IsVirtual.never,
+                            description='Inlet / return temperature characteristic for this array.'),
+        'te_out': SensorSlot('te_out', st.fluid_temperature,
+                             'Outlet temperature', IsVirtual.possible,
+                             description='Outlet / flow / supply temperature characteristic for this array.'),
+        'is_shadowed': SensorSlot('is_shadowed', st.bool,
+                                  'Array is shadowed', IsVirtual.possible,
+                                  description='Boolean variable that describes whether at a particular timestamp the '
+                                              'array is considered shadowed, so either partly or completely shadowed. '
+                                              'A user can set `is_shadowed` as a real sensor to provide shadow '
+                                              'information from external sources, e.g. from a calculation that takes '
+                                              'horizon or the 3D surroundings of the array into account. If not '
+                                              'provided by user, `is_shadowed` is calculated as a virtual sensor '
+                                              'taking into account maximum angle of incidence, minimum sun elevation, '
+                                              'no internal (row-to-row) shading.'),
+        'in_global': SensorSlot('in_global', st.global_radiation,
+                                'Global radiation input', IsVirtual.never,
+                                description='Global radiation sensor to be used to calculate tilted radiation '
+                                            'components for the array. The sensor may be installed at a non-zero '
+                                            'tilt angle, in that case the horizontal radiation components will be '
+                                            'calculated by a radiation model.'),
+        'in_beam': SensorSlot('in_beam', st.direct_radiation,
+                              'Direct radiation input', IsVirtual.never,
+                              description='Direct / beam radiation sensor to be used to calculate tilted radiation '
+                                          'components for the array. The sensor may be installed at a '
+                                          'non-zero tilt angle, in that case the horizontal radiation components '
+                                          'will be calculated by a radiation model.'),
+        'in_diffuse': SensorSlot('in_diffuse', st.diffuse_radiation,
+                                 'Diffuse radiation input', IsVirtual.never,
+                                 description='Diffuse radiation sensor to be used to calculate tilted radiation '
+                                             'components for the array. The sensor may be installed at a non-zero '
+                                             'tilt angle, in that case the horizontal radiation components will be '
+                                             'calculated by a radiation model.'),
+        'in_dni': SensorSlot('in_dni', st.dni_radiation,
+                             'DNI radiation input', IsVirtual.never,
+                             description='Direct normal irradiance (DNI) sensor to be used to calculate tilted '
+                                         'radiation components for the array.'),
+        'rd_gti': SensorSlot('rd_gti', st.global_radiation,
+                             'Global radiation', IsVirtual.always,
+                             description='Global horizontal irradiance. Calculated by a radiation conversion model '
+                                         'from in_global, in_beam, in_diffuse, in_dni.'),
+        'rd_bti': SensorSlot('rd_bti', st.direct_radiation,
+                             'Direct radiation', IsVirtual.always,
+                             description='Direct / beam horizontal irradiance. Calculated by a radiation conversion '
+                                         'model from in_global, in_beam, in_diffuse, in_dni.'),
+        'rd_dti': SensorSlot('rd_dti', st.diffuse_radiation,
+                             'Diffuse radiation', IsVirtual.always,
+                             description='Diffuse horizontal irradiance. Calculated by a radiation conversion model '
+                                         'from in_global, in_beam, in_diffuse, in_dni.'),
+        'aoi': SensorSlot('aoi', st.angle__90_90,
+                          'Angle of incidence', IsVirtual.possible,
+                          description='Angle of incidence of sun on plane of array, i.e. the angle between the solar '
+                                      'vector and the array surface normal.'),
+        'shadow_angle': SensorSlot('shadow_angle', st.angle_0_90,
+                                   'Shadow angle between collector rows', IsVirtual.always,
+                                   description='Shadow angle between collector rows: Required minimum sun elevation '
+                                               'in order not to have beam shading.'),
+        'shadow_angle_midpoint': SensorSlot('shadow_angle_midpoint', st.angle_0_90,
+                                            'Shadow angle between collector rows, at half slant height',
+                                            IsVirtual.always,
+                                            description="Shadow angle between collector rows, at half of the "
+                                                        "collector's slant height (the 'midpoint'): Sun elevation that "
+                                                        "corresponds to a internal_shading_ratio of 0.5. This can be "
+                                                        "used as a typical angle for diffuse masking."),
+        'internal_shading_ratio': SensorSlot('internal_shading_ratio', st.float_0_1,
+                                             'Internal shading of the array', IsVirtual.always,
+                                             description='Internal shading (row-to-row shading) of the array, a numeric'
+                                                         ' value between 0 (no shading) and 1 (completely shaded).'),
+        'te_op': SensorSlot('te_op', st.fluid_temperature,
+                            'Mean fluid temperature', IsVirtual.always,
+                            description='Mean fluid temperature, arithmetic mean of inlet and outlet temperatures.'),
+        'te_op_deriv': SensorSlot('te_op_deriv', st.temperature_derivative,
+                                  'Derivative of mean fluid temperature', IsVirtual.always,
+                                  description='Derivative of the mean operating temperature te_op.'),
+        'iam': SensorSlot('iam', st.float,
+                          'Incidence angle modifier of direct radiation', IsVirtual.always,
+                          description='Incidence angle modifier of direct radiation.'),
     }
 
-    def __init__(self, name=None, plant=None, collector=None, area_gr=None, area_ap=None, azim=None, tilt=None,
-                 row_spacing=None, n_rows=None, ground_tilt=None, mounting_level=None,
-                 fluidvol_total=None, rho_ground=None, rho_colbackside=None, rho_colsurface=None,
-                 max_aoi_shadow=None, min_elevation_shadow=None, sensor_map=None, **kwargs):
+    def __init__(self, name=None, plant=None, collector_type=None, area_gr=None, area_ap=None, azim=None, tilt=None,
+                 row_spacing=None, n_rows=None, ground_tilt=Q(0, 'deg'), mounting_level=Q(0, 'm'),
+                 fluidvol_total=None, rho_ground=None, rho_colbackside=None, rho_colsurface=Q(0),
+                 max_aoi_shadow=Q(80, 'deg'), min_elevation_shadow=None, sensor_map={}, **kwargs):
 
         self.defer_post_config_changed_actions = True
         self.name = name
-        self.collector = collector
+        self.collector_type = collector_type
 
         self.area_ap = area_ap
-        self.area_gr = area_gr or self.calc_area_gr_from_collector()
+        self.area_gr = area_gr if area_gr is not None else self.calc_area_gr_from_collector()
         self.azim = azim
         self.tilt = tilt
 
         self.row_spacing = row_spacing
         self.n_rows = n_rows
-        self.ground_tilt = ground_tilt or Q(0, 'deg')
-        self.mounting_level = mounting_level or Q(0, 'm')
+        self.ground_tilt = ground_tilt
+        self.mounting_level = mounting_level
 
         self.fluidvol_total = fluidvol_total
         self.rho_ground = rho_ground
         self.rho_colbackside = rho_colbackside
-        self.rho_colsurface = rho_colsurface or Q(0)
-        self.max_aoi_shadow = max_aoi_shadow or Q(80, 'deg')
+        self.rho_colsurface = rho_colsurface
+        self.max_aoi_shadow = max_aoi_shadow
         self.min_elevation_shadow = min_elevation_shadow
         self.plant = plant
-        self.sensor_map = sensor_map or {}
-        self.set_sensors(**kwargs)
+        self.sensor_map = sensor_map
+        if len(kwargs) > 0:
+            self.set_sensors(**kwargs)
+
+        self.defer_post_config_changed_actions = False
 
     def calc_area_gr_from_collector(self):
-        """Set array.area_gr from area_ap and collector information, if None
+        """Set array.area_gr from area_ap and collector_type information, if None
         """
-        coll = self.collector
-        if coll is None or isinstance(coll, UninitialisedCollector):
+        coll = self.collector_type
+        if (self.area_ap is None) or (coll is None) or isinstance(coll, UninitialisedCollectorType):
             return None
-        if coll.area_ap is None:
+        if (coll.area_gr is None) or (coll.area_ap is None):
             return None
         return self.area_ap * (coll.area_gr / coll.area_ap)
 
-    # @property
-    # def radiation_input_slots(self):
-    #     return self.in_global, self.in_beam, self.in_diffuse, self.in_dni
+    @property
+    def radiation_input_slots(self):
+        return self.in_global, self.in_beam, self.in_diffuse, self.in_dni
 
     @property
-    def collector(self):
-        return self._collector
-
-    @collector.setter
-    def collector(self, value):
-        if isinstance(value, Collector):
-            self._collector = value
+    def collector_type(self):
+        return self._collector_type
+
+    @collector_type.setter
+    def collector_type(self, value):
+        if isinstance(value, CollectorType):
+            self._collector_type = value
         elif isinstance(value, str) and sqlalchemy.orm.object_session(self) is not None:
-            _convert_to_concrete_coll(sqlalchemy.orm.object_session(self), self, 'collector',
-                                      UninitialisedCollector(value, parent=self, attribute='collector'))
+            _convert_to_concrete_col_type(sqlalchemy.orm.object_session(self), self, 'collector_type',
+                                          UninitialisedCollectorType(value, parent=self, attribute='collector_type'))
         elif isinstance(value, str):
-            self._collector = UninitialisedCollector(value, parent=self, attribute='collector')
+            self._collector_type = UninitialisedCollectorType(value, parent=self, attribute='collector_type')
         elif value is None:
-            self._collector = None
+            self._collector_type = None
         else:
-            raise err.ConfigurationError("Collector must be a Collector object, or the name of an existing collector.")
+            raise ConfigurationError("collector_type must be a CollectorType object, or the name of an existing "
+                                     "collector_type")
 
     @property
     def fluid_solar(self):
         return self.plant.fluid_solar
 
     @property
     def orientation(self):
@@ -892,107 +831,112 @@
 
     def has_orientation(self):
         """Returns True if array has tilt and azimuth well-defined. Useful for radiation calculations.
         """
         return (self.tilt is not None) and (self.azim is not None)
 
 
-# Commented, because currently not used. Database table still exists, not removed by Alembic.
-# class HeatExchanger(Component):
-#     """
-#     Implements a heat exchangers including references to its hot- and cold-side fluids.
-#
-#     Attributes
-#     ----------
-#     plant : Plant object
-#         Plant to which the heat exchanger belongs.
-#     fluid_hot : Sensor, optional
-#         Fluid on the hot side of the heat exchanger (often an antifreeze, in a solar thermal plant).
-#     fluid_cold : Sensor, optional
-#         Fluid on the cold side of the heat exchanger (often water).
-#     ua_nom : pint Quantity, optional
-#         Nominal heat transfer coefficient.
-#     """
-#     __tablename__ = 'heat_exchangers'
-#
-#     __mapper_args__ = {
-#         "polymorphic_identity": "heat_exchanger"
-#     }
-#
-#     id = Column(Integer, ForeignKey('components.component_id'), primary_key=True)
-#
-#     plant_id = Column(Integer, ForeignKey('plant.id', ondelete="CASCADE"))
-#     plant = relationship("Plant", foreign_keys=[plant_id],
-#                          backref=backref("heat_exchangers", cascade="all, delete-orphan"))
-#     fluid_hot_id = Column(Integer, ForeignKey('fluids.id'))
-#     fluid_hot = relationship("Fluid", foreign_keys=[fluid_hot_id],
-#                              cascade="all, delete", uselist=False, passive_deletes=True)
-#     fluid_cold_id = Column(Integer, ForeignKey('fluids.id'))
-#     fluid_cold = relationship("Fluid", foreign_keys=[fluid_cold_id],
-#                               cascade="all, delete", uselist=False, passive_deletes=True)
-#     name = Column(String)
-#
-#     ua_nom = ComponentParam('kW K**-1', 0, np.Inf)
-#
-#     def __init__(self, name, plant, fluid_hot=None, fluid_cold=None, ua_nom=None):
-#         self.defer_post_config_changed_actions = True
-#         self.name = name
-#         self.plant = plant
-#         self.fluid_hot = fluid_hot
-#         self.fluid_cold = fluid_cold
-#         self.ud_nom = ua_nom
-#         self.defer_post_config_changed_actions = False
+class HeatExchanger(Component):
+    """
+    Implements a heat exchangers including references to its hot- and cold-side fluids.
+
+    Attributes
+    ----------
+    plant : Plant object
+        Plant to which the heat exchanger belongs.
+    fluid_hot : Sensor, optional
+        Fluid on the hot side of the heat exchanger (often an antifreeze, in a solar thermal plant).
+    fluid_cold : Sensor, optional
+        Fluid on the cold side of the heat exchanger (often water).
+    ua_nom : pint Quantity, optional
+        Nominal heat transfer coefficient.
+    """
+    __tablename__ = 'heat_exchangers'
+
+    __mapper_args__ = {
+        "polymorphic_identity": "heat_exchanger"
+    }
+
+    id = Column(Integer, ForeignKey('components.component_id'), primary_key=True)
+
+    plant_id = Column(Integer, ForeignKey('plant.id', ondelete="CASCADE"))
+    plant = relationship("Plant", foreign_keys=[plant_id],
+                         backref=backref("heat_exchangers", cascade="all, delete-orphan"))
+    fluid_hot_id = Column(Integer, ForeignKey('fluids.id'))
+    fluid_hot = relationship("Fluid", foreign_keys=[fluid_hot_id],
+                             cascade="all, delete", uselist=False, passive_deletes=True)
+    fluid_cold_id = Column(Integer, ForeignKey('fluids.id'))
+    fluid_cold = relationship("Fluid", foreign_keys=[fluid_cold_id],
+                              cascade="all, delete", uselist=False, passive_deletes=True)
+    name = Column(String)
+
+    ua_nom = ComponentParam('kW K**-1', 0, np.Inf)
+
+    def __init__(self, name, plant, fluid_hot=None, fluid_cold=None, ua_nom=None):
+        self.defer_post_config_changed_actions = True
+        self.name = name
+        self.plant = plant
+        self.fluid_hot = fluid_hot
+        self.fluid_cold = fluid_cold
+        self.ud_nom = ua_nom
+        self.defer_post_config_changed_actions = False
+
+
+# class RawData(ORMBase):
+#     ts = Column(DateTime(timezone=True))
+#     raw_name = Column(String)
 
 
-def _check_duplicate_coll_defs(session, inst):
+def _check_duplicate_col_type_defs(session, inst):
     try:
-        db_coll = session.query(Collector).filter(Collector.name == inst.name).one()
-        if inst == db_coll:
+        db_col_type = session.query(CollectorType).filter(CollectorType.name == inst.name).one()
+        if inst == db_col_type:
             return True
         else:
-            raise err.DuplicateNameError(
-                f"Attempting to create a Collector called {inst.name}, however a Collector "
-                f"with this name already exists, but with different attributes.")
+            raise DuplicateNameError(
+                f"Attempting to create a CollectorType called {inst.name}, however a CollectorType "
+                f"with this name already exists, but with different attributes")
     except sqlalchemy.exc.NoResultFound:
         return False
 
 
-def _convert_to_concrete_coll(session, inst, attribute, val):
+def _convert_to_concrete_col_type(session, inst, attribute, val):
     with session.no_autoflush:
-        db_coll = session.query(Collector).filter(Collector.name == val.name).one()
-        setattr(inst, attribute, db_coll)
+        db_col_type = session.query(CollectorType).filter(CollectorType.name == val.name).one()
+        setattr(inst, attribute, db_col_type)
 
 
 def _convert_to_concrete_fluid(session, inst, attribute, val):
     with session.no_autoflush:
         fluid_def = FluidDefinition.get_definition(val.fluid_def_name, session)
         kwargs = val.stored_args
         kwargs['fluid'] = fluid_def
         fluid = FluidFactory(**kwargs)
         setattr(inst, attribute, fluid)
 
 
 @sqlalchemy.event.listens_for(Session, "transient_to_pending")
-def convert_to_concrete_components(session, inst):
+def _convert_to_concrete_components(session, inst):
     if isinstance(inst, Component):
         with session.no_autoflush:
+            # attrs = copy.copy(inst.__dict__)
             fluids = {attr: val for attr, val in inst.__dict__.items() if isinstance(val, UninitialisedFluid)}
-            u_cols = {attr: val for attr, val in inst.__dict__.items() if isinstance(val, UninitialisedCollector)}
-            collectors = {attr: val for attr, val in inst.__dict__.items()
-                          if isinstance(val, Collector) and not isinstance(val, UninitialisedCollector)}
+            u_cols = {attr: val for attr, val in inst.__dict__.items() if isinstance(val, UninitialisedCollectorType)}
+            col_types = {attr: val for attr, val in inst.__dict__.items()
+                         if isinstance(val, CollectorType) and not isinstance(val, UninitialisedCollectorType)}
             for attribute, val in fluids.items():
                 if val in session:
                     session.expunge(val)
                 _convert_to_concrete_fluid(session, inst, attribute, val)
             for attribute, val in u_cols.items():
                 if val in session:
                     session.expunge(val)
-                _convert_to_concrete_coll(session, inst, attribute, val)
+                _convert_to_concrete_col_type(session, inst, attribute, val)
                 # Check for duplicate definitions
-            for attribute, val in collectors.items():
-                _check_duplicate_coll_defs(session, val)
+            for attribute, val in col_types.items():
+                _check_duplicate_col_type_defs(session, val)
 
 
 @sqlalchemy.event.listens_for(Session, "before_commit")
 def _update_before_commit(session):
     for inst in session.dirty:
-        convert_to_concrete_components(session, inst)
+        _convert_to_concrete_components(session, inst)
```

## sunpeek/components/sensor.py

```diff
@@ -154,62 +154,38 @@
         if self._native_unit is not None and self._native_unit != 'None':
             return getattr(units, self._native_unit)
         elif self.sensor_type is not None:
             return getattr(units, self.sensor_type.compatible_unit_str)
         else:
             return None
 
-    @staticmethod
-    def _validate_unit(native_unit, sensor_type_unit):
-        if (sensor_type_unit is not None) and (native_unit is not None):
-            try:
-                uu.assert_compatible(sensor_type_unit, native_unit)
-            except AssertionError:
-                raise ValueError(f'Cannot set sensor native_unit: "{str(native_unit)}" '
-                                 f'is not compatible with sensor_type unit "{sensor_type_unit}".')
-        return
-
-    @staticmethod
-    def _get_unit(native_unit_str):
-        if native_unit_str is None:
-            return None
-        try:
-            native_unit = units[native_unit_str]
-        except uu.pint.errors.UndefinedUnitError:
-            raise uu.pint.errors.UndefinedUnitError(
-                "'sensor_native_unit' must be a valid python pint library unit string")
-        return native_unit
-
     @native_unit.setter
     def native_unit(self, native_unit_str):
         sensor_type_unit = None
-        if (self.sensor_type is not None) and (self.sensor_type.compatible_unit_str is not None):
+        if self.sensor_type is not None and self.sensor_type.compatible_unit_str is not None:
             sensor_type_unit = self.sensor_type.compatible_unit_str
 
-        native_unit = self._get_unit(native_unit_str=native_unit_str)
-        if (self.is_virtual) and (native_unit_str is None):
-            # If sensor is virtual, it is required that the unit is inferred based on the sensor type
+        # None unit: get from sensor_type. Relevant for virtual sensors
+        if native_unit_str is None:
+            if sensor_type_unit is None:
+                pass
             native_unit = sensor_type_unit
-
-        # validate if unit makes sense together with sensor_type
-        self._validate_unit(native_unit=native_unit, sensor_type_unit=sensor_type_unit)
-
+        else:
+            # Check unit exists
+            try:
+                native_unit = units[native_unit_str]
+            except uu.pint.errors.UndefinedUnitError:
+                raise uu.pint.errors.UndefinedUnitError(
+                    "'sensor_native_unit' must be a valid python pint library unit string")
+            # Make sure native_unit is compatible with SensorType unit
+            if sensor_type_unit is not None:
+                uu.assert_compatible(sensor_type_unit, native_unit)
         # normalise unit name, store as string for DB compatibility
         self._native_unit = str(native_unit)
 
-        # This is because data are stored as numbers-only (without unit) in parquet, so changing native_unit
-        # defines a new interpretation of the numeric data. This would leave to an inconsistency with the cache
-        # because the cache holds unit-aware data (thus immutable tho this native_unit change).
-        # The cache is re-filled with data, interpreting data in the new native_unit, next time self.data is queried.
-        self.reset_cache()
-
-    def reset_cache(self):
-        if self.plant is not None:
-            self.plant.reset_cache()
-
     @property
     def problems(self):
         # self.problems is not persisted in the database, so if self._problems is None, on a virtual sensor,
         # config_virtuals() is called on the plant which sets self.problems for all virtual sensors.
         if self._problems is None and self.is_virtual:
             if self.plant is not None:
                 [func(self.plant) for func in self.plant.post_config_changed_callbacks]
@@ -286,18 +262,14 @@
         if val is None:
             return
         if isinstance(val, str):
             try:
                 val = getattr(st, val)
             except AttributeError:
                 raise ConfigurationError(f'{val} is not a known sensor type')
-
-        if (self.is_mapped) and (self.sensor_type != val):
-            raise ConfigurationError('Cannot change the "sensor_type" of a mapped sensor. '
-                                     'Mapped sensors get their sensor type from the component slot they are mapped to.')
         s_type_def = getattr(st, val.name, False)
         wrn_txt = (f'Custom sensor types cannot be stored. If you are using a database to save configuration, '
                    f'this sensor type will be lost on reload.')
         if not s_type_def:
             warnings.warn(f'Setting custom sensor type {val.name}. ' + wrn_txt)
             self._sensor_type = val
         elif s_type_def and val != s_type_def:
@@ -320,20 +292,15 @@
         Notes
         -----
         Does not change data time index, e.g. values are not resampled. To resample, use sensor.get_data()
         """
         if self.plant is None:
             raise ConfigurationError('Cannot access sensor.data because the sensor does not have a plant associated.')
 
-        try:
-            sensor_data = self.plant.context.get_sensor_data(sensor=self)
-        except FileNotFoundError:   # returned by parquet-datastore-utils if data folder doesn't exist
-            sensor_data = None
-
-        return sensor_data
+        return self.plant.context.get_sensor_data(sensor=self)
 
     @property
     def is_mapped(self):
         if len(self.mappings) > 0:
             return True
         return False
 
@@ -365,15 +332,16 @@
 
         return reps_native
 
     @value_replacements.setter
     def value_replacements(self, val):
         # assert isinstance(val, dict), 'Value replacement expected to be a dict.'
         # Changing value_replacements potentially affects cleaned value
-        self.reset_cache()
+        if self.plant is not None:
+            self.plant.context.reset_cache()
 
         def check_tuple(s, d, key):
             if len(d[key]) != 3:
                 raise ValueError(
                     'To define a sensor replacement interval, pass a dictionary with a 3-value tuple representing the '
                     'left and right interval boundaries and the replacement value.')
             left, right, replace = d[key]
@@ -468,44 +436,44 @@
 
         if (algo_result.output is not None) and (result_key not in algo_result.output):
             raise CalculationError(
                 f'Trying to update data for virtual sensors, but required key "{result_key}" not found in algo result.')
 
         if (algo_result.output is None) or (algo_result.output[result_key] is None):
             # Save report of tried but not successful strategies.
-            self.problems = algo_result.feedback
+            self.problems = algo_result.problems
             if self.is_virtual:
                 # No strategy succeeded, Context will store an all-NaN series for the vsensor.
                 sp_logger.debug(f'Sensor.update(): virtual sensor algo output is None. Storing with sensor.')
-                self.plant.context.store_virtual_data(self, None)
+                self.plant.context.store_data(self, None)
             else:
                 sp_logger.debug(
                     f'Sensor.update(): Got called on a real sensor with virtual sensor algo output None. '
                     f'Will keep using the unchanged real sensor data.')
             return
         if not self.is_virtual:
             raise CalculationError(
                 'Can only update data for virtual sensors, but got called on a real sensor with non-None output.')
 
         # Save report of tried but not successful strategies.
-        self.problems = algo_result.feedback
+        self.problems = algo_result.problems
 
         # Save calculation results to virtual sensor
         data = algo_result.output[result_key]
         try:
             data.pint
         except AttributeError:
             raise CalculationError(
                 'Calculated virtual sensor data expected to be pd.Series with dtype pint.')
         if not self.native_unit.is_compatible_with(data.pint.units):
             raise CalculationError(
                 f'Calculated virtual sensor data not compatible with expected unit {self.native_unit}.')
         # Trying to minimize the changes made to sensor object: don't overwrite sensor native_unit
         data = data.pint.to(self.native_unit)
-        self.plant.context.store_virtual_data(self, data)
+        self.plant.context.store_data(self, data)
 
         return
 
     def remove_references(self, include_plant: bool = True):
         """This makes sure SunPeek does not use the sensor anymore, and leaves the application in a consistent stage.
         It does not do a database delete of the sensor.
 
@@ -529,29 +497,14 @@
 
     def is_info_missing(self, info_item: str) -> bool:
         """Make sure sensor info has an item with the specified key.
         """
         return (self.info is None) or (info_item not in self.info._info)
 
     @property
-    def is_infos_set(self):
-        if (self.sensor_type is None):
-            return True
-
-        required_infos = self.sensor_type.info_checks
-        for required_info in required_infos.keys():
-            if self.is_info_missing(required_info):
-                return False
-        return True
-
-    @is_infos_set.setter
-    def is_infos_set(self, val):
-        pass
-
-    @property
     def orientation(self):
         """Return dictionary with array's "tilt" and "azim" values converted to deg, for radiation calculations.
         """
         return {'tilt': self.info['tilt'].m_as('deg'),
                 'azim': self.info['azim'].m_as('deg')}
 
     def has_orientation(self):
```

## sunpeek/components/sensor_types.py

```diff
@@ -1,138 +1,137 @@
-"""
-This module contains a the definitions of all SensorTypes which are used to provide unit compatibility and data sanity
-checks, depending on what a sensor does. Normally Sensors inherit their SensorType from any SensorSlots they are mapped
-into on a component, but sensors can also be created with a type explicitly.
-"""
-
 import sys
 from sunpeek.common.unit_uncertainty import Q
 from sunpeek.common import common_units
 from sunpeek.components.types import SensorType
 
 # Some arguments to SensorType() are commented since uncertainty propagation is not implemented right now. Left
 # commented for later, just in case.
 
+'''
+This module contains a the definitions of all SensorTypes which are used to provide unit compatibility and data sanity 
+checks, depending on what a sensor does. Normally Sensors inherit their SensorType from any SensorSlots they are mapped 
+into on a component, but sensors can also be created with a type explicitly.
+'''
 
 fluid_temperature = SensorType(name='fluid_temperature',
                                description='Fluid temperature',
-                               compatible_unit_str='C',
+                               compatible_unit_str='degC',
                                lower_replace_min=Q(-20.0, 'degC'),
                                upper_replace_max=Q(200.0, 'degC'),
                                info_checks=None,
                                common_units=list(common_units.temperature),
                                )
 
 ambient_temperature = SensorType(name='ambient_temperature',
                                  description='Ambient temperature',
-                                 compatible_unit_str='C',
+                                 compatible_unit_str='degC',
                                  lower_replace_min=Q(-30.0, 'degC'),
                                  upper_replace_max=Q(60.0, 'degC'),
                                  info_checks=None,
                                  common_units=list(common_units.temperature),
                                  )
 
 global_radiation = SensorType(name='global_radiation',
                               description='Global irradiance',
-                              compatible_unit_str='W/m',
+                              compatible_unit_str='watt / meter**2',
                               lower_replace_min=Q(-10.0, 'watt / meter**2'),
                               lower_replace_max=Q(0.0, 'watt / meter**2'),
                               lower_replace_value=Q(0.0, 'watt / meter**2'),
                               upper_replace_max=Q(1700.0, 'watt / meter**2'),
-                              info_checks={'tilt': {'unit': '', 'minimum': 0, 'maximum': 90,
+                              info_checks={'tilt': {'unit': 'deg', 'minimum': 0, 'maximum': 90,
                                                     'description': 'Radiation sensor tilt angle.'},
-                                           'azim': {'unit': '', 'minimum': 0, 'maximum': 360,
+                                           'azim': {'unit': 'deg', 'minimum': 0, 'maximum': 360,
                                                     'description': 'Radiation sensor azimuth angle.'}},
                               common_units=list(common_units.power / common_units.area),
                               )
 
 direct_radiation = SensorType(name='direct_radiation',
                               description='Direct / beam irradiance',
-                              compatible_unit_str='W/m',
+                              compatible_unit_str='watt / meter**2',
                               lower_replace_min=Q(-10.0, 'watt / meter**2'),
                               lower_replace_max=Q(0.0, 'watt / meter**2'),
                               lower_replace_value=Q(0.0, 'watt / meter**2'),
                               upper_replace_max=Q(1400.0, 'watt / meter**2'),
-                              info_checks={'tilt': {'unit': '', 'minimum': 0, 'maximum': 90,
+                              info_checks={'tilt': {'unit': 'deg', 'minimum': 0, 'maximum': 90,
                                                     'description': 'Radiation sensor tilt angle.'},
-                                           'azim': {'unit': '', 'minimum': 0, 'maximum': 360,
+                                           'azim': {'unit': 'deg', 'minimum': 0, 'maximum': 360,
                                                     'description': 'Radiation sensor azimuth angle.'}},
                               common_units=list(common_units.power / common_units.area),
                               )
 
 diffuse_radiation = SensorType(name='diffuse_radiation',
                                description='Diffuse irradiance',
-                               compatible_unit_str='W/m',
+                               compatible_unit_str='watt / meter**2',
                                lower_replace_min=Q(-10.0, 'watt / meter**2'),
                                lower_replace_max=Q(0.0, 'watt / meter**2'),
                                lower_replace_value=Q(0.0, 'watt / meter**2'),
                                upper_replace_max=Q(1100.0, 'watt / meter**2'),
-                               info_checks={'tilt': {'unit': '', 'minimum': 0, 'maximum': 90,
+                               info_checks={'tilt': {'unit': 'deg', 'minimum': 0, 'maximum': 90,
                                                      'description': 'Radiation sensor tilt angle.'},
-                                            'azim': {'unit': '', 'minimum': 0, 'maximum': 360,
+                                            'azim': {'unit': 'deg', 'minimum': 0, 'maximum': 360,
                                                      'description': 'Radiation sensor azimuth angle.'}},
                                common_units=list(common_units.power / common_units.area),
                                )
 
 dni_radiation = SensorType(name='dni_radiation',
                            description='DNI (direct normal) irradiance',
-                           compatible_unit_str='W/m',
+                           compatible_unit_str='watt / meter**2',
                            lower_replace_min=Q(-10.0, 'watt / meter**2'),
                            lower_replace_max=Q(0.0, 'watt / meter**2'),
                            lower_replace_value=Q(0.0, 'watt / meter**2'),
                            upper_replace_max=Q(1400.0, 'watt / meter**2'),
                            info_checks=None,
                            common_units=list(common_units.power / common_units.area),
                            )
 
 thermal_power = SensorType(name='thermal_power',
                            description='Thermal power',
-                           compatible_unit_str='W',
+                           compatible_unit_str='watt',
                            lower_replace_min=Q(-10.0, 'watt'),
                            lower_replace_max=Q(0.0, 'watt'),
                            lower_replace_value=Q(0.0, 'watt'),
                            info_checks=None,
                            common_units=list(common_units.power)
                            )
 
 mass_flow = SensorType(name='mass_flow',
                        description='Mass flow',
-                       compatible_unit_str='kg/s',
+                       compatible_unit_str='kilogram / second',
                        lower_replace_min=Q(-100.0, 'kilogram / second'),
                        lower_replace_max=Q(0.0, 'kilogram / second'),
                        lower_replace_value=Q(0.0, 'kilogram / second'),
                        info_checks=None,
                        common_units=list(common_units.mass / common_units.time),
                        )
 
 volume_flow = SensorType(name='volume_flow',
                          description='Volume flow',
-                         compatible_unit_str='m/s',
+                         compatible_unit_str='meter ** 3 / second',
                          lower_replace_min=Q(-0.1, 'meter ** 3 / second'),
                          lower_replace_max=Q(0.0, 'meter ** 3 / second'),
                          lower_replace_value=Q(0.0, 'meter ** 3 / second'),
                          info_checks={'position': {'unit': '', 'minimum': 0, 'maximum': 1,
                                                    'description': 'Position of the volume flow sensor, near te_in '
                                                                   '(0), near te_out (1) or in between (0..1).'}},
                          common_units=list(common_units.volume / common_units.time),
                          )
 
 wind_speed = SensorType(name='wind_speed',
                         description='Wind speed',
-                        compatible_unit_str='m/s',
+                        compatible_unit_str='meter / second',
                         lower_replace_min=Q(-1.0, 'meter / second'),
                         lower_replace_max=Q(0.0, 'meter / second'),
                         lower_replace_value=Q(0.0, 'meter / second'),
                         info_checks=None,
                         common_units=list(common_units.length / common_units.time),
                         )
 
 temperature_derivative = SensorType(name='temperature_derivative',
                                     description='Temperature derivative',
-                                    compatible_unit_str='K/s',
+                                    compatible_unit_str='kelvin / second',
                                     lower_replace_min=Q(-100.0, 'kelvin / second'),
                                     upper_replace_max=Q(100.0, 'kelvin / second'),
                                     info_checks=None,
                                     common_units=list(common_units.temperature / common_units.time),
                                     )
 
 pressure = SensorType(name='pressure',
@@ -145,72 +144,68 @@
                       common_units=list(common_units.pressure),
                       )
 
 bool = SensorType(name='bool',
                   description='Boolean',
                   compatible_unit_str='dimensionless',
                   info_checks=None,
-                  common_units=list(common_units.bool)
                   )
 
 float = SensorType(name='float',
                    description='Float',
                    compatible_unit_str='dimensionless',
                    info_checks=None,
-                   common_units=list(common_units.float)
                    )
 
 float_0_1 = SensorType(name='float_0_1',
                        description='Float between 0 and 1',
                        compatible_unit_str='dimensionless',
                        lower_replace_min=Q(0.0),
                        upper_replace_max=Q(1.0),
                        info_checks=None,
-                       common_units=list(common_units.float)
                        )
 
 float_0_100 = SensorType(name='float_0_100',
                          description='Float between 0 and 100',
                          compatible_unit_str='dimensionless',
                          lower_replace_min=Q(0.0),
                          upper_replace_max=Q(100.0),
                          info_checks=None,
-                         common_units=list(common_units.float)
                          )
 
 angle_0_180 = SensorType(name='angle_0_180',
                          description='Angle between 0 and 180',
-                         compatible_unit_str='',
+                         compatible_unit_str='degree',
                          lower_replace_min=Q(0.0, 'degree'),
                          upper_replace_max=Q(180.0, 'degree'),
                          info_checks=None,
                          common_units=list(common_units.angle),
                          )
 
 angle_0_360 = SensorType(name='angle_0_360',
                          description='Angle between 0 and 360',
-                         compatible_unit_str='',
+                         compatible_unit_str='degree',
                          lower_replace_min=Q(0.0, 'degree'),
                          upper_replace_max=Q(360.0, 'degree'),
                          info_checks=None,
                          common_units=list(common_units.angle),
                          )
 
 angle_0_90 = SensorType(name='angle_0_90',
                         description='Angle between 0 and 90',
-                        compatible_unit_str='',
+                        compatible_unit_str='degree',
                         lower_replace_min=Q(0.0, 'degree'),
                         upper_replace_max=Q(90.0, 'degree'),
                         info_checks=None,
                         common_units=list(common_units.angle),
                         )
 
 angle__90_90 = SensorType(name='angle__90_90',
                           description='Angle between -90 and 90',
-                          compatible_unit_str='',
+                          compatible_unit_str='degree',
                           lower_replace_min=Q(-90.0, 'degree'),
                           upper_replace_max=Q(90.0, 'degree'),
                           info_checks=None,
                           common_units=list(common_units.angle),
                           )
 
 all_sensor_types = [x for x in sys.modules[__name__].__dict__.values() if isinstance(x, SensorType)]
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## sunpeek/components/types.py

```diff
@@ -1,49 +1,40 @@
 import numpy as np
 import enum
-
-import pint
 import sqlalchemy
 from sqlalchemy.orm import relationship
 from sqlalchemy import Column, String, Integer, DateTime, Enum, Identity, JSON
 from sqlalchemy import inspect
-from typing import Union, Tuple
-import datetime as dt
+from typing import Union
+import datetime
 import copy
 import dataclasses
 
 from sunpeek.components import iam_methods
 from sunpeek.components.iam_methods import IAM_Method
 from sunpeek.common import unit_uncertainty as uu
 from sunpeek.common.unit_uncertainty import Q
 from sunpeek.common.errors import CollectorDefinitionError
 from sunpeek.components.helpers import ORMBase, AttrSetterMixin, ComponentParam
-from sunpeek.base_model import BaseModel, Quantity
-
-
-class ApertureParameters(BaseModel):
-    a1: Quantity | None
-    a2: Quantity | None
-    a5: Quantity | None
-    a8: Quantity | None
 
 
 @dataclasses.dataclass
 class SensorType:
     name: str
     compatible_unit_str: str
     description: str
     lower_replace_min: Union[Q, None] = None
     lower_replace_max: Union[Q, None] = None
     lower_replace_value: Union[Q, None] = None
     upper_replace_min: Union[Q, None] = None
     upper_replace_max: Union[Q, None] = None
     upper_replace_value: Union[Q, None] = None
-    max_fill_period: Union[dt.timedelta, None] = None
-    sensor_hangs_period: Union[dt.timedelta, None] = None
+    equation: Union[str, None] = None
+    max_fill_period: Union[datetime.timedelta, None] = None
+    sensor_hangs_period: Union[datetime.timedelta, None] = None
     info_checks: Union[dict, None] = None
     common_units: Union[list, None] = None
 
     @property
     def info_checks(self):
         if getattr(self, '_info_checks', None) is not None:
             return self._info_checks
@@ -51,54 +42,38 @@
             return {}
 
     @info_checks.setter
     def info_checks(self, val):
         self._info_checks = val
 
 
-class CollectorTypes(str, enum.Enum):
-    flat_plate = "flat_plate"
-    concentrating = "concentrating"
-    WISC = "WISC"  # Wind and Infrared Sensitive Collector
-
-
-class CollectorTestTypes(str, enum.Enum):
-    SST = "SST"  # Steady-State Test
-    QDT = "QDT"  # Quasi-Dynamic Test
-
-
-class CollectorReferenceAreaTypes(str, enum.Enum):
-    gross = "gross"
-    aperture = "aperture"
-
-
-class Collector(AttrSetterMixin, ORMBase):
+class CollectorType(AttrSetterMixin, ORMBase):
     """
     Implements a specific collector (product of some manufacturer), including all performance data acc. to data sheet.
 
     Stores two different collector type parameters, referring to the two test procedures defined in `ISO 9806`_,
     either quasi-dynamic or steady-state test. The type of test procedure is available from the standard collector
     data sheet / Solar Keymark certificate and must be specified in `test_type`.
 
     Test parameters may refer to either gross or aperture area. This must be specified in `test_reference_area`. The
-    collector parameters stored in Collector _always_ refer to gross area.
+    collector parameters stored in CollectorType _always_ refer to gross area.
 
     IAM (incidence angle modifier) information may be given as an instance of the IAM_Method class. This holds
     several implementations where the IAM information can be given in either of these ways:
     - If only IAM information at an aoi of 50 degrees is given, use `IAM_K50(k50)`. Internally, this uses the ASHRAE equation.
     - To use the ASHRAE IAM equation with a known parameter `b`, use `IAM_ASHRAE(b)`.
     - To use the Ambrosetti IAM equation with a known parameter `kappa`, use `IAM_Ambrosetti(kappa)`.
     - To use an IAM with given / known IAM values at given aoi angles, use `IAM_Interpolated()`. This requires a list
     of reference aoi's, and either a) 1 list of IAM values or b) 2 lists with transversal and longitudinal IAM
     values.
 
     Attributes
     ----------
     name : str
-        Name of collector type. Must be unique within HarvestIT 'collector' database.
+        Name of collector type. Must be unique within HarvestIT 'collector_types' database.
     manufacturer_name : str, optional
         Manufacturer name. Example: "GREENoneTEC Solarindustrie GmbH"
     product_name : str, optional
         Product name. Example: "GK 3133"
 
     licence_number : str, optional
         Licence number (often also known as Registration number) of the Solar Keymark certificate.
@@ -106,17 +81,15 @@
         "Test Report(s)" field on Solar Keymark certificate.
     certificate_date_issued : datetime, optional
         "Date issued" field on Solar Keymark certificate.
     certificate_lab : str, optional
         Laboratory / testing institution that issued the collector test certificate.
     certificate_details : str, optional
         Details concerning the official collector test / Solar Keymark certificate, such as testing institution etc.
-    collector_type : CollectorTypes or str
-        Construction type of the collector, as defined in Solar Keymark / ISO 9806.
-        Main distinction is between flat plate and concentrating collectors.
+
     test_type : str
         Type of collector test, according to `ISO 9806`_. Valid values: 'QDT' | 'dynamic' | 'SST' | 'static'
     test_reference_area : str
         Collector area to which the test data refer. Valid values: 'area_ap | 'aperture' | 'area_gr' | 'gross'.
     area_gr : pint Quantity, optional
         Gross collector area. Mandatory if `test_reference_area`=='aperture', optional otherwise.
     area_ap : pint Quantity, optional
@@ -135,360 +108,318 @@
         or steady state test.
     a2 : pint Quantity
         Quadratic heat loss coefficient, according to collector test data sheet of quasi dynamic
         or steady state test.
     a5 : pint Quantity
         Effective thermal heat capacity, according to collector test data sheet of quasi dynamic
         or steady state test.
-    a8 : pint Quantity
-        Radiative heat loss coefficient, according to collector test data sheet of quasi dynamic
-        or steady state test.
     kd : pint Quantity, optional
         Incidence angle modifier for diffuse radiation, according to collector test data sheet of quasi dynamic test.
-        Mandatory if `test_type`=='QDT'.
+        Mandatory if `test_type`=='dynamic'.
     eta0b : pint Quantity, optional
         Peak collector efficiency (= zero loss coefficient) based on beam irradiance, according
         to collector test data sheet of quasi dynamic test.
+        Mandatory if `test_type`=='dynamic'.
     eta0hem : pint Quantity, optional
         Peak collector efficiency (= zero loss coefficient) based on hemispherical irradiance,
         according to collector test data sheet of steady state test (or calculated from quasi-dynamic test).
+        Mandatory if `test_type`=='static'.
     f_prime : pint Quantity
         Collector efficiency factor, i.e. ratio of heat transfer resistances of absorber to ambient vs. fluid to ambient.
-    concentration_ratio : pint Quantity
-        Geometric concentration ratio: Factor by which solar irradiance is concentrated onto the collector's
-        absorbing surface.
-        When applying a ISO 24194 Thermal Power Check, the `concentration_ratio` is used to determine which of the
-        3 formulae defined in ISO 24194 to apply.
-    calculation_info : dictionary
-        Contains information about calculated collector parameters, where specific information was not given at
-        instantiation of the object, e.g. because the Solar Keymark data sheet does not include a specific parameter.
-        Some parameters can be calculated based on given ones, e.g. `Kd` (diffuse IAM) can be calculated based on
-        given IAM information. Dictionary keys are the names of calculated parameters (e.g. `kd`), dictionary values
-        hold information concerning specific calculation details (e.g. calculation method).
+    calculated_parameters : dictionary
+        Contains information about calculated collector parameters, where specific information was not given at 
+        instantiation of the object, e.g. because the Solar Keymark data sheet does not include a specific parameter. 
+        Some parameters can be calculated based on given ones, e.g. `Kd` (diffuse IAM) can be calculated based on 
+        given IAM information. Dictionary keys are the names of calculated parameters (e.g. `kd`), dictionary values 
+        hold information concerning specific calculation details (e.g. calculation method).    
     plant : None
         Not used, included for compatibility with other component types.
 
     .. _ISO 9806:
         https://www.iso.org/standard/67978.html
     .. _ASHRAE model:
         https://pvlib-python.readthedocs.io/en/stable/generated/pvlib.iam.ashrae.html
     """
-    __tablename__ = 'collectors'
+    __tablename__ = 'collector_types'
+
+    class t_types(str, enum.Enum):
+        SST = "SST"
+        static = "static"
+        QDT = "QDT"
+        dynamic = "dynamic"
+
+    class ref_a_types(str, enum.Enum):
+        area_gr = "area_gr"
+        gross = "gross"
+        area_ap = "area_ap"
+        aperture = "aperture"
 
-    # Name is the PK, and is required whenever a DB store is in use.
-    id = Column(Integer, Identity(), primary_key=True)
+    VALID_TEST_TYPES = [el.name for el in t_types]
+    VALID_AREA_TYPES = [el.name for el in ref_a_types]
+
+    id = Column(Integer, Identity(0), primary_key=True)
+    # the name is optional for using it in Python, but required if the collector Type is to be stored in the database.
     name = Column(String, unique=True, nullable=False)
     manufacturer_name = Column(String)
     product_name = Column(String)
     licence_number = Column(String)
     test_report_id = Column(String)
     certificate_date_issued = Column(DateTime)
     certificate_lab = Column(String)
     certificate_details = Column(String)
-    collector_type = Column(Enum(CollectorTypes), nullable=False)
-
-    test_type = Column(Enum(CollectorTestTypes))
-    test_reference_area = Column(Enum(CollectorReferenceAreaTypes))
-    calculation_info = Column(JSON)
-    aperture_parameters = Column(JSON)
+    iam_method = relationship("IAM_Method", back_populates='collector_type', uselist=False, cascade="all, delete",
+                              passive_deletes=True)
+    test_type = Column(Enum(t_types))
+    test_reference_area = Column(Enum(ref_a_types))
+    calculated_parameters = Column(JSON)
 
     # No limit checks for these attributes to avoid code duplication with self._set_collector_parameters()
-    iam_method = relationship("IAM_Method", back_populates='collector', uselist=False, cascade="all, delete",
-                              passive_deletes=True)
     area_gr = ComponentParam('m**2', minimum=0.1)
     area_ap = ComponentParam('m**2', minimum=0.1)
     gross_length = ComponentParam('cm', minimum=0)
     gross_width = ComponentParam('cm', minimum=0)
     gross_height = ComponentParam('cm', minimum=0)
-    a1 = ComponentParam('W m**-2 K**-1', minimum=0, maximum=20)
-    a2 = ComponentParam('W m**-2 K**-2', minimum=0, maximum=1)
-    a5 = ComponentParam('J m**-2 K**-1', minimum=1, maximum=100000)
-    a8 = ComponentParam('W m**-2 K**-4', minimum=0, maximum=1)
-    kd = ComponentParam('', minimum=0, maximum=2)
-    eta0b = ComponentParam('', minimum=0, maximum=1)
-    eta0hem = ComponentParam('', minimum=0, maximum=1)
+    a1 = ComponentParam('W m**-2 K**-1')
+    a2 = ComponentParam('W m**-2 K**-2')
+    a5 = ComponentParam('J m**-2 K**-1')
+    kd = ComponentParam('')
+    eta0b = ComponentParam('')
+    eta0hem = ComponentParam('')
     f_prime = ComponentParam('', minimum=0, maximum=1)
-    concentration_ratio = ComponentParam('', minimum=1)
 
-    def __init__(self, test_reference_area, test_type, gross_length, collector_type: CollectorTypes,
-                 iam_method: IAM_Method = None, concentration_ratio=None,
+    def __init__(self, test_reference_area, test_type, gross_length, iam_method: IAM_Method = None,
                  name=None, manufacturer_name=None, product_name=None, test_report_id=None, licence_number=None,
                  certificate_date_issued=None, certificate_lab=None, certificate_details=None,
                  area_gr=None, area_ap=None, gross_width=None, gross_height=None,
-                 a1=None, a2=None, a5=None, a8=None, kd=None, eta0b=None, eta0hem=None, f_prime=None, **kwargs):
+                 a1=None, a2=None, a5=None, kd=None, eta0b=None, eta0hem=None, f_prime=None, plant=None):
 
         self.test_reference_area = self._infer_test_reference_area(test_reference_area)
         self.test_type = self._infer_test_type(test_type)
         self.iam_method = iam_method
 
         self.name = name
         self.manufacturer_name = manufacturer_name
         self.product_name = product_name
 
         self.licence_number = licence_number
         self.test_report_id = test_report_id
         self.certificate_date_issued = certificate_date_issued
         self.certificate_lab = certificate_lab
         self.certificate_details = certificate_details
-        self.collector_type = collector_type
-        self.concentration_ratio = concentration_ratio
+
         # self.description = description
 
+        if gross_length is None:
+            raise CollectorDefinitionError('Collector "gross_length" is None, but must be specified.')
         self.gross_length = gross_length
         self.gross_width = gross_width
         self.gross_height = gross_height
         self.f_prime = f_prime
 
-        self.area_gr = area_gr
-        self.area_ap = area_ap
-        self.a1 = a1
-        self.a2 = a2
-        self.a5 = a5
-        self.a8 = a8
-        self.kd = kd
-        self.eta0b = eta0b
-        self.eta0hem = eta0hem
-
-        self._aperture_parameters = {}
-        self.update_parameters()
-
-    def update_parameters(self) -> None:
-        """
-        Check and set collector parameters. Convert parameters from aperture to gross are, if ref area is aperture.
-
-        Raises
-        ------
-        CollectorDefinitionException
-            If definition of collector parameters is incomplete or contradictory.
-
-        Notes
-        -----
-        - This method checks that we have a complete and valid Collector definition, for both test_types 'SST' | 'QDT'.
-        - This method exists because parameters are interdependent, so setting collector parameters can't be done
-           per-attribute (in setter methods e.g.). Attributes that don't depend on others are set in __init__ directly.
-        - This methods sets the instance attributes, with reference to gross area "area_gr", if params is sane.
-        - Converts from aperture to gross area, if necessary. To do so, both "area_gr" and "area_ap" are required.
-        - Either "eta0b" or "eta0hem" must be provided in params
-        - For statically-tested collectors only: Estimates `kd` using calculate_kd_Hess_and_Hanby(), if necessary.
-        """
-        # Parse & store current object parameters -> might refer to aperture, may be overwritten
-        required_not_none = {'gross_length', 'area_gr', 'a1', 'a2', 'a5'}
-        for k in required_not_none:
-            if self.__getattribute__(k) is None:
-                raise CollectorDefinitionError(f'Collector parameter is None, but must be specified: "{k}".')
-
-        coll_params = {'a1', 'a2', 'a5', 'a8', 'kd', 'eta0b', 'eta0hem'}
-        p = {k: uu.parse_quantity(self.__getattribute__(k)) for k in coll_params}
-
-        if self.area_ap is not None and (self.area_ap > self.area_gr):
-            raise CollectorDefinitionError(f'Aperture area must be smaller than gross area. You provided '
-                                           f'"area_ap"={self.area_ap}, "area_gr"={self.area_gr}.')
-
-        if (p['eta0hem'] is None) and (p['eta0b'] is None):
-            raise CollectorDefinitionError('Either "eta0b" or "eta0hem" must be provided for a collector, '
-                                           'but both are missing.')
-
-        is_aperture = (self.test_reference_area == CollectorReferenceAreaTypes.aperture.value)
-        if is_aperture and self.area_ap is None:
-            raise CollectorDefinitionError(f'In a collector with test_reference_area "aperture", '
-                                           f'both gross and aperture collector areas must be given.')
-
-        is_qdt = (self.test_type == CollectorTestTypes.QDT.value)
-        if is_qdt and p['kd'] is None:
-            raise CollectorDefinitionError('For a collector with QDT collector test ("dynamically-tested"), '
-                                           '"kd" must be provided.')
-
-        # Calculate missing parameters, if needed and if possible  -------------------------
-        self.calculation_info = {}
-
-        if p['kd'] is None and not is_qdt:  # Is applied to SST collectors only, QDT must already have 'kd'
-            p['kd'], info = calculate_kd_Hess_and_Hanby(self.iam_method)
-            self.calculation_info['kd'] = info
-
-        if p['eta0hem'] is None:
-            p['eta0hem'], info = calculate_eta0hem(eta0b=p['eta0b'], kd=p['kd'])
-            self.calculation_info['eta0hem'] = info
-
-        if p['eta0b'] is None:
-            p['eta0b'], info = calculate_eta0b(eta0hem=p['eta0hem'], kd=p['kd'])
-            self.calculation_info['eta0b'] = info
-
-        if self.test_reference_area == CollectorReferenceAreaTypes.gross.value:
-            self.aperture_parameters = {}
-            self._write_attribs(p)
-            return  # Nothing to do, no area conversion needed. Otherwise, conversion aperture -> gross follows.
-
-        # Convert aperture -> gross  ------------------------
-
-        # Use values stored in self.aperture_parameters if possible
-        if not self.aperture_parameters:
-            # Store parameters with reference area "aperture"
-            self.aperture_parameters = {k: uu.to_dict(p[k]) for k in ['a1', 'a2', 'a5', 'a8']}
-
-        # Do the area conversion
-        conversion_factor = self.area_ap / self.area_gr
-        p_final = {k: None if v is None else conversion_factor * uu.parse_quantity(v)
-                   for k, v in self.aperture_parameters.items()}
-        p_final.update({k: p[k] for k in coll_params if k not in p_final})  # Update with non-converted params
-        self._write_attribs(p_final)
-
-        # Add calculation_info about area conversion
-        for k, v in self.aperture_parameters.items():
-            self.calculation_info.setdefault(k, '')
-            self.calculation_info[k] += 'Converted from aperture to gross area by SunPeek.'
-
-    def _write_attribs(self, p: dict) -> None:
-        """Set collector object attributes from dictionary. Dict values may be dicts, parsable by parse_quantity.
-        """
-        for k, v in p.items():
-            try:
-                self.__setattr__(k, None if v is None else uu.parse_quantity(v))
-            except:
-                pass
-
-    @sqlalchemy.orm.validates('aperture_parameters')
-    def _validate_aperture_parameters(self, _, val):
-        if val == {} or val is None:
-            return None
-        return ApertureParameters(**val).dict()
+        self.a1 = None
+        self.a2 = None
+        self.a5 = None
+        self.kd = None
+        self.eta0b = None
+        self.eta0hem = None
+        self.calculated_parameters = {}
+        self._set_collector_parameters(uu.parse_quantity(area_gr), uu.parse_quantity(area_ap), uu.parse_quantity(a1),
+                                       uu.parse_quantity(a2), uu.parse_quantity(a5), uu.parse_quantity(kd),
+                                       uu.parse_quantity(eta0b), uu.parse_quantity(eta0hem))
 
     @sqlalchemy.orm.validates('iam_method')
     def _validate_iam_method(self, _, val):
         if isinstance(val, dict):
             val = copy.copy(val)
             # because the iam methods expect Quantities, we need to convert them here in case we have dict...
             for key, value in val.items():
                 if "magnitude" in value:
                     val[key] = Q(value["magnitude"], value["units"])
             return iam_methods.__dict__[val.pop('method_type')](**val)
         return val
 
-    @staticmethod
-    def _infer_test_type(test_type: Union[str, None]) -> str:
-        """Returns test type (static, dynamic) based on user input.
+    def _infer_test_type(self, test_type):
+        """Returns test type (static, dynamic) based on user input."""
+        if test_type is None:
+            raise CollectorDefinitionError(
+                'Collector "test_type" is None, but must be specified. Use: ' + ", ".join(self.VALID_TEST_TYPES))
+        elif test_type.lower() not in [s.lower() for s in self.VALID_TEST_TYPES]:
+            raise ValueError(f'Parameter "test_type" must be one of {self.VALID_TEST_TYPES}.')
+
+        if test_type.lower() in ['sst', 'static']:
+            return 'SST'
+        else:
+            return 'QDT'
+
+    def _infer_test_reference_area(self, area):
+        if area is None:
+            raise CollectorDefinitionError(
+                'Collector "test_reference_area" is None, but must be specified. Use: ' + ", ".join(
+                    self.VALID_AREA_TYPES))
+        if area.lower() not in [s.lower() for s in self.VALID_AREA_TYPES]:
+            raise ValueError(f'Parameter "test_reference_area" must be one of {self.VALID_AREA_TYPES}.')
+        if area.lower() in ['gross', 'area_gr']:
+            return 'gross'
+        else:
+            return 'aperture'
+
+    def _set_collector_parameters(self, area_gr, area_ap, a1, a2, a5, kd, eta0b, eta0hem):
         """
-        if test_type is None or test_type not in list(CollectorTestTypes):
-            raise CollectorDefinitionError(f'Collector "test_type" invalid: {test_type}. '
-                                           f'Must be one of {", ".join(CollectorTestTypes)}.')
-        return test_type
-
-    @staticmethod
-    def _infer_test_reference_area(area: Union[str, None]) -> str:
-        """Return test reference area type (gross, aperture) based on user input.
+        Checks and converts provided collector information. Sets instance attributes if collector definition is sane.
+
+        Note
+        ----
+        Converts given collector parameters to gross area.
+        Checks that we have a complete and valid CollectorType definition, for both self.test_type cases 'SST' | 'QDT'.
+
+        Raises
+        ------
+        CollectorDefinitionException
+            If definition of collector parameters is incomplete or contradictory.
+        TypeError, ValueError
+            May be raised by check_quantity().
         """
-        if area is None or area not in list(CollectorReferenceAreaTypes):
-            raise CollectorDefinitionError(f'Collector "test_reference_area" invalid: {area}. '
-                                           f'Must be one of: {", ".join(CollectorReferenceAreaTypes)}.')
-        return area
+
+        # Check if required datapoints exist for specified method
+        is_dynamic_test = self.test_type == 'QDT'
+        if is_dynamic_test:
+            if kd is None:
+                raise CollectorDefinitionError("""If test_type=='dynamic', 'kd' must be provided.""")
+
+        # Estimation / calculation of missing parameters, if needed
+        if kd is None:
+            kd, info = estimate_kd_Hess_and_Hanby(self.iam_method)
+            self.calculated_parameters['kd'] = info
+        if eta0hem is None:
+            if eta0b is None:
+                raise CollectorDefinitionError("""Either 'eta0b' or 'eta0hem' must be provided in the 
+                collector definition, but both are missing.""")
+            eta0hem, info = estimate_eta0hem(eta0b=eta0b, kd=kd)
+            self.calculated_parameters['eta0hem'] = info
+        if eta0b is None:
+            if eta0hem is None:
+                raise CollectorDefinitionError("""Either 'eta0b' or 'eta0hem' must be provided in the 
+                collector definition, but both are missing.""")
+            eta0b, info = estimate_eta0b(eta0hem=eta0hem, kd=kd)
+            self.calculated_parameters['eta0b'] = info
+
+        # Convert to gross area if needed
+        is_aperture = self.test_reference_area == 'aperture'
+        if is_aperture:
+            if None in [area_gr, area_ap]:
+                raise CollectorDefinitionError("""If test_reference_area=='aperture', both 'area_gr' and 'area_ap' 
+                    must be provided.""")
+            if area_ap > area_gr:
+                raise CollectorDefinitionError("""Aperture area 'area_ap' must be smaller than gross area 'area_gr'.""")
+
+            conversion_factor = (area_ap / area_gr)
+            a1 = a1 * conversion_factor
+            a2 = a2 * conversion_factor
+            a5 = a5 * conversion_factor
+            eta0b = eta0b * conversion_factor if eta0b is not None else None
+            eta0hem = eta0hem * conversion_factor if eta0hem is not None else None
+
+        # Set values and check range
+        self.area_gr = uu.check_quantity(area_gr, 'm**2', 0.1, none_allowed=True)
+        self.area_ap = uu.check_quantity(area_ap, 'm**2', 0.1, none_allowed=True)
+        self.a1 = uu.check_quantity(a1, 'W m**-2 K**-1', min_limit=0, max_limit=20)
+        self.a2 = uu.check_quantity(a2, 'W m**-2 K**-2', min_limit=0, max_limit=1)
+        self.a5 = uu.check_quantity(a5, 'J m**-2 K**-1', min_limit=0, max_limit=100000)
+        self.eta0hem = uu.check_quantity(eta0hem, min_limit=0, max_limit=1)
+        self.eta0b = uu.check_quantity(eta0b, min_limit=0, max_limit=1)
+        self.kd = uu.check_quantity(kd, min_limit=0, max_limit=1)
+
+        return
 
     def is_attrib_missing(self, attrib_name):
         # May raise AttributeError
         attrib = getattr(self, attrib_name)
         if attrib is None:
             return True
-        if not isinstance(attrib, Q):
-            return True
         return False
 
-    def is_zero(self, param_name: str) -> bool:
-        """Return True if some collector parameter is zero.
-        """
-        param = getattr(self, param_name)
-        return (param is None) or (param == Q(0, param.units))
-
-    def is_nonzero(self, param_name: str) -> bool:
-        """Return True if some collector parameter is greater than zero.
-        """
-        param = getattr(self, param_name)
-        return (param is not None) and (param > Q(0, param.units))
 
     def __eq__(self, other):
         try:
             inst = inspect(self)
             attr_names = [c_attr.key for c_attr in inst.mapper.column_attrs if c_attr.key != 'id']
 
             for attr in attr_names:
                 if getattr(self, attr) != getattr(other, attr):
                     return False
             return True
         except AttributeError:
             return False
 
 
-def calculate_eta0hem(eta0b: Q, kd: Q) -> Tuple[Q, str]:
+class UninitialisedCollectorType(CollectorType):
+    def __init__(self, collector_type_name, parent, attribute):
+        self.name = collector_type_name
+        self.parent = parent
+        self.attribute = attribute
+
+
+def estimate_eta0hem(eta0b: Q, kd: Q):
     """
-    Calculate hemispherical peak collector efficiency `eta_0hem` from beam peak `eta0b` and diffuse IAM `kd`.
+    Calculates the hemispherical peak collector efficiency 'eta_0hem'
+    based on the QDT-SST conversion formulas in EN ISO 9806 ANNEX B.
 
     Parameters
     ----------
     eta0b: beam peak collector efficiency based on QDT test
-    kd:  diffuse incidence angle modifier
-
-    Notes
-    -----
-    This method is based on the QDT-SST conversion formulas in EN ISO 9806 ANNEX B.
+    kd:  diffuse incidence angle
 
     Returns
     -------
-    eta0hem: Quantity, The calculated hemispherical peak collector efficiency
+    eta0hem: Quantity, estimated hemispherical peak collector efficiency based on SST test
     info: string, information on calculation method used
     """
     eta0hem = eta0b * (0.85 + 0.15 * kd)
-    info = ('Collector parameter "eta0hem" (hemispherical peak collector efficiency) is calculated by SunPeek '
-            'based on "eta0b" and "Kd" (diffuse incidence angle modifier), '
-            'using the formula: eta0hem = eta0b * (0.85 + 0.15 * Kd). ')
+    info = 'Parameter "eta0hem" (hemispherical peak collector efficiency) calculated based on "eta0b" and diffuse ' \
+           'incidence angle modifier "Kd" using the formula: eta0hem = eta0b * (0.85 + 0.15 * Kd)'
     return eta0hem, info
 
 
-def calculate_eta0b(eta0hem: Q, kd: Q) -> Tuple[Q, str]:
+def estimate_eta0b(eta0hem: Q, kd: Q):
     """
-    Calculate beam peak collector efficiency `eta_0b` from hemispheric peak `eta0hem` and diffuse IAM `kd`.
+    Calculates the beam peak collector efficiency 'eta_0b'
+    based on the SST-QDT conversion formulas in EN ISO 9806 ANNEX B.
 
     Parameters
     ----------
     eta0hem: hemispherical peak collector efficiency based on SST test
-    kd:  diffuse incidence angle modifier
-
-    Notes
-    -----
-    This method is based on the SST-QDT conversion formulas in EN ISO 9806 ANNEX B.
+    kd:  diffuse incidence angle
 
     Returns
     -------
-    eta0hem: Quantity, The calculated hemispherical peak collector efficiency
+    eta0hem: Quantity, estimated hemispherical peak collector efficiency based on QDT test
     info: string, information on calculation method used
     """
     eta0b = eta0hem / (0.85 + 0.15 * kd)
-    info = ('Collector parameter "eta0b" (beam peak collector efficiency) is calculated by SunPeek '
-            'based on "eta0hem" and "Kd" (diffuse incidence angle modifier), '
-            'using the formula: eta0b = eta0hem / (0.85 + 0.15 * Kd). ')
+    info = 'Parameter "eta0b" (beam peak collector efficiency) calculated based on "eta0hem" and diffuse incidence ' \
+           'angle modifier "Kd" using the formula: eta0b = eta0hem / (0.85 + 0.15 * Kd)'
     return eta0b, info
 
 
-def calculate_kd_Hess_and_Hanby(iam_method: IAM_Method) -> Tuple[Q, str]:
+def estimate_kd_Hess_and_Hanby(iam_method: IAM_Method):
     """
-    Calculate collector parameter "Kd" (incidence angle modifier for diffuse radiation) based on "Kb" (IAM for beam).
+    Estimates the diffuse IAM (incidence angle modifier) ``Kd`` by integrating the IAM values for beam irradiation
+    over the hemispherical plane. Assumes isotropic diffuse radiation (which typically underestimates the derived ``Kd``
+    values).
 
     Parameters
     ----------
     iam_method : IAM_Method
-        Instance based on a _IAM_Method class with a method ``get_iam(aoi, azimuth_diff)`` to calculate the IAM
-        (incidence angle modifier) based on the angle of incidence ``aoi`` and the solar azimuth angle.
+        instance based on a _IAM_Method class with a method ``get_iam(aoi, azimuth_diff)`` to calculate the iam based
+        on the angle of incidence ``aoi`` and the solar azimuth angle ``phi``
 
     Returns
     -------
-    kd: Quantity, The calculated diffuse radiation incidence angle modifier.
-    info: string. Information on the used calculation method.
-
-    Notes
-    -----
-    - This method calculates an estimated value of "Kd" by integrating the "eta0b" values
-    (incidence angle modifier for beam radiation) over the hemispherical plane.
-    - Use results with caution. The used method reported by Hess & Hanby assumes isotropic diffuse radiation.
-    This typically underestimates the derived "Kd" values.
+    kd: Quantity, estimated diffuse radiation incidence angle modifier
+    info: string, information on calculation method used
 
     References
     ----------
     S. Hess and V. I. Hanby, Collector Simulation Model with Dynamic Incidence Angle Modifier for Anisotropic Diffuse
     Irradiance, Energy Procedia, vol. 48, pp. 8796, 2014, doi: 10.1016/j.egypro.2014.02.011.
     https://repositorio.lneg.pt/bitstream/10400.9/1063/1/SOLARTHRMAL.pdf
     """
@@ -509,23 +440,11 @@
 
     iam = iam_method.get_iam(aoi=theta_angles, azimuth_diff=phi_angles)
 
     v = np.sin(np.deg2rad(theta_angles)) * np.cos(np.deg2rad(theta_angles))
     W = v.sum()
     kd = np.multiply(v, iam).sum() / W
 
-    info = ('Collector parameter "Kd" (incidence angle modifier for diffuse radiation) is calculated by SunPeek '
-            'based on integration of the values of "eta0b" (beam IAM) over the hemispherical plane '
-            '(Hess & Hanby method), as described in doi: 10.1016/j.egypro.2014.02.011')
+    info = 'Parameter "Kd" (incidence angle modifier for diffuse radiation) calculated based on integration of the ' \
+           'beam IAM values over the hemispherical plane (Hess & Hanby method), as described in ' \
+           'doi: 10.1016/j.egypro.2014.02.011'
     return kd, info
-
-
-class UninitialisedCollector(Collector):
-    def __init__(self, collector_name, parent, attribute):
-        self.name = collector_name
-        self.parent = parent
-        self.attribute = attribute
-        self.a1 = Q(0, "W m**-2 K**-1")
-        self.a2 = Q(0, "W m**-2 K**-2")
-        self.a5 = Q(1, "kJ m**-2 K**-1")
-        self.gross_length = Q(0, 'm')
-
```

## sunpeek/core_methods/common/main.py

```diff
@@ -1,73 +1,75 @@
-from typing import List, Optional, Any
-from abc import ABC, abstractmethod
-import warnings
 import enum
 import time
 import traceback
 from dataclasses import dataclass
-import datetime as dt
+from typing import List, Optional
+from abc import ABC, abstractmethod
 import pandas as pd
 
 from sunpeek.common.errors import AlgorithmError
 from sunpeek.common.utils import sp_logger
 from sunpeek.components import Plant, Component
-from sunpeek.components.helpers import AlgoCheckMode
+from sunpeek.components.base import AlgoCheckMode
 from sunpeek.components.fluids import UninitialisedFluid
-from sunpeek.components.types import UninitialisedCollector
-from sunpeek.serializable_models import ProblemType, CoreProblem, CoreMethodFeedback, PCMethodFeedback
+from sunpeek.components.types import UninitialisedCollectorType
+from sunpeek.serializable_models import ProblemType, AlgoProblem, ProblemReport
 
 
 @dataclass
 class AlgoResult:
     """AlgoResult is returned by CoreAlgorithm.run(). It holds the algorithm output, the successful strategy (on of all
-    possible algorithm strategies), and a CoreMethodFeedback with details about problems in any of the strategies.
+    possible algorithm strategies), and a ProblemReport with details about problems in any of the strategies.
     """
-    output: Optional[Any]
+    output: Optional[dict]
     successful_strategy: Optional['CoreStrategy']
-    feedback: Optional[CoreMethodFeedback]
+    problems: Optional[ProblemReport]
 
     @property
     def success(self):
-        return self.feedback.success
+        return self.problems.success
 
     @property
     def successful_strategy_str(self):
-        return self.feedback.successful_strategy_str
+        return self.problems.successful_strategy_str
 
 
 class CoreStrategy(ABC):
     """Strategy of some CoreAlgorithm. To be attached to an algorithm with algo.define_strategies().
 
-    A strategy is defined for a specific component and should implement methods _calc() and _get_feedback().
+    A strategy is defined for a specific component and should implement methods calc() and _report_problems().
     """
     name = '(unnamed strategy)'
     feedthrough_real_sensor = False
 
     def __init__(self, component: Component):
         self.component: Component = component
 
     @abstractmethod
     def _calc(self):
         """Implement calculation of strategy, using information from self.component
         """
         raise NotImplementedError()
 
     @abstractmethod
-    def _get_feedback(self, check_mode: AlgoCheckMode) -> CoreMethodFeedback:
-        """Return CoreMethodProblem for given strategy.
+    def _report_problems(self, check_mode: AlgoCheckMode) -> ProblemReport:
+        """Return list of AlgoProblem objects.
+        #
+        # Raises
+        # ------
+        # AttributeError : if component has no slot or attribute with a name required by the .
         """
         raise NotImplementedError()
 
-    def get_feedback(self, check_mode: AlgoCheckMode) -> CoreMethodFeedback:
-        feedback = self._get_feedback(check_mode)
-        if not isinstance(feedback, CoreMethodFeedback):
-            raise AlgorithmError(f'Strategy "{self}" returned problems with invalid type. '
-                                 f'Expected "CoreMethodFeedback", got "{type(feedback)}".')
-        return feedback
+    def get_problem_report(self, check_mode: AlgoCheckMode) -> ProblemReport:
+        report = self._report_problems(check_mode)
+        if not isinstance(report, ProblemReport):
+            raise AlgorithmError(f'Strategy "{self}" returned problems with invalid type. Expected "ProblemReport", '
+                                 f'got "{type(report)}".')
+        return report
 
     def execute(self):
         """Try to calculate strategy, sanitize check output dict and return if ok.
 
         Returns
         -------
         elapsed_time : float, elapsed time in algorithm in seconds
@@ -148,102 +150,101 @@
 
     This class handles various strategies for an algorithm (e.g. various implementations to calculate thermal power,
     or various Performance Check methods, equations etc.
 
     *args and **kwargs passed to object creation are forwarded to :meth:`define_strategies`.
     """
 
-    name = 'CoreAlgorithm'
-
     def __init__(self, component: Component, strategies: Optional[List[VirtualSensorStrategy]] = None, *args, **kwargs):
         self.component = component
+        # self.strategies = strategies if strategies is not None else self.define_strategies(*args, **kwargs)
         self.strategies = strategies or self.define_strategies(*args, **kwargs)
-        self.feedback = CoreMethodFeedback()
+        self.problems = ProblemReport()
 
     @abstractmethod
     def define_strategies(self, *args, **kwargs) -> List[VirtualSensorStrategy]:
         raise NotImplementedError()
 
     def run(self, on_strategy_error: StrategyErrorBehavior = 'skip') -> AlgoResult:
         """Calculates algorithm using its defined strategies, stopping at the first successful strategy.
 
         Parameters
         ----------
         on_strategy_error : str, optional
             If 'raise', exceptions that occur during a strategy.execute() are raised. If not, they are saved as
-            own_feedback in self.problems. In any case, errors are logged.
+            own_problems in self.problems. In any case, errors are logged.
 
         Raises
         ------
         AlgorithmError : if algorithm has no strategies defined, or if getting some strategy problems fails.
         """
         if on_strategy_error not in list(StrategyErrorBehavior):
             raise AlgorithmError(f'Invalid value for "on_strategy_error": {on_strategy_error}. '
                                  f'Valid values are: {", ".join(StrategyErrorBehavior)}')
 
         if not self.strategies:
             raise AlgorithmError(f'Cannot run algo "{self}": No calculation strategies defined.')
 
-        self.feedback = CoreMethodFeedback(success=False)
+        self.problems = ProblemReport(success=False)
         for strategy in self.strategies:
-            feedback = strategy.get_feedback(check_mode=AlgoCheckMode.config_and_data)
-            self.feedback.add_sub(strategy.name, feedback)
+            report = strategy.get_problem_report(check_mode=AlgoCheckMode.config_and_data)
+            self.problems.add_sub(strategy.name, report)
 
-            if feedback.success:
+            if report.success:
                 try:
                     elapsed_time, output = strategy.execute()
-                    self.feedback.success = True
-                    self.feedback.problem_slots = feedback.problem_slots
+                    self.problems.success = True
+                    self.problems.problem_slots = report.problem_slots
                     sp_logger.debug(f'Done in {elapsed_time:3.1f}s '
                                     f'Algo "{self}" run() on component "{self.component.name}": '
                                     f'Successful using strategy "{strategy.name}". ')
-                    return AlgoResult(output, strategy, self.feedback)
+                    return AlgoResult(output, strategy, self.problems)
 
                 except Exception as e:
                     # The philosophy behind catching all Exceptions here: We always calculate all virtual sensors
                     # at `calculate_virtuals(plant)`, we don't know beforehand and therefore don't calculate virtuals
                     # specifically for some particular evaluation (like the PC method).
-                    # For example: A particular virtual sensor that is not required by the PC method might fail
+                    # To bring an example: A particular virtual sensor that is not required by the PC method might fail
                     # to calculate, but that would not affect running the PC method. That's why we decided to catch
-                    # all exceptions during calculation here and feedback them as `AlgoProblem`.
+                    # a calculation exception here and report it as `AlgoProblem`.
                     # The full exception trace is reported in the log files.
                     sp_logger.error(f'Algo "{self}" run() on component {self.component.name}: '
                                     f'error in strategy.execute() for "{strategy}": {traceback.format_exc()}')
                     if on_strategy_error == StrategyErrorBehavior.error:
                         raise
                     else:
-                        self.feedback.add_own(
-                            CoreProblem(ProblemType.unexpected_in_calc,
+                        self.problems.add_own(
+                            AlgoProblem(ProblemType.unexpected_in_calc,
                                         description=f'An unexpected calculation error of type "{type(e)}" has occurred '
                                                     f'during calculation of strategy "{strategy}". '
                                                     f'For further information, see '
                                                     f'https://docs.sunpeek.org/errors.html#unexpected-calculation-error'))
 
         sp_logger.info(f'Algo "{self}" run(): Could not calculate, none of the {len(self.strategies)} strategies was '
                        f'successful.')
 
-        return AlgoResult(None, None, self.feedback)
+        return AlgoResult(None, None, self.problems)
 
-    def get_config_feedback(self) -> CoreMethodFeedback:
-        """Cycle through all algo strategies, return CoreMethodFeedback of all strategy problems.
+    def get_config_problems(self) -> ProblemReport:
+        """Cycle through all algo strategies, return ProblemReport of all strategy problems.
         Stops at first successful strategy, copies problem slots from strategy.
         """
         if not self.strategies:
             raise AlgorithmError(f'Cannot run algo "{self}": No calculation strategies defined.')
 
-        algo_feedback = CoreMethodFeedback(success=False)
+        algo_report = ProblemReport(success=False)
         for strategy in self.strategies:
-            feedback = strategy.get_feedback(AlgoCheckMode.config_only)
-            algo_feedback.add_sub(strategy.name, feedback)
-            if feedback.success:
-                algo_feedback.success = True
-                algo_feedback.problem_slots = feedback.problem_slots
+            report = strategy.get_problem_report(AlgoCheckMode.config_only)
+            algo_report.add_sub(strategy.name, report)
+            if report.success:
+                algo_report.success = True
+                algo_report.problem_slots = report.problem_slots
                 break
 
-        return algo_feedback
+        return algo_report
 
     def allowed_components(self) -> tuple:
         # List of allowed components. By default, only Plant is allowed.
         return Plant,
 
     @property
     def component(self):
@@ -295,76 +296,31 @@
     def valid_components(self):
         valid_component_names = [c.__name__ for c in self.allowed_components()]
         return ', '.join(valid_component_names)
 
     @property
     def successful_strategy(self) -> Optional[CoreStrategy]:
         for strategy in self.strategies:
-            fb = strategy.get_feedback(AlgoCheckMode.config_only)
-            if fb.success:
+            r = strategy.get_problem_report(AlgoCheckMode.config_only)
+            if r.success:
                 return strategy
         return None
 
     def __str__(self):
         return self.__class__.__name__
 
     def __repr__(self):
         return f'SunPeek algorithm "{self.__class__.__name__}"'
 
-    @staticmethod
-    def create_variants(arg: Any, allowed_type: type, default: Any) -> List[Any]:
-        """Create list with sanitized inputs for algo strategies. Set default if needed.
-
-        Raises
-        ------
-        TypeError : if `arg` or `default` does not match `allowed_type`.
-        AlgorithmError : if no valid variants are found
-        """
-        def check_args(args: List[Any]) -> List[Any]:
-            if args is None:
-                return []
-            args = args if isinstance(args, list) else [args]
-            if is_enum:
-                args = [allowed_type(item) for item in args if item is not None]
-            for item in args:
-                if item is not None and not isinstance(item, allowed_type):
-                    raise TypeError(f'Input is not a valid {allowed_type.__name__}.')
-            # Remove None
-            args = [x for x in args if x is not None]
-            # Return without duplicates
-            return list(dict.fromkeys(args))
-
-        is_enum = isinstance(allowed_type, type) and issubclass(allowed_type, enum.Enum)
-        args = check_args(arg)
-        args = args if args else check_args(default)
-        if not args:
-            raise AlgorithmError('No valid variants found.')
-        return args
-
-    def check_interval(self,
-                       eval_start: dt.datetime | pd.Timestamp | None = None,
-                       eval_end: dt.datetime | pd.Timestamp | None = None) -> None:
-        """Make sure we have data in the specified interval, and check that plant's virtual sensors are up-to-date.
-        """
-        plant = self.component.plant
-        if not plant.virtuals_calculation_uptodate:
-            warnings.warn(f'{self.name} is called on a plant with outdated virtual sensors'
-                          f' (plant.virtuals_calculation_uptodate flag is False). '
-                          f'{self.name} results might be outdated or inconsistent with the plant configuration. '
-                          f'To overcome this, call "virtuals.calculate_virtuals(plant)".')
-
-        plant.context.set_eval_interval(eval_start, eval_end, check_overlap=True, method_name=self.name)
-
-
 
 ## Specific validation code
 
 def is_valid_fluid(fluid, check_mode: AlgoCheckMode) -> bool:
     if check_mode == AlgoCheckMode.config_only:
         return fluid is not None
     return (fluid is not None) and (not isinstance(fluid, UninitialisedFluid))
 
 
 def is_valid_collector(collector, check_mode: AlgoCheckMode) -> bool:
     if check_mode == AlgoCheckMode.config_only:
         return collector is not None
-    return (collector is not None) and (not isinstance(collector, UninitialisedCollector))
+    return (collector is not None) and (not isinstance(collector, UninitialisedCollectorType))
```

## sunpeek/core_methods/pc_method/__init__.py

```diff
@@ -1,54 +1,50 @@
 """
-Implements Performance Check (PC) Method according to technical standard ISO 24194:2022.
+Implements Performance Check (PC) Method according to technical standard ISO/DIS 24194.
 
 HowTo
 =====
 
 See :py:mod:main docstring.
 
 Why
 ===
 
 The Performance Check (PC) method can be used to decide whether the measured performance of a solar thermal plant
 matches the performance estimated based on given information (data sheets, boundary conditions etc).
 
-From the ISO 24194:2022:
+From the ISO/DIS 24194:
 "
     # 1 Scope
-    [This method allows to] check the performance of solar thermal collector fields.
-    [This method is applicable to] glazed flat plate collectors, evacuated tube collectors and/or
-    tracking, concentrating collectors used as collectors in fields.
+    [This method allows to] verify the performance of solar thermal collector fields. The
+    collectors in the fields can be glazed flat plate collectors, evacuated tube collectors and/or
+    tracking, concentrating collectors.
     [...]
 
     # 5 Procedure for checking the power performance of solar thermal collector fields
     The estimated power output of the collector array is given as an equation depending on collector parameters
     according to ISO 9806 and operating conditions. The measured power shall comply with the corresponding
     calculated power according to this equation. Measured and calculated power are only compared under some specific
     conditions to avoid too large uncertainties - see section 5.4.
 
     # 5.2 Calculating power output
     The estimate is given by stating the equation to be used for calculating the power output, including specific values
     for the parameters in equation. The three possible equations are given in the next three subsections.
     The collector module efficiency parameters eta0_hem, eta0_b, Kb(theta) Kd, a1, a2, a5 [1] and a8 should be based on
-    specific [2] test results. When an estimate is given it shall always be stated which equation shall be used for
+    certified test results. When an estimate is given it shall always be stated which equation shall be used for
     checking the performance:
 
     a) Simple check, using total radiation on the collector plane when checking the power output
-    (this document, Formula 1).
+    (ISO this standard, eq 1).
     b) Advanced check, using direct and diffuse radiation on collector plane when checking the power output
-    (this document, Formula 2).
+    (ISO this standard, eq 2).
     c) Advanced check, using only direct radiation on collector plane when checking the power output
-    (this document, Formula 3).
-
-    Ensure that the parameters are related to gross collector area, A_GF.
-    If necessary, the parameters shall be converted in accordance with ISO 9806.
+    (ISO this standard, eq3)
 
     [1] in the older Solar Keymark data sheets a5 is denoted c_eff
-    [2] E.g. Solar Keymark or similar.
 "
 
 # Notes & usage hints
 
 - Assumes tilted beam and diffuse radiations available (no radiation splitting inside this code)
 - Assumes wind velocity available. There is an `ignore_wind` flag to ignore wind measurements at all.
 - Power is assumed to be measured once (for the plant object). If multiple power measurements are available,
@@ -69,43 +65,30 @@
     `eta0b`: Peak collector efficiency (etab at theta_m -theta_a = 0 K) based on beam irradiance Gb
     `eta0hem`: Peak collector efficiency (`eta0hem` at theta_m - theta_a = 0 K) based on
               hemispherical irradiance G_hem
     `kd`: Incidence angle modifier for diffuse solar radiation (named as Kd in ISO 24194:2022)
 """
 
 import enum
+# from .wrapper import run_performance_check
+from .plotting import plot_all, plot_square, plot_time, plot_sums, plot_sensor_data
+
+# from .wrapper import run_performance_check, pc_strategy_generator
+# from .main import PCMethod, PCSettings, PCMethodISO, PCMethodExtended
+# from .equation import Equation1, Equation2
+# from .verify_validate import verify_config, validate_data
+
+# OLD MAIN
+# from .main import PCMethod
+# from .equation import Equation1, Equation2
+# from .plotting import plot_all, plot_square, plot_time, plot_sums, plot_sensor_data
+# from .verify_validate import verify_config, validate_data
 
 
-class PCFormulae(enum.IntEnum):
+class AvailablePCEquations(enum.IntEnum):
     one = 1
     two = 2
-    three = 3
 
 
-class PCMethods(str, enum.Enum):
-    iso = 'ISO'
+class AvailablePCMethods(str, enum.Enum):
+    iso = 'iso'
     extended = 'extended'
-
-    @classmethod
-    def _missing_(cls, value: str):
-        value = value.lower()
-        for member in cls:
-            if member.lower() == value:
-                return member
-        return None
-
-
-class PCAccuracyClasses(str, enum.Enum):
-    one = "I"
-    two = "II"
-    three = "III"
-    none = "none"
-
-
-class OutputUnits(str, enum.Enum):
-    tp = 'pint[kW]'
-    tp_sp = 'pint[W m**-2]'
-    rd = 'pint[W m**-2]'
-    iam = 'pint[dimensionless]'
-    te = 'pint[degC]'
-    te_deriv = 'pint[K hour**-1]'
-    angle = 'pint[deg]'
```

## sunpeek/core_methods/pc_method/main.py

```diff
@@ -1,36 +1,35 @@
 """
-Implements Performance Check (PC) Method according to technical standard ISO 24194:2022.
+Implements Performance Check (PC) Method according to technical standard ISO/DIS 24194.
 
 HowTo
 =====
 
 To create instances, use
-- ISO mode: PCMethod.from_method('ISO')
-- Extended mode: PCMethod.from_method('Extended')
+- ISO mode: PCMethod.from_method('iso')
+- Extended mode: PCMethod.from_method('extended')
 
-The entry-point method by the classes in this module is :method:`pc_method.run()`.
-This is called by :fun:`run_performance_check` in the wrapper / the PC strategies.
+Main method to run an analysis is pc_method.run()
 Results: pc_method.get_results() returns a components.results.PCMethodOutput object.
 
 See docstring in __init__ for more details.
 
 Implementations
 ===============
 PCMethodISO
 -----------
 
 The implementation variant that aligns as closely as possible to the ISO 24194 standard is in class PCMethodISO.
-Create an analysis with PCMethod.from_method('ISO', **kwargs).
+Create an analysis with PCMethod.from_method('iso', **kwargs).
 
 PCMethodExtended
 ----------------
 
-Reasoning: Some of the data analysis recommendations described in the ISO standard apparently assume the use of Excel
-or other spreadsheet based software. For instance, analysis is based on fixed 1-hourly that start at full hours. This
+Reasoning: Some of the data analysis recommendations described in the ISO standard apprently assume the use of Excel or
+other spreadsheet based software. For instance, analysis is based on fixed 1-hourly that start at full hours. This
 does not necessarily lead to the best / most useful results.
 
 This software package implements an extended variant of the Performance Check method that overcomes some limitations of
 the strictly fixed-hour variant described in ISO 24194.
 This 'extended' implementation has a few slight but significant improvements in data analysis,
 while sticking as closely as possible to the intentions and purpose of the ISO 24194 standard:
 It tends to produce more and less noisy intervals in a Performance Check analysis. Numerically, comparable in range to
@@ -53,66 +52,72 @@
 .. codeauthor:: Philip Ohnewein <p.ohnewein@aee.at>
 .. codeauthor:: Lukas Feierl <l.feierl@solid.at>
 .. codeauthor:: Daniel Tschopp <d.tschopp@aee.at>
 """
 
 from abc import ABC, abstractmethod
 import warnings
+import enum
 import datetime as dt
 import pandas as pd
 import numpy as np
-from typing import Union, Optional
+from typing import Union, Optional, Dict, Any
 from statsmodels.formula import api as smf
 
 from sunpeek.common.utils import sp_logger
 from sunpeek.common.unit_uncertainty import Q
 from sunpeek.components import Plant
-import sunpeek.components.outputs_pc_method as results
+import sunpeek.components.results as results
 from sunpeek.common.errors import PCMethodError
-from sunpeek.components.helpers import AlgoCheckMode
-from sunpeek.core_methods.pc_method import PCMethods, PCFormulae, PCAccuracyClasses, OutputUnits
-from sunpeek.core_methods.pc_method.formula import Formula
-from sunpeek.serializable_models import CoreMethodFeedback
+from sunpeek.components.base import AlgoCheckMode
+from sunpeek.core_methods.pc_method import AvailablePCMethods, AvailablePCEquations
+from sunpeek.core_methods.pc_method.equation import Equation
+from sunpeek.serializable_models import ProblemReport, ProblemType, AlgoProblem
 
 # ------------------------------------------------------------------------------------
 # PC Method parameters
 
-METHOD_DESCRIPTION = 'Power Check according to ISO 24194:2022'
-
 # Default values that appear in ISO 24194:
 INTERVAL_LENGTH_ISO = dt.timedelta(hours=1)  # 1 hour = specified in ISO standard
 MIN_INTERVALS_IN_OUTPUT = 20
 
-
-# Safety factors, as discussed in https://gitlab.com/sunpeek/sunpeek/-/issues/323#note_1607602081:
-F_PIPES = 0.99
-F_UNCERTAINTY = 0.93
-F_OTHERS = 0.98
-
-# This version is closer to ISO 24194:2022:
-# F_PIPES = 0.99
-# F_UNCERTAINTY = {PCAccuracyClasses.one: 0.95,
-#                  PCAccuracyClasses.two: 0.9,
-#                  PCAccuracyClasses.three: 0.9}
-# F_OTHERS = {PCFormulae.one: 0.98,
-#             PCFormulae.two: 0.99,
-#             PCFormulae.three: 0.99}
+# Default settings defined in this implementation:
+DEFAULT_ACCURACY_LEVEL = "II"
+# Safety factors
+F_PIPES = 0.98
+F_UNCERTAINTY = 0.90
+F_OTHERS_EQ1 = 0.98
+F_OTHERS_EQ2 = 0.99
 
 # Data
 MIN_DATA_IN_INTERVAL = 10
 LOWER_BOUND__MIN_DATA_IN_INTERVAL = 5
-MAX_NAN_DENSITY = 0.10
+MAX_NAN_DENSITY = 0.05
 
 # Intervals & gaps
-MAX_INTERVAL_LENGTH = dt.timedelta(hours=6)
-MAX_GAP_IN_INTERVAL = dt.timedelta(minutes=10)
-DEFAULT_RATIO__MAX_GAP__TO__INTERVAL_LENGTH = 0.17
+MAX_INTERVAL_LENGTH = dt.timedelta(hours=12)
+MAX_GAP_IN_INTERVAL = dt.timedelta(minutes=30)
+DEFAULT_RATIO__MAX_GAP__TO__INTERVAL_LENGTH = 0.5
 MIN_INTERVAL_LENGTH_EXTENDED = dt.timedelta(minutes=15)
 
 
+class PCAccuracyClasses(str, enum.Enum):
+    one = "I"
+    two = "II"
+    three = "III"
+
+
+# TODO This is a stub. Probably not needed anymore with the new ISO 24194? Check issue 383
+# def get_default_safety_factors(accuracy_level: PCAccuracyClasses = DEFAULT_ACCURACY_LEVEL):
+#     if accuracy_level == PCAccuracyClasses.one:
+#         pass
+#
+#     raise NotImplementedError()
+
+
 # ------------------------------------------------------------------------------------
 #
 class PCSettings:
     """PC Method settings: Holds defaults, parses and validates a PC method settings dictionary, replacing None or
     missing settings with defaults.
 
     safety_pipes : float (optional)
@@ -125,17 +130,17 @@
         Default: None (will be set according to plant_measurement_accuracy)
     safety_others: float (optional)
         Safety factor for other uncertainties e.g. related to non-ideal conditions such as:  non-ideal flow
         distribution. To be estimated - should be close to one.  unforeseen heat losses. To be estimated - should
         be close to one.  uncertainties in the model/procedure itself. To be estimated - should be close to one.
         Note - it is recommended to put fO  1 when eq. (1) is used, as eq. (1) does not consider the influence of
         incidence angle modifiers.
-        Default: None (will be set according to used formula)
+        Default: None (will be set according to used equation)
 
-    accuracy_level : {"I", "II", "III"} (optional)
+    check_accuracy_level : {"I", "II", "III"} (optional)
         Level of accuracy of sensor as specified in ISO chapter 6. Will only be used for reporting and does not
         influence the output of the pc method.
     interval_length : dt.datetime (optional)
         Length of the interval over which single data records are averaged.
         This is set to 1 hour in the ISO 24194 standard, but can be changed for PCMethodExtended.
 
     max_nan_density : float (optional)
@@ -148,101 +153,92 @@
         interval, and it doesn't make much sense to include such intervals.
     max_gap_in_interval : dt.timedelta
         Even if an interval has a minimum number of intervals (at least min_data_in_interval), those records might be
         clustered e.g. at the beginning or end of the interval, with large gaps without data records in between.
 
     Notes
     -----
-    Some settings for the Performance Check calculations depend on the chosen method (ISO | extended) and formula.
-    That is why method and formula must be known at __init__ time.
+    Some settings for the Performance Check calculations depend on the chosen method (ISO | extended) and equation.
+    That is why method and equation must be known at __init__ time.
     """
     safety_pipes: Optional[float]
     safety_uncertainty: Optional[float]
-    safety_others: Optional[float]
+    safety_others: Optional[float]  # has no single default, depends on & needs to be set by Equation
     interval_length: Optional[dt.timedelta]
     max_gap_in_interval: Optional[dt.timedelta]
     min_data_in_interval: Optional[int]
     max_nan_density: Optional[float]
     min_intervals_in_output: Optional[int]
-    accuracy_level: Optional[str]
+    check_accuracy_level: Optional[str]
 
     default_settings = dict(
         safety_pipes=F_PIPES,
-        safety_uncertainty=None,  # Has no single default, depends on accuracy class
-        safety_others=None,  # Has no single default, depends on formula
+        safety_uncertainty=F_UNCERTAINTY,
+        safety_others=None,  # Has no single default, depends on & needs to be set by Equation
         interval_length=INTERVAL_LENGTH_ISO,
         min_data_in_interval=MIN_DATA_IN_INTERVAL,
         max_gap_in_interval=None,  # Default: fixed ratio of interval_length
         max_nan_density=MAX_NAN_DENSITY,
         min_intervals_in_output=MIN_INTERVALS_IN_OUTPUT,
-        accuracy_level=PCAccuracyClasses.none,
+        check_accuracy_level=DEFAULT_ACCURACY_LEVEL,
     )
 
     @property
     def safety_combined(self):
-        # Round to 2 digits, as required by ISO 24194
-        f_safe = self.safety_uncertainty * self.safety_pipes * self.safety_others
-        return np.round(f_safe, 2)
+        return self.safety_uncertainty * self.safety_pipes * self.safety_others
 
     @property
     def names(self):
         return self.default_settings.keys()
 
     def __init__(self,
-                 method: Union[PCMethods, str],
-                 formula: Union[PCFormulae, int],
+                 method: Union[AvailablePCMethods, str],
+                 equation: Union[AvailablePCEquations, int],
                  **kwargs):
 
         settings = kwargs.copy()
         defaults = self.default_settings
 
         for k in settings:
             if k not in self.names:
-                raise PCMethodError(f'Invalid Performance Check method setting: {k}. '
-                                    f'Valid settings: {", ".join(self.names)}.')
+                raise PCMethodError(f'Unkown Performance Check method setting: {k}.'
+                                    f'Known settings: {", ".join(self.names)}.')
 
         # Replace None / missing with default
         settings = {k: v for k, v in settings.items() if v is not None}
         for key in self.names:
             self.__setattr__(key, settings.get(key, defaults[key]))
 
-        # Accuracy level
-        try:
-            self.accuracy_level = PCAccuracyClasses(self.accuracy_level)
-        except ValueError as e:
-            raise PCMethodError(
-                f'{e}. Performance Check accuracy_level must be one of {", ".join(PCAccuracyClasses)}. ')
-
         # Safety factors
-        # Safety factor uncertainty: depends on accuracy level
-        if self.safety_uncertainty is None:
-            self.safety_uncertainty = F_UNCERTAINTY
-        # Safety factor others: depends on formula
         if self.safety_others is None:
-            self.safety_others = F_OTHERS
+            if equation == AvailablePCEquations.one:
+                self.safety_others = F_OTHERS_EQ1
+            elif equation == AvailablePCEquations.two:
+                self.safety_others = F_OTHERS_EQ2
+
+        # Safety factors
         safety_ok = lambda x: (x is None) or ((x > 0) and (x <= 1))
         for f in ['safety_pipes', 'safety_uncertainty', 'safety_others']:
-            if not safety_ok(getattr(self, f)):
+            if not safety_ok(self.__getattribute__(f)):
                 raise PCMethodError(
-                    f'All Performance Check safety factors (for pipes, uncertainty and others) '
-                    f'must be either None or floats between 0 and 1. '
+                    f'All Performance Check safety factors must be either None or floats between 0 and 1. '
                     f'Got "{f}" = {str(f)}.')
 
-        # Interval length
+        # interval length
         if self.interval_length > MAX_INTERVAL_LENGTH:
             raise PCMethodError(
                 f'Performance Check maximum allowed interval length is {str(MAX_INTERVAL_LENGTH)}.')
 
-        if method == PCMethods.iso:
+        if method == AvailablePCMethods.iso:
             if self.interval_length != INTERVAL_LENGTH_ISO:
                 raise PCMethodError(
                     f'For a Performance Check evaluation following the fixed-hour ("ISO") scheme, '
                     f'the "interval_length" is fixed to 1 hour, as defined in the ISO 24194.')
 
-        if method == PCMethods.extended:
+        if method == AvailablePCMethods.extended:
             if self.interval_length < MIN_INTERVAL_LENGTH_EXTENDED:
                 raise PCMethodError(
                     f'For a Performance Check evaluation following the rolling-hour ("extended") scheme, '
                     f'"interval_length" should not be lower than {str(MIN_INTERVAL_LENGTH_EXTENDED)}.')
 
         # max_gap_in_interval
         if self.max_gap_in_interval is None:
@@ -266,99 +262,111 @@
                 f'Got {str(self.max_nan_density)}.')
 
         # min_intervals_in_output
         if self.min_intervals_in_output <= 0:
             raise PCMethodError(
                 f'Performance Check option "min_intervals_in_output" must be None or greater than 0. '
                 f'Got {str(self.min_intervals_in_output)}.')
-        if method == PCMethods.iso:
+        if method == AvailablePCMethods.iso:
             if self.min_intervals_in_output != MIN_INTERVALS_IN_OUTPUT:
                 raise PCMethodError(
                     f'For a Performance Check evaluation following the fixed-hour ("ISO") scheme, '
                     f'"min_intervals_in_output" is fixed to {MIN_INTERVALS_IN_OUTPUT}, as defined in the ISO 24194.')
 
+        # check_accuracy_level
+        if self.check_accuracy_level is None:
+            self.check_accuracy_level = DEFAULT_ACCURACY_LEVEL
+        if self.check_accuracy_level not in list(PCAccuracyClasses):
+            raise PCMethodError(
+                f'Performance Check option "check_accuracy_level" invalid. '
+                f'Must be one of {", ".join(list(PCAccuracyClasses))}. '
+                f'Got {str(self.check_accuracy_level)}.')
+
         # Now all settings should have been set / no None left
-        none_settings = [s for s in self.names if getattr(self, s) is None]
+        none_settings = [s for s in self.names if self.__getattribute__(s) is None]
         if none_settings:
             raise PCMethodError(f'Some settings are None after initializing PCSettings. This is an internal error. '
-                                f'Settings being None: {", ".join(none_settings)}')
+                                f'None settings: {", ".join(none_settings)}')
 
 
 # ------------------------------------------------------------------------------------
 # PC Method
 
 # noinspection PyArgumentList
 class PCMethod(ABC):
     """Superclass for various variants of the Performance Check Method.
 
     Parameters
     ----------
     plant : Plant
         Fully-configured plant with at least one array, and with virtual sensors calculated.
-    formula : Formula
-        Formula / equation to be used for the PC method. See ISO 24194 chapter 5.2.1.
+    equation : Equation
+        Equation to be used for the PC method. See ISO 24194 chapter 5.2.1.
     kwargs : passed to PCSettings
     """
 
     method_name = ""
     mode = ""
 
     @classmethod
-    def create(cls,
-               plant: Plant,
-               method: PCMethods | str,
-               formula: PCFormulae | int,
-               use_wind: bool = True,
-               **kwargs):
-
-        method = PCMethods(method)
-        if method == PCMethods.iso:
-            return PCMethodISO(plant, formula, use_wind, **kwargs)
-        elif method == PCMethods.extended:
-            return PCMethodExtended(plant, formula, use_wind, **kwargs)
+    def from_method(cls,
+                    method: Union[AvailablePCMethods, str],
+                    plant: Plant,
+                    equation: Union[AvailablePCEquations, int],
+                    use_wind: bool = True,
+                    **kwargs):
+
+        if method == AvailablePCMethods.iso:
+            return PCMethodISO(plant, equation, use_wind, **kwargs)
+        elif method == AvailablePCMethods.extended:
+            return PCMethodExtended(plant, equation, use_wind, **kwargs)
         else:
-            raise PCMethodError(f'Cannot create PC method, invalid method "{method}".')
+            raise PCMethodError(f'Cannot create PC method, unknown method "{method}".')
 
     def __init__(self,
                  plant: Plant,
-                 formula: PCFormulae | int,
+                 equation: Union[AvailablePCEquations, int],
                  use_wind: bool = True,
                  **kwargs):
         self.plant = plant
-        self.formula = Formula.create(formula, use_wind)
-        self.settings = PCSettings(method=self.mode, formula=formula, **kwargs)
+        self.equation = Equation.create(equation, use_wind)
+        self.settings = PCSettings(method=self.mode, equation=equation, **kwargs)
 
         self._mask = None
         self._bins = None
         self._output = {}
         return
 
     def run(self) -> results.PCMethodOutput:
         """ Applies the Performance Check on the plant and returns the estimated and calculated power.
         """
+        start_time = dt.datetime.now(dt.timezone.utc)
+
         if self._filter_intervals():
             self._calc_output()
 
-        pc_method_output = self._create_output_object()
+        pc_method_output = self._create_output_object(start_time)
         return pc_method_output
 
-    def get_feedback(self, check_mode: AlgoCheckMode) -> CoreMethodFeedback:
-        r = CoreMethodFeedback()
+    def report_problems(self, check_mode: AlgoCheckMode) -> ProblemReport:
+        r = ProblemReport()
 
         if not self.plant.arrays:
-            r.add_missing_component(self.plant, 'arrays',
-                                    'Plant has no arrays. To run a Performance Check analysis, '
-                                    'the plant needs one or more arrays.')
+            r.add_own(AlgoProblem(ProblemType.component_missing, self.plant, 'arrays',
+                                  'To run the Performance Check method, you need to add arrays to the plant'))
 
         for slot in ['te_amb', 'tp']:
-            if self.plant.is_sensor_missing(slot, check_mode):
+            if self.plant.is_slot_missing(slot, check_mode):
                 r.add_missing_sensor(self.plant, slot, check_mode)
 
         for array in self.plant.arrays:
-            r = self.formula.get_feedback(r, array, check_mode)
+            # r_array = self.equation.report_problems(array, check_mode)
+            # r.add_own(r_array.own_problems)
+            # r.add_virtual()
+            r = self.equation.report_problems(r, array, check_mode)
 
         return r
 
     @abstractmethod
     def _aggregate_candidates(self, s: pd.Series, agg_fun: str):
         """Implements the aggregation of sensor data records, e.g. hourly mean (ISO) or rolling mean (extended),
         into candidate intervals that may be selected as the final PC method intervals.
@@ -403,36 +411,37 @@
 
         # min_data_in_interval
         value_count = self._aggregate_candidates(self.plant.time_index.to_series(), 'count')
         self._mask['min_data_ok'] = (value_count >= self.settings.min_data_in_interval)
 
         # max_nan_density
         # nan_mask: True where _any_ of the sensors used in the PC method is NaN. Those records are rejected.
-        nan_mask = self.formula.get_nan_mask(self.plant)
+        nan_mask = self.equation.get_nan_mask(self.plant)
         nan_density = self._aggregate_candidates(nan_mask, 'sum') / value_count
         self._mask['nan_density_ok'] = (nan_density <= self.settings.max_nan_density)
 
         # max_gap_in_interval
         # Define gap of an index as average between backward and forward gap.
         bwd = self.plant.time_index.to_series().diff().dt.total_seconds()
         fwd = bwd.shift(-1)
         gaps = pd.concat([bwd, fwd], axis=1).mean(axis=1)
         max_gap = self._aggregate_candidates(gaps, 'max')
         self._mask['max_gap_ok'] = (max_gap <= self.settings.max_gap_in_interval.total_seconds())
 
         # Restrictions to interval filtering described in ISO 24194 Table 1, chapter 5.4.
         self._mask['pc_restrictions'] = \
-            self.formula.calc_pc_restrictions(plant=self.plant,
-                                              resampler=lambda s, fun='mean': self._aggregate_candidates(s, fun))
+            self.equation.calc_pc_restrictions(plant=self.plant,
+                                               resampler=lambda s, fun='mean': self._aggregate_candidates(s, fun))
 
         self._mask['best_intervals'] = self._select_best_intervals()
         n_intervals = self._mask['best_intervals'].sum()
         self._output['n_intervals'] = n_intervals
 
-        if (n_intervals < self.settings.min_intervals_in_output) and (self.mode == PCMethods.iso):
+        if (n_intervals < self.settings.min_intervals_in_output) and (self.mode == AvailablePCMethods.iso):
+            # TODO transport this warning over the API
             sp_logger.warn(
                 f'Performance check analysis found {n_intervals} intervals. For checking the collector performance, '
                 f'the ISO 24194 recommends to have at least {MIN_INTERVALS_IN_OUTPUT} intervals.')
         if n_intervals == 0:
             return False
 
         # Out of the marked best intervals, create bins for groupby
@@ -449,38 +458,30 @@
         Returns
         -------
         Nothing. Sets self._output_plant, self._slopes and self._output_arrays
         """
         tp_estimated = 0
         te_op_mean_area = 0
         self._output['arrays'] = {}
-        self._output['data'] = {}
 
         # Aggregation of sensor data records for the final intervals selected among the candidates.
         #  - The final intervals meet data quality requirements and the restrictions of PC method Table 1.
         #  - The number of intervals is usually much smaller than the number of candidates. Thus groupby should be
         #    faster than resampling / rolling again and then filtering on self._mask['best_intervals']
         #  - The same aggregation is used for PCMethodISO and PCMethodExtended.
+        # aggregator = self._aggregate_intervals
         aggregator = lambda s: s.groupby(self._bins).mean()
 
         for array in self.plant.arrays:
-            df = self.formula.calc_estimated_power(array, aggregator)
+            df = self.equation.calc_estimated_power(array, aggregator).to_frame(name='tp_sp_estimated')
             df['tp_sp_estimated_safety'] = df['tp_sp_estimated'] * self.settings.safety_combined
             df['tp_estimated'] = df['tp_sp_estimated'] * array.area_gr
             if (not array.tp.is_virtual) or (array.tp.is_virtual and array.tp.can_calculate):
                 df['tp_sp_measured'] = aggregator(array.tp.data) / array.area_gr
-
-            # Additional data, for current array, if not returned by calc_estimated_power()
-            df['te_in'] = aggregator(array.te_in.data)
-            df['te_out'] = aggregator(array.te_out.data)
-            df['aoi'] = aggregator(array.aoi.data)
-            if self.formula.use_wind:
-                df['ve_wind'] = aggregator(self.plant.ve_wind.data)
-
-            # Array results for current array is the DataFrame
+            # save DataFrame with array results in dict
             self._output['arrays'][array] = df
             tp_estimated += df['tp_estimated']
             te_op_mean_area += aggregator(array.te_op.data) * array.area_gr
 
         df = aggregator(self.plant.tp.data.astype('pint[kW]')).to_frame(name='tp_measured')
         df['tp_sp_measured'] = df['tp_measured'].astype('pint[W]') / self.plant.area_gr
         df['tp_sp_estimated'] = tp_estimated / self.plant.area_gr
@@ -498,15 +499,15 @@
         self._output['slopes'] = {}
         fit = smf.ols('tp_sp_measured ~ tp_sp_estimated -1', data=df_slopes).fit()
         self._output['slopes']['target_actual'] = Q(fit.params.to_numpy()[0], '')
 
         fit = smf.ols('tp_sp_measured ~ tp_sp_estimated_safety -1', data=df_slopes).fit()
         self._output['slopes']['target_actual_safety'] = Q(fit.params.to_numpy()[0], '')
 
-    def _create_output_object(self) -> results.PCMethodOutput:
+    def _create_output_object(self, start_time) -> results.PCMethodOutput:
         """Gather all PC method calculation outputs required for ISO 24194 Annex A1, and a few more.
 
         Returns
         -------
         PCMethodOutput object
         """
         out = results.PCMethodOutput()
@@ -514,95 +515,97 @@
         out.plant = self.plant
 
         out.datetime_eval_start = self.plant.context.eval_start
         out.datetime_eval_end = self.plant.context.eval_end
 
         # Algorithm / Strategy
         out.pc_method_name = self.method_name
-        out.evaluation_mode = PCMethods.iso.value if self.mode.value == PCMethods.iso else PCMethods.extended.value
-        out.formula = self.formula.id
-        out.wind_used = self.formula.use_wind
+        out.evaluation_mode = self.mode.value
+        out.equation = self.equation.id
+        out.wind_used = self.equation.use_wind
 
         # Strategy PCSettings
-        settings = {k: v for k, v in vars(self.settings).items() if not k.startswith('_')}
-        settings['safety_combined'] = self.settings.safety_combined
-        out.settings = settings
+        out.settings = dict(
+            safety_pipes=self.settings.safety_pipes,
+            safety_uncertainty=self.settings.safety_uncertainty,
+            safety_others=self.settings.safety_others,
+            safety_combined=self.settings.safety_combined,
+            interval_length=self.settings.interval_length,
+            max_gap_in_interval=self.settings.max_gap_in_interval,
+            min_data_in_interval=self.settings.min_data_in_interval,
+            max_nan_density=self.settings.max_nan_density,
+            min_intervals_in_output=self.settings.min_intervals_in_output,
+            check_accuracy_level=self.settings.check_accuracy_level,
+        )
 
         # Plant results
         plant_out = results.PCMethodOutputPlant()
         plant_out.plant = self.plant
         plant_out.n_intervals = self._output['n_intervals']
-        plant_out.total_interval_length = plant_out.n_intervals * self.settings.interval_length
+
         intervals_end = self._mask.index[self._mask['best_intervals']].to_pydatetime()
         plant_out.datetime_intervals_start = intervals_end - self.settings.interval_length
         plant_out.datetime_intervals_end = intervals_end
 
         has_intervals = (self._output['n_intervals'] > 0)
+
         if has_intervals:
             df = self._output['plant']
+            unit_tp = 'pint[kW]'
+            unit_tp_sp = 'pint[W m**-2]'
 
             # This is necessary to prevent unneeded calls of config_virtuals -> PCMethodOutputPlant is AttrSetterMixin
             try:
                 plant_out.defer_post_config_changed_actions = True
-                plant_out.tp_measured = df['tp_measured'].astype(OutputUnits.tp)
-                plant_out.tp_sp_measured = df['tp_sp_measured'].astype(OutputUnits.tp_sp)
-                plant_out.tp_sp_estimated = df['tp_sp_estimated'].astype(OutputUnits.tp_sp)
-                plant_out.tp_sp_estimated_safety = df['tp_sp_estimated_safety'].astype(OutputUnits.tp_sp)
+                plant_out.tp_measured = df['tp_measured'].astype(unit_tp)
+                plant_out.tp_sp_measured = df['tp_sp_measured'].astype(unit_tp_sp)
+                plant_out.tp_sp_estimated = df['tp_sp_estimated'].astype(unit_tp_sp)
+                plant_out.tp_sp_estimated_safety = df['tp_sp_estimated_safety'].astype(unit_tp_sp)
                 plant_out.mean_tp_sp_measured = plant_out.tp_sp_measured.mean()
                 plant_out.mean_tp_sp_estimated = plant_out.tp_sp_estimated.mean()
                 plant_out.mean_tp_sp_estimated_safety = plant_out.tp_sp_estimated_safety.mean()
                 plant_out.target_actual_slope = self._output['slopes']['target_actual']
                 plant_out.target_actual_slope_safety = self._output['slopes']['target_actual_safety']
 
                 te = self._output['plant']['te_op_mean'].mean().to('degC')
                 plant_out.mean_temperature = te
                 te_s = pd.Series(data=te.to('K').magnitude).astype('pint[K]')
 
                 plant_out.fluid_solar = self.plant.fluid_solar
                 no_fluid = (self.plant.fluid_solar is None)
-                plant_out.mean_fluid_density = None if no_fluid else self.plant.fluid_solar.get_density(te_s)[0]
-                plant_out.mean_fluid_heat_capacity = None if no_fluid else \
-                    self.plant.fluid_solar.get_heat_capacity(te_s)[0]
+                plant_out.mean_fluid_density = None if no_fluid else Q(self.plant.fluid_solar.get_density(te_s)[0])
+                plant_out.mean_fluid_heat_capacity = None if no_fluid else Q(
+                    self.plant.fluid_solar.get_heat_capacity(te_s)[0])
             finally:
                 plant_out.defer_post_config_changed_actions = False
 
         out.plant_output = plant_out
 
         # Array results
         array_results = []
         for array in self.plant.arrays:
             arr_out = results.PCMethodOutputArray()
             arr_out.array = array
 
             if has_intervals:
                 df = self._output['arrays'][array]
+                # This is necessary to prevent unneeded calls of config_virtuals -> PCMethodOutputArray is AttrSetterMixin
                 try:
-                    # Necessary to prevent unneeded calls of config_virtuals -> PCMethodOutputArray is AttrSetterMixin
                     plant_out.defer_post_config_changed_actions = True
                     if 'tp_sp_measured' in df.columns:
-                        arr_out.tp_sp_measured = df['tp_sp_measured'].astype(OutputUnits.tp_sp)
+                        arr_out.tp_sp_measured = df['tp_sp_measured'].astype(unit_tp_sp)
                         arr_out.mean_tp_sp_measured = arr_out.tp_sp_measured.mean()
                     else:
                         arr_out.tp_sp_measured = None
                         arr_out.mean_tp_sp_measured = None
 
-                    arr_out.tp_sp_estimated = df['tp_sp_estimated'].astype(OutputUnits.tp_sp)
-                    arr_out.tp_sp_estimated_safety = df['tp_sp_estimated_safety'].astype(OutputUnits.tp_sp)
+                    arr_out.tp_sp_estimated = df['tp_sp_estimated'].astype(unit_tp_sp)
+                    arr_out.tp_sp_estimated_safety = df['tp_sp_estimated_safety'].astype(unit_tp_sp)
                     arr_out.mean_tp_sp_estimated = arr_out.tp_sp_estimated.mean()
                     arr_out.mean_tp_sp_estimated_safety = arr_out.tp_sp_estimated_safety.mean()
-
-                    data = results.PCMethodOutputData()
-                    # Data columns that are always returned by Formula
-                    for col in ['te_amb', 'te_in', 'te_out', 'te_op', 'te_op_deriv', 'aoi', 'iam_b']:
-                        data.__setattr__(col, df[col])
-                    # Data columns that might be None
-                    for col in ['rd_gti', 'rd_bti', 'rd_dti', 've_wind']:
-                        data.__setattr__(col, df[col] if col in df.columns else None)
-                    arr_out.data = data
-
                 finally:
                     plant_out.defer_post_config_changed_actions = False
 
             array_results.append(arr_out)
 
         out.array_output = array_results
 
@@ -611,15 +614,15 @@
 
 class PCMethodISO(PCMethod):
     """This PC method implementation aligns as strictly as possible to the method as defined in the technical
     standard ISO 24194.
     """
 
     method_name = "PC Method 'ISO 24194'"
-    mode = PCMethods.iso
+    mode = AvailablePCMethods.iso
 
     def _aggregate_candidates(self, s, agg_fun):
         s = s.resample(self.settings.interval_length, closed='right', label='right').aggregate(agg_fun)
         return s
 
     def _select_best_intervals(self):
         """Due to the fixed-hour resampling pattern used in the PCMethodISO data aggregation, we have no overlapping
@@ -630,37 +633,37 @@
 
 class PCMethodExtended(PCMethod):
     """This class implements the "extended" variant of the Performance Check method, with improvements in data
     analysis. See class docstring for more info.
     """
 
     method_name = "PC Method 'ISO 24194' extended"
-    mode = PCMethods.extended
+    mode = AvailablePCMethods.extended
 
     def _aggregate_candidates(self, s, agg_fun='mean'):
         s_out = s.rolling(window=self.settings.interval_length, closed='right').aggregate(agg_fun)
         # For some reason, the "rolling" operation drops the pint dtype.
         if agg_fun not in ['sum', 'count']:
             s_out = s_out.astype(s.dtype)
         return s_out
 
     def _select_best_intervals(self):
         """Due to the rolling averaging used in the PCMethodExtended data aggregation, we might have overlapping
         intervals that fulfill all ISO 24194 requirements.
         This algorithm ranks intervals according to a score, which is relative standard deviation of thermal power,
-        evaluated over the interval.
+        evaluiated over the interval.
         This algorithm chooses the best interval, then excludes all overlapping intervals, goes on with the
         remaining intervals etc. until no intervals are left.
         """
         # Intervals that fulfill all constraints so far:
         is_candidate = self._mask.all(axis='columns')
 
         # Criterion to find "best" interval among overlapping: smallest relative standard deviation of plant power.
         tp = self.plant.tp.data
-        variation = (self._aggregate_candidates(tp, 'std') / self._aggregate_candidates(tp, 'mean')).astype(
+        variation = (self._aggregate_candidates(tp, 'mean') / self._aggregate_candidates(tp, 'std')).astype(
             'float64')
         score = 1 / variation
         score[~is_candidate] = 0
 
         # Iteratively add the best interval and remove overlapping intervals from candidates.
         idx = self.plant.time_index
         best_intervals_mask = pd.Series(index=idx, data=False)
```

## sunpeek/core_methods/pc_method/plotting.py

```diff
@@ -1,1304 +1,657 @@
-"""Module for creating plots and pdf report for the PC method.
-
-Notes
------
-- Main method is :fun:`create_pdf_report`.
-- Plots can be created individually by functions like :fun:`plot_cover()` etc., or also
-by using the methods in the :class:`PCPlots` class. The functions are thin wrappers around the class methods.
-- Settings are documented in the :class:`plot_utils.PlotSettings` class.
-- Tests for this module are in :mod:`test_pc__FHW_plot.py`
-- Only tested for FHW plant. Not guaranteed to work if `array.tp` is None.
+"""Plotting module. Development plots only.
 """
-from collections import OrderedDict
-from typing import Union, Optional, List
+
+import warnings
+from pathlib import Path
 import numpy as np
 import pandas as pd
 import datetime as dt
-from pathlib import Path
-import pendulum
-import pytz
-
-import matplotlib.figure
+import pint
 import matplotlib.pyplot as plt
 import matplotlib.dates as mdates
-import matplotlib.patches as mpatches
-import matplotlib.ticker as mtick
-from matplotlib.offsetbox import AnnotationBbox, TextArea, HPacker, VPacker, OffsetImage
-from matplotlib.colors import LinearSegmentedColormap
-
-from sunpeek.common.errors import PCMethodError
-from sunpeek.common.utils import sp_logger
-from sunpeek.common import plot_utils as pu
-from sunpeek.common.unit_uncertainty import Q
-import sunpeek.components.outputs_pc_method as results
-from sunpeek.core_methods.pc_method.main import METHOD_DESCRIPTION
-
-
-def create_pdf_report(pc_output: results.PCMethodOutput,
-                      settings: Optional[pu.PlotSettings] = None,
-                      fig_list: List[matplotlib.figure.Figure] = None,
-                      filename: str | None = None,
-                      export_folder: Union[str, Path] = None,
-                      add_page_numbers: bool = True,
-                      add_page_number_first_page: bool = False,
-                      **kwargs,
-                      ) -> Optional[Path]:
-    """Create PC method report, with page numbers and metadata.
-
-    Parameters
-    ----------
-    pc_output : results.PCMethodOutput
-        Main object to hold the data used to create the plots.
-    settings: :class:`pu.PlotSettings`
-        Settings used by the various :class:`PCPlots` methods.
-    fig_list: List[matplotlib.figure.Figure]
-        List of matplotlib figures used to create
-        If `fig_list` is None, will create all figures using :func:`plot_all`.
-    filename: str = 'pc_report', optional
-        The generated pdf report will be saved under this name, with extension ".pdf".
-        If None, a default filename is generated, based on the PC settings.
-    export_folder: Union[str, Path], optional
-        Folder to which the pdf file is saved. If None, a temporary folder is used.
-    add_page_numbers: bool, optional
-        If True, page numbers are added
-    add_page_number_first_page: bool, optional
-        If False, first report page (cover page, see :func:`plot_cover`) has no page number.
-
-    Notes
-    -----
-    - kwargs are passed to individual plot methods of PCPlots class via `plot_all()`.
-    - square_axis_range : List
-        Axes limits (minimum and maximum), used for x and y axis in :func:`plot_square`.
-    - y_ratio_limits : List
-        Axes limits (minimum and maximum), used for y axis in :func:`plot_time`.
-    - axes_limits_interval_plots : dict
-        Maximum y axis limits, used for the subplot axes in :func:`plot_intervals`.
-        Must contain keys 'te_max', 'rd_max', 'tp_max', 'vf_sp_max'.
-    """
-    pc_plots = PCPlots(pc_output, settings)
-    fig_list = fig_list or pc_plots.plot_all(**kwargs)
-    if not fig_list:
-        raise ValueError('Cannot produce report: No report page plots given, or none returned from plot_all().')
-
-    # pdf Metadata
-    metadata = {'Title': METHOD_DESCRIPTION,  # f'Thermal Power Check {pu.iso_string}',
-                'Author': f'SunPeek, {pu.SUNPEEK_URL}',
-                'Creator': f'SunPeek, {pu.SUNPEEK_URL}',
-                'Subject': f'Performance Check for Large Solar Thermal Plants according to {pu.ISO_STRING}',
-                'Producer': 'SunPeek using matplotlib',
-                'CreationDate': dt.datetime.now(tz=pytz.timezone(pc_output.plant.local_tz_string_with_DST)),
-                'Keywords': '',
-                }
-
-    # Create pdf document
-    full_fn = pu.create_pdf(fig_list=fig_list,
-                            filename=filename or default_filename(pc_output, pc_plots.settings.with_interval_plots),
-                            export_folder=export_folder,
-                            add_page_numbers=add_page_numbers,
-                            add_page_number_first_page=add_page_number_first_page,
-                            metadata=metadata,
-                            )
-
-    sp_logger.info(f'Saved PC report to "{full_fn}"')
-
-    return full_fn
-
-
-def default_filename(pc_output: results.PCMethodOutput,
-                     with_interval_plots: bool) -> str:
-    return (f'PC_report, {pc_output.plant.name}, '
-            f'{pc_output.evaluation_mode}, '
-            f'formula_{pc_output.formula}, '
-            f'wind_{"used" if pc_output.wind_used else "ignored"}'
-            f'{"" if not with_interval_plots else ", with_interval_plots"}')
-
-
-def plot_all(pc_output: results.PCMethodOutput,
-             settings: Optional[pu.PlotSettings] = None,
-             **kwargs,
-             ) -> Optional[List[matplotlib.figure.Figure]]:
-    """Produce "all" PC figures that should go into a report.
-    """
-    return PCPlots(pc_output, settings).plot_all(**kwargs)
+import matplotlib.transforms as transforms
 
+from sunpeek.common.utils import ROOT_DIR, sp_logger
+from sunpeek.components.results import PCMethodOutput
 
-def plot_cover(pc_output: results.PCMethodOutput, settings: Optional[pu.PlotSettings] = None,
-               ) -> Optional[pu.PlotResult]:
-    """Cover page for Power Check report.
-    include_creation_date : Overwrites value in settings if not None.
-    """
-    return PCPlots(pc_output, settings).plot_cover()
-
+PLOTS_MISSING_TXT = 'Modules "plotly" or "pendulum" not found. Install them to use development plots.'
 
-def plot_bars(pc_output: results.PCMethodOutput, settings: Optional[pu.PlotSettings] = None,
-              ) -> Optional[List[matplotlib.figure.Figure]]:
-    """Plot overview result plot, horizontal bars with average powers in PC intervals, measured vs. estimated.
-    """
-    return PCPlots(pc_output, settings).plot_bars()
-
-
-def plot_shadow_and_intervals(pc_output: results.PCMethodOutput, settings: Optional[pu.PlotSettings] = None,
-                              ) -> Optional[List[matplotlib.figure.Figure]]:
+try:
+    from plotly import graph_objects as go
+    from plotly.subplots import make_subplots
+    import pendulum
+
+    plots_available = True
+except ModuleNotFoundError:
+    warnings.warn(PLOTS_MISSING_TXT)
+    plots_available = False
+
+
+def assert_modules():
+    if not plots_available:
+        raise ModuleNotFoundError(PLOTS_MISSING_TXT)
+
+
+# Color: matplotlib style 'tableau-colorblind10'
+# Style: https://viscid-hub.github.io/Viscid-docs/docs/dev/styles/tableau-colorblind10.html
+# Color names https://stackoverflow.com/questions/74830439/list-of-color-names-for-matplotlib-style-tableau-colorblind10
+COLORS = {
+    'blue': '#5F9ED1',
+    'sky_blue': '#006BA4',
+    'sail_blue': '#A2C8EC',
+    'pumpkin': '#FF800E',
+    'orange': '#C85200',
+    'cheese': '#FFBC79',
+    'gray': '#595959',
+    'warm_gray': '#898989',
+    'light_gray': '#CFCFCF',
+    'dark_gray': '#ABABAB',
+    'almost_black': '#373737',
+}
+
+FULL_WIDTH = 16.59 / 2.54  # full page width, cm to inches
+FONT_SIZE = 6
+plt.rcParams.update({
+    'figure.dpi': 600,
+    # 'font.family': 'Times New Roman',
+    'font.family': 'Open Sans',
+    'font.size': FONT_SIZE,
+    'text.usetex': False,
+    'axes.labelsize': FONT_SIZE,
+    'axes.titlesize': FONT_SIZE,
+    'axes.labelpad': 2,
+    'axes.titlepad': 4,
+    'axes.linewidth': 0.5,
+    'xaxis.labellocation': 'left',
+    'grid.alpha': 0.5,
+    'grid.linewidth': 0.5,
+    'lines.linewidth': 0.75,
+    'patch.linewidth': 0.25,
+    'legend.fontsize': FONT_SIZE - 1,
+    'legend.title_fontsize': FONT_SIZE - 1,
+    'legend.framealpha': 0.8,
+    'legend.borderpad': 0.2,
+    'legend.columnspacing': 1.5,
+    'legend.labelspacing': 0.25,
+    'xtick.labelsize': FONT_SIZE - 1,
+    'ytick.labelsize': FONT_SIZE,
+    'xtick.major.size': 2,
+    'xtick.major.pad': 1,
+    'xtick.major.width': 0.5,
+    'ytick.major.size': 2,
+    'ytick.major.pad': 1,
+    'ytick.major.width': 0.5,
+    'xtick.minor.size': 2,
+    'xtick.minor.pad': 1,
+    'xtick.minor.width': 0.5,
+    'ytick.minor.size': 2,
+    'ytick.minor.pad': 1,
+    'ytick.minor.width': 0.5,
+})
+# Avoid black unless necessary
+# Taken from https://atchen.me/research/code/data-viz/2022/01/04/plotting-matplotlib-reference.html
+plt.rcParams.update({
+    'text.color': COLORS['almost_black'],
+    'patch.edgecolor': COLORS['gray'],
+    'patch.force_edgecolor': True,
+    'hatch.color': COLORS['almost_black'],
+    'axes.edgecolor': COLORS['almost_black'],
+    'axes.labelcolor': COLORS['almost_black'],
+    'xtick.color': COLORS['almost_black'],
+    'ytick.color': COLORS['almost_black']
+})
+
+
+def plot_all(pc_output: PCMethodOutput,
+             mode='screen',
+             show_image=True,
+             write_image=False,
+             use_safety=True,
+             anonymize=False):
+    """Create all defined plots.
+    """
+    assert_modules()
+    assert mode in ['screen', 'presentation']
+    if pc_output is None or (pc_output.plant_output.n_intervals == 0):
+        sp_logger.info('Nothing to plot, no Performance Check intervals found.')
+        # print('Nothing to plot, no Performance Check intervals found.')
+        return
+
+    settings = dict(
+        mode=mode,
+        show_image=show_image,
+        write_image=write_image,
+        use_safety=use_safety,
+        anonymize=anonymize,
+    )
+
+    figures = dict(
+        square=plot_square(pc_output, **settings),
+        time=plot_time(pc_output, **settings),
+        sums=plot_sums(pc_output, **settings),
+    )
+
+    return figures
+
+
+def plot_square(pc_output,
+                mode: str,
+                use_safety=True,
+                show_image=True,
+                write_image=False,
+                anonymize=False,
+                axis_range=None,
+                tick0=None):
+    """Plot measured vs. estimated power in intervals + trend line.
+    """
+
+    assert_modules()
+    if pc_output is None or (pc_output.plant_output.n_intervals == 0):
+        print('Nothing to plot, no PC intervals found.')
+        return
+
+    if axis_range is None:
+        axis_range = [250, 650]
+    if tick0 is None:
+        tick0 = 300
+
+    pout = pc_output.plant_output
+    measured = pout.tp_sp_measured.magnitude
+    estimated = pout.tp_sp_estimated_safety.magnitude if use_safety else pout.tp_sp_estimated.magnitude
+    # slope = pout.target_actual_slope_safety.magnitude if use_safety else pout.target_actual_slope.magnitude
+    font_size = 24 if mode == 'presentation' else 14
+
+    fig = go.Figure()
+    fig.add_scatter(x=estimated, y=measured,
+                    name=f"Interval averages ("
+                         f"{pendulum.duration(seconds=pc_output.settings['interval_length'].total_seconds()).in_words(locale='en')}"
+                         f")",
+                    mode='markers', marker_size=10, marker_opacity=0.8)
+
+    fig.update_layout(legend=dict(
+        orientation="h",
+        yanchor="bottom",
+        y=1.01,
+        xanchor="right",
+        x=1
+    ))
+
+    # Linear trendline with 0 intercept
+    # rng = [estimated.min(), estimated.max()]
+    # fig.add_scatter(x=rng, y=slope * np.array(rng),
+    #                 name=f'y = {slope:.3f} x',
+    #                 mode='lines', line_color='grey', line_width=3)
+
+    # Bisection line
+    fig.add_scatter(x=axis_range, y=axis_range, name='', showlegend=False,
+                    mode='lines',
+                    # line_dash='dash',
+                    line_color='grey', line_width=0.5)
+    fig.update_layout(
+        width=1000,
+        height=1000,
+        autosize=False,
+        xaxis=dict(
+            type='linear',
+            constrain="domain",
+            title="<b>Estimated</b> power [W/m]",
+            linecolor="#BCCCDC",
+            range=axis_range,
+            title_font_size=font_size,
+            tickfont_size=font_size,
+            tick0=tick0,
+            dtick=100,
+        ),
+        yaxis=dict(
+            type='linear',
+            constrain="domain",
+            title="<b>Measured</b> power [W/m]",
+            title_font_size=font_size,
+            title_standoff=30,
+            linecolor="#BCCCDC",
+            range=axis_range,
+            scaleanchor="x",
+            scaleratio=1,
+            autorange=False,
+            tickfont_size=font_size,
+            tick0=tick0,
+            dtick=100,
+        ),
+        legend_font_size=font_size,
+    )
+
+    # fig.update_xaxes(
+    #     showspikes=True,
+    #     spikecolor="grey",
+    #     spikesnap="cursor",
+    #     spikemode="across",
+    #     spikedash="solid",
+    # )
+    # fig.update_yaxes(
+    #     showspikes=True,
+    #     spikecolor="grey",
+    #     spikesnap="cursor",
+    #     spikemode="across",
+    #     spikedash="solid",
+    # )
+
+    if show_image:
+        _show(fig)
+    if write_image:
+        fig.write_image(_get_filename('square', pc_output))
+
+    return fig
+
+
+def plot_time(pc_output,
+              mode: str,
+              use_safety=True,
+              show_image=True,
+              write_image=False,
+              anonymize=False,
+              plot_trend=True,
+              yrange=None,
+              ):
     """Plots ratio of measured vs. estimated power over time.
     """
-    return PCPlots(pc_output, settings).plot_shadow_and_intervals()
-
-
-def plot_square(pc_output: results.PCMethodOutput, settings: Optional[pu.PlotSettings] = None,
-                axis_range: Optional[List] = None,
-                ) -> Optional[List[matplotlib.figure.Figure]]:
-    """Plot measured vs. estimated yield in intervals. Always creates 2 subplots (with, w/o f_safe).
-    """
-    return PCPlots(pc_output, settings).plot_square(axis_range)
-
-
-def plot_time(pc_output: results.PCMethodOutput, settings: Optional[pu.PlotSettings] = None,
-              y_ratio_limits: Optional[List] = None,
-              ) -> Optional[List[matplotlib.figure.Figure]]:
-    """Plot ratio of measured vs. estimated power over time.
-    """
-    return PCPlots(pc_output, settings).plot_time(y_ratio_limits)
-
-
-def plot_plant_overview(pc_output: results.PCMethodOutput, settings: Optional[pu.PlotSettings] = None,
-                        ) -> Optional[List[matplotlib.figure.Figure]]:
-    """Plant overview page: plant details, arrays + areas etc.
-    """
-    return PCPlots(pc_output, settings).plot_plant_overview()
-
-
-def plot_collector_overview(pc_output: results.PCMethodOutput, settings: Optional[pu.PlotSettings] = None,
-                            ) -> Optional[List[matplotlib.figure.Figure]]:
-    """Collector overview page: collector details, IAMs etc.
-    """
-    return PCPlots(pc_output, settings).plot_collector_overview()
-
-
-def plot_data_overview(pc_output: results.PCMethodOutput, settings: Optional[pu.PlotSettings] = None,
-                       ) -> Optional[List[matplotlib.figure.Figure]]:
-    """Page with considered data: Table with data points considered, on the basis of ISO 24194 Annex A.
-    """
-    return PCPlots(pc_output, settings).plot_data_overview()
-
-
-def plot_symbols_overview(pc_output: results.PCMethodOutput, settings: Optional[pu.PlotSettings] = None,
-                          ) -> Optional[List[matplotlib.figure.Figure]]:
-    """Page with symbols / abbreviations used in the report.
-    """
-    return PCPlots(pc_output, settings).plot_symbols_overview()
-
-
-def plot_intervals(pc_output: results.PCMethodOutput, settings: Optional[pu.PlotSettings] = None,
-                   axes_limits: Optional[List] = None,
-                   ) -> Optional[List[matplotlib.figure.Figure]]:
-    """Plot full-resolution data for temperatures, power, volume flow etc. for all intervals.
-    """
-    return PCPlots(pc_output, settings).plot_intervals(axes_limits)
-
+    assert_modules()
+    if pc_output is None or (pc_output.plant_output.n_intervals == 0):
+        print('Nothing to plot, no PC intervals found.')
+        return
+
+    if yrange is None:
+        yrange = [0.8, 1.2]
+
+    pout = pc_output.plant_output
+    estimated = pout.tp_sp_estimated_safety.magnitude if use_safety else pout.tp_sp_estimated.magnitude
+    measured = pout.tp_sp_measured.magnitude
+    ratio = measured / estimated
+    # Plotting ratio against midpoint of intervals
+    time_display = pout.datetime_intervals_start + 0.5 * pc_output.settings['interval_length']
+    font_size = 24 if mode == 'presentation' else 14
+
+    fig = go.Figure()
+    fig.add_scatter(x=time_display, y=ratio,
+                    mode='markers', marker_size=10, marker_opacity=0.5,
+                    showlegend=False)
+
+    if plot_trend:
+        rm = pd.Series(data=ratio, index=time_display) \
+            .rolling(dt.timedelta(days=45), min_periods=25, center=True, closed='both').median()
+        fig.add_scatter(y=rm, x=rm.index, mode='lines', line_color='grey', line_width=5, showlegend=False)
+
+    fig.update_layout(
+        width=2000,
+        height=600,
+        xaxis=dict(title="",
+                   title_font_size=font_size,
+                   tickfont_size=font_size,
+                   linecolor="#BCCCDC"),
+        yaxis=dict(title="<b>Ratio measured vs. estimated</b> power [-]",
+                   title_standoff=30,
+                   title_font_size=font_size,
+                   tickfont_size=font_size,
+                   tickformat='0%',
+                   range=yrange,
+                   dtick=0.1,
+                   linecolor="#BCCCDC")
+
+    )
+    if show_image:
+        _show(fig)
+    if write_image:
+        fig.write_image(_get_filename('time', pc_output))
+
+    return fig
+
+
+def plot_sums(pc_output,
+              mode: str,
+              use_safety=False,
+              show_image=True,
+              write_image=False,
+              anonymize=False,
+              ):
+    """Plot sums / average powers in intervals, measured vs. estimated.
+    """
+    assert_modules()
+    if pc_output is None or (pc_output.plant_output.n_intervals == 0):
+        print('Nothing to plot, no PC intervals found.')
+        return
+
+    pout = pc_output.plant_output
+    # estimated = np.mean(pout.tp_sp_estimated.magnitude)
+    estimated_safe = np.mean(pout.tp_sp_estimated_safety.magnitude)
+    measured = np.mean(pout.tp_sp_measured.magnitude)
+
+    fig = make_subplots(rows=1, cols=2, specs=[[{}, {}]],
+                        shared_xaxes=False,
+                        shared_yaxes=False, vertical_spacing=0.001)
+
+    font_size = 18 if mode == 'presentation' else 14
+    labels = ['Average power<br><b>measured</b>', 'Average power<br><b>estimated</b>']
+    x = [measured, estimated_safe]
+    colors = ['rgba(192, 0, 0, 0.6)', 'rgba(68, 114, 196, 0.6)']
+
+    text = [str(np.round(val, decimals=1)) + ' W/m' for val in x]
+    fig.append_trace(go.Bar(
+        x=x,
+        y=labels,
+        marker=dict(
+            color=colors,
+            line=dict(color='rgba(90, 90, 90, 1.0)', width=1),
+        ),
+        orientation='h',
+        width=0.4,
+        text=text,
+        textposition="outside",
+        insidetextanchor="end",
+        outsidetextfont_size=font_size
+    ), 1, 1)
+
+    plant_name = '<anonymized>' if anonymize else pout.plant.name
+    if plant_name.startswith('FHW Arcon South'):
+        plant_name = 'FHW Fernheizwerk Graz, Arcon South'
+    title = (f"<b>Check of Performance</b> ({pc_output.pc_method_name}).<br>"
+             f"Mode: {pc_output.evaluation_mode}. "
+             f"Equation: {pc_output.equation}. "
+             f"Wind: {'' if pc_output.wind_used else 'Not '}used.<br>"
+             # f"Plant name: <b>{plant_name}</b>.<br>"
+             f"n={pout.n_intervals} intervals. Interval duration: "
+             f"{pendulum.duration(seconds=pc_output.settings['interval_length'].total_seconds()).in_words(locale='en')}."
+             f"<br>Data from {pc_output.datetime_eval_start.tz_convert(pout.plant.tz_data)} to "
+             f"{pc_output.datetime_eval_end.tz_convert(pout.plant.tz_data)}.<br>"
+             )
+    axis_range = [0, np.max(x).round(-1) + 100]
+    fig.update_layout(
+        title=dict(text=title,
+                   xanchor='left',
+                   xref='paper',
+                   x=fig.layout.xaxis.domain[0],
+                   # yanchor='top',
+                   yanchor='bottom',
+                   y=fig.layout.height,
+                   font_size=font_size + 1,
+                   ),
+        autosize=False,
+        height=700,
+        width=2400,
+        margin=dict(l=250, r=20, t=70, b=250),
+        yaxis=dict(
+            tickfont=dict(size=font_size),
+            title_standoff=30,
+            showgrid=False,
+            showline=False,
+            showticklabels=True,
+            domain=[0, 0.85],
+        ),
+        xaxis=dict(
+            title=dict(text='Specific thermal power [W/m]',
+                       font_size=font_size,
+                       ),
+            # title='Specific thermal power [W/m]',
+            title_font_size=font_size - 1,
+            tickfont=dict(size=font_size),
+            range=axis_range,
+            zeroline=False,
+            showline=False,
+            showticklabels=True,
+            showgrid=True,
+            domain=[0, 0.42],
+        ),
+    )
+
+    # Adding labels
+    # x_power = np.round([measured, estimated], decimals=1)
+    # for xp, yp in zip(x_power, labels):
+    #     # labeling the bar net worth
+    #     annotations.append(dict(xref='x1', yref='y1',
+    #                             y=yp, x=xp + 10,
+    #                             text=str(xp) + ' W/m',
+    #                             align="left",
+    #                             font=dict(family='Arial', size=font_size,
+    #                                       color='rgb(0, 0, 0)'),
+    #                             showarrow=False))
+
+    # Guarantee fulfilled statement
+    ratio = measured / estimated_safe
+    fulfill_txt = 'not ' if ratio < 1 else ''
+    min_intervals_ok = (pout.n_intervals >= pc_output.settings['min_intervals_in_output'])
+    ratio_text = (f'<b>Performance Check {fulfill_txt}fulfilled:'
+                  f'</b> Ratio measured / estimated power = {ratio:.1%}'
+                  f'<br>Combined safety factor f<sub>safe</sub> = {pc_output.settings["safety_combined"]:.2} '
+                  f'taken into account.'
+                  f'<br>{pout.n_intervals} intervals found: '
+                  f'The minimum number of intervals ({pc_output.settings["min_intervals_in_output"]}) '
+                  f'has {"" if min_intervals_ok else "not "}been reached.'
+                  )
+    # fig.add_annotation(dict(
+    #     text=ratio_text,
+    #     font_size=font_size,
+    #     yref='paper',
+    #     yanchor="bottom",
+    #     y=-1.02,
+    #     xref='paper',
+    #     xanchor="left",
+    #     x=0,
+    #     # align='left',
+    #     showarrow=False))
+
+    # Calculate position of annotation
+    fig.add_annotation(
+        text=ratio_text,
+        font_size=font_size,
+        xref='paper',
+        x=fig.layout.xaxis.domain[0],
+        align='left',
+        # xanchor='left',
+        y=-230 / fig.layout.height,
+        yref='paper',
+        showarrow=False,
+    )
+
+    fig.update_layout(showlegend=False)
+
+    if show_image:
+        _show(fig)
+    if write_image:
+        fig.write_image(_get_filename('sums', pc_output))
+
+    return fig
+
+
+def plot_mask(pc_output, mask):
+    """Produces a subplot figure with the PC criteria mask.
+    mask is usually pc_obj._mask
+    """
+    assert_modules()
+    if pc_output is None or (pc_output.plant_output.n_intervals == 0):
+        print('Nothing to plot, no PC intervals found.')
+        return
+
+    subplot_cols = mask.columns
+    fig = make_subplots(rows=1 + len(subplot_cols), shared_xaxes=True, subplot_titles=subplot_cols)
+
+    t = mask.index
+    for i, col in enumerate(subplot_cols):
+        fig.add_scatter(x=t, y=mask[col], row=i + 1, col=1,
+                        mode='lines+markers', line={'width': 1}, marker={'size': 3})
+        fig.add_scatter(x=t, y=mask[col], row=i + 1, col=1,
+                        mode='lines', line={'width': 0.5})
+
+    fig.layout.update(showlegend=False)
+    _show(fig)
+
+    return fig
+
+
+def format_xaxis(ax,
+                 tz,
+                 interval,
+                 # date_start=DAY_START, date_end=DAY_END,
+                 minor_locator=mdates.MinuteLocator(interval=5),
+                 major_locator=mdates.MinuteLocator(interval=15),
+                 major_formatter=None):
+    ax.set_xlim(interval[0], interval[1], auto=None)
+    if major_formatter is None:
+        major_formatter = mdates.DateFormatter("%#H:%M", tz=tz)
+    ax.xaxis_date(tz)
+    ax.xaxis.set_minor_locator(minor_locator)
+    ax.xaxis.set_major_locator(major_locator)
+    ax.xaxis.set_major_formatter(major_formatter)
+    # adjust_xlabel_positions(ax, 4, remove_last=True)
+
+    # Position extra text below x axis
+    vertical_offset_points = 10
+    offset = transforms.ScaledTranslation(0, vertical_offset_points / 72, ax.figure.dpi_scale_trans)
+    ax.text(0, 0, f'{mdates.num2date(ax.get_xlim()[0]):%Y-%m-%d} ({str(tz)})', ha='left',
+            transform=ax.transAxes - offset, va='top', fontsize=plt.rcParams['axes.labelsize'] - 1)
+
+    return ax
+
+
+def plot_sensor_data(array,
+                     interval,
+                     anonymize=False,
+                     ):
+    """Plot main sensor values in given interval.
+    Using matplotlib here because plotly doesn't behave with subplot grid legends...
+    """
+    a = array
+    p = array.plant
+
+    fig, ax = plt.subplots(ncols=2, nrows=3, sharex=True,
+                           constrained_layout=True,
+                           gridspec_kw=dict(width_ratios=[1, 1], height_ratios=[3, 3, 2]),
+                           )
+    fig.subplots_adjust(left=.15, bottom=.16, right=.99, top=.97)
+
+    def _get_plot_data(sensor, unit):
+        idx = sensor.data.index
+        mask = (idx > interval[0]) & (idx <= interval[1])
+        sub_series = sensor.data.loc[mask]
+        sub_series = sub_series.pint.to(unit).pint.magnitude
+
+        remove_virtual = lambda s: s.split('__', 1)[0] if '__' in s else s
+        sub_series.rename(remove_virtual(sensor.raw_name), inplace=True)
+
+        return sub_series
+
+    def _format_subplot(ax_, title_txt, unit, ylims=None, has_legend=True):
+        ax_.set_title(title_txt, loc='center')
+        print_unit = lambda s: f"[{pint.Unit(s):~P}]" if s else "[dimensionless]"
+        if isinstance(unit, list):
+            ylabel_txt = ", ".join([print_unit(s) for s in unit])
+        else:
+            ylabel_txt = print_unit(unit)
+        ax_.set_ylabel(ylabel_txt)
+        if ylims is not None:
+            ax_.set_ylim(ylims[0], ylims[1])
+        if has_legend:
+            # ax.get_legend().remove()
+            legend = ax_.legend(loc="upper right")
+            legend.get_frame().set_facecolor((1, 1, 1, 0.8))
+            legend.set_zorder(1000)
+        ax_.grid()
+        ax_.set_axisbelow('line')
+
+    # plot data (Fluid Temperatures)
+    ax_ = ax[0][0]
+    unit = 'degC'
+    data = _get_plot_data(a.te_out, unit)
+    ax_.plot(data, color=COLORS['orange'], label=data.name, zorder=10)
+    data = _get_plot_data(a.te_op, unit)
+    ax_.plot(data, color=COLORS['warm_gray'], label=data.name, zorder=15)
+    data = _get_plot_data(a.te_in, unit)
+    ax_.plot(data, color=COLORS['blue'], label=data.name, zorder=5)
+    _format_subplot(ax_, "(a) Fluid Temperatures", unit)
+
+    # plot data (Thermal Power)
+    ax_ = ax[0][1]
+    data = _get_plot_data(p.tp, 'kW')
+    ax_.plot(data, color=COLORS['warm_gray'], label=data.name, zorder=5)
+    _format_subplot(ax_, "(b) Thermal Power", 'kW', has_legend=False)
+
+    # plot data (Solar Radiation)
+    ax_ = ax[1][0]
+    unit = 'W m**-2'
+    if a.rd_gti is not None:
+        data = _get_plot_data(a.rd_gti, unit)
+        ax_.plot(data, color=COLORS['orange'], label=data.name, zorder=15)
+    if a.rd_bti is not None:
+        data = _get_plot_data(a.rd_bti, unit)
+        ax_.plot(data, color=COLORS['cheese'], label=data.name, zorder=10)
+    if a.rd_dti is not None:
+        data = _get_plot_data(a.rd_dti, unit)
+        ax_.plot(data, color=COLORS['dark_gray'], label=data.name, zorder=5)
+    _format_subplot(ax_, "(c) Solar Radiation", unit, ylims=[0, 1200])
+
+    # plot data (Ambient)
+    ax_ = ax[1][1]
+    data = _get_plot_data(p.te_amb, 'degC')
+    ax_.plot(data, color=COLORS['cheese'], label=data.name, zorder=15)
+    if p.ve_wind is not None:
+        data = _get_plot_data(p.ve_wind, 'm s**-1')
+        ax_.plot(data, color=COLORS['light_gray'], label=data.name, zorder=10)
+        _format_subplot(ax_, "(d) Ambient", ['degC', 'm s**-1'])
+    else:
+        _format_subplot(ax_, "(d) Ambient", 'degC')
+
+    # plot data (Angle of Incidence)
+    ax_ = ax[2][0]
+    data = _get_plot_data(a.aoi, 'deg')
+    ax_.plot(data, color=COLORS['almost_black'], label=data.name, zorder=15)
+    _format_subplot(ax_, "(e) Angle of Incidence", 'deg', has_legend=False)
+
+    # plot data (Incidence Angle Modifier)
+    ax_ = ax[2][1]
+    data = _get_plot_data(a.iam, '')
+    ax_.plot(data, color=COLORS['almost_black'], label=data.name, zorder=15)
+    _format_subplot(ax_, "(f) Incidence Angle Modifier", '', has_legend=False)
+
+    fig.set_size_inches(w=FULL_WIDTH, h=FULL_WIDTH / 1.9)
+    fig.tight_layout()
+    format_xaxis(ax[2][0], tz=p.tz_data, interval=interval)
+    format_xaxis(ax[2][1], tz=p.tz_data, interval=interval)
+
+    plant_name = '<anonymized>' if anonymize else array.plant.name
+    if plant_name.startswith('FHW Arcon South'):
+        plant_name = 'FHW Fernheizwerk Graz, Arcon South'
+    title_txt = (f"Sensor data for plant: {plant_name}. "
+                 # f"Data from {interval[0]} to {interval[1]}."
+                 )
+    plt.suptitle(title_txt, fontweight='bold',
+                 x=0.05, y=0.98,
+                 horizontalalignment='left')
+
+    plt.tight_layout()
+
+    plt.savefig(_get_filename('sensor_data'))
+    plt.close()
+    print(f'Saved plot "sensor_data".')
+
+
+def _show(fig):
+    fig.update_layout(dragmode='pan')
+    config = {'scrollZoom': True,
+              'displayModeBar': True,
+              'modeBarButtonsToRemove': ['zoomIn', 'zoomOut'],
+              'modeBarButtonsToAdd': ['drawline',
+                                      'drawrect',
+                                      'eraseshape'
+                                      ]}
+    fig.show(config=config)
+
+
+def _get_filename(plot_name, pc_output=None):
+    """Generate png file name from method, equation and plot name.
+    """
+    save_folder = Path(ROOT_DIR).parent / 'tests' / 'resources'
+    if pc_output is None:
+        filename = f'{plot_name}__{dt.datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
+    else:
+        m = pc_output.evaluation_mode
+        e = pc_output.equation
+        filename = f'{m}_eq{e}__{plot_name}__{dt.datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
 
-class PCPlots:
-    pc_output: results.PCMethodOutput
-    settings: pu.PlotSettings
-
-    def __init__(self, pc_output, settings):
-        self.pc_output = pc_output
-        self.settings = settings or pu.PlotSettings()
-
-    def plot_all(self,
-                 **kwargs
-                 ) -> Optional[List[matplotlib.figure.Figure]]:
-        """Produce "all" PC figures that should go into a report.
-        """
-        if self.no_output():
-            raise PCMethodError('Cannot produce Performance Check report: No intervals found.')
-
-        fig_list = []
-        fig_list.extend(self.plot_cover())
-        fig_list.extend(self.plot_bars())
-        fig_list.extend(self.plot_shadow_and_intervals())
-
-        # Add square and time plots, in plant / array order
-        squares = self.plot_square(kwargs.get('square_axis_range'))
-        times = self.plot_time(kwargs.get('y_ratio_limits'))
-        squares_and_times = [v for pair in zip(squares, times) for v in pair]
-        fig_list.extend(squares_and_times)
-
-        fig_list.extend(self.plot_plant_overview())
-        fig_list.extend(self.plot_collector_overview())
-        fig_list.extend(self.plot_data_overview())
-        fig_list.extend(self.plot_symbols_overview())
-
-        if self.settings.with_interval_plots:
-            fig_list.extend(self.plot_intervals(kwargs.get('axes_limits_interval_plots')))
-
-        return fig_list
-
-    def plot_cover(self,
-                   ) -> Optional[List[matplotlib.figure.Figure]]:
-        """See `plot_cover` function.
-        """
-        if self.no_output():
-            return None
-
-        # Plot
-        fig, params = pu.prepare_figure(settings=self.settings, suppress_footer=True)
-
-        # Title
-        box_title = pu.box_title(METHOD_DESCRIPTION,  # 'Power Check according to ISO 24194',
-                                 fontsize=plt.rcParams['figure.titlesize'] + 3)
-
-        # ISO Text
-        textprops = dict(size=params['font_size_text'], linespacing=pu.Defaults.linespacing_text.value)
-        txt_iso = (
-            f'This report is based on {pu.ISO_STRING} "Solar energy  Collector fields  Check of performance".\n'
-            f'Report generated with SunPeek: {pu.SUNPEEK_URL}'
-            # f'Report generated with the FOSS software SunPeek: {pu.sunpeek_url}'
-        )
-
-        # Plant Text
-        p = self.pc_output.plant
-        txt_plant = OrderedDict([
-            ('Plant name', p.name),
-            ('Plant owner', p.owner),
-            ('Included arrays', f'{", ".join([ao.array.name for ao in self.pc_output.array_output])}'),
-        ])
-        txt_plant = pu.anonymize(txt_plant, do_anonymize=self.settings.anonymize)
-        txt_plant['Measuring period'] = (
-            f'{pu.utc_str(self.pc_output.datetime_eval_start, p.tz_data)} to '
-            f'{pu.utc_str(self.pc_output.datetime_eval_end, p.tz_data)}')
-        box_plant = pu.dict_to_box(txt_plant, textprops=textprops)
-
-        # Method Text
-        txt_method = OrderedDict([
-            ('Date', f'{pendulum.now().to_date_string()}' if self.settings.include_creation_date else ''),
-            ('Algorithm details', self.algorithm_details()),
-            # ('Algorithm details', (
-            #     f'Formula: {self.pc_output.formula}. '
-            #     f'Wind: {"Used" if self.pc_output.wind_used else "Not used"}. '
-            #     f'Mode: {self.pc_output.evaluation_mode}.')),
-            ('Check done by', f'SunPeek version {pu.sunpeek_version}'),
-        ])
-        box_method = pu.dict_to_box(txt_method, textprops=textprops)
-        # Include accuracy_level? Only once it is included in web-ui.
-
-        box_main = VPacker(children=[
-            box_plant,
-            box_method,
-            TextArea(txt_iso, textprops=textprops),
-        ], pad=0, sep=pu.Defaults.sep_minor.value)
-
-        # Logo
-        box_logo = OffsetImage(plt.imread(pu.logo_path), zoom=0.25)
-        box_logo.set_url(pu.SUNPEEK_URL)
-
-        # Place objects
-        box = VPacker(children=[box_title,
-                                box_main,
-                                box_logo,
-                                ], pad=0, sep=pu.Defaults.sep_huge.value)
-
-        artist = pu.annotation_bbox(box, xy=(0.15, 0.8))
-        fig.add_artist(artist)
-
-        return [fig]
-
-    def plot_bars(self) -> Optional[List[matplotlib.figure.Figure]]:
-        """See `plot_bars` function.
-        """
-        if self.no_output():
-            return None
-
-        # Data to plot
-        tp_sp_estimated_safety = np.mean(self.pc_output.plant_output.tp_sp_estimated_safety.magnitude)
-        tp_sp_measured = np.mean(self.pc_output.plant_output.tp_sp_measured.magnitude)
-        tp_ratio = tp_sp_measured / tp_sp_estimated_safety
-        min_intervals_ok = self.pc_output.plant_output.n_intervals >= self.pc_output.settings['min_intervals_in_output']
-
-        # Plot
-        fig, params = pu.prepare_figure(settings=self.settings)
-
-        # Title
-        box = VPacker(children=[pu.box_title(METHOD_DESCRIPTION)],  # 'Power Check ISO 24194')],
-                      pad=0, sep=pu.Defaults.sep_major.value)
-        artist = pu.annotation_bbox(box, xy=pu.Defaults.xy_topleft.value)
-        fig.add_artist(artist)
-        title_artist = artist  # saved for use further down
-
-        # Text above plot
-        box_check = TextArea(f'Power Check {"not " if tp_ratio < 1 else ""}fulfilled:',
-                             textprops=dict(weight='bold', size=params['font_size_text']))
-
-        textprops = dict(size=params['font_size_text'], linespacing=pu.Defaults.linespacing_text.value)
-        box_note = TextArea((f'Ratio measured / estimated power = {float(tp_ratio):.1%}'
-                             f'\nThis takes a combined safety factor '
-                             r'$f_{safe} = '
-                             rf'{float(self.pc_output.settings["safety_combined"]):.2}$ into account.'
-                             f'\nThe minimum number of intervals ({self.pc_output.settings["min_intervals_in_output"]}, '
-                             f'defined in {pu.ISO_STRING}) '
-                             f'has {"" if min_intervals_ok else "not "}been reached: '
-                             f'n={self.pc_output.plant_output.n_intervals} intervals found, each '
-                             f'{pendulum.duration(seconds=self.pc_output.settings["interval_length"].total_seconds()).in_words(locale="en")}'
-                             f' long.'
-                             ), textprops=textprops)
-        box = VPacker(children=[box_check, box_note],
-                      pad=0, sep=pu.Defaults.sep_minor.value)
-        artist = pu.annotation_bbox(box, xy=pu.get_xy_below(artist, vsep=pu.Defaults.sep_major.value))
-        fig.add_artist(artist)
-
-        # Bars plot
-        rect = pu.get_rectangle_below(artist, vsep=pu.Defaults.sep_huge.value, bottom=0.45)
-        ax = fig.add_axes(rect)
-
-        bar_labels = [f'Average power estimated, '
-                      r'with $f_{safe}$',
-                      'Average power measured']
-        bar_data = [tp_sp_estimated_safety, tp_sp_measured]
-        bar_colors = [pu.Colors.gray, pu.Colors.red]
-        data_tips = [f'{d:.1f} W/m' for d in bar_data]
-        ax.barh(bar_labels, bar_data, color=bar_colors, height=0.5, zorder=2, alpha=1)
-
-        ax.set_ylim([-0.4, 1.4])
-        ax.set_xlabel('Specific thermal power [W/m]', ha='center')
-        ax.grid(axis='x')
-        ax.set_axisbelow('line')
-
-        # Place y axis labels inside axes
-        ax.set_yticks([])
-        for y, label in enumerate(bar_labels):
-            ab = AnnotationBbox(
-                TextArea(label, textprops={'fontsize': params['font_size_text'], 'va': 'baseline'}),
-                xy=(0, y), xycoords='data',
-                xybox=(10, 0), boxcoords='offset points', box_alignment=(0, 0.5),
-                pad=0, zorder=10)
-            ab.patch.set_boxstyle('round,pad=0.5')
-            ab.patch.set_edgecolor(pu.Colors.almost_black)
-            ab.patch.set_alpha(0.8)
-            ax.add_artist(ab)
-
-        # Add data-tip text at the right end of the bars
-        txt = []
-        for i, tip in enumerate(data_tips):
-            txt.append(ax.text(bar_data[i] * 1.01, i, tip, ha='left', va='center',
-                               size=plt.rcParams['axes.labelsize']))
-        # Adjust xlims to accommodate for datatip texts
-        plt.draw()  # Needed because matplotlib doesn't know how big the text is without drawing...
-        bb = [t.get_window_extent() for t in txt]
-        max_data = [ax.transData.inverted().transform(b.max) for b in bb]
-        ax.set_xlim([0, max([md[0] for md in max_data])])
-        # Include at least next minor tick
-        minor_tick = pd.Series(ax.get_xticks(minor=True)).diff()[1]
-        ax.set_xlim([0, np.ceil(ax.get_xlim()[1] / minor_tick) * minor_tick])
-
-        # Text at bottom: Guarantee fulfilled statement
-        plant_name = '<anonymized>' if self.settings.anonymize else self.pc_output.plant.name
-        tz = self.pc_output.plant.tz_data
-        array_names = [f'"{ao.array.name}"' for ao in self.pc_output.array_output]
-        box_plant = TextArea((f'Plant name: "{plant_name}".\n'
-                              f'Included arrays: {", ".join(array_names)}.\n'
-                              # f'n={self.pc_output.plant_output.n_intervals} intervals. '
-                              # f"Total interval duration: "
-                              # f"{pendulum.duration(seconds=dcat_output.total_interval_length.total_seconds()).in_words(locale='en')}.\n"
-                              f'Data from {pu.utc_str(self.pc_output.datetime_eval_start, tz)} '
-                              f'to {pu.utc_str(self.pc_output.datetime_eval_end, tz)}.'
-                              ), textprops=textprops)
-        box_details = TextArea((f'{METHOD_DESCRIPTION}\n'  # f'Power Check according to {pu.iso_string}.\n'
-                                f'Algorithm details: {self.algorithm_details()}'
-                                # f'Formula: {self.pc_output.formula}. '
-                                # f'Wind: {"Used" if self.pc_output.wind_used else "Not used"}. '
-                                # f'Mode: {self.pc_output.evaluation_mode}. '
-                                ), textprops=textprops)
-
-        box = VPacker(children=[pu.box_title('Notes', fontsize=params['font_size_text']),
-                                box_plant,
-                                box_details
-                                ], pad=0, sep=pu.Defaults.sep_minor.value)
-        xy = pu.get_xy_below(ax, vsep=50)
-        xy_title = pu.get_xy_below(title_artist)
-        artist = pu.annotation_bbox(box, xy=(xy_title[0], xy[1]))
-        # artist = pu.annotation_bbox(box, xy=pu.get_xy_below(ax, vsep=50))
-        ax.add_artist(artist)
-
-        return [fig]
-
-    def plot_shadow_and_intervals(self
-                                  ) -> Optional[List[matplotlib.figure.Figure]]:
-        """See `plot_shadow_and_intervals` function.
-        """
-        if self.no_output():
-            return None
-
-        # Data to plot
-        p = self.pc_output.plant
-        s = pd.Series(data=True, index=p.time_index)
-        for ao in self.pc_output.array_output:
-            s2 = ao.array.is_shadowed.data.astype(float)
-            s = (s & s2.astype(bool)).where(~pd.isna(s2), np.nan)
-        df = (1 - s.astype(float)).to_frame(name='not_shadowed')
-        is_na = df['not_shadowed'].isna()
-
-        df['no_data'] = is_na.astype(float)
-        df['sun_above_horizon'] = (p.sun_apparent_elevation.data > 0).astype(float)
-        df.loc[is_na, 'sun_above_horizon'] = np.nan
-
-        df['no_pc_interval'] = 1.0
-        po = self.pc_output.plant_output
-        for i_start, i_end in zip(po.datetime_intervals_start, po.datetime_intervals_end):
-            df['no_pc_interval'].loc[i_start:i_end] = 0.0
-
-        day_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq="1D").date
-        minute_of_day = 60 * df.index.hour + df.index.minute
-
-        def pivoting(column):
-            pivoted = pd.pivot_table(df, values=column, index=df.index.date, columns=minute_of_day)
-            pivoted = pivoted.reindex(day_index)
-            return pivoted.transpose().iloc[::-1]
-
-        # Plot
-        fig, params = pu.prepare_figure(settings=self.settings)
-
-        # Text on top
-        total_duration = pendulum.duration(seconds=self.pc_output.plant_output.n_intervals *
-                                                   self.pc_output.settings["interval_length"].total_seconds())
-        box_info = TextArea((
-            f'n={self.pc_output.plant_output.n_intervals} intervals, each '
-            f'{pendulum.duration(seconds=self.pc_output.settings["interval_length"].total_seconds()).in_words(locale="en")}'
-            f' long. Total interval duration: '
-            f'{total_duration.in_hours()} hours {total_duration.in_minutes() % 60} minutes.\n'
-            f'Algorithm details: {self.algorithm_details()}'
-        ), textprops=dict(size=params['font_size_text']))
-        box = VPacker(children=[pu.box_title('Intervals used for Power Check'),
-                                box_info,
-                                ], pad=0, sep=pu.Defaults.sep_major.value)
-        artist = pu.annotation_bbox(box, xy=pu.Defaults.xy_topleft.value)
-        fig.add_artist(artist)
-
-        # Shadow Plot
-        rect = pu.get_rectangle_below(artist, vsep=pu.Defaults.sep_huge.value, bottom=pu.Defaults.y_bottom.value)
-        ax = fig.add_axes(rect)
-
-        colors = dict(
-            horizon=pu.Colors.very_dark_gray,
-            shadowed=pu.Colors.dark_gray,
-            white=pu.Colors.white,
-            # pc=pu.Defaults.marker_facecolor.value,
-            pc=pu.Colors.red.value,
-        )
-        ax.set_facecolor(pu.Colors.missing_data)
-        im_kwargs = dict(aspect='auto', interpolation='none',
-                         extent=[mdates.date2num(day_index[0]), mdates.date2num(day_index[-1]), 0, 24])
-        ax.imshow(pivoting('no_data'),
-                  cmap=LinearSegmentedColormap.from_list('no_data', [colors['white'], 'none']), **im_kwargs)
-        ax.imshow(pivoting('not_shadowed'),
-                  cmap=LinearSegmentedColormap.from_list('shadowed', [colors['shadowed'], 'none']), **im_kwargs)
-        ax.imshow(pivoting('sun_above_horizon'),
-                  cmap=LinearSegmentedColormap.from_list('sun', [colors['horizon'], 'none']), **im_kwargs)
-        ax.imshow(pivoting('no_pc_interval'),
-                  cmap=LinearSegmentedColormap.from_list('pc', [colors['pc'], 'none']), **im_kwargs)
-
-        ax.grid(True)
-        ax.set_axisbelow('line')
-
-        ax.legend(loc='upper right', handles=[
-            mpatches.Patch(facecolor=colors['horizon'], label='Sun below horizon'),
-            mpatches.Patch(facecolor=colors['shadowed'], label='Arrays shadowed'),
-            mpatches.Patch(facecolor=pu.Colors.missing_data, label='Missing data'),
-            mpatches.Patch(facecolor=colors['pc'], label='Performance Check intervals'),
-        ])
-
-        # Axis formatting
-        locator = mdates.AutoDateLocator(tz=p.tz_data)
-        ax.xaxis.set_major_locator(locator)
-        ax.xaxis.set_minor_locator(mdates.DayLocator())
-        # ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(locator, show_offset=False, tz=p.tz_data))
-        ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(locator, tz=p.tz_data))
-        # tz = self.pc_output.plant.tz_data
-        # ax.xaxis.set_major_formatter(mdates.AutoDateFormatter(locator, tz=p.tz_data))
-
-        ax.set_yticks([0, 6, 12, 18, 24])
-        ax.set_yticklabels(["0:00", "6:00", "12:00", "18:00", "24:00"])
-        utc_str = f'(UTC{int(df.index[0].utcoffset().total_seconds() / 3600):+0})'
-        ax.set_ylabel(f'Time of day {utc_str}')
-
-        return [fig]
-
-    def plot_square(self,
-                    square_axis_range: Optional[List] = None,
-                    ) -> Optional[List[matplotlib.figure.Figure]]:
-        """See `plot_square` function.
-        """
-        if self.no_output():
-            return None
-
-        axis_range = square_axis_range or [0, 800]
-
-        # Plant plot
-        data = dict(
-            tp_sp_measured=self.pc_output.plant_output.tp_sp_measured.magnitude,
-            tp_sp_estimated_safety=self.pc_output.plant_output.tp_sp_estimated_safety.magnitude,
-            tp_sp_estimated=self.pc_output.plant_output.tp_sp_estimated.magnitude,
-        )
-
-        # Text on top of axes
-        _, params = pu.prepare_figure(settings=self.settings)
-        textprops = dict(size=pu.Defaults.fontsize_text.value, linespacing=pu.Defaults.linespacing_text.value,
-                         family=params['font_family'])
-        txt_plant = OrderedDict([
-            ('Plant', self.pc_output.plant.name),
-            ('Included arrays', f'{", ".join([ao.array.name for ao in self.pc_output.array_output])}'),
-        ])
-        txt_plant = pu.anonymize(txt_plant, do_anonymize=self.settings.anonymize)
-        box_component = pu.dict_to_box(txt_plant, textprops=textprops)
-
-        fig_list = [self._fig_square(data, box_component, axis_range, pu.Colors.red.value)]
-
-        if len(self.pc_output.array_output) == 1:
-            return fig_list
-
-        # Array plots
-        for ao in self.pc_output.array_output:
-            a = ao.array
-            has_tp = a.tp is not None and not a.tp.data.isna().all()
-            if not has_tp:
-                continue
-
-            data = dict(
-                # Plot ratio against midpoint of intervals
-                tp_sp_measured=ao.tp_sp_measured.magnitude,
-                tp_sp_estimated_safety=ao.tp_sp_estimated_safety.magnitude,
-                tp_sp_estimated=ao.tp_sp_estimated.magnitude,
-            )
-
-            # Text on top of axes
-            _, params = pu.prepare_figure(settings=self.settings)
-            textprops = dict(size=pu.Defaults.fontsize_text.value, linespacing=pu.Defaults.linespacing_text.value,
-                             family=params['font_family'])
-            txt_plant = OrderedDict([
-                ('Plant', self.pc_output.plant.name),
-                ('Array', a.name),
-            ])
-            txt_plant = pu.anonymize(txt_plant, do_anonymize=self.settings.anonymize)
-            box_component = pu.dict_to_box(txt_plant, textprops=textprops)
-
-            fig_list.append(self._fig_square(data, box_component, axis_range, pu.Colors.blue.value))
-
-        return fig_list
-
-    def _fig_square(self,
-                    data: dict,
-                    box_top: matplotlib.offsetbox.OffsetBox,
-                    axis_range: List,
-                    color: str,
-                    ) -> matplotlib.figure.Figure:
-        """Create one square figure, given data for array or plant.
-        """
-        fig, params = pu.prepare_figure(settings=self.settings)
-
-        # Title
-        box = VPacker(children=[pu.box_title('Thermal Power Output: Measured vs. Estimated'),
-                                box_top],
-                      pad=0, sep=pu.Defaults.sep_major.value)
-        artist = pu.annotation_bbox(box, xy=pu.Defaults.xy_topleft.value)
-        fig.add_artist(artist)
-
-        # Plot
-        def _plot_ax(ax_, y_estimated, x_label, title):
-            """Plot one of the 2 square axes, either the left or right one.
-            """
-            ax_.scatter(
-                y_estimated,
-                data['tp_sp_measured'],
-                s=pu.Defaults.marker_size_scatter.value,
-                alpha=pu.Defaults.marker_alpha.value,
-                facecolors=color,
-                edgecolors=color,
-                zorder=3,
-            )
-            # Bisection line
-            ax_.plot(axis_range, axis_range, linestyle='-',
-                     color=pu.Colors.dark_gray, linewidth=pu.Defaults.linewidth_thin.value, zorder=1)
-            ax_.grid(True)
-            ax_.set_axisbelow('line')
-            ax_.set_aspect('equal')
-            ax_.set_xlim(axis_range)
-            ax_.set_ylim(axis_range)
-            ax_.set_yticks(ax_.get_xticks())
-            ax_.set_xlabel(x_label)
-            ax_.set_ylabel('Measured power [W/m]')
-            ax_.set_title(title, loc='center', pad=pu.Defaults.sep_axestitle.value)
-
-        # Create the 2 axes and plot
-        ax, ax_safe = fig.subplots(1, 2)
-        x, y = pu.get_xy_below(artist, vsep=pu.Defaults.sep_major.value)
-        plt.subplots_adjust(left=x, top=y, bottom=0.22, right=pu.Defaults.x_right.value, wspace=0.35)
-
-        _plot_ax(ax, y_estimated=data['tp_sp_estimated'],
-                 x_label='Estimated power [W/m]',
-                 title='Without safety factor')
-        _plot_ax(ax_safe, y_estimated=data['tp_sp_estimated_safety'],
-                 x_label=r'Estimated power [W/m], with $f_{safe}$',
-                 title=f'With safety factor '
-                       r'$f_{safe} = '
-                       rf'{float(self.pc_output.settings["safety_combined"]):.2}$')
-
-        # Heading: Notes
-        box_note = TextArea((f'Each dot in the plots is the average thermal power output of a '
-                             f'{pendulum.duration(seconds=self.pc_output.settings["interval_length"].total_seconds()).in_words(locale="en")}'
-                             f' interval.\n'
-                             f'The left plot is based on estimated and measured data without safety factor. '
-                             f'The right plot takes the combined safety factor '
-                             r'$f_{safe} = '
-                             rf'{float(self.pc_output.settings["safety_combined"]):.2}$'
-                             f' into account.\n'
-                             f'Algorithm details: {self.algorithm_details()}'
-                             ), textprops=dict(weight='normal', size=params['font_size_text']))
-        box = VPacker(children=[pu.box_title('Notes', fontsize=params['font_size_text']),
-                                box_note,
-                                ], pad=0, sep=pu.Defaults.sep_minor.value)
-        # artist = pu.annotation_bbox(box, xy=pu.get_xy_below(ax, vsep=pu.Defaults.sep_huge.value))
-        artist = pu.annotation_bbox(box, xy=pu.get_xy_below(ax, vsep=50))
-        fig.add_artist(artist)
-
-        return fig
-
-    def plot_time(self,
-                  y_ratio_limits: Optional[List] = None,
-                  ) -> Optional[List[matplotlib.figure.Figure]]:
-        """See `plot_time` function.
-        """
-        if self.no_output():
-            return None
-
-        # Plot: Plant
-        time_display = self.pc_output.plant_output.datetime_intervals_start + 0.5 * self.pc_output.settings[
-            'interval_length']
-        data = dict(
-            # Plot ratio against midpoint of intervals
-            tp_sp_measured=self.pc_output.plant_output.tp_sp_measured.magnitude,
-            tp_sp_estimated_safety=self.pc_output.plant_output.tp_sp_estimated_safety.magnitude,
-            tp_sp_estimated=self.pc_output.plant_output.tp_sp_estimated.magnitude,
-        )
-
-        # Text on top of axes
-        _, params = pu.prepare_figure(settings=self.settings)
-        textprops = dict(size=pu.Defaults.fontsize_text.value, linespacing=pu.Defaults.linespacing_text.value,
-                         family=params['font_family'])
-        txt_plant = OrderedDict([
-            ('Plant', self.pc_output.plant.name),
-            ('Included arrays', f'{", ".join([ao.array.name for ao in self.pc_output.array_output])}'),
-            ('Algorithm details', self.algorithm_details()),
-        ])
-        txt_plant = pu.anonymize(txt_plant, do_anonymize=self.settings.anonymize)
-        box_component = pu.dict_to_box(txt_plant, textprops=textprops)
-
-        fig_list = [self._fig_time(data, time_display, box_component, y_ratio_limits, pu.Colors.red.value)]
-
-        if len(self.pc_output.array_output) == 1:
-            return fig_list
-
-        # Array plots
-        for ao in self.pc_output.array_output:
-            a = ao.array
-            has_tp = a.tp is not None and not a.tp.data.isna().all()
-            if not has_tp:
-                continue
-
-            data = dict(
-                # Plot ratio against midpoint of intervals
-                tp_sp_measured=ao.tp_sp_measured.magnitude,
-                tp_sp_estimated_safety=ao.tp_sp_estimated_safety.magnitude,
-                tp_sp_estimated=ao.tp_sp_estimated.magnitude,
-            )
-
-            # Text on top of axes
-            textprops = dict(size=pu.Defaults.fontsize_text.value, linespacing=pu.Defaults.linespacing_text.value)
-            txt_plant = OrderedDict([
-                ('Plant', self.pc_output.plant.name),
-                ('Array', a.name),
-            ])
-            txt_plant = pu.anonymize(txt_plant, do_anonymize=self.settings.anonymize)
-            box_component = pu.dict_to_box(txt_plant, textprops=textprops)
-
-            fig_list.append(self._fig_time(data, time_display, box_component, y_ratio_limits, pu.Colors.blue.value))
-
-        return fig_list
-
-    def _fig_time(self,
-                  data: dict,
-                  time_display,
-                  box_top: matplotlib.offsetbox.OffsetBox,
-                  y_limits: List,
-                  color: str,
-                  ) -> matplotlib.figure.Figure:
-        # Determine y axis limits
-        ratio = data['tp_sp_measured'] / data['tp_sp_estimated']
-        ratio_safety = data['tp_sp_measured'] / data['tp_sp_estimated_safety']
-        if y_limits is None:
-            y_min = min(0.8, np.floor(min(ratio) * 10) / 10)
-            y_max = max(1.2, np.ceil(max(ratio_safety) * 10) / 10)
-            y_limits = [y_min, y_max]
-
-        fig, params = pu.prepare_figure(settings=self.settings)
-
-        # Title
-        box = VPacker(children=[pu.box_title('Power Output over Time: Measured vs. Estimated'),
-                                box_top],
-                      pad=0, sep=pu.Defaults.sep_major.value)
-        artist = pu.annotation_bbox(box, xy=pu.Defaults.xy_topleft.value)
-        fig.add_artist(artist)
-
-        # Plot
-        def _plot_ax(ax_, ratio, y_label, title):
-            """Plot one of the 2 time axes, either the top or bottom one.
-            """
-            ax_.scatter(x=time_display, y=ratio,
-                        s=pu.Defaults.marker_size_scatter.value,
-                        alpha=pu.Defaults.marker_alpha.value,
-                        facecolors=color,
-                        edgecolors=color,
-                        zorder=3,
-                        )
-            # Horizontal line at 100%
-            ax_.axhline(y=1, linestyle='-',
-                        linewidth=pu.Defaults.linewidth_thin.value, color=pu.Colors.dark_gray, zorder=1)
-            # Trend line
-            # Currently not being displayed correctly, see https://gitlab.com/sunpeek/sunpeek/-/issues/562
-            # rm = pd.Series(data=ratio, index=time_display) \
-            #     .rolling(dt.timedelta(days=45), min_periods=20, center=True, closed='both').median()
-            # ax_.plot(rm.index, rm, color=pu.Colors.gray, linestyle='-', linewidth=3)
-            # Style
-            ax_.grid(True)
-            ax_.set_axisbelow('line')
-            ax_.set_ylim(y_limits)
-            ax_.set_ylabel(y_label)
-            ax_.set_title(title, pad=pu.Defaults.sep_axestitle.value)
-            # X axis style
-            tz = self.pc_output.plant.tz_data
-            ax_.xaxis_date(tz)
-            ax_.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))
-            ax_.set_xlim(pd.to_datetime(self.pc_output.datetime_eval_start).tz_convert(tz),
-                         pd.to_datetime(self.pc_output.datetime_eval_end).tz_convert(tz), auto=None)
-            ax_.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax_.xaxis.get_major_locator(), tz=tz))
-
-        # Create the 2 axes and plot
-        ax, ax_safe = fig.subplots(2, 1)
-        x, y = pu.get_xy_below(artist, vsep=pu.Defaults.sep_huge.value)
-        plt.subplots_adjust(left=x, top=y, bottom=pu.Defaults.y_bottom.value, right=pu.Defaults.x_right.value,
-                            hspace=0.4)
-
-        _plot_ax(ax, ratio=ratio,
-                 y_label=f'Ratio measured vs. estimated power\n',
-                 title='Without safety factor')
-        _plot_ax(ax_safe, ratio=ratio_safety,
-                 y_label=f'Ratio measured vs. estimated power,\n'
-                         r'with $f_{safe}$',
-                 title=f'With safety factor '
-                       r'$f_{safe} = '
-                       rf'{float(self.pc_output.settings["safety_combined"]):.2}$')
-
-        return fig
-
-    def plot_plant_overview(self) -> Optional[List[matplotlib.figure.Figure]]:
-        """See `plot_plant_overview` function.
-        """
-        if self.no_output():
-            return None
-
-        # Data to plot: Plant info
-        p = self.pc_output.plant
-        txt_plant = OrderedDict([
-            ('Plant name', p.name),
-            ('Plant owner', p.owner),
-            ('Plant location', p.location_name),
-            ('Plant latitude', f'{p.latitude.m:.2f}'),
-            ('Plant longitude', f'{p.longitude.m:.2f}'),
-            ('Plant elevation', f'{p.elevation:.0f~P}'),
-        ])
-        txt_plant = pu.anonymize(txt_plant, do_anonymize=self.settings.anonymize)
-
-        # Data to plot: Collector arrays
-        format_arrays = {
-            'Array name': pu.TableColumnFormat(width=2),
-            'Collector name': pu.TableColumnFormat(width=1.5),
-            'Manufacturer': pu.TableColumnFormat(width=1.6),
-            'Gross area': pu.TableColumnFormat(unit_str=r'$m^2$', ha='right'),
-            'Tilt': pu.TableColumnFormat(ha='right', width=0.9),
-            'Azimuth': pu.TableColumnFormat(ha='right', width=0.9),
-            'Row spacing': pu.TableColumnFormat(unit_str='m', ha='right', width=1.1),
-        }
-        tbl_rows = []
-        for ao in self.pc_output.array_output:
-            a = ao.array
-            tbl_rows.append({
-                'Array name': a.name,
-                'Collector name': a.collector.product_name,
-                'Manufacturer': a.collector.manufacturer_name,
-                'Gross area': f'{a.area_gr.m:.0f}',
-                'Tilt': f'{a.tilt.m:.1f}',
-                'Azimuth': f'{a.azim.m:.1f}',
-                'Row spacing': f'{a.row_spacing.m:.1f}',
-            })
-        df_arrays = pd.DataFrame(tbl_rows)
-
-        # Plot Plant details
-        fig, params = pu.prepare_figure(settings=self.settings)
-        box_plant = pu.dict_to_box(txt_plant, textprops=dict(size=params['font_size_text'],
-                                                             linespacing=pu.Defaults.linespacing_text.value))
-        box = VPacker(children=[pu.box_title('Plant Overview'),
-                                box_plant,
-                                ], pad=0, sep=pu.Defaults.sep_major.value)
-        artist = pu.annotation_bbox(box, xy=pu.Defaults.xy_topleft.value)
-        fig.add_artist(artist)
-
-        # Plot Heading: Arrays
-        box = VPacker(children=[pu.box_title('Collector Arrays', fontsize=params['font_size_text']),
-                                ], pad=0, sep=pu.Defaults.sep_major.value)
-        artist = pu.annotation_bbox(box, xy=pu.get_xy_below(artist, vsep=pu.Defaults.sep_huge.value))
-        fig.add_artist(artist)
-
-        # Plot Table: Arrays
-        # Note: This assumes that all arrays fit on one page. This is valid up to approx. 10 arrays in a plant.
-        # For more arrays, see the approach taken e.g. in plot_data_overview()
-        rect = pu.get_rectangle_below(artist, vsep=pu.Defaults.sep_minor.value,
-                                      bottom=pu.Defaults.y_bottom_extreme.value,
-                                      right=pu.Defaults.x_right.value)
-        ax = fig.add_axes(rect)
-        pu.add_table(ax, df_arrays, format_arrays, cell_vspace_header=2.2, vpad_header=4)
-
-        return [fig]
-
-    def plot_collector_overview(self) -> Optional[List[matplotlib.figure.Figure]]:
-        """See `plot_collector_overview` function.
-        """
-        if self.no_output():
-            return None
-
-        # Data to plot: unique Collectors
-        p_width = 0.8
-        d = pu.Defaults.dimensionless_unit.value
-        format_collectors = {
-            'Collector name': pu.TableColumnFormat(width=1.6),
-            'Manufacturer': pu.TableColumnFormat(width=1.6),
-            'License number': pu.TableColumnFormat(header_str='License\nnumber', width=1.2),
-            'Date issued': pu.TableColumnFormat(header_str='Date\nissued', width=0.8),
-            'Gross area': pu.TableColumnFormat(unit_str=r'$m^2$', ha='right', header_str='Gross\narea', width=p_width),
-            'a1': pu.TableColumnFormat(ha='right', unit_str=r'$\dfrac{W}{m^2 K}$', header_str=r'$a_1$', width=p_width),
-            'a2': pu.TableColumnFormat(ha='right', unit_str=r'$\dfrac{W}{m^2 K^2}$', header_str=r'$a_2$',
-                                       width=p_width),
-            'a5': pu.TableColumnFormat(ha='right', unit_str=r'$\dfrac{J}{m^2 K}$', header_str=r'$a_5$', width=p_width),
-            'a8': pu.TableColumnFormat(ha='right', unit_str=r'$\dfrac{W}{m^2 K^4}$', header_str=r'$a_8$',
-                                       width=p_width),
-            'eta0b': pu.TableColumnFormat(ha='right', unit_str=d, header_str=r'$\eta_b$', width=p_width),
-            'eta0hem': pu.TableColumnFormat(ha='right', unit_str=d, header_str=r'$\eta_{hem}$', width=p_width),
-            'kd': pu.TableColumnFormat(ha='right', unit_str=d, header_str=r'$K_d$', width=p_width),
-            'f_prime': pu.TableColumnFormat(ha='right', unit_str=d, header_str=r'$F\prime$', width=p_width),
-            # 'f_prime_c': pu.TableColumnFormat(ha='right', unit_str=d, header_str=r'$F\prime_c$', width=p_width),
-            'C_R': pu.TableColumnFormat(ha='right', unit_str=d, header_str=r'$C_R$', width=p_width),
-        }
-        collectors = [ao.array.collector for ao in self.pc_output.array_output]
-        unique_collectors = list({c.name: c for c in collectors}.values())
-        tbl_rows = []
-        for c in unique_collectors:
-            tbl_rows.append({
-                'Collector name': c.product_name if c.product_name else pu.NA_STR,
-                'Manufacturer': c.manufacturer_name if c.manufacturer_name else pu.NA_STR,
-                'License number': c.licence_number if c.licence_number else pu.NA_STR,
-                'Date issued': f'{c.certificate_date_issued:{pu.Defaults.format_date.value}}'
-                if c.certificate_date_issued else pu.NA_STR,
-                'Gross area': f'{c.area_gr.to("m**2").m:.2f}',
-                'a1': f'{c.a1.to("W m**-2 K**-1").m:.2f}' if c.a1 is not None else pu.NA_STR,
-                'a2': f'{c.a2.to("W m**-2 K**-2").m:.3f}' if c.a2 is not None else pu.NA_STR,
-                'a5': f'{c.a5.to("J m**-2 K**-1").m:.0f}' if c.a5 is not None else pu.NA_STR,
-                'a8': f'{c.a8.to("W m**-2 K**-4").m:.2e}' if c.a8 is not None else pu.NA_STR,
-                'eta0b': f'{c.eta0b.to("").m:.3f}',
-                'eta0hem': f'{c.eta0hem.to("").m:.3f}',
-                'kd': f'{c.kd.to("").m:.2f}' if c.kd is not None else pu.NA_STR,
-                'f_prime': f'{c.f_prime.to("").m:.2f}' if c.f_prime is not None else pu.NA_STR,
-                # 'f_prime_c': pu.NA_STR if c.f_prime_c is None else f'{c.f_prime_c.to("").m:.2f}',
-                'C_R': f'{c.concentration_ratio.to("").m:.1f}' if c.concentration_ratio is not None else pu.NA_STR,
-            })
-        df_collectors = pd.DataFrame(tbl_rows)
-
-        # Data to plot: IAMs of unique collectors
-        angles = np.linspace(10, 90, 9)
-        angle_names = {a: f'{a:.0f}' for a in angles}
-        f_iams = {k: pu.TableColumnFormat(header_str=v, ha='right', width=0.6) for k, v in angle_names.items()}
-        f_iams['Collector name'] = pu.TableColumnFormat(width=2.2)
-        f_iams['trans_long'] = pu.TableColumnFormat(header_str='IAM type', width=1.)
-        f_iams['symbol'] = pu.TableColumnFormat(header_str='', width=0.7)
-
-        tbl_rows = []
-        for c in unique_collectors:
-            for iam_type in ['Transversal', 'Longitudinal']:
-                row = {'Collector name': c.product_name if iam_type == 'Transversal' else '',
-                       'trans_long': iam_type,
-                       'symbol': r'$K_b(\theta_T)$' if iam_type == 'Transversal' else r'$K_b(\theta_L)$'}
-                row.update(
-                    {a: f'{c.iam_method.get_iam(aoi=Q(a, "deg"), azimuth_diff=Q(0, "deg"))[0].m:.2f}' for a in angles})
-                tbl_rows.append(row)
-        df_iams = pd.DataFrame(tbl_rows)
-
-        # Plot Title
-        fig, params = pu.prepare_figure(settings=self.settings)
-        box = VPacker(children=[pu.box_title('Collector Overview'),
-                                ], pad=0, sep=pu.Defaults.sep_major.value)
-        artist = pu.annotation_bbox(box, xy=pu.Defaults.xy_topleft.value)
-        fig.add_artist(artist)
-
-        # Plot Heading: Collectors
-        box = VPacker(children=[pu.box_title('Collectors', fontsize=params['font_size_text']),
-                                ], pad=0, sep=pu.Defaults.sep_major.value)
-        artist = pu.annotation_bbox(box, xy=pu.get_xy_below(artist))
-        fig.add_artist(artist)
-
-        # Plot Table: Collectors
-        rect = pu.get_rectangle_below(artist, vsep=pu.Defaults.sep_minor.value,
-                                      # bottom=pu.Defaults.y_bottom.value,
-                                      right=pu.Defaults.x_right_extreme.value)
-        ax = fig.add_axes(rect)
-        pu.add_table(ax, df_collectors, format_collectors, cell_vspace_header=2.8, vpad_header=3)
-        artist = ax
-
-        # Plot Heading: IAMs
-        box = VPacker(children=[pu.box_title('Incidence Angle Modifiers (IAM)', fontsize=params['font_size_text']),
-                                ], pad=0, sep=pu.Defaults.sep_major.value)
-        artist = pu.annotation_bbox(box, xy=pu.get_xy_below(artist, vsep=pu.Defaults.sep_huge.value))
-        fig.add_artist(artist)
-
-        # Plot Table: IAMs
-        rect = pu.get_rectangle_below(artist, vsep=pu.Defaults.sep_minor.value,
-                                      bottom=pu.Defaults.y_bottom_extreme.value,
-                                      right=pu.Defaults.x_right_extreme.value)
-        ax = fig.add_axes(rect)
-        pu.add_table(ax, df_iams, f_iams, vpad_header=5,
-                     divider_lines=[n for n in range(2, 2 * len(unique_collectors)) if n % 2 == 0])
-
-        return [fig]
-
-    def plot_data_overview(self) -> Optional[List[matplotlib.figure.Figure]]:
-        """See `plot_data_overview` function.
-        """
-        if self.no_output():
-            return None
-
-        # Data to plot
-        p_width = 0.8
-        fontsize = pu.Defaults.fontsize_table.value - 1
-        # Using symbols used in ISO 24194.
-        format_data = {
-            'n': pu.TableColumnFormat(width=0.4, fontsize=fontsize),
-            'Date': pu.TableColumnFormat(width=1.1, fontsize=fontsize),
-            'Start': pu.TableColumnFormat(width=0.7, fontsize=fontsize),
-            'End': pu.TableColumnFormat(width=0.5, fontsize=fontsize),
-            'te_amb': pu.TableColumnFormat(header_str=r'$\vartheta_a$', unit_str=r'$C$',
-                                           fontsize=fontsize, ha='right', width=p_width),
-            'te_in': pu.TableColumnFormat(header_str=r'$\vartheta_{in}$', unit_str=r'$C$',
-                                          fontsize=fontsize, ha='right', width=p_width),
-            'te_out': pu.TableColumnFormat(header_str=r'$\vartheta_{out}$', unit_str=r'$C$',
-                                           fontsize=fontsize, ha='right', width=p_width),
-            'te_op': pu.TableColumnFormat(header_str=r'$\vartheta_{op}$', unit_str=r'$C$',
-                                          fontsize=fontsize, ha='right', width=p_width),
-            'te_op_deriv': pu.TableColumnFormat(header_str=r'$\vartheta\backprime_{op}$', unit_str=r'$K/h$',
-                                                fontsize=fontsize, ha='right', width=p_width),
-            'aoi': pu.TableColumnFormat(header_str=r'$\theta$', unit_str=r'',
-                                        fontsize=fontsize, ha='right', width=0.5),
-            'iam_b': pu.TableColumnFormat(header_str=r'$K_b$', unit_str=pu.Defaults.dimensionless_unit.value,
-                                          fontsize=fontsize, ha='right', width=p_width),
-            'rd_gti': pu.TableColumnFormat(header_str=r'$G_{hem}$', unit_str=r'$\dfrac{W}{m^2}$',
-                                           fontsize=fontsize, ha='right', width=p_width),
-            'rd_bti': pu.TableColumnFormat(header_str=r'$G_{b}$', unit_str=r'$\dfrac{W}{m^2}$',
-                                           fontsize=fontsize, ha='right', width=p_width),
-            'rd_dti': pu.TableColumnFormat(header_str=r'$G_{d}$', unit_str=r'$\dfrac{W}{m^2}$',
-                                           fontsize=fontsize, ha='right', width=p_width),
-            've_wind': pu.TableColumnFormat(header_str=r'$u$', unit_str=r'$\dfrac{m}{s}$',
-                                            fontsize=fontsize, ha='right', width=p_width),
-            'tp_measured': pu.TableColumnFormat(header_str=r'$\.Q_{meas,sp}$', unit_str=r'$\dfrac{W}{m^2}$',
-                                                fontsize=fontsize, ha='right', width=1),
-            'tp_estimated_safety': pu.TableColumnFormat(header_str=r'$\.Q_{est,sp}*f_{safe}$',
-                                                        unit_str=r'$\dfrac{W}{m^2}$',
-                                                        fontsize=fontsize, ha='right', width=1),
-            'tp_ratio': pu.TableColumnFormat(header_str=r'$\.Q$ ratio', unit_str='%',
-                                             fontsize=fontsize, ha='right', width=p_width),
-        }
-
-        # Create data table for each array
-        fig_list = []
-        for ao in self.pc_output.array_output:
-            a = ao.array
-            has_tp = a.tp is not None and not a.tp.data.isna().all()
-            tbl_rows = []
-            for i in range(self.pc_output.plant_output.n_intervals):
-                tbl_rows.append({
-                    'n': i + 1,
-                    'Date': f'{self.pc_output.plant_output.datetime_intervals_start[i]:{pu.Defaults.format_date.value}}',
-                    'Start': f'{self.pc_output.plant_output.datetime_intervals_start[i]:{pu.Defaults.format_time.value}}',
-                    'End': f'{self.pc_output.plant_output.datetime_intervals_end[i]:{pu.Defaults.format_time.value}}',
-                    'te_amb': f'{ao.data.te_amb[i].to("degC").m:0.1f}',
-                    'te_in': f'{ao.data.te_in[i].to("degC").m:0.1f}',
-                    'te_out': f'{ao.data.te_out[i].to("degC").m:0.1f}',
-                    'te_op': f'{ao.data.te_op[i].to("degC").m:0.1f}',
-                    'te_op_deriv': f'{ao.data.te_op_deriv[i].to("K hour**-1").m:0.2f}',
-                    'aoi': f'{ao.data.aoi[i].to("deg").m:0.0f}',
-                    'iam_b': f'{ao.data.iam_b[i].to("").m:0.2f}',
-                    'rd_gti': f'{ao.data.rd_gti[i].to("W m**-2").m:0.0f}' if ao.data.rd_gti is not None else pu.NA_STR,
-                    'rd_bti': f'{ao.data.rd_bti[i].to("W m**-2").m:0.0f}' if ao.data.rd_bti is not None else pu.NA_STR,
-                    'rd_dti': f'{ao.data.rd_dti[i].to("W m**-2").m:0.0f}' if ao.data.rd_dti is not None else pu.NA_STR,
-                    've_wind': f'{ao.data.ve_wind[i].to("m s**-1").m:0.1f}' if ao.data.ve_wind is not None else pu.NA_STR,
-                    'tp_measured': f'{ao.tp_sp_measured[i].to("W m**-2").m:0.0f}' if has_tp else pu.NA_STR,
-                    'tp_estimated_safety': f'{ao.tp_sp_estimated_safety[i].to("W m**-2").m:0.0f}',
-                    'tp_ratio': f'{(ao.tp_sp_measured[i] / ao.tp_sp_estimated_safety[i]).to("percent").m:0.1f}' if has_tp else pu.NA_STR,
-                })
-            df = pd.DataFrame(tbl_rows)
-
-            # Plot Title
-            fig, params = pu.prepare_figure(settings=self.settings)
-            box_note = TextArea((f'Array: {ao.array.name}\n'
-                                 f'Each table row shows the average values of a '
-                                 f'{pendulum.duration(seconds=self.pc_output.settings["interval_length"].total_seconds()).in_words(locale="en")}'
-                                 f' interval. The rightmost column takes a combined safety factor '
-                                 r'$f_{safe} = '
-                                 rf'{float(self.pc_output.settings["safety_combined"]):.2}$ into account.'
-                                 ), textprops=dict(weight='normal', size=params['font_size_text']))
-            box = VPacker(children=[pu.box_title('Data Points Considered'),
-                                    box_note,
-                                    ], pad=0, sep=pu.Defaults.sep_major.value)
-            artist = pu.annotation_bbox(box, xy=pu.Defaults.xy_topleft.value)
-            fig.add_artist(artist)
-
-            # Plot Data Table, potentially over multiple pages
-            y_bottom = pu.Defaults.y_bottom_extreme.value
-            x_right = pu.Defaults.x_right_extreme.value
-            n_rows_plotted = [0]
-            while np.sum(n_rows_plotted) < len(df):
-                if len(n_rows_plotted) == 1:  # 1st page
-                    rect = pu.get_rectangle_below(artist, vsep=pu.Defaults.sep_minor.value,
-                                                  bottom=y_bottom, right=x_right)
-                else:  # subsequent pages
-                    xy = pu.Defaults.xy_topleft.value
-                    rect = (xy[0], y_bottom, x_right - xy[0], xy[1] - y_bottom)
-                    fig, _ = pu.prepare_figure(settings=self.settings)
-                fig_list.append(fig)
-                ax = fig.add_axes(rect)
-                df_ = df.loc[np.sum(n_rows_plotted):]
-                n_rows_plotted.append(
-                    pu.add_table(ax, df_, format_data, cell_vspace=1.9, cell_vspace_header=2.5, vpad_header=3,
-                                 divider_lines=range(1, len(df_)))
-                )
-
-        return fig_list
-
-    def plot_symbols_overview(self) -> Optional[List[matplotlib.figure.Figure]]:
-        """See `plot_symbols_overview` function.
-        """
-        if self.no_output():
-            return None
-
-        # Data to plot
-        fontsize = pu.Defaults.fontsize_table.value - 1
-        format_symbols = {
-            'Symbol': pu.TableColumnFormat(width=0.25, fontsize=fontsize),
-            'Description': pu.TableColumnFormat(width=1.2, fontsize=fontsize),
-            'Typical unit': pu.TableColumnFormat(width=0.3, fontsize=fontsize),
-        }
-        du = pu.Defaults.dimensionless_unit.value
-        symbols = [
-            # Latin
-            [r'$a_1$', 'Linear heat loss coefficient', r'$\dfrac{W}{m^2 K}$'],
-            [r'$a_2$', 'Quadratic heat loss coefficient', r'$\dfrac{W}{m^2 K^2}$'],
-            [r'$a_5$', 'Effective thermal heat capacity', r'$\dfrac{J}{m^2 K}$'],
-            [r'$a_8$', 'Radiative heat loss coefficient', r'$\dfrac{W}{m^2 K^4}$'],
-            [r'$C_R$', 'Geometric concentration ratio', du],
-            [r'$F\prime$',
-             'Collector efficiency factor (ratio of heat transfer of fluid to absorber vs. fluid to ambient)', du],
-            [r'$F\prime_c$',
-             r'Constant collector efficiency factor (like $F\prime$, but neglecting $u_2$)', du],
-            [r'$G_{hem}$', 'Hemispherical solar irradiance on the collector plane', r'$\dfrac{W}{m^2}$'],
-            [r'$G_{b}$', 'Direct solar irradiance (beam irradiance) on the collector plane', r'$\dfrac{W}{m^2}$'],
-            [r'$G_{d}$', 'Diffuse solar irradiance on the collector plane', r'$\dfrac{W}{m^2}$'],
-            [r'$K_b$', 'Incidence angle for direct solar radiation', du],
-            [r'$K_d$', 'Incidence angle modifier for diffuse solar radiation', du],
-            [r'$\.Q_{meas,sp}$', r'Measured specific power output (per $m^2$ gross collector array area)',
-             r'$\dfrac{W}{m^2}$'],
-            [r'$\.Q_{est,sp}$', r'Estimated specific power output (per $m^2$ gross collector array area)',
-             r'$\dfrac{W}{m^2}$'],
-            [r'$u$', 'Surround air speed (wind velocity)', r'$\dfrac{m}{s}$'],
-            # Greek
-            [r'$\eta_b$', r'Collector efficiency based on beam irradiance $G_b$', du],
-            [r'$\eta_{hem}$', r'Collector efficiency based on hemispherical irradiance $G_{hem}$', du],
-            [r'$\theta$', 'Angle of incidence between the normal vector of the collector plane and the sun-beam vector',
-             r''],
-            [r'$\vartheta_a$', 'Ambient air temperature', r'$C$'],
-            [r'$\vartheta_{in}$', 'Collector array inlet temperature', r'$C$'],
-            [r'$\vartheta_{out}$', 'Collector array outlet temperature', r'$C$'],
-            [r'$\vartheta_{op}$', (f'Collector array operating temperature '
-                                   r'$\vartheta_{op}=mean(\vartheta_{in}, \vartheta_{out})$'), r'$C$'],
-            [r'$\vartheta\backprime_{op}$', f'Derivative of the collector array operating temperature ', r'$K/h$'],
-        ]
-        tbl_rows = [{k: v for k, v in zip(format_symbols.keys(), s)} for s in symbols]
-        df = pd.DataFrame(tbl_rows)
-
-        # Plot Title
-        fig, params = pu.prepare_figure(settings=self.settings)
-        box = VPacker(children=[pu.box_title('Symbols'),
-                                ], pad=0, sep=pu.Defaults.sep_major.value)
-        artist = pu.annotation_bbox(box, xy=pu.Defaults.xy_topleft.value)
-        fig.add_artist(artist)
-
-        # Plot Symbols Table
-        rect = pu.get_rectangle_below(artist, vsep=pu.Defaults.sep_minor.value,
-                                      # bottom=pu.Defaults.y_bottom.value,
-                                      right=0.8)
-        ax = fig.add_axes(rect)
-        pu.add_table(ax, df, format_symbols, cell_vspace=2.5, vpad_header=4,
-                     divider_lines=range(1, len(tbl_rows)))
-
-        return [fig]
-
-    def plot_intervals(self,
-                       axes_limits_interval_plots: dict = None,
-                       ) -> Optional[List[matplotlib.figure.Figure]]:
-        """See `plot_intervals` function.
-        """
-        if self.no_output():
-            return None
-
-        fig_list = []
-        for i in range(self.pc_output.plant_output.n_intervals):
-            for ao in self.pc_output.array_output:
-                fig_list.append(
-                    self._plot_single_interval(array_output=ao,
-                                               interval_idx=i,
-                                               axes_limits=axes_limits_interval_plots)
-                )
-
-        return fig_list
-
-    def _plot_single_interval(self,
-                              array_output: results.PCMethodOutputArray,
-                              interval_idx: int,
-                              axes_limits: dict = None,
-                              ) -> matplotlib.figure.Figure:
-        # Data to plot
-        def _data(sensor, unit_str: str) -> pd.Series:
-            return sensor.data.pint.to(unit_str).loc[interval_start:interval_end]
-
-        a = array_output.array
-        p = a.plant
-        has_tp = a.tp is not None and not a.tp.data.isna().all()
-        has_vf = a.vf is not None and not a.vf.data.isna().all()
-        interval_start = self.pc_output.plant_output.datetime_intervals_start[interval_idx]
-        interval_end = self.pc_output.plant_output.datetime_intervals_end[interval_idx]
-
-        # Plot
-        fig, params = pu.prepare_figure(settings=self.settings)
-        ax_te, ax_tp, ax_rd, ax_vf = fig.subplots(4, sharex='all', height_ratios=[5, 4, 4, 2])
-
-        # Text at top
-        utc_str = lambda \
-                d: f'{d:%Y-%m-%d %H:%M} (UTC{int(d.utcoffset().total_seconds() / 3600):+0})'
-        array_text = f'Plant: {p.name}. Array: {a.name}. '
-        interval_text = f'Interval #{interval_idx + 1}: {utc_str(interval_start)} to {utc_str(interval_end)}'
-
-        textprops = dict(size=params['font_size_text'])
-        box = VPacker(children=[pu.box_title('Measurement Data in Interval'),
-                                HPacker(children=[TextArea(array_text, textprops=textprops),
-                                                  TextArea(interval_text, textprops=textprops)])
-                                ], pad=0, sep=pu.Defaults.sep_minor.value)
-        artist = pu.annotation_bbox(box, xy=pu.Defaults.xy_topleft.value)
-        fig.add_artist(artist)
-
-        fig.subplots_adjust(left=pu.Defaults.x_left.value, bottom=pu.Defaults.y_bottom_extreme.value,
-                            right=pu.Defaults.x_right_extreme.value, top=pu.get_xy_below(artist)[1])
-        # Temperatures
-        ax_te.plot(_data(a.te_out, 'degC'), "-", label="Outlet temperature", color=pu.Colors.red, zorder=2)
-        ax_te.plot(_data(a.te_in, 'degC'), "-", label="Inlet temperature", color=pu.Colors.blue, zorder=1.9)
-        ax_te.plot(_data(p.te_amb, 'degC'), "-", label="Ambient temperature", color=pu.Colors.yellow, zorder=1.8)
-
-        # Thermal power(specific): measured and simulated
-        if has_tp:
-            tp_sp_measured = _data(a.tp, 'W') / a.area_gr
-            ax_tp.plot(tp_sp_measured, label="Measured power", color=pu.Colors.almost_black, zorder=2)
-        # Averages: Horizontal Lines
-        if has_tp:
-            tp_sp_meas = array_output.tp_sp_measured[interval_idx].magnitude
-        tp_sp_est = array_output.tp_sp_estimated[interval_idx].magnitude
-        tp_sp_est_safety = array_output.tp_sp_estimated_safety[interval_idx].magnitude
-
-        if has_tp:
-            ax_tp.axhline(y=tp_sp_meas, label='Average measured power',
-                          linewidth=0.75, color=pu.Colors.red, zorder=1.5, linestyle=(5, (10, 3)))
-            ax_tp.fill_between(tp_sp_measured.index, tp_sp_est_safety, tp_sp_est,
-                               label=r'Estimated power, range with $f_{safe}$', zorder=1,
-                               color=pu.Colors.dark_gray, linewidth=plt.rcParams['grid.linewidth'], alpha=0.4)
-
-        # Irradiance
-        if a.rd_gti is not None:
-            rd_g = _data(a.rd_gti, 'W m**-2')
-            ax_rd.fill_between(rd_g.index, 0, rd_g, label="Global tilted irradiance", zorder=1,
-                               color=pu.Colors.yellow,
-                               alpha=0.4,
-                               )
-        if a.rd_bti is not None:
-            ax_rd.plot(_data(a.rd_bti, 'W m**-2'), label="Beam tilted irradiance", color=pu.Colors.red, zorder=3)
-        if a.rd_dti is not None:
-            ax_rd.plot(_data(a.rd_dti, 'W m**-2'), label="Diffuse tilted irradiance", color=pu.Colors.dark_gray,
-                       zorder=2)
-
-        # Specific volume flow
-        if has_vf:
-            vf_sp = _data(a.vf, 'l hour**-1') / a.area_gr
-            ax_vf.fill_between(vf_sp.index, 0, vf_sp, label="Specific volume flow",
-                               color=pu.Colors.gray, linewidth=plt.rcParams['grid.linewidth'], alpha=0.4)
-
-        # Axes formatting (labels)
-        ax_te.set_ylabel("Temperature [C]")
-        ax_tp.set_ylabel("Specific thermal\npower [W/m]")
-        ax_rd.set_ylabel("Irradiance\n[W/m]")
-        ax_vf.set_ylabel("Specific volume\nflow [l/mh]")
-
-        # Axes formatting (limits)
-        axes_limits = {} if axes_limits is None else axes_limits
-        te_max = axes_limits.get('te_max', max(100, np.ceil(_data(a.te_out, 'degC').astype(float).max())))
-        rd_max = axes_limits.get('rd_max', 1250)
-        tp_max = axes_limits.get('tp_max', 1000)
-        vf_sp_max = axes_limits.get('vf_sp_max', max(20, np.ceil(vf_sp.astype(float).max())) if has_vf else 20)
-
-        ax_te.set_ylim(0, te_max)
-        ax_tp.set_ylim(0, tp_max)
-        ax_rd.set_ylim(0, rd_max)
-        ax_vf.set_ylim(0, vf_sp_max)
-
-        # Axes formatting
-        ax_names = ['(a) Temperatures',  # '(a) Temperatures collector array and ambient',
-                    '(b) Thermal power',
-                    '(c) Irradiance',
-                    '(d) Volume flow']
-        for ax_, ax_title in zip([ax_te, ax_tp, ax_rd, ax_vf], ax_names):
-            leg = ax_.legend(loc="upper right")
-            leg.set_zorder(3)
-            ax_.set_title(ax_title, loc='center', pad=3)
-            ax_.grid()
-            ax_.set_axisbelow('line')
-
-        # Layout
-        ax_vf.set_xlim(interval_start, interval_end, auto=None)
-        tz = p.tz_data
-        ax_vf.xaxis_date(tz)
-        ax_vf.xaxis.set_minor_locator(mdates.MinuteLocator(interval=5))
-        ax_vf.xaxis.set_major_locator(mdates.MinuteLocator(interval=15))
-        ax_vf.xaxis.set_major_formatter(mdates.DateFormatter("%#H:%M", tz=tz))
-        ax_vf.tick_params(axis='x', pad=5)
-
-        return fig
-
-    def no_output(self) -> bool:
-        if self.pc_output is None or (self.pc_output.plant_output.n_intervals == 0):
-            sp_logger.info('Nothing to plot, no Performance Check intervals found.')
-            return True
-        return False
-
-    def algorithm_details(self):
-        mode = self.pc_output.evaluation_mode
-        mode = mode if mode.isupper() else mode.capitalize()
-        return (f'Formula: {self.pc_output.formula}. '
-                f'Wind: {"Used" if self.pc_output.wind_used else "Not used"}. '
-                f'Averaging mode: {mode}. ')
+    return save_folder / filename
```

## sunpeek/core_methods/pc_method/wrapper.py

```diff
@@ -1,179 +1,216 @@
-import warnings
-from typing import List
 import datetime as dt
+from typing import Union, Any, List, Optional
 import itertools
+from enum import Enum
+import traceback
 
 from sunpeek.common.utils import sp_logger
 from sunpeek.components import Plant
-from sunpeek.components.helpers import AlgoCheckMode
+from sunpeek.components.base import AlgoCheckMode
 from sunpeek.core_methods import CoreAlgorithm, CoreStrategy
-from sunpeek.serializable_models import CoreMethodFeedback, PCMethodFeedback
+from sunpeek.serializable_models import ProblemReport, PCMethodProblem, AlgoProblem, ProblemType
 from sunpeek.core_methods.pc_method.main import PCMethod
-from sunpeek.core_methods.pc_method import PCFormulae, PCMethods
+from sunpeek.core_methods.pc_method import AvailablePCEquations, AvailablePCMethods
 from sunpeek.core_methods.common.main import AlgoResult
 
 
 def run_performance_check(plant: Plant,
-                          method: List[PCMethods | str | None] | None = None,
-                          formula: List[PCFormulae | int | None] | None = None,
-                          use_wind: List[None | bool] | None = None,
-                          # Context
-                          eval_start: dt.datetime | None = None,
-                          eval_end: dt.datetime | None = None,
+                          method: Optional[List[Union[None, str, AvailablePCMethods]]] = None,
+                          equation: Optional[List[Union[None, int, AvailablePCEquations]]] = None,
+                          use_wind: Optional[List[Union[None, bool]]] = None,
                           # Settings:
-                          safety_pipes: float | None = None,
-                          safety_uncertainty: float | None = None,
-                          safety_others: float | None = None,
-                          interval_length: dt.timedelta | None = None,
-                          min_data_in_interval: int | None = None,
-                          max_gap_in_interval: dt.timedelta | None = None,
-                          max_nan_density: float | None = None,
-                          min_intervals_in_output: int | None = None,
-                          accuracy_level: str | None = None,
+                          safety_pipes: Optional[float] = None,
+                          safety_uncertainty: Optional[float] = None,
+                          safety_others: Optional[float] = None,
+                          interval_length: Optional[dt.timedelta] = None,
+                          min_data_in_interval: Optional[int] = None,
+                          max_gap_in_interval: Optional[dt.timedelta] = None,
+                          max_nan_density: Optional[float] = None,
+                          min_intervals_in_output: Optional[int] = None,
+                          check_accuracy_level: Optional[str] = None,
                           ) -> AlgoResult:
     """Run Performance Check analysis with given settings, trying all possible strategies in order.
     """
     kwds = {
         'safety_pipes': safety_pipes,
         'safety_uncertainty': safety_uncertainty,
         'safety_others': safety_others,
         'interval_length': interval_length,
         'min_data_in_interval': min_data_in_interval,
         'max_gap_in_interval': max_gap_in_interval,
         'max_nan_density': max_nan_density,
         'min_intervals_in_output': min_intervals_in_output,
-        'accuracy_level': accuracy_level,
+        'check_accuracy_level': check_accuracy_level,
     }
-    pc_algo = PCAlgo(plant, methods=method, formulae=formula, use_wind=use_wind, **kwds)
-    pc_algo.check_interval(eval_start, eval_end)
+
+    pc_algo = PCAlgo(plant, methods=method, equations=equation, use_wind=use_wind, **kwds)
     algo_result = pc_algo.run()
     return algo_result
 
 
-def list_feedback(plant: Plant,
-                  method: List[PCMethods | str | None] | None = None,
-                  formula: List[PCFormulae | int | None] | None = None,
-                  use_wind: List[bool | None] | None = None,
-                  ) -> List[PCMethodFeedback]:
-    """Report which strategies of the Performance Check analysis can be run for given plant config and settings.
-    Does not actually run PC calculation. Can operate on a plant without data uploaded.
-    """
-    pc_algo = PCAlgo(plant, methods=method, formulae=formula, use_wind=use_wind)
-    pc_feedback = []
-    for strategy in pc_algo.strategies:
-        fb = strategy.get_feedback(AlgoCheckMode.config_only)
-        pc_feedback.append(strategy.get_pc_from_core_feedback(fb))
-
-    return pc_feedback
-
-
-def get_feedback(plant: Plant,
-                 method: List[PCMethods | str | None] | None = None,
-                 formula: List[PCFormulae | int | None] | None = None,
-                 use_wind: List[bool | None] | None = None,
-                 # Settings:
-                 safety_pipes: float | None = None,
-                 safety_uncertainty: float | None = None,
-                 safety_others: float | None = None,
-                 interval_length: dt.timedelta | None = None,
-                 min_data_in_interval: int | None = None,
-                 max_gap_in_interval: dt.timedelta | None = None,
-                 max_nan_density: float | None = None,
-                 min_intervals_in_output: int | None = None,
-                 accuracy_level: str | None = None,
-                 ) -> CoreMethodFeedback:
-    """Report which strategy of the Performance Check analysis can be run for given plant config and settings, if any.
-    Stops at first successful strategy.
-    Does not actually run PC calculation. Can operate on a plant without data uploaded.
+def get_pc_problemreport(plant: Plant,
+                         method: Optional[List[Union[None, str, AvailablePCMethods]]] = None,
+                         equation: Optional[List[Union[None, int, AvailablePCEquations]]] = None,
+                         use_wind: Optional[List[Union[None, bool]]] = None,
+                         # Settings:
+                         safety_pipes: Optional[float] = None,
+                         safety_uncertainty: Optional[float] = None,
+                         safety_others: Optional[float] = None,
+                         interval_length: Optional[dt.timedelta] = None,
+                         min_data_in_interval: Optional[int] = None,
+                         max_gap_in_interval: Optional[dt.timedelta] = None,
+                         max_nan_density: Optional[float] = None,
+                         min_intervals_in_output: Optional[int] = None,
+                         check_accuracy_level: Optional[str] = None,
+                         ) -> ProblemReport:
+    """Report which strategies of the Performance Check analysis can be run with given plant and settings,
+    without actually running calculations.
     """
     kwds = {
         'safety_pipes': safety_pipes,
         'safety_uncertainty': safety_uncertainty,
         'safety_others': safety_others,
         'interval_length': interval_length,
         'min_data_in_interval': min_data_in_interval,
         'max_gap_in_interval': max_gap_in_interval,
         'max_nan_density': max_nan_density,
         'min_intervals_in_output': min_intervals_in_output,
-        'accuracy_level': accuracy_level,
+        'check_accuracy_level': check_accuracy_level,
     }
 
-    pc_algo = PCAlgo(plant, methods=method, formulae=formula, use_wind=use_wind, **kwds)
-    return pc_algo.get_config_feedback()
+    pc_algo = PCAlgo(plant, methods=method, equations=equation, use_wind=use_wind, **kwds)
+    return pc_algo.get_config_problems()
+
+
+def list_pc_problems(plant: Plant,
+                     method: Optional[List[Union[None, str, AvailablePCMethods]]] = None,
+                     equation: Optional[List[Union[None, int, AvailablePCEquations]]] = None,
+                     use_wind: Optional[List[Union[None, bool]]] = None,
+                     # Settings:
+                     # safety_pipes: Optional[float] = None,
+                     # safety_uncertainty: Optional[float] = None,
+                     # safety_others: Optional[float] = None,
+                     # interval_length: Optional[dt.timedelta] = None,
+                     # min_data_in_interval: Optional[int] = None,
+                     # max_gap_in_interval: Optional[dt.timedelta] = None,
+                     # max_nan_density: Optional[float] = None,
+                     # min_intervals_in_output: Optional[int] = None,
+                     # check_accuracy_level: Optional[str] = None,
+                     ) -> List[PCMethodProblem]:
+    """Report which strategies of the Performance Check analysis can be run with given plant config,
+    without actually running calculations.
+    """
+    pc_algo = PCAlgo(plant, methods=method, equations=equation, use_wind=use_wind)
+    out = []
+    for strategy in pc_algo.strategies:
+        report = strategy.get_problem_report(AlgoCheckMode.config_only)
+        out.append(PCMethodProblem(evaluation_mode=strategy.pc.mode.value,
+                                   equation=strategy.pc.equation.id,
+                                   wind_used=strategy.pc.equation.use_wind,
+                                   success=report.success,
+                                   problem_str=report.parse()))
+    return out
 
 
 class PCStrategy(CoreStrategy):
     def __init__(self, pc: PCMethod):
         super().__init__(pc.plant)
         self.pc = pc
-        self.name = (f'Thermal Power Check with '
-                     f'Mode: {pc.mode.value}, '
-                     f'Formula: {pc.formula.id}, '
-                     f'{"Using wind" if pc.formula.use_wind else "Ignoring wind"}')
+        self.name = f'Evaluation mode: {"ISO" if pc.mode == AvailablePCMethods.iso else "ISO extended"}, ' \
+                    f'Equation: {pc.equation.id}, ' \
+                    f'{"Using wind" if pc.equation.use_wind else "Ignoring wind"}'
 
     def _calc(self):
+        # return {'pc_method_output': self.pc.run()}     # results.PCMethodOutput
         return self.pc.run()  # results.PCMethodOutput
 
-    def _get_feedback(self, check_mode: AlgoCheckMode) -> CoreMethodFeedback:
-        return self.pc.get_feedback(check_mode)
-
-    def get_pc_from_core_feedback(self, core_feedback: CoreMethodFeedback) -> PCMethodFeedback:
-        return PCMethodFeedback(self.pc.mode.value,
-                                self.pc.formula.id,
-                                self.pc.formula.use_wind,
-                                core_feedback.success,
-                                core_feedback.parse())
+    def _report_problems(self, check_mode: AlgoCheckMode) -> ProblemReport:
+        return self.pc.report_problems(check_mode)
 
 
 class PCAlgo(CoreAlgorithm):
 
-    name = 'Performance Check analysis'
-
-    def define_strategies(self, methods=None, formulae=None, use_wind=None, **kwargs) -> List[PCStrategy]:
+    def define_strategies(self, methods=None, equations=None, use_wind=None, **kwargs) -> List[PCStrategy]:
         """Returns list of all possible PC method strategies in the order they will be executed.
         """
-        variants = {'methods': self.create_variants(methods, allowed_type=PCMethods,
-                                                    default=[PCMethods.iso, PCMethods.extended]),
-                    'formulae': self.create_variants(formulae, allowed_type=PCFormulae,
-                                                     default=[PCFormulae.two, PCFormulae.one, PCFormulae.three]),
-                    'wind': self.create_variants(use_wind, allowed_type=bool, default=[True, False])}
-        all_variants = list(itertools.product(*variants.values()))
-        strategies = [PCStrategy(PCMethod.create(self.component, m, f, w, **kwargs)) for m, f, w in all_variants]
+
+        def process_args(arg, arg_name, allowed_type) -> List[Any]:
+            # Make sure arg is a list of allowed_type (bool or Enum). Remove None elements and duplicates.
+
+            if isinstance(allowed_type, type) and issubclass(allowed_type, Enum):
+                allowed_values = {x.value for x in allowed_type}
+            elif allowed_type is bool:
+                allowed_values = {True, False}
+            else:
+                raise TypeError("Invalid allowed_type: Must be either bool or an Enum class.")
+
+            arg = arg if isinstance(arg, list) else [arg]
+            for item in arg:
+                if item not in allowed_values | {None}:
+                    raise ValueError(f'Invalid input value "{item}" for "{arg_name}". '
+                                     f'Expected: {", ".join(map(str, allowed_values))}.')
+
+            # Remove None and duplicates
+            processed = [x for x in arg if x is not None]
+            processed = list(dict.fromkeys(processed))
+            return processed
+
+        all_methods = process_args(methods, 'methods', AvailablePCMethods)
+        all_methods = all_methods if all_methods else [AvailablePCMethods.iso, AvailablePCMethods.extended]
+
+        all_equations = process_args(equations, 'equations', AvailablePCEquations)
+        all_equations = all_equations if all_equations else [AvailablePCEquations.two, AvailablePCEquations.one]
+
+        all_wind = process_args(use_wind, 'use_wind', bool)
+        all_wind = all_wind if all_wind else [True, False]
+
+        all_variants = list(itertools.product(*[all_methods, all_equations, all_wind]))
+        strategies = [pc_strategy_generator(self.component, m, e, w, **kwargs) for m, e, w in all_variants]
 
         return strategies
 
 
-def get_successful_strategy(plant: Plant,
-                            method: List[PCMethods | str | None] | None = None,
-                            formula: List[PCFormulae | int | None] | None = None,
-                            use_wind: List[bool | None] | None = None,
-                            # Settings:
-                            safety_pipes: float | None = None,
-                            safety_uncertainty: float | None = None,
-                            safety_others: float | None = None,
-                            interval_length: dt.timedelta | None = None,
-                            min_data_in_interval: int | None = None,
-                            max_gap_in_interval: dt.timedelta | None = None,
-                            max_nan_density: float | None = None,
-                            min_intervals_in_output: int | None = None,
-                            accuracy_level: str | None = None,
-                            ) -> PCStrategy:
+def pc_strategy_generator(plant: Plant,
+                          method: AvailablePCMethods,
+                          equation: AvailablePCEquations,
+                          use_wind: bool,
+                          **kwargs) -> PCStrategy:
+    pc = PCMethod.from_method(method, plant, equation, use_wind, **kwargs)
+
+    return PCStrategy(pc)
+
+
+def get_pc_successful_strategy(plant: Plant,
+                               method: Optional[List[Union[None, str, AvailablePCMethods]]] = None,
+                               equation: Optional[List[Union[None, int, AvailablePCEquations]]] = None,
+                               use_wind: Optional[List[Union[None, bool]]] = None,
+                               # Settings:
+                               safety_pipes: Optional[float] = None,
+                               safety_uncertainty: Optional[float] = None,
+                               safety_others: Optional[float] = None,
+                               interval_length: Optional[dt.timedelta] = None,
+                               min_data_in_interval: Optional[int] = None,
+                               max_gap_in_interval: Optional[dt.timedelta] = None,
+                               max_nan_density: Optional[float] = None,
+                               min_intervals_in_output: Optional[int] = None,
+                               check_accuracy_level: Optional[str] = None,
+                               ) -> PCStrategy:
     """Report the first strategy of the Performance Check analysis that is successful with given plant and
-    settings. Like `get_feedback()`, this does not actually run calculations.
+    settings. Like `get_pc_problemreport()`, this does not actually run calculations.
     """
     kwds = {
         'safety_pipes': safety_pipes,
         'safety_uncertainty': safety_uncertainty,
         'safety_others': safety_others,
         'interval_length': interval_length,
         'min_data_in_interval': min_data_in_interval,
         'max_gap_in_interval': max_gap_in_interval,
         'max_nan_density': max_nan_density,
         'min_intervals_in_output': min_intervals_in_output,
-        'accuracy_level': accuracy_level,
+        'check_accuracy_level': check_accuracy_level,
     }
 
-    pc_algo = PCAlgo(plant, methods=method, formulae=formula, use_wind=use_wind, **kwds)
+    pc_algo = PCAlgo(plant, methods=method, equations=equation, use_wind=use_wind, **kwds)
     strategy = pc_algo.successful_strategy
 
     return strategy  # noqa
```

## sunpeek/core_methods/virtuals/calculations.py

```diff
@@ -2,28 +2,28 @@
 
 To add an implementation, subclass CoreAlgorithm (see ThermalPower class for instance).
 - allowed_components(): specify to which component the algorithm applies
 - define_strategies(): add specific ways to calculate the algorithm output, in terms of CoreStrategy classes
 
 For each such CoreStrategy or VirtualSensorStrategy, implement
 - _calc(): The calculation itself. Output must be a dict containing unit-aware pd.Series (with dtype pint[unit]).
-- _get_feedback(): Add checks for all inputs required by _calc(). This is used by config_virtuals(plant).
+- _report_problems(): Add checks for all inputs required by _calc(). This is used by config_virtuals(plant).
 
 .. codeauthor:: Philip Ohnewein <p.ohnewein@aee.at>
 .. codeauthor:: Daniel Tschopp <d.tschopp@aee.at>
 """
 import pandas as pd
 import numpy as np
 import scipy.signal
-import pvlib
-# from metpy.calc import dewpoint_from_relative_humidity
+import pvlib as pv
+from metpy.calc import dewpoint_from_relative_humidity
 
 import sunpeek.common.unit_uncertainty as uu
 from sunpeek.components import Plant, Array
-from sunpeek.serializable_models import CoreMethodFeedback
+from sunpeek.serializable_models import ProblemReport
 from sunpeek.core_methods.common.main import CoreAlgorithm, VirtualSensorStrategy, is_valid_fluid, is_valid_collector
 from sunpeek.common.errors import CalculationError
 import sunpeek.core_methods.virtuals.radiation as rd
 
 
 # ----------------------------------------------------------------------------------------------------------------------
 # Thermal Power
@@ -44,50 +44,50 @@
         ]
 
 
 # noinspection PyArgumentList
 class StrategyPowerFromSensor(VirtualSensorStrategy):
     """Feedthrough strategy, taking power from a Plant or Array `tp` real sensor.
     """
-    name = 'Calculate thermal power from real sensor'
+    name = 'Power from real sensor'
     feedthrough_real_sensor = True
 
     def _calc(self):
         return {'tp': None}
 
-    def _get_feedback(self, check_mode):
+    def _report_problems(self, check_mode):
         """Make sure real sensor 'tp' exists in Plant or Array.
         """
-        fb = CoreMethodFeedback()
+        r = ProblemReport()
         slot = 'tp'
-        if self.component.is_real_sensor_missing(slot, check_mode):
-            fb.add_missing_real_sensor(self.component, slot)
-        return fb
+        if self.component.is_real_slot_missing(slot, check_mode):
+            r.add_missing_sensor(self.component, slot, check_mode, enforce_real=True)
+        return r
 
 
 # noinspection PyArgumentList
 class StrategyPowerFromVolumeFlow(VirtualSensorStrategy):
     """For Plant and Arrays, calculate thermal power from fluid, volume flow and inlet & outlet temperatures.
     """
-    name = 'Calculate thermal power from volume flow'
+    name = 'Power from volume flow'
 
-    def _get_feedback(self, check_mode):
-        fb = CoreMethodFeedback()
+    def _report_problems(self, check_mode):
+        r = ProblemReport()
         if not is_valid_fluid(self.component.fluid_solar, check_mode):
-            fb.add_missing_fluid(self.component, 'fluid_solar')
+            r.add_missing_fluid(self.component, 'fluid_solar')
 
         for slot in ['vf', 'te_in', 'te_out']:
-            if self.component.is_sensor_missing(slot, check_mode):
-                fb.add_missing_sensor(self.component, slot, check_mode)
+            if self.component.is_slot_missing(slot, check_mode):
+                r.add_missing_sensor(self.component, slot, check_mode)
 
         slot, info_name = 'vf', 'position'
-        if not self.component.is_sensor_missing('vf', check_mode):
+        if not self.component.is_slot_missing('vf', check_mode):
             if self.component.vf.is_info_missing(info_name):
-                fb.add_missing_sensor_info(self.component, slot, info_name)
-        return fb
+                r.add_missing_sensor_info(self.component, slot, info_name)
+        return r
 
     def _calc(self):
         """
         Notes
         -----
         Position of volume flow sensor decides which temperature (inlet or outlet or a weighted average) is used for
         density / mass flow calculation.
@@ -123,26 +123,26 @@
         ]
 
 
 # noinspection PyArgumentList
 class StrategyMassFlowFromPower(VirtualSensorStrategy):
     """For Plants and Arrays, calculate mass flow from fluid, thermal power and inlet & outlet temperatures.
     """
-    name = 'Calculate mass flow from thermal power'
+    name = 'Mass flow from thermal power'
 
-    def _get_feedback(self, check_mode):
-        fb = CoreMethodFeedback()
+    def _report_problems(self, check_mode):
+        r = ProblemReport()
         if not is_valid_fluid(self.component.fluid_solar, check_mode):
-            fb.add_missing_fluid(self.component, 'fluid_solar')
+            r.add_missing_fluid(self.component, 'fluid_solar')
 
         for slot in ['tp', 'te_in', 'te_out']:
-            if self.component.is_sensor_missing(slot, check_mode):
-                fb.add_missing_sensor(self.component, slot, check_mode)
+            if self.component.is_slot_missing(slot, check_mode):
+                r.add_missing_sensor(self.component, slot, check_mode)
 
-        return fb
+        return r
 
     def _calc(self):
         """
         Returns
         -------
         tp : pd.Series
             Calculated mass flow.
@@ -158,31 +158,31 @@
         return {'mf': mf.pint.to('kg s**-1')}
 
 
 # noinspection PyArgumentList
 class StrategyMassFlowFromVolumeFlow(VirtualSensorStrategy):
     """For Plants and Arrays, calculate mass flow from fluid, thermal power and inlet & outlet temperatures.
     """
-    name = 'Calculate mass flow from volume flow'
+    name = 'Mass flow from volume flow'
 
-    def _get_feedback(self, check_mode):
-        fb = CoreMethodFeedback()
+    def _report_problems(self, check_mode):
+        r = ProblemReport()
         if not is_valid_fluid(self.component.fluid_solar, check_mode):
-            fb.add_missing_fluid(self.component, 'fluid_solar')
+            r.add_missing_fluid(self.component, 'fluid_solar')
 
         for slot in ['tp', 'te_in', 'te_out']:
-            if self.component.is_sensor_missing(slot, check_mode):
-                fb.add_missing_sensor(self.component, slot, check_mode)
+            if self.component.is_slot_missing(slot, check_mode):
+                r.add_missing_sensor(self.component, slot, check_mode)
 
         slot, info_name = 'vf', 'position'
-        if not self.component.is_sensor_missing('vf', check_mode):
+        if not self.component.is_slot_missing(slot, check_mode):
             if self.component.vf.is_info_missing(info_name):
-                fb.add_missing_sensor_info(self.component, slot, info_name)
+                r.add_missing_sensor_info(self.component, slot, info_name)
 
-        return fb
+        return r
 
     def _calc(self):
         """
         Returns
         -------
         tp : pd.Series
             Calculated mass flow.
@@ -210,52 +210,52 @@
         return [
             StrategySolarPosition_pvlib(self.component),
         ]
 
 
 # noinspection PyArgumentList
 class StrategySolarPosition_pvlib(VirtualSensorStrategy):
-    name = 'Calculate solar position using pvlib'
+    name = 'Solar position using pvlib'
 
-    def _get_feedback(self, check_mode):
-        fb = CoreMethodFeedback()
+    def _report_problems(self, check_mode):
+        r = ProblemReport()
         for attrib in ['latitude', 'longitude']:
             if self.component.is_attrib_missing(attrib):
-                fb.add_missing_attrib(self.component, attrib)
-        return fb
+                r.add_missing_attrib(self.component, attrib)
+        return r
 
     def _calc(self):
         """Calculates solar angles (azimuth, elevation, zenith), based on pvlib.
         https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.solarposition.get_solarposition.html
 
         Returns
         -------
         azimuth, zenitz, apparenzt_zenith, elevation, apparent_elevation : pd.Series
             Angles defining the solar position.
         """
         p = self.plant
         longitude = p.longitude.m_as('deg')
         latitude = p.latitude.m_as('deg')
-        elevation = None if (p.elevation is None) else p.elevation.m_as('m')
+        altitude = None if (p.altitude is None) else p.altitude.m_as('m')
         te_amb = p.te_amb
         if te_amb is None:
             # returns pd.DataFrame
-            sol_pos = pvlib.solarposition.get_solarposition(time=p.time_index,
-                                                            latitude=latitude,
-                                                            longitude=longitude,
-                                                            altitude=elevation)
+            sol_pos = pv.solarposition.get_solarposition(time=p.time_index,
+                                                         latitude=latitude,
+                                                         longitude=longitude,
+                                                         altitude=altitude)
         else:
             te_amb = te_amb.data.pint.to('degC')
             # 12 degC is the pvlib default in case no ambient temperature is known
             te_amb = te_amb.fillna(12).astype('float64').to_numpy()
-            sol_pos = pvlib.solarposition.get_solarposition(time=p.time_index,
-                                                            latitude=latitude,
-                                                            longitude=longitude,
-                                                            altitude=elevation,
-                                                            temperature=te_amb)
+            sol_pos = pv.solarposition.get_solarposition(time=p.time_index,
+                                                         latitude=latitude,
+                                                         longitude=longitude,
+                                                         altitude=altitude,
+                                                         temperature=te_amb)
 
         return {'azimuth': uu.to_s(sol_pos['azimuth'], 'deg'),
                 'zenith': uu.to_s(sol_pos['zenith'], 'deg'),
                 'apparent_zenith': uu.to_s(sol_pos['apparent_zenith'], 'deg'),
                 'elevation': uu.to_s(sol_pos['elevation'], 'deg'),
                 'apparent_elevation': uu.to_s(sol_pos['apparent_elevation'], 'deg'),
                 }
@@ -274,37 +274,37 @@
 #             StrategyDewPointFromTemperatureHumidty(self.component),
 #         ]
 #
 #
 # class StrategyDewPointFromSensor(VirtualSensorStrategy):
 #     """Feedthrough strategy, taking dew point from a Plant `te_dew_amb` real sensor.
 #     """
-#     name = 'Calculate dew point from real sensor'
+#     name = 'Dew point from real sensor'
 #     use_real_sensor = True
 #
 #     def _calc(self):
 #         return {'te_dew_amb': None}
 #
-#     def _get_feedback(self, check_mode):
+#     def _report_problems(self, check_mode):
 #         """Make sure real sensor 'te_dew' exists in Plant.
 #         """
-#         r = CoreMethodFeedback()
+#         r = ProblemReport()
 #         if self.component.is_real_slot_missing('te_dew_amb', check_mode):
 #             r.add_own(AlgoProblem(ProblemType.component_slot,
 #                                   self.component, 'te_dew_amb', 'Sensor is None or virtual.'))
 #         return r
 #
 #
 # class StrategyDewPointFromTemperatureHumidty(VirtualSensorStrategy):
 #     """Calculates ambient dew point temperature based on ambient temperature and ambient relative humidity of component.
 #     """
-#     name = 'Calculate dew point temperature from air temperature and relative humidity'
+#     name = 'Dew point temperature from air temperature and relative humidity'
 #
-#     def _get_feedback(self, check_mode):
-#         r = CoreMethodFeedback()
+#     def _report_problems(self, check_mode):
+#         r = ProblemReport()
 #         for slot in ['te_amb', 'rh_amb']:
 #             if self.component.is_slot_missing(slot, check_mode):
 #                 r.add_own(AlgoProblem(ProblemType.component_slot,
 #                                       self.component, slot, 'Sensor missing.'))
 #         return r
 #
 #     def _calc(self):
@@ -334,30 +334,30 @@
 #     def define_strategies(self):
 #         return [
 #             StrategyDNIExtra_pvlib(self.component),
 #         ]
 #
 #
 # class StrategyDNIExtra_pvlib(VirtualSensorStrategy):
-#     name = 'Calculate extraterrestrial solar radiation using pvlib'
+#     name = 'Extraterrestrial solar radiation using pvlib'
 #
-#     def _get_feedback(self, check_mode):
+#     def _report_problems(self, check_mode):
 #         # Only depends on plant time_index, which we assume here is always available.
-#         return CoreMethodFeedback()
+#         return ProblemReport()
 #
 #     def _calc(self):
 #         """Calculates extraterrestrial solar radiation using pvlib function.
 #         https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.irradiance.get_extra_radiation.html
 #
 #         Returns
 #         -------
 #         dni_extra : pd.Series
 #             Extraterrestrial solar radiation in W/m.
 #         """
-#         dni_extra = pvlib.irradiance.get_extra_radiation(self.plant.time_index)
+#         dni_extra = pv.irradiance.get_extra_radiation(self.plant.time_index)
 #
 #         return {'dni_extra': to_rd(dni_extra)}
 
 
 # ----------------------------------------------------------------------------------------------------------------------
 # Airmass
 
@@ -368,18 +368,18 @@
 #     def define_strategies(self):
 #         return [
 #             StrategyAirmass_pvlib(self.component),
 #         ]
 #
 #
 # class StrategyAirmass_pvlib(VirtualSensorStrategy):
-#     name = 'Calculate airmass using pvlib'
+#     name = 'Airmass using pvlib'
 #
-#     def _get_feedback(self, check_mode):
-#         r = CoreMethodFeedback()
+#     def _report_problems(self, check_mode):
+#         r = ProblemReport()
 #         if self.component.is_slot_missing('sun_apparent_zenith', check_mode):
 #             r.add_own(AlgoProblem(ProblemType.component_attrib, 'sun_apparent_zenith'))
 #
 #         return r
 #
 #     def _calc(self):
 #         """Calculate absolute airmass using pvlib function.
@@ -390,16 +390,16 @@
 #         -------
 #         rel_airmass : pd.Series
 #             Relative airmass (numeric value).
 #         abs_airmass : pd.Series
 #             Absolute, pressure-corrected airmass (numeric value).
 #         """
 #         p = self.plant
-#         rel_airmass = pvlib.atmosphere.get_relative_airmass(zenith=p.sun_apparent_zenith.s_as('deg'))
-#         abs_airmass = pvlib.atmosphere.get_absolute_airmass(airmass_relative=rel_airmass)
+#         rel_airmass = pv.atmosphere.get_relative_airmass(zenith=p.sun_apparent_zenith.s_as('deg'))
+#         abs_airmass = pv.atmosphere.get_absolute_airmass(airmass_relative=rel_airmass)
 #
 #         return {'rel_airmass': rel_airmass.astype('pint[dimensionless]'),
 #                 'abs_airmass': abs_airmass.astype('pint[dimensionless]')}
 
 
 # ----------------------------------------------------------------------------------------------------------------------
 # Linke Turbidity
@@ -410,18 +410,18 @@
 #             StrategyLinkeTurbidity_pvlib(self.component),
 #         ]
 #
 #
 # class StrategyLinkeTurbidity_pvlib(VirtualSensorStrategy):
 #     """For Plant, calculate Linke turbidity.
 #     """
-#     name = 'Calculate Linke turbidity from pvlib'
+#     name = 'Linke turbidity from pvlib'
 #
-#     def _get_feedback(self, check_mode):
-#         r = CoreMethodFeedback()
+#     def _report_problems(self, check_mode):
+#         r = ProblemReport()
 #         for attrib in ['latitude', 'longitude']:
 #             if self.component.is_attrib_missing(attrib):
 #                 r.add_own(AlgoProblem(ProblemType.component_attrib, self.component, attrib))
 #         return r
 #
 #     def _calc(self):
 #         """Calculate Linke turbidity using pvlib, required for clearsky irradiance calculation.
@@ -431,15 +431,15 @@
 #         -------
 #         linke_turbidity : pd.Series, Linke turbidity
 #         """
 #         p = self.plant
 # Catch "divide by zero" warning that occurs naturally at very low sun angles, not caught in pvlib
 #  with warnings.filterwarnings("ignore", category=RuntimeWarning):
 #
-#         linke_turbidity = pvlib.clearsky.lookup_linke_turbidity(p.time_index,
+#         linke_turbidity = pv.clearsky.lookup_linke_turbidity(p.time_index,
 #                                                              p.latitude.m_as('deg'),
 #                                                              p.longitude.m_as('deg'))
 #         return {'linke_turbidity': uu.to_s(linke_turbidity, 'dimensionless')}
 
 
 # ----------------------------------------------------------------------------------------------------------------------
 # Clearsky Radiation
@@ -453,18 +453,18 @@
 #             StrategyClearskyRadiation_pvlib(self.component),
 #         ]
 #
 #
 # class StrategyClearskyRadiation_pvlib(VirtualSensorStrategy):
 #     """For Plants and Arrays, calculate clearsky global horizontal irradiance and DNI using pvlib.
 #     """
-#     name = 'Calculate clearsky radiation from pvlib'
+#     name = 'Clearsky radiation from pvlib'
 #
-#     def _get_feedback(self, check_mode):
-#         r = CoreMethodFeedback()
+#     def _report_problems(self, check_mode):
+#         r = ProblemReport()
 #         for slot in ['sun_apparent_zenith', 'abs_airmass', 'linke_turbidity', 'rd_dni_extra']:
 #             if self.component.is_slot_missing(slot, check_mode):
 #                 r.add_own(AlgoProblem(ProblemType.component_slot,
 #                                       self.component, slot, 'Sensor missing.'))
 #         return r
 #
 #     def _calc(self):
@@ -478,16 +478,16 @@
 #         -------
 #         rd_ghi_clearsky : pd.Series
 #             Global horizontal clearsky radiation
 #         rd_dni_clearsky : pd.Series
 #             Clearsky DNI radiation
 #         """
 #         p = self.plant
-#         elevation = 0 if p.elevation is None else p.elevation.m_as('m')
-#         clearsky = pvlib.clearsky.ineichen(p.sun_apparent_zenith.m_as('deg'),
+#         altitude = 0 if p.altitude is None else p.altitude.m_as('m')
+#         clearsky = pv.clearsky.ineichen(p.sun_apparent_zenith.m_as('deg'),
 #                                         p.abs_airmass.m_as(''),
 #                                         p.linke_turbidity.m_as('dimensionless'),
 #                                         altitude,
 #                                         p.rd_dni_extra.m_as('W m**-2'))
 #
 #         return {'ghi_clearsky': to_rd(clearsky['ghi']),
 #                 'dni_clearsky': to_rd(clearsky['dni'])}
@@ -563,36 +563,35 @@
 
 
 # noinspection PyArgumentList
 class StrategyTiltedIrradiance_feedthrough(VirtualSensorStrategy):
     """Array tilted irradiance components using only available real sensors, no radiation modeling.
     """
 
-    name = 'Tilted irradiance as feedthrough of real sensors'
+    name = 'Feedthrough'
     feedthrough_real_sensor = True
 
-    def _get_feedback(self, check_mode):
-        r = CoreMethodFeedback()
+    def _report_problems(self, check_mode):
+        r = ProblemReport()
         a = self.component
 
         input_pattern = rd.get_radiation_pattern(a)
         # global only
         if input_pattern == '1000':
             r.add_generic_slot_problem(a, f'Only global irradiance given, cannot calculate '
                                           f'beam and diffuse irradiances.')
             r.problem_slots.extend(['rd_bti', 'rd_dti'])
             r.success = True
 
         # beam + diffuse
         elif input_pattern == '0110':
-            pass  # all good
-
-        # beam + diffuse + DNI
-        elif input_pattern == '0111':
-            pass  # all good
+            r.add_generic_slot_problem(a, f'Only beam and diffuse irradiances given, cannot calculate '
+                                          f'global irradiance.')
+            r.problem_slots.extend(['rd_gti'])
+            r.success = True
 
         # global + beam + diffuse
         elif input_pattern in ['1110', '1111']:
             pass  # all good, nothing missing
 
         else:
             r.add_generic_slot_problem(a, f'Invalid radiation input pattern {input_pattern}')
@@ -619,30 +618,24 @@
         # global only
         if input_pattern == '1000':
             gti = glob
 
         # beam + diffuse
         elif input_pattern == '0110':
             bti, dti = beam, diff
-            gti = beam + diff
-
-        # beam + diffuse + DNI
-        elif input_pattern == '0111':
-            bti, dti = beam, diff
-            gti = beam + diff
 
         # global + beam + diffuse
         elif input_pattern in ['1110', '1111']:
             gti, bti, dti = glob, beam, diff
 
         else:
             raise CalculationError(
                 f'Array irradiance calculation "{self}" only accepts input slots "global", or "beam + diffuse", '
                 f'or "global + beam + diffuse". Got input pattern {input_pattern}. This should have caught by '
-                f'self._get_feedback().')
+                f'self._report_problems().')
 
         return {'rd_gti': rd.to_rd(gti),
                 'rd_bti': rd.to_rd(bti),
                 'rd_dti': rd.to_rd(dti)}
 
     # def get_poa_irradiances(component, **kwargs):
     #     rc = RadiationConversionTilted(component=component, strategy='poa',
@@ -668,27 +661,27 @@
     #         -------
     #         iam : pd.Series
     #             Incidence Angle Modifier
     #         rd_bti_iam : pd.Series
     #             IAM-corrected (reduced) beam irradiance on component
     #         """
     #         ar = self.component
-    #         iam = ar.collector.iam_method.get_iam(aoi=ar.aoi.data,
+    #         iam = ar.collector_type.iam_method.get_iam(aoi=ar.aoi.data,
     #                                                    azimuth_diff=ar.component.sun_azimuth.data - ar.azim)
     #
     #         try:
     #             self.component.assert_verify_validate(AlgoCheckMode.config_and_data, 'rd_bti')
     #             rd_bti_iam = uu.to_numpy(iam) * self.component.rd_bti.data
     #         except AssertionError:
     #             rd_bti_iam = None
     #
     #         return iam, rd_bti_iam
     #
     #     def _do_assert(self, check_mode):
-    #         assert not isinstance(self.component.collector, UninitialisedCollector)
+    #         assert not isinstance(self.component.collector_type, UninitialisedCollectorType)
     #         self.component.assert_verify_validate(check_mode, 'aoi', 'azim')
     #         self.component.component.assert_verify_validate(check_mode, 'sun_azimuth')
 
 
 # ----------------------------------------------------------------------------------------------------------------------
 # IAM Incidence Angle Modifier
 
@@ -705,45 +698,45 @@
         ]
 
 
 # noinspection PyArgumentList
 class StrategyIAMFromCollector(VirtualSensorStrategy):
     """Calculate incidence angle modifier (IAM) based on IAM method defined in array's collector.
     """
-    name = 'Calculate incidence angle modifier (IAM) from collector'
+    name = 'IAM from collector'
 
     def _calc(self):
         """Calculate IAM.
 
         Returns
         -------
         iam : pd.Series
             Incidence Angle Modifier
         """
         a = self.component
         p = self.plant
-        iam = a.collector.iam_method.get_iam(aoi=a.aoi.data,
-                                             azimuth_diff=p.sun_azimuth.data - a.azim)
+        iam = a.collector_type.iam_method.get_iam(aoi=a.aoi.data,
+                                                  azimuth_diff=p.sun_azimuth.data - a.azim)
         return {'iam': iam}
 
-    def _get_feedback(self, check_mode):
-        r = CoreMethodFeedback()
-        if not is_valid_collector(self.component.collector, check_mode):
-            r.add_missing_collector(self.component, 'collector')
+    def _report_problems(self, check_mode):
+        r = ProblemReport()
+        if not is_valid_collector(self.component.collector_type, check_mode):
+            r.add_missing_collector(self.component, 'collector_type')
 
         slot = 'aoi'
-        if self.component.is_sensor_missing(slot, check_mode):
+        if self.component.is_slot_missing(slot, check_mode):
             r.add_missing_sensor(self.component, slot, check_mode)
 
         attrib = 'azim'
         if self.component.is_attrib_missing(attrib):
             r.add_missing_attrib(self.component, attrib)
 
         slot = 'sun_azimuth'
-        if self.plant.is_sensor_missing(slot, check_mode):
+        if self.plant.is_slot_missing(slot, check_mode):
             r.add_missing_sensor(self.plant, slot, check_mode)
 
         return r
 
 
 # ----------------------------------------------------------------------------------------------------------------------
 # Angle of Incidence
@@ -759,36 +752,36 @@
         return [
             StrategyAOI_pvlib(self.component),
         ]
 
 
 # noinspection PyArgumentList
 class StrategyAOI_pvlib(VirtualSensorStrategy):
-    name = 'Calculate angle of incidence (aoi) from pvlib'
+    name = 'Angle of incidence from pvlib'
 
     def _calc(self):
         """https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.irradiance.aoi.html
 
         Returns
         -------
         aoi : pd.Series
             Angle of incidence
         """
         a = self.component
         p = self.plant
-        aoi = pvlib.irradiance.aoi(surface_tilt=a.tilt.m_as('deg'),
-                                   surface_azimuth=a.azim.m_as('deg'),
-                                   solar_zenith=p.sun_zenith.m_as('deg'),
-                                   solar_azimuth=p.sun_azimuth.m_as('deg'))
+        aoi = pv.irradiance.aoi(surface_tilt=a.tilt.m_as('deg'),
+                                surface_azimuth=a.azim.m_as('deg'),
+                                solar_zenith=p.sun_zenith.m_as('deg'),
+                                solar_azimuth=p.sun_azimuth.m_as('deg'))
         return {'aoi': uu.to_s(aoi, 'deg')}
 
-    def _get_feedback(self, check_mode):
-        r = CoreMethodFeedback()
+    def _report_problems(self, check_mode):
+        r = ProblemReport()
         for slot in ['sun_zenith', 'sun_azimuth']:
-            if self.plant.is_sensor_missing(slot, check_mode):
+            if self.plant.is_slot_missing(slot, check_mode):
                 r.add_missing_sensor(self.plant, slot, check_mode)
 
         for attrib in ['tilt', 'azim']:
             if self.component.is_attrib_missing(attrib):
                 r.add_missing_attrib(self.component, attrib)
 
         return r
@@ -808,15 +801,15 @@
         return [
             StrategyInternalShading_BanyAppelbaum(self.component),
         ]
 
 
 # noinspection PyArgumentList
 class StrategyInternalShading_BanyAppelbaum(VirtualSensorStrategy):
-    name = 'Calculate internal shading based on Bany Appelbaum 1987 paper'
+    name = 'Internal shading based on Bany Appelbaum 1987 paper'
 
     def _calc(self):
         """Calculates internal shading (row-to-row shading) and several related virtual sensors of a collector component.
 
         Returns
         -------
         Dict with these keys and values:
@@ -847,27 +840,27 @@
         ----------
         .. [1] Bany, J. and Appelbaum, J. (1987): "The effect of shading on the design of a field of solar collectors",
             Solar Cells 20, p. 201 - 228
             :doi:`https://doi.org/10.1016/0379-6787(87)90029-9`
         """
         a = self.component
         p = self.plant
-        aoi_projection = pvlib.irradiance.aoi_projection(surface_tilt=a.tilt.m_as('deg'),
-                                                         surface_azimuth=a.azim.m_as('deg'),
-                                                         solar_zenith=p.sun_zenith.m_as('deg'),
-                                                         solar_azimuth=p.sun_azimuth.m_as('deg'))
+        aoi_projection = pv.irradiance.aoi_projection(surface_tilt=a.tilt.m_as('deg'),
+                                                      surface_azimuth=a.azim.m_as('deg'),
+                                                      solar_zenith=p.sun_zenith.m_as('deg'),
+                                                      solar_azimuth=p.sun_azimuth.m_as('deg'))
         sun_behind_coll = (aoi_projection < 0)
         sun_below_horizon = (a.plant.sun_elevation.m_as('deg') <= 0)
 
         # Formula (18), nomenclature according to BANY and APPELBAUM (1987)
         beta = a.tilt.m_as('rad') - a.ground_tilt.m_as('rad')
         sb = np.sin(beta)
         cb = np.cos(beta)
 
-        A = a.collector.gross_length.m_as('m')
+        A = a.collector_type.gross_length.m_as('m')
 
         Hc = A * np.sin(beta)
         D = a.row_spacing.m_as('m') - A * cb
         # Relative collector spacing:
         Drel = D / Hc
         gamma = p.sun_azimuth.m_as('rad') - a.azim.m_as('rad')
         alpha = p.sun_elevation.m_as('rad')
@@ -899,34 +892,34 @@
 
         return {'is_shadowed': is_shadowed,
                 'internal_shading_ratio': internal_shading_ratio,
                 'shadow_angle': shadow_angle,
                 'shadow_angle_midpoint': shadow_angle_midpoint,
                 }
 
-    def _get_feedback(self, check_mode):
-        r = CoreMethodFeedback()
-        for slot in ['sun_zenith', 'sun_azimuth', 'sun_elevation']:
-            if self.plant.is_sensor_missing(slot, check_mode):
+    def _report_problems(self, check_mode):
+        r = ProblemReport()
+        for slot in ['sun_zenith', 'sun_azimuth']:
+            if self.plant.is_slot_missing(slot, check_mode):
                 r.add_missing_sensor(self.plant, slot, check_mode)
 
         slot = 'aoi'
-        if self.component.is_sensor_missing(slot, check_mode):
+        if self.component.is_slot_missing(slot, check_mode):
             r.add_missing_sensor(self.component, slot, check_mode)
 
-        for attrib in ['tilt', 'azim', 'row_spacing', 'ground_tilt']:
+        for attrib in ['tilt', 'azim', 'row_spacing']:
             if self.component.is_attrib_missing(attrib):
                 r.add_missing_attrib(self.component, attrib)
 
-        if not is_valid_collector(self.component.collector, check_mode):
-            r.add_missing_collector(self.component, 'collector')
+        if not is_valid_collector(self.component.collector_type, check_mode):
+            r.add_missing_collector(self.component, 'collector_type')
         else:
             attrib = 'gross_length'
-            if self.component.collector.is_attrib_missing(attrib):
-                r.add_missing_attrib(self.component.collector, attrib)
+            if self.component.collector_type.is_attrib_missing(attrib):
+                r.add_missing_attrib(self.component.collector_type, attrib)
 
         return r
 
 
 # ----------------------------------------------------------------------------------------------------------------------
 # Array Temperatures and Temperature Derivatives
 
@@ -941,15 +934,15 @@
         return [
             StrategyArrayTemperatures_savgol(self.component),
         ]
 
 
 # noinspection PyArgumentList
 class StrategyArrayTemperatures_savgol(VirtualSensorStrategy):
-    name = 'Calculate array operating temperature and temperature derivative'
+    name = 'Array operating temperature and temperature derivative.'
 
     def _calc(self):
         """Calculate mean operating temperature of collector component and its temperature derivative.
 
         Returns
         -------
         Dict with these keys and values:
@@ -982,18 +975,18 @@
         te_op_deriv = scipy.signal.savgol_filter(te_op, mode='mirror', window_length=15, polyorder=3, deriv=1)
         te_op_deriv_final = te_op_deriv / mean_sampling_rate.total_seconds()  # now in K / s
 
         return {'te_op': uu.to_s(te_op, 'K'),
                 'te_op_deriv': uu.to_s(te_op_deriv_final, 'K s**-1'),
                 }
 
-    def _get_feedback(self, check_mode):
-        r = CoreMethodFeedback()
+    def _report_problems(self, check_mode):
+        r = ProblemReport()
         for slot in ['te_in', 'te_out']:
-            if self.component.is_sensor_missing(slot, check_mode):
+            if self.component.is_slot_missing(slot, check_mode):
                 r.add_missing_sensor(self.component, slot, check_mode)
 
         return r
 
 
 def _get_weighted_temperature(te1, te2, w1=0.5, w2=0.5):
     """Return weighted average between temperature pd.Series te1 and te2.
```

## sunpeek/core_methods/virtuals/main.py

```diff
@@ -1,15 +1,15 @@
 
 """
 This module implements functionality for calculation and verification of virtual sensors.
 
 It contains two entry-point functions for doing virtual sensor stuff.
 - config_virtuals():
    - Is fast to run, only depends on plant config, no calculations.
-   - Attaches a CoreMethodFeedback to all virtual sensors, stating if it _can_ be calculated / why not
+   - Attaches a ProblemReport to all virtual sensors, stating if it _can_ be calculated / why not
 - calculate_virtuals():
    - Actually calculates virtual sensors for all components.
    - Calculation logic is hard coded.
 
 ## Details
 
 - Implements functions that calculate groups of virtual sensors
@@ -36,42 +36,42 @@
 
 from sunpeek.common.utils import sp_logger
 import sunpeek.core_methods.virtuals.virtuals_plant as vp
 import sunpeek.core_methods.virtuals.virtuals_array as va
 from sunpeek.components.base import Component
 
 
-def config_virtuals(plant) -> None:
+def config_virtuals(plant):
     """Creates & maps virtual sensors for all components.
 
     Raises
     ------
     VirtualSensorConfigurationError
     """
+    # TODO Remove debug statement?
+    sp_logger.debug('In config_virtuals().')
     start_time = time.time()
 
     vp.config_virtuals_ambient(plant)
     vp.config_virtuals_power(plant)
-    # Uncomment if plant horizontal radiations are needed e.g. for KPIs.
+    # TODO Uncomment if plant horizontal radiations are needed e.g. for KPIs.
     # vp.config_virtuals_radiation_conversion(plant)
 
     for array in plant.arrays:
         va.config_virtuals_ambient(array)
         va.config_virtuals_power(array)
         va.config_virtuals_temperature(array)
         va.config_virtuals_radiation(array)
-        # Uncomment if vsensor array.te_out (averaged over row outlet temperatures) is required.
+        # TODO Uncomment if vsensor array.te_out (averaged over row outlet temperatures) is required.
         # va.config_virtuals_te_out(array)
 
-    plant.virtuals_calculation_uptodate = False
-
     sp_logger.debug(f"  [config_virtuals] --- Done in {(time.time() - start_time):.1f} seconds ---")
 
 
-def calculate_virtuals(plant) -> None:
+def calculate_virtuals(plant):
     """Implements all the logic of the virtual sensor calculation. Typically called by data upload.
 
     Raises
     ------
     CalculationError
 
     Notes
@@ -84,25 +84,23 @@
     start_time = time.time()
 
     sp_logger.debug(f"[calculate_virtuals] Calculating virtual sensors data.")
     plant.context.verify_time_index()
 
     vp.calculate_virtuals_ambient(plant)
     vp.calculate_virtuals_power(plant)
-    # Uncomment if horizontal radiations needed
+    # TODO Uncomment if horizontal radiations needed
     # vp.calculate_virtuals_radiation_conversion(plant)
 
     # Arrays
     for array in plant.arrays:
         va.calculate_virtuals_ambient(array)
         va.calculate_virtuals_power(array)
         va.calculate_virtuals_temperature(array)
         va.calculate_virtuals_radiation(array)
-        # Uncomment if vsensor array.te_out (averaged over row outlet temperatures) is required.
+        # TODO Uncomment if vsensor array.te_out (averaged over row outlet temperatures) is required.
         # va.calculate_virtuals_te_out(array)
 
-    plant.virtuals_calculation_uptodate = True
-
     sp_logger.debug(f"[calculate_virtuals] --- Done in {(time.time() - start_time):.1f} seconds ---")
 
 
 Component.register_callback('post_config_changed_callbacks', config_virtuals)
```

## sunpeek/core_methods/virtuals/radiation.py

```diff
@@ -1,12 +1,12 @@
 from abc import ABC, abstractmethod
 import numpy as np
 import enum
 
-import pvlib
+import pvlib as pv
 from sunpeek.common.errors import VirtualSensorConfigurationError, CalculationError
 from sunpeek.common.unit_uncertainty import to_s
 
 
 ## Utility functions
 
 def get_radiation_inputs(component):
@@ -108,49 +108,50 @@
 ## Elementary functions for radiation converter
 
 def _get_aoi(plant, tilt, azim):
     # TODO add docstring
     # tilt, azim : array-likes in degrees
     # Returns: angle of incidence in degrees
     # https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.irradiance.aoi.html
-    aoi = pvlib.irradiance.aoi(surface_tilt=tilt,
-                               surface_azimuth=azim,
-                               solar_zenith=plant.sun_zenith.m_as('deg'),
-                               solar_azimuth=plant.sun_azimuth.m_as('deg'))
+    aoi = pv.irradiance.aoi(surface_tilt=tilt,
+                            surface_azimuth=azim,
+                            solar_zenith=plant.sun_zenith.m_as('deg'),
+                            solar_azimuth=plant.sun_azimuth.m_as('deg'))
     return aoi
 
+
 # To be uncommented, once radiation conversion for Arrays is implemented.
 # def _get_dni_from_ghi(plant, ghi):
 #     # TODO add docstring
 #
 #     # ghi : array-like in W/m
 #     # Decomposition
 #     # DNI from global horitzonal (ghi) using dirindex model
 #     # Returns: DNI in W/m
 #     # case [b] in design https://gitlab.com/sunpeek/sunpeek/-/issues/128/
 #     # https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.irradiance.dirindex.html
 #     # TODO Philip Note that dirindex needs timeseries data! (vector of at least length 2), may not be case for all vs calculations
 #     #  https://pvlib-python.readthedocs.io/en/v0.6.3/generated/pvlib.irradiance.dirindex.html
 #     # TODO Philip I think we should have a parameter for radiation decompositon model to allow for other models in the future, e.g. https://pvlib-python.readthedocs.io/en/v0.6.3/generated/pvlib.irradiance.erbs.html
 #     #  decomposition models are usually very easy to implement (e.g. Engerer)
-#     dni = pvlib.irradiance.dirindex(ghi=ghi,
+#     dni = pv.irradiance.dirindex(ghi=ghi,
 #                                  ghi_clearsky=plant.rd_ghi_clearsky.m_as('W m**-2'),
 #                                  dni_clearsky=plant.rd_dni_clearsky.m_as('W m**-2'),
 #                                  zenith=plant.sun_zenith.m_as('deg'),
 #                                  times=plant.time_index)
 #     return dni
 
 
 # To be uncommented, once radiation conversion for Arrays is implemented.
 # def _get_beam_from_dni(plant, dni, tilt=0, azim=180):
 #     # TODO add docstring
 #     # Calculates beam irradiance on arbitrary surface (tilt, azim) from DNI
 #     # Returns: beam irradiane (bhi or bti, depending on tilt) in W/m
 #
-#     bti = pvlib.irradiance.beam_component(surface_tilt=tilt,
+#     bti = pv.irradiance.beam_component(surface_tilt=tilt,
 #                                        surface_azimuth=azim,
 #                                        solar_zenith=plant.sun_zenith.m_as('deg'),
 #                                        solar_azimuth=plant.sun_azimuth.m_as('deg'),
 #                                        dni=dni)
 #     return bti
 
 # To be uncommented, once radiation conversion for Arrays is implemented.
@@ -161,15 +162,15 @@
 #     # ghi : array-like in W/m
 #     # Returns: ghi, bhi, dhi, DNI in W/m
 #     # Decomposition
 #
 #     if dni is None:
 #         dni = _get_dni_from_ghi(plant, ghi)
 #     bhi = _get_beam_from_dni(plant, dni)
-#     # bhi = pvlib.irradiance.beam_component(0, 180, plant.sun_zenith.to('deg'), plant.sun_azimuth.to('deg'), dni)
+#     # bhi = pv.irradiance.beam_component(0, 180, plant.sun_zenith.to('deg'), plant.sun_azimuth.to('deg'), dni)
 #     dhi = ghi - bhi
 #     return ghi, bhi, dhi, dni
 
 
 # def _get_horizontal_from_gti(plant, gti, tilt, azim):
 #     """Inverse decomposition. Returns horizontal components from global tilted irradiance with given tilt and azimuth.
 #
@@ -198,15 +199,15 @@
 #     # Horizontal components (ghi, dhi, DNI) from global tilted (gti) using gti_dirint model
 #     # https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.irradiance.gti_dirint.html
 #
 #     # TODO calculate_gt_90 should be True, but then gti_dirint throws IndexError
 #     # gti[gti<0]=0
 #
 #     # TODO use plant dew point if available
-#     horiz = pvlib.irradiance.gti_dirint(poa_global=gti,
+#     horiz = pv.irradiance.gti_dirint(poa_global=gti,
 #                                      aoi=_get_aoi(plant, tilt, azim),
 #                                      solar_zenith=plant.sun_zenith.m_as('deg'),
 #                                      solar_azimuth=plant.sun_azimuth.m_as('deg'),
 #                                      times=plant.time_index,
 #                                      surface_tilt=tilt,
 #                                      surface_azimuth=azim,
 #                                      calculate_gt_90=False,
@@ -262,15 +263,15 @@
 #     # TODO add docstring
 #
 #     # ghi, dhi : array-like in W/m
 #     # Returns: DNI in W/m
 #     # Case [c] in design https://gitlab.com/sunpeek/sunpeek/-/issues/128/
 #     # uses pvlib dni: https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.irradiance.dni.html
 #
-#     dni = pvlib.irradiance.dni(ghi=ghi,
+#     dni = pv.irradiance.dni(ghi=ghi,
 #                             dhi=dhi,
 #                             zenith=plant.sun_zenith.m_as('deg'),
 #                             clearsky_dni=plant.rd_dni_clearsky.m_as('W m**-2'))
 #     return dni
 
 
 # def _get_horizontal_from_ghi_bhi(plant, ghi, bhi):
@@ -370,32 +371,32 @@
 #         self.in_dni = in_dni
 #         self.input_pattern = self.get_radiation_pattern()
 #
 #     @abstractmethod
 #     def get_irradiance_components(self):
 #         pass
 
-# @staticmethod
-# def unpack_radiations(*args):
-#     """Tries to read radiation as numeric value from Sensor.data (of 1 or several cmp.Sensors) in unit W/m**2.
-#     Useful to prepare component sensors for radiation converter.
-#     Parameters
-#     ----------
-#     args : cmp.Sensor
-#     Returns
-#     -------
-#     tuple : pd.Series
-#         For each Sensor, returns pd.Series of Sensor.data, converted to W/m, None if Sensor is None.
-#     """
-#     return (None if elem is None else elem.m_as('W m**-2') for elem in args)
-
-# def _is_horizontal(self):
-#     """Returns tuple with bool for each of the radiation Sensors self.in_global, .in_beam, .in_diffuse.
-#     """
-#     return is_horizontal(self.in_global, self.in_beam, self.in_diffuse)
+    # @staticmethod
+    # def unpack_radiations(*args):
+    #     """Tries to read radiation as numeric value from Sensor.data (of 1 or several cmp.Sensors) in unit W/m**2.
+    #     Useful to prepare component sensors for radiation converter.
+    #     Parameters
+    #     ----------
+    #     args : cmp.Sensor
+    #     Returns
+    #     -------
+    #     tuple : pd.Series
+    #         For each Sensor, returns pd.Series of Sensor.data, converted to W/m, None if Sensor is None.
+    #     """
+    #     return (None if elem is None else elem.m_as('W m**-2') for elem in args)
+
+    # def _is_horizontal(self):
+    #     """Returns tuple with bool for each of the radiation Sensors self.in_global, .in_beam, .in_diffuse.
+    #     """
+    #     return is_horizontal(self.in_global, self.in_beam, self.in_diffuse)
 
 
 # class RadiationConversionHorizontal(RadiationConversion):
 #     """Calculates horizontal irradiance components (global, beam, diffuse, DNI) for the plant.
 #     """
 #
 #     def __init__(self, plant, *args, **kwargs):
@@ -578,185 +579,185 @@
 #         elif self.strategy is TiltedStrategy.poa:
 #             return to_rd(*self._get_poa_irradiances())
 #         elif self.strategy is TiltedStrategy.detailed:
 #             return to_rd(*self._get_array_irradiances_detailed())
 #         else:
 #             return AttributeError(f'Invalid tilted strategy {self.strategy}.')
 
-# To be uncommented, once radiation conversion for Arrays is implemented.
-# def _get_array_irradiances_detailed(self):
-#     """Returns global, beam, diffuse irradiances on array, applying detailed modeling with plane-of-array irradiances.
-#     Returns
-#     -------
-#     gti : numeric array
-#       Global irradiance on array in W/m.
-#     bti : numeric array
-#       Beam / direct irradiance on array in W/m.
-#     dti : numeric array
-#       Diffuse irradiance on array in W/m.
-#
-#     Notes
-#     -----
-#     Calculates array irradiance components global, beam / direct and diffuse.
-#     Components of poa diffuse (e.g. horizon brightness) can be combined specifically.
-#     Calls _get_poa_irradiances() to yield DNI and poa diffuse components (diffuse sky, diffuse horizon etc.)
-#     For diffuse masking, uses shadow_angle_midpoint, i.e. the angle at half of the collector's slant height.
-#     """
-#     # TODO Daniel would be good to get your ideas about details on how to calculate dti
-#     # TODO Make sure array virtual sensors exist: shadow_angle_midpoint, internal_shading_ratio, plant.ghi
-#     # TODO Philip - conversion strategies should be 'consistent', i.e. if we apply inverse, then forward direction for a sensor, this should return the sensor value
-#     #   - step 1:
-#     #   - a) using gti (in same plane as array) --> apply dirint --> keep gti, calculate bti based on DNI, "normalize" shares for iffuse_iso + diffuse_circ + diffuse_horz
-#     #   - b) using gti (in different plane than array) --> apply dirint --> use perez model
-#     #   - step 2:
-#     #   - calcuate reduction (masking, horizon) RELATIVE to measurement at the top of the collector --> we need to define the default settings here
-#     #   - later:
-#     #   - alternatively, we could think of implementing our own radiation conversion model to make this consistent, e.g. like https://www.sciencedirect.com/science/article/pii/S0038092X21000633,
-#     #     for the solar community it would be help to have a very general radiation conversion algorithm for all possible measurement setups
-#
-#     # Plane-of-array irradiance components
-#     dni, poa_diffuse_iso, poa_diffuse_circ, poa_diffuse_horiz = self._get_poa_irradiances()
-#     bti = _get_beam_from_dni(self.plant, dni, self.tilt, self.azim)
-#
-#     # Diffuse masking
-#     if self.use_diffuse_masking:
-#         # https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.shading.sky_diffuse_passias.html
-#         iso_blocked = pvlib.shading.sky_diffuse_passias(masking_angle=self.array.shadow_angle_midpoint.m_as('deg'))
-#         poa_diffuse_iso *= np.ones_like(poa_diffuse_iso) - iso_blocked
-#         # TODO Philip - This calculation is NOT correct, it does not take the tilt angle of the array into account!
-#     dti = poa_diffuse_iso
-#
-#     # Circumsolar diffuse
-#     if self.treat_circumsolar_as_diffuse:
-#         dti += poa_diffuse_circ
-#     else:  # treat circumsolar as beam
-#         bti += poa_diffuse_circ
-#
-#     # Beam shading
-#     if self.use_beam_shading:
-#         bti *= np.ones_like(bti) - self.array.internal_shading_ratio.m_as('dimensionless')
-#
-#     # Horizon brightness
-#     if self.horizon_brightness_strategy == 'ignore':
-#         pass
-#     else:
-#         dti += poa_diffuse_horiz
-#
-#     # Ground diffuse
-#     # TODO Philip - these models need to be consistent with the transposition model - I would include the options
-#     #  - a) ground reflection as in Perez
-#     #  - b) ignore ground reflection (substract this from the diffuse irradiance?)
-#     #  - b) correct ground reflection (with view factor from sensors versus towards ground)
-#     if self.ground_diffuse_strategy == 'ignore':
-#         pass
-#     elif self.ground_diffuse_strategy == 'ghi':
-#         if self.plant.ghi is None:
-#             raise VirtualSensorCalculationError(
-#                 'Calculating array irradiances with strategy "ghi" requires plant.ghi to be not None.')
-#             # https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.irradiance.get_ground_diffuse.html
-#         dti += pvlib.irradiance.get_ground_diffuse(surface_tilt=self.tilt,
-#                                                 ghi=self.plant.ghi.m_as('W m**-2'),
-#                                                 albedo=self.ground_albedo)
-#     else:
-#         raise NotImplementedError
-#
-#     gti = bti + dti
-#     return gti, bti, dti
-
-# To be uncommented, once radiation conversion for Arrays is implemented.
-# def _get_poa_irradiances(self):
-#     """Calculates in-plane radiation components from array radiation input sensors in_global, in_beam, in_diffuse,
-#     in_dni. Returns tilted in-plane (poa, plane-of-array) irradiance components for DNI, sky and ground diffuse.
-#     Returns
-#     -------
-#     rd_dni : numeric array
-#       DNI (Direct Normal Irradiance) in W/m.
-#     rd_poa_diffuse_sky : numeric array
-#       Plane-of-array (poa) in-plane diffuse irradiance from the sky in W/m.
-#     rd_poa_diffuse_ground : numeric array
-#       Plane-of-array (poa) in-plane diffuse irradiance from the ground in W/m.
-#
-#     Note:
-#     -----
-#     Does not take beam shading and diffuse masking of the array into account.
-#     Logic is described in https://gitlab.com/sunpeek/sunpeek/-/issues/128/
-#     """
-#
-#     glob, beam, diff, dni = self.unpack_radiations()
-#     is_glob_horizontal, is_beam_horizontal, is_diff_horizontal = self.is_horizontal()
-#
-#     # no radiation inputs
-#     if self.input_pattern == '0000':
-#         dni, poa_diff_iso, poa_diff_circ, poa_diff_horiz = None, None, None, None
-#     # global only
-#     elif self.input_pattern == '1000':
-#         if is_glob_horizontal:
-#             # Input glob is ghi
-#             dni, poa_diff_iso, poa_diff_circ, poa_diff_horiz = self._get_poa_from_ghi(glob)
-#         else:
-#             # Input glob is some gti: inverse decomposition
-#             raise NotImplementedError
-#             # tilt, azim = _get_sensor_orientation(array._in_glob)
-#             # dni, poa_diff_iso, poa_diff_circ, poa_diff_horiz = _get_poa_from_gti(plant, glob, tilt, azim)
-#     else:
-#         # TODO implement more tilted radiation strategies here
-#         raise NotImplementedError
-#
-#     return dni, poa_diff_iso, poa_diff_circ, poa_diff_horiz
-
-# To be uncommented, once radiation conversion for Arrays is implemented.
-# def _get_poa_from_ghi(self, ghi):
-#     # TODO add docstring
-#     # Calculates plane-of-array radiation components from global horizontal irradiance / measurement.
-#     # Combines decomposition (dirindex) and transposition (Perez)
-#     # Returns plane-of-array irradiance, i.e. does not take beam shading and diffuse masking into account.
-#     # Returns: DNI, isotropic diffuse, circumsolar diffuse, horizon brightness radiations, all in W/m
-#
-#     dni = _get_dni_from_ghi(self.plant, ghi)
-#     bhi = _get_beam_from_dni(self.plant, dni)
-#     dhi = ghi - bhi
-#     poa_diff_iso, poa_diff_circ, poa_diff_horiz = self._get_poadiffuse_from_dhi(dhi, dni)
-#     return dni, poa_diff_iso, poa_diff_circ, poa_diff_horiz
+    # To be uncommented, once radiation conversion for Arrays is implemented.
+    # def _get_array_irradiances_detailed(self):
+    #     """Returns global, beam, diffuse irradiances on array, applying detailed modeling with plane-of-array irradiances.
+    #     Returns
+    #     -------
+    #     gti : numeric array
+    #       Global irradiance on array in W/m.
+    #     bti : numeric array
+    #       Beam / direct irradiance on array in W/m.
+    #     dti : numeric array
+    #       Diffuse irradiance on array in W/m.
+    #
+    #     Notes
+    #     -----
+    #     Calculates array irradiance components global, beam / direct and diffuse.
+    #     Components of poa diffuse (e.g. horizon brightness) can be combined specifically.
+    #     Calls _get_poa_irradiances() to yield DNI and poa diffuse components (diffuse sky, diffuse horizon etc.)
+    #     For diffuse masking, uses shadow_angle_midpoint, i.e. the angle at half of the collector's slant height.
+    #     """
+    #     # TODO Daniel would be good to get your ideas about details on how to calculate dti
+    #     # TODO Make sure array virtual sensors exist: shadow_angle_midpoint, internal_shading_ratio, plant.ghi
+    #     # TODO Philip - conversion strategies should be 'consistent', i.e. if we apply inverse, then forward direction for a sensor, this should return the sensor value
+    #     #   - step 1:
+    #     #   - a) using gti (in same plane as array) --> apply dirint --> keep gti, calculate bti based on DNI, "normalize" shares for iffuse_iso + diffuse_circ + diffuse_horz
+    #     #   - b) using gti (in different plane than array) --> apply dirint --> use perez model
+    #     #   - step 2:
+    #     #   - calcuate reduction (masking, horizon) RELATIVE to measurement at the top of the collector --> we need to define the default settings here
+    #     #   - later:
+    #     #   - alternatively, we could think of implementing our own radiation conversion model to make this consistent, e.g. like https://www.sciencedirect.com/science/article/pii/S0038092X21000633,
+    #     #     for the solar community it would be help to have a very general radiation conversion algorithm for all possible measurement setups
+    #
+    #     # Plane-of-array irradiance components
+    #     dni, poa_diffuse_iso, poa_diffuse_circ, poa_diffuse_horiz = self._get_poa_irradiances()
+    #     bti = _get_beam_from_dni(self.plant, dni, self.tilt, self.azim)
+    #
+    #     # Diffuse masking
+    #     if self.use_diffuse_masking:
+    #         # https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.shading.sky_diffuse_passias.html
+    #         iso_blocked = pv.shading.sky_diffuse_passias(masking_angle=self.array.shadow_angle_midpoint.m_as('deg'))
+    #         poa_diffuse_iso *= np.ones_like(poa_diffuse_iso) - iso_blocked
+    #         # TODO Philip - This calculation is NOT correct, it does not take the tilt angle of the array into account!
+    #     dti = poa_diffuse_iso
+    #
+    #     # Circumsolar diffuse
+    #     if self.treat_circumsolar_as_diffuse:
+    #         dti += poa_diffuse_circ
+    #     else:  # treat circumsolar as beam
+    #         bti += poa_diffuse_circ
+    #
+    #     # Beam shading
+    #     if self.use_beam_shading:
+    #         bti *= np.ones_like(bti) - self.array.internal_shading_ratio.m_as('dimensionless')
+    #
+    #     # Horizon brightness
+    #     if self.horizon_brightness_strategy == 'ignore':
+    #         pass
+    #     else:
+    #         dti += poa_diffuse_horiz
+    #
+    #     # Ground diffuse
+    #     # TODO Philip - these models need to be consistent with the transposition model - I would include the options
+    #     #  - a) ground reflection as in Perez
+    #     #  - b) ignore ground reflection (substract this from the diffuse irradiance?)
+    #     #  - b) correct ground reflection (with view factor from sensors versus towards ground)
+    #     if self.ground_diffuse_strategy == 'ignore':
+    #         pass
+    #     elif self.ground_diffuse_strategy == 'ghi':
+    #         if self.plant.ghi is None:
+    #             raise VirtualSensorCalculationError(
+    #                 'Calculating array irradiances with strategy "ghi" requires plant.ghi to be not None.')
+    #             # https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.irradiance.get_ground_diffuse.html
+    #         dti += pv.irradiance.get_ground_diffuse(surface_tilt=self.tilt,
+    #                                                 ghi=self.plant.ghi.m_as('W m**-2'),
+    #                                                 albedo=self.ground_albedo)
+    #     else:
+    #         raise NotImplementedError
+    #
+    #     gti = bti + dti
+    #     return gti, bti, dti
 
-# To be uncommented, once radiation conversion for Arrays is implemented.
-# def _get_poa_from_gti(self, gti):
-#     # TODO add docstring
-#     # Calculates plane-of-array radiation components from global tilted irradiance / measurement.
-#     raise NotImplementedError
-#     # tilt, azim = _get_sensor_orientation(array.in_global)
-#     # gti_dirint
+    # To be uncommented, once radiation conversion for Arrays is implemented.
+    # def _get_poa_irradiances(self):
+    #     """Calculates in-plane radiation components from array radiation input sensors in_global, in_beam, in_diffuse,
+    #     in_dni. Returns tilted in-plane (poa, plane-of-array) irradiance components for DNI, sky and ground diffuse.
+    #     Returns
+    #     -------
+    #     rd_dni : numeric array
+    #       DNI (Direct Normal Irradiance) in W/m.
+    #     rd_poa_diffuse_sky : numeric array
+    #       Plane-of-array (poa) in-plane diffuse irradiance from the sky in W/m.
+    #     rd_poa_diffuse_ground : numeric array
+    #       Plane-of-array (poa) in-plane diffuse irradiance from the ground in W/m.
+    #
+    #     Note:
+    #     -----
+    #     Does not take beam shading and diffuse masking of the array into account.
+    #     Logic is described in https://gitlab.com/sunpeek/sunpeek/-/issues/128/
+    #     """
+    #
+    #     glob, beam, diff, dni = self.unpack_radiations()
+    #     is_glob_horizontal, is_beam_horizontal, is_diff_horizontal = self.is_horizontal()
+    #
+    #     # no radiation inputs
+    #     if self.input_pattern == '0000':
+    #         dni, poa_diff_iso, poa_diff_circ, poa_diff_horiz = None, None, None, None
+    #     # global only
+    #     elif self.input_pattern == '1000':
+    #         if is_glob_horizontal:
+    #             # Input glob is ghi
+    #             dni, poa_diff_iso, poa_diff_circ, poa_diff_horiz = self._get_poa_from_ghi(glob)
+    #         else:
+    #             # Input glob is some gti: inverse decomposition
+    #             raise NotImplementedError
+    #             # tilt, azim = _get_sensor_orientation(array._in_glob)
+    #             # dni, poa_diff_iso, poa_diff_circ, poa_diff_horiz = _get_poa_from_gti(plant, glob, tilt, azim)
+    #     else:
+    #         # TODO implement more tilted radiation strategies here
+    #         raise NotImplementedError
+    #
+    #     return dni, poa_diff_iso, poa_diff_circ, poa_diff_horiz
 
-# To be uncommented, once radiation conversion for Arrays is implemented.
-# def _get_poadiffuse_from_dhi(self, dhi, dni):
-#     # TODO add docstring
-#     # dhi, dni : array-like in W/m
-#     # Returns:
-#     # --------
-#     # poa_diff_iso : numeric
-#     #   Tilted / poa isotropic diffuse radiation, in W/m
-#     # poa_diff_circ : numeric
-#     #   Tilted / poa circumsolar radiation, in W/m
-#     # poa_diff_horiz : numeric
-#     #   Tilted /poa horizon brightness radiation, in W/m
-#     # Transposition: Calculates sky diffuse irradiance in plane of array from global and diffuse horizontal.
-#     # This implementation uses the Perez method:
-#     # https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.irradiance.perez.html
-#     # case [f] in design https://gitlab.com/sunpeek/sunpeek/-/issues/128/
-#     # TODO Make sure the necessary sensors like plant.sun_apparent_zenit, plant.rel_airmass exist / not None
-#
-#     diffuse = pvlib.irradiance.perez(surface_tilt=self.tilt,
-#                                   surface_azimuth=self.azim,
-#                                   dhi=dhi,
-#                                   dni=dni,
-#                                   dni_extra=self.plant.rd_dni_extra.m_as('W m**-2'),
-#                                   solar_zenith=self.plant.sun_apparent_zenith.m_as('deg'),
-#                                   solar_azimuth=self.plant.sun_azimuth.m_as('deg'),
-#                                   airmass=self.plant.rel_airmass.m_as(''),
-#                                   return_components=True)
-#     return diffuse['isotropic'], diffuse['circumsolar'], diffuse['horizon']
+    # To be uncommented, once radiation conversion for Arrays is implemented.
+    # def _get_poa_from_ghi(self, ghi):
+    #     # TODO add docstring
+    #     # Calculates plane-of-array radiation components from global horizontal irradiance / measurement.
+    #     # Combines decomposition (dirindex) and transposition (Perez)
+    #     # Returns plane-of-array irradiance, i.e. does not take beam shading and diffuse masking into account.
+    #     # Returns: DNI, isotropic diffuse, circumsolar diffuse, horizon brightness radiations, all in W/m
+    #
+    #     dni = _get_dni_from_ghi(self.plant, ghi)
+    #     bhi = _get_beam_from_dni(self.plant, dni)
+    #     dhi = ghi - bhi
+    #     poa_diff_iso, poa_diff_circ, poa_diff_horiz = self._get_poadiffuse_from_dhi(dhi, dni)
+    #     return dni, poa_diff_iso, poa_diff_circ, poa_diff_horiz
+
+    # To be uncommented, once radiation conversion for Arrays is implemented.
+    # def _get_poa_from_gti(self, gti):
+    #     # TODO add docstring
+    #     # Calculates plane-of-array radiation components from global tilted irradiance / measurement.
+    #     raise NotImplementedError
+    #     # tilt, azim = _get_sensor_orientation(array.in_global)
+    #     # gti_dirint
+
+    # To be uncommented, once radiation conversion for Arrays is implemented.
+    # def _get_poadiffuse_from_dhi(self, dhi, dni):
+    #     # TODO add docstring
+    #     # dhi, dni : array-like in W/m
+    #     # Returns:
+    #     # --------
+    #     # poa_diff_iso : numeric
+    #     #   Tilted / poa isotropic diffuse radiation, in W/m
+    #     # poa_diff_circ : numeric
+    #     #   Tilted / poa circumsolar radiation, in W/m
+    #     # poa_diff_horiz : numeric
+    #     #   Tilted /poa horizon brightness radiation, in W/m
+    #     # Transposition: Calculates sky diffuse irradiance in plane of array from global and diffuse horizontal.
+    #     # This implementation uses the Perez method:
+    #     # https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.irradiance.perez.html
+    #     # case [f] in design https://gitlab.com/sunpeek/sunpeek/-/issues/128/
+    #     # TODO Make sure the necessary sensors like plant.sun_apparent_zenit, plant.rel_airmass exist / not None
+    #
+    #     diffuse = pv.irradiance.perez(surface_tilt=self.tilt,
+    #                                   surface_azimuth=self.azim,
+    #                                   dhi=dhi,
+    #                                   dni=dni,
+    #                                   dni_extra=self.plant.rd_dni_extra.m_as('W m**-2'),
+    #                                   solar_zenith=self.plant.sun_apparent_zenith.m_as('deg'),
+    #                                   solar_azimuth=self.plant.sun_azimuth.m_as('deg'),
+    #                                   airmass=self.plant.rel_airmass.m_as(''),
+    #                                   return_components=True)
+    #     return diffuse['isotropic'], diffuse['circumsolar'], diffuse['horizon']
 
 ## old stuff
 # pvlib surfaces (for Array definition)
 # https://pvlib-python.readthedocs.io/en/stable/_modules/pvlib/irradiance.html?highlight=surface_albedos
 # SURFACE_ALBEDOS = {'urban': 0.18,
 #                    'grass': 0.20,
 #                    'fresh grass': 0.26,
```

## sunpeek/core_methods/virtuals/virtuals_array.py

```diff
@@ -3,23 +3,23 @@
 from sunpeek.common.errors import CalculationError
 
 
 def config_virtuals_ambient(array):
     """Virtual sensors for sun- and shadow-related stuff in array.
     """
     # Angle of incidence
-    feedback = algos.AngleOfIncidence(array).get_config_feedback()
-    array.map_vsensor('aoi', feedback)
+    problems = algos.AngleOfIncidence(array).get_config_problems()
+    array.map_vsensor('aoi', problems)
 
     # Internal shading
-    feedback = algos.InternalShading(array).get_config_feedback()
-    array.map_vsensor('is_shadowed', feedback)
-    array.map_vsensor('internal_shading_ratio', feedback)
-    array.map_vsensor('shadow_angle', feedback)
-    array.map_vsensor('shadow_angle_midpoint', feedback)
+    problems = algos.InternalShading(array).get_config_problems()
+    array.map_vsensor('is_shadowed', problems)
+    array.map_vsensor('internal_shading_ratio', problems)
+    array.map_vsensor('shadow_angle', problems)
+    array.map_vsensor('shadow_angle_midpoint', problems)
 
 
 def calculate_virtuals_ambient(array):
     # Angle of incidence
     result = algos.AngleOfIncidence(array).run()
     array.aoi.update('aoi', result)
 
@@ -32,50 +32,50 @@
     array.internal_shading_ratio.update('internal_shading_ratio', result)
     array.shadow_angle.update('shadow_angle', result)
     array.shadow_angle_midpoint.update('shadow_angle_midpoint', result)
 
 
 def config_virtuals_power(array):
     # Thermal power
-    array.map_vsensor('tp', algos.ThermalPower(array).get_config_feedback())
+    array.map_vsensor('tp', algos.ThermalPower(array).get_config_problems())
     # Mass flow
-    array.map_vsensor('mf', algos.MassFlow(array).get_config_feedback())
+    array.map_vsensor('mf', algos.MassFlow(array).get_config_problems())
 
 
 def calculate_virtuals_power(array):
     # Thermal power
     array.tp.update('tp', algos.ThermalPower(array).run())
     # Mass flow
     array.mf.update('mf', algos.MassFlow(array).run())
 
 
 def config_virtuals_temperature(array):
     """Virtual sensors for mean operating temperature, temperature derivative etc.
     """
-    feedback = algos.ArrayTemperatures(array).get_config_feedback()
-    array.map_vsensor('te_op', feedback)
-    array.map_vsensor('te_op_deriv', feedback)
+    problems = algos.ArrayTemperatures(array).get_config_problems()
+    array.map_vsensor('te_op', problems)
+    array.map_vsensor('te_op_deriv', problems)
 
 
 def calculate_virtuals_temperature(array):
-    feedback = algos.ArrayTemperatures(array).run()
-    array.te_op.update('te_op', feedback)
-    array.te_op_deriv.update('te_op_deriv', feedback)
+    result = algos.ArrayTemperatures(array).run()
+    array.te_op.update('te_op', result)
+    array.te_op_deriv.update('te_op_deriv', result)
 
 
 def config_virtuals_radiation(array):
     """Array plane-of-array irradiance (global, beam, diffuse) including masking, shading etc.
     """
-    problems = algos.TiltedIrradiances(array).get_config_feedback()
+    problems = algos.TiltedIrradiances(array).get_config_problems()
     array.map_vsensor('rd_gti', problems)
     array.map_vsensor('rd_bti', problems)
     array.map_vsensor('rd_dti', problems)
 
     # Incidence angle modifier
-    array.map_vsensor('iam', algos.AlgoIAM(array).get_config_feedback())
+    array.map_vsensor('iam', algos.AlgoIAM(array).get_config_problems())
 
 
 def calculate_virtuals_radiation(array, strategy: Optional[str] = None):
     """Array irradiance components, including beam shading and diffuse masking.
     """
     if strategy is None:
         algo = algos.TiltedIrradiances(array)
```

## sunpeek/core_methods/virtuals/virtuals_plant.py

```diff
@@ -1,44 +1,44 @@
 from sunpeek.core_methods.virtuals import calculations as algos
 
 
 def config_virtuals_ambient(plant):
     """Solar position, airmass, clearsky radiation, dew point"""
-    feedback = algos.SolarPosition(plant).get_config_feedback()
-    plant.map_vsensor('sun_azimuth', feedback)
-    plant.map_vsensor('sun_zenith', feedback)
-    plant.map_vsensor('sun_apparent_zenith', feedback)
-    plant.map_vsensor('sun_elevation', feedback)
-    plant.map_vsensor('sun_apparent_elevation', feedback)
+    problems = algos.SolarPosition(plant).get_config_problems()
+    plant.map_vsensor('sun_azimuth', problems)
+    plant.map_vsensor('sun_zenith', problems)
+    plant.map_vsensor('sun_apparent_zenith', problems)
+    plant.map_vsensor('sun_elevation', problems)
+    plant.map_vsensor('sun_apparent_elevation', problems)
 
     # The following algorithms are commented because they are not needed for now (because radiation conversion
     # is not active at the moment).
 
     # Dew point temperature
-    # feedback = algos.DewPointTemperature(plant).get_config_feedback()
-    # plant.map_vsensor('te_dew_amb', feedback)
+    # problems = algos.DewPointTemperature(plant).config_problems
+    # plant.map_vsensor('te_dew_amb', problems)
 
     # Clearsky solar radiation
-    # feedback = algos.DNIExtra(plant).get_config_feedback()
-    # plant.map_vsensor('rd_dni_extra', feedback)
+    # problems = algos.DNIExtra(plant).config_problems
+    # plant.map_vsensor('rd_dni_extra', problems)
 
     # Airmass
-    # feedback = algos.Airmass(plant).get_config_feedback()
-    # plant.map_vsensor('rel_airmass', feedback)
-    # plant.map_vsensor('abs_airmass', feedback)
+    # problems = algos.Airmass(plant).config_problems
+    # plant.map_vsensor('rel_airmass', problems)
+    # plant.map_vsensor('abs_airmass', problems)
 
     # Turbidity
     # Not needed for now, uncomment if needed.
-    # feedback = algos.LinkeTurbidity(plant).get_config_feedback()
-    # plant.map_vsensor('linke_turbidity', feedback)
+    # problems = algos.LinkeTurbidity(plant).config_problems
+    # plant.map_vsensor('linke_turbidity', problems)
 
     # Clearsky radiation
-    # feedback = algos.ClearskyRadiation(plant).get_config_feedback()
-    # plant.map_vsensor('rd_ghi_clearsky', feedback)
-    # plant.map_vsensor('rd_dni_clearsky', feedback)
+    # problems = algos.ClearskyRadiation(plant).config_problems
+    # plant.map_vsensor('rd_ghi_clearsky', problems)
+    # plant.map_vsensor('rd_dni_clearsky', problems)
 
 
 def calculate_virtuals_ambient(plant):
     # Solar position
     result = algos.SolarPosition(plant).run()
     plant.sun_azimuth.update('azimuth', result)
     plant.sun_zenith.update('zenith', result)
@@ -67,17 +67,17 @@
     # result = algos.ClearskyRadiation(plant).run()
     # plant.rd_ghi_clearsky.update('ghi_clearsky', result)
     # plant.rd_dni_clearsky.update('dni_clearsky', result)
 
 
 def config_virtuals_power(plant):
     # Thermal power
-    plant.map_vsensor('tp', algos.ThermalPower(plant).get_config_feedback())
+    plant.map_vsensor('tp', algos.ThermalPower(plant).get_config_problems())
     # Mass flow
-    plant.map_vsensor('mf', algos.MassFlow(plant).get_config_feedback())
+    plant.map_vsensor('mf', algos.MassFlow(plant).get_config_problems())
 
 
 def calculate_virtuals_power(plant):
     # Thermal power
     result = algos.ThermalPower(plant).run()
     plant.tp.update('tp', result)
     # Mass flow
@@ -85,19 +85,19 @@
     plant.mf.update('mf', result)
 
 
 def config_virtuals_radiation(plant):
     """Horizontal irradiance components from plant radiation input slots
     """
     pass
-#     # feedback = algos.HorizontalIrradiances(plant).get_config_feedback()
-#     # plant.map_vsensor('rd_ghi', feedback)
-#     # plant.map_vsensor('rd_bhi', feedback)
-#     # plant.map_vsensor('rd_dhi', feedback)
-#     # plant.map_vsensor('rd_dni', feedback)
+#     # problems = algos.HorizontalIrradiances(plant).config_problems
+#     # plant.map_vsensor('rd_ghi', problems)
+#     # plant.map_vsensor('rd_bhi', problems)
+#     # plant.map_vsensor('rd_dhi', problems)
+#     # plant.map_vsensor('rd_dni', problems)
 
 
 def calculate_virtuals_radiation(plant):
     """Horizontal irradiance components from plant radiation input slots
     """
     pass
 #     # ghi, bhi, dhi, dni = algos.HorizontalIrradiances(plant).run()
```

## sunpeek/data_handling/context.py

```diff
@@ -5,727 +5,246 @@
 - Database is e.g. for use over WebUI, DataFrame for use as a package and for testing.
 A sensor cache is implemented to speed up database access times.
 - To quickly add csv data / test something, just do: use_csv(plant, [file_list])
 
 Internally, data is always stored without unit (double / float64); the correct unit (stored in sensor.native_unit)
 is attached at data access.
 """
-from abc import ABC, abstractmethod
-from pathlib import Path
+
 import numpy as np
 import enum
-import datetime as dt
+import datetime
 import warnings
-from typing import Union, Dict, Tuple
+from typing import Union, Dict
 from dataclasses import dataclass
 import pandas as pd
+import datetime
 import pytz
 from pydantic import validator
 
 import parquet_datastore_utils as pu
 from sunpeek.common.utils import sp_logger
 from sunpeek.base_model import BaseModel
-from sunpeek.db_utils import DATETIME_COL_NAME, PARTITION_COLS
-import sunpeek.common.errors as err
+from sunpeek.db_utils import DATETIME_COL_NAME
+from sunpeek.common.errors import ConfigurationError, CalculationError, TimeZoneError, TimeIndexError
 from sunpeek.common.time_zone import validate_timezone
 import sunpeek.core_methods.virtuals as virtuals
 
 
 class NanReportResponse(BaseModel):
     nan_report: Union[Dict[str, str], None]
 
     @validator('nan_report', pre=True)
     def df_to_val(cls, dct):
         if dct is not None:
             return {k: v.to_json(date_format='iso') for k, v in dct.items() if not isinstance(v, str)}
 
 
 def import_db_ops():
-    """Import the db_data_operations module and raises a specific error if any of the required modules is missing.
+    """
+    Imports the db_data_operations module and raises a specific error if any of the required modules is missing.
     This is used because some functions in this module only need the db modules imported when using the database backend
     """
     try:
         import sunpeek.db_utils.db_data_operations as db_ops
         return db_ops
     except ModuleNotFoundError:
         raise ModuleNotFoundError("Some modules that a required to work with the database backend are not installed. "
                                   "They can be installed with pip install sunpeek[db].")
 
 
-def sanitize_index(df: pd.DataFrame) -> (pd.DataFrame, int):
-    """Sort DataFrame index, removes NaN entries and duplicates. Guarantees df has a sorted and unique DatetimeIndex.
-
-    Parameters
-    ----------
-    df : pandas.DataFrame, must have DatetimeIndex.
-
-    Returns
-    -------
-    tuple : DataFrame with sanitized index, and number of duplicate time index entries (handy to spot error in data
-    definition, where certain time stamps happen to exist twice if the wrong time zone is selected).
-
-    Raises
-    ------
-    err.TimeIndexError : If df has no DatetimeIndex or the resulting index is not sorted (monotonic increasing).
-
-    Notes
-    -----
-    All duplicate entries are deleted. Automatically keeping any of them (e.g. always keep the first entry) is risky
-    because duplicates in the time index typically point to a problem in data logging or tagging, e.g. wrong time zone
-    setting in a data logger, or time zone mis-specifiec in SunPeek, or time zone changed at some point during the
-    data acquisition interval, etc.
-    """
+def sanitize_dataframe(df: pd.DataFrame) -> (pd.DataFrame, int):
     if df is None:
         return None, 0
 
-    if not isinstance(df.index, pd.DatetimeIndex):
-        raise err.TimeIndexError(f'Index error in DataFrame uploaded or passed to backend: Expected DatetimeIndex, '
-                                 f'but got {type(df.index)}.')
-
     df = df[~df.index.isna()]
     # Some core methods require data to be sorted (e.g. PC Method extended -> rolling operation)
     df = df.sort_index()
     # Check for duplicate indices
-    is_duplicate = df.index.duplicated(keep=False)
-    n_duplicates_index = is_duplicate.sum()
-    df = df[~is_duplicate]
+    index_duplicates = df.index.duplicated(keep=False)
+    n_duplicates_index = index_duplicates.sum()
+    df = df[~index_duplicates]
 
     if not df.index.is_monotonic_increasing:
-        raise err.TimeIndexError('Index error in DataFrame uploaded or passed to backend: '
-                                 'index could not be sorted (index is non-monotonic).')
-
-    # Index is always stored in UTC to avoid possible issues with parquet etc.
-    # See https://gitlab.com/sunpeek/sunpeek/-/issues/500
-    df.index = df.index.tz_convert('UTC')
+        raise TimeIndexError('DataFrame uploaded or passed to backend has non-monotonic index.')
 
     return df, n_duplicates_index
 
 
-class DataSources(str, enum.Enum):
-    pq = "pq"  # parquet
-    parquet = "pq"
-    df = "df"  # dataframe
-    dataframe = "df"
-    none = "none"
-
-
-class DataCache:
-    """Implements an ephemeral storage of sensor data and time index.
-    Data access from cache is faster than reading from parquet, and should be about
-    the same as from DataFrame.
-    This :class:`DataCache` is a lightweight wrapper around a dictionary. It implements some additional
-    checks / sanitizing and utils for using the cache.
-    """
-
-    def __init__(self):
-        self.cache = {}
-
-    def add(self, key: str, val: pd.Series | pd.DatetimeIndex,
-            ) -> None:
-        if val is None:
-            return
-        # Sanitize input
-        if not self.is_empty and (len(self) != len(val)):
-            raise ValueError(f'Cannot add value of length {len(val)}. '
-                             f'Cache must have unique length. Length is fixed at {len(self)}.')
-        self.cache[key] = val
-
-    def get(self, key: str, none_if_not_found: bool = False) -> pd.Series | pd.DatetimeIndex | None:
-        if key not in self:
-            if none_if_not_found:
-                return None
-            raise KeyError(f'No value for key {key} found in cache.')
-        return self.cache[key]
-
-    def reset(self):
-        self.cache = {}
-
-    def __contains__(self, key: str) -> bool:
-        return key in self.cache
-
-    def __len__(self):
-        # "vertical len", the len of pd.Series | DatetimeIndex
-        if self.is_empty:
-            return None
-        return len(next(iter(self.cache.values())))
-
-    @property
-    def is_empty(self):
-        # True if cache holds no values
-        return len(self.cache.keys()) == 0
-
-
-class DataOperations:
-    """This class encapsulates common data operations and data processing.
-    It uses DataOps objects, depending on the given context datasource.
-    DataOperations does not interfere with the context cache. Cache operations should be done in Context.
-    """
-
-    def __init__(self, context):
-        self.context = context
-
-    @property
-    def data_ops(self):
-        match self.context.datasource:
-            case DataSources.dataframe:
-                return DataOps_df(self.context)
-            case DataSources.parquet:
-                return DataOps_pq(self.context)
-            case _:
-                raise err.ConfigurationError(f'Cannot perform operation: Invalid context.datasource. '
-                                             f'Maybe Context is uninitialized. '
-                                             f'Context.datasource is {self.context.datasource.none}.')
-
-    def get_time_index(self) -> pd.DatetimeIndex | None:
-        return self.data_ops.get_time_index()
-
-    def get_sensor_data(self, sensor: 'sunpeek.components.Sensor') -> pd.Series:
-        s = self.data_ops.get_sensor_data(sensor)
-        if s is None:
-            raise err.ConfigurationError(f'Retrieved sensor value for {sensor.raw_name} is None.')
-
-        # Attach unit, call data cleaning
-        s = self._process_data__native_unit(sensor, s)
-        native_unit = sensor.native_unit if (sensor.native_unit is not None) else ""
-        s = s.astype(f'pint[{native_unit}]')
-
-        return s
-
-    def delete_sensor_data(self, start: dt.datetime, end: dt.datetime) -> None:
-        match self.context.datasource:
-            case DataSources.dataframe:
-                df = self.data_ops.delete_sensor_data(start, end)
-                self.context.use_dataframe(df)
-            case DataSources.parquet:
-                self.data_ops.delete_sensor_data(start, end)
-        return
-
-    def delete_all_data(self) -> None:
-        self.data_ops.delete_all_data()
-        return
-
-    def sanitize_virtual_data(self, sensor: 'sunpeek.components.Sensor', data: pd.Series) -> pd.Series:
-        # If a virtual sensor calculation was not possible (for whatever reason), Context will store an all-NaN series.
-        idx = self.context.time_index
-        if data is None:
-            return pd.Series(data=np.nan, index=idx, name=sensor.raw_name)
-
-        if len(data) != len(idx):
-            raise err.CalculationError(f'Size of virtual sensor data {len(data)} '
-                                       f'is incompatible with size of Plant.time_index {len(idx)}.')
-
-        data.index = idx
-        # pint unit is stored with sensor.native_unit and attached at context.get_sensor_data
-        # Double astype() needed to go from 'pint[xx]' to numpy float, not only to PandasDtype('float64')
-        data = data.astype(float).astype(float)
-        data.name = sensor.raw_name
-        # All subsequent algorithms shall rely that everything is either a number or NaN.
-        # Inf may arise in virtual sensors, e.g. CoolProp returns Inf when temperature exceeds allowed range.
-        data[~np.isfinite(data)] = np.nan
-
-        return data
-
-    def flush_virtuals(self) -> None:
-        # Only for parquet datasource: batch write virtuals to parquet
-        if self.context.datasource != DataSources.parquet:
-            raise ValueError("Storing virtual sensor data to parquet is only possible for parquet datasource.")
-        self.data_ops.flush_virtuals()
-
-    def _process_data__native_unit(self,
-                                   sensor: 'sunpeek.components.Sensor',
-                                   s_raw: pd.Series) -> pd.Series:
-        """This is the main data processing method, it implements e.g. min max filtering, plant.ignored_ranges etc.
-        This method is intended for sensor data which is given as numeric pd.Series, not dtype pint, to accelerate
-        runtime.
-
-        Parameters
-        ----------
-        sensor : Sensor. Process this sensor's data.
-        s_raw : pd.Series
-            Unprocessed data for sensor, with float (or equiv.) dtype, typically obtained from self.get_sensor_data()
-
-        Returns
-        -------
-        s : pd.Series
-            Processed data for sensor, with numeric dtype.
-        """
-        # Copying is necessary to avoid that data processing overwrites the dataframe column with processed values.
-        # This ensures that data processing is done on the fly, and changing e.g. data processing limits or
-        # a sensor native_unit will still use the originally-parsed values (from data_uploader),
-        # as opposed to the previously processed values.
-        s = s_raw.copy()
-
-        # Set values in ignored ranges to NaN
-        for ignored_range in self.context.plant.ignored_ranges:  # pd.Interval
-            mask = (s.index >= ignored_range.left) & (s.index <= ignored_range.right)
-            s[mask] = np.nan
-
-        # Lower and Upper replacement intervals (see HarvestIT #177)
-        s = self._replace_lower__native(s, sensor)
-        s = self._replace_upper__native(s, sensor)
-
-        return s
-
-    @staticmethod
-    def _replace_lower__native(data: pd.Series,
-                               sensor: 'sunpeek.components.Sensor') -> pd.Series:
-        """Implement lower replacement interval, see #177.
-
-        Parameters
-        ----------
-        data : pd.Series with unprocessed data
-        sensor : Sensor
-
-        Returns
-        -------
-        s : pd.Series with replaced values.
-        """
-        left, right, replace = sensor.value_replacements__native['lower']
-        if (left is None) and (right is None) and (replace is None):
-            # all None: nothing to do
-            return data
-
-        if right is None and replace is None:
-            # no replacement value given, only left is not None
-            data[data < left] = np.nan
-            return data
-
-        if left is None and replace is None:
-            # no replacement value given, only right is not None
-            data[data < right] = np.nan
-            return data
-
-        if left is None:
-            data[data < right] = replace
-            return data
-
-        # all are not-NaN
-        data[data < left] = np.nan
-        data[(data >= left) & (data < right)] = replace
-        # Does not work, package incompatibility... s[(s >= left) & (s < right)] = replace
-        return data
-
-    @staticmethod
-    def _replace_upper__native(data: pd.Series,
-                               sensor: 'sunpeek.components.Sensor') -> pd.Series:
-        """Implement upper replacement interval, see #177.
-
-        Parameters
-        ----------
-        data : pd.Series with unprocessed data
-        sensor : Sensor
-
-        Returns
-        -------
-        s : pd.Series with replaced values.
-        """
-        left, right, replace = sensor.value_replacements__native['upper']
-        if (left is None) and (right is None) and (replace is None):
-            # all None: nothing to do
-            return data
-
-        if left is None and replace is None:
-            # no replacement value given, only right is not None
-            data[data > right] = np.nan
-            return data
-
-        if right is None and replace is None:
-            # no replacement value given, only left is not None
-            data[data > left] = np.nan
-            return data
-
-        if right is None:
-            data[data > left] = replace
-            return data
-
-        # all are not-NaN
-        data[data > right] = np.nan
-        data[(data > left) & (data <= right)] = replace
-        return data
-
-
-class DataOps(ABC):
-    """Interface for data operations supported by Context datasource.
-    """
-
-    def __init__(self, context):
-        self.context = context
-
-    @abstractmethod
-    def get_time_index(self):
-        pass
-
-    @abstractmethod
-    def get_sensor_data(self, sensor):
-        pass
-
-    @abstractmethod
-    def delete_sensor_data(self):
-        pass
-
-    @abstractmethod
-    def delete_all_data(self):
-        pass
-
-
-class DataOps_df(DataOps):
-    """Data operations for dataframe datasource.
-    """
-
-    def get_time_index(self) -> pd.DatetimeIndex | None:
-        df = self.context.df
-        if df is None:
-            return None
-        idx = df.index
-        idx = idx[(idx >= self.context.eval_start) & (idx <= self.context.eval_end)]
-
-        return idx
-
-    def get_sensor_data(self, sensor) -> pd.Series:
-        df = self.context.df
-        if sensor.raw_name not in df.columns:
-            raise KeyError(f'Data for sensor {sensor.raw_name} was not found in the dataframe.'
-                           f' The context datasource is {self.context.datasource}.')
-        s = df.loc[self.context.eval_start:self.context.eval_end, sensor.raw_name]
-        return s
-
-    def delete_sensor_data(self, start: dt.datetime, end: dt.datetime) -> pd.DataFrame | None:
-        # Delete data, return DataFrame with remaining data
-        if start.tzinfo is None or end.tzinfo is None:
-            raise ValueError('In a context with dataframe datasource, start and end must be timezone-aware.')
-        if start > end:
-            raise ValueError(f'Timestamp "start" must be equal or less than "end". You provided '
-                             f'start={start.isoformat()}, end={end.isoformat()}.')
-        df = self.context.df
-        filtered_df = df.loc[(df.index < start) | (df.index > end)]
-        filtered_df = filtered_df if len(filtered_df) else None
-        return filtered_df
-
-    def delete_all_data(self) -> None:
-        self.context._df = None
-        self.context.set_eval_interval()
-
-class DataOps_pq(DataOps):
-    """Data operations for parquet datasource.
-    """
-
-    def get_time_index(self) -> pd.DatetimeIndex | None:
-        try:
-            idx = pu.read(self.context.plant.raw_data_path, columns=[DATETIME_COL_NAME],
-                          start_end=(self.context.eval_start, self.context.eval_end)).index
-            if not len(idx):
-                return None
-        except FileNotFoundError:  # returned by parquet-datastore-utils if data folder doesn't exist
-            return None
-        return idx
-
-    def get_sensor_data(self, sensor) -> pd.Series:
-        plant = self.context.plant
-        uri = plant.calc_data_path if sensor.is_virtual else plant.raw_data_path
-        type_dict = {sensor.raw_name: str} if getattr(sensor.sensor_type, 'compatible_unit_str', '') == 'str' else \
-            {sensor.raw_name: float}
-        sensor_data_df = pu.read(uri=uri, columns=[sensor.raw_name], types_dict=type_dict,
-                                 start_end=(self.context.eval_start, self.context.eval_end))
-        s = sensor_data_df.squeeze()
-        return s
-
-    def delete_sensor_data(self, start: dt.datetime, end: dt.datetime) -> None:
-        # Start and end format required by parquet-datastore-utils.
-        pd_dates = [pd.to_datetime(x) for x in [start, end]]
-        start_, end_ = ({'timestamp': x.to_pydatetime(), 'year': x.year, 'quarter': x.quarter} for x in pd_dates)
-        pu.delete_between(self.context.plant.raw_data_path, start_, end_, partition_cols=PARTITION_COLS)
-        pu.delete_between(self.context.plant.calc_data_path, start_, end_, partition_cols=PARTITION_COLS)
-        return
-
-    def delete_all_data(self) -> None:
-        """Delete parquet directories for raw and calc data.
-        """
-        def rmtree(root):
-            # Remove directory tree + contents
-            if not root.exists():
-                # This may happen if plant has no data uploaded
-                return
-            for p in root.iterdir():
-                if p.is_dir():
-                    rmtree(p)
-                else:
-                    p.unlink()
-            root.rmdir()
-
-        plant = self.context.plant
-        for f in [plant.raw_data_path, plant.calc_data_path]:
-            rmtree(Path(f))
-        self.context.set_eval_interval()
-        return
-
-    def flush_virtuals(self) -> None:
-        df = pd.DataFrame({name: self.context.cache.get(name).astype(float).astype(float)
-                           for name in self.context.plant.get_raw_names(only_virtuals=True)})
-
-        # Double astype() needed to go from 'pint[xx]' to numpy float, not only to PandasDtype('float64')
-        assert set(PARTITION_COLS) == {'year', 'quarter'}, 'Partition columns changed. Need to adapt Context code.'
-        df['year'] = df.index.year
-        df['quarter'] = df.index.quarter
-        pu.write(data=df, uri=self.context.plant.calc_data_path, partition_cols=PARTITION_COLS, overwrite_period=True)
+class Context:
+    class Datasources(str, enum.Enum):
+        pq = "pq"
+        parquet = "parquet"  # stored as 'pq'
+        df = "df"
+        dataframe = "dataframe"  # stored as 'df'
 
+    VALID_DATASOURCES = [el.value for el in Datasources]
 
-class Context:
     @dataclass
     class EvalInterval:
-        start: dt.date | None
-        end: dt.date | None
-
-        DEFAULT_START = pd.to_datetime('1970-01-01 00:00', utc=True)
-        DEFAULT_END = pd.to_datetime('2200-01-01 00:00', utc=True)
+        start: datetime.date
+        end: datetime.date
 
         def __post_init__(self):
-            self.start = self.start or self.DEFAULT_START
-            self.end = self.end or self.DEFAULT_END
-            if isinstance(self.start, pd.Timestamp):
-                self.start = self.start.to_pydatetime()
-            if isinstance(self.end, pd.Timestamp):
-                self.end = self.end.to_pydatetime()
-
+            if self.start is None:
+                self.start = pd.to_datetime('1970-01-01 00:00', utc=True)
+            if self.end is None:
+                self.end = pd.to_datetime('2200-01-01 00:00', utc=True)
+            # validate start, end
             for x in (self.start, self.end):
-                if not isinstance(x, dt.date):
+                if not isinstance(x, datetime.date):
                     raise TypeError('Context limits expected to be of type datetime.')
                 if x.tzinfo is None:
-                    raise err.TimeZoneError(
+                    raise TimeZoneError(
                         "Both elements of a Context eval_interval tuple must be timezone-aware datetime objects.")
             if self.end <= self.start:
-                raise ValueError('A Context eval_interval must be increasing: end must be greater than start.')
+                raise ValueError('A Context eval_interval must have increasing timestamps: end must be greater than '
+                                 'start.')
+
+    # Cannot add plant type hint because of circular imports.
 
     def __init__(self, plant,
-                 datasource: DataSources | str | None = None,
+                 datasource: str = None,
                  dataframe: pd.DataFrame = None,
                  df_timezone: str = None,
-                 eval_start: dt.date | None = None,
-                 eval_end: dt.date | None = None,
+                 eval_start: datetime.date = None,
+                 eval_end: datetime.date = None,
+                 raw_data_path: str = None,
+                 calc_data_path: str = None,
                  ):
         if plant is None:
-            raise err.ConfigurationError('Context parameter "plant" must not be None.')
+            raise ConfigurationError('Context parameter "plant" must not be None.')
         self.plant = plant
-        self._datasource = DataSources.none
-        self._eval_interval = self.EvalInterval(eval_start, eval_end)
-        self.cache = DataCache()
         self._df = None
+        self._sensor_cache = {}
+        self._eval_interval = None
+        self.set_eval_interval(eval_start, eval_end)
+        self.session = None
 
-        if datasource is None:
-            return
-
-        if dataframe is not None:
-            if datasource == DataSources.parquet:
-                raise ValueError('Cannot create Context with parquet datasource when given dataframe.')
-            self.datasource = DataSources.df
-            self.use_dataframe(dataframe, timezone=df_timezone, )
-            return
-
-        # Either df datasource, without dataframe, or parquet datasource
+        if datasource is None and dataframe is not None:
+            datasource = 'df'
         self.datasource = datasource
+        if self.datasource == 'df':
+            self.session = None
+            self.use_dataframe(dataframe, timezone=df_timezone)
+
+        elif self.datasource == 'pq':
+            if dataframe is not None:
+                raise ConfigurationError('You are trying to set a context datasource to "pq" but also provided a '
+                                         'DataFrame. This is ambiguous.')
+
+    @classmethod
+    def validate_datasource(cls, val):
+        """Validates datasource string and returns either 'df' or 'db'.
+
+        Raises
+        ------
+        ValueError
+        """
+        if (val is not None) and val.lower() not in [s.lower() for s in cls.VALID_DATASOURCES]:
+            raise ValueError(f'Context.datasource must be one of {cls.VALID_DATASOURCES}.')
+        val = 'df' if (val == 'dataframe') else val
+        val = 'pq' if (val == 'parquet') else val
+        return val
 
     @property
     def df(self):
         return self._df
 
     @df.setter
     def df(self, val):
-        raise NotImplementedError('You cannot directly set a Context DataFrame. Use context.use_dataframe() instead.')
-
-    @property
-    def datasource(self):
-        return self._datasource
-
-    @datasource.setter
-    def datasource(self, val: DataSources | str):
-        self._datasource = DataSources(val)
-        if self._datasource == DataSources.parquet:
-            self._df = None
-        self.cache.reset()
+        raise NotImplementedError(
+            'You cannot directly set the DataFrame df of a Context. Use context.use_dataframe() instead.')
 
     @property
     def eval_start(self):
         return self._eval_interval.start
 
     @property
     def eval_end(self):
         return self._eval_interval.end
 
     @property
-    def time_index(self) -> pd.DatetimeIndex | None:
-        if self.datasource is None:
-            return None
-        idx = self.cache.get(DATETIME_COL_NAME, none_if_not_found=True)
-        if idx is None:
-            idx = DataOperations(self).get_time_index()
-            self.cache.add(DATETIME_COL_NAME, idx)
-
-        if idx is None:
-            return None
-
-        index = idx.tz_convert(self.plant.tz_data)
-        return index
-
-    def get_data_start_end(self) -> Tuple[dt.datetime, dt.datetime] | None:
-        """Get timestamps when data associated with the plant start and end.
-        """
-        if self.datasource == DataSources.none:
-            return None
+    def time_index(self) -> pd.DatetimeIndex:
+        self._assert_datasource_df_pq()
 
-        self.cache.reset()
-        self.set_eval_interval()
-        idx = self.time_index
-        if idx is None:
-            return None
-        return idx[0], idx[-1]
+        if DATETIME_COL_NAME in self._sensor_cache:
+            return self._sensor_cache[DATETIME_COL_NAME]
 
-    def get_sensor_data(self, sensor: 'sunpeek.components.Sensor') -> pd.Series:
-        """Given a sensor and raw data Returns processed data for a given sensor. Usually called as sensor.data
+        idx = None
+        if self.datasource == 'df':
+            idx = self.df[self.eval_start:self.eval_end].index
+        elif self.datasource == 'pq':
+            idx = pu.read(self.plant.raw_data_path, columns=[DATETIME_COL_NAME],
+                          filters=((DATETIME_COL_NAME, '>=', self.eval_start), (DATETIME_COL_NAME, '<=', self.eval_end))
+                          ).index
 
-        Parameters
-        ----------
-        sensor : Sensor. Data are returned in this sensor's native units, with time zone aware DatetimeIndex.
-
-        Returns
-        -------
-        pd.Series, Processed sensor values
-
-        Notes
-        -----
-        - If sensor is found in cache, unprocessed data is returned (since the data in the cache is already processed).
-        - If sensor is not in cache, data is processed (see DataOperations class).
-        """
-        s = self.cache.get(sensor.raw_name, none_if_not_found=True)
-        if s is None:
-            s = DataOperations(self).get_sensor_data(sensor)
-            self.cache.add(sensor.raw_name, s)
+        index = idx.tz_convert(self.plant.tz_data)
+        self._sensor_cache[DATETIME_COL_NAME] = index
 
-        # Convert to plant data time zone
-        s = s.tz_convert(self.plant.tz_data)
-        s = s[~s.index.duplicated(keep='first')]
+        return index
 
-        return s
+    @property
+    def datasource(self):
+        return self._datasource
 
-    def delete_sensor_data(self, start: dt.datetime, end: dt.datetime) -> None:
-        """Delete measurement data from plant in given interval.
-        """
-        self.cache.reset()
-        DataOperations(self).delete_sensor_data(start, end)
+    @datasource.setter
+    def datasource(self, val):
+        self._datasource = self.validate_datasource(val)
+        if self._datasource == 'pq':
+            self._df = None
+        self._sensor_cache.clear()
 
-    def delete_all_data(self) -> None:
-        """Delete all data, as if it had never been uploaded.
-        In contrast to delete_sensor_data(), this also works for corrupted parquet files.
-        """
-        DataOperations(self).delete_all_data()
-
-    def set_eval_interval(self,
-                          eval_start: dt.datetime | pd.Timestamp | None = None,
-                          eval_end: dt.datetime | pd.Timestamp | None = None,
-                          check_overlap: bool = False,
-                          method_name: str = '',
-                          ) -> None:
-        """Try the best to get meaningful eval_interval from available info.
-        """
-        self.cache.reset()
-        self._eval_interval = self.EvalInterval(start=eval_start, end=eval_end)
-
-        if check_overlap:
-            overlap = self._check_overlap(eval_start, eval_end, method_name)
-            self.set_eval_interval(overlap.left, overlap.right)
-
-    def _check_overlap(self,
-                       start: dt.datetime | pd.Timestamp | None,
-                       end: dt.datetime | pd.Timestamp | None,
-                       method_name: str = '') -> pd.Interval:
-        """Return state of uploaded data with respect to given interval (start, end). Return state and overlap interval.
-        Raises
-        ------
-        :class:`err.NoDataError`
+    def set_eval_interval(self, eval_start=None, eval_end=None):
+        eval_interval = self.EvalInterval(start=eval_start, end=eval_end)
+        self._eval_interval = eval_interval
+        self.reset_cache()
+
+    def reset_cache(self):
+        """To be called whenever something happens (outside Context) that invalidates the sensor_cache, e.g.
+        editing a sensor's min or max, or adding an OperationEvent to a plant.
         """
-        raise_msg = '' if not method_name else f'Cannot run {method_name}. '
-
-        # No datasource set?
-        if self.datasource == DataSources.none:
-            msg = 'Context has a None datasource. Check if data have been added to the plant.'
-            raise err.NoDataError(raise_msg + msg)
-
-        # No data uploaded?
-        data_start_end = self.get_data_start_end()
-        if not data_start_end:
-            msg = 'No data have been uploaded to the plant.'
-            raise err.NoDataError(raise_msg + msg)
-        uploaded_rng_msg = f'Uploaded measurement data range: {data_start_end[0]} to {data_start_end[-1]}.'
-
-        # No data / not enough data available?
-        if len(self.time_index) < 2:
-            msg = f'No measurements available for the plant in the selected range {start} to {end}. ' + uploaded_rng_msg
-            raise err.NoDataError(raise_msg + msg)
-
-        # No overlap?
-        i_data = pd.Interval(pd.to_datetime(data_start_end[0]), pd.to_datetime(data_start_end[-1]), closed='both')
-        start = start or i_data.left
-        end = end or i_data.right
-        i_context = pd.Interval(pd.to_datetime(start), pd.to_datetime(end), closed='both')
-        try:
-            overlap = pd.Interval(max(i_data.left, i_context.left).tz_convert(self.plant.tz_data),
-                                  min(i_data.right, i_context.right).tz_convert(self.plant.tz_data), closed='both')
-        except ValueError:
-            msg = f'No measurements available in the range {start} to {end}. ' + uploaded_rng_msg
-            raise err.NoDataError(raise_msg + msg)
-
-        # Overlap, but no timestamps in overlap
-        overlap_idx = ((self.time_index >= overlap.left) & (self.time_index <= overlap.right))
-        if overlap_idx.sum() < 2:
-            msg = f'No measurements available for the plant in the selected range {start} to {end}. ' + uploaded_rng_msg
-            raise err.NoDataError(raise_msg + msg)
-
-        return overlap
+        self._sensor_cache.clear()
 
     def use_dataframe(self,
                       df: pd.DataFrame,
                       calculate_virtuals: bool = False,
                       timezone: Union[str, pytz.timezone] = None,
                       drop_unneeded_columns: bool = False,
-                      missing_columns: str = 'ignore',
-                      eval_start: dt.date | None = None,
-                      eval_end: dt.date | None = None,
-                      ) -> None:
+                      missing_columns: str = 'ignore'):
         """Configures Context to use the supplied dataframe as the datasource, instead of accessing the database.
 
         Parameters
         ----------
         df : pd.DataFrame. Must have a DateTimeIndex index.
         calculate_virtuals : bool. Whether virtual sensor calculation should be triggered (might be slow).
         timezone : timezone string or pytz timezone, example 'Europe/Berlin' or 'UTC' or pytz.FixedOffset(60).
         missing_columns : str, one of ['ignore', 'raise', 'nan']. Treatment of real sensor names expected but not found
         in the df columns.
         drop_unneeded_columns : bool. If True, columns not needed according to plant.get_raw_names(True) are dropped.
-        eval_start, eval_end : dt.datetime. Limit the data to part of the provided DataFrame.
 
         Notes
         -----
         - Only numeric information in df is used. pint dtypes are ignored. No automatic unit conversion implemented.
         - Treatment of missing columns in df compared to expected sensor raw_names: missing_columns kwarg
         """
         if df is None:
-            df_none_warning = 'Cannot set DataFrame in Context: DataFrame is None.'
+            df_none_warning = 'Cannot set plant DataFrame in context module: DataFrame is None.'
             sp_logger.warning(df_none_warning)
             warnings.warn(df_none_warning)
-            self._df = None
-            self.cache.reset()
-            self.set_eval_interval(None, None)
             return None
 
         df.index = validate_timezone(df.index, timezone=timezone, plant=self.plant)
-        df, n_duplicates = sanitize_index(df)
+        df, n_duplicates = sanitize_dataframe(df)
 
         # Store only numeric data.
         for (col, dtype) in zip(df.columns, df.dtypes):
             if not pd.api.types.is_numeric_dtype(dtype):
                 raise ValueError(
-                    "To use a DataFrame as data source for a plant / Context, "
+                    "To use a DataFarme as data source for a plant / Context, "
                     "the DataFrame must only contain numeric columns. "
                     f"Column {col} has dtype {dtype}.")
         # pint unit dtype is added at sensor.data, or plant.context.get_sensor_data()
         df = df.astype('float64', errors='raise')
 
         assert missing_columns in ['ignore', 'raise', 'nan'], f'Invalid "missing_columns": {missing_columns}'
         if missing_columns != 'ignore':
@@ -736,26 +255,80 @@
             if cols_missing == 'nan':
                 df[cols_missing] = np.nan
 
         # Drop unneeded columns (not used by any sensor in the self.plant)
         if drop_unneeded_columns:
             df = df.drop(df.columns.difference(self.plant.get_raw_names(include_virtuals=True)), axis=1)
 
-        self.datasource = DataSources.dataframe
+        self.reset_cache()
+        self._datasource = 'df'
         self._df = df
-        self.set_eval_interval(eval_start, eval_end)
+        # By default, use whole dataframe
+        self.set_eval_interval(eval_start=df.index[0], eval_end=df.index[-1])
 
         if calculate_virtuals:
             virtuals.calculate_virtuals(self.plant)
         else:
             virtuals.config_virtuals(self.plant)
 
         return
 
-    def store_virtual_data(self, sensor: 'sunpeek.components.Sensor', data: pd.Series) -> None:
+    def get_sensor_data(self, sensor: 'sunpeek.components.Sensor'):
+        """Given a sensor and raw data Returns processed data for a given sensor. Usually called as sensor.data
+
+        Parameters
+        ----------
+        sensor : Sensor. Data are returned in this sensor's native units, with time zone aware DatetimeIndex.
+
+        Returns
+        -------
+        pandas Series, raw sensor values to which processing steps defined in self.process_data() to raw sensor values.
+
+        Notes
+        -----
+        - If sensor.raw_name is found in context._sensor_cache, unprocessed data is returned (since the data in the cache
+        should already be processed. In all other cases, context._process_data() is called.
+        - For a parquet datasource, data is always returned, even if the raw name is not found in the sensor cache, in
+        which case an all null series will be returned.
+        """
+        self._assert_datasource_df_pq()
+
+        if sensor.raw_name in self._sensor_cache:
+            # Note: If found in _sensor_cache, data is returned as-is, therefore no call to _process_data()
+            s = self._sensor_cache[sensor.raw_name]
+
+        else:
+            # Retrieve data from datasource (df or db) and process it
+            if self.datasource == 'df':
+                if sensor.raw_name not in self.df.columns:
+                    raise KeyError(f'Data for sensor {sensor.raw_name} was not found in the cache or the dataframe.'
+                                   f' The context datasource is {self.datasource}.')
+                s = self.df.loc[self.eval_start:self.eval_end, sensor.raw_name]
+
+            elif self.datasource == 'pq':
+                uri = self.plant.calc_data_path if sensor.is_virtual else self.plant.raw_data_path
+                sensor_data_df = pu.read(uri=uri, columns=[sensor.raw_name], types_dict=self._get_types_dict(sensor),
+                                         filters=(
+                                             (DATETIME_COL_NAME, '>=', self.eval_start),
+                                             (DATETIME_COL_NAME, '<=', self.eval_end)))
+
+                s = sensor_data_df.squeeze()
+
+            # Attach unit, call data cleaning
+            s = self._process_data__native_unit(sensor, s)
+            s = s.astype(f'pint[{sensor.native_unit}]')
+            # Store processed data in cache
+            self._sensor_cache[sensor.raw_name] = s
+
+        # Convert to plant data time zone
+        s = s.tz_convert(self.plant.tz_data)
+
+        return s
+
+    def store_data(self, sensor: 'sunpeek.components.Sensor', data: pd.Series):
         """Stores virtual sensor calculation results in the cache and, if the datasource is `df`, to the dataframe. If
         the datasource is `pq`, data will _not_ be stored by this method; in this case, `flush_virtuals_to_parquet`
         method should be used when all virtual sensor updates have been completed.
 
         Parameters
         ----------
         sensor : Sensor. Data will be stored for this virtual sensor.
@@ -765,31 +338,59 @@
         Returns
         -------
         Nothing, new data is stored in the context DataFrame self.df
 
         Notes
         -----
         - Assumes dataframe backend for speed reason. Calling code needs to take care of storing things to database.
-        - Also populates cache
+        - Also populates ._sensor_cache
         """
-        data = DataOperations(self).sanitize_virtual_data(sensor, data)
+        self._assert_datasource_df_pq()
 
-        if self.datasource == DataSources.dataframe:
+        # If a vsensor calculation was not possible (for whatever reason), Context will store an all-NaN series.
+        if data is None:
+            data = pd.Series(data=np.nan, index=self.time_index, name=sensor.raw_name)
+        else:
+            if len(data) != len(self.time_index):
+                raise CalculationError(
+                    'Size of virtual sensor data is incompatible with size of Plant.time_index')
+            data.index = self.time_index
+
+            # pint unit is stored with sensor.native_unit and attached at context.get_sensor_data
+            data = data.astype(float).astype(float)
+
+            data.name = sensor.raw_name
+
+            # All subsequent algorithms shall rely that everything is either a number or NaN.
+            # Inf may arise in virtual sensors, e.g. due to CoolProp returning Inf.
+            data[~np.isfinite(data)] = np.nan
+
+        if self.datasource == 'df':
             self.df[sensor.raw_name] = data
-        # For parquet, all virtual sensors are stored in batch, see `flush_virtuals_to_parquet`
 
-        # Populate cache for faster data retrieval in subsequent accesses (mostly within virtual sensor calculations)
-        self.cache.add(sensor.raw_name, data.astype(f'pint[{sensor.native_unit}]'))
+        # Populate Context sensor cache for faster data retrieval in subsequent accesses
+        self._sensor_cache[sensor.raw_name] = data.astype(f'pint[{sensor.native_unit}]')
 
-    def flush_virtuals_to_parquet(self) -> None:
-        """Store all virtual sensor data from sensor cache to the configured parquet storage.
+    def flush_virtuals_to_parquet(self):
         """
-        DataOperations(self).flush_virtuals()
-
-    # NaN report -----------
+        Stores any virtual sensor data in the sensor cache to the configured parquet datasource. The data is removed
+        from the cache during this process.
+        """
+        self._assert_datasource_df_pq()
+        if self.datasource == 'df':
+            raise ValueError("Storing virtual sensor data to parquet is only possible when the backend is parquet")
+
+        v_sensors = []
+        for name in self.plant.get_raw_names(only_virtuals=True):
+            v_sensors.append(self._sensor_cache.pop(name))
+
+        data = pd.concat(v_sensors, axis=1)
+        data['year'] = data.index.year
+        data['quarter'] = data.index.quarter
+        pu.write(data=data, uri=self.plant.calc_data_path, partition_cols=['year', 'quarter'], overwrite_period=True)
 
     N_TOTAL_TIMESTAMPS = "n_total_timestamps"
     N_AVAILABLE_TIMESTAMPS = "n_available_timestamps"
     NAN_DENSITY_IN_AVAILABLE = "nan_density_in_available"
 
     def get_nan_report(self,
                        include_virtuals: bool = False) -> NanReportResponse:
@@ -850,12 +451,131 @@
                             n_available_timestamps.rename(self.N_AVAILABLE_TIMESTAMPS),
                             nan_density_in_available.rename(self.NAN_DENSITY_IN_AVAILABLE),
                             ], axis=1)
         df_out.index = pd.to_datetime(df_out.index)
 
         return df_out
 
-    def verify_time_index(self) -> None:
-        """Make sure context time_index is accessible.
+    def _process_data__native_unit(self, sensor: 'sunpeek.components.Sensor', s_raw):
+        """This is the main data processing method, it implements e.g. min max filtering, plant.ignored_ranges etc.
+        This method is intended for sensor data which is given as numeric pd.Series, not dtype pint, to accelerate
+        runtime.
+
+        Parameters
+        ----------
+        sensor : Sensor. Process this sensor's data.
+        s_raw : pd.Series
+            Unprocessed data for sensor, with float (or equiv.) dtype, typically obtained from self.get_sensor_data()
+
+        Returns
+        -------
+        s : pd.Series
+            Processed data for sensor, with numeric dtype.
         """
+        s = s_raw
+        # Set values in ignored ranges to NaN
+        for irng in self.plant.ignored_ranges:  # pd.Interval
+            mask = (s.index >= irng.left) & (s.index <= irng.right)
+            s[mask] = np.nan
+        # Lower and Upper replacement intervals (see HarvestIT #177)
+        s = self._replace_lower__native(s, sensor)
+        s = self._replace_upper__native(s, sensor)
+
+        return s
+
+    def _assert_datasource_df_pq(self):
+        if self.datasource is None:
+            raise ConfigurationError('Context datasource is None.')
+        if self.datasource not in ['df', 'pq']:
+            raise ConfigurationError(f'Unexpected Context datasource {self.datasource}.')
+
+    def verify_time_index(self):
+        """Make sure context has a valid time index."""
         self.time_index
         return
+
+    @staticmethod
+    def _replace_lower__native(data, sensor):
+        """Implement lower replacement interval, see #177.
+
+        Parameters
+        ----------
+        data : pd.Series with unprocessed data
+        sensor : Sensor
+
+        Returns
+        -------
+        s : pd.Series with replaced values.
+        """
+        left, right, replace = sensor.value_replacements__native['lower']
+        if (left is None) and (right is None) and (replace is None):
+            # all None: nothing to do
+            return data
+
+        if right is None and replace is None:
+            # no replacement value given, only left is not None
+            data[data < left] = np.nan
+            return data
+
+        if left is None and replace is None:
+            # no replacement value given, only right is not None
+            data[data < right] = np.nan
+            return data
+
+        if left is None:
+            data[data < right] = replace
+            return data
+
+        # all are not-NaN
+        data[data < left] = np.nan
+        data[(data >= left) & (data < right)] = replace
+        # Does not work, package incompatibility... s[(s >= left) & (s < right)] = replace
+        return data
+
+    @staticmethod
+    def _replace_upper__native(data, sensor):
+        """Implement upper replacement interval, see #177.
+
+        Parameters
+        ----------
+        data : pd.Series with unprocessed data
+        sensor : Sensor
+
+        Returns
+        -------
+        s : pd.Series with replaced values.
+        """
+        left, right, replace = sensor.value_replacements__native['upper']
+        if (left is None) and (right is None) and (replace is None):
+            # all None: nothing to do
+            return data
+
+        if left is None and replace is None:
+            # no replacement value given, only right is not None
+            data[data > right] = np.nan
+            return data
+
+        if right is None and replace is None:
+            # no replacement value given, only left is not None
+            data[data > left] = np.nan
+            return data
+
+        if right is None:
+            data[data > left] = replace
+            return data
+
+        # all are not-NaN
+        data[data > right] = np.nan
+        data[(data > left) & (data <= right)] = replace
+        return data
+
+    @staticmethod
+    def _get_types_dict(sensor):
+        types_dict = {}
+        # if getattr(sensor.sensor_type, 'name', '') == 'bool':
+        #     types_dict[sensor.raw_name] = bool
+        if getattr(sensor.sensor_type, 'compatible_unit_str', '') == 'str':
+            types_dict[sensor.raw_name] = str
+        else:
+            types_dict[sensor.raw_name] = float
+
+        return types_dict
```

## sunpeek/data_handling/data_uploader.py

```diff
@@ -34,77 +34,47 @@
 import pathlib
 import numpy as np
 from typing import List, Union
 import time
 import pandas as pd
 import pytz
 from io import BytesIO
-import datetime as dt
-from pydantic import validator
-from charset_normalizer import from_fp
+import datetime
 
 import sunpeek.common.time_zone as time_zone
-from sunpeek.common.utils import DatetimeTemplates, sp_logger
-from sunpeek.db_utils import DATETIME_COL_NAME, PARTITION_COLS
-from sunpeek.data_handling.context import Context, sanitize_index
+from sunpeek.common.utils import DatetimeTemplates
+from sunpeek.common.utils import sp_logger
+from sunpeek.data_handling.context import Context, sanitize_dataframe
 from sunpeek.base_model import BaseModel
 from sunpeek.common.errors import DataProcessingError, TimeZoneError
+from sunpeek.db_utils import DATETIME_COL_NAME
 import parquet_datastore_utils as pu
 from sunpeek.common.time_zone import process_timezone
-from sunpeek.components.helpers import UploadHistory
 
 
 class DataUploadResponseFile(BaseModel):
     name: Union[str, None]
     exists: Union[bool, None]
     size_bytes: Union[int, None]
-    missing_columns: List[str] = []
+    missing_columns: Union[List[str], None]
     error_cause: Union[str, None]
 
-    status: Union[str, None]
-    date_of_upload: Union[dt.datetime, None]
-    start: Union[dt.datetime, None]
-    end: Union[dt.datetime, None]
-    n_rows: Union[int, None]
-    id: Union[int, None]
-
 
 class DataUploadResponse(BaseModel):
     n_uploaded_data_rows: Union[int, None]
+    # n_available_data_rows: Union[int, None]
+    # nan_density: Union[float, None]
     n_duplicates_index: Union[int, None]
     response_per_file: Union[List[DataUploadResponseFile], None]
     db_response: Union[dict, None]
 
 
-class DataUploadSettings(BaseModel):
-    csv_separator: Union[str, None]
-    csv_decimal: Union[str, None]
-    csv_encoding: Union[str, None]
-    index_col: Union[int, None]
-    datetime_template: Union[str, None]
-    datetime_format: Union[str, None]
-    timezone: Union[str, None]
-
-    @validator('datetime_template', pre=True)
-    def to_string(cls, v):
-        if isinstance(v, DatetimeTemplates):
-            return str(v.value)
-        return str(v)
-
-    @validator('timezone', pre=True)
-    def to_string_pytz(cls, v):
-        return str(v)
-
-
-class DataInspectionResponse(BaseModel):
+class DataColumnsResponse(BaseModel):
     sensors: Union[List[str], None]
-    dtypes: Union[List[str], None]
     index: Union[str, None]
-    settings: Union[DataUploadSettings, None]
-    data: Union[dict, None]
 
 
 class DataUploader_df:
     """
     Data uploads of csv files to a plant using Context backend with datasource 'dataframe'.
 
     Notes
@@ -120,16 +90,16 @@
                  datetime_template: DatetimeTemplates = None,
                  datetime_format: str = None,
                  timezone: Union[str, pytz.timezone] = None,
                  csv_separator: str = ';',
                  csv_decimal: str = '.',
                  csv_encoding: str = 'utf-8',
                  index_col: int = 0,
-                 eval_start: dt.date = None,
-                 eval_end: dt.date = None,
+                 eval_start: datetime.date = None,
+                 eval_end: datetime.date = None,
                  on_file_error: str = 'report',
                  ):
         """
         Parameters
         ----------
         plant : Plant
         timezone : str or pytz.timezone.
@@ -140,68 +110,53 @@
             Used in pd.read_csv as 'decimal' kwarg
         csv_encoding : str
             Used in pd.read_csv as 'encoding' kwarg
         datetime_format : str
             Used to parse datetimes from csv file. Leave to None infers the format.
         index_col : int
             DataUploader will try to parse timestamps from this column.
-        eval_start : dt.datetime
+        eval_start : datetime
             Limit the data that is read and imported
-        eval_end : dt.datetime
+        eval_end : datetime
             Limit the data that is read and imported
         on_file_error : str
             Behaviour if an error is encountered reading a file, either `report` to store details in the file response
             and continue, or `raise`, to raise the error and stop.
         """
-        if (datetime_template is None) and (datetime_format is None):
-            raise DataProcessingError('Either "datetime_template" or "datetime_format" needs to be specified.')
-
         self.plant = plant
         self.eval_start = eval_start
         self.eval_end = eval_end
+        self.output = DataUploadResponse()
+
+        if (datetime_template is None) and (datetime_format is None):
+            raise DataProcessingError('Either "datetime_template" or "datetime_format" needs to be specified.')
+
+        if isinstance(datetime_template, str):
+            self.datetime_template = DatetimeTemplates[datetime_template]
+        else:
+            self.datetime_template = datetime_template
+
         self.datetime_format = datetime_format
-        self.datetime_template = DatetimeTemplates[datetime_template] if isinstance(datetime_template,
-                                                                                    str) else datetime_template
-        self._original_timezone = timezone
         self.timezone = process_timezone(timezone, plant=self.plant)
         self.csv_decimal = csv_decimal
         self.index_col = index_col
         self.on_file_error = on_file_error
-        self.csv_separator = csv_separator
-        self.csv_encoding = csv_encoding
-        self.output = DataUploadResponse()
 
-    def read_csv(self, csv, **kwargs):
-        return pd.read_csv(csv, on_bad_lines='skip', parse_dates=False, dtype='str', **kwargs)
-
-    def get_settings(self):
-        return self.__dict__
-
-    @staticmethod
-    def __validate_files(files):
-        if files is None:
-            raise DataProcessingError('No files to upload supplied.')
-        if not isinstance(files, list):
-            files = [files]
-        if not (len(files) > 0):
-            raise DataProcessingError('No files to upload supplied.')
-        return files
+        def read_csv(csv, **kwargs):
+            try:
+                return pd.read_csv(csv,
+                                   encoding=csv_encoding, sep=csv_separator,
+                                   on_bad_lines='skip',
+                                   parse_dates=False,
+                                   dtype='str',
+                                   **kwargs)
+            except LookupError as e:
+                raise DataProcessingError(str(e))
 
-    @staticmethod
-    def _to_BytesIO(bio_or_file):  # noqa
-        if hasattr(bio_or_file, 'filename'):
-            bio = bio_or_file.file
-        elif isinstance(bio_or_file, str) or isinstance(bio_or_file, os.PathLike):
-            with open(bio_or_file, 'rb') as f:
-                bio = BytesIO(f.read())
-        else:
-            # could be many types: BytesIO,  io.BufferedReader, tempfile.SpooledTemporaryFile, ...
-            bio = bio_or_file
-        bio.seek(0)
-        return bio
+        self.read_csv = read_csv
 
     def do_upload(self, files: Union[str, os.PathLike, List[Union[str, os.PathLike]]],
                   calculate_virtuals: bool = True) -> DataUploadResponse:
         """Full measurement data ingestion process, also triggers virtual sensor calculation and sensor validation.
 
         Parameters
         ----------
@@ -215,293 +170,405 @@
         FileNotFoundError
         ConnectionError
 
         Returns
         -------
         DataUploadResponse : Response from the data upload, various info fields.
         """
+        files = self.__validate_files(files)
+
         start_time = time.time()
+        self._pre_upload()
 
-        files = self.__validate_files(files)
-        df = self._parse_files(files)
-        self.plant.context = Context(plant=self.plant, datasource='df')
-        self.plant.context.use_dataframe(df, calculate_virtuals=calculate_virtuals)
-        self._post_upload()
+        # Full data ingestion process, common for all context datasources (dataframe, parquet).
+        self._csv_to_plant(files, calculate_virtuals)
 
+        self._post_upload()
         sp_logger.debug(f"[data_uploader] --- Finished after {(time.time() - start_time):.1f} seconds ---")
+
         return self.output
 
-    def _parse_files(self, files):
+    def get_sensor_names(self, files):
+        """Returns names of the sensors based on an example file (or BytesIO)
+
+        Parameters
+        ----------
+        files : UploadFile, str, os.PathLike
+            Files to upload.
+        """
+        files = self.__validate_files(files)
+        bio = self._to_BytesIO(files[0])
+        bio.seek(0)
+        df = self.read_csv(bio, nrows=1, index_col=self.index_col)
+
+        return df.columns
+
+    def get_index_name(self, files):
+        """Returns name of the index column based on an example file (or BytesIO)
+
+        Parameters
+        ----------
+        files : UploadFile, str, os.PathLike
+            Files to upload.
+        """
+        files = self.__validate_files(files)
+        bio = self._to_BytesIO(files[0])
+        bio.seek(0)
+        df = self.read_csv(bio, nrows=1, index_col=self.index_col, usecols=[])
+
+        return df.index.name
+
+    @staticmethod
+    def __validate_files(files):
+        if files is None:
+            raise DataProcessingError('No files to upload supplied.')
+        if not isinstance(files, list):
+            files = [files]
+        if not (len(files) > 0):
+            raise DataProcessingError('No files to upload supplied.')
+        return files
+
+    @staticmethod
+    def _to_BytesIO(bio_or_file):  # noqa
+        if hasattr(bio_or_file, 'filename'):
+            bio = bio_or_file.file
+        elif isinstance(bio_or_file, BytesIO):
+            bio = bio_or_file
+        else:
+            with open(bio_or_file, 'rb') as f:
+                bio = BytesIO(f.read())
+        return bio
+
+    def _csv_to_plant(self, files, calculate_virtuals: bool):
+        """Full data ingestion process, from csv to plant with dataframe context.
+        Reads files to DataFrame, sets plant context datasource, does sensor validation.
+        """
+        df = self._all_csv_to_single_df(files)
+        self.plant.context.use_dataframe(df, calculate_virtuals=calculate_virtuals)
+        return
+
+    def _all_csv_to_single_df(self, files):
         """Concatenates the uploaded files into a single df.
 
+        Returns
+        -------
+        df_all_files : pd.DataFrame
+
+        Raises
+        ------
+        AssertionError
+
         Notes
         -----
         - Columns which do not match with any of the plant's sensor raw_names are dropped.
         - Works for fastAPI's UploadFile as well as for normal csv files.
         """
         sp_logger.debug(f"[data_uploader] Reading csv files to DataFrame.")
         sp_logger.debug(f"[data_uploader] Concatenating {len(files)} files.")
         start_time = time.time()
 
         # Iterate trough files and gather DataFrames
         df_all_files = None
         self.output.response_per_file = []
         for file in files:
-            file_response = UploadHistory(plant=self.plant, date_of_upload=dt.datetime.now())
+            file_response = DataUploadResponseFile()
             try:
                 # is either a FlaskApi File or file-path, or a BytesIO object
                 if hasattr(file, 'filename'):
                     file_response.name = file.filename
                     file_response.exists = True
                 elif isinstance(file, str) or isinstance(file, pathlib.Path):
                     file_response.name = os.path.basename(file)
                     file_response.exists = os.path.exists(file)
-                    if not file_response.exists:
-                        raise FileNotFoundError(f'Cannot find file: "{file_response.name}".')
                 elif isinstance(file, BytesIO):
                     file_response.name = None
                     file_response.exists = True
                 else:
                     raise FileNotFoundError(f'Cannot interpret input for file: "{file}".')
 
-                # get size
+                if not file_response.exists:
+                    raise FileNotFoundError(f'Cannot find file: "{file_response.name}".')
+
+                # Create BytesIO object
                 bio = self._to_BytesIO(file)
+
+                # get size
                 bio.seek(0, os.SEEK_END)
                 file_response.size_bytes = bio.tell()
                 bio.seek(0)
 
                 try:
-                    # parsing file
-                    expected_sensors = self.plant.get_raw_names(include_virtuals=False)
-                    df_file = self._parse_single_file(bio, usecols=expected_sensors)
-
-                    # after-processing
-                    df_file = df_file.rename_axis(DATETIME_COL_NAME)
-                    missing_columns = set(expected_sensors) - set(df_file.columns)
-                    df_file[list(missing_columns)] = np.nan
-
-                    # get statistics
-                    file_response.start = df_file.index.min()
-                    file_response.end = df_file.index.max()
-                    file_response.missing_columns = missing_columns
-                    file_response.n_rows = len(df_file)
-
-                    # Check if df has at least one column, except index col and valid timestamps
-                    if len(missing_columns) == len(expected_sensors):
-                        raise ValueError("Uploaded file contains no data columns that match with sensor names.")
-                    elif len(df_file.index) == 0:
-                        raise ValueError("Uploaded file contains no valid timestamps. "
-                                         "Is it possible that the uploaded file contains no measurement data?")
-
-                except Exception as ex:
-                    sp_logger.warning(ex)
-                    file_response.error_cause = f'Error: {ex}'
+                    df_file, missing_columns = self._one_csv_to_df(bio)
+                except (ValueError, TimeZoneError) as ex:
+                    sp_logger.exception(ex)
                     if self.on_file_error == 'raise':
                         raise
-                    warnings.warn(f'Failed to read csv file using pandas read_csv. {ex}')
+                    file_response.error_cause = f'Error: {ex}'
                     continue
+                file_response.missing_columns = missing_columns
 
                 # Concatenate the dataframes
                 if len(df_file) > 0:
                     df_all_files = pd.concat([df_all_files, df_file], ignore_index=False)
                     if not isinstance(df_all_files.index, pd.DatetimeIndex):
                         raise DataProcessingError('Cannot concatenate DataFrames with mixed timezones since this '
                                                   'results in the DataFrame index not being a DatetimeIndex anymore.')
 
             finally:
                 self.output.response_per_file.append(file_response)
 
         # Check for duplicates etc.
-        df_all_files, n_duplicates_index = sanitize_index(df_all_files)
+        df_all_files, n_duplicates_index = sanitize_dataframe(df_all_files)
         self.output.n_duplicates_index = n_duplicates_index
         if self.output.n_duplicates_index:
             duplicate_warning = f"Found {self.output.n_duplicates_index} duplicate index entries in data. " \
                                 f"All rows with duplicate index will be removed."
             sp_logger.warning(duplicate_warning)
             warnings.warn(duplicate_warning)
 
         if (df_all_files is None) or len(df_all_files) < 2:
             df_all_files = None
             self.output.n_uploaded_data_rows = 0
+            # self.output.n_available_data_rows = 0
+            # self.output.nan_density = 1
+
             df_none_warning = 'Reading csv files resulted in a DataFrame with less than 2 rows.'
             sp_logger.warning(df_none_warning)
             warnings.warn(df_none_warning)
+            # raise DataProcessingError('Reading csv files resulted in a DataFrame with less than 2 rows.')
         else:
             self.output.n_uploaded_data_rows = len(df_all_files)
+            # # not_ignored: marks the relevant pieces of data. True if timestamp is not within ignored range
+            # not_ignored = pd.Series(True, index=df_all_files.index)
+            # for r in self.plant.ignored_ranges:
+            #     mask = (df_all_files.index >= r.left) & (df_all_files.index <= r.right)
+            #     not_ignored.loc[mask] = False
+            # self.output.n_available_data_rows = not_ignored.to_numpy().sum()
+            # self.output.nan_density =
 
         sp_logger.debug(
             f"[data_uploader] --- Done parsing {len(files)} files in {(time.time() - start_time):.1f} seconds.")
+
         return df_all_files
 
-    def _parse_single_file(self, bio, usecols=None, nrows=None) -> pd.DataFrame:
+    def _one_csv_to_df(self, bio):
         """Read a BytesIO object to DataFrame.
 
         Parameters
         ----------
-        bio : BytesIO object or File
+        bio : BytesIO object
             From an UploadFile or from a normal csv file.
 
         Returns
         -------
         df : pandas.DataFrame
             DataFrame with tz-aware DatetimeIndex
+        missing_columns : List[str]
+            Columns not found in the file
 
         Raises
         ------
         AssertionError
 
         Notes
         -----
         - Returns a DataFrame with DatetimeIndex taken from the first column, index is named according to
          sunpeek.db_utils.DATETIME_COL_NAME.
         - Missing columns are added as all-NaN columns.
         """
-        bio = self._to_BytesIO(bio)
-
-        # If bounds (start|end) are provided, the index column is parsed alone, to determine rows to skip.
-        # this is a slight overhead as the file is read twice. However, it can speed up the data import when a lot of
-        # rows are skipped. In addition, this allows to skip line which would lead to errors otherwise.
-        skiprows = None
-        bounds_provided = (self.eval_start is not None) or (self.eval_end is not None)
-        if bounds_provided:
-            index = self.read_csv(bio, usecols=[self.index_col], encoding=self.csv_encoding, sep=self.csv_separator,
-                                  nrows=nrows).iloc[:, 0]
-            index = self.__parse_datetime_index(index)
-
-            skiprows = index.isna()
+        # Parse timestamps from first column
+        ds_cache = self._parse_timestamps(bio)
+        try:
+            skiprows = ds_cache.isna()
+            # Limit the rows to read in the file, if bounds were provided
             if self.eval_start is not None:
-                skiprows = skiprows | (index < self.eval_start)
+                skiprows = skiprows | (ds_cache < self.eval_start)
             if self.eval_end is not None:
-                skiprows = skiprows | (index > self.eval_end)
+                skiprows = skiprows | (ds_cache > self.eval_end)
+            ds_cache = ds_cache[~skiprows]
+            # convert to numbers, for pd.read_csv()
             skiprows = [i for i, x in enumerate(np.insert(skiprows, 0, False)) if x]
 
-        # Pandas requires that index_column name is inside usecols
-        index_col = self.index_col
-        if usecols is not None:
-            index_name = self.get_index_name(bio)
-            all_cols = [index_name] + usecols
-            usecols = lambda x: x in all_cols
-            index_col = index_name
-
-        # load data
-        bio.seek(0)
-        try:
-            df = self.read_csv(bio,
-                               usecols=usecols,
-                               skiprows=skiprows,
-                               nrows=nrows,
-                               index_col=[index_col],
-                               encoding=self.csv_encoding,
-                               sep=self.csv_separator
-                               )
-        except UnicodeDecodeError as e:
+            # Main read_csv call
             bio.seek(0)
-            suggested_encoding = from_fp(bio).best().encoding  # The most probable encoding string
-            raise DataProcessingError(f'Cannot parse file due to an encoding problem. '
-                                      f'Probably, "{self.csv_encoding}" is not the right encoding of this file. '
-                                      f'We suggest to try the "{suggested_encoding}" encoding. '
-                                      f'Original error message: {str(e)}')
-
-        # conversion to valid date
-        df.index = self.__parse_datetime_index(df.index.to_series())
-
-        # read_csv with decimal kwarg fails when reading string, hence the two calls to apply()
-        if self.csv_decimal is not None:
-            df = df.apply(lambda x: x.str.replace(self.csv_decimal, '.'))
-        df = df.apply(pd.to_numeric, errors='coerce')
+            df = self.read_csv(bio,
+                               usecols=lambda x: x in self.plant.get_raw_names(include_virtuals=False),
+                               skiprows=skiprows)
+            # read_csv with decimal kwarg fails when reading string, hence the two calls to apply()
+            if self.csv_decimal is not None:
+                df = df.apply(lambda x: x.str.replace(self.csv_decimal, '.'))
+            df = df.apply(pd.to_numeric, errors='coerce')
+            df = pd.DataFrame(index=ds_cache) if df.empty else df.set_index(ds_cache)
+            df = df.rename_axis(DATETIME_COL_NAME)
+
+            # Add missing real sensor column names as NaN columns
+            missing_columns = set(self.plant.get_raw_names(include_virtuals=False)) - set(df.columns)
+            df[list(missing_columns)] = np.nan
+
+            return df, list(missing_columns)
+
+        except Exception as ex:
+            sp_logger.exception(ex)
+            warnings.warn(f'Failed to read csv file using pandas read_csv. {ex}')
+            raise ValueError(f'Failed to read csv file using pandas read_csv. {ex}') from ex
 
-        return df
+    def _parse_timestamps(self, bio):
+        """Parse timestamps from first column in bio, validate timezone, return tz-aware DatetimeIndex
+        """
+        bio.seek(0)
+        ds = self.read_csv(bio, usecols=[self.index_col]).iloc[:, 0]
+        # In parsing timestamps, priority is given to the more explicit self.datetime_format:
+        if self.datetime_format is not None:
+            ds = pd.to_datetime(ds, errors='coerce', format=self.datetime_format)
+        else:
+            dayfirst = True if (self.datetime_template == DatetimeTemplates.day_month_year) else False
+            yearfirst = True if (self.datetime_template == DatetimeTemplates.year_month_day) else False
+            try:
+                ds = pd.to_datetime(ds, errors='coerce', dayfirst=dayfirst, yearfirst=yearfirst)
+            except (pd.errors.ParserError, ValueError) as e:
+                raise DataProcessingError(
+                    f"Pandas to_datetime was unable to parse timestamps from the file, given datetime_template="
+                    f"{self.datetime_template}. Try to set an explicit 'datetime_format' instead.")
+        # Does not parse timezone-aware datetimes correctly, e.g. '2017-04-30 00:00:00+00:00' is parsed as NaT:
+        # ds = pd.to_datetime(ds, errors='coerce', infer_datetime_format=True)
 
-    def __parse_datetime_index(self, ds):
         try:
-            if self.datetime_format is not None:
-                fmt = self.datetime_format
-                day_first = None
-                year_first = None
-            else:
-                fmt = None
-                day_first = True if (self.datetime_template == DatetimeTemplates.day_month_year) else False
-                year_first = True if (self.datetime_template == DatetimeTemplates.year_month_day) else False
-
-            ds = pd.to_datetime(ds, errors='coerce', format=fmt, dayfirst=day_first, yearfirst=year_first)
             ds = pd.DatetimeIndex(ds)
-
-            if ds.isna().all():
-                raise DataProcessingError(
-                    f"Pandas to_datetime was unable to parse timestamps from the file, given datetime_format="
-                    f"{self.datetime_format} and datetime_template={self.datetime_template}."
-                    f"Please check your input for 'datetime_format'.")
-
-            ds = time_zone.validate_timezone(ds, timezone=self._original_timezone, plant=self.plant)
-            return ds
-
-        except (DataProcessingError, TimeZoneError):
-            raise
-        except (pd.errors.ParserError, ValueError) as e:
-            raise DataProcessingError(
-                f"Pandas to_datetime was unable to parse timestamps from the file, given datetime_template="
-                f"{self.datetime_template}. Try to set an explicit 'datetime_format' instead.")
-        except Exception as e:
+        except:
             # Mixed timezone timestamp columns lead to Index class df.index with dtype 'object'
             # see https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html
             raise TimeZoneError(
                 '[data_uploader] Could not convert timestamps of the csv file to a DatetimeIndex. '
-                'One cause why this happens are mixed-timezone timestamps or only some rows having timezones.', e)
+                'One cause why this happens are mixed-timezone timestamps or only some rows having timezones.')
 
-    def do_inspection(self, files, nrows=500) -> pd.DataFrame:
-        """Returns the data of an example measurement file (or BytesIO) for inspection purposes.
-        The same method as do_upload is called, but without storing the data or restricting expected columns
-        """
-        files = self.__validate_files(files)
-        bio = self._to_BytesIO(files[0])
-        df = self._parse_single_file(bio, nrows=nrows)
+        ds = time_zone.validate_timezone(ds, timezone=self.timezone, plant=self.plant)
 
-        return df
+        return ds
 
-    def get_index_name(self, files):
-        """Returns name of the index column based on an example file (or BytesIO)
-        Parameters
-        ----------
-        files : UploadFile, str, os.PathLike
-            Files to upload.
-        """
-        files = self.__validate_files(files)
-        bio = self._to_BytesIO(files[0])
-        df = self.read_csv(bio, nrows=0, encoding=self.csv_encoding, sep=self.csv_separator)
-
-        return list(df.columns.values)[self.index_col]
+    def _pre_upload(self):
+        pass
 
     def _post_upload(self):
         pass
 
 
+# class DataUploader_db(DataUploader_df):
+#     """Data upload from csv files to database.
+#     """
+#
+#     def __init__(self, session, **kwargs):
+#         super().__init__(**kwargs)
+#         self._sensor_raw_names = None
+#         self.output.db_response = {}
+#         # Try to establish database connection, raises ConnectionError
+#         self.db_connection = import_db_ops().get_db_connection()
+#         self.session = session
+#
+#     @property
+#     def table_name(self):
+#         return self.plant.raw_table_name
+#
+#     def _get_types_dict(self):
+#         types_dict = {DATETIME_COL_NAME: datetime.datetime}
+#         for sensor in self.plant.raw_sensors:
+#             if getattr(sensor.sensor_type, 'name', '') == 'bool':
+#                 types_dict[sensor.raw_name] = bool
+#             elif getattr(sensor.sensor_type, 'compatible_unit_str', '') == 'str':
+#                 types_dict[sensor.raw_name] = str
+#             else:
+#                 types_dict[sensor.raw_name] = float
+#
+#         return types_dict
+#
+#     def _create_raw_data_table(self):
+#         """Creates raw data table if it does not exists in the database.
+#         """
+#         # Create database inferring runtime types
+#         try:
+#             types_dict = self._get_types_dict()
+#             import_db_ops().create_table_dynamic(self.session.get_bind(), self.table_name, types_dict)
+#             self.output.db_response['new_table_created'] = True
+#             self.output.db_response['new_table_name'] = self.table_name
+#
+#         except Exception as ex:
+#             sp_logger.exception(ex)
+#             raise
+#
+#     def _update_table(self):
+#         types_dict = self._get_types_dict()
+#         import_db_ops().create_new_data_cols(self.session.get_bind(), self.table_name, types_dict)
+#
+#     def _pre_upload(self):
+#         table_exists = import_db_ops().db_table_exists(self.session.get_bind(), self.table_name)
+#         if table_exists:
+#             sp_logger.debug(f"[data_uploader] Table {self.table_name} exists in database. Adding columns for any "
+#                             f"new sensors for plant {self.plant.name}.")
+#             self._update_table()
+#         else:
+#             sp_logger.debug(f"[data_uploader] Creating table {self.table_name} in database.")
+#             self._create_raw_data_table()
+#
+#     def _post_upload(self):
+#         """Save dataframe to database
+#         """
+#         df = self.plant.context.df
+#         #if df is not None:
+#         #    # Before writing to database, any overlapping (not only duplicate) data in db is deleted.
+#         #    sp_logger.debug(f"[data_uploader] Deleting overlapping data from table {self.table_name}.")
+#         #    self.output.db_response['overlap_response'] = \
+#         #        import_db_ops().delete_overlapping_data(self.db_connection, self.table_name,
+#         #                                                overlapping_boundaries=(df.index[0], df.index[-1]))
+#
+#         # Before writing to database, any overlapping (not only duplicate) data in db is deleted.
+#         sp_logger.debug(f"[data_uploader] Deleting overlapping data from table {self.table_name}.")
+#         self.output.db_response['overlap_response'] = \
+#             import_db_ops().delete_overlapping_data(self.db_connection, self.table_name,
+#                                                     overlapping_boundaries=(df.index[0], df.index[-1]))
+#
+#         #    # Write new data (including virtual sensors) to db.
+#         #    sp_logger.debug(f"[data_uploader] Writing dataframe to table {self.table_name}...")
+#         #    self.output.db_response['measure_data_saved_db_ok'] = import_db_ops().df_to_db(self.db_connection, df,
+#         #                                                                                   self.table_name)
+#         #    if self.output.db_response['measure_data_saved_db_ok']:
+#         #        self.db_connection.commit()
+#         #        sp_logger.debug(f"[data_uploader] Data succesfully saved in db table {self.table_name}.")
+#         #        import_db_ops().disconnect_db(self.db_connection)
+#         #    else:
+#         #        sp_logger.debug(f"[data_uploader] Error writing data to db table {self.table_name}.")
+#         #        self.db_connection.rollback()
+#         #        import_db_ops().disconnect_db(self.db_connection)
+#         #        raise ConnectionError('Failed to store data in database table.')
+#
+#         # From now on, data is accessed by 'db' and uses the full datetime range available in the db
+#         self.plant.context = Context(plant=self.plant, datasource='db',
+#                                      eval_start=self.eval_start, eval_end=self.eval_end)
+
+
 class DataUploader_pq(DataUploader_df):
     """
     Data upload from csv files to parquet datastore.
     """
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self._sensor_raw_names = None
         self.raw_data_path = self.plant.raw_data_path
         self.calc_data_path = self.plant.calc_data_path
 
-    def _post_upload(self) -> None:
-        """This comes after self.do_upload(), so we have a dataframe context and all data in the context dataframe.
-        """
-        # Behavior of uploader is to start with a dataframe context.
+    def _post_upload(self):
         df = self.plant.context.df
         if df is None:
             # Do nothing, this is already accounted for by Context. 
             return
 
         df['year'] = df.index.year
         df['quarter'] = df.index.quarter
 
-        raw_df = df[self.plant.get_raw_names(include_virtuals=False) + PARTITION_COLS]
-        calc_df = df[self.plant.get_raw_names(only_virtuals=True) + PARTITION_COLS]
+        raw_df = df[self.plant.get_raw_names(include_virtuals=False) + ['year', 'quarter']]
+        calc_df = df[self.plant.get_raw_names(only_virtuals=True) + ['year', 'quarter']]
 
-        pu.write(data=raw_df, uri=self.raw_data_path, partition_cols=PARTITION_COLS, overwrite_period=True)
-        pu.write(data=calc_df, uri=self.calc_data_path, partition_cols=PARTITION_COLS, overwrite_period=True)
+        pu.write(data=raw_df, uri=self.raw_data_path, partition_cols=['year', 'quarter'], overwrite_period=True)
+        pu.write(data=calc_df, uri=self.calc_data_path, partition_cols=['year', 'quarter'], overwrite_period=True)
 
         # From now on, data is accessed by 'pq' and uses the full datetime range available in the datastore
-        self.plant.context = Context(plant=self.plant, datasource='pq')
+        self.plant.context = Context(plant=self.plant, datasource='pq', eval_start=self.eval_start,
+                                     eval_end=self.eval_end)
```

## sunpeek/db_utils/__init__.py

```diff
@@ -1,2 +1 @@
 DATETIME_COL_NAME = "ds"
-PARTITION_COLS = ['year', 'quarter']
```

## sunpeek/db_utils/crud.py

```diff
@@ -1,20 +1,17 @@
+import datetime
 from typing import Union
-import datetime as dt
 
 from sqlalchemy.orm import Session
-from sqlalchemy import or_, and_, column
+from sqlalchemy import or_, and_
 import sunpeek.components as cmp
 from sunpeek.common.errors import SensorNotFoundError
 
 
-def get_plants(session: Session,
-               plant_id: int = None,
-               plant_name: str = None,
-               ):
+def get_plants(session: Session, plant_id: int=None, plant_name: str=None):
     """
     Gets a plant by name from the database, or all plants if no `plant_name` or `plant_id` parameter is supplied
     Parameters
     ----------
     session
     plant_id
     plant_name
@@ -29,65 +26,55 @@
         return session.query(cmp.Plant).filter(cmp.Plant.id == plant_id).one()
     elif plant_name is not None:
         return session.query(cmp.Plant).filter(cmp.Plant.name == plant_name).one()
     else:
         return session.query(cmp.Plant).all()
 
 
-def get_components(session: Session,
-                   # component: Union[cmp.Component, cmp.Sensor, cmp.OperationalEvent, str],
-                   component: Union['cmp.Component', 'cmp.Sensor', 'cmp.OperationalEvent', str],
-                   id: int = None,
-                   name: str = None,
-                   plant_id: int = None,
-                   plant_name: str = None,
-                   attr = None):
+def get_components(session: Session, component: Union[cmp.Component, str], id: int = None, name:str = None, plant_id: int=None, plant_name: str=None):
     """
     Get a component, or list of components from the database.
 
     Parameters
     ----------
     session
     component: An instance of a subclass of cmp.Component
     id
     name
     plant_id
     plant_name
 
     Returns
     -------
-    Component object, or list of Component objects
+    Comonent object, or list of Component objects
     """
 
     if isinstance(component, str):
         component = cmp.__dict__[component]
 
     qry = session.query(component)
-
     if id is not None:
         obj = qry.filter(component.id == id).one()
         if plant_id is not None and obj.plant_id != plant_id:
-            raise SensorNotFoundError(
-                f"{component.__name__} with id {id} has a plant_id that does not match the passed "
-                f"plant_id. This means that the component is associated with a different plant or "
-                f"no plant, probably an incorrect id or plant_id was passed.")
+            raise SensorNotFoundError(f"{component.__name__} with id {id} has a plant_id that does not match the passed "
+                                      f"plant_id. This means that the component is associated with a different plant or "
+                                      f"no plant, probably an incorrect id or plant_id was passed.")
         return qry.filter(component.id == id).one()
     if plant_id is not None:
         qry = qry.filter(component.plant_id == plant_id)
     if name is not None and component != cmp.Sensor:
         return qry.filter(component.name == name).one()
     elif name is not None and component == cmp.Sensor:
         return qry.filter(component.raw_name == name).one()
     if plant_name is not None:
         qry = qry.join(cmp.Plant, component.plant_id == cmp.Plant.id).filter(cmp.Plant.name == plant_name)
-
     return qry.all()
 
 
-def get_sensors(session: Session, id: int = None, raw_name: str = None, plant_id: int = None, plant_name: str = None):
+def get_sensors(session: Session, id: int = None, raw_name: str = None, plant_id: int=None, plant_name: str=None):
     """
     Get all sensors, all sensors of a given plant, or a specific sensor. Note, parameters have the following precedence:
     id, name, plant_id, plant_name. So if a component name is given, all further parameters are ignored
 
     Parameters
     ----------
     session
@@ -112,67 +99,68 @@
     ----------
     session
     component: An instance of a subclass of cmp.Component
     commit: whether to commit the new components, if set to false, session.commit() must be called later.
 
     Returns
     -------
-    The updated object after commit to the database. This may have had modifications made by database side logic.
+    The updated object after commit to the database (this may have had modifications made by database side logic
     """
     session.add(component)
     if commit:
         session.commit()
     return component
 
 
 def update_component(session: Session, component: cmp.helpers.ORMBase, commit=True):
-    """Updates a component to the database.
+    """
+    Updates a component to the database
 
     Parameters
     ----------
     session
     component: An instance of a subclass of cmp.Component
-    commit: whether to commit the new components, if set to false, session.commit() must be called later.
 
     Returns
     -------
-    The updated object after commit to the database. This may have had modifications made by database side logic.
+    The updated object after commit to the database (this may have had modifications made by database side logic
+    commit: whether to commit the new components, if set to false, session.commit() must be called later.
     """
     session.add(component)
     if commit:
         session.commit()
-
     return component
 
 
-def delete_component(session: Session, component: cmp.Component) -> None:
+def delete_component(session: Session, component: cmp.Component):
     """
     Removes a component from the database
 
     Parameters
     ----------
     session
     component: An instance of a subclass of cmp.Component
 
     Returns
     -------
-    The updated object after commit to the database. This may have had modifications made by database side logic.
+    The updated object after commit to the database (this may have had modifications made by database side logic
     """
+
     session.delete(component)
     session.commit()
 
 
 def get_operational_events(session: Session, event_id=None, plant_id=None, search_start=None, search_end=None):
     if search_start is None and search_end is None:
         return get_components(session, cmp.OperationalEvent, id=event_id, plant_id=plant_id)
 
     if search_end is None:
-        search_end = dt.datetime(9999, 1, 1, 0, 0)
+        search_end = datetime.datetime(9999, 1, 1, 0, 0)
     if search_start is None:
-        search_end = dt.datetime(1900, 1, 1, 0, 0)
+        search_end = datetime.datetime(1900, 1, 1, 0, 0)
 
     events = session.query(cmp.OperationalEvent).join(cmp.Plant).where(cmp.Plant.id == plant_id).where(
         or_(
             and_(cmp.OperationalEvent.event_start >= search_start, cmp.OperationalEvent.event_start <= search_end),
             and_(cmp.OperationalEvent.event_end >= search_start, cmp.OperationalEvent.event_end <= search_end),
             and_(cmp.OperationalEvent.event_start <= search_end, cmp.OperationalEvent.event_end >= search_start)
         )).all()
```

## sunpeek/db_utils/init_db.py

```diff
@@ -1,88 +1,51 @@
 import os
 from sqlalchemy_utils import database_exists, create_database
 from sqlalchemy import event
 from sqlalchemy.engine import Engine
-import sqlalchemy.exc
-import alembic
 import alembic.config
-import importlib
-from sunpeek.common.unit_uncertainty import Q
 
 import sunpeek.components as cmp
 from sunpeek.common import utils
-from sunpeek.definitions import collectors, fluid_definitions
+from sunpeek.common.errors import DatabaseAlreadyExistsError
+import sunpeek.definitions.collector_types
+import sunpeek.definitions.fluid_definitions
 
 
 def init_db():
     db_url = utils.get_db_conection_string()
-    importlib.reload(collectors)
-    importlib.reload(fluid_definitions)
-    if not database_exists(db_url):
-        utils.sp_logger.info(f'[init_db] Attempting to setup fresh DB {os.environ.get("HIT_DB_NAME", "harvestit")} on '
-                             f'{os.environ.get("HIT_DB_HOST", "localhost:5432")}')
-
-        create_database(db_url)
-        cmp.make_tables(utils.db_engine)
-
-        with utils.S.begin() as session:
-            # Add collectors
-            for item in collectors.all_definitions:
-                session.add(item)
-
-            # Add fluids
-            for item in fluid_definitions.all_definitions:
-                session.add(item)
-
-            session.commit()
-            session.expunge_all()
-
-        os.chdir(os.path.join(os.path.dirname(os.path.realpath(__file__)), '../..'))
-        alembicArgs = ['--raiseerr', 'stamp', 'head']
-        alembic.config.main(argv=alembicArgs)
-    else:
-        apply_db_migrations()
-
-
-def apply_db_migrations():
-    utils.sp_logger.info(f'[init_db] Applying migrations and updates to DB {os.environ.get("HIT_DB_NAME", "harvestit")} '
-                         f'on {os.environ.get("HIT_DB_HOST", "localhost:5432")}')
-    os.chdir(os.path.join(os.path.dirname(os.path.realpath(__file__)), '../..'))
-    alembic.config.main(argv=['--raiseerr', 'upgrade', 'head'])
+    if database_exists(db_url):
+        raise DatabaseAlreadyExistsError(f"Database {db_url.split('/')[-1]} already exists, please set HIT_DB_NAME to "
+                                         f"a database doesn't exist yet, it will be created for you")
+
+    utils.sp_logger.info(f'[init_db] Attempting to setup DB {os.environ.get("HIT_DB_NAME", "harvestit")} on '
+                         f'{os.environ.get("HIT_DB_HOST", "localhost:5432")}')
+    # engine = sqlalchemy.create_engine('/'.join(utils.get_db_conection_string().split('/')[:-1]))
+    # engine.dispose()
+
+    create_database(db_url)
 
-    col_types = [(item, item.name) for item in collectors.all_definitions]
-    fluid_defs = [(item, item.name) for item in fluid_definitions.all_definitions]
+    cmp.make_tables(utils.db_engine)
 
     with utils.S.begin() as session:
-        with session.no_autoflush:
-            # Add collector types
-            for col_type, name in col_types:
-                try:
-                    existing_col = session.query(cmp.Collector).filter(cmp.Collector.name == name).one()
-                    col_type.id = existing_col.id
-                    print(f'overwriting {name}')
-                    session.merge(col_type)
-                except sqlalchemy.exc.NoResultFound:
-                    print(f'adding {col_type.name}')
-                    session.add(col_type)
-
-            # Add fluids
-            for fluid, name in fluid_defs:
-                try:
-                    existing_fluid = session.query(cmp.FluidDefinition).filter(cmp.FluidDefinition.name == name).one()
-                    fluid.id = existing_fluid.id
-                    print(f'overwriting {name}')
-                    session.merge(fluid)
-                except sqlalchemy.exc.NoResultFound:
-                    print(f'adding {name}')
-                    session.add(fluid)
+        # Add collector types
+        for item in sunpeek.definitions.collector_types.all_definitions:
+            session.add(item)
+
+        # Add fluids
+        for item in sunpeek.definitions.fluid_definitions.all_definitions:
+            session.add(item)
 
         session.commit()
         session.expunge_all()
 
+    os.chdir(os.path.join(os.path.dirname(os.path.realpath(__file__)), '../..'))
+    alembicArgs = ['--raiseerr', 'stamp', 'head']
+    alembic.config.main(argv=alembicArgs)
+
 
 @event.listens_for(Engine, "connect")
 def set_sqlite_pragma(dbapi_connection, connection_record):
     if os.environ.get('HIT_DB_TYPE', 'postgresql') == 'sqlite':
         cursor = dbapi_connection.cursor()
         cursor.execute("PRAGMA foreign_keys=ON")
         cursor.close()
```

## sunpeek/definitions/__init__.py

```diff
@@ -1,14 +1,17 @@
 """
 This package contains definitions for the pre-defined components such as fluids or collectors.
 These are used to pre-populate the application database.
 """
 import enum
 from pathlib import Path
 
+# from sunpeek.definitions import collector_types
+# from sunpeek.definitions import fluid_definitions
+
 
 class FluidProps(str, enum.Enum):
     density = 'density'
     heat_capacity = 'heat capacity'
 
 
 fluid_data_dir = Path(__file__).with_name('fluid_data')
```

## sunpeek/demo/demo_plant.py

```diff
@@ -1,9 +1,9 @@
 import json
-import datetime as dt
+from datetime import datetime
 from sqlalchemy.orm import Session
 
 import sunpeek.demo
 from sunpeek.common import config_parser
 from sunpeek.data_handling import data_uploader
 from sunpeek.data_handling.wrapper import use_csv
 from sunpeek.db_utils import crud
@@ -20,21 +20,21 @@
 
 @requires_demo_data
 def create_demoplant(session: Session, name: str = None):
     with open(sunpeek.demo.DEMO_CONFIG_PATH, 'r') as f:
         conf = json.load(f)
 
     # Plant name must be unique in database => create unique
-    if name is None:
-        name = f'demoplant_{dt.datetime.now().strftime("%Y%m%d_%H%M%S")}'
+    unique_plant_name = f'demoplant_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
+    name = unique_plant_name if (name is None) else name
     conf['plant']['name'] = name
 
     config_parser.make_and_store_plant(conf, session)
 
-    plant = crud.get_plants(session, plant_name=name)
+    plant = crud.get_plants(session, plant_name=conf['plant']['name'])
     virtuals.config_virtuals(plant)
     session.commit()
     return plant
 
 
 @requires_demo_data
 def add_demo_data(plant: cmp.Plant, session: Session = None):
@@ -44,12 +44,11 @@
 
     if session is not None:
         up = data_uploader.DataUploader_pq(plant=plant,
                                            timezone=timezone,
                                            datetime_template=datetime_template,
                                            )
         up.do_upload(files=files)  # includes virtual sensor calculation
-        session.commit()
     else:
         use_csv(plant, csv_files=files,
                 timezone=timezone,
                 datetime_template=datetime_template)
```

## sunpeek/demo/demo_plant_script.py

```diff
@@ -7,28 +7,28 @@
 The data used here (together with a detailed description) is available at https://zenodo.org/record/7741084
 
 .. codeauthor:: Philip Ohnewein <p.ohnewein@aee.at>
 .. codeauthor:: Marnoch Hamilton-Jones <m.hamilton-jones@aee.at>
 .. codeauthor:: Daniel Tschopp <d.tschopp@aee.at>
 """
 
-import os
 import json
-import datetime as dt
-import pytz
+import warnings
+from datetime import datetime
+import webbrowser
 
 import sunpeek.demo
 from sunpeek.data_handling.wrapper import use_csv
 from sunpeek.demo.demo_plant import requires_demo_data
 from sunpeek.core_methods.pc_method.wrapper import run_performance_check
-from sunpeek.core_methods.pc_method.plotting import create_pdf_report
+from sunpeek.core_methods.pc_method import plot_all
 from sunpeek.common.unit_uncertainty import Q
 from sunpeek.common import config_parser
 from sunpeek.common.utils import DatetimeTemplates
-from sunpeek.components import CollectorQDT, CollectorTypes, iam_methods, FluidFactory, CoolPropFluid
+from sunpeek.components import CollectorTypeQDT, iam_methods, FluidFactory, CoolPropFluid
 from sunpeek.definitions.fluid_definitions import get_definition, WPDFluids
 
 
 def get_fluid(fluid_str: str = WPDFluids.fhw_pekasolar.value.name):
     """Return heat transfer fluid: Default is fluid of FHW plant. Choose other fluid to see how they would behave.
     """
     if fluid_str == WPDFluids.fhw_pekasolar.value.name:
@@ -45,71 +45,69 @@
         return fluid
     raise ValueError(f'Unknown fluid string "{fluid_str}".')
 
 
 def get_collector():
     """Return collector definition of flat plate collector used in collector array
     """
-    return CollectorQDT(name="Arcon 3510",
-                        manufacturer_name="Arcon-Sunmark A/S",
-                        product_name="HTHEATstore 35/10",
-                        licence_number='SP SC0843-14',
-                        test_report_id="6P02267-C-Rev 1 (2016-07-06), 4P04266-C-Rev 2 (2015-11-10)",
-                        certificate_date_issued=dt.datetime(2016, 7, 14),
-                        certificate_lab='SP Technical Research Institute of Sweden',
-                        collector_type=CollectorTypes.flat_plate.value,
-                        description="Cover: single-glazed & foil. Absorber: harp. Hydraulics: Non-Tichelmann",
-                        test_reference_area="gross",
-                        area_gr=Q(13.57, "m**2"),
-                        gross_width=Q(5973, "mm"),
-                        gross_length=Q(2272, "mm"),
-                        gross_height=Q(145, "mm"),
-                        a1=Q(2.067, "W m**-2 K**-1"),
-                        a2=Q(0.009, "W m**-2 K**-2"),
-                        a5=Q(7.313, "kJ m**-2 K**-1"),
-                        a8=Q(0, 'W m**-2 K**-4'),
-                        kd=Q(0.93, ""),
-                        eta0b=Q(0.745, ""),
-                        f_prime=Q(0.95, ""),
-                        iam_method=iam_methods.IAM_Interpolated(
-                            aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                            iam_reference=Q([1, 0.99, 0.97, 0.94, 0.9, 0.82, 0.65, 0.32, 0]))
-                        )
+    return CollectorTypeQDT(name="Arcon 3510",
+                            manufacturer_name="Arcon-Sunmark A/S",
+                            product_name="HTHEATstore 35/10",
+                            licence_number='SP SC0843-14',
+                            test_report_id="6P02267-C-Rev 1 (2016-07-06), 4P04266-C-Rev 2 (2015-11-10)",
+                            certificate_date_issued=datetime(2016, 7, 14),
+                            certificate_lab='SP Technical Research Institute of Sweden',
+                            description="Cover: single-glazed & foil. Absorber: harp. Hydraulics: Non-Tichelmann",
+                            test_reference_area="gross",
+                            area_gr=Q(13.57, "m**2"),
+                            gross_width=Q(5973, "mm"),
+                            gross_length=Q(2272, "mm"),
+                            gross_height=Q(145, "mm"),
+                            a1=Q(2.067, "W m**-2 K**-1"),
+                            a2=Q(0.009, "W m**-2 K**-2"),
+                            a5=Q(7.313, "kJ m**-2 K**-1"),
+                            kd=Q(0.93, ""),
+                            eta0b=Q(0.745, ""),
+                            f_prime=Q(0.95, ""),
+                            iam_method=iam_methods.IAM_Interpolated(
+                                aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                iam_reference=Q([1, 0.99, 0.97, 0.94, 0.9, 0.82, 0.65, 0.32, 0]))
+                            )
 
 
 if __name__ == '__main__':
     requires_demo_data(None)
     # STEP 1: Make Plant from Config
     with open(sunpeek.demo.DEMO_CONFIG_PATH) as f:
         conf = json.load(f)
     plant = config_parser.make_full_plant(conf=conf)
     # Define collector type
-    plant.arrays[0].collector = get_collector()
+    plant.arrays[0].collector_type = get_collector()
     # Define heat transfer fluid
     plant.fluid_solar = get_fluid()
     # This is just to showcase how other fluids would be used:
     # plant.fluid_solar = get_fluid('water')
     # plant.fluid_solar = get_fluid('ASHRAE, Propylene Glycol')
     # plant.fluid_solar = get_fluid('Antifrogen L')
 
     # STEP 2: Submit measurement data
     # data = sunpeek.demo.DEMO_DATA_PATH_2DAYS
     data = sunpeek.demo.DEMO_DATA_PATH_1MONTH
     # data = sunpeek.demo.DEMO_DATA_PATH_1YEAR
-    data_output = use_csv(plant, csv_files=[data], timezone='utc', datetime_template=DatetimeTemplates.year_month_day)
+    data_output = use_csv(plant, csv_files=[data], timezone='UTC', datetime_template=DatetimeTemplates.year_month_day)
 
     # STEP 3: Run Performance Check method & create plots
     # Use default settings:
     pc_output = run_performance_check(plant).output
     # or try specific settings:
     # pc_output = run_performance_check(plant,
     #                                   method=['extended'],
-    #                                   formula=[2],
+    #                                   equation=[2],
     #                                   safety_uncertainty=0.9,
     #                                   ).output
 
-    # Create pdf report
-    report_path = create_pdf_report(pc_output)
-    # Include all hourly-interval plots -> may be slow!
-    # report_path = create_pdf(pc_output, include_interval_plots=True)
-    # Optionally, open file
-    # os.startfile(report_path)
+    try:
+        plot_all(pc_output)
+    except webbrowser.Error:
+        warnings.warn('Cannot plot results, no runnable browser detected')
+    except ModuleNotFoundError:
+        warnings.warn('Cannot plot results, module plotly not installed')
```

## sunpeek/exporter.py

```diff
@@ -13,20 +13,20 @@
 from sunpeek.components.helpers import ResultStatus
 import sunpeek.serializable_models as smodels
 from sunpeek.db_utils import db_data_operations, crud
 # from sunpeek.db_utils import DATETIME_COL_NAME
 
 
 def create_export_config(plant):
-    collectors = [array.collector for array in plant.arrays]
-    fluid_definitions = [plant.fluid_solar.fluid if plant.fluid_solar is not None else None]
 
-    return {"collectors": collectors,
-            "fluid_definitions": fluid_definitions,
-            "plant": plant}
+    collectors = [array.collector_type for array in plant.arrays]
+    sensor_types = [sensor.sensor_type for sensor in plant.raw_sensors if sensor.sensor_type is not None]
+    fluid_definitions = [plant.fluid_solar.fluid]
+
+    return {"collectors": collectors, "sensor_types": sensor_types, "fluid_definitions": fluid_definitions, "plant": plant}
 
 
 def _bundle(conf, sensors, plant, session):
     years = plant.time_index.year.unique()
     conf_file = io.BytesIO(bytes(conf.json(), 'UTF-8'))
     temp_handle, temp_path = tempfile.mkstemp(suffix='.tar.gz')
     index = pd.Series(plant.time_index, index=plant.time_index)
```

## sunpeek/serializable_models.py

```diff
@@ -1,31 +1,30 @@
+import datetime
 import uuid
 import enum
 from dataclasses import field
-import datetime as dt
 
 from pydantic.dataclasses import dataclass
 import numpy as np
-from pydantic import validator, root_validator, constr, Field
+from pydantic import validator, constr
 from typing import Union, Any, Dict, List, Tuple
-# import pint.errors
+import pint.errors
 
 import sunpeek.components as cmp
-# import sunpeek.components.physical
+import sunpeek.components.physical
 from sunpeek.common.errors import AlgorithmError
-from sunpeek.common.unit_uncertainty import Q, parse_quantity
-from sunpeek.components.helpers import SensorMap, DatetimeTemplates, AccuracyClass, InstallCondition, AlgoCheckMode, IsVirtual
+from sunpeek.common.unit_uncertainty import Q
+from sunpeek.components.base import IsVirtual, AlgoCheckMode
+from sunpeek.components.helpers import SensorMap, DatetimeTemplates, AccuracyClass, InstallCondition
 from sunpeek.components.fluids import UninitialisedFluid
-from sunpeek.base_model import BaseModel, Quantity, np_to_list
-from sunpeek.components.types import Collector as ORMCollector
-from sunpeek.components.types import ApertureParameters
+from sunpeek.base_model import BaseModel
 
 
 class ComponentBase(BaseModel):
-    sensor_map: Dict[str, str | None] | None
+    sensor_map: Union[Dict[str, Union[str, None]], None]
 
     @validator('sensor_map', pre=True)
     def get_raw_name(cls, v):
         out = {}
         for key, item in v.items():
             if isinstance(item, SensorMap):
                 try:
@@ -33,165 +32,151 @@
                 except AttributeError:
                     pass
             else:
                 out[key] = item
         return out
 
 
+def np_to_list(val):
+    if isinstance(val, np.ndarray) and val.ndim == 1:
+        return list(val)
+    elif isinstance(val, np.ndarray) and val.ndim > 1:
+        out = []
+        for array in list(val):
+            out.append(np_to_list(array))
+        return out
+    return val
+
+
+class Quantity(BaseModel):
+    magnitude: Union[float, List[float], List[List[float]]]
+    units: str
+
+    @validator('magnitude', pre=True)
+    def convert_numpy(cls, val):
+        return np_to_list(val)
+
+    @validator('units', pre=True)
+    def pretty_unit(cls, val):
+        if isinstance(val, pint.Unit):
+            return f"{val:~P}"
+        return val
+
+
 class SensorTypeValidator(BaseModel):
     name: str
     compatible_unit_str: str
     description: str
-    # min_limit: Quantity | None
-    # max_limit: Quantity | None
+    # min_limit: Union[Quantity, None]
+    # max_limit: Union[Quantity, None]
     # # non_neg: bool
-    # max_fill_period: dt.datetime | None
-    # sensor_hangs_period: dt.datetime | None
-    # # high_maxerr_const: Quantity | None
-    # # high_maxerr_perc: Quantity | None
-    # # medium_maxerr_const: Quantity | None
-    # # medium_maxerr_perc: Quantity | None
-    # # low_maxerr_const: Quantity | None
-    # # low_maxerr_perc: Quantity | None
-    # # standard_install_maxerr_const: Quantity | None
-    # # standard_install_maxerr_perc: Quantity | None
-    # # poor_install_maxerr_const: Quantity | None
-    # # poor_install_maxerr_perc: Quantity | None
-    info_checks: dict | None
-    max_fill_period: dt.datetime | None
-    sensor_hangs_period: dt.datetime | None
-    lower_replace_min: Quantity | None
-    lower_replace_max: Quantity | None
-    lower_replace_value: Quantity | None
-    upper_replace_min: Quantity | None
-    upper_replace_max: Quantity | None
-    upper_replace_value: Quantity | None
-    # equation: str | None
-    common_units: list | None
+    # max_fill_period: Union[datetime.timedelta, None]
+    # sensor_hangs_period: Union[datetime.timedelta, None]
+    # # high_maxerr_const: Union[Quantity, None]
+    # # high_maxerr_perc: Union[Quantity, None]
+    # # medium_maxerr_const: Union[Quantity, None]
+    # # medium_maxerr_perc: Union[Quantity, None]
+    # # low_maxerr_const: Union[Quantity, None]
+    # # low_maxerr_perc: Union[Quantity, None]
+    # # standard_install_maxerr_const: Union[Quantity, None]
+    # # standard_install_maxerr_perc: Union[Quantity, None]
+    # # poor_install_maxerr_const: Union[Quantity, None]
+    # # poor_install_maxerr_perc: Union[Quantity, None]
+    info_checks: Union[dict, None]
+    max_fill_period: Union[datetime.timedelta, None]
+    sensor_hangs_period: Union[datetime.timedelta, None]
+    lower_replace_min: Union[Quantity, None]
+    lower_replace_max: Union[Quantity, None]
+    lower_replace_value: Union[Quantity, None]
+    upper_replace_min: Union[Quantity, None]
+    upper_replace_max: Union[Quantity, None]
+    upper_replace_value: Union[Quantity, None]
+    equation: Union[str, None]
+    common_units: Union[list, None]
 
 
 class IAM_Method(BaseModel):
     method_type: str
 
 
 class IAM_ASHRAE(IAM_Method):
     method_type = 'IAM_ASHRAE'
     b: Quantity
 
 
 class IAM_K50(IAM_Method):
     method_type = 'IAM_K50'
     k50: Quantity
-    b: Quantity | None
+    b: Union[Quantity, None]
 
 
 class IAM_Ambrosetti(IAM_Method):
     method_type = 'IAM_Ambrosetti'
     kappa: Quantity
 
 
 class IAM_Interpolated(IAM_Method):
     method_type = 'IAM_Interpolated'
     aoi_reference: Quantity
     iam_reference: Quantity
 
 
-class CollectorBase(BaseModel):
+class CollectorTypeBase(BaseModel):
+    test_reference_area: Union[str, None]
+    test_type: Union[str, None]
+    gross_length: Union[Quantity, None]
+    iam_method: Union[IAM_K50, IAM_ASHRAE, IAM_Ambrosetti, IAM_Interpolated, None]
     name: str
-    test_reference_area: str | None = Field(...)
-    test_type: str | None
-    gross_length: Quantity | None
-    iam_method: IAM_K50 | IAM_ASHRAE | IAM_Ambrosetti | IAM_Interpolated | None
-    manufacturer_name: str | None
-    product_name: str | None
-    test_report_id: str | None
-    licence_number: str | None
-    certificate_date_issued: dt.datetime | str | None
-    certificate_lab: str | None
-    certificate_details: str | None
-    collector_type: str
-    area_gr: Quantity | None
-    area_ap: Quantity | None
-    gross_width: Quantity | None
-    gross_height: Quantity | None
-    a1: Quantity | None = Field(...)
-    a2: Quantity | None = Field(...)
-    a5: Quantity | None
-    a8: Quantity | None
-    kd: Quantity | None
-    eta0b: Quantity | None
-    eta0hem: Quantity | None
-    f_prime: Quantity | None
-    concentration_ratio: Quantity | None
-    calculation_info: Dict[str, str] | None
-    aperture_parameters: ApertureParameters | None
-
-
-class CollectorUpdate(CollectorBase):
-    name: str | None
-    collector_type: str | None
-
-
-class Collector(CollectorBase):
-    id: int | None
-    name: str | None
+    manufacturer_name: Union[str, None]
+    product_name: Union[str, None]
+    test_report_id: Union[str, None]
+    licence_number: Union[str, None]
+    certificate_date_issued: Union[datetime.datetime, str, None]
+    certificate_lab: Union[str, None]
+    certificate_details: Union[str, None]
+    area_gr: Union[Quantity, None]
+    area_ap: Union[Quantity, None]
+    gross_width: Union[Quantity, None]
+    gross_height: Union[Quantity, None]
+    a1: Union[Quantity, None]
+    a2: Union[Quantity, None]
+    a5: Union[Quantity, None]
+    kd: Union[Quantity, None]
+    eta0b: Union[Quantity, None]
+    eta0hem: Union[Quantity, None]
+    f_prime: Union[Quantity, None]
+
+
+class CollectorType(CollectorTypeBase):
+    id: Union[int, None]
 
     def __str__(self):
         return f'{self.__class__.__name__} {self.name}'
 
     def __repr__(self):
         return self.__str__()
 
-    @validator('a1', 'a2', 'a5', 'a8', 'kd', 'eta0b', 'eta0hem', 'f_prime', 'concentration_ratio', pre=True)
-    @classmethod
-    def to_default_unit(cls, val, field):
-        # Convert collector performance parameters to default unit -> easier to get unified display in web-ui
-        if val is None:
-            return None
-        default_unit = ORMCollector.get_default_unit(field.name)
-        val = parse_quantity(val).to(default_unit)
-        return val
-        # return Quantity.from_orm(val)
-
-    @validator('aperture_parameters', pre=True)
-    @classmethod
-    def to_default_unit__aperture(cls, val):
-        # Convert collector aperture_parameters to default unit -> easier to get unified display in web-ui
-        if val is None:
-            return None
-        for k, v in val.items():
-            if v is None:
-                val[k] = None
-            else:
-                default_unit = ORMCollector.get_default_unit(k)
-                val[k] = parse_quantity(v).to(default_unit)
-                # val[k] = Quantity.from_orm(parse_quantity(v).to(default_unit))
-
-        return val
-
 
-class CollectorQDT(CollectorBase):
+class CollectorTypeQDT(CollectorTypeBase):
     a1: Quantity
     a2: Quantity
     a5: Quantity
-    a8: Quantity | None
 
 
-class CollectorSST(CollectorBase):
+class CollectorTypeSST(CollectorTypeBase):
     ceff: Quantity
-    
+
 
 class SensorBase(BaseModel):
-    description: str | None
-    accuracy_class: AccuracyClass | None
-    installation_condition: InstallCondition | None
-    info: dict | None = {}
-    raw_name: str | None
-    native_unit: str | None
-    sensor_type: str | None
+    description: Union[str, None]
+    accuracy_class: Union[AccuracyClass, None]
+    installation_condition: Union[InstallCondition, None]
+    info: Union[dict, None] = {}
+    raw_name: Union[str, None]
+    native_unit: Union[str, None]
 
     @validator('info', pre=True)
     def convert_info(cls, v):
         if isinstance(v, cmp.SensorInfo):
             return v._info
         return v
 
@@ -200,24 +185,23 @@
         if isinstance(v, str):
             Q(1, v)
 
         return v
 
 
 class Sensor(SensorBase):
-    id: int | None
-    plant_id: int | None
-    raw_name: str | None
-    sensor_type: str | None
-    native_unit: str | None
-    formatted_unit: str | None
-    is_virtual: bool | None
-    can_calculate: bool | None
-    is_mapped: bool | None
-    is_infos_set: bool | None
+    id: Union[int, None]
+    plant_id: Union[int, None]
+    raw_name: Union[str, None]
+    sensor_type: Union[str, None]
+    native_unit: Union[str, None]
+    formatted_unit: Union[str, None]
+    is_virtual: Union[bool, None]
+    can_calculate: Union[bool, None]
+    is_mapped: Union[bool, None]
 
     @validator('sensor_type', pre=True)
     def convert_sensor_type(cls, v):
         if isinstance(v, cmp.SensorType):
             return v.name
         return v
 
@@ -227,251 +211,190 @@
     native_unit: str = None
 
 
 class BulkUpdateSensor(Sensor):
     id: int
 
 
-class FluidDefinition(BaseModel):
-    id: int | None
+class FluidDefintion(SensorBase):
+    id: Union[int, None]
     model_type: str
     name: str
-    manufacturer: str | None
-    description: str | None
+    manufacturer: Union[str, None]
+    description: Union[str, None]
     is_pure: bool
-    dm_model_sha1: str | None
-    hc_model_sha1: str | None
-    heat_capacity_unit_te: str | None
-    heat_capacity_unit_out: str | None
-    heat_capacity_unit_c: str | None
-    density_unit_te: str | None
-    density_unit_out: str | None
-    density_unit_c: str | None
-    # heat_capacity_onnx: str | None
-    # density_onnx: str | None
+    dm_model_sha1: Union[str, None]
+    hc_model_sha1: Union[str, None]
+    heat_capacity_unit_te: Union[str, None]
+    heat_capacity_unit_out: Union[str, None]
+    heat_capacity_unit_c: Union[str, None]
+    density_unit_te: Union[str, None]
+    density_unit_out: Union[str, None]
+    density_unit_c: Union[str, None]
+    # heat_capacity_onnx: Union[str, None]
+    # density_onnx: Union[str, None]
 
     # @validator('heat_capacity_onnx', 'density_onnx', pre=True)
     # def onnx_to_str(cls, v):
     #     try:
     #         return v.hex()
     #     except AttributeError:
     #         return v
 
 
 class Fluid(BaseModel):
-    id: int | None
-    name: str | None
-    manufacturer_name: str | None
-    product_name: str | None
-    fluid: FluidDefinition
-    concentration: Quantity | None
+    id: Union[int, None]
+    name: Union[str, None]
+    manufacturer_name: Union[str, None]
+    product_name: Union[str, None]
+    fluid: FluidDefintion
+    concentration: Union[Quantity, None]
 
 
 class FluidSummary(BaseModel):
-    name: str | None
-    fluid: str
-    concentration: Quantity | None
-
-    @validator('fluid', pre=True)
-    def fluid_name(cls, v):
-        try:
-            return v.name
-        except AttributeError:
-            return v
-
-
-class FluidExport(BaseModel):
+    name: Union[str, None]
     fluid: str
     concentration: Union[Quantity, None]
 
     @validator('fluid', pre=True)
     def fluid_name(cls, v):
         try:
             return v.name
         except AttributeError:
             return v
 
 
 class Array(ComponentBase):
-    id: int | None
-    plant_id: int | None
-    name: str | None
-    collector: str | None = Field(...)
-    area_gr: Quantity | None
-    area_ap: Quantity | None
-    azim: Quantity | None = Field(...)
-    tilt: Quantity | None = Field(...)
-    row_spacing: Quantity | None
-    n_rows: Quantity | None
-    ground_tilt: Quantity | None
-    mounting_level: Quantity | None
-    fluidvol_total: Quantity | None
-    rho_ground: Quantity | None
-    rho_colbackside: Quantity | None
-    rho_colsurface: Quantity | None
-    max_aoi_shadow: Quantity | None
-    min_elevation_shadow: Quantity | None
-
-    @validator('collector', pre=True)
-    def convert_coll(cls, v):
-        if isinstance(v, cmp.Collector):
-            if v.name is None:
-                return 'unnamed collector'
-            else:
-                return v.name
+    id: Union[int, None]
+    plant_id: Union[int, None]
+    name: Union[str, None]
+    collector_type: Union[str, None]
+    area_gr: Union[Quantity, None]
+    area_ap: Union[Quantity, None]
+    azim: Union[Quantity, None]
+    tilt: Union[Quantity, None]
+    row_spacing: Union[Quantity, None]
+    n_rows: Union[Quantity, None]
+    ground_tilt: Union[Quantity, None]
+    mounting_level: Union[Quantity, None]
+    fluidvol_total: Union[Quantity, None]
+    rho_ground: Union[Quantity, None]
+    rho_colbackside: Union[Quantity, None]
+    rho_colsurface: Union[Quantity, None]
+    max_aoi_shadow: Union[Quantity, None]
+    min_elevation_shadow: Union[Quantity, None]
+
+    @validator('collector_type', pre=True)
+    def convert_col_type(cls, v):
+        if isinstance(v, cmp.CollectorType):
+            return v.name
         return v
 
     def __str__(self):
         return f'{self.__class__.__name__} {self.name}'
 
     def __repr__(self):
         return self.__str__()
 
 
 class NewArray(Array):
     name: str
-    collector: str
-    sensors: Dict[str, NewSensor] | None
-    sensor_map: dict | None
-
-
-class ArrayUpdate(Array):
-    collector: str | None
-    azim: Quantity | None
-    tilt: Quantity | None
-
-
-class ArrayExport(Array):
-    @validator('sensor_map')
-    def remove_mapped_virtuals(cls, val):
-        return {slot: s for slot, s in  val.items() if not '__virtual__' in s}
+    collector_type: str
+    sensors: Union[Dict[str, NewSensor], None]
+    sensor_map: Union[dict, None]
 
 
 class DataUploadDefaults(BaseModel):
-    id: int | None
-    datetime_template: DatetimeTemplates | None
-    datetime_format: str | None
-    timezone: str | None
-    csv_separator: str | None
-    csv_decimal: str | None
-    csv_encoding: str | None
-    index_col: int | None
+    id: Union[int, None]
+    datetime_template: Union[DatetimeTemplates, None]
+    datetime_format: Union[str, None]
+    timezone: Union[str, None]
+    csv_separator: Union[str, None]
+    csv_decimal: Union[str, None]
+    csv_encoding: Union[str, None]
+    index_col: Union[int, None]
 
 
 class PlantBase(ComponentBase):
-    latitude: Quantity | None = Field(...)
-    longitude: Quantity | None = Field(...)
-    owner: str | None
-    operator: str | None
-    description: str | None
-    location_name: str | None
-    elevation: Quantity | None
-    fluid_solar: FluidSummary | str | None
-    arrays: List[Array] | None
-    fluidvol_total: Quantity | None
-    raw_sensors: List[Sensor] | None
+    owner: Union[str, None]
+    operator: Union[str, None]
+    description: Union[str, None]
+    location_name: Union[str, None]
+    altitude: Union[Quantity, None]
+    fluid_solar: Union[FluidSummary, str, None]
+    arrays: Union[List[Array], None]
+    fluid_vol: Union[Quantity, None]
+    raw_sensors: Union[List[Sensor], None]
 
     @validator('fluid_solar', pre=True)
     def convert_fluid(cls, v):
         if isinstance(v, cmp.Fluid):
             if isinstance(v, UninitialisedFluid):
                 return FluidSummary(name=v.fluid_def_name, fluid=v.fluid_def_name, concentration=None)
             return FluidSummary(name=v.name, fluid=v.fluid.name, concentration=getattr(v, 'concentration', None))
         return v
 
 
 class Plant(PlantBase):
-    name: str
-    id: int | None
-    local_tz_string_with_DST: str | None
-    tz_data_offset: float | None
-    data_upload_defaults: DataUploadDefaults | None
-    virtuals_calculation_uptodate: bool | None
+    name: Union[str, None]
+    id: Union[int, None]
+    latitude: Union[Quantity, None]
+    longitude: Union[Quantity, None]
+    fluid_solar: Union[FluidSummary, str, None]
+    local_tz_string_with_DST: Union[str, None]
+    data_upload_defaults: Union[DataUploadDefaults, None]
 
     def __str__(self):
         return f'{self.__class__.__name__} {self.name}'
 
     def __repr__(self):
         return self.__str__()
 
 
-class UpdatePlant(PlantBase):
-    name: str | None
-    sensors: Dict[str, NewSensor] | None
-    fluid_solar: FluidSummary | None
-    latitude: Quantity | None
-    longitude: Quantity | None
-    fluid_solar: FluidSummary | str | None
-    data_upload_defaults: DataUploadDefaults | None
+class UpdatePlant(Plant):
+    sensors: Union[Dict[str, NewSensor], None]
+    fluid_solar: Union[FluidSummary, None]
 
 
 class NewPlant(PlantBase):
     name: str
     latitude: Quantity
     longitude: Quantity
-    fluid_solar: FluidSummary | None
-    raw_sensors: List[NewSensor] | None
-    sensor_map: dict | None
-
-
-class PlantExport(PlantBase):
-    name: str | None
-    latitude: Quantity | None
-    longitude: Quantity | None
-    local_tz_string_with_DST: str | None
-    tz_data_offset: float | None
-    data_upload_defaults: DataUploadDefaults | None
-    arrays: List[ArrayExport | None]
-    fluid_solar: FluidExport | None
-
-    @validator('raw_sensors')
-    def replace_raw_sensors(cls, val):
-        try:
-            return [sensor for sensor in val if not sensor['is_virtual']]
-        except TypeError:
-            return [sensor for sensor in val if not sensor.is_virtual]
-
-    @validator('sensor_map')
-    def remove_mapped_virtuals(cls, val):
-        return {slot: s for slot, s in  val.items() if not '__virtual__' in s}
+    fluid_solar: Union[FluidSummary, None]
+    raw_sensors: Union[List[NewSensor], None]
+    sensor_map: Union[dict, None]
 
 
 class PlantSummaryBase(BaseModel):
-    name: str | None
-    owner: str | None
-    operator: str | None
-    description: str | None
-    location_name: str | None
-    latitude: Quantity | None
-    longitude: Quantity | None
-    elevation: Quantity | None
+    name: Union[str, None]
+    owner: Union[str, None]
+    operator: Union[str, None]
+    description: Union[str, None]
+    location_name: Union[str, None]
+    latitude: Union[Quantity, None]
+    longitude: Union[Quantity, None]
+    altitude: Union[Quantity, None]
 
 
 class PlantSummary(PlantSummaryBase):
     id: int
     name: str
-    virtuals_calculation_uptodate: bool | None
-
-
-class PlantDataStartEnd(BaseModel):
-    start: dt.datetime | None
-    end: dt.datetime | None
 
 
 class Error(BaseModel):
     error: str
     message: str
     detail: str
 
 
 class Job(BaseModel):
     id: uuid.UUID
     status: cmp.helpers.ResultStatus
-    result_url: str | None
-    plant: str | None
+    result_url: Union[str, None]
+    plant: Union[str, None]
 
     @validator('plant', pre=True)
     def plant_to_str(cls, v):
         if v is not None:
             return v.name
 
 
@@ -482,54 +405,28 @@
     @validator('job_id')
     def uuid_to_str(cls, v):
         if v is not None:
             return str(v)
 
 
 class ConfigExport(BaseModel):
-    collectors: List[Collector]
-    fluid_definitions: List[FluidDefinition | None]
-    plant: PlantExport
-
-    @classmethod
-    def _remove_ids(cls, item):
-        if hasattr(item, 'id'):
-            del item.id
-        if hasattr(item, 'plant_id'):
-            del item.plant_id
-        if isinstance(item, BaseModel):
-            for atr, val in item.__dict__.items():
-                if isinstance(val, list):
-                    item.__dict__[atr] = [cls._remove_ids(val_i) for val_i in val]
-                else:
-                    item.__dict__[atr] = cls._remove_ids(val)
-        return item
-
-    @root_validator()
-    def _exclude_id(cls, val):
-        val['collectors'] = [cls._remove_ids(col) for col in val['collectors']]
-        val['fluid_definitions'] = [cls._remove_ids(fdef) for fdef in cls._remove_ids(val['fluid_definitions'])]
-        val['plant'] = cls._remove_ids(val['plant'])
-        return val
-
-
-class ConfigImport(BaseModel):
-    collectors: List[Collector]
-    fluid_definitions: List[FluidDefinition | None]
-    plant: NewPlant
+    collectors: List[CollectorType]
+    sensor_types: List[SensorTypeValidator]
+    fluid_definitions: List[FluidDefintion]
+    plant: Plant
 
 
 class SensorSlotValidator(BaseModel):
     """
     A pydantic class used to hold and validate information on a component sensor slot.
 
     Parameters
     ----------
     name : str
-        The name of the slot, which behaves like a component attribute and can be used to access the mapped sensor from
+        The name of the slot, which beahvaes like a component attribute and can be used to access the mapped sensor from
         the component. e.g. te_amb. `name` only needs to be unique and understandable in the context of a specific
         component, e.g. the `tp` slot of a plant includes the total power of all arrays, whereas `tp` of an array is
         just that array's power.
     descriptive_name : str
         A longer more descriptive name, e.g. for display to a user in a front end client. Limited to 24 characters
     description : str
         A description of the purpose and use of the slot.
@@ -537,116 +434,95 @@
         Whether the sensor for a slot is always virtual, can be virtual given certain conditions, or is never virtual
     """
 
     name: str
     sensor_type: Union[str, SensorTypeValidator]
     descriptive_name: constr(max_length=57)
     virtual: IsVirtual
-    description: str | None
+    description: Union[str, None]
 
+    # def __init__(self,
+    #              name: str,
+    #              sensor_type: str,
+    #              descriptive_name: str,
+    #              virtual: Union[IsVirtual, str],
+    #              description: str = None):
+    #     super().__init__(name=name, sensor_type=sensor_type, descriptive_name=descriptive_name, virtual=virtual,
+    #                      description=description)
 
-## PC Method -----------------------
 
 class PCMethodOutputPlant(BaseModel):
-    id: int | None
+    id: Union[int, None]
     plant: Plant
 
-    n_intervals: int | None
-    total_interval_length: Union[dt.timedelta, None]
-    datetime_intervals_start: Union[List[dt.datetime], None]
-    datetime_intervals_end: Union[List[dt.datetime], None]
-
-    tp_measured: Quantity | None
-    tp_sp_measured: Quantity | None
-    tp_sp_estimated: Quantity | None
-    tp_sp_estimated_safety: Quantity | None
-    mean_tp_sp_measured: Quantity | None
-    mean_tp_sp_estimated: Quantity | None
-    mean_tp_sp_estimated_safety: Quantity | None
-
-    target_actual_slope: Quantity | None
-    target_actual_slope_safety: Quantity | None
-
-    fluid_solar: FluidSummary | None
-    mean_temperature: Quantity | None
-    mean_fluid_density: Quantity | None
-    mean_fluid_heat_capacity: Quantity | None
+    n_intervals: Union[int, None]
+    datetime_intervals_start: Union[List[datetime.datetime], None]
+    datetime_intervals_end: Union[List[datetime.datetime], None]
+
+    tp_measured: Union[Quantity, None]
+    tp_sp_measured: Union[Quantity, None]
+    tp_sp_estimated: Union[Quantity, None]
+    tp_sp_estimated_safety: Union[Quantity, None]
+    mean_tp_sp_measured: Union[Quantity, None]
+    mean_tp_sp_estimated: Union[Quantity, None]
+    mean_tp_sp_estimated_safety: Union[Quantity, None]
+
+    target_actual_slope: Union[Quantity, None]
+    target_actual_slope_safety: Union[Quantity, None]
+
+    fluid_solar: Union[FluidSummary, None]
+    mean_temperature: Union[Quantity, None]
+    mean_fluid_density: Union[Quantity, None]
+    mean_fluid_heat_capacity: Union[Quantity, None]
 
     @validator('datetime_intervals_start', 'datetime_intervals_end', pre=True)
     def array_to_list(cls, val):
         if isinstance(val, np.ndarray):
             return list(val)
 
 
-class PCMethodOutputData(BaseModel):
-    id: int | None
-
-    te_in: Quantity | None
-    te_out: Quantity | None
-    te_op: Quantity | None
-    te_op_deriv: Quantity | None
-
-    aoi: Quantity | None
-    iam_b: Quantity | None
-    ve_wind: Quantity | None
-
-    rd_gti: Quantity | None
-    rd_bti: Quantity | None
-    rd_dti: Quantity | None
-
-
 class PCMethodOutputArray(BaseModel):
-    id: int | None
+    id: Union[int, None]
     array: Array
-    data: PCMethodOutputData | None
 
-    tp_sp_measured: Quantity | None
-    tp_sp_estimated: Quantity | None
-    tp_sp_estimated_safety: Quantity | None
-    mean_tp_sp_measured: Quantity | None
-    mean_tp_sp_estimated: Quantity | None
-    mean_tp_sp_estimated_safety: Quantity | None
+    tp_sp_measured: Union[Quantity, None]
+    tp_sp_estimated: Union[Quantity, None]
+    tp_sp_estimated_safety: Union[Quantity, None]
+    mean_tp_sp_measured: Union[Quantity, None]
+    mean_tp_sp_estimated: Union[Quantity, None]
+    mean_tp_sp_estimated_safety: Union[Quantity, None]
 
 
 class PCMethodOutput(BaseModel):
-    id: int | None
+    id: Union[int, None]
     plant: PlantSummary
 
-    datetime_eval_start: dt.datetime
-    datetime_eval_end: dt.datetime
+    datetime_eval_start: datetime.datetime
+    datetime_eval_end: datetime.datetime
 
     # Algorithm settings
     pc_method_name: str
     evaluation_mode: str
-    formula: int
+    equation: int
     wind_used: bool
 
     # Results
     settings: Dict[str, Any]  # Type checking done in PCSettings
     plant_output: PCMethodOutputPlant
     array_output: List[PCMethodOutputArray]
 
 
 class OperationalEvent(BaseModel):
-    id: int | None
+    id: Union[int, None]
     plant: Union[str, PlantSummary]
-    event_start: dt.datetime
-    event_end: dt.datetime | None
+    event_start: datetime.datetime
+    event_end: Union[datetime.datetime, None]
     ignored_range: bool = False
-    description: str | None
-    original_timezone: str | None
-
-
-class PCMethodSettings(BaseModel):
-    safety_uncertainty: Union[float, None]
-    safety_pipes: Union[float, None]
-    safety_others: Union[float, None]
-    evaluation_mode: str | None
-    formula: int | None
-    wind_used: bool | None
+    description: Union[str, None]
+    original_timezone: Union[str, None]
 
 
 # def dataclass_to_pydantic(cls: dataclasses.dataclass, name: str) -> BaseModel:
 #     # get attribute names and types from dataclass into pydantic format
 #     pydantic_field_kwargs = dict()
 #     for _field in dataclasses.fields(cls):
 #         # check is field has default value
@@ -669,401 +545,311 @@
 
 class ProblemType(str, enum.Enum):
     component_slot = 'Component slot'
     real_sensor_missing = 'Real sensor'
     virtual_sensor_missing = 'Virtual sensor'
     real_or_virtual_sensor_missing = 'Real or virtual sensor'
     component_attrib = 'Component attribute problem'
-    fluid_missing = 'Fluid missing'
-    collector_missing = 'Collector missing'
-    collector_type = 'Wrong collector type'
-    collector_param = 'Invalid collector parameter'
     sensor_info = 'Sensor info problem'
     component_missing = 'Component missing'
     other_problem = 'Unspecified problem'
     unexpected_in_calc = 'Unexpected calculation error'
     unexpected_getting_problems = 'Unexpected error getting problem report'
 
 
 @dataclass
-class CoreProblem:
+class AlgoProblem:
     """A class used to hold information on a problem / missing info for a calculation / CoreStrategy.
     Can be used to track problems / missing information back to the root cause.
 
     Parameters
     ----------
     problem_type : ProblemType enum
-    affected_component : Plant, Array, Collector, optional
+    affected_component : Plant, Array, CollectorType, optional
         The component where some problem occurs / information is missing.
     affected_item_name : str, optional
         Typically the name of the affected sensor slot or attribute of the affected component.
     description : str, optional
     """
     problem_type: ProblemType
-    affected_component: Union[Plant, Array, Collector, None] = None
-    affected_item_name: str | None = None
-    description: str | None = None
-
-    # def __init__(self, problem_type, affected_component=None, affected_item_name=None, description=None):
-    #     # Defining an explicit init because affected_component got silently cast into the wrong serializable model.
-    #     self.problem_type = problem_type
-    #     self.affected_item_name = affected_item_name
-    #     self.description = description
-    #
-    #     if affected_component is None:
-    #         self.affected_component = None
-    #         return
-    #
-    #     if isinstance(affected_component, sunpeek.components.physical.Plant):
-    #         self.affected_component = Plant.from_orm(affected_component)
-    #     elif isinstance(affected_component, sunpeek.components.physical.Array):
-    #         self.affected_component = Array.from_orm(affected_component)
-    #     elif isinstance(affected_component, sunpeek.components.types.Collector):
-    #         self.affected_component = Collector.from_orm(affected_component)
-    #     else:
-    #         raise ValueError(f'Unexpected component: Expected ORM Plant, Array or Collector, '
-    #                          f'got {type(affected_component)}.')
+    affected_component: Union[Any, None] = None
+    affected_item_name: Union[str, None] = None
+    description: Union[str, None] = None
+
+    def __init__(self, problem_type, affected_component=None, affected_item_name=None, description=None):
+        # Defining an explicit init because affected_component got silently cast into the wrong serializable model.
+        self.problem_type = problem_type
+        self.affected_item_name = affected_item_name
+        self.description = description
+
+        if affected_component is None:
+            self.affected_component = None
+            return
+
+        if isinstance(affected_component, sunpeek.components.physical.Plant):
+            self.affected_component = Plant.from_orm(affected_component)
+        elif isinstance(affected_component, sunpeek.components.physical.Array):
+            self.affected_component = Array.from_orm(affected_component)
+        elif isinstance(affected_component, sunpeek.components.types.CollectorType):
+            self.affected_component = CollectorType.from_orm(affected_component)
+        else:
+            raise ValueError(f'Unexpected component: Expected ORM Plant, Array or CollectorType, '
+                             f'got {type(affected_component)}.')
 
 
 @dataclass
-class CoreMethodFeedback:
+class ProblemReport:
     """Standardized reporting of problems / missing information required to perform some calculation.
 
     This applies to all calculations in SunPeek, i.e. both virtual sensors and other calculations e.g. PC method.
-    Any CoreStrategy and CoreAlgorithm holds / can return a CoreMethodFeedback which holds structured information as to
+    Any CoreStrategy and CoreAlgorithm holds / can return a ProblemReport which holds structured information as to
     what problems / missing information there is that prevents the strategy / algo to complete.
 
-    CoreMethodFeedback implements an n-level tree, where each node (CoreMethodFeedback) has n leaves (own_feedback)
-    and points at m other nodes (sub_feedback). sub_feedback is implemented as dict with key == strategy name.
+    ProblemReport implements an n-level tree, where each node (ProblemReport) has n leaves (own_problems) and points
+    at m other nodes (sub_problems). sub_problems are implemented as dict with key == strategy name.
 
     Parameters
     ----------
     success : bool, optional, default True
         True if the algo or strategy holding / producing the problem report is successful, meaning that at least
         parts of its results can be calculated and / or only optional information is missing.
-    own_feedback : List[AlgoProblem], optional
+    own_problems : List[AlgoProblem], optional
         List of reported problems that affect the algo / strategy itself (as opposed to problems coming from called /
         sub algorithms). Example: Strategy needs some component attribute, but that attribute is None.
-    sub_feedback : Dict[str, CoreMethodFeedback], optional
-        Problems that are not directly associated to the algo / strategy holding this CoreMethodFeedback, but rather
-        stem from a previous calculation / strategy. Example: Strategy needs some virtual sensor, but that had its own
-        problems, reported as a CoreMethodFeedback.
-    virtuals_feedback : Dict[Tuple[Any, str], 'CoreMethodFeedback']
-        Problems arising from virtual sensors. These are kept separate from sub_feedback because the same virtual sensor
+    sub_reports : Dict[str, ProblemReport], optional
+        Problems that are not directly associated to the algo / strategy holding this ProblemReport, but rather stem
+        from a previous calculation / strategy. Example: Strategy needs some virtual sensor, but that had its own
+        problems, reported as a ProblemReport.
+    virtuals_reports : Dict[Tuple[Any, str], 'ProblemReport']
+        Problems arising from virtual sensors. These are kept separate from sub_reports because the same virtual sensor
         report might appear in several locations of the problem tree, but should only be parsed once.
     problem_slots : List[str], optional
         Set by virtual sensor strategies, problem_slots can be used to report partial success, i.e.:
         If a strategy is successful for some but not all virtual sensors, the success flag can be set to True,
-        and the CoreMethodFeedback applies only to the virtual sensor slot names which cannot be calculated,
+        and the ProblemReport applies only to the virtual sensor slot names which cannot be calculated,
         i.e. the problem_slots.
     """
-    success: bool | None = True
-    own_feedback: Union[List[CoreProblem], None] = None
-    sub_feedback: Union[Dict[str, 'CoreMethodFeedback'], None] = None
-    virtuals_feedback: Union[Dict[Tuple[Any, str], 'CoreMethodFeedback'], None] = None
+    success: Union[bool, None] = True
+    own_problems: Union[List[AlgoProblem], None] = None
+    sub_reports: Union[Dict[str, 'ProblemReport'], None] = None
+    virtuals_reports: Union[Dict[Tuple[Any, str], 'ProblemReport'], None] = None
     problem_slots: Union[List[str], None] = field(default_factory=list)  # Used if some virtual sensors / slots fail
 
     @property
-    def successful_strategy_str(self) -> str | None:
+    def successful_strategy_str(self) -> Union[str, None]:
         """Loop through strategies, return name of first successful strategy, or None if no strategy was successful.
         """
         if not self.success:
             return None
-        for strategy_name, feedback in self.sub_feedback.items():
-            if feedback.success:
+        for strategy_name, problem in self.sub_reports.items():
+            if problem.success:
                 return strategy_name
         return None
 
-    @staticmethod
-    def get_virtual_state(component, slot_name) -> IsVirtual:
-        try:
-            is_virtual = component.sensor_slots[slot_name].virtual
-        except KeyError:
-            raise AlgorithmError(f'Error adding AlgoProblem: '
-                                 f'Component slot {slot_name} not found in component {component}.')
-        return is_virtual
-
-    @staticmethod
-    def _cname(component: cmp.Component) -> str:
-        """Return verbose component name with class + name.
-        """
-        class_name = component.__class__.__name__.lower()
-        if isinstance(component, cmp.Plant):
-            return class_name
-        return f'{class_name} "{component.name}"'
-
-    def add_own(self, feedback: CoreProblem | List[CoreProblem]) -> None:
+    def add_own(self, algo_problems: Union[AlgoProblem, List[AlgoProblem]]) -> None:
         """Add "leaf" to problem tree: add 1 or more AlgoProblems to report.
         """
-        if feedback is None:
+        # lst = [] if self.own_problems is None else self.own_problems
+        if algo_problems is None:
             return
-        lst = self.own_feedback or []
-        if not isinstance(feedback, list):
-            feedback = [feedback]
-        lst.extend(feedback)
-        self.own_feedback = lst
+        lst = self.own_problems or []
+        if not isinstance(algo_problems, list):
+            algo_problems = [algo_problems]
+        lst.extend(algo_problems)
+        self.own_problems = lst
         self.success = False
 
-    def add_virtual(self, component: cmp.Component, slot_name: str, feedback: 'CoreMethodFeedback') -> None:
+    def add_virtual(self, component: cmp.Component, slot_name: str, problem_report: 'ProblemReport') -> None:
         """Add subtree of virtual sensor problems to `self.virtuals_reports`.
         """
-        self.virtuals_feedback = self.virtuals_feedback or {}
-        self.virtuals_feedback[(component, slot_name)] = feedback
-
-    def add_sub(self, strategy_name: str, feedback: 'CoreMethodFeedback') -> None:
-        """Add subtree to problem tree: Add 1 CoreMethodFeedback subtree.
-        """
-        self.sub_feedback = self.sub_feedback or {}
-        self.sub_feedback[strategy_name] = feedback
+        virtuals_dict = self.virtuals_reports or {}
+        virtuals_dict.update({(component, slot_name): problem_report})
+        self.virtuals_reports = virtuals_dict
+
+    def add_sub(self, strategy_name: str, problem_report: 'ProblemReport') -> None:
+        """Add subtree to problem tree: Add 1 ProblemReport subtree.
+        """
+        sub_dict = self.sub_reports or {}
+        sub_dict.update({strategy_name: problem_report})
+        self.sub_reports = sub_dict
         self.success = False
 
-    def add_missing_component(self, component: cmp.Component,
-                              missing_component_class_name: str,
-                              description: str) -> None:
-        """Add a "missing component" AlgoProblem as own problem.
-        """
-        algo_problem = CoreProblem(ProblemType.component_missing, component, missing_component_class_name, description)
-        self.add_own(algo_problem)
-
     def add_missing_sensor(self, component: cmp.Component,
                            slot_name: str,
                            check_mode: AlgoCheckMode,
-                           # enforce_real: bool = False
-                           ) -> None:
-        """Add a "missing sensor" AlgoProblem as own problem.
+                           enforce_real: bool = False) -> None:
+        """Add a "missing sensor" AlgoProblem as own problem. Set ProblemType & description depending on sensor / slot.
         """
-        is_virtual = self.get_virtual_state(component, slot_name)
-        if is_virtual is IsVirtual.never:
-            self.add_missing_real_sensor(component, slot_name)
-            return
-
-        if is_virtual is IsVirtual.always:
-            problem_type = ProblemType.virtual_sensor_missing
-            description = (f'"{component.sensor_slots[slot_name].descriptive_name}" '
-                           f'({slot_name}) in {self._cname(component)}: '
-                           f'Virtual sensor calculation failed.')
-
-        elif is_virtual is IsVirtual.possible:
-            problem_type = ProblemType.real_or_virtual_sensor_missing
-            description = (f'"{component.sensor_slots[slot_name].descriptive_name}" ({slot_name}) '
-                           f'in {self._cname(component)}: '
-                           f'Sensor missing or virtual sensor calculation failed.')
-        else:
-            raise ValueError(f'Unexpected IsVirtual value of slot {slot_name}: "{is_virtual}". '
-                             f'Expected one of {", ".join(list(IsVirtual))}')
-
-        self.add_own(CoreProblem(problem_type, component, slot_name, description))
+        try:
+            is_virtual = component.sensor_slots[slot_name].virtual
+        except KeyError:
+            raise AlgorithmError(f'Error trying to add AlgoProblem: '
+                                 f'Component slot {slot_name} not found in component {component}.')
+        p_types = {IsVirtual.always: ProblemType.virtual_sensor_missing,
+                   IsVirtual.possible: ProblemType.real_or_virtual_sensor_missing,
+                   IsVirtual.never: ProblemType.real_sensor_missing}
+        descriptions = {IsVirtual.always: 'Virtual sensor is all-NaN',
+                        IsVirtual.possible: 'Real sensor missing or virtual sensor is all-NaN',
+                        IsVirtual.never: 'Real sensor missing'}
+        if is_virtual not in p_types.keys():
+            raise ValueError(f'Unexpected value of IsVirtual enum: "{is_virtual}".')
+
+        # Under certain circumstances, a calling calculation might strictly require a _real_ sensor to exist.
+        # E.g. CoreStrategy with feedthrough_real_sensor==True, like StrategyPowerFromSensor
+        if enforce_real:
+            is_virtual = IsVirtual.never
+
+        problem_type = p_types[is_virtual]
+        description = descriptions[is_virtual]
+        self.add_own(AlgoProblem(problem_type, component, slot_name, description))
 
-        s = getattr(component, slot_name)
-        if s is None or not s.is_virtual:
+        # Specifically if component slot is virtual: Add subtree to problem tree
+        if slot_name not in component.sensors:
             return
+        s = component.sensors[slot_name]
 
-        # If virtual sensor for which calculation failed: Add subtree to problem tree
-        if check_mode is None:
-            raise AlgorithmError(f'Input "check_mode" required to treat a virtual sensor in problem reporting.')
-        add_vsensor = ((check_mode == AlgoCheckMode.config_and_data) or
-                       (check_mode == AlgoCheckMode.config_only and s._problems is not None))
-        if add_vsensor:
-            self.add_virtual(component, slot_name, s.problems)
-
-    def add_missing_real_sensor(self, component: cmp.Component,
-                                slot_name: str,
-                                # description: str = None,
-                                ) -> None:
-        """Add a "missing real sensor" AlgoProblem as own problem.
-        """
-        if IsVirtual.always == self.get_virtual_state(component, slot_name):
-            raise ValueError(f'Component slot {slot_name} in {component.name} can never be real. '
-                             f'This is an internal SunPeek error. Please report it.')
-
-        description = (f'"{component.sensor_slots[slot_name].descriptive_name}" '
-                       f'({slot_name}) in {self._cname(component)}: '
-                       f'Sensor missing.')
+        if s.is_virtual:
+            if check_mode is None:
+                raise AlgorithmError(f'Input "check_mode" required to treat a virtual sensor in problem reporting.')
+            add_vsensor = ((check_mode == AlgoCheckMode.config_and_data) or
+                           (check_mode == AlgoCheckMode.config_only and s._problems is not None))
+            if add_vsensor:
+                self.add_virtual(component, slot_name, s.problems)
 
-        self.add_own(CoreProblem(ProblemType.real_sensor_missing, component, slot_name, description))
-
-    def add_missing_sensor_info(self, component: cmp.Component, slot_name: str = None,
-                                info_name: str = None, description: str = None) -> None:
-        """Add a "missing fluid" AlgoProblem as own problem.
-        """
-        if description is None:
-            if info_name is None:
-                raise AlgorithmError(f'"info_name" required to generate missing sensor info description.')
-            description = (f'Sensor info "{info_name}" missing for sensor '
-                           f'"{component.sensors[slot_name].raw_name}" '
-                           f'({slot_name} in {self._cname(component)}). '
-                           f'This can be fixed on the Sensor Details page.')
-        algo_problem = CoreProblem(ProblemType.sensor_info, component, slot_name, description)
-        self.add_own(algo_problem)
-
-    def add_missing_attrib(self, component: Union[cmp.Component, cmp.Collector],
+    def add_missing_attrib(self, component: Union[cmp.Component, cmp.CollectorType],
                            attrib_name: str, description: str = None) -> None:
         """Add a "missing attribute" AlgoProblem as own problem.
         """
-        description = (f'Missing information "{attrib_name}" in {self._cname(component)}. '
-                       f'{"" if description is None else description}')
-        algo_problem = CoreProblem(ProblemType.component_slot, component, attrib_name, description)
-        self.add_own(algo_problem)
-
-    def add_zero_collector_param(self, component: cmp.Collector,
-                                 attrib_name: str) -> None:
-        """Add a "component_attrib" AlgoProblem as own problem, for collector parameters that should be nonzero.
-        """
-        description = (f'Collector parameter "{attrib_name}" is None or zero but is required to be nonzero.'
-                       f'in {self._cname(component)}.')
-        algo_problem = CoreProblem(ProblemType.collector_param, component, attrib_name, description)
-        self.add_own(algo_problem)
-
-    def add_nonzero_collector_param(self, component: cmp.Collector,
-                                    attrib_name: str) -> None:
-        """Add a "component_attrib" AlgoProblem as own problem, for collector parameters that should be zero.
-        """
-        description = (f'Collector parameter "{attrib_name}" is nonzero but is required to be zero, '
-                       f'in {self._cname(component)}.')
-        algo_problem = CoreProblem(ProblemType.collector_param, component, attrib_name, description)
+        algo_problem = AlgoProblem(ProblemType.component_slot, component, attrib_name, description)
         self.add_own(algo_problem)
 
     def add_missing_collector(self, component: cmp.Component, slot_name: str) -> None:
-        """Add a "missing collector" AlgoProblem as own problem.
+        """Add a "missing fluid" AlgoProblem as own problem. Set ProblemType & description.
         """
-        description = (f'"{slot_name}" in {self._cname(component)}: '
-                       f'Collector is missing (None) or invalid (UninitialisedCollector). '
-                       f'In case you defined a collector, this is an internal SunPeek error. Please report it.')
-        algo_problem = CoreProblem(ProblemType.component_missing, component, slot_name, description)
+        algo_problem = AlgoProblem(ProblemType.component_attrib, component, slot_name,
+                                   'Collector type is None or UninitialisedCollectorType')
         self.add_own(algo_problem)
 
-    def add_wrong_collector_type(self, component: cmp.Collector,
-                                 expected: cmp.CollectorTypes | List[cmp.CollectorTypes],
-                                 received: cmp.CollectorTypes) -> None:
-        """Add a "wrong collector type" AlgoProblem as own problem.
+    def add_missing_fluid(self, component: cmp.Component, slot_name: str) -> None:
+        """Add a "missing fluid" AlgoProblem as own problem. Set ProblemType & description.
         """
-        expected = expected if isinstance(expected, list) else [expected]
-        expected = [x.value if isinstance(x, enum.Enum) else x for x in expected]
-        received = received.value if isinstance(received, enum.Enum) else received
-        description = (f'Wrong collector type: '
-                       f'Expected a collector of type {" or ".join(expected)}, '
-                       f'but received "{received}".')
-        algo_problem = CoreProblem(ProblemType.collector_type, component, '', description)
+        algo_problem = AlgoProblem(ProblemType.component_attrib, component, slot_name,
+                                   'Fluid is None or UninitialisedFluid')
         self.add_own(algo_problem)
 
-    def add_missing_fluid(self, component: cmp.Component, slot_name: str) -> None:
-        """Add a "missing fluid" AlgoProblem as own problem.
+    def add_missing_sensor_info(self, component: cmp.Component, slot_name: str = None,
+                                info_name: str = None, description: str = None) -> None:
+        """Add a "missing fluid" AlgoProblem as own problem. Set ProblemType & description.
         """
-        description = (f'"{slot_name}" in {self._cname(component)}: '
-                       f'Fluid is missing (None) or invalid (UninitialisedFluid). '
-                       f'In case you defined a fluid, this is an internal SunPeek error. Please report it.')
-        algo_problem = CoreProblem(ProblemType.fluid_missing, component, slot_name, description)
+        if description is None:
+            if info_name is None:
+                raise AlgorithmError(f'"info_name" required to generate missing sensor info description.')
+            description = f'Sensor info "{info_name}" not found.'
+        algo_problem = AlgoProblem(ProblemType.sensor_info, component, slot_name, description)
         self.add_own(algo_problem)
 
-    def add_generic_slot_problem(self, component: cmp.Component, description: str) -> None:
+    def add_generic_slot_problem(self, component: cmp.Component, description: str = None) -> None:
         """Add a generic ProblemType.component_slot AlgoProblem as own problem
         """
-        self.add_own(CoreProblem(ProblemType.component_slot, component, description=description))
-
-    def parse(self,
-              include_successful_strategies: bool = False,
-              include_problem_slots: bool = True,
-              ) -> str:
-        """Parse CoreMethodFeedback into single string. Includes virtual sensors as sub-report.
-        """
-        main_report = self.to_tree(include_successful_strategies, include_problem_slots).parse()
-        virtuals_root = self.virtuals_to_tree(include_successful_strategies)
-        virtuals = '' if virtuals_root.is_leaf else f'\nVirtual Sensors:\n{virtuals_root.parse(node_whitespace=False)}'
+        self.add_own(AlgoProblem(ProblemType.component_slot, component, description=description))
 
-        return main_report + virtuals
+    def parse_with_virtuals(self, include_successful_strategies: bool = False,
+                            include_problem_slots: bool = True) -> str:
+        s_report = self.parse(include_successful_strategies, include_problem_slots)
+        s_virtuals = self.parse_virtuals()
+        s_virtuals = '\n' + s_virtuals if s_virtuals else ''
+        return s_report + s_virtuals
+
+    def parse(self, include_successful_strategies: bool = False, include_problem_slots: bool = True) -> str:
+        """Recursively parse a problem report into a string.
+        Virtual sensor problems (self.virtuals_report) are handled separately in `self.parse_virtuals`.
+        """
+
+        def parse_problem(algo_problem: AlgoProblem) -> str:
+            if algo_problem is None:
+                return ''
+            sensor_problems = [ProblemType.real_sensor_missing,
+                               ProblemType.virtual_sensor_missing,
+                               ProblemType.real_or_virtual_sensor_missing]
+            if algo_problem.problem_type in sensor_problems:
+                result = '\n- '
+            else:
+                result = f'\n- {algo_problem.problem_type.value}: '
+            c = algo_problem.affected_component
+            if algo_problem.description is not None:
+                result += f'{algo_problem.description}: '
+            if algo_problem.affected_item_name is not None:
+                if c is not None:
+                    result += f'"{c.__class__.__name__.lower()}.'
+                else:
+                    result += '"'
+                result += f'{algo_problem.affected_item_name}" '
+            if algo_problem.affected_component is not None:
+                result += f'in {c.__class__.__name__.lower()} "{c.name}"'
+            else:
+                result += '.'
+            return result
 
-    def to_tree(self, include_successful_strategies: bool, include_problem_slots: bool = True) -> 'TreeNode':
-        """Return the root node representing the CoreMethodFeedback as an n-tree. Virtual sensor problems are left out.
-        """
-        root_node = TreeNode()
-        # Add own problems as children leaves
-        if self.own_feedback is not None:
-            for algo_problem in self.own_feedback:
-                root_node.add(TreeNode(algo_problem.description))
-        # Add sub_feedback as children nodes
-        if self.sub_feedback is not None:
-            for k, v in self.sub_feedback.items():
-                include_slots = v.problem_slots and include_problem_slots
-                skip = v.success and not include_successful_strategies and not include_slots
+        def parse_sub_reports(reports: Dict[str, 'ProblemReport']) -> str:
+            # Recursive walk through the problem reports of all strategies (strategy name is dict key).
+            result = ''
+            for strategy_name, problem_report in reports.items():
+                include_slots = self.problem_slots and include_problem_slots
+                skip = problem_report.success and not include_successful_strategies and not include_slots
                 if skip:
                     continue
-                if not v.success:
-                    root_node.add(TreeNode(k, v.to_tree(include_successful_strategies, include_problem_slots).children))
-                    continue
-                if not v.problem_slots:
-                    root_node.add(TreeNode(k, [TreeNode('No problems found.')]))
-                else:  # partial success, some virtual sensors missing
-                    message = f'Some virtual sensors could not be calculated: {", ".join(self.problem_slots)}. {k}'
-                    root_node.add(
-                        TreeNode(message, v.to_tree(include_successful_strategies, include_problem_slots).children))
-
-        return root_node
-
-    def virtuals_to_tree(self, include_successful_strategies: bool) -> 'TreeNode':
-        """Return the root node representing the virtual sensor ProblemReports as an n-tree.
-        """
-        virtuals_feedback = self.collect_virtuals_feedback()
-        root_node = TreeNode()
-        if not virtuals_feedback:
-            return root_node
-
-        for k, v in virtuals_feedback.items():
+                if not problem_report.success:
+                    sub = problem_report.parse(include_successful_strategies, include_problem_slots)
+                else:
+                    if not self.problem_slots:
+                        sub = ' No problems found.'
+                    else:  # partial success, some virtual sensors missing
+                        sub = f'Some virtual sensors could not be calculated: {", ".join(self.problem_slots)}. '
+                        # if not problem_report.no_problems:
+                        sub += problem_report.parse(include_successful_strategies, include_problem_slots)
+                result += '\n' if result else ''
+                result += f'Strategy "{strategy_name}":'
+                result += sub
+            return result
+
+        result = ''
+        if self.own_problems is not None:
+            for algo_problem in self.own_problems:
+                result += parse_problem(algo_problem)
+
+        if self.sub_reports is not None:
+            result += parse_sub_reports(self.sub_reports)
+
+        return result
+
+    def collect_virtual_reports(self) -> Dict[Tuple[Any, str], 'ProblemReport']:
+        """Recursively collect all virtual sensor problems in ProblemReport tree.
+        """
+        v_reports = {}
+        if self.virtuals_reports is not None:
+            v_reports.update(self.virtuals_reports)
+        if self.sub_reports is not None:
+            for sub_report in self.sub_reports.values():
+                v_reports.update(sub_report.collect_virtual_reports())
+
+        return v_reports
+
+    def parse_virtuals(self) -> str:
+        indent = '  '
+        result = ''
+        for k, v in self.collect_virtual_reports().items():
             component, slot = k
-            message = (f'"{component.sensor_slots[slot].descriptive_name}" '
-                       f'({slot}) in {self._cname(component)}: '
-                       f'Virtual sensor calculation failed. Details:')
-            root_node.add(TreeNode(message, v.to_tree(include_successful_strategies).children))
-
-        return root_node
-
-    def collect_virtuals_feedback(self) -> Dict[Tuple[Any, str], 'CoreMethodFeedback']:
-        """Recursively collect all virtual sensor feedback in CoreMethodFeedback, avoiding duplicate entries.
-        """
-        v_feedback = {}
-        # Collect own virtual sensor report
-        if self.virtuals_feedback is not None:
-            v_feedback.update(self.virtuals_feedback)
-        # Collect virtual sensor report in sub-strategies
-        if self.sub_feedback is not None:
-            for sub_report in self.sub_feedback.values():
-                v_feedback.update(sub_report.collect_virtuals_feedback())
-
-        return v_feedback
-
+            result += (f'\n- Virtual sensor "{component.__class__.__name__.lower()}.{slot}" '
+                       f'in {component.__class__.__name__.lower()} "{component.name}":\n')
+            result += indent + v.parse().replace('\n', '\n' + indent)
 
-class TreeNode:
-    """n-tree, consisting of structural information (nodes, leaves) and string messages.
-    """
-
-    def __init__(self, message: str = '', children: List['TreeNode'] = None):
-        self.message = message
-        self.children = children or []
-
-    def add(self, child: 'TreeNode'):
-        self.children.append(child)
-
-    @property
-    def is_leaf(self) -> bool:
-        return not bool(self.children)
-
-    def parse(self, level: int = -1, node_whitespace: bool = True) -> str:
-        """Return string representation of the tree.
-        """
-        output = ''
-        indentation = '  ' * level
-        if self.message:
-            bullet = '-' if self.is_leaf else ">"
-            newline = f'\n' if node_whitespace and not self.is_leaf else ''
-            output += f'{newline}{indentation}{bullet} {self.message}\n'
-        for child in self.children:
-            output += child.parse(level + 1, node_whitespace=node_whitespace)
-        return output
+        return ('Virtual sensors:' + result) if result else ''
 
 
 # Goal = Report success / problems of a specific PC method strategy.
 @dataclass
-class PCMethodFeedback:
+class PCMethodProblem:
     evaluation_mode: str
-    formula: int
+    equation: int
     wind_used: bool
     success: bool
     problem_str: str
```

## Comparing `sunpeek/components/outputs_pc_method.py` & `sunpeek/components/results.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,9 @@
 from sqlalchemy.orm import relationship
-from sqlalchemy import Column, String, Float, Integer, Identity, ForeignKey, DateTime, Enum, Interval, Boolean, ARRAY, \
-    JSON
+from sqlalchemy import Column, String, Float, Integer, Identity, ForeignKey, DateTime, Enum, Interval, Boolean, ARRAY, JSON
 from sunpeek.components.helpers import ORMBase, ComponentParam, AttrSetterMixin
 from sunpeek.components import Fluid
 
 
 class PCMethodOutput(ORMBase, AttrSetterMixin):
     __tablename__ = 'pc_method_outputs'
 
@@ -15,15 +14,15 @@
 
     datetime_eval_start = Column(DateTime(timezone=True))
     datetime_eval_end = Column(DateTime(timezone=True))
 
     # Algorithm / Strategy
     pc_method_name = Column(String)  # The version of the PC method used to reflect the plant hydraulic layout
     evaluation_mode = Column(Enum('ISO', 'extended', name='pc_evaluation_modes'))
-    formula = Column(Integer)
+    equation = Column(Integer)
     wind_used = Column(Boolean)
 
     # PCSettings
     settings = Column(JSON)
 
     # Plant results
     plant_output = relationship("PCMethodOutputPlant", uselist=False)
@@ -38,15 +37,14 @@
     id = Column(Integer, Identity(0), primary_key=True)
     parent_result_id = Column(Integer, ForeignKey('pc_method_outputs.id', ondelete="CASCADE"))
 
     plant_id = Column(Integer, ForeignKey('plant.id'))
     plant = relationship("Plant")
 
     n_intervals = Column(Integer)
-    total_interval_length = Column(Interval)
 
     datetime_intervals_start = Column(JSON)
     datetime_intervals_end = Column(JSON)
 
     tp_measured = ComponentParam('W', param_type='array')
     tp_sp_measured = ComponentParam('W m**-2', param_type='array')
     tp_sp_estimated = ComponentParam('W m**-2', param_type='array')
@@ -70,38 +68,13 @@
 
     id = Column(Integer, Identity(0), primary_key=True)
     parent_result_id = Column(Integer, ForeignKey('pc_method_outputs.id', ondelete="CASCADE"))
 
     array_id = Column(ForeignKey('arrays.id'))
     array = relationship("Array")
 
-    # Input data used in calculation
-    data_id = Column(Integer, ForeignKey('pc_results_data.id'))
-    data = relationship("PCMethodOutputData", uselist=False, foreign_keys=[data_id])
-
     tp_sp_measured = ComponentParam('W m**-2', param_type='array')
     tp_sp_estimated = ComponentParam('W m**-2', param_type='array')
     tp_sp_estimated_safety = ComponentParam('W m**-2', param_type='array')
     mean_tp_sp_measured = ComponentParam('W m**-2')
     mean_tp_sp_estimated = ComponentParam('W m**-2')
     mean_tp_sp_estimated_safety = ComponentParam('W m**-2')
-
-
-class PCMethodOutputData(ORMBase, AttrSetterMixin):
-    __tablename__ = 'pc_results_data'
-
-    id = Column(Integer, Identity(0), primary_key=True)
-    parent_result_id = Column(Integer, ForeignKey('pc_results_arrays.id', ondelete="CASCADE"))
-
-    te_amb = ComponentParam('degC', param_type='array')
-    te_in = ComponentParam('degC', param_type='array')
-    te_out = ComponentParam('degC', param_type='array')
-    te_op = ComponentParam('degC', param_type='array')
-    te_op_deriv = ComponentParam('K hour**-1', param_type='array')
-
-    aoi = ComponentParam('deg', param_type='array')
-    iam_b = ComponentParam('dimensionless', param_type='array')
-    ve_wind = ComponentParam('m s**-1', param_type='array')
-
-    rd_gti = ComponentParam('W m**-2', param_type='array')
-    rd_bti = ComponentParam('W m**-2', param_type='array')
-    rd_dti = ComponentParam('W m**-2', param_type='array')
```

## Comparing `sunpeek/definitions/collectors.py` & `sunpeek/definitions/collector_types.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,532 +1,426 @@
-import datetime as dt
+from datetime import datetime
 from sunpeek.common.unit_uncertainty import Q
-from sunpeek.components.components_factories import Collector, CollectorSST, CollectorQDT, CollectorTypes
+from sunpeek.components.components_factories import CollectorType, CollectorTypeSST, CollectorTypeQDT
 import sunpeek.components.iam_methods as iam
 
 all_definitions = [
-    CollectorQDT(name="Arcon 3510",
-                 manufacturer_name="Arcon-Sunmark A/S",
-                 product_name="HTHEATstore 35/10",
-                 licence_number='SP SC0843-14',
-                 test_report_id="6P02267-C-Rev 1 (2016-07-06), 4P04266-C-Rev 2 (2015-11-10)",
-                 certificate_date_issued=dt.datetime(2016, 7, 14),
-                 certificate_lab='SP Technical Research Institute of Sweden',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 description="Cover: single-glazed & foil. Absorber: harp. Hydraulics: Non-Tichelmann",
-                 test_reference_area="gross",
-                 area_gr=Q(13.57, "m**2"),
-                 gross_width=Q(5973, "mm"),
-                 gross_length=Q(2272, "mm"),
-                 gross_height=Q(145, "mm"),
-                 a1=Q(2.067, "W m**-2 K**-1"),
-                 a2=Q(0.009, "W m**-2 K**-2"),
-                 a5=Q(7.313, "kJ m**-2 K**-1"),
-                 kd=Q(0.93, ""),
-                 eta0b=Q(0.745, ""),
-                 f_prime=Q(0.95, ""),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([1, 0.99, 0.97, 0.94, 0.9, 0.82, 0.65, 0.32, 0]))
-                 ),
-    CollectorQDT(name='powerSol 55',
-                 manufacturer_name="Gasokol GmbH",
-                 product_name="powerSol 55",
-                 licence_number='011-7S2928 F',
-                 test_report_id="21246087.001 (2019-05-24)",
-                 certificate_date_issued=dt.datetime(2019, 6, 4),
-                 certificate_lab="TV Rheinland Energy GmbH, Germany",
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area='gross',
-                 area_gr=Q(5.46, 'm**2'),
-                 area_ap=Q(5.04, 'm**2'),
-                 gross_width=Q(2522, "mm"),
-                 gross_length=Q(2166, "mm"),
-                 gross_height=Q(150, "mm"),
-                 a1=Q(2.34, "W m**-2 K**-1"),
-                 a2=Q(0.012, "W m**-2 K**-2"),
-                 a5=Q(13.489, "kJ m**-2 K**-1"),
-                 kd=Q(0.88, ""),
-                 eta0b=Q(0.741, ""),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([1, 0.99, 0.97, 0.94, 0.89, 0.8, 0.61, 0.3, 0]))
-                 ),
-    CollectorQDT(name='powerSol 120',
-                 manufacturer_name="Gasokol GmbH",
-                 product_name="powerSol 120",
-                 licence_number='011-7S2928 F',
-                 test_report_id="21246087.001 (2019-05-24)",
-                 certificate_date_issued=dt.datetime(2019, 6, 4),
-                 certificate_lab="TV Rheinland Energy GmbH, Germany",
-                 collector_type=CollectorTypes.flat_plate.value,
-                 area_gr=Q(12.0, 'm**2'),
-                 area_ap=Q(11.0, 'm**2'),
-                 test_reference_area='gross',
-                 gross_width=Q(5770, "mm"),
-                 gross_length=Q(2080, "mm"),
-                 gross_height=Q(150, "mm"),
-                 a1=Q(2.34, "W m**-2 K**-1"),
-                 a2=Q(0.012, "W m**-2 K**-2"),
-                 a5=Q(13.489, "kJ m**-2 K**-1"),
-                 kd=Q(0.88, ""),
-                 eta0b=Q(0.741, ""),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([1, 0.99, 0.97, 0.94, 0.89, 0.8, 0.61, 0.3, 0]))
-                 ),
-    CollectorQDT(name='powerSol 136',
-                 manufacturer_name="Gasokol GmbH",
-                 product_name="powerSol 136",
-                 licence_number='011-7S2928 F',
-                 test_report_id="21246087.001 (2019-05-24)",
-                 certificate_date_issued=dt.datetime(2019, 6, 4),
-                 test_reference_area='gross',
-                 certificate_lab="TV Rheinland Energy GmbH, Germany",
-                 collector_type=CollectorTypes.flat_plate.value,
-                 area_gr=Q(13.59, 'm**2'),
-                 area_ap=Q(12.50, 'm**2'),
-                 gross_width=Q(6275, "mm"),
-                 gross_length=Q(2166, "mm"),
-                 gross_height=Q(150, "mm"),
-                 a1=Q(2.34, "W m**-2 K**-1"),
-                 a2=Q(0.012, "W m**-2 K**-2"),
-                 a5=Q(13.489, "kJ m**-2 K**-1"),
-                 kd=Q(0.88, ""),
-                 eta0b=Q(0.741, ""),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([1, 0.99, 0.97, 0.94, 0.89, 0.8, 0.61, 0.3, 0]))
-                 ),
-    CollectorQDT(name='Greenonetec 3803',
-                 manufacturer_name="GREENoneTEC Solarindustrie GmbH",
-                 product_name='GK 3803',
-                 licence_number='011-7S2565 F',
-                 test_report_id='15COL1247 (2015-08-28), 15COL1247Q (2015-08-28)',
-                 certificate_date_issued=dt.datetime(2015, 8, 28),
-                 certificate_lab='TZS, ITW University Stuttgart, Germany',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area="aperture",
-                 area_gr=Q(7.91, 'm**2'),
-                 area_ap=Q(7.41, 'm**2'),
-                 gross_width=Q(3557, "mm"),
-                 gross_length=Q(2224, "mm"),
-                 gross_height=Q(135, "mm"),
-                 a1=Q(2.102, "W m**-2 K**-1"),
-                 a2=Q(0.016, "W m**-2 K**-2"),
-                 a5=Q(9.664, "kJ m**-2 K**-1"),
-                 kd=Q(0.931, ""),
-                 eta0b=Q(0.814, ""),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([1, 0.99, 0.98, 0.96, 0.91, 0.82, 0.53, 0.27, 0]))
-                 ),
-    CollectorQDT(name='Greenonetec 3803S',
-                 manufacturer_name="GREENoneTEC Solarindustrie GmbH",
-                 product_name='GK 3803S',
-                 licence_number='011-7S2566 F',
-                 test_report_id='15COL1257 (2015-08-28), 15COL1257Q (2015-08-28)',
-                 certificate_date_issued=dt.datetime(2015, 8, 28),
-                 certificate_lab='TZS, ITW University Stuttgart, Germany',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area="aperture",
-                 area_ap=Q(7.41, 'm**2'),
-                 area_gr=Q(7.91, 'm**2'),
-                 gross_width=Q(3557, "mm"),
-                 gross_length=Q(2224, "mm"),
-                 gross_height=Q(135, "mm"),
-                 a1=Q(3.083, "W m**-2 K**-1"),
-                 a2=Q(0.013, "W m**-2 K**-2"),
-                 a5=Q(9.985, "kJ m**-2 K**-1"),
-                 kd=Q(0.918, ""),
-                 eta0b=Q(0.857, ""),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([1, 0.99, 0.97, 0.95, 0.91, 0.83, 0.68, 0.21, 0]))
-                 ),
-    CollectorQDT(name='Greenonetec 3133',
-                 manufacturer_name="GREENoneTEC Solarindustrie GmbH",
-                 product_name='GK 3133',
-                 licence_number='011-7S2565 F',
-                 test_report_id='15COL1247 (2015-08-28), 15COL1247Q (2015-08-28)',
-                 certificate_date_issued=dt.datetime(2015, 8, 28),
-                 certificate_lab='TZS, ITW University Stuttgart, Germany',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area="aperture",
-                 area_gr=Q(13.17, 'm**2'),
-                 area_ap=Q(12.35, 'm**2'),
-                 gross_width=Q(5920, "mm"),
-                 gross_length=Q(2224, "mm"),
-                 gross_height=Q(135, "mm"),
-                 a1=Q(2.102, "W m**-2 K**-1"),
-                 a2=Q(0.016, "W m**-2 K**-2"),
-                 a5=Q(9.664, "kJ m**-2 K**-1"),
-                 kd=Q(0.931, ""),
-                 eta0b=Q(0.814, ""),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([1, 0.99, 0.98, 0.96, 0.91, 0.82, 0.53, 0.27, 0]))
-                 ),
-    CollectorQDT(name='Greenonetec 3133S',
-                 manufacturer_name="GREENoneTEC Solarindustrie GmbH",
-                 product_name='GK 3133S',
-                 licence_number='011-7S2566 F',
-                 test_report_id='15COL1257 (2015-08-28), 15COL1257Q (2015-08-28)',
-                 certificate_date_issued=dt.datetime(2015, 8, 28),
-                 certificate_lab='TZS, ITW University Stuttgart, Germany',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area="aperture",
-                 area_gr=Q(13.17, 'm**2'),
-                 area_ap=Q(12.35, 'm**2'),
-                 gross_width=Q(5920, "mm"),
-                 gross_length=Q(2224, "mm"),
-                 gross_height=Q(135, "mm"),
-                 a1=Q(3.083, "W m**-2 K**-1"),
-                 a2=Q(0.013, "W m**-2 K**-2"),
-                 a5=Q(9.985, "kJ m**-2 K**-1"),
-                 kd=Q(0.918, ""),
-                 eta0b=Q(0.857, ""),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([1, 0.99, 0.97, 0.95, 0.91, 0.83, 0.68, 0.21, 0]))
-                 ),
-    CollectorQDT(name='KBB K5-3Giga+',
-                 manufacturer_name="KBB Kollektorbau GmbH",
-                 product_name='KG-3Giga+',
-                 licence_number='011-7S2437 F',
-                 test_report_id='16COL1338 (2016-11-29), 16COL1338Q (2016-11-29)',
-                 certificate_date_issued=dt.datetime(2016, 12, 2),
-                 certificate_lab='TZS, ITW University Stuttgart, Germany',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area='gross',
-                 area_gr=Q(7.45, 'm**2'),
-                 gross_width=Q(3450, "mm"),
-                 gross_length=Q(2160, "mm"),
-                 gross_height=Q(119, "mm"),
-                 eta0b=Q(0.752, ''),
-                 a1=Q(2.416, 'W m**-2 K**-1'),
-                 a2=Q(0.008, 'W m**-2 K**-2'),
-                 a5=Q(8.3, "kJ m**-2 K**-1"),
-                 kd=Q(0.964, ''),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([1, 1, 0.99, 0.98, 0.94, 0.83, 0.56, 0.28, 0]))
-                 ),
-    CollectorQDT(name='KBB K5-4Giga+',
-                 manufacturer_name="KBB Kollektorbau GmbH",
-                 product_name='KG-4Giga+',
-                 licence_number='011-7S2437 F',
-                 test_report_id='16COL1338 (2016-11-29), 16COL1338Q (2016-11-29)',
-                 certificate_date_issued=dt.datetime(2016, 12, 2),
-                 certificate_lab='TZS, ITW University Stuttgart, Germany',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area="gross",
-                 area_gr=Q(9.94, 'm**2'),
-                 gross_width=Q(4600, "mm"),
-                 gross_length=Q(2160, "mm"),
-                 gross_height=Q(119, "mm"),
-                 eta0b=Q(0.752, ''),
-                 a1=Q(2.416, 'W m**-2 K**-1'),
-                 a2=Q(0.008, 'W m**-2 K**-2'),
-                 a5=Q(8.3, "kJ m**-2 K**-1"),
-                 kd=Q(0.964, ''),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([1, 1, 0.99, 0.98, 0.94, 0.83, 0.56, 0.28, 0]))
-                 ),
-    CollectorQDT(name='KBB K5Giga+',
-                 manufacturer_name="KBB Kollektorbau GmbH",
-                 product_name='K5Giga+',
-                 licence_number='011-7S2437 F',
-                 test_report_id='16COL1338 (2016-11-29), 16COL1338Q (2016-11-29)',
-                 certificate_date_issued=dt.datetime(2016, 12, 2),
-                 certificate_lab='TZS, ITW University Stuttgart, Germany',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area="gross",
-                 area_gr=Q(12.42, 'm**2'),
-                 gross_width=Q(5750, "mm"),
-                 gross_length=Q(2160, "mm"),
-                 gross_height=Q(119, "mm"),
-                 eta0b=Q(0.752, ''),
-                 a1=Q(2.416, 'W m**-2 K**-1'),
-                 a2=Q(0.008, 'W m**-2 K**-2'),
-                 a5=Q(8.3, "kJ m**-2 K**-1"),
-                 kd=Q(0.964, ''),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([1, 1, 0.99, 0.98, 0.94, 0.83, 0.56, 0.28, 0]))
-                 ),
-    CollectorQDT(name='koTech HT 16.7 Export',
-                 manufacturer_name="koTech Solarkollektoren GmbH",
-                 product_name='HT',
-                 licence_number='011-7S837 F',
-                 test_report_id='2.04.01120.1.0- QT, 2.04.01120.1.0- LT, 2.04.00667.1.0-2 - QT',
-                 certificate_date_issued=dt.datetime(2017, 2, 7),
-                 certificate_lab='TV Rheinland Energy GmbH, Germany',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area="gross",
-                 area_gr=Q(16.7, 'm**2'),
-                 gross_width=Q(7170, "mm"),
-                 gross_length=Q(2330, "mm"),
-                 gross_height=Q(187, "mm"),
-                 eta0b=Q(0.701, ''),
-                 a1=Q(2.582, 'W m**-2 K**-1'),
-                 a2=Q(0.005, 'W m**-2 K**-2'),
-                 a5=Q(14.7, "kJ m**-2 K**-1"),
-                 kd=Q(0.983, ''),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 90], 'deg'),
-                                                 iam_reference=Q([1, 0.99, 0.97, 0.93, 0.88, 0.78, 0.58, 0.0]))
-                 ),
-    CollectorQDT(name='koTech HT 4.2 Export',
-                 manufacturer_name="koTech Solarkollektoren GmbH",
-                 product_name='HT',
-                 licence_number='011-7S837 F',
-                 test_report_id='2.04.01120.1.0- QT, 2.04.01120.1.0- LT, 2.04.00667.1.0-2 - QT',
-                 certificate_date_issued=dt.datetime(2017, 2, 7),
-                 certificate_lab='TV Rheinland Energy GmbH, Germany',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area="gross",
-                 area_gr=Q(4.2, 'm**2'),
-                 gross_width=Q(2076, "mm"),
-                 gross_length=Q(2050, "mm"),
-                 gross_height=Q(187, "mm"),
-                 eta0b=Q(0.701, ''),
-                 a1=Q(2.582, 'W m**-2 K**-1'),
-                 a2=Q(0.005, 'W m**-2 K**-2'),
-                 a5=Q(14.7, "kJ m**-2 K**-1"),
-                 kd=Q(0.983, ''),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 90], 'deg'),
-                                                 iam_reference=Q([1, 0.99, 0.97, 0.93, 0.88, 0.78, 0.58, 0.0]))
-                 ),
-    CollectorQDT(name='ensol DIS 150',
-                 manufacturer_name="Energetyka Solarna ensol Sp. z.o.o.",
-                 product_name='DIS 150',
-                 licence_number='011-7S2978 F',
-                 test_report_id='21249150.001',
-                 certificate_date_issued=dt.datetime(2020, 7, 20),
-                 certificate_lab='TV Rheinland Energy GmbH, Germany',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area="gross",
-                 area_gr=Q(15.5, 'm**2'),
-                 gross_width=Q(6606, "mm"),
-                 gross_length=Q(2350, "mm"),
-                 gross_height=Q(173, "mm"),
-                 eta0b=Q(0.765, ''),
-                 a1=Q(2.23, 'W m**-2 K**-1'),
-                 a2=Q(0.008, 'W m**-2 K**-2'),
-                 a5=Q(6.483, "kJ m**-2 K**-1"),
-                 kd=Q(0.91, ''),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([1, 0.99, 0.98, 0.95, 0.91, 0.84, 0.70, 0.35, 0]))
-                 ),
-    CollectorQDT(name='Sunrain FPC1500C',
-                 manufacturer_name="Jiangsu Sunrain Solar Energy Co., Ltd",
-                 product_name='FPC1500C',
-                 licence_number='011-7S3004 F',
-                 test_report_id='201203085GZU-001',
-                 certificate_date_issued=dt.datetime(2020, 2, 25),
-                 certificate_lab='DIN CERTCO',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area="gross",
-                 area_gr=Q(15.02, 'm**2'),
-                 gross_width=Q(5960, "mm"),
-                 gross_length=Q(2520, "mm"),
-                 gross_height=Q(166, "mm"),
-                 eta0b=Q(0.852, ''),
-                 a1=Q(3.03, 'W m**-2 K**-1'),
-                 a2=Q(0.014, 'W m**-2 K**-2'),
-                 a5=Q(6.833, "kJ m**-2 K**-1"),
-                 kd=Q(0.92, ''),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([1, 1, 1, 0.99, 0.96, 0.9, 0.78, 0.52, 0.0]))
-                 ),
-    CollectorQDT(name='TVP MT-Power v4',
-                 manufacturer_name='TVP Solar SA',
-                 product_name='MT- Power v4',
-                 licence_number='011-7S1890F',
-                 test_report_id='16COL1343, 16COL1343Q',
-                 certificate_date_issued=dt.datetime(2017, 5, 24),
-                 certificate_lab='DIN CERTCO',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area='gross',
-                 area_gr=Q(1.96, 'm**2'),
-                 gross_length=Q(975, 'mm'),
-                 gross_width=Q(2015, 'mm'),
-                 gross_height=Q(51, 'mm'),
-                 eta0b=Q(0.737, ''),
-                 a1=Q(0.504, 'W m**-2 K**-1'),
-                 a2=Q(0.006, 'W m**-2 K**-2'),
-                 a5=Q(15.32, "kJ m**-2 K**-1"),
-                 kd=Q(0.957, ''),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([1, 1, 0.99, 0.98, 0.95, 0.88, 0.72, 0.36, 0.0]))
-                 ),
-    CollectorQDT(name='Absolicon T160',
-                 manufacturer_name='Absolicon Solar Collector AB',
-                 product_name='T160',
-                 licence_number='011-7S2902 C',
-                 test_report_id='C1730ISO',
-                 certificate_date_issued=dt.datetime(2019, 1, 15),
-                 certificate_lab='DIN CERTCO',
-                 collector_type=CollectorTypes.concentrating.value,
-                 test_reference_area='gross',
-                 area_gr=Q(6.04, 'm**2'),
-                 gross_length=Q(1095, 'mm'),
-                 gross_width=Q(5514, 'mm'),
-                 gross_height=Q(5.51, 'mm'),
-                 eta0b=Q(0.697, ''),
-                 a1=Q(0.73, 'W m**-2 K**-1'),
-                 a2=Q(0.000, 'W m**-2 K**-2'),
-                 a5=Q(1483, "J m**-2 K**-1"),
-                 kd=Q(0.12, ''),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([[1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0],
-                                                                  [0.99, 0.99, 0.98, 0.96, 0.91, 0.77, 0.53, 0.18, 0]]))
-                 ),
-
-    CollectorQDT(name='Ako Tec MEGA 26',
-                 manufacturer_name='Ako Tec Produktionsgesellschaft',
-                 product_name='MEGA 26',
-                 licence_number='011-7S2827 R',
-                 test_report_id='21242732.001rev1',
-                 certificate_date_issued=dt.datetime(2019, 2, 1),
-                 certificate_lab='TV Rheinland Energy GmbH',
-                 collector_type=CollectorTypes.concentrating.value,
-                 test_reference_area='gross',
-                 area_gr=Q(4.33, 'm**2'),
-                 gross_length=Q(1983, 'mm'),
-                 gross_width=Q(2184, 'mm'),
-                 eta0b=Q(0.483, ''),
-                 a1=Q(0.63, 'W m**-2 K**-1'),
-                 a2=Q(0.000, 'W m**-2 K**-2'),
-                 a5=Q(8136, "J m**-2 K**-1"),
-                 kd=Q(1.1, ''),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([[1.24, 1.25, 1.29, 1.08, 1.23, 1.18, 1.15, 0.97, 0],
-                                                                  [1.00, 1.00, 0.99, 0.98, 0.97, 0.94, 0.89, 0.44, 0]]))
-                 ),
-
-    CollectorQDT(name='Ako Tec MEGA 78',
-                 manufacturer_name='Ako Tec Produktionsgesellschaft',
-                 product_name='MEGA 78',
-                 licence_number='011-7S2827 R',
-                 test_report_id='21242732.001rev1',
-                 certificate_date_issued=dt.datetime(2019, 2, 1),
-                 certificate_lab='TV Rheinland Energy GmbH',
-                 collector_type=CollectorTypes.concentrating.value,
-                 test_reference_area='gross',
-                 area_gr=Q(12.99, 'm**2'),
-                 gross_length=Q(5950, 'mm'),
-                 gross_width=Q(2184, 'mm'),
-                 eta0b=Q(0.483, ''),
-                 a1=Q(0.63, 'W m**-2 K**-1'),
-                 a2=Q(0.000, 'W m**-2 K**-2'),
-                 a5=Q(8136, "J m**-2 K**-1"),
-                 kd=Q(1.1, ''),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([[1.24, 1.25, 1.29, 1.08, 1.23, 1.18, 1.15, 0.97, 0],
-                                                                  [1.00, 1.00, 0.99, 0.98, 0.97, 0.94, 0.89, 0.44, 0]]))
-                 ),
+    CollectorTypeQDT(name="Arcon 3510",
+                     manufacturer_name="Arcon-Sunmark A/S",
+                     product_name="HTHEATstore 35/10",
+                     licence_number='SP SC0843-14',
+                     test_report_id="6P02267-C-Rev 1 (2016-07-06), 4P04266-C-Rev 2 (2015-11-10)",
+                     certificate_date_issued=datetime(2016, 7, 14),
+                     certificate_lab='SP Technical Research Institute of Sweden',
+                     description="Cover: single-glazed & foil. Absorber: harp. Hydraulics: Non-Tichelmann",
+                     test_reference_area="gross",
+                     area_gr=Q(13.57, "m**2"),
+                     gross_width=Q(5973, "mm"),
+                     gross_length=Q(2272, "mm"),
+                     gross_height=Q(145, "mm"),
+                     a1=Q(2.067, "W m**-2 K**-1"),
+                     a2=Q(0.009, "W m**-2 K**-2"),
+                     a5=Q(7.313, "kJ m**-2 K**-1"),
+                     kd=Q(0.93, ""),
+                     eta0b=Q(0.745, ""),
+                     f_prime=Q(0.95, ""),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([1, 0.99, 0.97, 0.94, 0.9, 0.82, 0.65, 0.32, 0]))
+                     ),
+    CollectorTypeQDT(name='powerSol 55',
+                     manufacturer_name="Gasokol GmbH",
+                     product_name="powerSol 55",
+                     licence_number='011-7S2928 F',
+                     test_report_id="21246087.001 (2019-05-24)",
+                     certificate_date_issued=datetime(2019, 6, 4),
+                     certificate_lab="TV Rheinland Energy GmbH, Germany",
+                     test_reference_area='gross',
+                     area_gr=Q(5.46, 'm**2'),
+                     area_ap=Q(5.04, 'm**2'),
+                     gross_width=Q(2522, "mm"),
+                     gross_length=Q(2166, "mm"),
+                     gross_height=Q(150, "mm"),
+                     a1=Q(2.34, "W m**-2 K**-1"),
+                     a2=Q(0.012, "W m**-2 K**-2"),
+                     a5=Q(13.489, "kJ m**-2 K**-1"),
+                     kd=Q(0.88, ""),
+                     eta0b=Q(0.741, ""),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([1, 0.99, 0.97, 0.94, 0.89, 0.8, 0.61, 0.3, 0]))
+                     ),
+    CollectorTypeQDT(name='powerSol 120',
+                     manufacturer_name="Gasokol GmbH",
+                     product_name="powerSol 120",
+                     licence_number='011-7S2928 F',
+                     test_report_id="21246087.001 (2019-05-24)",
+                     certificate_date_issued=datetime(2019, 6, 4),
+                     certificate_lab="TV Rheinland Energy GmbH, Germany",
+                     area_gr=Q(12.0, 'm**2'),
+                     area_ap=Q(11.0, 'm**2'),
+                     test_reference_area='gross',
+                     gross_width=Q(5770, "mm"),
+                     gross_length=Q(2080, "mm"),
+                     gross_height=Q(150, "mm"),
+                     a1=Q(2.34, "W m**-2 K**-1"),
+                     a2=Q(0.012, "W m**-2 K**-2"),
+                     a5=Q(13.489, "kJ m**-2 K**-1"),
+                     kd=Q(0.88, ""),
+                     eta0b=Q(0.741, ""),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([1, 0.99, 0.97, 0.94, 0.89, 0.8, 0.61, 0.3, 0]))
+                     ),
+    CollectorTypeQDT(name='powerSol 136',
+                     manufacturer_name="Gasokol GmbH",
+                     product_name="powerSol 136",
+                     licence_number='011-7S2928 F',
+                     test_report_id="21246087.001 (2019-05-24)",
+                     certificate_date_issued=datetime(2019, 6, 4),
+                     test_reference_area='gross',
+                     certificate_lab="TV Rheinland Energy GmbH, Germany",
+                     area_gr=Q(13.59, 'm**2'),
+                     area_ap=Q(12.50, 'm**2'),
+                     gross_width=Q(6275, "mm"),
+                     gross_length=Q(2166, "mm"),
+                     gross_height=Q(150, "mm"),
+                     a1=Q(2.34, "W m**-2 K**-1"),
+                     a2=Q(0.012, "W m**-2 K**-2"),
+                     a5=Q(13.489, "kJ m**-2 K**-1"),
+                     kd=Q(0.88, ""),
+                     eta0b=Q(0.741, ""),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([1, 0.99, 0.97, 0.94, 0.89, 0.8, 0.61, 0.3, 0]))
+                     ),
+    CollectorTypeQDT(name='Greenonetec 3803',
+                     manufacturer_name="GREENoneTEC Solarindustrie GmbH",
+                     product_name='GK 3803',
+                     licence_number='011-7S2565 F',
+                     test_report_id='15COL1247 (2015-08-28), 15COL1247Q (2015-08-28)',
+                     certificate_date_issued=datetime(2015, 8, 28),
+                     certificate_lab='TZS, ITW University Stuttgart, Germany',
+                     test_reference_area="aperture",
+                     area_gr=Q(7.91, 'm**2'),
+                     area_ap=Q(7.41, 'm**2'),
+                     gross_width=Q(3557, "mm"),
+                     gross_length=Q(2224, "mm"),
+                     gross_height=Q(135, "mm"),
+                     a1=Q(2.102, "W m**-2 K**-1"),
+                     a2=Q(0.016, "W m**-2 K**-2"),
+                     a5=Q(9.664, "kJ m**-2 K**-1"),
+                     kd=Q(0.931, ""),
+                     eta0b=Q(0.814, ""),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([1, 0.99, 0.98, 0.96, 0.91, 0.82, 0.53, 0.27, 0]))
+                     ),
+    CollectorTypeQDT(name='Greenonetec 3803S',
+                     manufacturer_name="GREENoneTEC Solarindustrie GmbH",
+                     product_name='GK 3803S',
+                     licence_number='011-7S2566 F',
+                     test_report_id='15COL1257 (2015-08-28), 15COL1257Q (2015-08-28)',
+                     certificate_date_issued=datetime(2015, 8, 28),
+                     certificate_lab='TZS, ITW University Stuttgart, Germany',
+                     test_reference_area="aperture",
+                     area_ap=Q(7.41, 'm**2'),
+                     area_gr=Q(7.91, 'm**2'),
+                     gross_width=Q(3557, "mm"),
+                     gross_length=Q(2224, "mm"),
+                     gross_height=Q(135, "mm"),
+                     a1=Q(3.083, "W m**-2 K**-1"),
+                     a2=Q(0.013, "W m**-2 K**-2"),
+                     a5=Q(9.985, "kJ m**-2 K**-1"),
+                     kd=Q(0.918, ""),
+                     eta0b=Q(0.857, ""),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([1, 0.99, 0.97, 0.95, 0.91, 0.83, 0.68, 0.21, 0]))
+                     ),
+    CollectorTypeQDT(name='Greenonetec 3133',
+                     manufacturer_name="GREENoneTEC Solarindustrie GmbH",
+                     product_name='GK 3133',
+                     licence_number='011-7S2565 F',
+                     test_report_id='15COL1247 (2015-08-28), 15COL1247Q (2015-08-28)',
+                     certificate_date_issued=datetime(2015, 8, 28),
+                     certificate_lab='TZS, ITW University Stuttgart, Germany',
+                     test_reference_area="aperture",
+                     area_gr=Q(13.17, 'm**2'),
+                     area_ap=Q(12.35, 'm**2'),
+                     gross_width=Q(5920, "mm"),
+                     gross_length=Q(2224, "mm"),
+                     gross_height=Q(135, "mm"),
+                     a1=Q(2.102, "W m**-2 K**-1"),
+                     a2=Q(0.016, "W m**-2 K**-2"),
+                     a5=Q(9.664, "kJ m**-2 K**-1"),
+                     kd=Q(0.931, ""),
+                     eta0b=Q(0.814, ""),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([1, 0.99, 0.98, 0.96, 0.91, 0.82, 0.53, 0.27, 0]))
+                     ),
+    CollectorTypeQDT(name='Greenonetec 3133S',
+                     manufacturer_name="GREENoneTEC Solarindustrie GmbH",
+                     product_name='GK 3133S',
+                     licence_number='011-7S2566 F',
+                     test_report_id='15COL1257 (2015-08-28), 15COL1257Q (2015-08-28)',
+                     certificate_date_issued=datetime(2015, 8, 28),
+                     certificate_lab='TZS, ITW University Stuttgart, Germany',
+                     test_reference_area="aperture",
+                     area_gr=Q(13.17, 'm**2'),
+                     area_ap=Q(12.35, 'm**2'),
+                     gross_width=Q(5920, "mm"),
+                     gross_length=Q(2224, "mm"),
+                     gross_height=Q(135, "mm"),
+                     a1=Q(3.083, "W m**-2 K**-1"),
+                     a2=Q(0.013, "W m**-2 K**-2"),
+                     a5=Q(9.985, "kJ m**-2 K**-1"),
+                     kd=Q(0.918, ""),
+                     eta0b=Q(0.857, ""),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([1, 0.99, 0.97, 0.95, 0.91, 0.83, 0.68, 0.21, 0]))
+                     ),
+    CollectorTypeQDT(name='KBB K5-3Giga+',
+                     manufacturer_name="KBB Kollektorbau GmbH",
+                     product_name='KG-3Giga+',
+                     licence_number='011-7S2437 F',
+                     test_report_id='16COL1338 (2016-11-29), 16COL1338Q (2016-11-29)',
+                     certificate_date_issued=datetime(2016, 12, 2),
+                     certificate_lab='TZS, ITW University Stuttgart, Germany',
+                     test_reference_area='gross',
+                     area_gr=Q(7.45, 'm**2'),
+                     gross_width=Q(3450, "mm"),
+                     gross_length=Q(2160, "mm"),
+                     gross_height=Q(119, "mm"),
+                     eta0b=Q(0.752, ''),
+                     a1=Q(2.416, 'W m**-2 K**-1'),
+                     a2=Q(0.008, 'W m**-2 K**-2'),
+                     a5=Q(8.3, "kJ m**-2 K**-1"),
+                     kd=Q(0.964, ''),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([1, 1, 0.99, 0.98, 0.94, 0.83, 0.56, 0.28, 0]))
+                     ),
+    CollectorTypeQDT(name='KBB K5-4Giga+',
+                     manufacturer_name="KBB Kollektorbau GmbH",
+                     product_name='KG-4Giga+',
+                     licence_number='011-7S2437 F',
+                     test_report_id='16COL1338 (2016-11-29), 16COL1338Q (2016-11-29)',
+                     certificate_date_issued=datetime(2016, 12, 2),
+                     certificate_lab='TZS, ITW University Stuttgart, Germany',
+                     test_reference_area="gross",
+                     area_gr=Q(9.94, 'm**2'),
+                     gross_width=Q(4600, "mm"),
+                     gross_length=Q(2160, "mm"),
+                     gross_height=Q(119, "mm"),
+                     eta0b=Q(0.752, ''),
+                     a1=Q(2.416, 'W m**-2 K**-1'),
+                     a2=Q(0.008, 'W m**-2 K**-2'),
+                     a5=Q(8.3, "kJ m**-2 K**-1"),
+                     kd=Q(0.964, ''),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([1, 1, 0.99, 0.98, 0.94, 0.83, 0.56, 0.28, 0]))
+                     ),
+    CollectorTypeQDT(name='KBB K5Giga+',
+                     manufacturer_name="KBB Kollektorbau GmbH",
+                     product_name='K5Giga+',
+                     licence_number='011-7S2437 F',
+                     test_report_id='16COL1338 (2016-11-29), 16COL1338Q (2016-11-29)',
+                     certificate_date_issued=datetime(2016, 12, 2),
+                     certificate_lab='TZS, ITW University Stuttgart, Germany',
+                     test_reference_area="gross",
+                     area_gr=Q(12.42, 'm**2'),
+                     gross_width=Q(5750, "mm"),
+                     gross_length=Q(2160, "mm"),
+                     gross_height=Q(119, "mm"),
+                     eta0b=Q(0.752, ''),
+                     a1=Q(2.416, 'W m**-2 K**-1'),
+                     a2=Q(0.008, 'W m**-2 K**-2'),
+                     a5=Q(8.3, "kJ m**-2 K**-1"),
+                     kd=Q(0.964, ''),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([1, 1, 0.99, 0.98, 0.94, 0.83, 0.56, 0.28, 0]))
+                     ),
+    CollectorTypeQDT(name='koTech HT 16.7 Export',
+                     manufacturer_name="koTech Solarkollektoren GmbH",
+                     product_name='HT',
+                     licence_number='011-7S837 F',
+                     test_report_id='2.04.01120.1.0- QT, 2.04.01120.1.0- LT, 2.04.00667.1.0-2 - QT',
+                     certificate_date_issued=datetime(2017, 2, 7),
+                     certificate_lab='TV Rheinland Energy GmbH, Germany',
+                     test_reference_area="gross",
+                     area_gr=Q(16.7, 'm**2'),
+                     gross_width=Q(7170, "mm"),
+                     gross_length=Q(2330, "mm"),
+                     gross_height=Q(187, "mm"),
+                     eta0b=Q(0.701, ''),
+                     a1=Q(2.582, 'W m**-2 K**-1'),
+                     a2=Q(0.005, 'W m**-2 K**-2'),
+                     a5=Q(14.7, "kJ m**-2 K**-1"),
+                     kd=Q(0.983, ''),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 90], 'deg'),
+                                                     iam_reference=Q([1, 0.99, 0.97, 0.93, 0.88, 0.78, 0.58, 0.0]))
+                     ),
+    CollectorTypeQDT(name='koTech HT 4.2 Export',
+                     manufacturer_name="koTech Solarkollektoren GmbH",
+                     product_name='HT',
+                     licence_number='011-7S837 F',
+                     test_report_id='2.04.01120.1.0- QT, 2.04.01120.1.0- LT, 2.04.00667.1.0-2 - QT',
+                     certificate_date_issued=datetime(2017, 2, 7),
+                     certificate_lab='TV Rheinland Energy GmbH, Germany',
+                     test_reference_area="gross",
+                     area_gr=Q(4.2, 'm**2'),
+                     gross_width=Q(2076, "mm"),
+                     gross_length=Q(2050, "mm"),
+                     gross_height=Q(187, "mm"),
+                     eta0b=Q(0.701, ''),
+                     a1=Q(2.582, 'W m**-2 K**-1'),
+                     a2=Q(0.005, 'W m**-2 K**-2'),
+                     a5=Q(14.7, "kJ m**-2 K**-1"),
+                     kd=Q(0.983, ''),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 90], 'deg'),
+                                                     iam_reference=Q([1, 0.99, 0.97, 0.93, 0.88, 0.78, 0.58, 0.0]))
+                     ),
+    CollectorTypeQDT(name='ensol DIS 150',
+                     manufacturer_name="Energetyka Solarna ensol Sp. z.o.o.",
+                     product_name='DIS 150',
+                     licence_number='011-7S2978 F',
+                     test_report_id='21249150.001',
+                     certificate_date_issued=datetime(2020, 7, 20),
+                     certificate_lab='TV Rheinland Energy GmbH, Germany',
+                     test_reference_area="gross",
+                     area_gr=Q(15.5, 'm**2'),
+                     gross_width=Q(6606, "mm"),
+                     gross_length=Q(2350, "mm"),
+                     gross_height=Q(173, "mm"),
+                     eta0b=Q(0.765, ''),
+                     a1=Q(2.23, 'W m**-2 K**-1'),
+                     a2=Q(0.008, 'W m**-2 K**-2'),
+                     a5=Q(6.483, "kJ m**-2 K**-1"),
+                     kd=Q(0.91, ''),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([1, 0.99, 0.98, 0.95, 0.91, 0.84, 0.70, 0.35, 0]))
+                     ),
+    CollectorTypeQDT(name='Sunrain FPC1500C',
+                     manufacturer_name="Jiangsu Sunrain Solar Energy Co., Ltd",
+                     product_name='FPC1500C',
+                     licence_number='011-7S3004 F',
+                     test_report_id='201203085GZU-001',
+                     certificate_date_issued=datetime(2020, 2, 25),
+                     certificate_lab='DIN CERTCO',
+                     test_reference_area="gross",
+                     area_gr=Q(15.02, 'm**2'),
+                     gross_width=Q(5960, "mm"),
+                     gross_length=Q(2520, "mm"),
+                     gross_height=Q(166, "mm"),
+                     eta0b=Q(0.852, ''),
+                     a1=Q(3.03, 'W m**-2 K**-1'),
+                     a2=Q(0.014, 'W m**-2 K**-2'),
+                     a5=Q(6.833, "kJ m**-2 K**-1"),
+                     kd=Q(0.92, ''),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([1, 1, 1, 0.99, 0.96, 0.9, 0.78, 0.52, 0.0]))
+                     ),
+    CollectorTypeQDT(name='TVP MT-Power v4',
+                     manufacturer_name='TVP Solar SA',
+                     product_name='MT- Power v4',
+                     licence_number='011-7S1890F',
+                     test_report_id='16COL1343, 16COL1343Q',
+                     certificate_date_issued=datetime(2017,5,24),
+                     certificate_lab='DIN CERTCO',
+                     test_reference_area='gross',
+                     area_gr=Q(1.96, 'm**2'),
+                     gross_length=Q(975, 'mm'),
+                     gross_width=Q(2015, 'mm'),
+                     gross_height=Q(51, 'mm'),
+                     eta0b=Q(0.737, ''),
+                     a1=Q(0.504, 'W m**-2 K**-1'),
+                     a2=Q(0.006, 'W m**-2 K**-2'),
+                     a5=Q(15.32, "kJ m**-2 K**-1"),
+                     kd=Q(0.957, ''),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([1, 1, 0.99, 0.98, 0.95, 0.88, 0.72, 0.36, 0.0]))
+                     ),
 
     # Collectors tested with static test ----------------------------------------------------------------------------
-    # Overwritten by Collector below, because CollectorSST does not allow for an eta0b parameter.
-    # CollectorSST(name='Greenonetec HT 13,6',
+    # Overwritten by CollectorType below, because CollectorTypeSST does not allow for an eta0b parameter.
+    # CollectorTypeSST(name='Greenonetec HT 13,6',
     #                  manufacturer_name="GREENoneTEC Solarindustrie GmbH",
     #                  product_name='GK HT 13,6',
     #                  licence_number='011-7S2819 F',
     #                  test_report_id='20COLC1735ISO (2020-11-10), 20COLC1735ISOQ (2010-11-10), C1735L, '
     #                                 'C1781L (2018-09-06)',
-    #                  certificate_date_issued=dt.datetime(2020, 11, 10),
+    #                  certificate_date_issued=datetime(2020, 11, 10),
     #                  certificate_lab='TZS, ITW University Stuttgart, Germany',
     #                  test_reference_area="gross",
     #                  area_gr=Q(13.61, 'm**2'),
     #                  gross_width=Q(5970, "mm"),
     #                  gross_length=Q(2280, "mm"),
     #                  gross_height=Q(185, "mm"),
-    #                  eta0hem=Q(0.782),  # Misleading solar keymark data sheet: eta0hem or eta0b?
+    #                  eta0hem=Q(0.782),  # TODO Misleading solar keymark data sheet: eta0hem or eta0b?
     #                  a1=Q(2.27, "W m**-2 K**-1"),
     #                  a2=Q(0.018, "W m**-2 K**-2"),
     #                  ceff=Q(5980, "J m**-2 K**-1"),
     #                  kd=Q(0.92, ""),
     #                  iam_method=iam.IAM_K50(Q(0.97))
     #                  ),
-    Collector(name='Greenonetec HT 13,6',
-              manufacturer_name="GREENoneTEC Solarindustrie GmbH",
-              product_name='GK HT 13,6',
-              licence_number='011-7S2819 F',
-              test_report_id='20COLC1735ISO (2020-11-10), 20COLC1735ISOQ (2010-11-10), C1735L, '
-                             'C1781L (2018-09-06)',
-              certificate_date_issued=dt.datetime(2020, 11, 10),
-              certificate_lab='TZS, ITW University Stuttgart, Germany',
-              collector_type=CollectorTypes.flat_plate.value,
-              test_type='SST',
-              test_reference_area='gross',
-              area_gr=Q(13.61, 'm**2'),
-              gross_width=Q(5970, "mm"),
-              gross_length=Q(2280, "mm"),
-              gross_height=Q(185, "mm"),
-              eta0b=Q(0.782),
-              a1=Q(2.27, "W m**-2 K**-1"),
-              a2=Q(0.018, "W m**-2 K**-2"),
-              a5=Q(5980, "J m**-2 K**-1"),
-              kd=Q(0.92, ""),
-              iam_method=iam.IAM_K50(Q(0.97))
-              ),
-    CollectorSST(name="Savo SF500-15",
-                 manufacturer_name="Savo Solar Oyj",
-                 product_name="Savo SF500-15",
-                 licence_number='011-7S2688 F',
-                 test_report_id="C1704LPEN (2016-09-28), C1704QPEN (2016-09-28)",
-                 certificate_date_issued=dt.datetime(2016, 10, 7),
-                 certificate_lab='SPF, Switzerland',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area="gross",
-                 area_gr=Q(15.96, 'm**2'),
-                 gross_width=Q(6158, "mm"),
-                 gross_length=Q(2591, "mm"),
-                 gross_height=Q(213, "mm"),
-                 eta0hem=Q(0.812, ''),
-                 a1=Q(2.936, 'W m**-2 K**-1'),
-                 a2=Q(0.009, 'W m**-2 K**-2'),
-                 ceff=Q(10.2, "kJ m**-2 K**-1"),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([[1, 1, 1, 0.99, 0.97, 0.91, 0.75, 0.42, 0],
-                                                                  [1, 1, 1, 0.99, 0.98, 0.94, 0.84, 0.59, 0]]))
-                 ),
-    CollectorSST(name="Savo SF500-15DG",
-                 manufacturer_name="Savo Solar Oyj",
-                 product_name="Savo SF500-15DG",
-                 licence_number='011-7S2689 F',
-                 test_report_id="C1705LPEN (2016-12-15), C1705QPEN (2016-12-15)",
-                 certificate_date_issued=dt.datetime(2016, 12, 15),
-                 certificate_lab='SPF, Switzerland',
-                 collector_type=CollectorTypes.flat_plate.value,
-                 test_reference_area="gross",
-                 area_gr=Q(15.96, 'm**2'),
-                 gross_width=Q(6158, "mm"),
-                 gross_length=Q(2591, "mm"),
-                 gross_height=Q(213, "mm"),
-                 eta0hem=Q(0.793, ''),
-                 a1=Q(2.520, 'W m**-2 K**-1'),
-                 a2=Q(0.004, 'W m**-2 K**-2'),
-                 ceff=Q(12.0, "kJ m**-2 K**-1"),
-                 iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
-                                                 iam_reference=Q([[1, 1, 0.99, 0.98, 0.96, 0.87, 0.68, 0.38, 0],
-                                                                  [1, 1, 1.00, 0.99, 0.96, 0.91, 0.78, 0.53, 0]]))
-                 ),
-    CollectorSST(name="CONA CCS+",
-                 manufacturer_name="CONA Entwicklungs- u. Handelsges.m.b.H.",
-                 product_name="CCS+",
-                 licence_number='011-7S2471 L',
-                 test_report_id="ktb-2014-27",
-                 certificate_date_issued=dt.datetime(2015, 1, 14),
-                 certificate_lab='Fraunhofer ISE',
-                 test_reference_area="aperture",   # <-- aperture
-                 area_ap=Q(1.92, 'm**2'),
-                 area_gr=Q(2.05, 'm**2'),
-                 gross_length=Q(1012, "mm"),
-                 gross_width=Q(2024, "mm"),
-                 gross_height=Q(90, "mm"),
-                 eta0hem=Q(0.772, ''),  # <-- eta0hem, no eta0b
-                 a1=Q(11.247, 'W m**-2 K**-1'),
-                 a2=Q(0, 'W m**-2 K**-2'),
-                 a8=None,
-                 ceff=Q(16.2, "kJ m**-2 K**-1"),
-                 iam_method=iam.IAM_K50(k50=Q(0.89)),
-                 collector_type=CollectorTypes.flat_plate.value,
-                 ),
+    CollectorType(name='Greenonetec HT 13,6',
+                  manufacturer_name="GREENoneTEC Solarindustrie GmbH",
+                  product_name='GK HT 13,6',
+                  licence_number='011-7S2819 F',
+                  test_report_id='20COLC1735ISO (2020-11-10), 20COLC1735ISOQ (2010-11-10), C1735L, '
+                                 'C1781L (2018-09-06)',
+                  certificate_date_issued=datetime(2020, 11, 10),
+                  certificate_lab='TZS, ITW University Stuttgart, Germany',
+                  test_type='SST',
+                  test_reference_area='gross',
+                  area_gr=Q(13.61, 'm**2'),
+                  gross_width=Q(5970, "mm"),
+                  gross_length=Q(2280, "mm"),
+                  gross_height=Q(185, "mm"),
+                  eta0b=Q(0.782),
+                  a1=Q(2.27, "W m**-2 K**-1"),
+                  a2=Q(0.018, "W m**-2 K**-2"),
+                  a5=Q(5980, "J m**-2 K**-1"),
+                  kd=Q(0.92, ""),
+                  iam_method=iam.IAM_K50(Q(0.97))
+                  ),
+    CollectorTypeSST(name="Savo SF500-15",
+                     manufacturer_name="Savo Solar Oyj",
+                     product_name="Savo SF500-15",
+                     licence_number='011-7S2688 F',
+                     test_report_id="C1704LPEN (2016-09-28), C1704QPEN (2016-09-28)",
+                     certificate_date_issued=datetime(2016, 10, 7),
+                     certificate_lab='SPF, Switzerland',
+                     test_reference_area="gross",
+                     area_gr=Q(15.96, 'm**2'),
+                     gross_width=Q(6158, "mm"),
+                     gross_length=Q(2591, "mm"),
+                     gross_height=Q(213, "mm"),
+                     eta0hem=Q(0.812, ''),
+                     a1=Q(2.936, 'W m**-2 K**-1'),
+                     a2=Q(0.009, 'W m**-2 K**-2'),
+                     ceff=Q(10.2, "kJ m**-2 K**-1"),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([[1, 1, 1, 0.99, 0.97, 0.91, 0.75, 0.42, 0],
+                                                                      [1, 1, 1, 0.99, 0.98, 0.94, 0.84, 0.59, 0]]))
+                     ),
+    CollectorTypeSST(name="Savo SF500-15DG",
+                     manufacturer_name="Savo Solar Oyj",
+                     product_name="Savo SF500-15DG",
+                     licence_number='011-7S2689 F',
+                     test_report_id="C1705LPEN (2016-12-15), C1705QPEN (2016-12-15)",
+                     certificate_date_issued=datetime(2016, 12, 15),
+                     certificate_lab='SPF, Switzerland',
+                     test_reference_area="gross",
+                     area_gr=Q(15.96, 'm**2'),
+                     gross_width=Q(6158, "mm"),
+                     gross_length=Q(2591, "mm"),
+                     gross_height=Q(213, "mm"),
+                     eta0hem=Q(0.793, ''),
+                     a1=Q(2.520, 'W m**-2 K**-1'),
+                     a2=Q(0.004, 'W m**-2 K**-2'),
+                     ceff=Q(12.0, "kJ m**-2 K**-1"),
+                     iam_method=iam.IAM_Interpolated(aoi_reference=Q([10, 20, 30, 40, 50, 60, 70, 80, 90], 'deg'),
+                                                     iam_reference=Q([[1, 1, 0.99, 0.98, 0.96, 0.87, 0.68, 0.38, 0],
+                                                                      [1, 1, 1.00, 0.99, 0.96, 0.91, 0.78, 0.53, 0]]))
+                     )
 ]
 
 
 def get_definition(name):
     for coll_type in all_definitions:
         if coll_type.name == name:
             return coll_type
```

## Comparing `sunpeek-0.3.82.dist-info/COPYING.LESSER` & `sunpeek-0.3.9.dist-info/COPYING.LESSER`

 * *Files identical despite different names*

## Comparing `sunpeek-0.3.82.dist-info/METADATA` & `sunpeek-0.3.9.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: sunpeek
-Version: 0.3.82
+Version: 0.3.9
 Summary: Large Solar Thermal Monitoring Tool. Implements the Performance Check Method of ISO 24194
 Home-page: https://gitlab.com/sunpeek/sunpeek
 License: LGPL-3.0-only
 Keywords: solarthermal,solar,energy,monitoring
 Author: Philip Ohnewein, Daniel Tschopp, Lukas Feierl, Marnoch Hamilton-Jones, Jonathan Cazco
 Maintainer: Marnoch Hamilton-Jones
 Maintainer-email: m.hamilton-jones@aee.at
@@ -19,40 +19,37 @@
 Provides-Extra: api
 Provides-Extra: db
 Provides-Extra: demo
 Requires-Dist: alembic ; extra == "db" or extra == "all"
 Requires-Dist: coolprop (>=6.4,<6.5)
 Requires-Dist: fastapi (>=0.92) ; extra == "api" or extra == "all"
 Requires-Dist: httpx ; extra == "api" or extra == "all"
+Requires-Dist: kaleido (==0.2.1)
 Requires-Dist: lxml
-Requires-Dist: matplotlib (>=3.7)
 Requires-Dist: metpy
 Requires-Dist: numpy
 Requires-Dist: orjson
-Requires-Dist: pandas (>=2)
-Requires-Dist: parquet-datastore-utils (>=0.1.11)
-Requires-Dist: pendulum
+Requires-Dist: pandas (>=2,<3)
+Requires-Dist: parquet-datastore-utils
 Requires-Dist: pint (>=0.22)
 Requires-Dist: pint-pandas (>=0.2)
 Requires-Dist: protobuf
 Requires-Dist: psycopg2-binary ; extra == "db" or extra == "api" or extra == "all"
 Requires-Dist: pvlib
 Requires-Dist: pydantic (<2)
 Requires-Dist: pyephem
-Requires-Dist: pypdf (>=3.13.0,<4.0.0)
-Requires-Dist: pyperclip (>=1.8.2,<2.0.0)
 Requires-Dist: pyproj
 Requires-Dist: python-dotenv
 Requires-Dist: python-multipart ; extra == "api" or extra == "all"
 Requires-Dist: scikit-learn
 Requires-Dist: scipy
 Requires-Dist: sqlalchemy (>=1.4,<2.0)
 Requires-Dist: sqlalchemy-utils
 Requires-Dist: statsmodels
-Requires-Dist: sunpeek-exampledata (>=0.1.2) ; extra == "demo" or extra == "all"
+Requires-Dist: sunpeek-exampledata (>=0.1.0) ; extra == "demo" or extra == "all"
 Requires-Dist: times
 Requires-Dist: timezonefinder
 Requires-Dist: tomli (>=2.0.1,<3.0.0)
 Requires-Dist: trio
 Requires-Dist: uvicorn[standard] ; extra == "api" or extra == "all"
 Requires-Dist: yamlloader
 Project-URL: Documentation, https://docs.sunpeek.org
@@ -111,15 +108,15 @@
 
 # Running SunPeek
 ## Prerequisites
 * Ideally, a modern linux environment, although running on Windows and MacOS is also possible, with _at least_:
   * 5GB free disk space (this is needed for unpacking the application, after installation it will use around 2 GB)
   * A 7th Gen i5 or better processor.
   * 8GB RAM
-* Docker and Docker compose installed ([see below](#get-docker)), version 2.20 or newer. 
+* Docker and Docker compose (v2) installed ([see below](#get-docker))
 
 ## Get Docker
 In order to provide a consistent environment and allow SunPeek to work across a wide range of install environments, 
 it is provided as a set of Docker images (essentially, very lightweight virtual machines). A docker compose 
 configuration is also provided, if you are installing sunpeek on a single machine, this is probably what you want to 
 use, follow the links below for instructions depending on your environment.
 
@@ -139,19 +136,18 @@
 If you have previously set up SunPeek using the default configuration, you must first 
 remove all stored data by running the command `docker volume rm harvestit_hit_postgres_data` in a terminal/command 
 prompt, _this will also remove uploaded data._ You do not need to do this to update the software, see 
 [Upgrading to a new version of SunPeek](#upgrading-to-a-new-version-of-sunpeek)
 ```
 
 #### On Linux
-1. In the location you want to store sunpeek configuration, run 
-`curl https://gitlab.com/sunpeek/sunpeek/-/raw/main/deploy/quick-setup.sh?inline=false -o quick-setup.sh`,
-2. Run `quick-setup.sh` (usually just with the command `./quick-setup.sh`) and enter the url which sunpeek can be accessed
-at when prompted.
-3. Run `docker compose up -d`
+1. Use https://gitlab.com/api/v4/projects/43333900/repository/archive?path=deploy to download an archive of the 
+deployment files and unzip it to the location you want to run it from.
+2. Run `quick-setup.sh` and answer the prompts, this will create a database password and default configuration files from templates.
+3. In a terminal in the unzipped folder, run `docker compose up -d`
 4. After at most 2 minutes (usually a few seconds), the web UI should be accessible at http://localhost, or the url set 
 in step 2.
 
 #### On Windows
 1. Download [this file](https://gitlab.com/sunpeek/sunpeek/-/raw/main/sunpeek_easy_installer.zip?inline=false), and unzip
 it to a temporary location.
 2. Run `sunpeek_easy_installer.exe`
```

## Comparing `sunpeek-0.3.82.dist-info/COPYING` & `sunpeek-0.3.9.dist-info/COPYING`

 * *Files identical despite different names*

## Comparing `sunpeek-0.3.82.dist-info/RECORD` & `sunpeek-0.3.9.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,77 +1,74 @@
-sunpeek/__init__.py,sha256=_sD8F6QUD1ntSresA2jSHACz-hK66Wk1GNHNk8Rhth8,705
+sunpeek/__init__.py,sha256=3OVGGe7fF52quxTfpeGZ9WLlVZ9wc3BK7sWngF73KpM,1035
 sunpeek/api/__init__.py,sha256=6D8BHGZVV1B9mw90ii9DPj8aqgk7T2uGcMSJgpcVgrY,264
 sunpeek/api/dependencies.py,sha256=_JRAjCRDF3hV8Bfyq6E2MD8gYhojSekgJRMTka_-Qp8,581
-sunpeek/api/main.py,sha256=A-O8_uET7svYW3srQ9eVJyognyMM1Na0Prz5ptYYaQY,7981
+sunpeek/api/main.py,sha256=3hugUdeSc-PUVLN9ErPk98Gb8ddrkqXnZRBT4AoFV-U,7932
 sunpeek/api/routers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-sunpeek/api/routers/api_jobs.py,sha256=lsqmidG0KuWKvb1Ahs-bOtg8gmTJdpTI7qs1GmEBiT0,1712
-sunpeek/api/routers/config.py,sha256=k1JBKV83S3Of7ea1JaaTw1ePMrH2w2s1weogb4xCfAI,7924
-sunpeek/api/routers/evaluations.py,sha256=DIry-YF8kphVOO-yRM1alpA6z65e1RYPTi13bBGF0v4,11261
-sunpeek/api/routers/files.py,sha256=vtI15ETCTKXLSPWAIKtk0neLZ2LNnNzOxuAj_hXqzjU,9314
-sunpeek/api/routers/helper.py,sha256=Wv_sWKKPn4N3vz7QUdX2mLMGdM_PSPCx3BNGeWCbTjE,1402
-sunpeek/api/routers/plant.py,sha256=eQK04Vxah3b3ddP5q6IRJv0p2Kobq1F2eyBEaC7hz1w,25685
-sunpeek/base_model.py,sha256=VC4UrkHKllrZTrTW0oZBSQRxE4RxT3IJNvkZPMEss18,1456
+sunpeek/api/routers/api_jobs.py,sha256=ow_AdoC04SyibS7enKsD4Cq3aNRfJIY6tLdLEgxEuU4,1718
+sunpeek/api/routers/config.py,sha256=5QTn7dvwiYFnMvn30HWSe9YLDt0IMupTIiSlL5aRx0M,6312
+sunpeek/api/routers/evaluations.py,sha256=-eSj6hLG70L6Y42NbHqCQVgAuDih_dlvQ_2VV_IjCSA,6090
+sunpeek/api/routers/files.py,sha256=ZViDGFt1FqX7h_bm7_fK44oaM4AIo8ZXk6V28mPqKAg,6582
+sunpeek/api/routers/helper.py,sha256=VbxR96aCsbzuKS7mG3tLeTC8Bb8yPNnPK4n8ssBXkN4,245
+sunpeek/api/routers/plant.py,sha256=vBsWmZSGfx-f5QiCW5vg81j-FX5lmgDMGXoUG_T3Cc0,19263
+sunpeek/base_model.py,sha256=v3tIRStkycYr5uYReaQ7VDztTRFw0Js6j9hNtUDGUyc,780
 sunpeek/common/__init__.py,sha256=WVUQ1PQnwc5lHNsQm0dnkXsUfmgfC8qC9UEgpUKnE_E,100
-sunpeek/common/common_units.py,sha256=xJbWUeGxXFqbmFxiDL7TY-gLspxwatrvNHgZMFa75VY,1316
-sunpeek/common/config_parser.py,sha256=oLU4XARSGkwsxzR8Nfe-3dGvt_wroqzSyrIbYWsD2Gk,1843
-sunpeek/common/errors.py,sha256=Td3zvqvaU58-uCVaYilPfSgok0h2XFkLtaFBcRhYaHY,1893
-sunpeek/common/logos/Icon_Transparent.png,sha256=DWeCgqOCJPN1F-hk6pnEqHlFyI7OUKSP-PlHSo8YpPk,21050
-sunpeek/common/logos/Logo_wide.png,sha256=9VHzZt-Di5RL22Geer0ciKqbkbJZJ4TkkZk2y4Eq5Eg,23452
-sunpeek/common/plot_utils.py,sha256=ehqrKpTCYdSiOzt4Vh3Lfalbm3GgYeEvD6h-DuHh-xA,29201
-sunpeek/common/time_zone.py,sha256=YN79uTDrN6vY_NazA6KcDNBHnBx1FHXlPL2qO4NdN2k,6731
-sunpeek/common/unit_uncertainty.py,sha256=qW9fBtq5nxs4NCHqsRQ7yRx4l5Oo6KCom7prRsGtfmE,18481
-sunpeek/common/utils.py,sha256=7XLko9yDX2Ne7Knq_Yna3dp3yldjhyZRyW4rriiAC6o,6710
-sunpeek/components/__init__.py,sha256=3sbjTmEYXFzaAJl2pAMfVGhuwk5cnFQdfTfCV2W0xBU,985
-sunpeek/components/base.py,sha256=qEgnWvxHDCZoTe4uS0A9wxHx1p_um0DJlbh2jnl26z4,13537
-sunpeek/components/components_factories.py,sha256=kqKnFR3aGIigN3KSFN-0UCEDfTGbL5kSNDzL8riEjWY,3182
-sunpeek/components/fluids.py,sha256=G3dxtmwC6ZanV2StTWfk-1wsEBHLkBxxVFPhRuCyhxc,26722
-sunpeek/components/fluids_wpd_models.py,sha256=ZUfjbx6ElXBU-SRYXUuuCkUfmNaB90nJxzptwI4oppE,15692
-sunpeek/components/helpers.py,sha256=QHY0RrMuJvh1q24effGevxc1MwuYWiTp3rylntHm76A,16645
-sunpeek/components/iam_methods.py,sha256=1MXrFkC-5hOW_fMNo3p4h6pvR541RUePf17RKjKcNQA,15768
+sunpeek/common/common_units.py,sha256=DPykzkJEX8oGql7iw6ZK6kiI5rsoIHrR3KCXydqPbRc,1248
+sunpeek/common/config_parser.py,sha256=NfKIR2a9abhPxQI52UFLm_R1o9s_IDu1-mc39PI86no,1968
+sunpeek/common/errors.py,sha256=lASviLRJhtDEdro41Lj__Br4sYp5EG89u2iNVoRaXuk,1802
+sunpeek/common/time_zone.py,sha256=q73QtB4gBxuCDqnO6yxtWYUNXE2nm6ydFpmyL6Gy7iM,6639
+sunpeek/common/unit_uncertainty.py,sha256=355iPUVFtlFDASsezyT_58GUWXD8dIJ5PZPne1HJ20s,17863
+sunpeek/common/utils.py,sha256=nJzOZhtgm95C1DA3H53pjKm6OU-9Bx9LX2Dsy96m3Tw,5134
+sunpeek/components/__init__.py,sha256=ffw2cp0QyLk9YEzjPPI2igVO-YU7SzkbfOv0WsUGtD4,986
+sunpeek/components/base.py,sha256=7UC_Z1WaNMV-kts3UUpgkGx42JOURepzqg6BpXQXrEw,13697
+sunpeek/components/components_factories.py,sha256=nDQSyiCUw-3PQqm4T0ItRf6h9p25Q52XUCtooitQnK0,2800
+sunpeek/components/fluids.py,sha256=FpYwEOauBJJhrnxBnIVAQ4QGs8jvs8c9Jyjr3Tx56ig,26948
+sunpeek/components/fluids_wpd_models.py,sha256=dU685R5VBey42l1tlPqb82xiIEQwi4MbvSsxXL-TqFg,14800
+sunpeek/components/helpers.py,sha256=hKgEg1Uhn4tmM9ZOwlcQ9RqETWNNSPdy850LwogHCkw,14886
+sunpeek/components/iam_methods.py,sha256=qKV1RT4TpEK2qoEcJxMnd9i5Yu0Nf2gtfsMEAg-8V8g,15784
 sunpeek/components/jobs.py,sha256=iYU2KpDCFZJVcJkMISPctC8c5yAzECKleoqK5Pn0o9U,674
 sunpeek/components/operational_events.py,sha256=mKT8QamEfXYLIclwGJeXUqG44zozruGg560XHvM8rfU,3804
-sunpeek/components/outputs_pc_method.py,sha256=eH53ZquRJ7gj_1IwvkcHn6aSJ8_bItVAAmDs8IA_Qz8,4201
-sunpeek/components/physical.py,sha256=xaoGGetfFv8cwz0CmpTsUH-LNNQe5EC_LKCJOkT3qzk,49950
-sunpeek/components/sensor.py,sha256=nfCn5yv7i3WDRaGTuFoALhU3Q1S3hua5fE6TzrSMma0,25262
-sunpeek/components/sensor_types.py,sha256=Ja_s0YWAIUaKnf2RbHtBSH2xKv0oSga6Ybw5m3auPyk,11598
-sunpeek/components/types.py,sha256=7mc1RyiP4nX0-zYue1hbOnjWNbs_jHLH4fK30uzbgOg,24175
+sunpeek/components/physical.py,sha256=VWbUxUi8VMmdiKPuh8F-AYpNS2ViHJ3VYn8k7nfIPZo,49813
+sunpeek/components/results.py,sha256=_sNDJbwHgTl-f7f-tP7Rr9NgjGde_EwXgL-kTUSTo-g,3075
+sunpeek/components/sensor.py,sha256=rPdkDFbQYkWGlOuNNqDMhgdhKaHV8GYkuhI7moY6380,23308
+sunpeek/components/sensor_types.py,sha256=0pp_-vBJAbI6--WaMXXUnCMtrUBvNhHje8Xe9Na5Uu0,11480
+sunpeek/components/types.py,sha256=MZuAS7HhZDzr_aa8gctKUTlqpxBYt4o4oEFsDjDshx4,20317
 sunpeek/core_methods/__init__.py,sha256=Js-UR8fqFyHIkPCX_2XQX-rGLPRb-AzkElBEK57nz2w,87
 sunpeek/core_methods/common/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-sunpeek/core_methods/common/main.py,sha256=E8WlmUlwiAzU79ptoE4DjdtS8Rt_MUszZp_L9Biuhio,15831
-sunpeek/core_methods/pc_method/__init__.py,sha256=mnPG8yMwUFc1ozBeQ_BVxDs-ReAfdx0x_XkgVhZbGOg,4717
-sunpeek/core_methods/pc_method/formula.py,sha256=j7GsxFMooLwXfSiw19s6itHczhpWwb8qCm3EVYm2-bQ,20409
-sunpeek/core_methods/pc_method/main.py,sha256=v_qV0EkVsoFTxCiAayYYPAJ2k2LLM0UOpGb_61bWDgg,31722
-sunpeek/core_methods/pc_method/plotting.py,sha256=vey6o0dbc4YKT5dsbohGqulvaIQ_O7eqjwX2AnDHGSM,66092
-sunpeek/core_methods/pc_method/wrapper.py,sha256=jIqJQKte5zMQ9ynmDJrgTf3L5-c0PMVmEQhwLdEsOcg,8517
+sunpeek/core_methods/common/main.py,sha256=bDba3S60SBmHmyuuYMb5-Uw0WxIaoXp3Cr5QbX7AEao,13751
+sunpeek/core_methods/pc_method/__init__.py,sha256=ay7JztMWmxBEb2U-cuKdL7-UKcX6Xat3slqhpS27mGA,4570
+sunpeek/core_methods/pc_method/equation.py,sha256=Ygj68Lz47FkeUEYflEWtc31AFyNWdcczKMdupDy2_WM,14730
+sunpeek/core_methods/pc_method/main.py,sha256=Yhgu2bQTwU2sX7ZiYVNt1QLChe4fuDu7oSzqYxZddww,31591
+sunpeek/core_methods/pc_method/plotting.py,sha256=hln6QFu7ShcsK___zIwczjdPPoO3X7MmawlJHhmjqos,22617
+sunpeek/core_methods/pc_method/wrapper.py,sha256=nxgvDk3ML8_ciINrN4qh8-kUrSzUYZjtEzmi3TQVibs,10738
 sunpeek/core_methods/virtuals/__init__.py,sha256=Mabq_yA70c9I63nDY8A63jyw8PMShe3f6X4QzqRDoGg,83
-sunpeek/core_methods/virtuals/calculations.py,sha256=QXOkBGi3ZPN7Jp2PGqoltl_5G9iv1ryHyzcBUmm6SXA,39734
-sunpeek/core_methods/virtuals/main.py,sha256=RCHS1C4SMaB2ja0fkA7gmxEXn_zcSDrZaRnUBbHvb1U,4220
-sunpeek/core_methods/virtuals/radiation.py,sha256=B4sWQvgXpmEc0uUtp_tLpOEP0u39t3zvWN3KFY2CNBk,34383
-sunpeek/core_methods/virtuals/virtuals_array.py,sha256=sBo8G93vhtrkxwMMMa_w5m5vHu0-h9MEzlajRrQnzg8,3452
-sunpeek/core_methods/virtuals/virtuals_plant.py,sha256=FsWUnIvSprBaEtTDLGhA557AR38PQrzCivprxsAbzwc,3809
+sunpeek/core_methods/virtuals/calculations.py,sha256=jCc_3W4Uzu_B9op91-mtXNFmoyVjtUi4Zz_nk9B8rks,39360
+sunpeek/core_methods/virtuals/main.py,sha256=jaEHDbS0Hn_rWG4FpMySs-K0yWLCsNSMnAb_kNfdvWM,4202
+sunpeek/core_methods/virtuals/radiation.py,sha256=rzDZaA8pldmxuF1oZJ6N_eUtYud52XjglnENH2mA5jQ,35090
+sunpeek/core_methods/virtuals/virtuals_array.py,sha256=yvlsjC7hs8FZqRitEvY5YXxnusYBUFf43Gx-pZ1D1a0,3446
+sunpeek/core_methods/virtuals/virtuals_plant.py,sha256=SOCH8vMbcCuuZNovIC8Gtt-qLYCyS1T6B6w9okxgMOk,3773
 sunpeek/data_handling/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-sunpeek/data_handling/context.py,sha256=4EheVwDVCNeIWU00Rxj7MpgN9PvcB4uIiHzjbiBXAIc,35243
-sunpeek/data_handling/data_uploader.py,sha256=WR5BPFY740DamytbMzfkqe-7qttmlOs1kfRuBq02GWw,21521
+sunpeek/data_handling/context.py,sha256=tmCD7YSGLvHbIvf2poXpIBrcZ0cU6COyeigxruJRHV0,23667
+sunpeek/data_handling/data_uploader.py,sha256=4qcdfI9yVVYVAnbohe-_kDVqL8m0mlMLepVXuEls0oc,24335
 sunpeek/data_handling/wrapper.py,sha256=DqTzGs5afZZuSng6azWTOD2qMzXbsOIsD4jTTKCobVo,1849
-sunpeek/db_utils/__init__.py,sha256=bMo-AemiN0XPsMSf4jCu4ctF0O786kz14jrSTCYgxUU,61
-sunpeek/db_utils/crud.py,sha256=Oz2XpQVRR62UoCcFqBeqI8VQvNHY5ibD4uZvkbUqrMU,5898
+sunpeek/db_utils/__init__.py,sha256=btLFbporMPPB8l8OvHVb7D4ZwQHd0nj0Tfgj0eOGqfg,25
+sunpeek/db_utils/crud.py,sha256=xhY0jRRGtinnVwYzdwKuBYVifXjF_M_Fdj6NgOEJEvo,5593
 sunpeek/db_utils/db_data_operations.py,sha256=kyqkWnCGLQETmhj_PPY-ahzCgJZAErYq2fb87PSirXc,16575
-sunpeek/db_utils/init_db.py,sha256=XmF6L_tsaIe5M1JGk_kSGDsAKG6liCkgJVT4QVpJSWs,3371
-sunpeek/definitions/__init__.py,sha256=VkqWBjM8Lw0_6PXiMMGgAjZWIPHdAkN0pXPPbk2Znmc,351
-sunpeek/definitions/collectors.py,sha256=EXvGuGbsKKhvx6xiZ2ZBpszZNLn0AUBE3iHAkG52dPg,28733
+sunpeek/db_utils/init_db.py,sha256=m07Es2bbc1B9YUDE0TbSGUVXobl5B9_7ZyBs4isXEHs,1921
+sunpeek/definitions/__init__.py,sha256=KEbtAAapQNPxxfKYp_HpCIZQ7Ugj4wM0BCaZW6K7G8A,454
+sunpeek/definitions/collector_types.py,sha256=Ffyz1MI4C3PsbgZh8vp4SmVBXoGEnCncEAAiU4vZ1b4,24370
 "sunpeek/definitions/fluid_data/Gasokol, corroStar mixture/density.csv",sha256=6R3gcJJiKOuiy2ebwzMrvZ1vodzfP4Eh0ONztBzsAFk,334
 "sunpeek/definitions/fluid_data/Gasokol, corroStar mixture/heat capacity.csv",sha256=clSvcbd6ar_FPqw-eWn_qMFNZSmiGGD67tBDCKxT0ps,337
 sunpeek/definitions/fluid_data/Pekasolar_FHW/density.csv,sha256=ZYl0nS6erP8Gn2GlsmOjaA3l_3xo3nvt7WVU0Nk2leI,88
 sunpeek/definitions/fluid_data/Pekasolar_FHW/heat capacity.csv,sha256=uVb9p15EsbH1Ule1VAoQfT_C1du8NP0q1YhmOMGB-lM,241
 sunpeek/definitions/fluid_data/Wocklum Thermum P/density.csv,sha256=rb1uk1d9bj3MdDQfWb1-YcxhC_RZ7mjhNAdSoHn7XZ8,3450
 sunpeek/definitions/fluid_data/Wocklum Thermum P/heat capacity.csv,sha256=F-IZeYyL2U3rww2tTP1qRWH0bCY3FhGY7f9-FtlvZ_w,2532
 sunpeek/definitions/fluid_definitions.py,sha256=Qry_kWr-cHKEnEbKl9WolUiAJ9g2NzO-rnmIVNpJ2lI,33003
 sunpeek/demo/__init__.py,sha256=djMLgvrYODa5bcBr2KnbPZ37s1yO_3igMAEYNXp_iPQ,528
-sunpeek/demo/demo_plant.py,sha256=_NPGifuWXsKIxTsSDF0lFJWVUmMjo1pSNfzlMUQ4y1w,1860
-sunpeek/demo/demo_plant_script.py,sha256=zryNvlMJa6c1Biwo0zeiT6X3YalFHcvxu6dV2Xf6fK0,5216
-sunpeek/exporter.py,sha256=JkkoouyMPkH2brn4feq0NGeCFmhEQAIJnJIW2d4Bjhw,6116
-sunpeek/serializable_models.py,sha256=ZyEHbulP_1Tup-5njEZ8ofbQFDmSLie_OauxQCpTCcg,38773
-sunpeek-0.3.82.dist-info/COPYING.LESSER,sha256=46mU2C5kSwOnkqkw9XQAJlhBL2JAf1_uCD8lVcXyMRg,7652
-sunpeek-0.3.82.dist-info/METADATA,sha256=wClgq3oumMIdv7A6rjyNsmfwqa_p0wXiZuAG5q47iUw,15279
-sunpeek-0.3.82.dist-info/WHEEL,sha256=vVCvjcmxuUltf8cYhJ0sJMRDLr1XsPuxEId8YDzbyCY,88
-sunpeek-0.3.82.dist-info/COPYING,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
-sunpeek-0.3.82.dist-info/RECORD,,
+sunpeek/demo/demo_plant.py,sha256=tVfvaM7wa2bWLIeZxwpykS0HHxPsoxXthU9zfwJV6Gs,1902
+sunpeek/demo/demo_plant_script.py,sha256=ifWIWRvLTA-IYaTfM-BGZc6mnh15A3-SOwFTs2ehKuE,5168
+sunpeek/exporter.py,sha256=okT1A2hiRjtu-usUAnl5Oq84bIMrkvMuSvTYCitlhGs,6191
+sunpeek/serializable_models.py,sha256=EprZOsa_p95wSaRZzr2S5yd2Ucr2oRrsnMIlDrmXt84,31326
+sunpeek-0.3.9.dist-info/COPYING.LESSER,sha256=46mU2C5kSwOnkqkw9XQAJlhBL2JAf1_uCD8lVcXyMRg,7652
+sunpeek-0.3.9.dist-info/METADATA,sha256=Ncu9ECCt3kZWgUuAk8GQqDGy7G5C1iRlO0lk_eoKaYc,15186
+sunpeek-0.3.9.dist-info/WHEEL,sha256=vVCvjcmxuUltf8cYhJ0sJMRDLr1XsPuxEId8YDzbyCY,88
+sunpeek-0.3.9.dist-info/COPYING,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
+sunpeek-0.3.9.dist-info/RECORD,,
```

