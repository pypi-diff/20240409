# Comparing `tmp/Python_EasyGraph-1.1-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip` & `tmp/Python_EasyGraph-1.2-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,268 +1,227 @@
-Zip file size: 775028 bytes, number of entries: 266
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 Python_EasyGraph-1.1.dist-info/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 Python_EasyGraph.libs/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/
--rwxr-xr-x  2.0 unx   970360 b- defN 24-Feb-05 17:02 cpp_easygraph.pypy39-pp73-x86_64-linux-gnu.so
--rw-r--r--  2.0 unx       24 b- defN 24-Feb-05 17:02 Python_EasyGraph-1.1.dist-info/top_level.txt
--rw-r--r--  2.0 unx      162 b- defN 24-Feb-05 17:02 Python_EasyGraph-1.1.dist-info/WHEEL
--rw-rw-r--  2.0 unx    21977 b- defN 24-Feb-05 17:02 Python_EasyGraph-1.1.dist-info/RECORD
--rw-r--r--  2.0 unx     1561 b- defN 24-Feb-05 17:02 Python_EasyGraph-1.1.dist-info/LICENSE
--rw-r--r--  2.0 unx     1619 b- defN 24-Feb-05 17:02 Python_EasyGraph-1.1.dist-info/LICENSE.NetworkX
--rw-r--r--  2.0 unx     6633 b- defN 24-Feb-05 17:02 Python_EasyGraph-1.1.dist-info/METADATA
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/readwrite/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/ml_metrics/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/model/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/tests/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/classes/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/datapipe/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/datasets/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/utils/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/experiments/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/nn/
--rw-r--r--  2.0 unx      389 b- defN 24-Feb-05 17:02 easygraph/_global.py
--rw-r--r--  2.0 unx     2246 b- defN 24-Feb-05 17:02 easygraph/exception.py
--rw-r--r--  2.0 unx    19169 b- defN 24-Feb-05 17:02 easygraph/convert.py
--rw-r--r--  2.0 unx      702 b- defN 24-Feb-05 17:02 easygraph/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/readwrite/tests/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/readwrite/json_graph/
--rw-r--r--  2.0 unx    10202 b- defN 24-Feb-05 17:02 easygraph/readwrite/ucinet.py
--rw-r--r--  2.0 unx    38050 b- defN 24-Feb-05 17:02 easygraph/readwrite/gexf.py
--rw-r--r--  2.0 unx    28326 b- defN 24-Feb-05 17:02 easygraph/readwrite/gml.py
--rw-r--r--  2.0 unx     5038 b- defN 24-Feb-05 17:02 easygraph/readwrite/graphviz.py
--rw-r--r--  2.0 unx      259 b- defN 24-Feb-05 17:02 easygraph/readwrite/pickle.py
--rw-r--r--  2.0 unx    10793 b- defN 24-Feb-05 17:02 easygraph/readwrite/pajek.py
--rw-r--r--  2.0 unx    39166 b- defN 24-Feb-05 17:02 easygraph/readwrite/graphml.py
--rw-r--r--  2.0 unx    13552 b- defN 24-Feb-05 17:02 easygraph/readwrite/edgelist.py
--rw-r--r--  2.0 unx      372 b- defN 24-Feb-05 17:02 easygraph/readwrite/__init__.py
--rw-r--r--  2.0 unx    17416 b- defN 24-Feb-05 17:02 easygraph/readwrite/tests/test_gml.py
--rw-r--r--  2.0 unx     1446 b- defN 24-Feb-05 17:02 easygraph/readwrite/tests/test_graphviz.py
--rw-r--r--  2.0 unx    15231 b- defN 24-Feb-05 17:02 easygraph/readwrite/tests/test_gexf.py
--rw-r--r--  2.0 unx     7995 b- defN 24-Feb-05 17:02 easygraph/readwrite/tests/test_ucinet.py
--rw-r--r--  2.0 unx     2177 b- defN 24-Feb-05 17:02 easygraph/readwrite/tests/test_pickle.py
--rw-r--r--  2.0 unx    10062 b- defN 24-Feb-05 17:02 easygraph/readwrite/tests/test_edgelist.py
--rw-r--r--  2.0 unx     9253 b- defN 24-Feb-05 17:02 easygraph/readwrite/tests/test_pajek.py
--rw-r--r--  2.0 unx    65180 b- defN 24-Feb-05 17:02 easygraph/readwrite/tests/test_graphml.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-05 17:02 easygraph/readwrite/tests/__init__.py
--rw-r--r--  2.0 unx     3331 b- defN 24-Feb-05 17:02 easygraph/readwrite/json_graph/node_link.py
--rw-r--r--  2.0 unx      521 b- defN 24-Feb-05 17:02 easygraph/readwrite/json_graph/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/ml_metrics/hypergraphs/
--rw-r--r--  2.0 unx     8298 b- defN 24-Feb-05 17:02 easygraph/ml_metrics/base.py
--rw-r--r--  2.0 unx     6393 b- defN 24-Feb-05 17:02 easygraph/ml_metrics/classification.py
--rw-r--r--  2.0 unx     1574 b- defN 24-Feb-05 17:02 easygraph/ml_metrics/__init__.py
--rw-r--r--  2.0 unx     1730 b- defN 24-Feb-05 17:02 easygraph/ml_metrics/hypergraphs/hypergraph.py
--rw-r--r--  2.0 unx       64 b- defN 24-Feb-05 17:02 easygraph/ml_metrics/hypergraphs/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/model/hypergraphs/
--rw-r--r--  2.0 unx      507 b- defN 24-Feb-05 17:02 easygraph/model/__init__.py
--rw-r--r--  2.0 unx    36302 b- defN 24-Feb-05 17:02 easygraph/model/hypergraphs/setgnn.py
--rw-r--r--  2.0 unx     1840 b- defN 24-Feb-05 17:02 easygraph/model/hypergraphs/hgnn.py
--rw-r--r--  2.0 unx     2577 b- defN 24-Feb-05 17:02 easygraph/model/hypergraphs/hypergcn.py
--rw-r--r--  2.0 unx     2556 b- defN 24-Feb-05 17:02 easygraph/model/hypergraphs/dhne.py
--rw-r--r--  2.0 unx     3736 b- defN 24-Feb-05 17:02 easygraph/model/hypergraphs/dhcf.py
--rw-r--r--  2.0 unx     7888 b- defN 24-Feb-05 17:02 easygraph/model/hypergraphs/unignn.py
--rw-r--r--  2.0 unx      261 b- defN 24-Feb-05 17:02 easygraph/model/hypergraphs/__init__.py
--rw-r--r--  2.0 unx     1626 b- defN 24-Feb-05 17:02 easygraph/model/hypergraphs/hgnnp.py
--rw-r--r--  2.0 unx     1605 b- defN 24-Feb-05 17:02 easygraph/model/hypergraphs/hnhn.py
--rw-r--r--  2.0 unx     3043 b- defN 24-Feb-05 17:02 easygraph/tests/test_convert.py
--rw-r--r--  2.0 unx     5871 b- defN 24-Feb-05 17:02 easygraph/tests/teddy_test_cpp_easygraph_sanity_check.py
--rw-r--r--  2.0 unx      302 b- defN 24-Feb-05 17:02 easygraph/tests/test_cpp_easygraph.py
--rw-r--r--  2.0 unx     7380 b- defN 24-Feb-05 17:02 easygraph/tests/script_test_cpp_easygraph.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-05 17:02 easygraph/tests/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/classes/tests/
--rw-r--r--  2.0 unx   106226 b- defN 24-Feb-05 17:02 easygraph/classes/hypergraph.py
--rw-r--r--  2.0 unx      337 b- defN 24-Feb-05 17:02 easygraph/classes/graphviews.py
--rw-r--r--  2.0 unx    44973 b- defN 24-Feb-05 17:02 easygraph/classes/base.py
--rw-r--r--  2.0 unx    50454 b- defN 24-Feb-05 17:02 easygraph/classes/graph.py
--rw-r--r--  2.0 unx    24639 b- defN 24-Feb-05 17:02 easygraph/classes/multigraph.py
--rw-r--r--  2.0 unx    14760 b- defN 24-Feb-05 17:02 easygraph/classes/directed_multigraph.py
--rw-r--r--  2.0 unx    36196 b- defN 24-Feb-05 17:02 easygraph/classes/directed_graph.py
--rw-r--r--  2.0 unx    13931 b- defN 24-Feb-05 17:02 easygraph/classes/operation.py
--rw-r--r--  2.0 unx      525 b- defN 24-Feb-05 17:02 easygraph/classes/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-05 17:02 easygraph/classes/tests/test_multidigraph.py
--rw-r--r--  2.0 unx     2011 b- defN 24-Feb-05 17:02 easygraph/classes/tests/test_multigraph.py
--rw-r--r--  2.0 unx    30053 b- defN 24-Feb-05 17:02 easygraph/classes/tests/test_hypergraph.py
--rw-r--r--  2.0 unx      455 b- defN 24-Feb-05 17:02 easygraph/classes/tests/test_operation.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-05 17:02 easygraph/classes/tests/__init__.py
--rw-r--r--  2.0 unx     2909 b- defN 24-Feb-05 17:02 easygraph/datapipe/common.py
--rw-r--r--  2.0 unx     2734 b- defN 24-Feb-05 17:02 easygraph/datapipe/normalize.py
--rw-r--r--  2.0 unx     2897 b- defN 24-Feb-05 17:02 easygraph/datapipe/loader.py
--rw-r--r--  2.0 unx      717 b- defN 24-Feb-05 17:02 easygraph/datapipe/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/datasets/hypergraph/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/datasets/dynamic/
--rw-r--r--  2.0 unx     6662 b- defN 24-Feb-05 17:02 easygraph/datasets/ppi.py
--rw-r--r--  2.0 unx     2554 b- defN 24-Feb-05 17:02 easygraph/datasets/karate.py
--rw-r--r--  2.0 unx    10334 b- defN 24-Feb-05 17:02 easygraph/datasets/utils.py
--rw-r--r--  2.0 unx    28329 b- defN 24-Feb-05 17:02 easygraph/datasets/citation_graph.py
--rw-r--r--  2.0 unx     6948 b- defN 24-Feb-05 17:02 easygraph/datasets/gnn_benchmark.py
--rw-r--r--  2.0 unx     6876 b- defN 24-Feb-05 17:02 easygraph/datasets/get_sample_graph.py
--rw-r--r--  2.0 unx     9514 b- defN 24-Feb-05 17:02 easygraph/datasets/graph_dataset_base.py
--rw-r--r--  2.0 unx      744 b- defN 24-Feb-05 17:02 easygraph/datasets/__init__.py
--rw-r--r--  2.0 unx     4060 b- defN 24-Feb-05 17:02 easygraph/datasets/hypergraph/House_Committees.py
--rw-r--r--  2.0 unx      336 b- defN 24-Feb-05 17:02 easygraph/datasets/hypergraph/_global.py
--rw-r--r--  2.0 unx     3998 b- defN 24-Feb-05 17:02 easygraph/datasets/hypergraph/senate_committees.py
--rw-r--r--  2.0 unx    11351 b- defN 24-Feb-05 17:02 easygraph/datasets/hypergraph/cocitation.py
--rw-r--r--  2.0 unx     4258 b- defN 24-Feb-05 17:02 easygraph/datasets/hypergraph/mathoverflow_answers.py
--rw-r--r--  2.0 unx     4106 b- defN 24-Feb-05 17:02 easygraph/datasets/hypergraph/cat_edge_Cooking.py
--rw-r--r--  2.0 unx     3361 b- defN 24-Feb-05 17:02 easygraph/datasets/hypergraph/Yelp.py
--rw-r--r--  2.0 unx     3854 b- defN 24-Feb-05 17:02 easygraph/datasets/hypergraph/walmart_trips.py
--rw-r--r--  2.0 unx     4005 b- defN 24-Feb-05 17:02 easygraph/datasets/hypergraph/contact_primary_school.py
--rw-r--r--  2.0 unx     4265 b- defN 24-Feb-05 17:02 easygraph/datasets/hypergraph/hypergraph_dataset_base.py
--rw-r--r--  2.0 unx     3460 b- defN 24-Feb-05 17:02 easygraph/datasets/hypergraph/cooking_200.py
--rw-r--r--  2.0 unx     3734 b- defN 24-Feb-05 17:02 easygraph/datasets/hypergraph/trivago_clicks.py
--rw-r--r--  2.0 unx      276 b- defN 24-Feb-05 17:02 easygraph/datasets/hypergraph/__init__.py
--rw-r--r--  2.0 unx     7690 b- defN 24-Feb-05 17:02 easygraph/datasets/hypergraph/coauthorship.py
--rw-r--r--  2.0 unx     4202 b- defN 24-Feb-05 17:02 easygraph/datasets/dynamic/hospital_lyon.py
--rw-r--r--  2.0 unx     2471 b- defN 24-Feb-05 17:02 easygraph/datasets/dynamic/load_dataset.py
--rw-r--r--  2.0 unx     2499 b- defN 24-Feb-05 17:02 easygraph/datasets/dynamic/email_enron.py
--rw-r--r--  2.0 unx      108 b- defN 24-Feb-05 17:02 easygraph/datasets/dynamic/__init__.py
--rw-r--r--  2.0 unx     2370 b- defN 24-Feb-05 17:02 easygraph/datasets/dynamic/email_eu.py
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/centrality/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/basic/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/graph_embedding/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/hypergraph/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/path/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/community/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/components/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/drawing/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/structural_holes/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/graph_generator/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/core/
--rw-r--r--  2.0 unx     2274 b- defN 24-Feb-05 17:02 easygraph/functions/isolate.py
--rw-r--r--  2.0 unx      566 b- defN 24-Feb-05 17:02 easygraph/functions/__init__.py
--rw-r--r--  2.0 unx     2939 b- defN 24-Feb-05 17:02 easygraph/functions/centrality/closeness.py
--rw-r--r--  2.0 unx     1322 b- defN 24-Feb-05 17:02 easygraph/functions/centrality/pagerank.py
--rw-r--r--  2.0 unx     3772 b- defN 24-Feb-05 17:02 easygraph/functions/centrality/laplacian.py
--rw-r--r--  2.0 unx     6563 b- defN 24-Feb-05 17:02 easygraph/functions/centrality/betweenness.py
--rw-r--r--  2.0 unx     4111 b- defN 24-Feb-05 17:02 easygraph/functions/centrality/flowbetweenness.py
--rw-r--r--  2.0 unx     3026 b- defN 24-Feb-05 17:02 easygraph/functions/centrality/degree.py
--rw-r--r--  2.0 unx     1500 b- defN 24-Feb-05 17:02 easygraph/functions/centrality/ego_betweenness.py
--rw-r--r--  2.0 unx      185 b- defN 24-Feb-05 17:02 easygraph/functions/centrality/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/basic/tests/
--rw-r--r--  2.0 unx      629 b- defN 24-Feb-05 17:02 easygraph/functions/basic/avg_degree.py
--rw-r--r--  2.0 unx     7064 b- defN 24-Feb-05 17:02 easygraph/functions/basic/localassort.py
--rw-r--r--  2.0 unx     2934 b- defN 24-Feb-05 17:02 easygraph/functions/basic/predecessor_path_based.py
--rw-r--r--  2.0 unx    19309 b- defN 24-Feb-05 17:02 easygraph/functions/basic/cluster.py
--rw-r--r--  2.0 unx      114 b- defN 24-Feb-05 17:02 easygraph/functions/basic/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-05 17:02 easygraph/functions/basic/tests/test_avg_degree.py
--rw-r--r--  2.0 unx      494 b- defN 24-Feb-05 17:02 easygraph/functions/basic/tests/test_predecessor.py
--rw-r--r--  2.0 unx    10992 b- defN 24-Feb-05 17:02 easygraph/functions/basic/tests/test_cluster.py
--rw-r--r--  2.0 unx     1064 b- defN 24-Feb-05 17:02 easygraph/functions/basic/tests/test_localassort.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-05 17:02 easygraph/functions/basic/tests/__init__.py
--rw-r--r--  2.0 unx     9777 b- defN 24-Feb-05 17:02 easygraph/functions/graph_embedding/line.py
--rw-r--r--  2.0 unx     9288 b- defN 24-Feb-05 17:02 easygraph/functions/graph_embedding/sdne.py
--rw-r--r--  2.0 unx     2909 b- defN 24-Feb-05 17:02 easygraph/functions/graph_embedding/deepwalk.py
--rw-r--r--  2.0 unx     8992 b- defN 24-Feb-05 17:02 easygraph/functions/graph_embedding/node2vec.py
--rw-r--r--  2.0 unx     3933 b- defN 24-Feb-05 17:02 easygraph/functions/graph_embedding/NOBE.py
--rw-r--r--  2.0 unx      293 b- defN 24-Feb-05 17:02 easygraph/functions/graph_embedding/__init__.py
--rw-r--r--  2.0 unx     5095 b- defN 24-Feb-05 17:02 easygraph/functions/graph_embedding/net_emb_example_citeseer.py
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/hypergraph/centrality/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/hypergraph/hypergraph_generator/
--rw-r--r--  2.0 unx     2268 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/hypergraph_operation.py
--rw-r--r--  2.0 unx     8101 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/hypergraph_clustering.py
--rw-r--r--  2.0 unx     4645 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/assortativity.py
--rw-r--r--  2.0 unx      164 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/__init__.py
--rw-r--r--  2.0 unx     6282 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/centrality/cycle_ratio.py
--rw-r--r--  2.0 unx    11579 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/centrality/hypercoreness.py
--rw-r--r--  2.0 unx     1786 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/centrality/vector_centrality.py
--rw-r--r--  2.0 unx     2273 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/centrality/s_centrality.py
--rw-r--r--  2.0 unx      474 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/centrality/degree.py
--rw-r--r--  2.0 unx      139 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/centrality/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/hypergraph/hypergraph_generator/tests/
--rw-r--r--  2.0 unx     1874 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/hypergraph_generator/simple.py
--rw-r--r--  2.0 unx    12592 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/hypergraph_generator/uniform.py
--rw-r--r--  2.0 unx     1462 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/hypergraph_generator/lattice.py
--rw-r--r--  2.0 unx    13577 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/hypergraph_generator/random.py
--rw-r--r--  2.0 unx      940 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/hypergraph_generator/hypergraph_classic.py
--rw-r--r--  2.0 unx      124 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/hypergraph_generator/__init__.py
--rw-r--r--  2.0 unx     1458 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/hypergraph_generator/tests/test_classic.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-05 17:02 easygraph/functions/hypergraph/hypergraph_generator/tests/__init__.py
--rw-r--r--  2.0 unx     2584 b- defN 24-Feb-05 17:02 easygraph/functions/path/diameter.py
--rw-r--r--  2.0 unx    22611 b- defN 24-Feb-05 17:02 easygraph/functions/path/mst.py
--rw-r--r--  2.0 unx     5979 b- defN 24-Feb-05 17:02 easygraph/functions/path/bridges.py
--rw-r--r--  2.0 unx     4060 b- defN 24-Feb-05 17:02 easygraph/functions/path/average_shortest_path_length.py
--rw-r--r--  2.0 unx     6448 b- defN 24-Feb-05 17:02 easygraph/functions/path/path.py
--rw-r--r--  2.0 unx      130 b- defN 24-Feb-05 17:02 easygraph/functions/path/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/functions/community/tests/
--rw-r--r--  2.0 unx     3328 b- defN 24-Feb-05 17:02 easygraph/functions/community/motif.py
--rw-r--r--  2.0 unx    12340 b- defN 24-Feb-05 17:02 easygraph/functions/community/louvain.py
--rw-r--r--  2.0 unx     1918 b- defN 24-Feb-05 17:02 easygraph/functions/community/ego_graph.py
--rw-r--r--  2.0 unx    23852 b- defN 24-Feb-05 17:02 easygraph/functions/community/LPA.py
--rw-r--r--  2.0 unx     1964 b- defN 24-Feb-05 17:02 easygraph/functions/community/modularity.py
--rw-r--r--  2.0 unx     7371 b- defN 24-Feb-05 17:02 easygraph/functions/community/modularity_max_detection.py
--rw-r--r--  2.0 unx      154 b- defN 24-Feb-05 17:02 easygraph/functions/community/__init__.py
--rw-r--r--  2.0 unx      518 b- defN 24-Feb-05 17:02 easygraph/functions/community/tests/test_motif.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-05 17:02 easygraph/functions/community/tests/__init__.py
--rw-r--r--  2.0 unx     3572 b- defN 24-Feb-05 17:02 easygraph/functions/components/connected.py
--rw-r--r--  2.0 unx     4084 b- defN 24-Feb-05 17:02 easygraph/functions/components/weakly_connected.py
--rw-r--r--  2.0 unx     6999 b- defN 24-Feb-05 17:02 easygraph/functions/components/strongly_connected.py
--rw-r--r--  2.0 unx     7790 b- defN 24-Feb-05 17:02 easygraph/functions/components/biconnected.py
--rw-r--r--  2.0 unx      118 b- defN 24-Feb-05 17:02 easygraph/functions/components/__init__.py
--rw-r--r--  2.0 unx     7891 b- defN 24-Feb-05 17:02 easygraph/functions/drawing/defaults.py
--rw-r--r--  2.0 unx    20431 b- defN 24-Feb-05 17:02 easygraph/functions/drawing/positioning.py
--rw-r--r--  2.0 unx    19115 b- defN 24-Feb-05 17:02 easygraph/functions/drawing/utils.py
--rw-r--r--  2.0 unx     6229 b- defN 24-Feb-05 17:02 easygraph/functions/drawing/plot.py
--rw-r--r--  2.0 unx     7115 b- defN 24-Feb-05 17:02 easygraph/functions/drawing/simulator.py
--rw-r--r--  2.0 unx    45419 b- defN 24-Feb-05 17:02 easygraph/functions/drawing/drawing.py
--rw-r--r--  2.0 unx      975 b- defN 24-Feb-05 17:02 easygraph/functions/drawing/layout.py
--rw-r--r--  2.0 unx      803 b- defN 24-Feb-05 17:02 easygraph/functions/drawing/geometry.py
--rw-r--r--  2.0 unx       70 b- defN 24-Feb-05 17:02 easygraph/functions/drawing/__init__.py
--rw-r--r--  2.0 unx     9822 b- defN 24-Feb-05 17:02 easygraph/functions/structural_holes/MaxD.py
--rw-r--r--  2.0 unx    18955 b- defN 24-Feb-05 17:02 easygraph/functions/structural_holes/maxBlock.py
--rw-r--r--  2.0 unx     7104 b- defN 24-Feb-05 17:02 easygraph/functions/structural_holes/ICC.py
--rw-r--r--  2.0 unx    13179 b- defN 24-Feb-05 17:02 easygraph/functions/structural_holes/metrics.py
--rw-r--r--  2.0 unx     4364 b- defN 24-Feb-05 17:02 easygraph/functions/structural_holes/HIS.py
--rw-r--r--  2.0 unx     9978 b- defN 24-Feb-05 17:02 easygraph/functions/structural_holes/weakTie.py
--rw-r--r--  2.0 unx    12818 b- defN 24-Feb-05 17:02 easygraph/functions/structural_holes/evaluation.py
--rw-r--r--  2.0 unx    10650 b- defN 24-Feb-05 17:02 easygraph/functions/structural_holes/SHII_metric.py
--rw-r--r--  2.0 unx    11359 b- defN 24-Feb-05 17:02 easygraph/functions/structural_holes/AP_Greedy.py
--rw-r--r--  2.0 unx     3857 b- defN 24-Feb-05 17:02 easygraph/functions/structural_holes/NOBE.py
--rw-r--r--  2.0 unx     8168 b- defN 24-Feb-05 17:02 easygraph/functions/structural_holes/HAM.py
--rw-r--r--  2.0 unx      171 b- defN 24-Feb-05 17:02 easygraph/functions/structural_holes/__init__.py
--rw-r--r--  2.0 unx     1894 b- defN 24-Feb-05 17:02 easygraph/functions/graph_generator/classic.py
--rw-r--r--  2.0 unx    12333 b- defN 24-Feb-05 17:02 easygraph/functions/graph_generator/RandomNetwork.py
--rw-r--r--  2.0 unx       52 b- defN 24-Feb-05 17:02 easygraph/functions/graph_generator/__init__.py
--rw-r--r--  2.0 unx     1945 b- defN 24-Feb-05 17:02 easygraph/functions/core/k_core.py
--rw-r--r--  2.0 unx       22 b- defN 24-Feb-05 17:02 easygraph/functions/core/__init__.py
--rw-r--r--  2.0 unx    34870 b- defN 24-Feb-05 17:02 easygraph/utils/convert_to_matrix.py
--rw-r--r--  2.0 unx     1018 b- defN 24-Feb-05 17:02 easygraph/utils/exception.py
--rw-r--r--  2.0 unx      291 b- defN 24-Feb-05 17:02 easygraph/utils/index_of_node.py
--rw-r--r--  2.0 unx     3662 b- defN 24-Feb-05 17:02 easygraph/utils/relabel.py
--rw-r--r--  2.0 unx     6017 b- defN 24-Feb-05 17:02 easygraph/utils/mapped_queue.py
--rw-r--r--  2.0 unx    42004 b- defN 24-Feb-05 17:02 easygraph/utils/decorators.py
--rw-r--r--  2.0 unx     2595 b- defN 24-Feb-05 17:02 easygraph/utils/alias.py
--rw-r--r--  2.0 unx     1216 b- defN 24-Feb-05 17:02 easygraph/utils/sparse.py
--rw-r--r--  2.0 unx     4385 b- defN 24-Feb-05 17:02 easygraph/utils/type_change.py
--rw-r--r--  2.0 unx     1343 b- defN 24-Feb-05 17:02 easygraph/utils/logging.py
--rw-r--r--  2.0 unx      516 b- defN 24-Feb-05 17:02 easygraph/utils/convert_class.py
--rw-r--r--  2.0 unx     5846 b- defN 24-Feb-05 17:02 easygraph/utils/misc.py
--rw-r--r--  2.0 unx     2552 b- defN 24-Feb-05 17:02 easygraph/utils/download.py
--rw-r--r--  2.0 unx      525 b- defN 24-Feb-05 17:02 easygraph/utils/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/experiments/hypergraphs/
--rw-r--r--  2.0 unx     8648 b- defN 24-Feb-05 17:02 easygraph/experiments/base.py
--rw-r--r--  2.0 unx     6408 b- defN 24-Feb-05 17:02 easygraph/experiments/vertex_classification.py
--rw-r--r--  2.0 unx      260 b- defN 24-Feb-05 17:02 easygraph/experiments/__init__.py
--rw-r--r--  2.0 unx     4924 b- defN 24-Feb-05 17:02 easygraph/experiments/hypergraphs/hypergraph.py
--rw-r--r--  2.0 unx       59 b- defN 24-Feb-05 17:02 easygraph/experiments/hypergraphs/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/nn/tests/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/nn/convs/
--rw-r--r--  2.0 unx     1722 b- defN 24-Feb-05 17:02 easygraph/nn/loss.py
--rw-r--r--  2.0 unx      883 b- defN 24-Feb-05 17:02 easygraph/nn/regularization.py
--rw-r--r--  2.0 unx      856 b- defN 24-Feb-05 17:02 easygraph/nn/__init__.py
--rw-r--r--  2.0 unx      431 b- defN 24-Feb-05 17:02 easygraph/nn/tests/test_regularization.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-05 17:02 easygraph/nn/tests/test_gcnconv.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-05 17:02 easygraph/nn/tests/test_gatconv.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-05 17:02 easygraph/nn/tests/test_graphsageconv.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-05 17:02 easygraph/nn/tests/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 24-Feb-05 17:02 easygraph/nn/convs/hypergraphs/
--rw-r--r--  2.0 unx     5520 b- defN 24-Feb-05 17:02 easygraph/nn/convs/common.py
--rw-r--r--  2.0 unx     6561 b- defN 24-Feb-05 17:02 easygraph/nn/convs/pma.py
--rw-r--r--  2.0 unx        1 b- defN 24-Feb-05 17:02 easygraph/nn/convs/__init__.py
--rw-r--r--  2.0 unx    12363 b- defN 24-Feb-05 17:02 easygraph/nn/convs/hypergraphs/unignn_conv.py
--rw-r--r--  2.0 unx     3267 b- defN 24-Feb-05 17:02 easygraph/nn/convs/hypergraphs/hgnn_conv.py
--rw-r--r--  2.0 unx     2175 b- defN 24-Feb-05 17:02 easygraph/nn/convs/hypergraphs/hnhn_conv.py
--rw-r--r--  2.0 unx     2747 b- defN 24-Feb-05 17:02 easygraph/nn/convs/hypergraphs/hgnnp_conv.py
--rw-r--r--  2.0 unx     2789 b- defN 24-Feb-05 17:02 easygraph/nn/convs/hypergraphs/hypergcn_conv.py
--rw-r--r--  2.0 unx     2234 b- defN 24-Feb-05 17:02 easygraph/nn/convs/hypergraphs/dhcf_conv.py
--rw-r--r--  2.0 unx     3702 b- defN 24-Feb-05 17:02 easygraph/nn/convs/hypergraphs/halfnlh_conv.py
--rw-r--r--  2.0 unx      313 b- defN 24-Feb-05 17:02 easygraph/nn/convs/hypergraphs/__init__.py
-266 files, 2575072 bytes uncompressed, 734490 bytes compressed:  71.5%
+Zip file size: 645331 bytes, number of entries: 225
+-rw-rw-rw-  2.0 fat   492544 b- defN 24-Apr-09 08:21 cpp_easygraph.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat      729 b- defN 24-Apr-09 08:13 easygraph/__init__.py
+-rw-rw-rw-  2.0 fat      404 b- defN 24-Apr-09 08:13 easygraph/_global.py
+-rw-rw-rw-  2.0 fat    19760 b- defN 24-Apr-09 08:13 easygraph/convert.py
+-rw-rw-rw-  2.0 fat     2330 b- defN 24-Apr-09 08:13 easygraph/exception.py
+-rw-rw-rw-  2.0 fat      544 b- defN 24-Apr-09 08:13 easygraph/classes/__init__.py
+-rw-rw-rw-  2.0 fat    45970 b- defN 24-Apr-09 08:13 easygraph/classes/base.py
+-rw-rw-rw-  2.0 fat    38487 b- defN 24-Apr-09 08:13 easygraph/classes/directed_graph.py
+-rw-rw-rw-  2.0 fat    15177 b- defN 24-Apr-09 08:13 easygraph/classes/directed_multigraph.py
+-rw-rw-rw-  2.0 fat    52087 b- defN 24-Apr-09 08:13 easygraph/classes/graph.py
+-rw-rw-rw-  2.0 fat      352 b- defN 24-Apr-09 08:13 easygraph/classes/graphviews.py
+-rw-rw-rw-  2.0 fat   108689 b- defN 24-Apr-09 08:13 easygraph/classes/hypergraph.py
+-rw-rw-rw-  2.0 fat    25368 b- defN 24-Apr-09 08:13 easygraph/classes/multigraph.py
+-rw-rw-rw-  2.0 fat    14378 b- defN 24-Apr-09 08:13 easygraph/classes/operation.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-09 08:13 easygraph/classes/tests/__init__.py
+-rw-rw-rw-  2.0 fat    31054 b- defN 24-Apr-09 08:13 easygraph/classes/tests/test_hypergraph.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-09 08:13 easygraph/classes/tests/test_multidigraph.py
+-rw-rw-rw-  2.0 fat     2072 b- defN 24-Apr-09 08:13 easygraph/classes/tests/test_multigraph.py
+-rw-rw-rw-  2.0 fat      470 b- defN 24-Apr-09 08:13 easygraph/classes/tests/test_operation.py
+-rw-rw-rw-  2.0 fat      746 b- defN 24-Apr-09 08:13 easygraph/datapipe/__init__.py
+-rw-rw-rw-  2.0 fat     3015 b- defN 24-Apr-09 08:13 easygraph/datapipe/common.py
+-rw-rw-rw-  2.0 fat     2987 b- defN 24-Apr-09 08:13 easygraph/datapipe/loader.py
+-rw-rw-rw-  2.0 fat     2808 b- defN 24-Apr-09 08:13 easygraph/datapipe/normalize.py
+-rw-rw-rw-  2.0 fat      744 b- defN 24-Apr-09 08:13 easygraph/datasets/__init__.py
+-rw-rw-rw-  2.0 fat    29204 b- defN 24-Apr-09 08:13 easygraph/datasets/citation_graph.py
+-rw-rw-rw-  2.0 fat     7086 b- defN 24-Apr-09 08:13 easygraph/datasets/get_sample_graph.py
+-rw-rw-rw-  2.0 fat     7164 b- defN 24-Apr-09 08:13 easygraph/datasets/gnn_benchmark.py
+-rw-rw-rw-  2.0 fat     9833 b- defN 24-Apr-09 08:13 easygraph/datasets/graph_dataset_base.py
+-rw-rw-rw-  2.0 fat     2647 b- defN 24-Apr-09 08:13 easygraph/datasets/karate.py
+-rw-rw-rw-  2.0 fat     6878 b- defN 24-Apr-09 08:13 easygraph/datasets/ppi.py
+-rw-rw-rw-  2.0 fat    10692 b- defN 24-Apr-09 08:13 easygraph/datasets/utils.py
+-rw-rw-rw-  2.0 fat      112 b- defN 24-Apr-09 08:13 easygraph/datasets/dynamic/__init__.py
+-rw-rw-rw-  2.0 fat     2586 b- defN 24-Apr-09 08:13 easygraph/datasets/dynamic/email_enron.py
+-rw-rw-rw-  2.0 fat     2452 b- defN 24-Apr-09 08:13 easygraph/datasets/dynamic/email_eu.py
+-rw-rw-rw-  2.0 fat     4334 b- defN 24-Apr-09 08:13 easygraph/datasets/dynamic/hospital_lyon.py
+-rw-rw-rw-  2.0 fat     2565 b- defN 24-Apr-09 08:13 easygraph/datasets/dynamic/load_dataset.py
+-rw-rw-rw-  2.0 fat     4172 b- defN 24-Apr-09 08:13 easygraph/datasets/hypergraph/House_Committees.py
+-rw-rw-rw-  2.0 fat     3443 b- defN 24-Apr-09 08:13 easygraph/datasets/hypergraph/Yelp.py
+-rw-rw-rw-  2.0 fat      285 b- defN 24-Apr-09 08:13 easygraph/datasets/hypergraph/__init__.py
+-rw-rw-rw-  2.0 fat      350 b- defN 24-Apr-09 08:13 easygraph/datasets/hypergraph/_global.py
+-rw-rw-rw-  2.0 fat     4221 b- defN 24-Apr-09 08:13 easygraph/datasets/hypergraph/cat_edge_Cooking.py
+-rw-rw-rw-  2.0 fat     7879 b- defN 24-Apr-09 08:13 easygraph/datasets/hypergraph/coauthorship.py
+-rw-rw-rw-  2.0 fat    11630 b- defN 24-Apr-09 08:13 easygraph/datasets/hypergraph/cocitation.py
+-rw-rw-rw-  2.0 fat     4117 b- defN 24-Apr-09 08:13 easygraph/datasets/hypergraph/contact_primary_school.py
+-rw-rw-rw-  2.0 fat     3545 b- defN 24-Apr-09 08:13 easygraph/datasets/hypergraph/cooking_200.py
+-rw-rw-rw-  2.0 fat     4384 b- defN 24-Apr-09 08:13 easygraph/datasets/hypergraph/hypergraph_dataset_base.py
+-rw-rw-rw-  2.0 fat     4371 b- defN 24-Apr-09 08:13 easygraph/datasets/hypergraph/mathoverflow_answers.py
+-rw-rw-rw-  2.0 fat     4104 b- defN 24-Apr-09 08:13 easygraph/datasets/hypergraph/senate_committees.py
+-rw-rw-rw-  2.0 fat     3838 b- defN 24-Apr-09 08:13 easygraph/datasets/hypergraph/trivago_clicks.py
+-rw-rw-rw-  2.0 fat     3963 b- defN 24-Apr-09 08:13 easygraph/datasets/hypergraph/walmart_trips.py
+-rw-rw-rw-  2.0 fat      270 b- defN 24-Apr-09 08:13 easygraph/experiments/__init__.py
+-rw-rw-rw-  2.0 fat     8852 b- defN 24-Apr-09 08:13 easygraph/experiments/base.py
+-rw-rw-rw-  2.0 fat     6574 b- defN 24-Apr-09 08:13 easygraph/experiments/vertex_classification.py
+-rw-rw-rw-  2.0 fat       60 b- defN 24-Apr-09 08:13 easygraph/experiments/hypergraphs/__init__.py
+-rw-rw-rw-  2.0 fat     5045 b- defN 24-Apr-09 08:13 easygraph/experiments/hypergraphs/hypergraph.py
+-rw-rw-rw-  2.0 fat      579 b- defN 24-Apr-09 08:13 easygraph/functions/__init__.py
+-rw-rw-rw-  2.0 fat     2377 b- defN 24-Apr-09 08:13 easygraph/functions/isolate.py
+-rw-rw-rw-  2.0 fat      118 b- defN 24-Apr-09 08:13 easygraph/functions/basic/__init__.py
+-rw-rw-rw-  2.0 fat      660 b- defN 24-Apr-09 08:13 easygraph/functions/basic/avg_degree.py
+-rw-rw-rw-  2.0 fat    19868 b- defN 24-Apr-09 08:13 easygraph/functions/basic/cluster.py
+-rw-rw-rw-  2.0 fat     7290 b- defN 24-Apr-09 08:13 easygraph/functions/basic/localassort.py
+-rw-rw-rw-  2.0 fat     3035 b- defN 24-Apr-09 08:13 easygraph/functions/basic/predecessor_path_based.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-09 08:13 easygraph/functions/basic/tests/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-09 08:13 easygraph/functions/basic/tests/test_avg_degree.py
+-rw-rw-rw-  2.0 fat    11371 b- defN 24-Apr-09 08:13 easygraph/functions/basic/tests/test_cluster.py
+-rw-rw-rw-  2.0 fat     1100 b- defN 24-Apr-09 08:13 easygraph/functions/basic/tests/test_localassort.py
+-rw-rw-rw-  2.0 fat      512 b- defN 24-Apr-09 08:13 easygraph/functions/basic/tests/test_predecessor.py
+-rw-rw-rw-  2.0 fat      192 b- defN 24-Apr-09 08:13 easygraph/functions/centrality/__init__.py
+-rw-rw-rw-  2.0 fat     6805 b- defN 24-Apr-09 08:13 easygraph/functions/centrality/betweenness.py
+-rw-rw-rw-  2.0 fat     3041 b- defN 24-Apr-09 08:13 easygraph/functions/centrality/closeness.py
+-rw-rw-rw-  2.0 fat     3164 b- defN 24-Apr-09 08:13 easygraph/functions/centrality/degree.py
+-rw-rw-rw-  2.0 fat     1552 b- defN 24-Apr-09 08:13 easygraph/functions/centrality/ego_betweenness.py
+-rw-rw-rw-  2.0 fat     4257 b- defN 24-Apr-09 08:13 easygraph/functions/centrality/flowbetweenness.py
+-rw-rw-rw-  2.0 fat     3906 b- defN 24-Apr-09 08:13 easygraph/functions/centrality/laplacian.py
+-rw-rw-rw-  2.0 fat     1377 b- defN 24-Apr-09 08:13 easygraph/functions/centrality/pagerank.py
+-rw-rw-rw-  2.0 fat    24613 b- defN 24-Apr-09 08:13 easygraph/functions/community/LPA.py
+-rw-rw-rw-  2.0 fat      160 b- defN 24-Apr-09 08:13 easygraph/functions/community/__init__.py
+-rw-rw-rw-  2.0 fat     1999 b- defN 24-Apr-09 08:13 easygraph/functions/community/ego_graph.py
+-rw-rw-rw-  2.0 fat    12690 b- defN 24-Apr-09 08:13 easygraph/functions/community/louvain.py
+-rw-rw-rw-  2.0 fat     2039 b- defN 24-Apr-09 08:13 easygraph/functions/community/modularity.py
+-rw-rw-rw-  2.0 fat     7571 b- defN 24-Apr-09 08:13 easygraph/functions/community/modularity_max_detection.py
+-rw-rw-rw-  2.0 fat     3448 b- defN 24-Apr-09 08:13 easygraph/functions/community/motif.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-09 08:13 easygraph/functions/community/tests/__init__.py
+-rw-rw-rw-  2.0 fat      534 b- defN 24-Apr-09 08:13 easygraph/functions/community/tests/test_motif.py
+-rw-rw-rw-  2.0 fat      122 b- defN 24-Apr-09 08:13 easygraph/functions/components/__init__.py
+-rw-rw-rw-  2.0 fat     8037 b- defN 24-Apr-09 08:13 easygraph/functions/components/biconnected.py
+-rw-rw-rw-  2.0 fat     3732 b- defN 24-Apr-09 08:13 easygraph/functions/components/connected.py
+-rw-rw-rw-  2.0 fat     7243 b- defN 24-Apr-09 08:13 easygraph/functions/components/strongly_connected.py
+-rw-rw-rw-  2.0 fat     4270 b- defN 24-Apr-09 08:13 easygraph/functions/components/weakly_connected.py
+-rw-rw-rw-  2.0 fat       23 b- defN 24-Apr-09 08:13 easygraph/functions/core/__init__.py
+-rw-rw-rw-  2.0 fat     2014 b- defN 24-Apr-09 08:13 easygraph/functions/core/k_core.py
+-rw-rw-rw-  2.0 fat       73 b- defN 24-Apr-09 08:13 easygraph/functions/drawing/__init__.py
+-rw-rw-rw-  2.0 fat     8142 b- defN 24-Apr-09 08:13 easygraph/functions/drawing/defaults.py
+-rw-rw-rw-  2.0 fat    55938 b- defN 24-Apr-09 08:13 easygraph/functions/drawing/drawing.py
+-rw-rw-rw-  2.0 fat      844 b- defN 24-Apr-09 08:13 easygraph/functions/drawing/geometry.py
+-rw-rw-rw-  2.0 fat     1008 b- defN 24-Apr-09 08:13 easygraph/functions/drawing/layout.py
+-rw-rw-rw-  2.0 fat     6458 b- defN 24-Apr-09 08:13 easygraph/functions/drawing/plot.py
+-rw-rw-rw-  2.0 fat    21074 b- defN 24-Apr-09 08:13 easygraph/functions/drawing/positioning.py
+-rw-rw-rw-  2.0 fat     7174 b- defN 24-Apr-09 08:13 easygraph/functions/drawing/simulator.py
+-rw-rw-rw-  2.0 fat    20175 b- defN 24-Apr-09 08:13 easygraph/functions/drawing/utils.py
+-rw-rw-rw-  2.0 fat     4117 b- defN 24-Apr-09 08:13 easygraph/functions/graph_embedding/NOBE.py
+-rw-rw-rw-  2.0 fat      306 b- defN 24-Apr-09 08:13 easygraph/functions/graph_embedding/__init__.py
+-rw-rw-rw-  2.0 fat     3012 b- defN 24-Apr-09 08:13 easygraph/functions/graph_embedding/deepwalk.py
+-rw-rw-rw-  2.0 fat    10080 b- defN 24-Apr-09 08:13 easygraph/functions/graph_embedding/line.py
+-rw-rw-rw-  2.0 fat     5270 b- defN 24-Apr-09 08:13 easygraph/functions/graph_embedding/net_emb_example_citeseer.py
+-rw-rw-rw-  2.0 fat     9300 b- defN 24-Apr-09 08:13 easygraph/functions/graph_embedding/node2vec.py
+-rw-rw-rw-  2.0 fat     9569 b- defN 24-Apr-09 08:13 easygraph/functions/graph_embedding/sdne.py
+-rw-rw-rw-  2.0 fat    12744 b- defN 24-Apr-09 08:13 easygraph/functions/graph_generator/RandomNetwork.py
+-rw-rw-rw-  2.0 fat       54 b- defN 24-Apr-09 08:13 easygraph/functions/graph_generator/__init__.py
+-rw-rw-rw-  2.0 fat     1967 b- defN 24-Apr-09 08:13 easygraph/functions/graph_generator/classic.py
+-rw-rw-rw-  2.0 fat      159 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/__init__.py
+-rw-rw-rw-  2.0 fat     4830 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/assortativity.py
+-rw-rw-rw-  2.0 fat     8396 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/hypergraph_clustering.py
+-rw-rw-rw-  2.0 fat     2338 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/hypergraph_operation.py
+-rw-rw-rw-  2.0 fat      144 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/centrality/__init__.py
+-rw-rw-rw-  2.0 fat     6483 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/centrality/cycle_ratio.py
+-rw-rw-rw-  2.0 fat      502 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/centrality/degree.py
+-rw-rw-rw-  2.0 fat    11930 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/centrality/hypercoreness.py
+-rw-rw-rw-  2.0 fat     2362 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/centrality/s_centrality.py
+-rw-rw-rw-  2.0 fat     1852 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/centrality/vector_centrality.py
+-rw-rw-rw-  2.0 fat      129 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/null_model/__init__.py
+-rw-rw-rw-  2.0 fat      979 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/null_model/hypergraph_classic.py
+-rw-rw-rw-  2.0 fat     1531 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/null_model/lattice.py
+-rw-rw-rw-  2.0 fat    14011 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/null_model/random.py
+-rw-rw-rw-  2.0 fat     1951 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/null_model/simple.py
+-rw-rw-rw-  2.0 fat    13047 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/null_model/uniform.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/null_model/tests/__init__.py
+-rw-rw-rw-  2.0 fat     1506 b- defN 24-Apr-09 08:13 easygraph/functions/hypergraph/null_model/tests/test_classic.py
+-rw-rw-rw-  2.0 fat      135 b- defN 24-Apr-09 08:13 easygraph/functions/path/__init__.py
+-rw-rw-rw-  2.0 fat     4175 b- defN 24-Apr-09 08:13 easygraph/functions/path/average_shortest_path_length.py
+-rw-rw-rw-  2.0 fat     6178 b- defN 24-Apr-09 08:13 easygraph/functions/path/bridges.py
+-rw-rw-rw-  2.0 fat     2689 b- defN 24-Apr-09 08:13 easygraph/functions/path/diameter.py
+-rw-rw-rw-  2.0 fat    23296 b- defN 24-Apr-09 08:13 easygraph/functions/path/mst.py
+-rw-rw-rw-  2.0 fat     6710 b- defN 24-Apr-09 08:13 easygraph/functions/path/path.py
+-rw-rw-rw-  2.0 fat    11761 b- defN 24-Apr-09 08:13 easygraph/functions/structural_holes/AP_Greedy.py
+-rw-rw-rw-  2.0 fat     8468 b- defN 24-Apr-09 08:13 easygraph/functions/structural_holes/HAM.py
+-rw-rw-rw-  2.0 fat     4511 b- defN 24-Apr-09 08:13 easygraph/functions/structural_holes/HIS.py
+-rw-rw-rw-  2.0 fat     7402 b- defN 24-Apr-09 08:13 easygraph/functions/structural_holes/ICC.py
+-rw-rw-rw-  2.0 fat    10269 b- defN 24-Apr-09 08:13 easygraph/functions/structural_holes/MaxD.py
+-rw-rw-rw-  2.0 fat     4024 b- defN 24-Apr-09 08:13 easygraph/functions/structural_holes/NOBE.py
+-rw-rw-rw-  2.0 fat    10944 b- defN 24-Apr-09 08:13 easygraph/functions/structural_holes/SHII_metric.py
+-rw-rw-rw-  2.0 fat      179 b- defN 24-Apr-09 08:13 easygraph/functions/structural_holes/__init__.py
+-rw-rw-rw-  2.0 fat    13319 b- defN 24-Apr-09 08:13 easygraph/functions/structural_holes/evaluation.py
+-rw-rw-rw-  2.0 fat    19566 b- defN 24-Apr-09 08:13 easygraph/functions/structural_holes/maxBlock.py
+-rw-rw-rw-  2.0 fat    13573 b- defN 24-Apr-09 08:13 easygraph/functions/structural_holes/metrics.py
+-rw-rw-rw-  2.0 fat    10316 b- defN 24-Apr-09 08:13 easygraph/functions/structural_holes/weakTie.py
+-rw-rw-rw-  2.0 fat     1619 b- defN 24-Apr-09 08:13 easygraph/ml_metrics/__init__.py
+-rw-rw-rw-  2.0 fat     8500 b- defN 24-Apr-09 08:13 easygraph/ml_metrics/base.py
+-rw-rw-rw-  2.0 fat     6555 b- defN 24-Apr-09 08:13 easygraph/ml_metrics/classification.py
+-rw-rw-rw-  2.0 fat       65 b- defN 24-Apr-09 08:13 easygraph/ml_metrics/hypergraphs/__init__.py
+-rw-rw-rw-  2.0 fat     1777 b- defN 24-Apr-09 08:13 easygraph/ml_metrics/hypergraphs/hypergraph.py
+-rw-rw-rw-  2.0 fat      523 b- defN 24-Apr-09 08:13 easygraph/model/__init__.py
+-rw-rw-rw-  2.0 fat      271 b- defN 24-Apr-09 08:13 easygraph/model/hypergraphs/__init__.py
+-rw-rw-rw-  2.0 fat     3831 b- defN 24-Apr-09 08:13 easygraph/model/hypergraphs/dhcf.py
+-rw-rw-rw-  2.0 fat     2626 b- defN 24-Apr-09 08:13 easygraph/model/hypergraphs/dhne.py
+-rw-rw-rw-  2.0 fat     1893 b- defN 24-Apr-09 08:13 easygraph/model/hypergraphs/hgnn.py
+-rw-rw-rw-  2.0 fat     1670 b- defN 24-Apr-09 08:13 easygraph/model/hypergraphs/hgnnp.py
+-rw-rw-rw-  2.0 fat     1649 b- defN 24-Apr-09 08:13 easygraph/model/hypergraphs/hnhn.py
+-rw-rw-rw-  2.0 fat     2644 b- defN 24-Apr-09 08:13 easygraph/model/hypergraphs/hypergcn.py
+-rw-rw-rw-  2.0 fat    37190 b- defN 24-Apr-09 08:13 easygraph/model/hypergraphs/setgnn.py
+-rw-rw-rw-  2.0 fat     8102 b- defN 24-Apr-09 08:13 easygraph/model/hypergraphs/unignn.py
+-rw-rw-rw-  2.0 fat      878 b- defN 24-Apr-09 08:13 easygraph/nn/__init__.py
+-rw-rw-rw-  2.0 fat     1768 b- defN 24-Apr-09 08:13 easygraph/nn/loss.py
+-rw-rw-rw-  2.0 fat      915 b- defN 24-Apr-09 08:13 easygraph/nn/regularization.py
+-rw-rw-rw-  2.0 fat        2 b- defN 24-Apr-09 08:13 easygraph/nn/convs/__init__.py
+-rw-rw-rw-  2.0 fat     5656 b- defN 24-Apr-09 08:13 easygraph/nn/convs/common.py
+-rw-rw-rw-  2.0 fat     6741 b- defN 24-Apr-09 08:13 easygraph/nn/convs/pma.py
+-rw-rw-rw-  2.0 fat      322 b- defN 24-Apr-09 08:13 easygraph/nn/convs/hypergraphs/__init__.py
+-rw-rw-rw-  2.0 fat     2288 b- defN 24-Apr-09 08:13 easygraph/nn/convs/hypergraphs/dhcf_conv.py
+-rw-rw-rw-  2.0 fat     3814 b- defN 24-Apr-09 08:13 easygraph/nn/convs/hypergraphs/halfnlh_conv.py
+-rw-rw-rw-  2.0 fat     3339 b- defN 24-Apr-09 08:13 easygraph/nn/convs/hypergraphs/hgnn_conv.py
+-rw-rw-rw-  2.0 fat     2814 b- defN 24-Apr-09 08:13 easygraph/nn/convs/hypergraphs/hgnnp_conv.py
+-rw-rw-rw-  2.0 fat     2228 b- defN 24-Apr-09 08:13 easygraph/nn/convs/hypergraphs/hnhn_conv.py
+-rw-rw-rw-  2.0 fat     2850 b- defN 24-Apr-09 08:13 easygraph/nn/convs/hypergraphs/hypergcn_conv.py
+-rw-rw-rw-  2.0 fat    12653 b- defN 24-Apr-09 08:13 easygraph/nn/convs/hypergraphs/unignn_conv.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-09 08:13 easygraph/nn/tests/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-09 08:13 easygraph/nn/tests/test_gatconv.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-09 08:13 easygraph/nn/tests/test_gcnconv.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-09 08:13 easygraph/nn/tests/test_graphsageconv.py
+-rw-rw-rw-  2.0 fat      445 b- defN 24-Apr-09 08:13 easygraph/nn/tests/test_regularization.py
+-rw-rw-rw-  2.0 fat      381 b- defN 24-Apr-09 08:13 easygraph/readwrite/__init__.py
+-rw-rw-rw-  2.0 fat    14015 b- defN 24-Apr-09 08:13 easygraph/readwrite/edgelist.py
+-rw-rw-rw-  2.0 fat    39067 b- defN 24-Apr-09 08:13 easygraph/readwrite/gexf.py
+-rw-rw-rw-  2.0 fat    29129 b- defN 24-Apr-09 08:13 easygraph/readwrite/gml.py
+-rw-rw-rw-  2.0 fat    40222 b- defN 24-Apr-09 08:13 easygraph/readwrite/graphml.py
+-rw-rw-rw-  2.0 fat     5218 b- defN 24-Apr-09 08:13 easygraph/readwrite/graphviz.py
+-rw-rw-rw-  2.0 fat    11130 b- defN 24-Apr-09 08:13 easygraph/readwrite/pajek.py
+-rw-rw-rw-  2.0 fat      274 b- defN 24-Apr-09 08:13 easygraph/readwrite/pickle.py
+-rw-rw-rw-  2.0 fat    10529 b- defN 24-Apr-09 08:13 easygraph/readwrite/ucinet.py
+-rw-rw-rw-  2.0 fat      537 b- defN 24-Apr-09 08:13 easygraph/readwrite/json_graph/__init__.py
+-rw-rw-rw-  2.0 fat     3442 b- defN 24-Apr-09 08:13 easygraph/readwrite/json_graph/node_link.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-09 08:13 easygraph/readwrite/tests/__init__.py
+-rw-rw-rw-  2.0 fat    10380 b- defN 24-Apr-09 08:13 easygraph/readwrite/tests/test_edgelist.py
+-rw-rw-rw-  2.0 fat    15678 b- defN 24-Apr-09 08:13 easygraph/readwrite/tests/test_gexf.py
+-rw-rw-rw-  2.0 fat    18005 b- defN 24-Apr-09 08:13 easygraph/readwrite/tests/test_gml.py
+-rw-rw-rw-  2.0 fat    66669 b- defN 24-Apr-09 08:13 easygraph/readwrite/tests/test_graphml.py
+-rw-rw-rw-  2.0 fat     1504 b- defN 24-Apr-09 08:13 easygraph/readwrite/tests/test_graphviz.py
+-rw-rw-rw-  2.0 fat     9560 b- defN 24-Apr-09 08:13 easygraph/readwrite/tests/test_pajek.py
+-rw-rw-rw-  2.0 fat     2230 b- defN 24-Apr-09 08:13 easygraph/readwrite/tests/test_pickle.py
+-rw-rw-rw-  2.0 fat     8384 b- defN 24-Apr-09 08:13 easygraph/readwrite/tests/test_ucinet.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-09 08:13 easygraph/tests/__init__.py
+-rw-rw-rw-  2.0 fat     7380 b- defN 24-Apr-09 08:13 easygraph/tests/script_test_cpp_easygraph.py
+-rw-rw-rw-  2.0 fat     5871 b- defN 24-Apr-09 08:13 easygraph/tests/teddy_test_cpp_easygraph_sanity_check.py
+-rw-rw-rw-  2.0 fat     3142 b- defN 24-Apr-09 08:13 easygraph/tests/test_convert.py
+-rw-rw-rw-  2.0 fat      314 b- defN 24-Apr-09 08:13 easygraph/tests/test_cpp_easygraph.py
+-rw-rw-rw-  2.0 fat      538 b- defN 24-Apr-09 08:13 easygraph/utils/__init__.py
+-rw-rw-rw-  2.0 fat     2714 b- defN 24-Apr-09 08:13 easygraph/utils/alias.py
+-rw-rw-rw-  2.0 fat      535 b- defN 24-Apr-09 08:13 easygraph/utils/convert_class.py
+-rw-rw-rw-  2.0 fat    35872 b- defN 24-Apr-09 08:13 easygraph/utils/convert_to_matrix.py
+-rw-rw-rw-  2.0 fat    43149 b- defN 24-Apr-09 08:13 easygraph/utils/decorators.py
+-rw-rw-rw-  2.0 fat     2647 b- defN 24-Apr-09 08:13 easygraph/utils/download.py
+-rw-rw-rw-  2.0 fat     1061 b- defN 24-Apr-09 08:13 easygraph/utils/exception.py
+-rw-rw-rw-  2.0 fat      303 b- defN 24-Apr-09 08:13 easygraph/utils/index_of_node.py
+-rw-rw-rw-  2.0 fat     1387 b- defN 24-Apr-09 08:13 easygraph/utils/logging.py
+-rw-rw-rw-  2.0 fat     6203 b- defN 24-Apr-09 08:13 easygraph/utils/mapped_queue.py
+-rw-rw-rw-  2.0 fat     6068 b- defN 24-Apr-09 08:13 easygraph/utils/misc.py
+-rw-rw-rw-  2.0 fat     3768 b- defN 24-Apr-09 08:13 easygraph/utils/relabel.py
+-rw-rw-rw-  2.0 fat     1255 b- defN 24-Apr-09 08:13 easygraph/utils/sparse.py
+-rw-rw-rw-  2.0 fat     4532 b- defN 24-Apr-09 08:13 easygraph/utils/type_change.py
+-rw-rw-rw-  2.0 fat     1590 b- defN 24-Apr-09 08:21 Python_EasyGraph-1.2.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     1652 b- defN 24-Apr-09 08:21 Python_EasyGraph-1.2.dist-info/LICENSE.NetworkX
+-rw-rw-rw-  2.0 fat     8230 b- defN 24-Apr-09 08:21 Python_EasyGraph-1.2.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 24-Apr-09 08:21 Python_EasyGraph-1.2.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       24 b- defN 24-Apr-09 08:21 Python_EasyGraph-1.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat    21663 b- defN 24-Apr-09 08:21 Python_EasyGraph-1.2.dist-info/RECORD
+225 files, 2156020 bytes uncompressed, 610337 bytes compressed:  71.7%
```

## zipnote {}

```diff
@@ -1,799 +1,676 @@
-Filename: Python_EasyGraph-1.1.dist-info/
+Filename: cpp_easygraph.cp39-win_amd64.pyd
 Comment: 
 
-Filename: Python_EasyGraph.libs/
-Comment: 
-
-Filename: easygraph/
-Comment: 
-
-Filename: cpp_easygraph.pypy39-pp73-x86_64-linux-gnu.so
-Comment: 
-
-Filename: Python_EasyGraph-1.1.dist-info/top_level.txt
-Comment: 
-
-Filename: Python_EasyGraph-1.1.dist-info/WHEEL
-Comment: 
-
-Filename: Python_EasyGraph-1.1.dist-info/RECORD
-Comment: 
-
-Filename: Python_EasyGraph-1.1.dist-info/LICENSE
-Comment: 
-
-Filename: Python_EasyGraph-1.1.dist-info/LICENSE.NetworkX
-Comment: 
-
-Filename: Python_EasyGraph-1.1.dist-info/METADATA
-Comment: 
-
-Filename: easygraph/readwrite/
-Comment: 
-
-Filename: easygraph/ml_metrics/
-Comment: 
-
-Filename: easygraph/model/
-Comment: 
-
-Filename: easygraph/tests/
-Comment: 
-
-Filename: easygraph/classes/
-Comment: 
-
-Filename: easygraph/datapipe/
-Comment: 
-
-Filename: easygraph/datasets/
-Comment: 
-
-Filename: easygraph/functions/
-Comment: 
-
-Filename: easygraph/utils/
-Comment: 
-
-Filename: easygraph/experiments/
-Comment: 
-
-Filename: easygraph/nn/
+Filename: easygraph/__init__.py
 Comment: 
 
 Filename: easygraph/_global.py
 Comment: 
 
-Filename: easygraph/exception.py
-Comment: 
-
 Filename: easygraph/convert.py
 Comment: 
 
-Filename: easygraph/__init__.py
+Filename: easygraph/exception.py
 Comment: 
 
-Filename: easygraph/readwrite/tests/
+Filename: easygraph/classes/__init__.py
 Comment: 
 
-Filename: easygraph/readwrite/json_graph/
+Filename: easygraph/classes/base.py
 Comment: 
 
-Filename: easygraph/readwrite/ucinet.py
+Filename: easygraph/classes/directed_graph.py
 Comment: 
 
-Filename: easygraph/readwrite/gexf.py
+Filename: easygraph/classes/directed_multigraph.py
 Comment: 
 
-Filename: easygraph/readwrite/gml.py
+Filename: easygraph/classes/graph.py
 Comment: 
 
-Filename: easygraph/readwrite/graphviz.py
+Filename: easygraph/classes/graphviews.py
 Comment: 
 
-Filename: easygraph/readwrite/pickle.py
+Filename: easygraph/classes/hypergraph.py
 Comment: 
 
-Filename: easygraph/readwrite/pajek.py
+Filename: easygraph/classes/multigraph.py
 Comment: 
 
-Filename: easygraph/readwrite/graphml.py
+Filename: easygraph/classes/operation.py
 Comment: 
 
-Filename: easygraph/readwrite/edgelist.py
+Filename: easygraph/classes/tests/__init__.py
 Comment: 
 
-Filename: easygraph/readwrite/__init__.py
+Filename: easygraph/classes/tests/test_hypergraph.py
 Comment: 
 
-Filename: easygraph/readwrite/tests/test_gml.py
+Filename: easygraph/classes/tests/test_multidigraph.py
 Comment: 
 
-Filename: easygraph/readwrite/tests/test_graphviz.py
+Filename: easygraph/classes/tests/test_multigraph.py
 Comment: 
 
-Filename: easygraph/readwrite/tests/test_gexf.py
+Filename: easygraph/classes/tests/test_operation.py
 Comment: 
 
-Filename: easygraph/readwrite/tests/test_ucinet.py
+Filename: easygraph/datapipe/__init__.py
 Comment: 
 
-Filename: easygraph/readwrite/tests/test_pickle.py
+Filename: easygraph/datapipe/common.py
 Comment: 
 
-Filename: easygraph/readwrite/tests/test_edgelist.py
+Filename: easygraph/datapipe/loader.py
 Comment: 
 
-Filename: easygraph/readwrite/tests/test_pajek.py
+Filename: easygraph/datapipe/normalize.py
 Comment: 
 
-Filename: easygraph/readwrite/tests/test_graphml.py
+Filename: easygraph/datasets/__init__.py
 Comment: 
 
-Filename: easygraph/readwrite/tests/__init__.py
+Filename: easygraph/datasets/citation_graph.py
 Comment: 
 
-Filename: easygraph/readwrite/json_graph/node_link.py
+Filename: easygraph/datasets/get_sample_graph.py
 Comment: 
 
-Filename: easygraph/readwrite/json_graph/__init__.py
+Filename: easygraph/datasets/gnn_benchmark.py
 Comment: 
 
-Filename: easygraph/ml_metrics/hypergraphs/
+Filename: easygraph/datasets/graph_dataset_base.py
 Comment: 
 
-Filename: easygraph/ml_metrics/base.py
+Filename: easygraph/datasets/karate.py
 Comment: 
 
-Filename: easygraph/ml_metrics/classification.py
+Filename: easygraph/datasets/ppi.py
 Comment: 
 
-Filename: easygraph/ml_metrics/__init__.py
+Filename: easygraph/datasets/utils.py
 Comment: 
 
-Filename: easygraph/ml_metrics/hypergraphs/hypergraph.py
+Filename: easygraph/datasets/dynamic/__init__.py
 Comment: 
 
-Filename: easygraph/ml_metrics/hypergraphs/__init__.py
+Filename: easygraph/datasets/dynamic/email_enron.py
 Comment: 
 
-Filename: easygraph/model/hypergraphs/
+Filename: easygraph/datasets/dynamic/email_eu.py
 Comment: 
 
-Filename: easygraph/model/__init__.py
+Filename: easygraph/datasets/dynamic/hospital_lyon.py
 Comment: 
 
-Filename: easygraph/model/hypergraphs/setgnn.py
+Filename: easygraph/datasets/dynamic/load_dataset.py
 Comment: 
 
-Filename: easygraph/model/hypergraphs/hgnn.py
+Filename: easygraph/datasets/hypergraph/House_Committees.py
 Comment: 
 
-Filename: easygraph/model/hypergraphs/hypergcn.py
+Filename: easygraph/datasets/hypergraph/Yelp.py
 Comment: 
 
-Filename: easygraph/model/hypergraphs/dhne.py
+Filename: easygraph/datasets/hypergraph/__init__.py
 Comment: 
 
-Filename: easygraph/model/hypergraphs/dhcf.py
+Filename: easygraph/datasets/hypergraph/_global.py
 Comment: 
 
-Filename: easygraph/model/hypergraphs/unignn.py
+Filename: easygraph/datasets/hypergraph/cat_edge_Cooking.py
 Comment: 
 
-Filename: easygraph/model/hypergraphs/__init__.py
+Filename: easygraph/datasets/hypergraph/coauthorship.py
 Comment: 
 
-Filename: easygraph/model/hypergraphs/hgnnp.py
+Filename: easygraph/datasets/hypergraph/cocitation.py
 Comment: 
 
-Filename: easygraph/model/hypergraphs/hnhn.py
+Filename: easygraph/datasets/hypergraph/contact_primary_school.py
 Comment: 
 
-Filename: easygraph/tests/test_convert.py
+Filename: easygraph/datasets/hypergraph/cooking_200.py
 Comment: 
 
-Filename: easygraph/tests/teddy_test_cpp_easygraph_sanity_check.py
+Filename: easygraph/datasets/hypergraph/hypergraph_dataset_base.py
 Comment: 
 
-Filename: easygraph/tests/test_cpp_easygraph.py
+Filename: easygraph/datasets/hypergraph/mathoverflow_answers.py
 Comment: 
 
-Filename: easygraph/tests/script_test_cpp_easygraph.py
+Filename: easygraph/datasets/hypergraph/senate_committees.py
 Comment: 
 
-Filename: easygraph/tests/__init__.py
+Filename: easygraph/datasets/hypergraph/trivago_clicks.py
 Comment: 
 
-Filename: easygraph/classes/tests/
+Filename: easygraph/datasets/hypergraph/walmart_trips.py
 Comment: 
 
-Filename: easygraph/classes/hypergraph.py
+Filename: easygraph/experiments/__init__.py
 Comment: 
 
-Filename: easygraph/classes/graphviews.py
+Filename: easygraph/experiments/base.py
 Comment: 
 
-Filename: easygraph/classes/base.py
+Filename: easygraph/experiments/vertex_classification.py
 Comment: 
 
-Filename: easygraph/classes/graph.py
+Filename: easygraph/experiments/hypergraphs/__init__.py
 Comment: 
 
-Filename: easygraph/classes/multigraph.py
+Filename: easygraph/experiments/hypergraphs/hypergraph.py
 Comment: 
 
-Filename: easygraph/classes/directed_multigraph.py
+Filename: easygraph/functions/__init__.py
 Comment: 
 
-Filename: easygraph/classes/directed_graph.py
+Filename: easygraph/functions/isolate.py
 Comment: 
 
-Filename: easygraph/classes/operation.py
+Filename: easygraph/functions/basic/__init__.py
 Comment: 
 
-Filename: easygraph/classes/__init__.py
+Filename: easygraph/functions/basic/avg_degree.py
 Comment: 
 
-Filename: easygraph/classes/tests/test_multidigraph.py
+Filename: easygraph/functions/basic/cluster.py
 Comment: 
 
-Filename: easygraph/classes/tests/test_multigraph.py
+Filename: easygraph/functions/basic/localassort.py
 Comment: 
 
-Filename: easygraph/classes/tests/test_hypergraph.py
+Filename: easygraph/functions/basic/predecessor_path_based.py
 Comment: 
 
-Filename: easygraph/classes/tests/test_operation.py
+Filename: easygraph/functions/basic/tests/__init__.py
 Comment: 
 
-Filename: easygraph/classes/tests/__init__.py
+Filename: easygraph/functions/basic/tests/test_avg_degree.py
 Comment: 
 
-Filename: easygraph/datapipe/common.py
+Filename: easygraph/functions/basic/tests/test_cluster.py
 Comment: 
 
-Filename: easygraph/datapipe/normalize.py
+Filename: easygraph/functions/basic/tests/test_localassort.py
 Comment: 
 
-Filename: easygraph/datapipe/loader.py
+Filename: easygraph/functions/basic/tests/test_predecessor.py
 Comment: 
 
-Filename: easygraph/datapipe/__init__.py
+Filename: easygraph/functions/centrality/__init__.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/
+Filename: easygraph/functions/centrality/betweenness.py
 Comment: 
 
-Filename: easygraph/datasets/dynamic/
+Filename: easygraph/functions/centrality/closeness.py
 Comment: 
 
-Filename: easygraph/datasets/ppi.py
+Filename: easygraph/functions/centrality/degree.py
 Comment: 
 
-Filename: easygraph/datasets/karate.py
+Filename: easygraph/functions/centrality/ego_betweenness.py
 Comment: 
 
-Filename: easygraph/datasets/utils.py
+Filename: easygraph/functions/centrality/flowbetweenness.py
 Comment: 
 
-Filename: easygraph/datasets/citation_graph.py
+Filename: easygraph/functions/centrality/laplacian.py
 Comment: 
 
-Filename: easygraph/datasets/gnn_benchmark.py
+Filename: easygraph/functions/centrality/pagerank.py
 Comment: 
 
-Filename: easygraph/datasets/get_sample_graph.py
+Filename: easygraph/functions/community/LPA.py
 Comment: 
 
-Filename: easygraph/datasets/graph_dataset_base.py
+Filename: easygraph/functions/community/__init__.py
 Comment: 
 
-Filename: easygraph/datasets/__init__.py
+Filename: easygraph/functions/community/ego_graph.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/House_Committees.py
+Filename: easygraph/functions/community/louvain.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/_global.py
+Filename: easygraph/functions/community/modularity.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/senate_committees.py
+Filename: easygraph/functions/community/modularity_max_detection.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/cocitation.py
+Filename: easygraph/functions/community/motif.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/mathoverflow_answers.py
+Filename: easygraph/functions/community/tests/__init__.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/cat_edge_Cooking.py
+Filename: easygraph/functions/community/tests/test_motif.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/Yelp.py
+Filename: easygraph/functions/components/__init__.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/walmart_trips.py
+Filename: easygraph/functions/components/biconnected.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/contact_primary_school.py
+Filename: easygraph/functions/components/connected.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/hypergraph_dataset_base.py
+Filename: easygraph/functions/components/strongly_connected.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/cooking_200.py
+Filename: easygraph/functions/components/weakly_connected.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/trivago_clicks.py
+Filename: easygraph/functions/core/__init__.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/__init__.py
+Filename: easygraph/functions/core/k_core.py
 Comment: 
 
-Filename: easygraph/datasets/hypergraph/coauthorship.py
+Filename: easygraph/functions/drawing/__init__.py
 Comment: 
 
-Filename: easygraph/datasets/dynamic/hospital_lyon.py
+Filename: easygraph/functions/drawing/defaults.py
 Comment: 
 
-Filename: easygraph/datasets/dynamic/load_dataset.py
+Filename: easygraph/functions/drawing/drawing.py
 Comment: 
 
-Filename: easygraph/datasets/dynamic/email_enron.py
+Filename: easygraph/functions/drawing/geometry.py
 Comment: 
 
-Filename: easygraph/datasets/dynamic/__init__.py
+Filename: easygraph/functions/drawing/layout.py
 Comment: 
 
-Filename: easygraph/datasets/dynamic/email_eu.py
+Filename: easygraph/functions/drawing/plot.py
 Comment: 
 
-Filename: easygraph/functions/centrality/
+Filename: easygraph/functions/drawing/positioning.py
 Comment: 
 
-Filename: easygraph/functions/basic/
+Filename: easygraph/functions/drawing/simulator.py
 Comment: 
 
-Filename: easygraph/functions/graph_embedding/
+Filename: easygraph/functions/drawing/utils.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/
+Filename: easygraph/functions/graph_embedding/NOBE.py
 Comment: 
 
-Filename: easygraph/functions/path/
+Filename: easygraph/functions/graph_embedding/__init__.py
 Comment: 
 
-Filename: easygraph/functions/community/
+Filename: easygraph/functions/graph_embedding/deepwalk.py
 Comment: 
 
-Filename: easygraph/functions/components/
+Filename: easygraph/functions/graph_embedding/line.py
 Comment: 
 
-Filename: easygraph/functions/drawing/
+Filename: easygraph/functions/graph_embedding/net_emb_example_citeseer.py
 Comment: 
 
-Filename: easygraph/functions/structural_holes/
+Filename: easygraph/functions/graph_embedding/node2vec.py
 Comment: 
 
-Filename: easygraph/functions/graph_generator/
+Filename: easygraph/functions/graph_embedding/sdne.py
 Comment: 
 
-Filename: easygraph/functions/core/
+Filename: easygraph/functions/graph_generator/RandomNetwork.py
 Comment: 
 
-Filename: easygraph/functions/isolate.py
+Filename: easygraph/functions/graph_generator/__init__.py
 Comment: 
 
-Filename: easygraph/functions/__init__.py
+Filename: easygraph/functions/graph_generator/classic.py
 Comment: 
 
-Filename: easygraph/functions/centrality/closeness.py
+Filename: easygraph/functions/hypergraph/__init__.py
 Comment: 
 
-Filename: easygraph/functions/centrality/pagerank.py
+Filename: easygraph/functions/hypergraph/assortativity.py
 Comment: 
 
-Filename: easygraph/functions/centrality/laplacian.py
+Filename: easygraph/functions/hypergraph/hypergraph_clustering.py
 Comment: 
 
-Filename: easygraph/functions/centrality/betweenness.py
+Filename: easygraph/functions/hypergraph/hypergraph_operation.py
 Comment: 
 
-Filename: easygraph/functions/centrality/flowbetweenness.py
+Filename: easygraph/functions/hypergraph/centrality/__init__.py
 Comment: 
 
-Filename: easygraph/functions/centrality/degree.py
+Filename: easygraph/functions/hypergraph/centrality/cycle_ratio.py
 Comment: 
 
-Filename: easygraph/functions/centrality/ego_betweenness.py
+Filename: easygraph/functions/hypergraph/centrality/degree.py
 Comment: 
 
-Filename: easygraph/functions/centrality/__init__.py
+Filename: easygraph/functions/hypergraph/centrality/hypercoreness.py
 Comment: 
 
-Filename: easygraph/functions/basic/tests/
+Filename: easygraph/functions/hypergraph/centrality/s_centrality.py
 Comment: 
 
-Filename: easygraph/functions/basic/avg_degree.py
+Filename: easygraph/functions/hypergraph/centrality/vector_centrality.py
 Comment: 
 
-Filename: easygraph/functions/basic/localassort.py
+Filename: easygraph/functions/hypergraph/null_model/__init__.py
 Comment: 
 
-Filename: easygraph/functions/basic/predecessor_path_based.py
+Filename: easygraph/functions/hypergraph/null_model/hypergraph_classic.py
 Comment: 
 
-Filename: easygraph/functions/basic/cluster.py
+Filename: easygraph/functions/hypergraph/null_model/lattice.py
 Comment: 
 
-Filename: easygraph/functions/basic/__init__.py
+Filename: easygraph/functions/hypergraph/null_model/random.py
 Comment: 
 
-Filename: easygraph/functions/basic/tests/test_avg_degree.py
+Filename: easygraph/functions/hypergraph/null_model/simple.py
 Comment: 
 
-Filename: easygraph/functions/basic/tests/test_predecessor.py
+Filename: easygraph/functions/hypergraph/null_model/uniform.py
 Comment: 
 
-Filename: easygraph/functions/basic/tests/test_cluster.py
+Filename: easygraph/functions/hypergraph/null_model/tests/__init__.py
 Comment: 
 
-Filename: easygraph/functions/basic/tests/test_localassort.py
+Filename: easygraph/functions/hypergraph/null_model/tests/test_classic.py
 Comment: 
 
-Filename: easygraph/functions/basic/tests/__init__.py
+Filename: easygraph/functions/path/__init__.py
 Comment: 
 
-Filename: easygraph/functions/graph_embedding/line.py
+Filename: easygraph/functions/path/average_shortest_path_length.py
 Comment: 
 
-Filename: easygraph/functions/graph_embedding/sdne.py
+Filename: easygraph/functions/path/bridges.py
 Comment: 
 
-Filename: easygraph/functions/graph_embedding/deepwalk.py
+Filename: easygraph/functions/path/diameter.py
 Comment: 
 
-Filename: easygraph/functions/graph_embedding/node2vec.py
+Filename: easygraph/functions/path/mst.py
 Comment: 
 
-Filename: easygraph/functions/graph_embedding/NOBE.py
+Filename: easygraph/functions/path/path.py
 Comment: 
 
-Filename: easygraph/functions/graph_embedding/__init__.py
+Filename: easygraph/functions/structural_holes/AP_Greedy.py
 Comment: 
 
-Filename: easygraph/functions/graph_embedding/net_emb_example_citeseer.py
+Filename: easygraph/functions/structural_holes/HAM.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/centrality/
+Filename: easygraph/functions/structural_holes/HIS.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/hypergraph_generator/
+Filename: easygraph/functions/structural_holes/ICC.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/hypergraph_operation.py
+Filename: easygraph/functions/structural_holes/MaxD.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/hypergraph_clustering.py
+Filename: easygraph/functions/structural_holes/NOBE.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/assortativity.py
+Filename: easygraph/functions/structural_holes/SHII_metric.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/__init__.py
+Filename: easygraph/functions/structural_holes/__init__.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/centrality/cycle_ratio.py
+Filename: easygraph/functions/structural_holes/evaluation.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/centrality/hypercoreness.py
+Filename: easygraph/functions/structural_holes/maxBlock.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/centrality/vector_centrality.py
+Filename: easygraph/functions/structural_holes/metrics.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/centrality/s_centrality.py
+Filename: easygraph/functions/structural_holes/weakTie.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/centrality/degree.py
+Filename: easygraph/ml_metrics/__init__.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/centrality/__init__.py
+Filename: easygraph/ml_metrics/base.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/hypergraph_generator/tests/
+Filename: easygraph/ml_metrics/classification.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/hypergraph_generator/simple.py
+Filename: easygraph/ml_metrics/hypergraphs/__init__.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/hypergraph_generator/uniform.py
+Filename: easygraph/ml_metrics/hypergraphs/hypergraph.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/hypergraph_generator/lattice.py
+Filename: easygraph/model/__init__.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/hypergraph_generator/random.py
+Filename: easygraph/model/hypergraphs/__init__.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/hypergraph_generator/hypergraph_classic.py
+Filename: easygraph/model/hypergraphs/dhcf.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/hypergraph_generator/__init__.py
+Filename: easygraph/model/hypergraphs/dhne.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/hypergraph_generator/tests/test_classic.py
+Filename: easygraph/model/hypergraphs/hgnn.py
 Comment: 
 
-Filename: easygraph/functions/hypergraph/hypergraph_generator/tests/__init__.py
+Filename: easygraph/model/hypergraphs/hgnnp.py
 Comment: 
 
-Filename: easygraph/functions/path/diameter.py
+Filename: easygraph/model/hypergraphs/hnhn.py
 Comment: 
 
-Filename: easygraph/functions/path/mst.py
+Filename: easygraph/model/hypergraphs/hypergcn.py
 Comment: 
 
-Filename: easygraph/functions/path/bridges.py
+Filename: easygraph/model/hypergraphs/setgnn.py
 Comment: 
 
-Filename: easygraph/functions/path/average_shortest_path_length.py
+Filename: easygraph/model/hypergraphs/unignn.py
 Comment: 
 
-Filename: easygraph/functions/path/path.py
+Filename: easygraph/nn/__init__.py
 Comment: 
 
-Filename: easygraph/functions/path/__init__.py
+Filename: easygraph/nn/loss.py
 Comment: 
 
-Filename: easygraph/functions/community/tests/
+Filename: easygraph/nn/regularization.py
 Comment: 
 
-Filename: easygraph/functions/community/motif.py
+Filename: easygraph/nn/convs/__init__.py
 Comment: 
 
-Filename: easygraph/functions/community/louvain.py
+Filename: easygraph/nn/convs/common.py
 Comment: 
 
-Filename: easygraph/functions/community/ego_graph.py
+Filename: easygraph/nn/convs/pma.py
 Comment: 
 
-Filename: easygraph/functions/community/LPA.py
+Filename: easygraph/nn/convs/hypergraphs/__init__.py
 Comment: 
 
-Filename: easygraph/functions/community/modularity.py
+Filename: easygraph/nn/convs/hypergraphs/dhcf_conv.py
 Comment: 
 
-Filename: easygraph/functions/community/modularity_max_detection.py
+Filename: easygraph/nn/convs/hypergraphs/halfnlh_conv.py
 Comment: 
 
-Filename: easygraph/functions/community/__init__.py
+Filename: easygraph/nn/convs/hypergraphs/hgnn_conv.py
 Comment: 
 
-Filename: easygraph/functions/community/tests/test_motif.py
+Filename: easygraph/nn/convs/hypergraphs/hgnnp_conv.py
 Comment: 
 
-Filename: easygraph/functions/community/tests/__init__.py
+Filename: easygraph/nn/convs/hypergraphs/hnhn_conv.py
 Comment: 
 
-Filename: easygraph/functions/components/connected.py
+Filename: easygraph/nn/convs/hypergraphs/hypergcn_conv.py
 Comment: 
 
-Filename: easygraph/functions/components/weakly_connected.py
+Filename: easygraph/nn/convs/hypergraphs/unignn_conv.py
 Comment: 
 
-Filename: easygraph/functions/components/strongly_connected.py
+Filename: easygraph/nn/tests/__init__.py
 Comment: 
 
-Filename: easygraph/functions/components/biconnected.py
+Filename: easygraph/nn/tests/test_gatconv.py
 Comment: 
 
-Filename: easygraph/functions/components/__init__.py
+Filename: easygraph/nn/tests/test_gcnconv.py
 Comment: 
 
-Filename: easygraph/functions/drawing/defaults.py
+Filename: easygraph/nn/tests/test_graphsageconv.py
 Comment: 
 
-Filename: easygraph/functions/drawing/positioning.py
+Filename: easygraph/nn/tests/test_regularization.py
 Comment: 
 
-Filename: easygraph/functions/drawing/utils.py
+Filename: easygraph/readwrite/__init__.py
 Comment: 
 
-Filename: easygraph/functions/drawing/plot.py
+Filename: easygraph/readwrite/edgelist.py
 Comment: 
 
-Filename: easygraph/functions/drawing/simulator.py
+Filename: easygraph/readwrite/gexf.py
 Comment: 
 
-Filename: easygraph/functions/drawing/drawing.py
+Filename: easygraph/readwrite/gml.py
 Comment: 
 
-Filename: easygraph/functions/drawing/layout.py
+Filename: easygraph/readwrite/graphml.py
 Comment: 
 
-Filename: easygraph/functions/drawing/geometry.py
+Filename: easygraph/readwrite/graphviz.py
 Comment: 
 
-Filename: easygraph/functions/drawing/__init__.py
+Filename: easygraph/readwrite/pajek.py
 Comment: 
 
-Filename: easygraph/functions/structural_holes/MaxD.py
+Filename: easygraph/readwrite/pickle.py
 Comment: 
 
-Filename: easygraph/functions/structural_holes/maxBlock.py
+Filename: easygraph/readwrite/ucinet.py
 Comment: 
 
-Filename: easygraph/functions/structural_holes/ICC.py
+Filename: easygraph/readwrite/json_graph/__init__.py
 Comment: 
 
-Filename: easygraph/functions/structural_holes/metrics.py
+Filename: easygraph/readwrite/json_graph/node_link.py
 Comment: 
 
-Filename: easygraph/functions/structural_holes/HIS.py
+Filename: easygraph/readwrite/tests/__init__.py
 Comment: 
 
-Filename: easygraph/functions/structural_holes/weakTie.py
+Filename: easygraph/readwrite/tests/test_edgelist.py
 Comment: 
 
-Filename: easygraph/functions/structural_holes/evaluation.py
+Filename: easygraph/readwrite/tests/test_gexf.py
 Comment: 
 
-Filename: easygraph/functions/structural_holes/SHII_metric.py
+Filename: easygraph/readwrite/tests/test_gml.py
 Comment: 
 
-Filename: easygraph/functions/structural_holes/AP_Greedy.py
+Filename: easygraph/readwrite/tests/test_graphml.py
 Comment: 
 
-Filename: easygraph/functions/structural_holes/NOBE.py
+Filename: easygraph/readwrite/tests/test_graphviz.py
 Comment: 
 
-Filename: easygraph/functions/structural_holes/HAM.py
+Filename: easygraph/readwrite/tests/test_pajek.py
 Comment: 
 
-Filename: easygraph/functions/structural_holes/__init__.py
+Filename: easygraph/readwrite/tests/test_pickle.py
 Comment: 
 
-Filename: easygraph/functions/graph_generator/classic.py
+Filename: easygraph/readwrite/tests/test_ucinet.py
 Comment: 
 
-Filename: easygraph/functions/graph_generator/RandomNetwork.py
+Filename: easygraph/tests/__init__.py
 Comment: 
 
-Filename: easygraph/functions/graph_generator/__init__.py
+Filename: easygraph/tests/script_test_cpp_easygraph.py
 Comment: 
 
-Filename: easygraph/functions/core/k_core.py
+Filename: easygraph/tests/teddy_test_cpp_easygraph_sanity_check.py
 Comment: 
 
-Filename: easygraph/functions/core/__init__.py
+Filename: easygraph/tests/test_convert.py
 Comment: 
 
-Filename: easygraph/utils/convert_to_matrix.py
+Filename: easygraph/tests/test_cpp_easygraph.py
 Comment: 
 
-Filename: easygraph/utils/exception.py
+Filename: easygraph/utils/__init__.py
 Comment: 
 
-Filename: easygraph/utils/index_of_node.py
+Filename: easygraph/utils/alias.py
 Comment: 
 
-Filename: easygraph/utils/relabel.py
+Filename: easygraph/utils/convert_class.py
 Comment: 
 
-Filename: easygraph/utils/mapped_queue.py
+Filename: easygraph/utils/convert_to_matrix.py
 Comment: 
 
 Filename: easygraph/utils/decorators.py
 Comment: 
 
-Filename: easygraph/utils/alias.py
+Filename: easygraph/utils/download.py
 Comment: 
 
-Filename: easygraph/utils/sparse.py
+Filename: easygraph/utils/exception.py
 Comment: 
 
-Filename: easygraph/utils/type_change.py
+Filename: easygraph/utils/index_of_node.py
 Comment: 
 
 Filename: easygraph/utils/logging.py
 Comment: 
 
-Filename: easygraph/utils/convert_class.py
+Filename: easygraph/utils/mapped_queue.py
 Comment: 
 
 Filename: easygraph/utils/misc.py
 Comment: 
 
-Filename: easygraph/utils/download.py
-Comment: 
-
-Filename: easygraph/utils/__init__.py
-Comment: 
-
-Filename: easygraph/experiments/hypergraphs/
-Comment: 
-
-Filename: easygraph/experiments/base.py
-Comment: 
-
-Filename: easygraph/experiments/vertex_classification.py
-Comment: 
-
-Filename: easygraph/experiments/__init__.py
-Comment: 
-
-Filename: easygraph/experiments/hypergraphs/hypergraph.py
-Comment: 
-
-Filename: easygraph/experiments/hypergraphs/__init__.py
-Comment: 
-
-Filename: easygraph/nn/tests/
-Comment: 
-
-Filename: easygraph/nn/convs/
-Comment: 
-
-Filename: easygraph/nn/loss.py
-Comment: 
-
-Filename: easygraph/nn/regularization.py
-Comment: 
-
-Filename: easygraph/nn/__init__.py
-Comment: 
-
-Filename: easygraph/nn/tests/test_regularization.py
-Comment: 
-
-Filename: easygraph/nn/tests/test_gcnconv.py
-Comment: 
-
-Filename: easygraph/nn/tests/test_gatconv.py
-Comment: 
-
-Filename: easygraph/nn/tests/test_graphsageconv.py
-Comment: 
-
-Filename: easygraph/nn/tests/__init__.py
-Comment: 
-
-Filename: easygraph/nn/convs/hypergraphs/
-Comment: 
-
-Filename: easygraph/nn/convs/common.py
-Comment: 
-
-Filename: easygraph/nn/convs/pma.py
-Comment: 
-
-Filename: easygraph/nn/convs/__init__.py
+Filename: easygraph/utils/relabel.py
 Comment: 
 
-Filename: easygraph/nn/convs/hypergraphs/unignn_conv.py
+Filename: easygraph/utils/sparse.py
 Comment: 
 
-Filename: easygraph/nn/convs/hypergraphs/hgnn_conv.py
+Filename: easygraph/utils/type_change.py
 Comment: 
 
-Filename: easygraph/nn/convs/hypergraphs/hnhn_conv.py
+Filename: Python_EasyGraph-1.2.dist-info/LICENSE
 Comment: 
 
-Filename: easygraph/nn/convs/hypergraphs/hgnnp_conv.py
+Filename: Python_EasyGraph-1.2.dist-info/LICENSE.NetworkX
 Comment: 
 
-Filename: easygraph/nn/convs/hypergraphs/hypergcn_conv.py
+Filename: Python_EasyGraph-1.2.dist-info/METADATA
 Comment: 
 
-Filename: easygraph/nn/convs/hypergraphs/dhcf_conv.py
+Filename: Python_EasyGraph-1.2.dist-info/WHEEL
 Comment: 
 
-Filename: easygraph/nn/convs/hypergraphs/halfnlh_conv.py
+Filename: Python_EasyGraph-1.2.dist-info/top_level.txt
 Comment: 
 
-Filename: easygraph/nn/convs/hypergraphs/__init__.py
+Filename: Python_EasyGraph-1.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## filetype from file(1)

```diff
@@ -1 +1 @@
-Zip archive data, at least v2.0 to extract, compression method=store
+Zip archive data, at least v2.0 to extract, compression method=deflate
```

## easygraph/_global.py

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-from pathlib import Path
-
-
-def get_eg_cache_root():
-    root = Path.home() / Path(".easygraph/")
-    root.mkdir(parents=True, exist_ok=True)
-    return root
-
-
-AUTHOR_EMAIL = "bdye22@m.fudan.edu.cn"
-# global paths
-CACHE_ROOT = get_eg_cache_root()
-DATASETS_ROOT = CACHE_ROOT / "datasets"
-REMOTE_ROOT = "https://download.moon-lab.tech:28501/"
-REMOTE_DATASETS_ROOT = REMOTE_ROOT + "datasets/"
+from pathlib import Path
+
+
+def get_eg_cache_root():
+    root = Path.home() / Path(".easygraph/")
+    root.mkdir(parents=True, exist_ok=True)
+    return root
+
+
+AUTHOR_EMAIL = "bdye22@m.fudan.edu.cn"
+# global paths
+CACHE_ROOT = get_eg_cache_root()
+DATASETS_ROOT = CACHE_ROOT / "datasets"
+REMOTE_ROOT = "https://download.moon-lab.tech:28501/"
+REMOTE_DATASETS_ROOT = REMOTE_ROOT + "datasets/"
```

## easygraph/exception.py

 * *Ordering differences only*

```diff
@@ -1,84 +1,84 @@
-"""
-**********
-Exceptions
-**********
-
-Base exceptions and errors for EasyGraph.
-"""
-
-__all__ = [
-    "HasACycle",
-    "NodeNotFound",
-    "EasyGraphAlgorithmError",
-    "EasyGraphException",
-    "EasyGraphError",
-    "EasyGraphNoCycle",
-    "EasyGraphNoPath",
-    "EasyGraphNotImplemented",
-    "EasyGraphPointlessConcept",
-    "EasyGraphUnbounded",
-    "EasyGraphUnfeasible",
-]
-
-
-class EasyGraphException(Exception):
-    """Base class for exceptions in EasyGraph."""
-
-
-class EasyGraphError(EasyGraphException):
-    """Exception for a serious error in EasyGraph"""
-
-
-class EasyGraphPointlessConcept(EasyGraphException):
-    """Raised when a null graph is provided as input to an algorithm
-    that cannot use it.
-
-    The null graph is sometimes considered a pointless concept [1]_,
-    thus the name of the exception.
-
-    References
-    ----------
-    .. [1] Harary, F. and Read, R. "Is the Null Graph a Pointless
-       Concept?"  In Graphs and Combinatorics Conference, George
-       Washington University.  New York: Springer-Verlag, 1973.
-
-    """
-
-
-class EasyGraphAlgorithmError(EasyGraphException):
-    """Exception for unexpected termination of algorithms."""
-
-
-class EasyGraphUnfeasible(EasyGraphAlgorithmError):
-    """Exception raised by algorithms trying to solve a problem
-    instance that has no feasible solution."""
-
-
-class EasyGraphNoPath(EasyGraphUnfeasible):
-    """Exception for algorithms that should return a path when running
-    on graphs where such a path does not exist."""
-
-
-class EasyGraphNoCycle(EasyGraphUnfeasible):
-    """Exception for algorithms that should return a cycle when running
-    on graphs where such a cycle does not exist."""
-
-
-class HasACycle(EasyGraphException):
-    """Raised if a graph has a cycle when an algorithm expects that it
-    will have no cycles.
-
-    """
-
-
-class EasyGraphUnbounded(EasyGraphAlgorithmError):
-    """Exception raised by algorithms trying to solve a maximization
-    or a minimization problem instance that is unbounded."""
-
-
-class EasyGraphNotImplemented(EasyGraphException):
-    """Exception raised by algorithms not implemented for a type of graph."""
-
-
-class NodeNotFound(EasyGraphException):
-    """Exception raised if requested node is not present in the graph"""
+"""
+**********
+Exceptions
+**********
+
+Base exceptions and errors for EasyGraph.
+"""
+
+__all__ = [
+    "HasACycle",
+    "NodeNotFound",
+    "EasyGraphAlgorithmError",
+    "EasyGraphException",
+    "EasyGraphError",
+    "EasyGraphNoCycle",
+    "EasyGraphNoPath",
+    "EasyGraphNotImplemented",
+    "EasyGraphPointlessConcept",
+    "EasyGraphUnbounded",
+    "EasyGraphUnfeasible",
+]
+
+
+class EasyGraphException(Exception):
+    """Base class for exceptions in EasyGraph."""
+
+
+class EasyGraphError(EasyGraphException):
+    """Exception for a serious error in EasyGraph"""
+
+
+class EasyGraphPointlessConcept(EasyGraphException):
+    """Raised when a null graph is provided as input to an algorithm
+    that cannot use it.
+
+    The null graph is sometimes considered a pointless concept [1]_,
+    thus the name of the exception.
+
+    References
+    ----------
+    .. [1] Harary, F. and Read, R. "Is the Null Graph a Pointless
+       Concept?"  In Graphs and Combinatorics Conference, George
+       Washington University.  New York: Springer-Verlag, 1973.
+
+    """
+
+
+class EasyGraphAlgorithmError(EasyGraphException):
+    """Exception for unexpected termination of algorithms."""
+
+
+class EasyGraphUnfeasible(EasyGraphAlgorithmError):
+    """Exception raised by algorithms trying to solve a problem
+    instance that has no feasible solution."""
+
+
+class EasyGraphNoPath(EasyGraphUnfeasible):
+    """Exception for algorithms that should return a path when running
+    on graphs where such a path does not exist."""
+
+
+class EasyGraphNoCycle(EasyGraphUnfeasible):
+    """Exception for algorithms that should return a cycle when running
+    on graphs where such a cycle does not exist."""
+
+
+class HasACycle(EasyGraphException):
+    """Raised if a graph has a cycle when an algorithm expects that it
+    will have no cycles.
+
+    """
+
+
+class EasyGraphUnbounded(EasyGraphAlgorithmError):
+    """Exception raised by algorithms trying to solve a maximization
+    or a minimization problem instance that is unbounded."""
+
+
+class EasyGraphNotImplemented(EasyGraphException):
+    """Exception raised by algorithms not implemented for a type of graph."""
+
+
+class NodeNotFound(EasyGraphException):
+    """Exception raised if requested node is not present in the graph"""
```

## easygraph/convert.py

 * *Ordering differences only*

```diff
@@ -1,591 +1,591 @@
-import warnings
-
-from collections.abc import Collection
-from collections.abc import Generator
-from collections.abc import Iterator
-from copy import deepcopy
-from typing import TYPE_CHECKING
-from typing import Any
-from typing import Iterable
-from typing import List
-from typing import Optional
-from typing import Union
-
-import easygraph as eg
-
-from easygraph.utils.exception import EasyGraphError
-
-
-if TYPE_CHECKING:
-    import dgl
-    import networkx as nx
-    import torch_geometric
-
-    from easygraph import DiGraph
-    from easygraph import Graph
-
-__all__ = [
-    "from_dict_of_dicts",
-    "to_easygraph_graph",
-    "from_edgelist",
-    "from_dict_of_lists",
-    "from_networkx",
-    "from_dgl",
-    "from_pyg",
-    "to_networkx",
-    "to_dgl",
-    "to_pyg",
-    "dict_to_hypergraph",
-]
-
-
-def to_easygraph_graph(data, create_using=None, multigraph_input=False):
-    """Make a EasyGraph graph from a known data structure.
-
-    The preferred way to call this is automatically
-    from the class constructor
-
-    >>> d = {0: {1: {"weight": 1}}}  # dict-of-dicts single edge (0,1)
-    >>> G = eg.Graph(d)
-
-    instead of the equivalent
-
-    >>> G = eg.from_dict_of_dicts(d)
-
-    Parameters
-    ----------
-    data : object to be converted
-
-        Current known types are:
-         any EasyGraph graph
-         dict-of-dicts
-         dict-of-lists
-         container (e.g. set, list, tuple) of edges
-         iterator (e.g. itertools.chain) that produces edges
-         generator of edges
-         Pandas DataFrame (row per edge)
-         numpy matrix
-         numpy ndarray
-         scipy sparse matrix
-         pygraphviz agraph
-
-    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
-        Graph type to create. If graph instance, then cleared before populated.
-
-    multigraph_input : bool (default False)
-        If True and  data is a dict_of_dicts,
-        try to create a multigraph assuming dict_of_dict_of_lists.
-        If data and create_using are both multigraphs then create
-        a multigraph from a multigraph.
-
-    """
-
-    # EasyGraph graph type
-    if hasattr(data, "adj"):
-        try:
-            result = from_dict_of_dicts(
-                data.adj,
-                create_using=create_using,
-                multigraph_input=data.is_multigraph(),
-            )
-            # data.graph should be dict-like
-            result.graph.update(data.graph)
-            # data.nodes should be dict-like
-            # result.add_node_from(data.nodes.items()) possible but
-            # for custom node_attr_dict_factory which may be hashable
-            # will be unexpected behavior
-            for n, dd in data.nodes.items():
-                result._node[n].update(dd)
-            return result
-        except Exception as err:
-            raise eg.EasyGraphError("Input is not a correct EasyGraph graph.") from err
-
-    # pygraphviz  agraph
-    if hasattr(data, "is_strict"):
-        try:
-            return eg.from_pyGraphviz_agraph(data, create_using=create_using)
-        except Exception as err:
-            raise eg.EasyGraphError("Input is not a correct pygraphviz graph.") from err
-
-    # dict of dicts/lists
-    if isinstance(data, dict):
-        try:
-            return from_dict_of_dicts(
-                data, create_using=create_using, multigraph_input=multigraph_input
-            )
-        except Exception as err:
-            if multigraph_input is True:
-                raise eg.EasyGraphError(
-                    f"converting multigraph_input raised:\n{type(err)}: {err}"
-                )
-            try:
-                return from_dict_of_lists(data, create_using=create_using)
-            except Exception as err:
-                raise TypeError("Input is not known type.") from err
-
-    # Pandas DataFrame
-    try:
-        import pandas as pd
-
-        if isinstance(data, pd.DataFrame):
-            if data.shape[0] == data.shape[1]:
-                try:
-                    return eg.from_pandas_adjacency(data, create_using=create_using)
-                except Exception as err:
-                    msg = "Input is not a correct Pandas DataFrame adjacency matrix."
-                    raise eg.EasyGraphError(msg) from err
-            else:
-                try:
-                    return eg.from_pandas_edgelist(
-                        data, edge_attr=True, create_using=create_using
-                    )
-                except Exception as err:
-                    msg = "Input is not a correct Pandas DataFrame adjacency edge-list."
-                    raise eg.EasyGraphError(msg) from err
-    except ImportError:
-        warnings.warn("pandas not found, skipping conversion test.", ImportWarning)
-
-    # numpy matrix or ndarray
-    try:
-        import numpy as np
-
-        if isinstance(data, np.ndarray):
-            try:
-                return eg.from_numpy_array(data, create_using=create_using)
-            except Exception as err:
-                raise eg.EasyGraphError(
-                    "Input is not a correct numpy matrix or array."
-                ) from err
-    except ImportError:
-        warnings.warn("numpy not found, skipping conversion test.", ImportWarning)
-
-    # scipy sparse matrix - any format
-    try:
-        if hasattr(data, "format"):
-            try:
-                return eg.from_scipy_sparse_matrix(data, create_using=create_using)
-            except Exception as err:
-                raise eg.EasyGraphError(
-                    "Input is not a correct scipy sparse matrix type."
-                ) from err
-    except ImportError:
-        warnings.warn("scipy not found, skipping conversion test.", ImportWarning)
-
-    # Note: most general check - should remain last in order of execution
-    # Includes containers (e.g. list, set, dict, etc.), generators, and
-    # iterators (e.g. itertools.chain) of edges
-
-    if isinstance(data, (Collection, Generator, Iterator)):
-        try:
-            return from_edgelist(data, create_using=create_using)
-        except Exception as err:
-            raise eg.EasyGraphError("Input is not a valid edge list") from err
-
-    raise eg.EasyGraphError("Input is not a known data type for conversion.")
-
-
-def from_dict_of_lists(d, create_using=None):
-    G = eg.empty_graph(0, create_using)
-    G.add_nodes_from(d)
-    if G.is_multigraph() and not G.is_directed():
-        # a dict_of_lists can't show multiedges.  BUT for undirected graphs,
-        # each edge shows up twice in the dict_of_lists.
-        # So we need to treat this case separately.
-        seen = {}
-        for node, nbrlist in d.items():
-            for nbr in nbrlist:
-                if nbr not in seen:
-                    G.add_edge(node, nbr)
-            seen[node] = 1  # don't allow reverse edge to show up
-    else:
-        G.add_edges_from(
-            ((node, nbr) for node, nbrlist in d.items() for nbr in nbrlist)
-        )
-    return G
-
-
-def from_dict_of_dicts(d, create_using=None, multigraph_input=False):
-    G = eg.empty_graph(0, create_using)
-    G.add_nodes_from(d)
-    # does dict d represent a MultiGraph or MultiDiGraph?
-    if multigraph_input:
-        if G.is_directed():
-            if G.is_multigraph():
-                G.add_edges_from(
-                    (u, v, key, data)
-                    for u, nbrs in d.items()
-                    for v, datadict in nbrs.items()
-                    for key, data in datadict.items()
-                )
-            else:
-                G.add_edges_from(
-                    (u, v, data)
-                    for u, nbrs in d.items()
-                    for v, datadict in nbrs.items()
-                    for key, data in datadict.items()
-                )
-        else:  # Undirected
-            if G.is_multigraph():
-                seen = set()  # don't add both directions of undirected graph
-                for u, nbrs in d.items():
-                    for v, datadict in nbrs.items():
-                        if (u, v) not in seen:
-                            G.add_edges_from(
-                                (u, v, key, data) for key, data in datadict.items()
-                            )
-                            seen.add((v, u))
-            else:
-                seen = set()  # don't add both directions of undirected graph
-                for u, nbrs in d.items():
-                    for v, datadict in nbrs.items():
-                        if (u, v) not in seen:
-                            G.add_edges_from(
-                                (u, v, data) for key, data in datadict.items()
-                            )
-                            seen.add((v, u))
-
-    else:  # not a multigraph to multigraph transfer
-        if G.is_multigraph() and not G.is_directed():
-            # d can have both representations u-v, v-u in dict.  Only add one.
-            # We don't need this check for digraphs since we add both directions,
-            # or for Graph() since it is done implicitly (parallel edges not allowed)
-            seen = set()
-            for u, nbrs in d.items():
-                for v, data in nbrs.items():
-                    if (u, v) not in seen:
-                        G.add_edge(u, v, key=0)
-                        G[u][v][0].update(data)
-                    seen.add((v, u))
-        else:
-            G.add_edges_from(
-                ((u, v, data) for u, nbrs in d.items() for v, data in nbrs.items())
-            )
-    return G
-
-
-def from_edgelist(edgelist, create_using=None):
-    """Returns a graph from a list of edges.
-
-    Parameters
-    ----------
-    edgelist : list or iterator
-      Edge tuples
-
-    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
-        Graph type to create. If graph instance, then cleared before populated.
-
-    Examples
-    --------
-    >>> edgelist = [(0, 1)]  # single edge (0,1)
-    >>> G = eg.from_edgelist(edgelist)
-
-    or
-
-    >>> G = eg.Graph(edgelist)  # use Graph constructor
-
-    """
-    G = eg.empty_graph(0, create_using)
-    G.add_edges_from(edgelist)
-    return G
-
-
-def to_networkx(g: "Union[Graph, DiGraph]") -> "Union[nx.Graph, nx.DiGraph]":
-    """Convert an EasyGraph to a NetworkX graph.
-
-    Args:
-        g (Union[Graph, DiGraph]): An EasyGraph graph
-
-    Raises:
-        ImportError is raised if NetworkX is not installed.
-
-    Returns:
-        Union[nx.Graph, nx.DiGraph]: Converted NetworkX graph
-    """
-    # if load_func_name in di_load_functions_name:
-    try:
-        import networkx as nx
-    except ImportError:
-        raise ImportError("NetworkX not found. Please install it.")
-    if g.is_directed():
-        G = nx.DiGraph()
-    else:
-        G = nx.Graph()
-
-    # copy attributes
-    G.graph = deepcopy(g.graph)
-
-    nodes_with_edges = set()
-    for v1, v2, _ in g.edges:
-        G.add_edge(v1, v2)
-        nodes_with_edges.add(v1)
-        nodes_with_edges.add(v2)
-    for node in set(g.nodes) - nodes_with_edges:
-        G.add_node(node)
-    return G
-
-
-def from_networkx(g: "Union[nx.Graph, nx.DiGraph]") -> "Union[Graph, DiGraph]":
-    """Convert a NetworkX graph to an EasyGraph graph.
-
-    Args:
-        g (Union[nx.Graph, nx.DiGraph]): A NetworkX graph
-
-    Returns:
-        Union[Graph, DiGraph]: Converted EasyGraph graph
-    """
-    # try:
-    #     import networkx as nx
-    # except ImportError:
-    #     raise ImportError("NetworkX not found. Please install it.")
-    if g.is_directed():
-        G = eg.DiGraph()
-    else:
-        G = eg.Graph()
-
-    # copy attributes
-    G.graph = deepcopy(g.graph)
-
-    nodes_with_edges = set()
-    for v1, v2 in g.edges:
-        G.add_edge(v1, v2)
-        nodes_with_edges.add(v1)
-        nodes_with_edges.add(v2)
-    for node in set(g.nodes) - nodes_with_edges:
-        G.add_node(node)
-    return G
-
-
-def to_dgl(g: "Union[Graph, DiGraph]"):
-    """Convert an EasyGraph graph to a DGL graph.
-
-    Args:
-        g (Union[Graph, DiGraph]): An EasyGraph graph
-
-    Raises:
-        ImportError: If DGL is not installed.
-
-    Returns:
-        DGLGraph: Converted DGL graph
-    """
-    try:
-        import dgl
-    except ImportError:
-        raise ImportError("DGL not found. Please install it.")
-    g_nx = to_networkx(g)
-    g_dgl = dgl.from_networkx(g_nx)
-    return g_dgl
-
-
-def from_dgl(g) -> "Union[Graph, DiGraph]":
-    """Convert a DGL graph to an EasyGraph graph.
-
-    Args:
-        g (DGLGraph): A DGL graph
-
-    Raises:
-        ImportError: If DGL is not installed.
-
-    Returns:
-        Union[Graph, DiGraph]: Converted EasyGraph graph
-    """
-    try:
-        import dgl
-    except ImportError:
-        raise ImportError("DGL not found. Please install it.")
-    g_nx = dgl.to_networkx(g)
-    g_eg = from_networkx(g_nx)
-    return g_eg
-
-
-def to_pyg(
-    G: Any,
-    group_node_attrs: Optional[Union[List[str], all]] = None,  # type: ignore
-    group_edge_attrs: Optional[Union[List[str], all]] = None,  # type: ignore
-) -> "torch_geometric.data.Data":  # type: ignore
-    r"""Converts a :obj:`easygraph.Graph` or :obj:`easygraph.DiGraph` to a
-    :class:`torch_geometric.data.Data` instance.
-
-    Args:
-        G (easygraph.Graph or easygraph.DiGraph): A easygraph graph.
-        group_node_attrs (List[str] or all, optional): The node attributes to
-            be concatenated and added to :obj:`data.x`. (default: :obj:`None`)
-        group_edge_attrs (List[str] or all, optional): The edge attributes to
-            be concatenated and added to :obj:`data.edge_attr`.
-            (default: :obj:`None`)
-
-    .. note::
-
-        All :attr:`group_node_attrs` and :attr:`group_edge_attrs` values must
-        be numeric.
-
-    Examples:
-
-        >>> import torch_geometric as pyg
-
-        >>> pyg_to_networkx = pyg.utils.convert.to_networkx  # type: ignore
-        >>> networkx_to_pyg = pyg.utils.convert.from_networkx  # type: ignore
-        >>> Data = pyg.data.Data  # type: ignore
-        >>> edge_index = torch.tensor([
-        ...     [0, 1, 1, 2, 2, 3],
-        ...     [1, 0, 2, 1, 3, 2],
-        ... ])
-        >>> data = Data(edge_index=edge_index, num_nodes=4)
-        >>> g = pyg_to_networkx(data)
-        >>> # A `Data` object is returned
-        >>> to_pyg(g)
-        Data(edge_index=[2, 6], num_nodes=4)
-    """
-    try:
-        import torch_geometric as pyg
-
-        pyg_to_networkx = pyg.utils.convert.to_networkx  # type: ignore
-        networkx_to_pyg = pyg.utils.convert.from_networkx  # type: ignore
-    except ImportError:
-        raise ImportError("pytorch_geometric not found. Please install it.")
-
-    g_nx = to_networkx(G)
-    g_pyg = networkx_to_pyg(g_nx, group_node_attrs, group_edge_attrs)
-    return g_pyg
-
-
-def from_pyg(
-    data: "torch_geometric.data.Data",  # type: ignore
-    node_attrs: Optional[Iterable[str]] = None,
-    edge_attrs: Optional[Iterable[str]] = None,
-    graph_attrs: Optional[Iterable[str]] = None,
-    to_undirected: Optional[Union[bool, str]] = False,
-    remove_self_loops: bool = False,
-) -> Any:
-    r"""Converts a :class:`torch_geometric.data.Data` instance to a
-    :obj:`easygraph.Graph` if :attr:`to_undirected` is set to :obj:`True`, or
-    a directed :obj:`easygraph.DiGraph` otherwise.
-
-    Args:
-        data (torch_geometric.data.Data): The data object.
-        node_attrs (iterable of str, optional): The node attributes to be
-            copied. (default: :obj:`None`)
-        edge_attrs (iterable of str, optional): The edge attributes to be
-            copied. (default: :obj:`None`)
-        graph_attrs (iterable of str, optional): The graph attributes to be
-            copied. (default: :obj:`None`)
-        to_undirected (bool or str, optional): If set to :obj:`True` or
-            "upper", will return a :obj:`easygraph.Graph` instead of a
-            :obj:`easygraph.DiGraph`. The undirected graph will correspond to
-            the upper triangle of the corresponding adjacency matrix.
-            Similarly, if set to "lower", the undirected graph will correspond
-            to the lower triangle of the adjacency matrix. (default:
-            :obj:`False`)
-        remove_self_loops (bool, optional): If set to :obj:`True`, will not
-            include self loops in the resulting graph. (default: :obj:`False`)
-
-    Examples:
-
-        >>> import torch_geometric as pyg
-
-        >>> Data = pyg.data.Data  # type: ignore
-        >>> edge_index = torch.tensor([
-        ...     [0, 1, 1, 2, 2, 3],
-        ...     [1, 0, 2, 1, 3, 2],
-        ... ])
-        >>> data = Data(edge_index=edge_index, num_nodes=4)
-        >>> from_pyg(data)
-        <easygraph.classes.digraph.DiGraph at 0x2713fdb40d0>
-
-    """
-
-    try:
-        import torch_geometric as pyg
-
-        pyg_to_networkx = pyg.utils.convert.to_networkx  # type: ignore
-        networkx_to_pyg = pyg.utils.convert.from_networkx  # type: ignore
-    except ImportError:
-        raise ImportError("pytorch_geometric not found. Please install it.")
-    g_nx = pyg_to_networkx(
-        data, node_attrs, edge_attrs, graph_attrs, to_undirected, remove_self_loops
-    )
-    g_eg = from_networkx(g_nx)
-    return g_eg
-
-
-def dict_to_hypergraph(data, max_order=None, is_dynamic=False):
-    """
-    A function to read a file in a standardized JSON format.
-
-    Parameters
-    ----------
-    data: dict
-        A dictionary in the hypergraph JSON format
-    max_order: int, optional
-        Maximum order of edges to add to the hypergraph
-
-    Returns
-    -------
-    A Hypergraph object
-        The loaded hypergraph
-
-    Raises
-    ------
-    EasyGraphError
-        If the JSON is not in a format that can be loaded.
-
-    See Also
-    --------
-    read_json
-
-    """
-
-    timestamp_lst = list()
-    node_data = data["node-data"]
-    node_num = len(node_data)
-    G = eg.Hypergraph(num_v=node_num)
-    try:
-        # print(len(data["node-data"]))
-        for index, dd in data["node-data"].items():
-            id = int(index) - 1
-            G.v_property[id] = dd
-    except KeyError:
-        raise EasyGraphError("Failed to import node attributes.")
-
-    # try:
-    # import time
-    rows = []
-    cols = []
-    edge_flag_dict = {}
-    e_property_dict = data["edge-data"]
-    edge_id = 0
-    for index, edge in data["edge-dict"].items():
-        # print("id:",id)
-        if max_order and len(edge) > max_order + 1:
-            continue
-
-        try:
-            id = int(index)
-        except ValueError as e:
-            raise TypeError(
-                f"Failed to convert the edge with ID {id} to type int."
-            ) from e
-
-        try:
-            edge = [int(n) - 1 for n in edge]
-            if tuple(edge) not in edge_flag_dict:
-                edge_flag_dict[tuple(edge)] = 1
-                rows.extend(edge)
-                cols.extend(len(edge) * [edge_id])
-                edge_id += 1
-
-        except ValueError as e:
-            raise TypeError(f"Failed to convert nodes to type int.") from e
-
-        if is_dynamic:
-            G.add_hyperedges(
-                e_list=edge,
-                e_property=e_property_dict[str(id)],
-                group_name=e_property_dict[str(id)]["timestamp"],
-            )
-
-            timestamp_lst.append(e_property_dict[str(id)]["timestamp"])
-        else:
-            G.add_hyperedges(e_list=edge, e_property=e_property_dict[str(id)])
-    G._rows = rows
-    G._cols = cols
-    return G, timestamp_lst
+import warnings
+
+from collections.abc import Collection
+from collections.abc import Generator
+from collections.abc import Iterator
+from copy import deepcopy
+from typing import TYPE_CHECKING
+from typing import Any
+from typing import Iterable
+from typing import List
+from typing import Optional
+from typing import Union
+
+import easygraph as eg
+
+from easygraph.utils.exception import EasyGraphError
+
+
+if TYPE_CHECKING:
+    import dgl
+    import networkx as nx
+    import torch_geometric
+
+    from easygraph import DiGraph
+    from easygraph import Graph
+
+__all__ = [
+    "from_dict_of_dicts",
+    "to_easygraph_graph",
+    "from_edgelist",
+    "from_dict_of_lists",
+    "from_networkx",
+    "from_dgl",
+    "from_pyg",
+    "to_networkx",
+    "to_dgl",
+    "to_pyg",
+    "dict_to_hypergraph",
+]
+
+
+def to_easygraph_graph(data, create_using=None, multigraph_input=False):
+    """Make a EasyGraph graph from a known data structure.
+
+    The preferred way to call this is automatically
+    from the class constructor
+
+    >>> d = {0: {1: {"weight": 1}}}  # dict-of-dicts single edge (0,1)
+    >>> G = eg.Graph(d)
+
+    instead of the equivalent
+
+    >>> G = eg.from_dict_of_dicts(d)
+
+    Parameters
+    ----------
+    data : object to be converted
+
+        Current known types are:
+         any EasyGraph graph
+         dict-of-dicts
+         dict-of-lists
+         container (e.g. set, list, tuple) of edges
+         iterator (e.g. itertools.chain) that produces edges
+         generator of edges
+         Pandas DataFrame (row per edge)
+         numpy matrix
+         numpy ndarray
+         scipy sparse matrix
+         pygraphviz agraph
+
+    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
+        Graph type to create. If graph instance, then cleared before populated.
+
+    multigraph_input : bool (default False)
+        If True and  data is a dict_of_dicts,
+        try to create a multigraph assuming dict_of_dict_of_lists.
+        If data and create_using are both multigraphs then create
+        a multigraph from a multigraph.
+
+    """
+
+    # EasyGraph graph type
+    if hasattr(data, "adj"):
+        try:
+            result = from_dict_of_dicts(
+                data.adj,
+                create_using=create_using,
+                multigraph_input=data.is_multigraph(),
+            )
+            # data.graph should be dict-like
+            result.graph.update(data.graph)
+            # data.nodes should be dict-like
+            # result.add_node_from(data.nodes.items()) possible but
+            # for custom node_attr_dict_factory which may be hashable
+            # will be unexpected behavior
+            for n, dd in data.nodes.items():
+                result._node[n].update(dd)
+            return result
+        except Exception as err:
+            raise eg.EasyGraphError("Input is not a correct EasyGraph graph.") from err
+
+    # pygraphviz  agraph
+    if hasattr(data, "is_strict"):
+        try:
+            return eg.from_pyGraphviz_agraph(data, create_using=create_using)
+        except Exception as err:
+            raise eg.EasyGraphError("Input is not a correct pygraphviz graph.") from err
+
+    # dict of dicts/lists
+    if isinstance(data, dict):
+        try:
+            return from_dict_of_dicts(
+                data, create_using=create_using, multigraph_input=multigraph_input
+            )
+        except Exception as err:
+            if multigraph_input is True:
+                raise eg.EasyGraphError(
+                    f"converting multigraph_input raised:\n{type(err)}: {err}"
+                )
+            try:
+                return from_dict_of_lists(data, create_using=create_using)
+            except Exception as err:
+                raise TypeError("Input is not known type.") from err
+
+    # Pandas DataFrame
+    try:
+        import pandas as pd
+
+        if isinstance(data, pd.DataFrame):
+            if data.shape[0] == data.shape[1]:
+                try:
+                    return eg.from_pandas_adjacency(data, create_using=create_using)
+                except Exception as err:
+                    msg = "Input is not a correct Pandas DataFrame adjacency matrix."
+                    raise eg.EasyGraphError(msg) from err
+            else:
+                try:
+                    return eg.from_pandas_edgelist(
+                        data, edge_attr=True, create_using=create_using
+                    )
+                except Exception as err:
+                    msg = "Input is not a correct Pandas DataFrame adjacency edge-list."
+                    raise eg.EasyGraphError(msg) from err
+    except ImportError:
+        warnings.warn("pandas not found, skipping conversion test.", ImportWarning)
+
+    # numpy matrix or ndarray
+    try:
+        import numpy as np
+
+        if isinstance(data, np.ndarray):
+            try:
+                return eg.from_numpy_array(data, create_using=create_using)
+            except Exception as err:
+                raise eg.EasyGraphError(
+                    "Input is not a correct numpy matrix or array."
+                ) from err
+    except ImportError:
+        warnings.warn("numpy not found, skipping conversion test.", ImportWarning)
+
+    # scipy sparse matrix - any format
+    try:
+        if hasattr(data, "format"):
+            try:
+                return eg.from_scipy_sparse_matrix(data, create_using=create_using)
+            except Exception as err:
+                raise eg.EasyGraphError(
+                    "Input is not a correct scipy sparse matrix type."
+                ) from err
+    except ImportError:
+        warnings.warn("scipy not found, skipping conversion test.", ImportWarning)
+
+    # Note: most general check - should remain last in order of execution
+    # Includes containers (e.g. list, set, dict, etc.), generators, and
+    # iterators (e.g. itertools.chain) of edges
+
+    if isinstance(data, (Collection, Generator, Iterator)):
+        try:
+            return from_edgelist(data, create_using=create_using)
+        except Exception as err:
+            raise eg.EasyGraphError("Input is not a valid edge list") from err
+
+    raise eg.EasyGraphError("Input is not a known data type for conversion.")
+
+
+def from_dict_of_lists(d, create_using=None):
+    G = eg.empty_graph(0, create_using)
+    G.add_nodes_from(d)
+    if G.is_multigraph() and not G.is_directed():
+        # a dict_of_lists can't show multiedges.  BUT for undirected graphs,
+        # each edge shows up twice in the dict_of_lists.
+        # So we need to treat this case separately.
+        seen = {}
+        for node, nbrlist in d.items():
+            for nbr in nbrlist:
+                if nbr not in seen:
+                    G.add_edge(node, nbr)
+            seen[node] = 1  # don't allow reverse edge to show up
+    else:
+        G.add_edges_from(
+            ((node, nbr) for node, nbrlist in d.items() for nbr in nbrlist)
+        )
+    return G
+
+
+def from_dict_of_dicts(d, create_using=None, multigraph_input=False):
+    G = eg.empty_graph(0, create_using)
+    G.add_nodes_from(d)
+    # does dict d represent a MultiGraph or MultiDiGraph?
+    if multigraph_input:
+        if G.is_directed():
+            if G.is_multigraph():
+                G.add_edges_from(
+                    (u, v, key, data)
+                    for u, nbrs in d.items()
+                    for v, datadict in nbrs.items()
+                    for key, data in datadict.items()
+                )
+            else:
+                G.add_edges_from(
+                    (u, v, data)
+                    for u, nbrs in d.items()
+                    for v, datadict in nbrs.items()
+                    for key, data in datadict.items()
+                )
+        else:  # Undirected
+            if G.is_multigraph():
+                seen = set()  # don't add both directions of undirected graph
+                for u, nbrs in d.items():
+                    for v, datadict in nbrs.items():
+                        if (u, v) not in seen:
+                            G.add_edges_from(
+                                (u, v, key, data) for key, data in datadict.items()
+                            )
+                            seen.add((v, u))
+            else:
+                seen = set()  # don't add both directions of undirected graph
+                for u, nbrs in d.items():
+                    for v, datadict in nbrs.items():
+                        if (u, v) not in seen:
+                            G.add_edges_from(
+                                (u, v, data) for key, data in datadict.items()
+                            )
+                            seen.add((v, u))
+
+    else:  # not a multigraph to multigraph transfer
+        if G.is_multigraph() and not G.is_directed():
+            # d can have both representations u-v, v-u in dict.  Only add one.
+            # We don't need this check for digraphs since we add both directions,
+            # or for Graph() since it is done implicitly (parallel edges not allowed)
+            seen = set()
+            for u, nbrs in d.items():
+                for v, data in nbrs.items():
+                    if (u, v) not in seen:
+                        G.add_edge(u, v, key=0)
+                        G[u][v][0].update(data)
+                    seen.add((v, u))
+        else:
+            G.add_edges_from(
+                ((u, v, data) for u, nbrs in d.items() for v, data in nbrs.items())
+            )
+    return G
+
+
+def from_edgelist(edgelist, create_using=None):
+    """Returns a graph from a list of edges.
+
+    Parameters
+    ----------
+    edgelist : list or iterator
+      Edge tuples
+
+    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
+        Graph type to create. If graph instance, then cleared before populated.
+
+    Examples
+    --------
+    >>> edgelist = [(0, 1)]  # single edge (0,1)
+    >>> G = eg.from_edgelist(edgelist)
+
+    or
+
+    >>> G = eg.Graph(edgelist)  # use Graph constructor
+
+    """
+    G = eg.empty_graph(0, create_using)
+    G.add_edges_from(edgelist)
+    return G
+
+
+def to_networkx(g: "Union[Graph, DiGraph]") -> "Union[nx.Graph, nx.DiGraph]":
+    """Convert an EasyGraph to a NetworkX graph.
+
+    Args:
+        g (Union[Graph, DiGraph]): An EasyGraph graph
+
+    Raises:
+        ImportError is raised if NetworkX is not installed.
+
+    Returns:
+        Union[nx.Graph, nx.DiGraph]: Converted NetworkX graph
+    """
+    # if load_func_name in di_load_functions_name:
+    try:
+        import networkx as nx
+    except ImportError:
+        raise ImportError("NetworkX not found. Please install it.")
+    if g.is_directed():
+        G = nx.DiGraph()
+    else:
+        G = nx.Graph()
+
+    # copy attributes
+    G.graph = deepcopy(g.graph)
+
+    nodes_with_edges = set()
+    for v1, v2, _ in g.edges:
+        G.add_edge(v1, v2)
+        nodes_with_edges.add(v1)
+        nodes_with_edges.add(v2)
+    for node in set(g.nodes) - nodes_with_edges:
+        G.add_node(node)
+    return G
+
+
+def from_networkx(g: "Union[nx.Graph, nx.DiGraph]") -> "Union[Graph, DiGraph]":
+    """Convert a NetworkX graph to an EasyGraph graph.
+
+    Args:
+        g (Union[nx.Graph, nx.DiGraph]): A NetworkX graph
+
+    Returns:
+        Union[Graph, DiGraph]: Converted EasyGraph graph
+    """
+    # try:
+    #     import networkx as nx
+    # except ImportError:
+    #     raise ImportError("NetworkX not found. Please install it.")
+    if g.is_directed():
+        G = eg.DiGraph()
+    else:
+        G = eg.Graph()
+
+    # copy attributes
+    G.graph = deepcopy(g.graph)
+
+    nodes_with_edges = set()
+    for v1, v2 in g.edges:
+        G.add_edge(v1, v2)
+        nodes_with_edges.add(v1)
+        nodes_with_edges.add(v2)
+    for node in set(g.nodes) - nodes_with_edges:
+        G.add_node(node)
+    return G
+
+
+def to_dgl(g: "Union[Graph, DiGraph]"):
+    """Convert an EasyGraph graph to a DGL graph.
+
+    Args:
+        g (Union[Graph, DiGraph]): An EasyGraph graph
+
+    Raises:
+        ImportError: If DGL is not installed.
+
+    Returns:
+        DGLGraph: Converted DGL graph
+    """
+    try:
+        import dgl
+    except ImportError:
+        raise ImportError("DGL not found. Please install it.")
+    g_nx = to_networkx(g)
+    g_dgl = dgl.from_networkx(g_nx)
+    return g_dgl
+
+
+def from_dgl(g) -> "Union[Graph, DiGraph]":
+    """Convert a DGL graph to an EasyGraph graph.
+
+    Args:
+        g (DGLGraph): A DGL graph
+
+    Raises:
+        ImportError: If DGL is not installed.
+
+    Returns:
+        Union[Graph, DiGraph]: Converted EasyGraph graph
+    """
+    try:
+        import dgl
+    except ImportError:
+        raise ImportError("DGL not found. Please install it.")
+    g_nx = dgl.to_networkx(g)
+    g_eg = from_networkx(g_nx)
+    return g_eg
+
+
+def to_pyg(
+    G: Any,
+    group_node_attrs: Optional[Union[List[str], all]] = None,  # type: ignore
+    group_edge_attrs: Optional[Union[List[str], all]] = None,  # type: ignore
+) -> "torch_geometric.data.Data":  # type: ignore
+    r"""Converts a :obj:`easygraph.Graph` or :obj:`easygraph.DiGraph` to a
+    :class:`torch_geometric.data.Data` instance.
+
+    Args:
+        G (easygraph.Graph or easygraph.DiGraph): A easygraph graph.
+        group_node_attrs (List[str] or all, optional): The node attributes to
+            be concatenated and added to :obj:`data.x`. (default: :obj:`None`)
+        group_edge_attrs (List[str] or all, optional): The edge attributes to
+            be concatenated and added to :obj:`data.edge_attr`.
+            (default: :obj:`None`)
+
+    .. note::
+
+        All :attr:`group_node_attrs` and :attr:`group_edge_attrs` values must
+        be numeric.
+
+    Examples:
+
+        >>> import torch_geometric as pyg
+
+        >>> pyg_to_networkx = pyg.utils.convert.to_networkx  # type: ignore
+        >>> networkx_to_pyg = pyg.utils.convert.from_networkx  # type: ignore
+        >>> Data = pyg.data.Data  # type: ignore
+        >>> edge_index = torch.tensor([
+        ...     [0, 1, 1, 2, 2, 3],
+        ...     [1, 0, 2, 1, 3, 2],
+        ... ])
+        >>> data = Data(edge_index=edge_index, num_nodes=4)
+        >>> g = pyg_to_networkx(data)
+        >>> # A `Data` object is returned
+        >>> to_pyg(g)
+        Data(edge_index=[2, 6], num_nodes=4)
+    """
+    try:
+        import torch_geometric as pyg
+
+        pyg_to_networkx = pyg.utils.convert.to_networkx  # type: ignore
+        networkx_to_pyg = pyg.utils.convert.from_networkx  # type: ignore
+    except ImportError:
+        raise ImportError("pytorch_geometric not found. Please install it.")
+
+    g_nx = to_networkx(G)
+    g_pyg = networkx_to_pyg(g_nx, group_node_attrs, group_edge_attrs)
+    return g_pyg
+
+
+def from_pyg(
+    data: "torch_geometric.data.Data",  # type: ignore
+    node_attrs: Optional[Iterable[str]] = None,
+    edge_attrs: Optional[Iterable[str]] = None,
+    graph_attrs: Optional[Iterable[str]] = None,
+    to_undirected: Optional[Union[bool, str]] = False,
+    remove_self_loops: bool = False,
+) -> Any:
+    r"""Converts a :class:`torch_geometric.data.Data` instance to a
+    :obj:`easygraph.Graph` if :attr:`to_undirected` is set to :obj:`True`, or
+    a directed :obj:`easygraph.DiGraph` otherwise.
+
+    Args:
+        data (torch_geometric.data.Data): The data object.
+        node_attrs (iterable of str, optional): The node attributes to be
+            copied. (default: :obj:`None`)
+        edge_attrs (iterable of str, optional): The edge attributes to be
+            copied. (default: :obj:`None`)
+        graph_attrs (iterable of str, optional): The graph attributes to be
+            copied. (default: :obj:`None`)
+        to_undirected (bool or str, optional): If set to :obj:`True` or
+            "upper", will return a :obj:`easygraph.Graph` instead of a
+            :obj:`easygraph.DiGraph`. The undirected graph will correspond to
+            the upper triangle of the corresponding adjacency matrix.
+            Similarly, if set to "lower", the undirected graph will correspond
+            to the lower triangle of the adjacency matrix. (default:
+            :obj:`False`)
+        remove_self_loops (bool, optional): If set to :obj:`True`, will not
+            include self loops in the resulting graph. (default: :obj:`False`)
+
+    Examples:
+
+        >>> import torch_geometric as pyg
+
+        >>> Data = pyg.data.Data  # type: ignore
+        >>> edge_index = torch.tensor([
+        ...     [0, 1, 1, 2, 2, 3],
+        ...     [1, 0, 2, 1, 3, 2],
+        ... ])
+        >>> data = Data(edge_index=edge_index, num_nodes=4)
+        >>> from_pyg(data)
+        <easygraph.classes.digraph.DiGraph at 0x2713fdb40d0>
+
+    """
+
+    try:
+        import torch_geometric as pyg
+
+        pyg_to_networkx = pyg.utils.convert.to_networkx  # type: ignore
+        networkx_to_pyg = pyg.utils.convert.from_networkx  # type: ignore
+    except ImportError:
+        raise ImportError("pytorch_geometric not found. Please install it.")
+    g_nx = pyg_to_networkx(
+        data, node_attrs, edge_attrs, graph_attrs, to_undirected, remove_self_loops
+    )
+    g_eg = from_networkx(g_nx)
+    return g_eg
+
+
+def dict_to_hypergraph(data, max_order=None, is_dynamic=False):
+    """
+    A function to read a file in a standardized JSON format.
+
+    Parameters
+    ----------
+    data: dict
+        A dictionary in the hypergraph JSON format
+    max_order: int, optional
+        Maximum order of edges to add to the hypergraph
+
+    Returns
+    -------
+    A Hypergraph object
+        The loaded hypergraph
+
+    Raises
+    ------
+    EasyGraphError
+        If the JSON is not in a format that can be loaded.
+
+    See Also
+    --------
+    read_json
+
+    """
+
+    timestamp_lst = list()
+    node_data = data["node-data"]
+    node_num = len(node_data)
+    G = eg.Hypergraph(num_v=node_num)
+    try:
+        # print(len(data["node-data"]))
+        for index, dd in data["node-data"].items():
+            id = int(index) - 1
+            G.v_property[id] = dd
+    except KeyError:
+        raise EasyGraphError("Failed to import node attributes.")
+
+    # try:
+    # import time
+    rows = []
+    cols = []
+    edge_flag_dict = {}
+    e_property_dict = data["edge-data"]
+    edge_id = 0
+    for index, edge in data["edge-dict"].items():
+        # print("id:",id)
+        if max_order and len(edge) > max_order + 1:
+            continue
+
+        try:
+            id = int(index)
+        except ValueError as e:
+            raise TypeError(
+                f"Failed to convert the edge with ID {id} to type int."
+            ) from e
+
+        try:
+            edge = [int(n) - 1 for n in edge]
+            if tuple(edge) not in edge_flag_dict:
+                edge_flag_dict[tuple(edge)] = 1
+                rows.extend(edge)
+                cols.extend(len(edge) * [edge_id])
+                edge_id += 1
+
+        except ValueError as e:
+            raise TypeError(f"Failed to convert nodes to type int.") from e
+
+        if is_dynamic:
+            G.add_hyperedges(
+                e_list=edge,
+                e_property=e_property_dict[str(id)],
+                group_name=e_property_dict[str(id)]["timestamp"],
+            )
+
+            timestamp_lst.append(e_property_dict[str(id)]["timestamp"])
+        else:
+            G.add_hyperedges(e_list=edge, e_property=e_property_dict[str(id)])
+    G._rows = rows
+    G._cols = cols
+    return G, timestamp_lst
```

## easygraph/__init__.py

 * *Ordering differences only*

```diff
@@ -1,27 +1,27 @@
-import easygraph.classes
-import easygraph.convert
-import easygraph.datapipe
-import easygraph.datasets
-import easygraph.experiments
-import easygraph.functions
-import easygraph.ml_metrics
-import easygraph.model
-import easygraph.nn
-import easygraph.readwrite
-import easygraph.utils
-
-from easygraph.classes import *
-from easygraph.convert import *
-from easygraph.datapipe import *
-from easygraph.datasets import *
-from easygraph.experiments import *
-from easygraph.functions import *
-from easygraph.ml_metrics import *
-from easygraph.model import *
-from easygraph.nn import *
-from easygraph.readwrite import *
-from easygraph.utils import *
-
-
-def __getattr__(name):
-    print(f"attr {name} doesn't exist!")
+import easygraph.classes
+import easygraph.convert
+import easygraph.datapipe
+import easygraph.datasets
+import easygraph.experiments
+import easygraph.functions
+import easygraph.ml_metrics
+import easygraph.model
+import easygraph.nn
+import easygraph.readwrite
+import easygraph.utils
+
+from easygraph.classes import *
+from easygraph.convert import *
+from easygraph.datapipe import *
+from easygraph.datasets import *
+from easygraph.experiments import *
+from easygraph.functions import *
+from easygraph.ml_metrics import *
+from easygraph.model import *
+from easygraph.nn import *
+from easygraph.readwrite import *
+from easygraph.utils import *
+
+
+def __getattr__(name):
+    print(f"attr {name} doesn't exist!")
```

## easygraph/readwrite/ucinet.py

 * *Ordering differences only*

```diff
@@ -1,327 +1,327 @@
-"""
-**************
-UCINET DL
-**************
-Read and write graphs in UCINET DL format.
-This implementation currently supports only the 'fullmatrix' data format.
-Format
-------
-The UCINET DL format is the most common file format used by UCINET package.
-Basic example:
-DL N = 5
-Data:
-0 1 1 1 1
-1 0 1 0 0
-1 1 0 0 1
-1 0 0 0 0
-1 0 1 0 0
-References
-----------
-    See UCINET User Guide or http://www.analytictech.com/ucinet/help/hs5000.htm
-    for full format information. Short version on http://www.analytictech.com/networks/dataentry.htm
-"""
-
-
-import re
-import shlex
-
-import easygraph as eg
-import numpy as np
-
-from easygraph.utils import open_file
-
-
-__all__ = ["generate_ucinet", "read_ucinet", "parse_ucinet", "write_ucinet"]
-
-
-def generate_ucinet(G):
-    """Generate lines in UCINET graph format.
-    Parameters
-    ----------
-    G : graph
-       A EasyGraph graph
-    Examples
-    --------
-    Notes
-    -----
-    The default format 'fullmatrix' is used (for UCINET DL format).
-
-    References
-    ----------
-    See UCINET User Guide or http://www.analytictech.com/ucinet/help/hs5000.htm
-    for full format information. Short version on http://www.analytictech.com/networks/dataentry.htm
-    """
-
-    n = G.number_of_nodes()
-    nodes = sorted(list(G.nodes))
-    yield "dl n=%i format=fullmatrix" % n
-
-    # Labels
-    try:
-        int(nodes[0])
-    except ValueError:
-        s = "labels:\n"
-        for label in nodes:
-            s += label + " "
-        yield s
-
-    yield "data:"
-
-    yield str(np.asmatrix(eg.to_numpy_array(G, nodelist=nodes, dtype=int))).replace(
-        "[", " "
-    ).replace("]", " ").lstrip().rstrip()
-
-
-@open_file(0, mode="rb")
-def read_ucinet(path, encoding="UTF-8"):
-    """Read graph in UCINET format from path.
-    Parameters
-    ----------
-    path : file or string
-       File or filename to read.
-       Filenames ending in .gz or .bz2 will be uncompressed.
-    Returns
-    -------
-    G : EasyGraph MultiGraph or MultiDiGraph.
-    Examples
-    --------
-    >>> G=eg.path_graph(4)
-    >>> eg.write_ucinet(G, "test.dl")
-    >>> G=eg.read_ucinet("test.dl")
-    To create a Graph instead of a MultiGraph use
-    >>> G1=eg.Graph(G)
-    See Also
-    --------
-    parse_ucinet()
-    References
-    ----------
-    See UCINET User Guide or http://www.analytictech.com/ucinet/help/hs5000.htm
-    for full format information. Short version on http://www.analytictech.com/networks/dataentry.htm
-    """
-    lines = (line.decode(encoding) for line in path)
-    return parse_ucinet(lines)
-
-
-@open_file(1, mode="wb")
-def write_ucinet(G, path, encoding="UTF-8"):
-    """Write graph in UCINET format to path.
-    Parameters
-    ----------
-    G : graph
-       A EasyGraph graph
-    path : file or string
-       File or filename to write.
-       Filenames ending in .gz or .bz2 will be compressed.
-    Examples
-    --------
-    >>> G=eg.path_graph(4)
-    >>> eg.write_ucinet(G, "test.net")
-    References
-    ----------
-    See UCINET User Guide or http://www.analytictech.com/ucinet/help/hs5000.htm
-    for full format information. Short version on http://www.analytictech.com/networks/dataentry.htm
-    """
-    for line in generate_ucinet(G):
-        line += "\n"
-        path.write(line.encode(encoding))
-
-
-def parse_ucinet(lines):
-    """Parse UCINET format graph from string or iterable.
-    Currently only the 'fullmatrix', 'nodelist1' and 'nodelist1b' formats are supported.
-    Parameters
-    ----------
-    lines : string or iterable
-       Data in UCINET format.
-    Returns
-    -------
-    G : EasyGraph graph
-    See Also
-    --------
-    read_ucinet()
-    References
-    ----------
-    See UCINET User Guide or http://www.analytictech.com/ucinet/help/hs5000.htm
-    for full format information. Short version on http://www.analytictech.com/networks/dataentry.htm
-    """
-    from numpy import genfromtxt
-    from numpy import isnan
-    from numpy import reshape
-
-    G = eg.MultiDiGraph()
-
-    if not isinstance(lines, str):
-        s = ""
-        for line in lines:
-            if type(line) == bytes:
-                s += line.decode("utf-8")
-            else:
-                s += line
-        lines = s
-    lexer = shlex.shlex(lines.lower())
-    lexer.whitespace += ",="
-    lexer.whitespace_split = True
-
-    number_of_nodes = 0
-    number_of_matrices = 0
-    nr = 0  # number of rows (rectangular matrix)
-    nc = 0  # number of columns (rectangular matrix)
-    ucinet_format = "fullmatrix"  # Format by default
-    labels = {}  # Contains labels of nodes
-    row_labels_embedded = False  # Whether labels are embedded in data or not
-    cols_labels_embedded = False
-    diagonal = True  # whether the main diagonal is present or absent
-
-    KEYWORDS = ("format", "data:", "labels:")  # TODO remove ':' in keywords
-
-    while lexer:
-        try:
-            token = next(lexer)
-        except StopIteration:
-            break
-        # print "Token : %s" % token
-        if token.startswith("n"):
-            if token.startswith("nr"):
-                nr = int(get_param(r"\d+", token, lexer))
-                number_of_nodes = max(nr, nc)
-            elif token.startswith("nc"):
-                nc = int(get_param(r"\d+", token, lexer))
-                number_of_nodes = max(nr, nc)
-            elif token.startswith("nm"):
-                number_of_matrices = int(get_param(r"\d+", token, lexer))
-            else:
-                number_of_nodes = int(get_param(r"\d+", token, lexer))
-                nr = number_of_nodes
-                nc = number_of_nodes
-
-        elif token.startswith("diagonal"):
-            diagonal = get_param("present|absent", token, lexer)
-
-        elif token.startswith("format"):
-            ucinet_format = get_param(
-                """^(fullmatrix|upperhalf|lowerhalf|nodelist1|nodelist2|nodelist1b|\
-edgelist1|edgelist2|blockmatrix|partition)$""",
-                token,
-                lexer,
-            )
-
-        # TODO : row and columns labels
-        elif token.startswith("row"):  # Row labels
-            pass
-        elif token.startswith("column"):  # Columns labels
-            pass
-
-        elif token.startswith("labels"):
-            token = next(lexer)
-            i = 0
-            while token not in KEYWORDS:
-                if token.startswith("embedded"):
-                    row_labels_embedded = True
-                    cols_labels_embedded = True
-                    break
-                else:
-                    labels[i] = token.replace(
-                        '"', ""
-                    )  # for labels with embedded spaces
-                    i += 1
-                    try:
-                        token = next(lexer)
-                    except StopIteration:
-                        break
-        elif token.startswith("data"):
-            break
-
-    data_lines = lines.lower().split("data:", 1)[1]
-    # Generate edges
-    params = {}
-    if cols_labels_embedded:
-        # params['names'] = True
-        labels = dict(zip(range(0, nc), data_lines.splitlines()[1].split()))
-        # params['skip_header'] = 2  # First character is \n
-    if row_labels_embedded:  # Skip first column
-        # TODO rectangular case : labels can differ from rows to columns
-        # params['usecols'] = range(1, nc + 1)
-        pass
-
-    if ucinet_format == "fullmatrix":
-        # In Python3 genfromtxt requires bytes string
-        try:
-            data_lines = bytes(data_lines, "utf-8")
-        except TypeError:
-            pass
-        # Do not use splitlines() because it is not necessarily written as a square matrix
-        data = genfromtxt([data_lines], case_sensitive=False, **params)
-        if cols_labels_embedded or row_labels_embedded:
-            # data = insert(data, 0, float('nan'))
-            data = data[~isnan(data)]
-        mat = reshape(data, (max(number_of_nodes, nr), -1))
-        G = eg.from_numpy_array(mat, create_using=eg.MultiDiGraph())
-
-    elif ucinet_format in (
-        "nodelist1",
-        "nodelist1b",
-    ):  # Since genfromtxt only accepts square matrix...
-        s = ""
-        for i, line in enumerate(data_lines.splitlines()):
-            row = line.split()
-            if row:
-                if ucinet_format == "nodelist1b" and row[0] == "0":
-                    pass
-                else:
-                    for neighbor in row[1:]:
-                        if ucinet_format == "nodelist1":
-                            source = row[0]
-                        else:
-                            source = str(i)
-                        s += source + " " + neighbor + "\n"
-
-        G = eg.parse_edgelist(
-            s.splitlines(),
-            nodetype=str if row_labels_embedded and cols_labels_embedded else int,
-            create_using=eg.MultiDiGraph(),
-        )
-
-        if not row_labels_embedded or not cols_labels_embedded:
-            G = eg.relabel_nodes(G, dict(zip(list(G.nodes), [i - 1 for i in G.nodes])))
-
-    elif ucinet_format == "edgelist1":
-        G = eg.parse_edgelist(
-            data_lines.splitlines(),
-            nodetype=str if row_labels_embedded and cols_labels_embedded else int,
-            create_using=eg.MultiDiGraph(),
-        )
-
-        if not row_labels_embedded or not cols_labels_embedded:
-            G = eg.relabel_nodes(G, dict(zip(list(G.nodes), [i - 1 for i in G.nodes])))
-
-    # Relabel nodes
-    if labels:
-        try:
-            if len(list(G.nodes)) < number_of_nodes:
-                G.add_nodes_from(
-                    labels.values() if labels else range(0, number_of_nodes)
-                )
-            G = eg.relabel_nodes(G, labels)
-        except KeyError:
-            pass  # Nodes already labelled
-
-    return G
-
-
-def get_param(regex, token, lines):
-    """
-    Get a parameter value in UCINET DL file
-    :param regex: string with the regex matching the parameter value
-    :param token: token (string) in which we search for the parameter
-    :param lines: to iterate through the next tokens
-    :return:
-    """
-    n = token
-    query = re.search(regex, n)
-    while query is None:
-        try:
-            n = next(lines)
-        except StopIteration:
-            raise Exception("Parameter %s value not recognized" % token)
-        query = re.search(regex, n)
-    return query.group()
+"""
+**************
+UCINET DL
+**************
+Read and write graphs in UCINET DL format.
+This implementation currently supports only the 'fullmatrix' data format.
+Format
+------
+The UCINET DL format is the most common file format used by UCINET package.
+Basic example:
+DL N = 5
+Data:
+0 1 1 1 1
+1 0 1 0 0
+1 1 0 0 1
+1 0 0 0 0
+1 0 1 0 0
+References
+----------
+    See UCINET User Guide or http://www.analytictech.com/ucinet/help/hs5000.htm
+    for full format information. Short version on http://www.analytictech.com/networks/dataentry.htm
+"""
+
+
+import re
+import shlex
+
+import easygraph as eg
+import numpy as np
+
+from easygraph.utils import open_file
+
+
+__all__ = ["generate_ucinet", "read_ucinet", "parse_ucinet", "write_ucinet"]
+
+
+def generate_ucinet(G):
+    """Generate lines in UCINET graph format.
+    Parameters
+    ----------
+    G : graph
+       A EasyGraph graph
+    Examples
+    --------
+    Notes
+    -----
+    The default format 'fullmatrix' is used (for UCINET DL format).
+
+    References
+    ----------
+    See UCINET User Guide or http://www.analytictech.com/ucinet/help/hs5000.htm
+    for full format information. Short version on http://www.analytictech.com/networks/dataentry.htm
+    """
+
+    n = G.number_of_nodes()
+    nodes = sorted(list(G.nodes))
+    yield "dl n=%i format=fullmatrix" % n
+
+    # Labels
+    try:
+        int(nodes[0])
+    except ValueError:
+        s = "labels:\n"
+        for label in nodes:
+            s += label + " "
+        yield s
+
+    yield "data:"
+
+    yield str(np.asmatrix(eg.to_numpy_array(G, nodelist=nodes, dtype=int))).replace(
+        "[", " "
+    ).replace("]", " ").lstrip().rstrip()
+
+
+@open_file(0, mode="rb")
+def read_ucinet(path, encoding="UTF-8"):
+    """Read graph in UCINET format from path.
+    Parameters
+    ----------
+    path : file or string
+       File or filename to read.
+       Filenames ending in .gz or .bz2 will be uncompressed.
+    Returns
+    -------
+    G : EasyGraph MultiGraph or MultiDiGraph.
+    Examples
+    --------
+    >>> G=eg.path_graph(4)
+    >>> eg.write_ucinet(G, "test.dl")
+    >>> G=eg.read_ucinet("test.dl")
+    To create a Graph instead of a MultiGraph use
+    >>> G1=eg.Graph(G)
+    See Also
+    --------
+    parse_ucinet()
+    References
+    ----------
+    See UCINET User Guide or http://www.analytictech.com/ucinet/help/hs5000.htm
+    for full format information. Short version on http://www.analytictech.com/networks/dataentry.htm
+    """
+    lines = (line.decode(encoding) for line in path)
+    return parse_ucinet(lines)
+
+
+@open_file(1, mode="wb")
+def write_ucinet(G, path, encoding="UTF-8"):
+    """Write graph in UCINET format to path.
+    Parameters
+    ----------
+    G : graph
+       A EasyGraph graph
+    path : file or string
+       File or filename to write.
+       Filenames ending in .gz or .bz2 will be compressed.
+    Examples
+    --------
+    >>> G=eg.path_graph(4)
+    >>> eg.write_ucinet(G, "test.net")
+    References
+    ----------
+    See UCINET User Guide or http://www.analytictech.com/ucinet/help/hs5000.htm
+    for full format information. Short version on http://www.analytictech.com/networks/dataentry.htm
+    """
+    for line in generate_ucinet(G):
+        line += "\n"
+        path.write(line.encode(encoding))
+
+
+def parse_ucinet(lines):
+    """Parse UCINET format graph from string or iterable.
+    Currently only the 'fullmatrix', 'nodelist1' and 'nodelist1b' formats are supported.
+    Parameters
+    ----------
+    lines : string or iterable
+       Data in UCINET format.
+    Returns
+    -------
+    G : EasyGraph graph
+    See Also
+    --------
+    read_ucinet()
+    References
+    ----------
+    See UCINET User Guide or http://www.analytictech.com/ucinet/help/hs5000.htm
+    for full format information. Short version on http://www.analytictech.com/networks/dataentry.htm
+    """
+    from numpy import genfromtxt
+    from numpy import isnan
+    from numpy import reshape
+
+    G = eg.MultiDiGraph()
+
+    if not isinstance(lines, str):
+        s = ""
+        for line in lines:
+            if type(line) == bytes:
+                s += line.decode("utf-8")
+            else:
+                s += line
+        lines = s
+    lexer = shlex.shlex(lines.lower())
+    lexer.whitespace += ",="
+    lexer.whitespace_split = True
+
+    number_of_nodes = 0
+    number_of_matrices = 0
+    nr = 0  # number of rows (rectangular matrix)
+    nc = 0  # number of columns (rectangular matrix)
+    ucinet_format = "fullmatrix"  # Format by default
+    labels = {}  # Contains labels of nodes
+    row_labels_embedded = False  # Whether labels are embedded in data or not
+    cols_labels_embedded = False
+    diagonal = True  # whether the main diagonal is present or absent
+
+    KEYWORDS = ("format", "data:", "labels:")  # TODO remove ':' in keywords
+
+    while lexer:
+        try:
+            token = next(lexer)
+        except StopIteration:
+            break
+        # print "Token : %s" % token
+        if token.startswith("n"):
+            if token.startswith("nr"):
+                nr = int(get_param(r"\d+", token, lexer))
+                number_of_nodes = max(nr, nc)
+            elif token.startswith("nc"):
+                nc = int(get_param(r"\d+", token, lexer))
+                number_of_nodes = max(nr, nc)
+            elif token.startswith("nm"):
+                number_of_matrices = int(get_param(r"\d+", token, lexer))
+            else:
+                number_of_nodes = int(get_param(r"\d+", token, lexer))
+                nr = number_of_nodes
+                nc = number_of_nodes
+
+        elif token.startswith("diagonal"):
+            diagonal = get_param("present|absent", token, lexer)
+
+        elif token.startswith("format"):
+            ucinet_format = get_param(
+                """^(fullmatrix|upperhalf|lowerhalf|nodelist1|nodelist2|nodelist1b|\
+edgelist1|edgelist2|blockmatrix|partition)$""",
+                token,
+                lexer,
+            )
+
+        # TODO : row and columns labels
+        elif token.startswith("row"):  # Row labels
+            pass
+        elif token.startswith("column"):  # Columns labels
+            pass
+
+        elif token.startswith("labels"):
+            token = next(lexer)
+            i = 0
+            while token not in KEYWORDS:
+                if token.startswith("embedded"):
+                    row_labels_embedded = True
+                    cols_labels_embedded = True
+                    break
+                else:
+                    labels[i] = token.replace(
+                        '"', ""
+                    )  # for labels with embedded spaces
+                    i += 1
+                    try:
+                        token = next(lexer)
+                    except StopIteration:
+                        break
+        elif token.startswith("data"):
+            break
+
+    data_lines = lines.lower().split("data:", 1)[1]
+    # Generate edges
+    params = {}
+    if cols_labels_embedded:
+        # params['names'] = True
+        labels = dict(zip(range(0, nc), data_lines.splitlines()[1].split()))
+        # params['skip_header'] = 2  # First character is \n
+    if row_labels_embedded:  # Skip first column
+        # TODO rectangular case : labels can differ from rows to columns
+        # params['usecols'] = range(1, nc + 1)
+        pass
+
+    if ucinet_format == "fullmatrix":
+        # In Python3 genfromtxt requires bytes string
+        try:
+            data_lines = bytes(data_lines, "utf-8")
+        except TypeError:
+            pass
+        # Do not use splitlines() because it is not necessarily written as a square matrix
+        data = genfromtxt([data_lines], case_sensitive=False, **params)
+        if cols_labels_embedded or row_labels_embedded:
+            # data = insert(data, 0, float('nan'))
+            data = data[~isnan(data)]
+        mat = reshape(data, (max(number_of_nodes, nr), -1))
+        G = eg.from_numpy_array(mat, create_using=eg.MultiDiGraph())
+
+    elif ucinet_format in (
+        "nodelist1",
+        "nodelist1b",
+    ):  # Since genfromtxt only accepts square matrix...
+        s = ""
+        for i, line in enumerate(data_lines.splitlines()):
+            row = line.split()
+            if row:
+                if ucinet_format == "nodelist1b" and row[0] == "0":
+                    pass
+                else:
+                    for neighbor in row[1:]:
+                        if ucinet_format == "nodelist1":
+                            source = row[0]
+                        else:
+                            source = str(i)
+                        s += source + " " + neighbor + "\n"
+
+        G = eg.parse_edgelist(
+            s.splitlines(),
+            nodetype=str if row_labels_embedded and cols_labels_embedded else int,
+            create_using=eg.MultiDiGraph(),
+        )
+
+        if not row_labels_embedded or not cols_labels_embedded:
+            G = eg.relabel_nodes(G, dict(zip(list(G.nodes), [i - 1 for i in G.nodes])))
+
+    elif ucinet_format == "edgelist1":
+        G = eg.parse_edgelist(
+            data_lines.splitlines(),
+            nodetype=str if row_labels_embedded and cols_labels_embedded else int,
+            create_using=eg.MultiDiGraph(),
+        )
+
+        if not row_labels_embedded or not cols_labels_embedded:
+            G = eg.relabel_nodes(G, dict(zip(list(G.nodes), [i - 1 for i in G.nodes])))
+
+    # Relabel nodes
+    if labels:
+        try:
+            if len(list(G.nodes)) < number_of_nodes:
+                G.add_nodes_from(
+                    labels.values() if labels else range(0, number_of_nodes)
+                )
+            G = eg.relabel_nodes(G, labels)
+        except KeyError:
+            pass  # Nodes already labelled
+
+    return G
+
+
+def get_param(regex, token, lines):
+    """
+    Get a parameter value in UCINET DL file
+    :param regex: string with the regex matching the parameter value
+    :param token: token (string) in which we search for the parameter
+    :param lines: to iterate through the next tokens
+    :return:
+    """
+    n = token
+    query = re.search(regex, n)
+    while query is None:
+        try:
+            n = next(lines)
+        except StopIteration:
+            raise Exception("Parameter %s value not recognized" % token)
+        query = re.search(regex, n)
+    return query.group()
```

## easygraph/readwrite/gexf.py

 * *Ordering differences only*

```diff
@@ -1,1017 +1,1017 @@
-import itertools
-import time
-
-from xml.etree.ElementTree import Element
-from xml.etree.ElementTree import ElementTree
-from xml.etree.ElementTree import SubElement
-from xml.etree.ElementTree import register_namespace
-from xml.etree.ElementTree import tostring
-
-import easygraph as eg
-
-from easygraph.utils import *
-
-
-__all__ = ["write_gexf", "relabel_gexf_graph", "generate_gexf", "read_gexf"]
-
-
-def write_gexf(G, path, encoding="utf-8", prettyprint=True, version="1.2draft"):
-    """Write G in GEXF format to path.
-
-    "GEXF (Graph Exchange XML Format) is a language for describing
-    complex networks structures, their associated data and dynamics" [1]_.
-
-    Node attributes are checked according to the version of the GEXF
-    schemas used for parameters which are not user defined,
-    e.g. visualization 'viz' [2]_. See example for usage.
-
-    Parameters
-    ----------
-    G : graph
-       An EasyGraph graph
-    path : file or string
-       File or file name to write.
-       File names ending in .gz or .bz2 will be compressed.
-    encoding : string (optional, default: 'utf-8')
-       Encoding for text data.
-    prettyprint : bool (optional, default: True)
-       If True use line breaks and indenting in output XML.
-    version: string (optional, default: '1.2draft')
-       The version of GEXF to be used for nodes attributes checking
-
-    Examples
-    --------
-    >>> G = eg.path_graph(4)
-    >>> eg.write_gexf(G, "test.gexf")
-
-    """
-    writer = GEXFWriter(encoding=encoding, prettyprint=prettyprint, version=version)
-    writer.add_graph(G)
-    writer.write(path)
-
-
-def generate_gexf(G, encoding="utf-8", prettyprint=True, version="1.2draft"):
-    """Generate lines of GEXF format representation of G.
-
-    "GEXF (Graph Exchange XML Format) is a language for describing
-    complex networks structures, their associated data and dynamics" [1]_.
-
-    Parameters
-    ----------
-    G : graph
-    A EasyGraph graph
-    encoding : string (optional, default: 'utf-8')
-    Encoding for text data.
-    prettyprint : bool (optional, default: True)
-    If True use line breaks and indenting in output XML.
-    version : string (default: 1.2draft)
-    Version of GEFX File Format (see http://gexf.net/schema.html)
-    Supported values: "1.1draft", "1.2draft"
-
-
-    Examples
-    --------
-    >>> G = eg.path_graph(4)
-    >>> linefeed = chr(10)  # linefeed=\n
-    >>> s = linefeed.join(eg.generate_gexf(G))
-    >>> for line in eg.generate_gexf(G):  # doctest: +SKIP
-    ...     print(line)
-
-    Notes
-    -----
-    This implementation does not support mixed graphs (directed and undirected
-    edges together).
-
-    The node id attribute is set to be the string of the node label.
-    If you want to specify an id use set it as node data, e.g.
-    node['a']['id']=1 to set the id of node 'a' to 1.
-
-    References
-    ----------
-    .. [1] GEXF File Format, https://gephi.org/gexf/format/
-    """
-    writer = GEXFWriter(encoding=encoding, prettyprint=prettyprint, version=version)
-    writer.add_graph(G)
-    yield from str(writer).splitlines()
-
-
-@open_file(0, mode="rb")
-def read_gexf(path, node_type=None, relabel=False, version="1.2draft"):
-    """Read graph in GEXF format from path.
-
-    "GEXF (Graph Exchange XML Format) is a language for describing
-    complex networks structures, their associated data and dynamics" [1]_.
-
-    Parameters
-    ----------
-    path : file or string
-       File or file name to read.
-       File names ending in .gz or .bz2 will be decompressed.
-    node_type: Python type (default: None)
-       Convert node ids to this type if not None.
-    relabel : bool (default: False)
-       If True relabel the nodes to use the GEXF node "label" attribute
-       instead of the node "id" attribute as the EasyGraph node label.
-    version : string (default: 1.2draft)
-    Version of GEFX File Format (see http://gexf.net/schema.html)
-       Supported values: "1.1draft", "1.2draft"
-
-    Returns
-    -------
-    graph: EasyGraph graph
-        If no parallel edges are found a Graph or DiGraph is returned.
-        Otherwise a MultiGraph or MultiDiGraph is returned.
-
-    Notes
-    -----
-    This implementation does not support mixed graphs (directed and undirected
-    edges together).
-
-    References
-    ----------
-    .. [1] GEXF File Format, http://gexf.net/
-    """
-    reader = GEXFReader(node_type=node_type, version=version)
-    if relabel:
-        G = relabel_gexf_graph(reader(path))
-    else:
-        G = reader(path)
-    return G
-
-
-class GEXF:
-    versions = {}
-    d = {
-        "NS_GEXF": "http://www.gexf.net/1.1draft",
-        "NS_VIZ": "http://www.gexf.net/1.1draft/viz",
-        "NS_XSI": "http://www.w3.org/2001/XMLSchema-instance",
-        "SCHEMALOCATION": " ".join(
-            ["http://www.gexf.net/1.1draft", "http://www.gexf.net/1.1draft/gexf.xsd"]
-        ),
-        "VERSION": "1.1",
-    }
-    versions["1.1draft"] = d
-    d = {
-        "NS_GEXF": "http://www.gexf.net/1.2draft",
-        "NS_VIZ": "http://www.gexf.net/1.2draft/viz",
-        "NS_XSI": "http://www.w3.org/2001/XMLSchema-instance",
-        "SCHEMALOCATION": " ".join(
-            ["http://www.gexf.net/1.2draft", "http://www.gexf.net/1.2draft/gexf.xsd"]
-        ),
-        "VERSION": "1.2",
-    }
-    versions["1.2draft"] = d
-
-    def construct_types(self):
-        types = [
-            (int, "integer"),
-            (float, "float"),
-            (float, "double"),
-            (bool, "boolean"),
-            (list, "string"),
-            (dict, "string"),
-            (int, "long"),
-            (str, "liststring"),
-            (str, "anyURI"),
-            (str, "string"),
-        ]
-
-        # These additions to types allow writing numpy types
-        try:
-            import numpy as np
-        except ImportError:
-            pass
-        else:
-            # prepend so that python types are created upon read (last entry wins)
-            types = [
-                (np.float64, "float"),
-                (np.float32, "float"),
-                (np.float16, "float"),
-                (np.float_, "float"),
-                (np.int_, "int"),
-                (np.int8, "int"),
-                (np.int16, "int"),
-                (np.int32, "int"),
-                (np.int64, "int"),
-                (np.uint8, "int"),
-                (np.uint16, "int"),
-                (np.uint32, "int"),
-                (np.uint64, "int"),
-                (np.int_, "int"),
-                (np.intc, "int"),
-                (np.intp, "int"),
-            ] + types
-
-        self.xml_type = dict(types)
-        self.python_type = dict(reversed(a) for a in types)
-
-    # http://www.w3.org/TR/xmlschema-2/#boolean
-    convert_bool = {
-        "true": True,
-        "false": False,
-        "True": True,
-        "False": False,
-        "0": False,
-        0: False,
-        "1": True,
-        1: True,
-    }
-
-    def set_version(self, version):
-        d = self.versions.get(version)
-        if d is None:
-            raise AssertionError(f"Unknown GEXF version {version}.")
-        self.NS_GEXF = d["NS_GEXF"]
-        self.NS_VIZ = d["NS_VIZ"]
-        self.NS_XSI = d["NS_XSI"]
-        self.SCHEMALOCATION = d["SCHEMALOCATION"]
-        self.VERSION = d["VERSION"]
-        self.version = version
-
-
-class GEXFWriter(GEXF):
-    # class for writing GEXF format files
-    # use write_gexf() function
-    def __init__(
-        self, graph=None, encoding="utf-8", prettyprint=True, version="1.2draft"
-    ):
-        self.construct_types()
-        self.prettyprint = prettyprint
-        self.encoding = encoding
-        self.set_version(version)
-        self.xml = Element(
-            "gexf",
-            {
-                "xmlns": self.NS_GEXF,
-                "xmlns:xsi": self.NS_XSI,
-                "xsi:schemaLocation": self.SCHEMALOCATION,
-                "version": self.VERSION,
-            },
-        )
-
-        # Make meta element a non-graph element
-        # Also add lastmodifieddate as attribute, not tag
-        meta_element = Element("meta")
-        subelement_text = f"EasyGraph"
-        SubElement(meta_element, "creator").text = subelement_text
-        meta_element.set("lastmodifieddate", time.strftime("%Y-%m-%d"))
-        self.xml.append(meta_element)
-
-        register_namespace("viz", self.NS_VIZ)
-
-        # counters for edge and attribute identifiers
-        self.edge_id = itertools.count()
-        self.attr_id = itertools.count()
-        self.all_edge_ids = set()
-        # default attributes are stored in dictionaries
-        self.attr = {}
-        self.attr["node"] = {}
-        self.attr["edge"] = {}
-        self.attr["node"]["dynamic"] = {}
-        self.attr["node"]["static"] = {}
-        self.attr["edge"]["dynamic"] = {}
-        self.attr["edge"]["static"] = {}
-
-        if graph is not None:
-            self.add_graph(graph)
-
-    def __str__(self):
-        if self.prettyprint:
-            self.indent(self.xml)
-        s = tostring(self.xml).decode(self.encoding)
-        return s
-
-    def add_graph(self, G):
-        # first pass through G collecting edge ids
-        for u, v, dd in G.edges:
-            eid = dd.get("id")
-            if eid is not None:
-                self.all_edge_ids.add(str(eid))
-        # set graph attributes
-        if G.graph.get("mode") == "dynamic":
-            mode = "dynamic"
-        else:
-            mode = "static"
-        # Add a graph element to the XML
-        if G.is_directed():
-            default = "directed"
-        else:
-            default = "undirected"
-        name = G.graph.get("name", "")
-        graph_element = Element("graph", defaultedgetype=default, mode=mode, name=name)
-        self.graph_element = graph_element
-        self.add_nodes(G, graph_element)
-        self.add_edges(G, graph_element)
-        self.xml.append(graph_element)
-
-    def add_nodes(self, G, graph_element):
-        nodes_element = Element("nodes")
-        for node, data in G.nodes.items():
-            node_data = data.copy()
-            node_id = str(node_data.pop("id", node))
-            kw = {"id": node_id}
-            label = str(node_data.pop("label", node))
-            kw["label"] = label
-            try:
-                pid = node_data.pop("pid")
-                kw["pid"] = str(pid)
-            except KeyError:
-                pass
-            try:
-                start = node_data.pop("start")
-                kw["start"] = str(start)
-                self.alter_graph_mode_timeformat(start)
-            except KeyError:
-                pass
-            try:
-                end = node_data.pop("end")
-                kw["end"] = str(end)
-                self.alter_graph_mode_timeformat(end)
-            except KeyError:
-                pass
-            # add node element with attributes
-            node_element = Element("node", **kw)
-            # add node element and attr subelements
-            default = G.graph.get("node_default", {})
-            node_data = self.add_parents(node_element, node_data)
-            if self.VERSION == "1.1":
-                node_data = self.add_slices(node_element, node_data)
-            else:
-                node_data = self.add_spells(node_element, node_data)
-            node_data = self.add_viz(node_element, node_data)
-            node_data = self.add_attributes("node", node_element, node_data, default)
-            nodes_element.append(node_element)
-        graph_element.append(nodes_element)
-
-    def get_attr_id(self, title, attr_type, edge_or_node, default, mode):
-        # find the id of the attribute or generate a new id
-        try:
-            return self.attr[edge_or_node][mode][title]
-        except KeyError:
-            # generate new id
-            new_id = str(next(self.attr_id))
-            self.attr[edge_or_node][mode][title] = new_id
-            attr_kwargs = {"id": new_id, "title": title, "type": attr_type}
-            attribute = Element("attribute", **attr_kwargs)
-            # add subelement for data default value if present
-            default_title = default.get(title)
-            if default_title is not None:
-                default_element = Element("default")
-                default_element.text = str(default_title)
-                attribute.append(default_element)
-            # new insert it into the XML
-            attributes_element = None
-            for a in self.graph_element.findall("attributes"):
-                # find existing attributes element by class and mode
-                a_class = a.get("class")
-                a_mode = a.get("mode", "static")
-                if a_class == edge_or_node and a_mode == mode:
-                    attributes_element = a
-            if attributes_element is None:
-                # create new attributes element
-                attr_kwargs = {"mode": mode, "class": edge_or_node}
-                attributes_element = Element("attributes", **attr_kwargs)
-                self.graph_element.insert(0, attributes_element)
-            attributes_element.append(attribute)
-        return new_id
-
-    def add_edges(self, G, graph_element):
-        def edge_key_data(G):
-            if G.is_multigraph():
-                for u, v, key, data in G.edges:
-                    edge_data = data.copy()
-                    edge_data.update(key=key)
-                    edge_id = edge_data.pop("id", None)
-                    if edge_id is None:
-                        edge_id = next(self.edge_id)
-                        while str(edge_id) in self.all_edge_ids:
-                            edge_id = next(self.edge_id)
-                        self.all_edge_ids.add(str(edge_id))
-                    yield u, v, edge_id, edge_data
-            else:
-                for u, v, data in G.edges:
-                    edge_data = data.copy()
-                    edge_id = edge_data.pop("id", None)
-                    if edge_id is None:
-                        edge_id = next(self.edge_id)
-                        while str(edge_id) in self.all_edge_ids:
-                            edge_id = next(self.edge_id)
-                        self.all_edge_ids.add(str(edge_id))
-                    yield u, v, edge_id, edge_data
-
-        edges_element = Element("edges")
-        for u, v, key, edge_data in edge_key_data(G):
-            kw = {"id": str(key)}
-            try:
-                edge_label = edge_data.pop("label")
-                kw["label"] = str(edge_label)
-            except KeyError:
-                pass
-            try:
-                edge_weight = edge_data.pop("weight")
-                kw["weight"] = str(edge_weight)
-            except KeyError:
-                pass
-            try:
-                edge_type = edge_data.pop("type")
-                kw["type"] = str(edge_type)
-            except KeyError:
-                pass
-            try:
-                start = edge_data.pop("start")
-                kw["start"] = str(start)
-                self.alter_graph_mode_timeformat(start)
-            except KeyError:
-                pass
-            try:
-                end = edge_data.pop("end")
-                kw["end"] = str(end)
-                self.alter_graph_mode_timeformat(end)
-            except KeyError:
-                pass
-            source_id = str(G.nodes[u].get("id", u))
-            target_id = str(G.nodes[v].get("id", v))
-            edge_element = Element("edge", source=source_id, target=target_id, **kw)
-            default = G.graph.get("edge_default", {})
-            if self.VERSION == "1.1":
-                edge_data = self.add_slices(edge_element, edge_data)
-            else:
-                edge_data = self.add_spells(edge_element, edge_data)
-            edge_data = self.add_viz(edge_element, edge_data)
-            edge_data = self.add_attributes("edge", edge_element, edge_data, default)
-            edges_element.append(edge_element)
-        graph_element.append(edges_element)
-
-    def add_attributes(self, node_or_edge, xml_obj, data, default):
-        # Add attrvalues to node or edge
-        attvalues = Element("attvalues")
-        if len(data) == 0:
-            return data
-        mode = "static"
-        for k, v in data.items():
-            # rename generic multigraph key to avoid any name conflict
-            if k == "key":
-                k = "easygraph_key"
-            val_type = type(v)
-            if val_type not in self.xml_type:
-                raise TypeError(f"attribute value type is not allowed: {val_type}")
-            if isinstance(v, list):
-                # dynamic data
-                for val, start, end in v:
-                    val_type = type(val)
-                    if start is not None or end is not None:
-                        mode = "dynamic"
-                        self.alter_graph_mode_timeformat(start)
-                        self.alter_graph_mode_timeformat(end)
-                        break
-                attr_id = self.get_attr_id(
-                    str(k), self.xml_type[val_type], node_or_edge, default, mode
-                )
-                for val, start, end in v:
-                    e = Element("attvalue")
-                    e.attrib["for"] = attr_id
-                    e.attrib["value"] = str(val)
-                    # Handle nan, inf, -inf differently
-                    if val_type == float:
-                        if e.attrib["value"] == "inf":
-                            e.attrib["value"] = "INF"
-                        elif e.attrib["value"] == "nan":
-                            e.attrib["value"] = "NaN"
-                        elif e.attrib["value"] == "-inf":
-                            e.attrib["value"] = "-INF"
-                    if start is not None:
-                        e.attrib["start"] = str(start)
-                    if end is not None:
-                        e.attrib["end"] = str(end)
-                    attvalues.append(e)
-            else:
-                # static data
-                mode = "static"
-                attr_id = self.get_attr_id(
-                    str(k), self.xml_type[val_type], node_or_edge, default, mode
-                )
-                e = Element("attvalue")
-                e.attrib["for"] = attr_id
-                if isinstance(v, bool):
-                    e.attrib["value"] = str(v).lower()
-                else:
-                    e.attrib["value"] = str(v)
-                    # Handle float nan, inf, -inf differently
-                    if val_type == float:
-                        if e.attrib["value"] == "inf":
-                            e.attrib["value"] = "INF"
-                        elif e.attrib["value"] == "nan":
-                            e.attrib["value"] = "NaN"
-                        elif e.attrib["value"] == "-inf":
-                            e.attrib["value"] = "-INF"
-                attvalues.append(e)
-        xml_obj.append(attvalues)
-        return data
-
-    def add_viz(self, element, node_data):
-        viz = node_data.pop("viz", False)
-        if viz:
-            color = viz.get("color")
-            if color is not None:
-                if self.VERSION == "1.1":
-                    e = Element(
-                        f"{{{self.NS_VIZ}}}color",
-                        r=str(color.get("r")),
-                        g=str(color.get("g")),
-                        b=str(color.get("b")),
-                    )
-                else:
-                    e = Element(
-                        f"{{{self.NS_VIZ}}}color",
-                        r=str(color.get("r")),
-                        g=str(color.get("g")),
-                        b=str(color.get("b")),
-                        a=str(color.get("a")),
-                    )
-                element.append(e)
-
-            size = viz.get("size")
-            if size is not None:
-                e = Element(f"{{{self.NS_VIZ}}}size", value=str(size))
-                element.append(e)
-
-            thickness = viz.get("thickness")
-            if thickness is not None:
-                e = Element(f"{{{self.NS_VIZ}}}thickness", value=str(thickness))
-                element.append(e)
-
-            shape = viz.get("shape")
-            if shape is not None:
-                if shape.startswith("http"):
-                    e = Element(
-                        f"{{{self.NS_VIZ}}}shape", value="image", uri=str(shape)
-                    )
-                else:
-                    e = Element(f"{{{self.NS_VIZ}}}shape", value=str(shape))
-                element.append(e)
-
-            position = viz.get("position")
-            if position is not None:
-                e = Element(
-                    f"{{{self.NS_VIZ}}}position",
-                    x=str(position.get("x")),
-                    y=str(position.get("y")),
-                    z=str(position.get("z")),
-                )
-                element.append(e)
-        return node_data
-
-    def add_parents(self, node_element, node_data):
-        parents = node_data.pop("parents", False)
-        if parents:
-            parents_element = Element("parents")
-            for p in parents:
-                e = Element("parent")
-                e.attrib["for"] = str(p)
-                parents_element.append(e)
-            node_element.append(parents_element)
-        return node_data
-
-    def add_slices(self, node_or_edge_element, node_or_edge_data):
-        slices = node_or_edge_data.pop("slices", False)
-        if slices:
-            slices_element = Element("slices")
-            for start, end in slices:
-                e = Element("slice", start=str(start), end=str(end))
-                slices_element.append(e)
-            node_or_edge_element.append(slices_element)
-        return node_or_edge_data
-
-    def add_spells(self, node_or_edge_element, node_or_edge_data):
-        spells = node_or_edge_data.pop("spells", False)
-        if spells:
-            spells_element = Element("spells")
-            for start, end in spells:
-                e = Element("spell")
-                if start is not None:
-                    e.attrib["start"] = str(start)
-                    self.alter_graph_mode_timeformat(start)
-                if end is not None:
-                    e.attrib["end"] = str(end)
-                    self.alter_graph_mode_timeformat(end)
-                spells_element.append(e)
-            node_or_edge_element.append(spells_element)
-        return node_or_edge_data
-
-    def alter_graph_mode_timeformat(self, start_or_end):
-        if self.graph_element.get("mode") == "static":
-            if start_or_end is not None:
-                if isinstance(start_or_end, str):
-                    timeformat = "date"
-                elif isinstance(start_or_end, float):
-                    timeformat = "double"
-                elif isinstance(start_or_end, int):
-                    timeformat = "long"
-                else:
-                    raise AssertionError(
-                        "timeformat should be of the type int, float or str"
-                    )
-                self.graph_element.set("timeformat", timeformat)
-                self.graph_element.set("mode", "dynamic")
-
-    def write(self, fh):
-        if self.prettyprint:
-            self.indent(self.xml)
-        document = ElementTree(self.xml)
-        document.write(fh, encoding=self.encoding, xml_declaration=True)
-
-    def indent(self, elem, level=0):
-        i = "\n" + "  " * level
-        if len(elem):
-            if not elem.text or not elem.text.strip():
-                elem.text = i + "  "
-            if not elem.tail or not elem.tail.strip():
-                elem.tail = i
-            for elem in elem:
-                self.indent(elem, level + 1)
-            if not elem.tail or not elem.tail.strip():
-                elem.tail = i
-            if not elem.tail or not elem.tail.strip():
-                elem.tail = i
-        else:
-            if level and (not elem.tail or not elem.tail.strip()):
-                elem.tail = i
-
-
-class GEXFReader(GEXF):
-    # Class to read GEXF format files
-    # use read_gexf() function
-    def __init__(self, node_type=None, version="1.2draft"):
-        self.construct_types()
-        self.node_type = node_type
-        # assume simple graph and test for multigraph on read
-        self.simple_graph = True
-        self.set_version(version)
-
-    def __call__(self, stream):
-        self.xml = ElementTree(file=stream)
-        g = self.xml.find(f"{{{self.NS_GEXF}}}graph")
-        if g is not None:
-            return self.make_graph(g)
-        # try all the versions
-        for version in self.versions:
-            self.set_version(version)
-            g = self.xml.find(f"{{{self.NS_GEXF}}}graph")
-            if g is not None:
-                return self.make_graph(g)
-        raise EasyGraphError("No <graph> element in GEXF file.")
-
-    def make_graph(self, graph_xml):
-        edgedefault = graph_xml.get("defaultedgetype", None)
-        if edgedefault == "directed":
-            G = eg.MultiDiGraph()
-        else:
-            G = eg.MultiGraph()
-
-        # graph attributes
-        graph_name = graph_xml.get("name", "")
-        if graph_name != "":
-            G.graph["name"] = graph_name
-        graph_start = graph_xml.get("start")
-        if graph_start is not None:
-            G.graph["start"] = graph_start
-        graph_end = graph_xml.get("end")
-        if graph_end is not None:
-            G.graph["end"] = graph_end
-        graph_mode = graph_xml.get("mode", "")
-        if graph_mode == "dynamic":
-            G.graph["mode"] = "dynamic"
-        else:
-            G.graph["mode"] = "static"
-
-        # timeformat
-        self.timeformat = graph_xml.get("timeformat")
-        if self.timeformat == "date":
-            self.timeformat = "string"
-
-        # node and edge attributes
-        attributes_elements = graph_xml.findall(f"{{{self.NS_GEXF}}}attributes")
-        # dictionaries to hold attributes and attribute defaults
-        node_attr = {}
-        node_default = {}
-        edge_attr = {}
-        edge_default = {}
-        for a in attributes_elements:
-            attr_class = a.get("class")
-            if attr_class == "node":
-                na, nd = self.find_gexf_attributes(a)
-                node_attr.update(na)
-                node_default.update(nd)
-                G.graph["node_default"] = node_default
-            elif attr_class == "edge":
-                ea, ed = self.find_gexf_attributes(a)
-                edge_attr.update(ea)
-                edge_default.update(ed)
-                G.graph["edge_default"] = edge_default
-            else:
-                raise  # unknown attribute class
-
-        # Hack to handle Gephi0.7beta bug
-        # add weight attribute
-        ea = {"weight": {"type": "double", "mode": "static", "title": "weight"}}
-        ed = {}
-        edge_attr.update(ea)
-        edge_default.update(ed)
-        G.graph["edge_default"] = edge_default
-
-        # add nodes
-        nodes_element = graph_xml.find(f"{{{self.NS_GEXF}}}nodes")
-        if nodes_element is not None:
-            for node_xml in nodes_element.findall(f"{{{self.NS_GEXF}}}node"):
-                self.add_node(G, node_xml, node_attr)
-
-        # add edges
-        edges_element = graph_xml.find(f"{{{self.NS_GEXF}}}edges")
-        if edges_element is not None:
-            for edge_xml in edges_element.findall(f"{{{self.NS_GEXF}}}edge"):
-                self.add_edge(G, edge_xml, edge_attr)
-
-        # switch to Graph or DiGraph if no parallel edges were found.
-        if self.simple_graph:
-            if G.is_directed():
-                G = eg.DiGraph(G)
-            else:
-                G = eg.Graph(G)
-        return G
-
-    def add_node(self, G, node_xml, node_attr, node_pid=None):
-        # add a single node with attributes to the graph
-
-        # get attributes and subattributues for node
-        data = self.decode_attr_elements(node_attr, node_xml)
-        data = self.add_parents(data, node_xml)  # add any parents
-        if self.VERSION == "1.1":
-            data = self.add_slices(data, node_xml)  # add slices
-        else:
-            data = self.add_spells(data, node_xml)  # add spells
-        data = self.add_viz(data, node_xml)  # add viz
-        data = self.add_start_end(data, node_xml)  # add start/end
-
-        # find the node id and cast it to the appropriate type
-        node_id = node_xml.get("id")
-        if self.node_type is not None:
-            node_id = self.node_type(node_id)
-
-        # every node should have a label
-        node_label = node_xml.get("label")
-        data["label"] = node_label
-
-        # parent node id
-        node_pid = node_xml.get("pid", node_pid)
-        if node_pid is not None:
-            data["pid"] = node_pid
-
-        # check for subnodes, recursive
-        subnodes = node_xml.find(f"{{{self.NS_GEXF}}}nodes")
-        if subnodes is not None:
-            for node_xml in subnodes.findall(f"{{{self.NS_GEXF}}}node"):
-                self.add_node(G, node_xml, node_attr, node_pid=node_id)
-
-        G.add_node(node_id, **data)
-
-    def add_start_end(self, data, xml):
-        # start and end times
-        ttype = self.timeformat
-        node_start = xml.get("start")
-        if node_start is not None:
-            data["start"] = self.python_type[ttype](node_start)
-        node_end = xml.get("end")
-        if node_end is not None:
-            data["end"] = self.python_type[ttype](node_end)
-        return data
-
-    def add_viz(self, data, node_xml):
-        # add viz element for node
-        viz = {}
-        color = node_xml.find(f"{{{self.NS_VIZ}}}color")
-        if color is not None:
-            if self.VERSION == "1.1":
-                viz["color"] = {
-                    "r": int(color.get("r")),
-                    "g": int(color.get("g")),
-                    "b": int(color.get("b")),
-                }
-            else:
-                viz["color"] = {
-                    "r": int(color.get("r")),
-                    "g": int(color.get("g")),
-                    "b": int(color.get("b")),
-                    "a": float(color.get("a", 1)),
-                }
-
-        size = node_xml.find(f"{{{self.NS_VIZ}}}size")
-        if size is not None:
-            viz["size"] = float(size.get("value"))
-
-        thickness = node_xml.find(f"{{{self.NS_VIZ}}}thickness")
-        if thickness is not None:
-            viz["thickness"] = float(thickness.get("value"))
-
-        shape = node_xml.find(f"{{{self.NS_VIZ}}}shape")
-        if shape is not None:
-            viz["shape"] = shape.get("shape")
-            if viz["shape"] == "image":
-                viz["shape"] = shape.get("uri")
-
-        position = node_xml.find(f"{{{self.NS_VIZ}}}position")
-        if position is not None:
-            viz["position"] = {
-                "x": float(position.get("x", 0)),
-                "y": float(position.get("y", 0)),
-                "z": float(position.get("z", 0)),
-            }
-
-        if len(viz) > 0:
-            data["viz"] = viz
-        return data
-
-    def add_parents(self, data, node_xml):
-        parents_element = node_xml.find(f"{{{self.NS_GEXF}}}parents")
-        if parents_element is not None:
-            data["parents"] = []
-            for p in parents_element.findall(f"{{{self.NS_GEXF}}}parent"):
-                parent = p.get("for")
-                data["parents"].append(parent)
-        return data
-
-    def add_slices(self, data, node_or_edge_xml):
-        slices_element = node_or_edge_xml.find(f"{{{self.NS_GEXF}}}slices")
-        if slices_element is not None:
-            data["slices"] = []
-            for s in slices_element.findall(f"{{{self.NS_GEXF}}}slice"):
-                start = s.get("start")
-                end = s.get("end")
-                data["slices"].append((start, end))
-        return data
-
-    def add_spells(self, data, node_or_edge_xml):
-        spells_element = node_or_edge_xml.find(f"{{{self.NS_GEXF}}}spells")
-        if spells_element is not None:
-            data["spells"] = []
-            ttype = self.timeformat
-            for s in spells_element.findall(f"{{{self.NS_GEXF}}}spell"):
-                start = self.python_type[ttype](s.get("start"))
-                end = self.python_type[ttype](s.get("end"))
-                data["spells"].append((start, end))
-        return data
-
-    def add_edge(self, G, edge_element, edge_attr):
-        # add an edge to the graph
-
-        # raise error if we find mixed directed and undirected edges
-        edge_direction = edge_element.get("type")
-        if G.is_directed() and edge_direction == "undirected":
-            raise EasyGraphError("Undirected edge found in directed graph.")
-        if (not G.is_directed()) and edge_direction == "directed":
-            raise EasyGraphError("Directed edge found in undirected graph.")
-
-        # Get source and target and recast type if required
-        source = edge_element.get("source")
-        target = edge_element.get("target")
-        if self.node_type is not None:
-            source = self.node_type(source)
-            target = self.node_type(target)
-
-        data = self.decode_attr_elements(edge_attr, edge_element)
-        data = self.add_start_end(data, edge_element)
-
-        if self.VERSION == "1.1":
-            data = self.add_slices(data, edge_element)  # add slices
-        else:
-            data = self.add_spells(data, edge_element)  # add spells
-
-        # GEXF stores edge ids as an attribute
-        # EasyGraph uses them as keys in multigraphs
-        # if easygraph_key is not specified as an attribute
-        edge_id = edge_element.get("id")
-        if edge_id is not None:
-            data["id"] = edge_id
-
-        # check if there is a 'multigraph_key' and use that as edge_id
-        multigraph_key = data.pop("easygraph_key", None)
-        if multigraph_key is not None:
-            edge_id = multigraph_key
-
-        weight = edge_element.get("weight")
-        if weight is not None:
-            data["weight"] = float(weight)
-
-        edge_label = edge_element.get("label")
-        if edge_label is not None:
-            data["label"] = edge_label
-
-        if G.has_edge(source, target):
-            # seen this edge before - this is a multigraph
-            self.simple_graph = False
-        G.add_edge(source, target, key=edge_id, **data)
-        if edge_direction == "mutual":
-            G.add_edge(target, source, key=edge_id, **data)
-
-    def decode_attr_elements(self, gexf_keys, obj_xml):
-        # Use the key information to decode the attr XML
-        attr = {}
-        # look for outer '<attvalues>' element
-        attr_element = obj_xml.find(f"{{{self.NS_GEXF}}}attvalues")
-        if attr_element is not None:
-            # loop over <attvalue> elements
-            for a in attr_element.findall(f"{{{self.NS_GEXF}}}attvalue"):
-                key = a.get("for")  # for is required
-                try:  # should be in our gexf_keys dictionary
-                    title = gexf_keys[key]["title"]
-                except KeyError as err:
-                    raise eg.EasyGraphError(f"No attribute defined for={key}.") from err
-                atype = gexf_keys[key]["type"]
-                value = a.get("value")
-                if atype == "boolean":
-                    value = self.convert_bool[value]
-                else:
-                    value = self.python_type[atype](value)
-                if gexf_keys[key]["mode"] == "dynamic":
-                    # for dynamic graphs use list of three-tuples
-                    # [(value1,start1,end1), (value2,start2,end2), etc]
-                    ttype = self.timeformat
-                    start = self.python_type[ttype](a.get("start"))
-                    end = self.python_type[ttype](a.get("end"))
-                    if title in attr:
-                        attr[title].append((value, start, end))
-                    else:
-                        attr[title] = [(value, start, end)]
-                else:
-                    # for static graphs just assign the value
-                    attr[title] = value
-        return attr
-
-    def find_gexf_attributes(self, attributes_element):
-        # Extract all the attributes and defaults
-        attrs = {}
-        defaults = {}
-        mode = attributes_element.get("mode")
-        for k in attributes_element.findall(f"{{{self.NS_GEXF}}}attribute"):
-            attr_id = k.get("id")
-            title = k.get("title")
-            atype = k.get("type")
-            attrs[attr_id] = {"title": title, "type": atype, "mode": mode}
-            # check for the 'default' subelement of key element and add
-            default = k.find(f"{{{self.NS_GEXF}}}default")
-            if default is not None:
-                if atype == "boolean":
-                    value = self.convert_bool[default.text]
-                else:
-                    value = self.python_type[atype](default.text)
-                defaults[title] = value
-        return attrs, defaults
-
-
-def relabel_gexf_graph(G):
-    """Relabel graph using "label" node keyword for node label.
-
-    Parameters
-    ----------
-    G : graph
-       A EasyGraph graph read from GEXF data
-
-    Returns
-    -------
-    H : graph
-      A EasyGraph graph with relabeled nodes
-
-    Raises
-    ------
-    EasyGraphError
-        If node labels are missing or not unique while relabel=True.
-
-    Notes
-    -----
-    This function relabels the nodes in a EasyGraph graph with the
-    "label" attribute.  It also handles relabeling the specific GEXF
-    node attributes "parents", and "pid".
-    """
-    # build mapping of node labels, do some error checking
-    try:
-        mapping = [(u, G.nodes[u]["label"]) for u in G]
-    except KeyError as err:
-        raise EasyGraphError(
-            "Failed to relabel nodes: missing node labels found. Use relabel=False."
-        ) from err
-    x, y = zip(*mapping)
-    if len(set(y)) != len(G):
-        raise EasyGraphError(
-            "Failed to relabel nodes: duplicate node labels found. Use relabel=False."
-        )
-    mapping = dict(mapping)
-    H = eg.relabel_nodes(G, mapping)
-    # relabel attributes
-    for n in G:
-        m = mapping[n]
-        H.nodes[m]["id"] = n
-        H.nodes[m].pop("label")
-        if "pid" in H.nodes[m]:
-            H.nodes[m]["pid"] = mapping[G.nodes[n]["pid"]]
-        if "parents" in H.nodes[m]:
-            H.nodes[m]["parents"] = [mapping[p] for p in G.nodes[n]["parents"]]
-    return H
+import itertools
+import time
+
+from xml.etree.ElementTree import Element
+from xml.etree.ElementTree import ElementTree
+from xml.etree.ElementTree import SubElement
+from xml.etree.ElementTree import register_namespace
+from xml.etree.ElementTree import tostring
+
+import easygraph as eg
+
+from easygraph.utils import *
+
+
+__all__ = ["write_gexf", "relabel_gexf_graph", "generate_gexf", "read_gexf"]
+
+
+def write_gexf(G, path, encoding="utf-8", prettyprint=True, version="1.2draft"):
+    """Write G in GEXF format to path.
+
+    "GEXF (Graph Exchange XML Format) is a language for describing
+    complex networks structures, their associated data and dynamics" [1]_.
+
+    Node attributes are checked according to the version of the GEXF
+    schemas used for parameters which are not user defined,
+    e.g. visualization 'viz' [2]_. See example for usage.
+
+    Parameters
+    ----------
+    G : graph
+       An EasyGraph graph
+    path : file or string
+       File or file name to write.
+       File names ending in .gz or .bz2 will be compressed.
+    encoding : string (optional, default: 'utf-8')
+       Encoding for text data.
+    prettyprint : bool (optional, default: True)
+       If True use line breaks and indenting in output XML.
+    version: string (optional, default: '1.2draft')
+       The version of GEXF to be used for nodes attributes checking
+
+    Examples
+    --------
+    >>> G = eg.path_graph(4)
+    >>> eg.write_gexf(G, "test.gexf")
+
+    """
+    writer = GEXFWriter(encoding=encoding, prettyprint=prettyprint, version=version)
+    writer.add_graph(G)
+    writer.write(path)
+
+
+def generate_gexf(G, encoding="utf-8", prettyprint=True, version="1.2draft"):
+    """Generate lines of GEXF format representation of G.
+
+    "GEXF (Graph Exchange XML Format) is a language for describing
+    complex networks structures, their associated data and dynamics" [1]_.
+
+    Parameters
+    ----------
+    G : graph
+    A EasyGraph graph
+    encoding : string (optional, default: 'utf-8')
+    Encoding for text data.
+    prettyprint : bool (optional, default: True)
+    If True use line breaks and indenting in output XML.
+    version : string (default: 1.2draft)
+    Version of GEFX File Format (see http://gexf.net/schema.html)
+    Supported values: "1.1draft", "1.2draft"
+
+
+    Examples
+    --------
+    >>> G = eg.path_graph(4)
+    >>> linefeed = chr(10)  # linefeed=\n
+    >>> s = linefeed.join(eg.generate_gexf(G))
+    >>> for line in eg.generate_gexf(G):  # doctest: +SKIP
+    ...     print(line)
+
+    Notes
+    -----
+    This implementation does not support mixed graphs (directed and undirected
+    edges together).
+
+    The node id attribute is set to be the string of the node label.
+    If you want to specify an id use set it as node data, e.g.
+    node['a']['id']=1 to set the id of node 'a' to 1.
+
+    References
+    ----------
+    .. [1] GEXF File Format, https://gephi.org/gexf/format/
+    """
+    writer = GEXFWriter(encoding=encoding, prettyprint=prettyprint, version=version)
+    writer.add_graph(G)
+    yield from str(writer).splitlines()
+
+
+@open_file(0, mode="rb")
+def read_gexf(path, node_type=None, relabel=False, version="1.2draft"):
+    """Read graph in GEXF format from path.
+
+    "GEXF (Graph Exchange XML Format) is a language for describing
+    complex networks structures, their associated data and dynamics" [1]_.
+
+    Parameters
+    ----------
+    path : file or string
+       File or file name to read.
+       File names ending in .gz or .bz2 will be decompressed.
+    node_type: Python type (default: None)
+       Convert node ids to this type if not None.
+    relabel : bool (default: False)
+       If True relabel the nodes to use the GEXF node "label" attribute
+       instead of the node "id" attribute as the EasyGraph node label.
+    version : string (default: 1.2draft)
+    Version of GEFX File Format (see http://gexf.net/schema.html)
+       Supported values: "1.1draft", "1.2draft"
+
+    Returns
+    -------
+    graph: EasyGraph graph
+        If no parallel edges are found a Graph or DiGraph is returned.
+        Otherwise a MultiGraph or MultiDiGraph is returned.
+
+    Notes
+    -----
+    This implementation does not support mixed graphs (directed and undirected
+    edges together).
+
+    References
+    ----------
+    .. [1] GEXF File Format, http://gexf.net/
+    """
+    reader = GEXFReader(node_type=node_type, version=version)
+    if relabel:
+        G = relabel_gexf_graph(reader(path))
+    else:
+        G = reader(path)
+    return G
+
+
+class GEXF:
+    versions = {}
+    d = {
+        "NS_GEXF": "http://www.gexf.net/1.1draft",
+        "NS_VIZ": "http://www.gexf.net/1.1draft/viz",
+        "NS_XSI": "http://www.w3.org/2001/XMLSchema-instance",
+        "SCHEMALOCATION": " ".join(
+            ["http://www.gexf.net/1.1draft", "http://www.gexf.net/1.1draft/gexf.xsd"]
+        ),
+        "VERSION": "1.1",
+    }
+    versions["1.1draft"] = d
+    d = {
+        "NS_GEXF": "http://www.gexf.net/1.2draft",
+        "NS_VIZ": "http://www.gexf.net/1.2draft/viz",
+        "NS_XSI": "http://www.w3.org/2001/XMLSchema-instance",
+        "SCHEMALOCATION": " ".join(
+            ["http://www.gexf.net/1.2draft", "http://www.gexf.net/1.2draft/gexf.xsd"]
+        ),
+        "VERSION": "1.2",
+    }
+    versions["1.2draft"] = d
+
+    def construct_types(self):
+        types = [
+            (int, "integer"),
+            (float, "float"),
+            (float, "double"),
+            (bool, "boolean"),
+            (list, "string"),
+            (dict, "string"),
+            (int, "long"),
+            (str, "liststring"),
+            (str, "anyURI"),
+            (str, "string"),
+        ]
+
+        # These additions to types allow writing numpy types
+        try:
+            import numpy as np
+        except ImportError:
+            pass
+        else:
+            # prepend so that python types are created upon read (last entry wins)
+            types = [
+                (np.float64, "float"),
+                (np.float32, "float"),
+                (np.float16, "float"),
+                (np.float_, "float"),
+                (np.int_, "int"),
+                (np.int8, "int"),
+                (np.int16, "int"),
+                (np.int32, "int"),
+                (np.int64, "int"),
+                (np.uint8, "int"),
+                (np.uint16, "int"),
+                (np.uint32, "int"),
+                (np.uint64, "int"),
+                (np.int_, "int"),
+                (np.intc, "int"),
+                (np.intp, "int"),
+            ] + types
+
+        self.xml_type = dict(types)
+        self.python_type = dict(reversed(a) for a in types)
+
+    # http://www.w3.org/TR/xmlschema-2/#boolean
+    convert_bool = {
+        "true": True,
+        "false": False,
+        "True": True,
+        "False": False,
+        "0": False,
+        0: False,
+        "1": True,
+        1: True,
+    }
+
+    def set_version(self, version):
+        d = self.versions.get(version)
+        if d is None:
+            raise AssertionError(f"Unknown GEXF version {version}.")
+        self.NS_GEXF = d["NS_GEXF"]
+        self.NS_VIZ = d["NS_VIZ"]
+        self.NS_XSI = d["NS_XSI"]
+        self.SCHEMALOCATION = d["SCHEMALOCATION"]
+        self.VERSION = d["VERSION"]
+        self.version = version
+
+
+class GEXFWriter(GEXF):
+    # class for writing GEXF format files
+    # use write_gexf() function
+    def __init__(
+        self, graph=None, encoding="utf-8", prettyprint=True, version="1.2draft"
+    ):
+        self.construct_types()
+        self.prettyprint = prettyprint
+        self.encoding = encoding
+        self.set_version(version)
+        self.xml = Element(
+            "gexf",
+            {
+                "xmlns": self.NS_GEXF,
+                "xmlns:xsi": self.NS_XSI,
+                "xsi:schemaLocation": self.SCHEMALOCATION,
+                "version": self.VERSION,
+            },
+        )
+
+        # Make meta element a non-graph element
+        # Also add lastmodifieddate as attribute, not tag
+        meta_element = Element("meta")
+        subelement_text = f"EasyGraph"
+        SubElement(meta_element, "creator").text = subelement_text
+        meta_element.set("lastmodifieddate", time.strftime("%Y-%m-%d"))
+        self.xml.append(meta_element)
+
+        register_namespace("viz", self.NS_VIZ)
+
+        # counters for edge and attribute identifiers
+        self.edge_id = itertools.count()
+        self.attr_id = itertools.count()
+        self.all_edge_ids = set()
+        # default attributes are stored in dictionaries
+        self.attr = {}
+        self.attr["node"] = {}
+        self.attr["edge"] = {}
+        self.attr["node"]["dynamic"] = {}
+        self.attr["node"]["static"] = {}
+        self.attr["edge"]["dynamic"] = {}
+        self.attr["edge"]["static"] = {}
+
+        if graph is not None:
+            self.add_graph(graph)
+
+    def __str__(self):
+        if self.prettyprint:
+            self.indent(self.xml)
+        s = tostring(self.xml).decode(self.encoding)
+        return s
+
+    def add_graph(self, G):
+        # first pass through G collecting edge ids
+        for u, v, dd in G.edges:
+            eid = dd.get("id")
+            if eid is not None:
+                self.all_edge_ids.add(str(eid))
+        # set graph attributes
+        if G.graph.get("mode") == "dynamic":
+            mode = "dynamic"
+        else:
+            mode = "static"
+        # Add a graph element to the XML
+        if G.is_directed():
+            default = "directed"
+        else:
+            default = "undirected"
+        name = G.graph.get("name", "")
+        graph_element = Element("graph", defaultedgetype=default, mode=mode, name=name)
+        self.graph_element = graph_element
+        self.add_nodes(G, graph_element)
+        self.add_edges(G, graph_element)
+        self.xml.append(graph_element)
+
+    def add_nodes(self, G, graph_element):
+        nodes_element = Element("nodes")
+        for node, data in G.nodes.items():
+            node_data = data.copy()
+            node_id = str(node_data.pop("id", node))
+            kw = {"id": node_id}
+            label = str(node_data.pop("label", node))
+            kw["label"] = label
+            try:
+                pid = node_data.pop("pid")
+                kw["pid"] = str(pid)
+            except KeyError:
+                pass
+            try:
+                start = node_data.pop("start")
+                kw["start"] = str(start)
+                self.alter_graph_mode_timeformat(start)
+            except KeyError:
+                pass
+            try:
+                end = node_data.pop("end")
+                kw["end"] = str(end)
+                self.alter_graph_mode_timeformat(end)
+            except KeyError:
+                pass
+            # add node element with attributes
+            node_element = Element("node", **kw)
+            # add node element and attr subelements
+            default = G.graph.get("node_default", {})
+            node_data = self.add_parents(node_element, node_data)
+            if self.VERSION == "1.1":
+                node_data = self.add_slices(node_element, node_data)
+            else:
+                node_data = self.add_spells(node_element, node_data)
+            node_data = self.add_viz(node_element, node_data)
+            node_data = self.add_attributes("node", node_element, node_data, default)
+            nodes_element.append(node_element)
+        graph_element.append(nodes_element)
+
+    def get_attr_id(self, title, attr_type, edge_or_node, default, mode):
+        # find the id of the attribute or generate a new id
+        try:
+            return self.attr[edge_or_node][mode][title]
+        except KeyError:
+            # generate new id
+            new_id = str(next(self.attr_id))
+            self.attr[edge_or_node][mode][title] = new_id
+            attr_kwargs = {"id": new_id, "title": title, "type": attr_type}
+            attribute = Element("attribute", **attr_kwargs)
+            # add subelement for data default value if present
+            default_title = default.get(title)
+            if default_title is not None:
+                default_element = Element("default")
+                default_element.text = str(default_title)
+                attribute.append(default_element)
+            # new insert it into the XML
+            attributes_element = None
+            for a in self.graph_element.findall("attributes"):
+                # find existing attributes element by class and mode
+                a_class = a.get("class")
+                a_mode = a.get("mode", "static")
+                if a_class == edge_or_node and a_mode == mode:
+                    attributes_element = a
+            if attributes_element is None:
+                # create new attributes element
+                attr_kwargs = {"mode": mode, "class": edge_or_node}
+                attributes_element = Element("attributes", **attr_kwargs)
+                self.graph_element.insert(0, attributes_element)
+            attributes_element.append(attribute)
+        return new_id
+
+    def add_edges(self, G, graph_element):
+        def edge_key_data(G):
+            if G.is_multigraph():
+                for u, v, key, data in G.edges:
+                    edge_data = data.copy()
+                    edge_data.update(key=key)
+                    edge_id = edge_data.pop("id", None)
+                    if edge_id is None:
+                        edge_id = next(self.edge_id)
+                        while str(edge_id) in self.all_edge_ids:
+                            edge_id = next(self.edge_id)
+                        self.all_edge_ids.add(str(edge_id))
+                    yield u, v, edge_id, edge_data
+            else:
+                for u, v, data in G.edges:
+                    edge_data = data.copy()
+                    edge_id = edge_data.pop("id", None)
+                    if edge_id is None:
+                        edge_id = next(self.edge_id)
+                        while str(edge_id) in self.all_edge_ids:
+                            edge_id = next(self.edge_id)
+                        self.all_edge_ids.add(str(edge_id))
+                    yield u, v, edge_id, edge_data
+
+        edges_element = Element("edges")
+        for u, v, key, edge_data in edge_key_data(G):
+            kw = {"id": str(key)}
+            try:
+                edge_label = edge_data.pop("label")
+                kw["label"] = str(edge_label)
+            except KeyError:
+                pass
+            try:
+                edge_weight = edge_data.pop("weight")
+                kw["weight"] = str(edge_weight)
+            except KeyError:
+                pass
+            try:
+                edge_type = edge_data.pop("type")
+                kw["type"] = str(edge_type)
+            except KeyError:
+                pass
+            try:
+                start = edge_data.pop("start")
+                kw["start"] = str(start)
+                self.alter_graph_mode_timeformat(start)
+            except KeyError:
+                pass
+            try:
+                end = edge_data.pop("end")
+                kw["end"] = str(end)
+                self.alter_graph_mode_timeformat(end)
+            except KeyError:
+                pass
+            source_id = str(G.nodes[u].get("id", u))
+            target_id = str(G.nodes[v].get("id", v))
+            edge_element = Element("edge", source=source_id, target=target_id, **kw)
+            default = G.graph.get("edge_default", {})
+            if self.VERSION == "1.1":
+                edge_data = self.add_slices(edge_element, edge_data)
+            else:
+                edge_data = self.add_spells(edge_element, edge_data)
+            edge_data = self.add_viz(edge_element, edge_data)
+            edge_data = self.add_attributes("edge", edge_element, edge_data, default)
+            edges_element.append(edge_element)
+        graph_element.append(edges_element)
+
+    def add_attributes(self, node_or_edge, xml_obj, data, default):
+        # Add attrvalues to node or edge
+        attvalues = Element("attvalues")
+        if len(data) == 0:
+            return data
+        mode = "static"
+        for k, v in data.items():
+            # rename generic multigraph key to avoid any name conflict
+            if k == "key":
+                k = "easygraph_key"
+            val_type = type(v)
+            if val_type not in self.xml_type:
+                raise TypeError(f"attribute value type is not allowed: {val_type}")
+            if isinstance(v, list):
+                # dynamic data
+                for val, start, end in v:
+                    val_type = type(val)
+                    if start is not None or end is not None:
+                        mode = "dynamic"
+                        self.alter_graph_mode_timeformat(start)
+                        self.alter_graph_mode_timeformat(end)
+                        break
+                attr_id = self.get_attr_id(
+                    str(k), self.xml_type[val_type], node_or_edge, default, mode
+                )
+                for val, start, end in v:
+                    e = Element("attvalue")
+                    e.attrib["for"] = attr_id
+                    e.attrib["value"] = str(val)
+                    # Handle nan, inf, -inf differently
+                    if val_type == float:
+                        if e.attrib["value"] == "inf":
+                            e.attrib["value"] = "INF"
+                        elif e.attrib["value"] == "nan":
+                            e.attrib["value"] = "NaN"
+                        elif e.attrib["value"] == "-inf":
+                            e.attrib["value"] = "-INF"
+                    if start is not None:
+                        e.attrib["start"] = str(start)
+                    if end is not None:
+                        e.attrib["end"] = str(end)
+                    attvalues.append(e)
+            else:
+                # static data
+                mode = "static"
+                attr_id = self.get_attr_id(
+                    str(k), self.xml_type[val_type], node_or_edge, default, mode
+                )
+                e = Element("attvalue")
+                e.attrib["for"] = attr_id
+                if isinstance(v, bool):
+                    e.attrib["value"] = str(v).lower()
+                else:
+                    e.attrib["value"] = str(v)
+                    # Handle float nan, inf, -inf differently
+                    if val_type == float:
+                        if e.attrib["value"] == "inf":
+                            e.attrib["value"] = "INF"
+                        elif e.attrib["value"] == "nan":
+                            e.attrib["value"] = "NaN"
+                        elif e.attrib["value"] == "-inf":
+                            e.attrib["value"] = "-INF"
+                attvalues.append(e)
+        xml_obj.append(attvalues)
+        return data
+
+    def add_viz(self, element, node_data):
+        viz = node_data.pop("viz", False)
+        if viz:
+            color = viz.get("color")
+            if color is not None:
+                if self.VERSION == "1.1":
+                    e = Element(
+                        f"{{{self.NS_VIZ}}}color",
+                        r=str(color.get("r")),
+                        g=str(color.get("g")),
+                        b=str(color.get("b")),
+                    )
+                else:
+                    e = Element(
+                        f"{{{self.NS_VIZ}}}color",
+                        r=str(color.get("r")),
+                        g=str(color.get("g")),
+                        b=str(color.get("b")),
+                        a=str(color.get("a")),
+                    )
+                element.append(e)
+
+            size = viz.get("size")
+            if size is not None:
+                e = Element(f"{{{self.NS_VIZ}}}size", value=str(size))
+                element.append(e)
+
+            thickness = viz.get("thickness")
+            if thickness is not None:
+                e = Element(f"{{{self.NS_VIZ}}}thickness", value=str(thickness))
+                element.append(e)
+
+            shape = viz.get("shape")
+            if shape is not None:
+                if shape.startswith("http"):
+                    e = Element(
+                        f"{{{self.NS_VIZ}}}shape", value="image", uri=str(shape)
+                    )
+                else:
+                    e = Element(f"{{{self.NS_VIZ}}}shape", value=str(shape))
+                element.append(e)
+
+            position = viz.get("position")
+            if position is not None:
+                e = Element(
+                    f"{{{self.NS_VIZ}}}position",
+                    x=str(position.get("x")),
+                    y=str(position.get("y")),
+                    z=str(position.get("z")),
+                )
+                element.append(e)
+        return node_data
+
+    def add_parents(self, node_element, node_data):
+        parents = node_data.pop("parents", False)
+        if parents:
+            parents_element = Element("parents")
+            for p in parents:
+                e = Element("parent")
+                e.attrib["for"] = str(p)
+                parents_element.append(e)
+            node_element.append(parents_element)
+        return node_data
+
+    def add_slices(self, node_or_edge_element, node_or_edge_data):
+        slices = node_or_edge_data.pop("slices", False)
+        if slices:
+            slices_element = Element("slices")
+            for start, end in slices:
+                e = Element("slice", start=str(start), end=str(end))
+                slices_element.append(e)
+            node_or_edge_element.append(slices_element)
+        return node_or_edge_data
+
+    def add_spells(self, node_or_edge_element, node_or_edge_data):
+        spells = node_or_edge_data.pop("spells", False)
+        if spells:
+            spells_element = Element("spells")
+            for start, end in spells:
+                e = Element("spell")
+                if start is not None:
+                    e.attrib["start"] = str(start)
+                    self.alter_graph_mode_timeformat(start)
+                if end is not None:
+                    e.attrib["end"] = str(end)
+                    self.alter_graph_mode_timeformat(end)
+                spells_element.append(e)
+            node_or_edge_element.append(spells_element)
+        return node_or_edge_data
+
+    def alter_graph_mode_timeformat(self, start_or_end):
+        if self.graph_element.get("mode") == "static":
+            if start_or_end is not None:
+                if isinstance(start_or_end, str):
+                    timeformat = "date"
+                elif isinstance(start_or_end, float):
+                    timeformat = "double"
+                elif isinstance(start_or_end, int):
+                    timeformat = "long"
+                else:
+                    raise AssertionError(
+                        "timeformat should be of the type int, float or str"
+                    )
+                self.graph_element.set("timeformat", timeformat)
+                self.graph_element.set("mode", "dynamic")
+
+    def write(self, fh):
+        if self.prettyprint:
+            self.indent(self.xml)
+        document = ElementTree(self.xml)
+        document.write(fh, encoding=self.encoding, xml_declaration=True)
+
+    def indent(self, elem, level=0):
+        i = "\n" + "  " * level
+        if len(elem):
+            if not elem.text or not elem.text.strip():
+                elem.text = i + "  "
+            if not elem.tail or not elem.tail.strip():
+                elem.tail = i
+            for elem in elem:
+                self.indent(elem, level + 1)
+            if not elem.tail or not elem.tail.strip():
+                elem.tail = i
+            if not elem.tail or not elem.tail.strip():
+                elem.tail = i
+        else:
+            if level and (not elem.tail or not elem.tail.strip()):
+                elem.tail = i
+
+
+class GEXFReader(GEXF):
+    # Class to read GEXF format files
+    # use read_gexf() function
+    def __init__(self, node_type=None, version="1.2draft"):
+        self.construct_types()
+        self.node_type = node_type
+        # assume simple graph and test for multigraph on read
+        self.simple_graph = True
+        self.set_version(version)
+
+    def __call__(self, stream):
+        self.xml = ElementTree(file=stream)
+        g = self.xml.find(f"{{{self.NS_GEXF}}}graph")
+        if g is not None:
+            return self.make_graph(g)
+        # try all the versions
+        for version in self.versions:
+            self.set_version(version)
+            g = self.xml.find(f"{{{self.NS_GEXF}}}graph")
+            if g is not None:
+                return self.make_graph(g)
+        raise EasyGraphError("No <graph> element in GEXF file.")
+
+    def make_graph(self, graph_xml):
+        edgedefault = graph_xml.get("defaultedgetype", None)
+        if edgedefault == "directed":
+            G = eg.MultiDiGraph()
+        else:
+            G = eg.MultiGraph()
+
+        # graph attributes
+        graph_name = graph_xml.get("name", "")
+        if graph_name != "":
+            G.graph["name"] = graph_name
+        graph_start = graph_xml.get("start")
+        if graph_start is not None:
+            G.graph["start"] = graph_start
+        graph_end = graph_xml.get("end")
+        if graph_end is not None:
+            G.graph["end"] = graph_end
+        graph_mode = graph_xml.get("mode", "")
+        if graph_mode == "dynamic":
+            G.graph["mode"] = "dynamic"
+        else:
+            G.graph["mode"] = "static"
+
+        # timeformat
+        self.timeformat = graph_xml.get("timeformat")
+        if self.timeformat == "date":
+            self.timeformat = "string"
+
+        # node and edge attributes
+        attributes_elements = graph_xml.findall(f"{{{self.NS_GEXF}}}attributes")
+        # dictionaries to hold attributes and attribute defaults
+        node_attr = {}
+        node_default = {}
+        edge_attr = {}
+        edge_default = {}
+        for a in attributes_elements:
+            attr_class = a.get("class")
+            if attr_class == "node":
+                na, nd = self.find_gexf_attributes(a)
+                node_attr.update(na)
+                node_default.update(nd)
+                G.graph["node_default"] = node_default
+            elif attr_class == "edge":
+                ea, ed = self.find_gexf_attributes(a)
+                edge_attr.update(ea)
+                edge_default.update(ed)
+                G.graph["edge_default"] = edge_default
+            else:
+                raise  # unknown attribute class
+
+        # Hack to handle Gephi0.7beta bug
+        # add weight attribute
+        ea = {"weight": {"type": "double", "mode": "static", "title": "weight"}}
+        ed = {}
+        edge_attr.update(ea)
+        edge_default.update(ed)
+        G.graph["edge_default"] = edge_default
+
+        # add nodes
+        nodes_element = graph_xml.find(f"{{{self.NS_GEXF}}}nodes")
+        if nodes_element is not None:
+            for node_xml in nodes_element.findall(f"{{{self.NS_GEXF}}}node"):
+                self.add_node(G, node_xml, node_attr)
+
+        # add edges
+        edges_element = graph_xml.find(f"{{{self.NS_GEXF}}}edges")
+        if edges_element is not None:
+            for edge_xml in edges_element.findall(f"{{{self.NS_GEXF}}}edge"):
+                self.add_edge(G, edge_xml, edge_attr)
+
+        # switch to Graph or DiGraph if no parallel edges were found.
+        if self.simple_graph:
+            if G.is_directed():
+                G = eg.DiGraph(G)
+            else:
+                G = eg.Graph(G)
+        return G
+
+    def add_node(self, G, node_xml, node_attr, node_pid=None):
+        # add a single node with attributes to the graph
+
+        # get attributes and subattributues for node
+        data = self.decode_attr_elements(node_attr, node_xml)
+        data = self.add_parents(data, node_xml)  # add any parents
+        if self.VERSION == "1.1":
+            data = self.add_slices(data, node_xml)  # add slices
+        else:
+            data = self.add_spells(data, node_xml)  # add spells
+        data = self.add_viz(data, node_xml)  # add viz
+        data = self.add_start_end(data, node_xml)  # add start/end
+
+        # find the node id and cast it to the appropriate type
+        node_id = node_xml.get("id")
+        if self.node_type is not None:
+            node_id = self.node_type(node_id)
+
+        # every node should have a label
+        node_label = node_xml.get("label")
+        data["label"] = node_label
+
+        # parent node id
+        node_pid = node_xml.get("pid", node_pid)
+        if node_pid is not None:
+            data["pid"] = node_pid
+
+        # check for subnodes, recursive
+        subnodes = node_xml.find(f"{{{self.NS_GEXF}}}nodes")
+        if subnodes is not None:
+            for node_xml in subnodes.findall(f"{{{self.NS_GEXF}}}node"):
+                self.add_node(G, node_xml, node_attr, node_pid=node_id)
+
+        G.add_node(node_id, **data)
+
+    def add_start_end(self, data, xml):
+        # start and end times
+        ttype = self.timeformat
+        node_start = xml.get("start")
+        if node_start is not None:
+            data["start"] = self.python_type[ttype](node_start)
+        node_end = xml.get("end")
+        if node_end is not None:
+            data["end"] = self.python_type[ttype](node_end)
+        return data
+
+    def add_viz(self, data, node_xml):
+        # add viz element for node
+        viz = {}
+        color = node_xml.find(f"{{{self.NS_VIZ}}}color")
+        if color is not None:
+            if self.VERSION == "1.1":
+                viz["color"] = {
+                    "r": int(color.get("r")),
+                    "g": int(color.get("g")),
+                    "b": int(color.get("b")),
+                }
+            else:
+                viz["color"] = {
+                    "r": int(color.get("r")),
+                    "g": int(color.get("g")),
+                    "b": int(color.get("b")),
+                    "a": float(color.get("a", 1)),
+                }
+
+        size = node_xml.find(f"{{{self.NS_VIZ}}}size")
+        if size is not None:
+            viz["size"] = float(size.get("value"))
+
+        thickness = node_xml.find(f"{{{self.NS_VIZ}}}thickness")
+        if thickness is not None:
+            viz["thickness"] = float(thickness.get("value"))
+
+        shape = node_xml.find(f"{{{self.NS_VIZ}}}shape")
+        if shape is not None:
+            viz["shape"] = shape.get("shape")
+            if viz["shape"] == "image":
+                viz["shape"] = shape.get("uri")
+
+        position = node_xml.find(f"{{{self.NS_VIZ}}}position")
+        if position is not None:
+            viz["position"] = {
+                "x": float(position.get("x", 0)),
+                "y": float(position.get("y", 0)),
+                "z": float(position.get("z", 0)),
+            }
+
+        if len(viz) > 0:
+            data["viz"] = viz
+        return data
+
+    def add_parents(self, data, node_xml):
+        parents_element = node_xml.find(f"{{{self.NS_GEXF}}}parents")
+        if parents_element is not None:
+            data["parents"] = []
+            for p in parents_element.findall(f"{{{self.NS_GEXF}}}parent"):
+                parent = p.get("for")
+                data["parents"].append(parent)
+        return data
+
+    def add_slices(self, data, node_or_edge_xml):
+        slices_element = node_or_edge_xml.find(f"{{{self.NS_GEXF}}}slices")
+        if slices_element is not None:
+            data["slices"] = []
+            for s in slices_element.findall(f"{{{self.NS_GEXF}}}slice"):
+                start = s.get("start")
+                end = s.get("end")
+                data["slices"].append((start, end))
+        return data
+
+    def add_spells(self, data, node_or_edge_xml):
+        spells_element = node_or_edge_xml.find(f"{{{self.NS_GEXF}}}spells")
+        if spells_element is not None:
+            data["spells"] = []
+            ttype = self.timeformat
+            for s in spells_element.findall(f"{{{self.NS_GEXF}}}spell"):
+                start = self.python_type[ttype](s.get("start"))
+                end = self.python_type[ttype](s.get("end"))
+                data["spells"].append((start, end))
+        return data
+
+    def add_edge(self, G, edge_element, edge_attr):
+        # add an edge to the graph
+
+        # raise error if we find mixed directed and undirected edges
+        edge_direction = edge_element.get("type")
+        if G.is_directed() and edge_direction == "undirected":
+            raise EasyGraphError("Undirected edge found in directed graph.")
+        if (not G.is_directed()) and edge_direction == "directed":
+            raise EasyGraphError("Directed edge found in undirected graph.")
+
+        # Get source and target and recast type if required
+        source = edge_element.get("source")
+        target = edge_element.get("target")
+        if self.node_type is not None:
+            source = self.node_type(source)
+            target = self.node_type(target)
+
+        data = self.decode_attr_elements(edge_attr, edge_element)
+        data = self.add_start_end(data, edge_element)
+
+        if self.VERSION == "1.1":
+            data = self.add_slices(data, edge_element)  # add slices
+        else:
+            data = self.add_spells(data, edge_element)  # add spells
+
+        # GEXF stores edge ids as an attribute
+        # EasyGraph uses them as keys in multigraphs
+        # if easygraph_key is not specified as an attribute
+        edge_id = edge_element.get("id")
+        if edge_id is not None:
+            data["id"] = edge_id
+
+        # check if there is a 'multigraph_key' and use that as edge_id
+        multigraph_key = data.pop("easygraph_key", None)
+        if multigraph_key is not None:
+            edge_id = multigraph_key
+
+        weight = edge_element.get("weight")
+        if weight is not None:
+            data["weight"] = float(weight)
+
+        edge_label = edge_element.get("label")
+        if edge_label is not None:
+            data["label"] = edge_label
+
+        if G.has_edge(source, target):
+            # seen this edge before - this is a multigraph
+            self.simple_graph = False
+        G.add_edge(source, target, key=edge_id, **data)
+        if edge_direction == "mutual":
+            G.add_edge(target, source, key=edge_id, **data)
+
+    def decode_attr_elements(self, gexf_keys, obj_xml):
+        # Use the key information to decode the attr XML
+        attr = {}
+        # look for outer '<attvalues>' element
+        attr_element = obj_xml.find(f"{{{self.NS_GEXF}}}attvalues")
+        if attr_element is not None:
+            # loop over <attvalue> elements
+            for a in attr_element.findall(f"{{{self.NS_GEXF}}}attvalue"):
+                key = a.get("for")  # for is required
+                try:  # should be in our gexf_keys dictionary
+                    title = gexf_keys[key]["title"]
+                except KeyError as err:
+                    raise eg.EasyGraphError(f"No attribute defined for={key}.") from err
+                atype = gexf_keys[key]["type"]
+                value = a.get("value")
+                if atype == "boolean":
+                    value = self.convert_bool[value]
+                else:
+                    value = self.python_type[atype](value)
+                if gexf_keys[key]["mode"] == "dynamic":
+                    # for dynamic graphs use list of three-tuples
+                    # [(value1,start1,end1), (value2,start2,end2), etc]
+                    ttype = self.timeformat
+                    start = self.python_type[ttype](a.get("start"))
+                    end = self.python_type[ttype](a.get("end"))
+                    if title in attr:
+                        attr[title].append((value, start, end))
+                    else:
+                        attr[title] = [(value, start, end)]
+                else:
+                    # for static graphs just assign the value
+                    attr[title] = value
+        return attr
+
+    def find_gexf_attributes(self, attributes_element):
+        # Extract all the attributes and defaults
+        attrs = {}
+        defaults = {}
+        mode = attributes_element.get("mode")
+        for k in attributes_element.findall(f"{{{self.NS_GEXF}}}attribute"):
+            attr_id = k.get("id")
+            title = k.get("title")
+            atype = k.get("type")
+            attrs[attr_id] = {"title": title, "type": atype, "mode": mode}
+            # check for the 'default' subelement of key element and add
+            default = k.find(f"{{{self.NS_GEXF}}}default")
+            if default is not None:
+                if atype == "boolean":
+                    value = self.convert_bool[default.text]
+                else:
+                    value = self.python_type[atype](default.text)
+                defaults[title] = value
+        return attrs, defaults
+
+
+def relabel_gexf_graph(G):
+    """Relabel graph using "label" node keyword for node label.
+
+    Parameters
+    ----------
+    G : graph
+       A EasyGraph graph read from GEXF data
+
+    Returns
+    -------
+    H : graph
+      A EasyGraph graph with relabeled nodes
+
+    Raises
+    ------
+    EasyGraphError
+        If node labels are missing or not unique while relabel=True.
+
+    Notes
+    -----
+    This function relabels the nodes in a EasyGraph graph with the
+    "label" attribute.  It also handles relabeling the specific GEXF
+    node attributes "parents", and "pid".
+    """
+    # build mapping of node labels, do some error checking
+    try:
+        mapping = [(u, G.nodes[u]["label"]) for u in G]
+    except KeyError as err:
+        raise EasyGraphError(
+            "Failed to relabel nodes: missing node labels found. Use relabel=False."
+        ) from err
+    x, y = zip(*mapping)
+    if len(set(y)) != len(G):
+        raise EasyGraphError(
+            "Failed to relabel nodes: duplicate node labels found. Use relabel=False."
+        )
+    mapping = dict(mapping)
+    H = eg.relabel_nodes(G, mapping)
+    # relabel attributes
+    for n in G:
+        m = mapping[n]
+        H.nodes[m]["id"] = n
+        H.nodes[m].pop("label")
+        if "pid" in H.nodes[m]:
+            H.nodes[m]["pid"] = mapping[G.nodes[n]["pid"]]
+        if "parents" in H.nodes[m]:
+            H.nodes[m]["parents"] = [mapping[p] for p in G.nodes[n]["parents"]]
+    return H
```

## easygraph/readwrite/gml.py

 * *Ordering differences only*

```diff
@@ -1,803 +1,803 @@
-"""
-Read graphs in GML format.
-
-"GML, the Graph Modelling Language, is our proposal for a portable
-file format for graphs. GML's key features are portability, simple
-syntax, extensibility and flexibility. A GML file consists of a
-hierarchical key-value lists. Graphs can be annotated with arbitrary
-data structures. The idea for a common file format was born at the
-GD'95; this proposal is the outcome of many discussions. GML is the
-standard file format in the Graphlet graph editor system. It has been
-overtaken and adapted by several other systems for drawing graphs."
-
-GML files are stored using a 7-bit ASCII encoding with any extended
-ASCII characters (iso8859-1) appearing as HTML character entities.
-You will need to give some thought into how the exported data should
-interact with different languages and even different Python versions.
-Re-importing from gml is also a concern.
-
-Without specifying a `stringizer`/`destringizer`, the code is capable of
-writing `int`/`float`/`str`/`dict`/`list` data as required by the GML
-specification.  For writing other data types, and for reading data other
-than `str` you need to explicitly supply a `stringizer`/`destringizer`.
-
-For additional documentation on the GML file format, please see the
-`GML website <https://web.archive.org/web/20190207140002/http://www.fim.uni-passau.de/index.php?id=17297&L=1>`_.
-
-Several example graphs in GML format may be found on Mark Newman's
-`Network data page <http://www-personal.umich.edu/~mejn/netdata/>`_.
-"""
-
-
-import html.entities as htmlentitydefs
-import re
-import warnings
-
-from ast import literal_eval
-from collections import defaultdict
-from enum import Enum
-from io import StringIO
-from typing import Any
-from typing import NamedTuple
-from unicodedata import category
-
-import easygraph as eg
-
-from easygraph.utils import open_file
-from easygraph.utils.exception import EasyGraphError
-
-
-__all__ = ["read_gml", "parse_gml", "generate_gml", "write_gml"]
-
-LIST_START_VALUE = "_easygraph_list_start"
-
-
-def escape(text):
-    """Use XML character references to escape characters.
-
-    Use XML character references for unprintable or non-ASCII
-    characters, double quotes and ampersands in a string
-    """
-
-    def fixup(m):
-        ch = m.group(0)
-        return "&#" + str(ord(ch)) + ";"
-
-    text = re.sub('[^ -~]|[&"]', fixup, text)
-    return text if isinstance(text, str) else str(text)
-
-
-def unescape(text):
-    """Replace XML character references with the referenced characters"""
-
-    def fixup(m):
-        text = m.group(0)
-        if text[1] == "#":
-            # Character reference
-            if text[2] == "x":
-                code = int(text[3:-1], 16)
-            else:
-                code = int(text[2:-1])
-        else:
-            # Named entity
-            try:
-                code = htmlentitydefs.name2codepoint[text[1:-1]]
-            except KeyError:
-                return text  # leave unchanged
-        try:
-            return chr(code)
-        except (ValueError, OverflowError):
-            return text  # leave unchanged
-
-    return re.sub("&(?:[0-9A-Za-z]+|#(?:[0-9]+|x[0-9A-Fa-f]+));", fixup, text)
-
-
-def literal_destringizer(rep):
-    """Convert a Python literal to the value it represents.
-
-    Parameters
-    ----------
-    rep : string
-        A Python literal.
-
-    Returns
-    -------
-    value : object
-        The value of the Python literal.
-
-    Raises
-    ------
-    ValueError
-        If `rep` is not a Python literal.
-    """
-    msg = "literal_destringizer is deprecated and will be removed in 3.0."
-    warnings.warn(msg, DeprecationWarning)
-    if isinstance(rep, str):
-        orig_rep = rep
-        try:
-            return literal_eval(rep)
-        except SyntaxError as err:
-            raise ValueError(f"{orig_rep!r} is not a valid Python literal") from err
-    else:
-        raise ValueError(f"{rep!r} is not a string")
-
-
-class Pattern(Enum):
-    """encodes the index of each token-matching pattern in `tokenize`."""
-
-    KEYS = 0
-    REALS = 1
-    INTS = 2
-    STRINGS = 3
-    DICT_START = 4
-    DICT_END = 5
-    COMMENT_WHITESPACE = 6
-
-
-class Token(NamedTuple):
-    category: Pattern
-    value: Any
-    line: int
-    position: int
-
-
-def parse_gml(lines, label="label", destringizer=None):
-    """Parse GML graph from a string or iterable.
-
-    Parameters
-    ----------
-    lines : string or iterable of strings
-       Data in GML format.
-
-    label : string, optional
-        If not None, the parsed nodes will be renamed according to node
-        attributes indicated by `label`. Default value: 'label'.
-
-    destringizer : callable, optional
-        A `destringizer` that recovers values stored as strings in GML. If it
-        cannot convert a string to a value, a `ValueError` is raised. Default
-        value : None.
-
-    Returns
-    -------
-    G : EasyGraph graph
-        The parsed graph.
-
-    Raises
-    ------
-    EasyGraphError
-        If the input cannot be parsed.
-
-    See Also
-    --------
-    write_gml, read_gml
-
-    Notes
-    -----
-    This stores nested GML attributes as dictionaries in the EasyGraph graph,
-    node, and edge attribute structures.
-
-    GML files are stored using a 7-bit ASCII encoding with any extended
-    ASCII characters (iso8859-1) appearing as HTML character entities.
-    Without specifying a `stringizer`/`destringizer`, the code is capable of
-    writing `int`/`float`/`str`/`dict`/`list` data as required by the GML
-    specification.  For writing other data types, and for reading data other
-    than `str` you need to explicitly supply a `stringizer`/`destringizer`.
-
-    For additional documentation on the GML file format, please see the
-    `GML url <https://web.archive.org/web/20190207140002/http://www.fim.uni-passau.de/index.php?id=17297&L=1>`_.
-
-    See the module docstring :mod:`easygraph.readwrite.gml` for more details.
-    """
-
-    def decode_line(line):
-        if isinstance(line, bytes):
-            try:
-                line.decode("ascii")
-            except UnicodeDecodeError as err:
-                raise EasyGraphError("input is not ASCII-encoded") from err
-        if not isinstance(line, str):
-            line = str(line)
-        return line
-
-    def filter_lines(lines):
-        if isinstance(lines, str):
-            lines = decode_line(lines)
-            lines = lines.splitlines()
-            yield from lines
-        else:
-            for line in lines:
-                line = decode_line(line)
-                if line and line[-1] == "\n":
-                    line = line[:-1]
-                if line.find("\n") != -1:
-                    raise EasyGraphError("input line contains newline")
-                yield line
-
-    G = parse_gml_lines(filter_lines(lines), label, destringizer)
-    return G
-
-
-def parse_gml_lines(lines, label, destringizer):
-    """Parse GML `lines` into a graph."""
-
-    def tokenize():
-        patterns = [
-            r"[A-Za-z][0-9A-Za-z_]*\b",  # keys
-            # reals
-            r"[+-]?(?:[0-9]*\.[0-9]+|[0-9]+\.[0-9]*|INF)(?:[Ee][+-]?[0-9]+)?",
-            r"[+-]?[0-9]+",  # ints
-            r'".*?"',  # strings
-            r"\[",  # dict start
-            r"\]",  # dict end
-            r"#.*$|\s+",  # comments and whitespaces
-        ]
-        tokens = re.compile("|".join(f"({pattern})" for pattern in patterns))
-        lineno = 0
-        for line in lines:
-            length = len(line)
-            pos = 0
-            while pos < length:
-                match = tokens.match(line, pos)
-                if match is None:
-                    m = f"cannot tokenize {line[pos:]} at ({lineno + 1}, {pos + 1})"
-                    raise EasyGraphError(m)
-                for i in range(len(patterns)):
-                    group = match.group(i + 1)
-                    if group is not None:
-                        if i == 0:  # keys
-                            value = group.rstrip()
-                        elif i == 1:  # reals
-                            value = float(group)
-                        elif i == 2:  # ints
-                            value = int(group)
-                        else:
-                            value = group
-                        if i != 6:  # comments and whitespaces
-                            yield Token(Pattern(i), value, lineno + 1, pos + 1)
-                        pos += len(group)
-                        break
-            lineno += 1
-        yield Token(None, None, lineno + 1, 1)  # EOF
-
-    def unexpected(curr_token, expected):
-        category, value, lineno, pos = curr_token
-        value = repr(value) if value is not None else "EOF"
-        raise EasyGraphError(f"expected {expected}, found {value} at ({lineno}, {pos})")
-
-    def consume(curr_token, category, expected):
-        if curr_token.category == category:
-            return next(tokens)
-        unexpected(curr_token, expected)
-
-    def parse_dict(curr_token):
-        # dict start
-        curr_token = consume(curr_token, Pattern.DICT_START, "'['")
-        # dict contents
-        curr_token, dct = parse_kv(curr_token)
-        # dict end
-        curr_token = consume(curr_token, Pattern.DICT_END, "']'")
-        return curr_token, dct
-
-    def parse_kv(curr_token):
-        dct = defaultdict(list)
-        while curr_token.category == Pattern.KEYS:
-            key = curr_token.value
-            curr_token = next(tokens)
-            category = curr_token.category
-            if category == Pattern.REALS or category == Pattern.INTS:
-                value = curr_token.value
-                curr_token = next(tokens)
-            elif category == Pattern.STRINGS:
-                value = unescape(curr_token.value[1:-1])
-                if destringizer:
-                    try:
-                        value = destringizer(value)
-                    except ValueError:
-                        pass
-                curr_token = next(tokens)
-            elif category == Pattern.DICT_START:
-                curr_token, value = parse_dict(curr_token)
-            else:
-                if key in ("id", "label", "source", "target"):
-                    try:
-                        # String convert the token value
-                        value = unescape(str(curr_token.value))
-                        if destringizer:
-                            try:
-                                value = destringizer(value)
-                            except ValueError:
-                                pass
-                        curr_token = next(tokens)
-                    except Exception:
-                        msg = (
-                            "an int, float, string, '[' or string"
-                            + " convertible ASCII value for node id or label"
-                        )
-                        unexpected(curr_token, msg)
-                elif curr_token.value in {"NAN", "INF"}:
-                    value = float(curr_token.value)
-                    curr_token = next(tokens)
-                else:  # Otherwise error out
-                    unexpected(curr_token, "an int, float, string or '['")
-            dct[key].append(value)
-
-        def clean_dict_value(value):
-            if not isinstance(value, list):
-                return value
-            if len(value) == 1:
-                return value[0]
-            if value[0] == LIST_START_VALUE:
-                return value[1:]
-            return value
-
-        dct = {key: clean_dict_value(value) for key, value in dct.items()}
-        return curr_token, dct
-
-    def parse_graph():
-        curr_token, dct = parse_kv(next(tokens))
-        if curr_token.category is not None:  # EOF
-            unexpected(curr_token, "EOF")
-        if "graph" not in dct:
-            raise EasyGraphError("input contains no graph")
-        graph = dct["graph"]
-        if isinstance(graph, list):
-            raise EasyGraphError("input contains more than one graph")
-        return graph
-
-    tokens = tokenize()
-    graph = parse_graph()
-    directed = graph.pop("directed", False)
-    multigraph = graph.pop("multigraph", False)
-    if not multigraph:
-        G = eg.DiGraph() if directed else eg.Graph()
-    else:
-        G = eg.MultiDiGraph() if directed else eg.MultiGraph()
-    graph_attr = {k: v for k, v in graph.items() if k not in ("node", "edge")}
-    G.graph.update(graph_attr)
-
-    def pop_attr(dct, category, attr, i):
-        try:
-            return dct.pop(attr)
-        except KeyError as err:
-            raise EasyGraphError(f"{category} #{i} has no {attr!r} attribute") from err
-
-    nodes = graph.get("node", [])
-    mapping = {}
-    node_labels = set()
-    for i, node in enumerate(nodes if isinstance(nodes, list) else [nodes]):
-        id = pop_attr(node, "node", "id", i)
-        if id in G:
-            raise EasyGraphError(f"node id {id!r} is duplicated")
-        if label is not None and label != "id":
-            node_label = pop_attr(node, "node", label, i)
-            if node_label in node_labels:
-                raise EasyGraphError(f"node label {node_label!r} is duplicated")
-            node_labels.add(node_label)
-            mapping[id] = node_label
-        G.add_node(id, **node)
-
-    edges = graph.get("edge", [])
-    for i, edge in enumerate(edges if isinstance(edges, list) else [edges]):
-        source = pop_attr(edge, "edge", "source", i)
-        target = pop_attr(edge, "edge", "target", i)
-        if source not in G:
-            raise EasyGraphError(f"edge #{i} has undefined source {source!r}")
-        if target not in G:
-            raise EasyGraphError(f"edge #{i} has undefined target {target!r}")
-        if not multigraph:
-            if not G.has_edge(source, target):
-                G.add_edge(source, target, **edge)
-            else:
-                arrow = "->" if directed else "--"
-                msg = f"edge #{i} ({source!r}{arrow}{target!r}) is duplicated"
-                raise EasyGraphError(msg)
-        else:
-            key = edge.pop("key", None)
-            if key is not None and G.has_edge(source, target, key):
-                arrow = "->" if directed else "--"
-                msg = f"edge #{i} ({source!r}{arrow}{target!r}, {key!r})"
-                msg2 = 'Hint: If multigraph add "multigraph 1" to file header.'
-                raise EasyGraphError(msg + " is duplicated\n" + msg2)
-            G.add_edge(source, target, key, **edge)
-
-    if label is not None and label != "id":
-        G = eg.relabel_nodes(G, mapping)
-    return G
-
-
-def generate_gml(G, stringizer=None):
-    r"""Generate a single entry of the graph `G` in GML format.
-
-    Parameters
-    ----------
-    G : EasyGraph graph
-        The graph to be converted to GML.
-
-    stringizer : callable, optional
-        A `stringizer` which converts non-int/non-float/non-dict values into
-        strings. If it cannot convert a value into a string, it should raise a
-        `ValueError` to indicate that. Default value: None.
-
-    Returns
-    -------
-    lines: generator of strings
-        Lines of GML data. Newlines are not appended.
-
-    Raises
-    ------
-    EasyGraphError
-        If `stringizer` cannot convert a value into a string, or the value to
-        convert is not a string while `stringizer` is None.
-
-    See Also
-    --------
-    literal_stringizer
-
-    Notes
-    -----
-    Graph attributes named 'directed', 'multigraph', 'node' or
-    'edge', node attributes named 'id' or 'label', edge attributes
-    named 'source' or 'target' (or 'key' if `G` is a multigraph)
-    are ignored because these attribute names are used to encode the graph
-    structure.
-
-    GML files are stored using a 7-bit ASCII encoding with any extended
-    ASCII characters (iso8859-1) appearing as HTML character entities.
-    Without specifying a `stringizer`/`destringizer`, the code is capable of
-    writing `int`/`float`/`str`/`dict`/`list` data as required by the GML
-    specification.  For writing other data types, and for reading data other
-    than `str` you need to explicitly supply a `stringizer`/`destringizer`.
-
-    For additional documentation on the GML file format, please see the
-    `GML url <https://web.archive.org/web/20190207140002/http://www.fim.uni-passau.de/index.php?id=17297&L=1>`_.
-
-    See the module docstring :mod:`easygraph.readwrite.gml` for more details.
-
-    Examples
-    --------
-    >>> G = eg.Graph()
-    >>> G.add_node("1")
-    >>> print("\n".join(eg.generate_gml(G)))
-    graph [
-      node [
-        id 0
-        label "1"
-      ]
-    ]
-    """
-    valid_keys = re.compile("^[A-Za-z][0-9A-Za-z_]*$")
-
-    def stringize(key, value, ignored_keys, indent, in_list=False):
-        if not isinstance(key, str):
-            raise EasyGraphError(f"{key!r} is not a string")
-        if not valid_keys.match(key):
-            raise EasyGraphError(f"{key!r} is not a valid key")
-        if not isinstance(key, str):
-            key = str(key)
-        if key not in ignored_keys:
-            if isinstance(value, (int, bool)):
-                if key == "label":
-                    yield indent + key + ' "' + str(value) + '"'
-                elif value is True:
-                    # python bool is an instance of int
-                    yield indent + key + " 1"
-                elif value is False:
-                    yield indent + key + " 0"
-                # GML only supports signed 32-bit integers
-                elif value < -(2**31) or value >= 2**31:
-                    yield indent + key + ' "' + str(value) + '"'
-                else:
-                    yield indent + key + " " + str(value)
-            elif isinstance(value, float):
-                text = repr(value).upper()
-                # GML matches INF to keys, so prepend + to INF. Use repr(float(*))
-                # instead of string literal to future proof against changes to repr.
-                if text == repr(float("inf")).upper():
-                    text = "+" + text
-                else:
-                    # GML requires that a real literal contain a decimal point, but
-                    # repr may not output a decimal point when the mantissa is
-                    # integral and hence needs fixing.
-                    epos = text.rfind("E")
-                    if epos != -1 and text.find(".", 0, epos) == -1:
-                        text = text[:epos] + "." + text[epos:]
-                if key == "label":
-                    yield indent + key + ' "' + text + '"'
-                else:
-                    yield indent + key + " " + text
-            elif isinstance(value, dict):
-                yield indent + key + " ["
-                next_indent = indent + "  "
-                for key, value in value.items():
-                    yield from stringize(key, value, (), next_indent)
-                yield indent + "]"
-            elif (
-                isinstance(value, (list, tuple))
-                and key != "label"
-                and value
-                and not in_list
-            ):
-                if len(value) == 1:
-                    yield indent + key + " " + f'"{LIST_START_VALUE}"'
-                for val in value:
-                    yield from stringize(key, val, (), indent, True)
-            else:
-                if stringizer:
-                    try:
-                        value = stringizer(value)
-                    except ValueError as err:
-                        raise EasyGraphError(
-                            f"{value!r} cannot be converted into a string"
-                        ) from err
-                if not isinstance(value, str):
-                    raise EasyGraphError(f"{value!r} is not a string")
-                yield indent + key + ' "' + escape(value) + '"'
-
-    yield "graph ["
-
-    # Output graph attributes
-    multigraph = G.is_multigraph()
-    if G.is_directed():
-        yield "  directed 1"
-    if multigraph:
-        yield "  multigraph 1"
-    ignored_keys = {"directed", "multigraph", "node", "edge"}
-    for attr, value in G.graph.items():
-        yield from stringize(attr, value, ignored_keys, "  ")
-
-    # Output node data
-    node_id = dict(zip(G, range(len(G))))
-    ignored_keys = {"id", "label"}
-    for node, attrs in G.nodes.items():
-        yield "  node ["
-        yield "    id " + str(node_id[node])
-        yield from stringize("label", node, (), "    ")
-        for attr, value in attrs.items():
-            yield from stringize(attr, value, ignored_keys, "    ")
-        yield "  ]"
-
-    # Output edge data
-    ignored_keys = {"source", "target"}
-    kwargs = {"data": True}
-    if multigraph:
-        ignored_keys.add("key")
-        kwargs["keys"] = True
-    for e in G.edges:
-        yield "  edge ["
-        yield "    source " + str(node_id[e[0]])
-        yield "    target " + str(node_id[e[1]])
-        if multigraph:
-            yield from stringize("key", e[2], (), "    ")
-        for attr, value in e[-1].items():
-            yield from stringize(attr, value, ignored_keys, "    ")
-        yield "  ]"
-    yield "]"
-
-
-@open_file(0, mode="rb")
-def read_gml(path, label="label", destringizer=None):
-    """Read graph in GML format from `path`.
-
-    Parameters
-    ----------
-    path : filename or filehandle
-        The filename or filehandle to read from.
-
-    label : string, optional
-        If not None, the parsed nodes will be renamed according to node
-        attributes indicated by `label`. Default value: 'label'.
-
-    destringizer : callable, optional
-        A `destringizer` that recovers values stored as strings in GML. If it
-        cannot convert a string to a value, a `ValueError` is raised. Default
-        value : None.
-
-    Returns
-    -------
-    G : EasyGraph graph
-        The parsed graph.
-
-    Raises
-    ------
-    EasyGraphError
-        If the input cannot be parsed.
-
-    See Also
-    --------
-    write_gml, parse_gml
-    literal_destringizer
-
-    Notes
-    -----
-    GML files are stored using a 7-bit ASCII encoding with any extended
-    ASCII characters (iso8859-1) appearing as HTML character entities.
-    Without specifying a `stringizer`/`destringizer`, the code is capable of
-    writing `int`/`float`/`str`/`dict`/`list` data as required by the GML
-    specification.  For writing other data types, and for reading data other
-    than `str` you need to explicitly supply a `stringizer`/`destringizer`.
-
-    For additional documentation on the GML file format, please see the
-    `GML url <https://web.archive.org/web/20190207140002/http://www.fim.uni-passau.de/index.php?id=17297&L=1>`_.
-
-    See the module docstring :mod:`easygraph.readwrite.gml` for more details.
-
-    Examples
-    --------
-    >>> G = eg.path_graph(4)
-    >>> eg.write_gml(G, "test.gml")
-
-    GML values are interpreted as strings by default:
-
-    >>> H = eg.read_gml("test.gml")
-    >>> H.nodes
-    NodeView(('0', '1', '2', '3'))
-
-    When a `destringizer` is provided, GML values are converted to the provided type.
-    For example, integer nodes can be recovered as shown below:
-
-    >>> J = eg.read_gml("test.gml", destringizer=int)
-    >>> J.nodes
-    NodeView((0, 1, 2, 3))
-
-    """
-
-    def filter_lines(lines):
-        for line in lines:
-            try:
-                line = line.decode("ascii")
-            except UnicodeDecodeError as err:
-                raise EasyGraphError("input is not ASCII-encoded") from err
-            if not isinstance(line, str):
-                lines = str(lines)
-            if line and line[-1] == "\n":
-                line = line[:-1]
-            yield line
-
-    G = parse_gml_lines(filter_lines(path), label, destringizer)
-    return G
-
-
-@open_file(1, mode="wb")
-def write_gml(G, path, stringizer=None):
-    """Write a graph `G` in GML format to the file or file handle `path`.
-
-    Parameters
-    ----------
-    G : EasyGraph graph
-        The graph to be converted to GML.
-
-    path : filename or filehandle
-        The filename or filehandle to write. Files whose names end with .gz or
-        .bz2 will be compressed.
-
-    stringizer : callable, optional
-        A `stringizer` which converts non-int/non-float/non-dict values into
-        strings. If it cannot convert a value into a string, it should raise a
-        `ValueError` to indicate that. Default value: None.
-
-    Raises
-    ------
-    EasyGraphError
-        If `stringizer` cannot convert a value into a string, or the value to
-        convert is not a string while `stringizer` is None.
-
-    See Also
-    --------
-    read_gml, generate_gml
-    literal_stringizer
-
-    Notes
-    -----
-    Graph attributes named 'directed', 'multigraph', 'node' or
-    'edge', node attributes named 'id' or 'label', edge attributes
-    named 'source' or 'target' (or 'key' if `G` is a multigraph)
-    are ignored because these attribute names are used to encode the graph
-    structure.
-
-    GML files are stored using a 7-bit ASCII encoding with any extended
-    ASCII characters (iso8859-1) appearing as HTML character entities.
-    Without specifying a `stringizer`/`destringizer`, the code is capable of
-    writing `int`/`float`/`str`/`dict`/`list` data as required by the GML
-    specification.  For writing other data types, and for reading data other
-    than `str` you need to explicitly supply a `stringizer`/`destringizer`.
-
-    Note that while we allow non-standard GML to be read from a file, we make
-    sure to write GML format. In particular, underscores are not allowed in
-    attribute names.
-    For additional documentation on the GML file format, please see the
-    `GML url <https://web.archive.org/web/20190207140002/http://www.fim.uni-passau.de/index.php?id=17297&L=1>`_.
-
-    See the module docstring :mod:`easygraph.readwrite.gml` for more details.
-
-    Examples
-    --------
-    >>> G = eg.path_graph(4)
-    >>> eg.write_gml(G, "test.gml")
-
-    Filenames ending in .gz or .bz2 will be compressed.
-
-    >>> eg.write_gml(G, "test.gml.gz")
-    """
-    for line in generate_gml(G, stringizer):
-        path.write((line + "\n").encode("ascii"))
-
-
-def literal_stringizer(value):
-    msg = "literal_stringizer is deprecated and will be removed in 3.0."
-    warnings.warn(msg, DeprecationWarning)
-
-    def stringize(value):
-        if isinstance(value, (int, bool)) or value is None:
-            if value is True:  # GML uses 1/0 for boolean values.
-                buf.write(str(1))
-            elif value is False:
-                buf.write(str(0))
-            else:
-                buf.write(str(value))
-        elif isinstance(value, str):
-            text = repr(value)
-            if text[0] != "u":
-                try:
-                    value.encode("latin1")
-                except UnicodeEncodeError:
-                    text = "u" + text
-            buf.write(text)
-        elif isinstance(value, (float, complex, str, bytes)):
-            buf.write(repr(value))
-        elif isinstance(value, list):
-            buf.write("[")
-            first = True
-            for item in value:
-                if not first:
-                    buf.write(",")
-                else:
-                    first = False
-                stringize(item)
-            buf.write("]")
-        elif isinstance(value, tuple):
-            if len(value) > 1:
-                buf.write("(")
-                first = True
-                for item in value:
-                    if not first:
-                        buf.write(",")
-                    else:
-                        first = False
-                    stringize(item)
-                buf.write(")")
-            elif value:
-                buf.write("(")
-                stringize(value[0])
-                buf.write(",)")
-            else:
-                buf.write("()")
-        elif isinstance(value, dict):
-            buf.write("{")
-            first = True
-            for key, value in value.items():
-                if not first:
-                    buf.write(",")
-                else:
-                    first = False
-                stringize(key)
-                buf.write(":")
-                stringize(value)
-            buf.write("}")
-        elif isinstance(value, set):
-            buf.write("{")
-            first = True
-            for item in value:
-                if not first:
-                    buf.write(",")
-                else:
-                    first = False
-                stringize(item)
-            buf.write("}")
-        else:
-            msg = "{value!r} cannot be converted into a Python literal"
-            raise ValueError(msg)
-
-    buf = StringIO()
-    stringize(value)
-    return buf.getvalue()
+"""
+Read graphs in GML format.
+
+"GML, the Graph Modelling Language, is our proposal for a portable
+file format for graphs. GML's key features are portability, simple
+syntax, extensibility and flexibility. A GML file consists of a
+hierarchical key-value lists. Graphs can be annotated with arbitrary
+data structures. The idea for a common file format was born at the
+GD'95; this proposal is the outcome of many discussions. GML is the
+standard file format in the Graphlet graph editor system. It has been
+overtaken and adapted by several other systems for drawing graphs."
+
+GML files are stored using a 7-bit ASCII encoding with any extended
+ASCII characters (iso8859-1) appearing as HTML character entities.
+You will need to give some thought into how the exported data should
+interact with different languages and even different Python versions.
+Re-importing from gml is also a concern.
+
+Without specifying a `stringizer`/`destringizer`, the code is capable of
+writing `int`/`float`/`str`/`dict`/`list` data as required by the GML
+specification.  For writing other data types, and for reading data other
+than `str` you need to explicitly supply a `stringizer`/`destringizer`.
+
+For additional documentation on the GML file format, please see the
+`GML website <https://web.archive.org/web/20190207140002/http://www.fim.uni-passau.de/index.php?id=17297&L=1>`_.
+
+Several example graphs in GML format may be found on Mark Newman's
+`Network data page <http://www-personal.umich.edu/~mejn/netdata/>`_.
+"""
+
+
+import html.entities as htmlentitydefs
+import re
+import warnings
+
+from ast import literal_eval
+from collections import defaultdict
+from enum import Enum
+from io import StringIO
+from typing import Any
+from typing import NamedTuple
+from unicodedata import category
+
+import easygraph as eg
+
+from easygraph.utils import open_file
+from easygraph.utils.exception import EasyGraphError
+
+
+__all__ = ["read_gml", "parse_gml", "generate_gml", "write_gml"]
+
+LIST_START_VALUE = "_easygraph_list_start"
+
+
+def escape(text):
+    """Use XML character references to escape characters.
+
+    Use XML character references for unprintable or non-ASCII
+    characters, double quotes and ampersands in a string
+    """
+
+    def fixup(m):
+        ch = m.group(0)
+        return "&#" + str(ord(ch)) + ";"
+
+    text = re.sub('[^ -~]|[&"]', fixup, text)
+    return text if isinstance(text, str) else str(text)
+
+
+def unescape(text):
+    """Replace XML character references with the referenced characters"""
+
+    def fixup(m):
+        text = m.group(0)
+        if text[1] == "#":
+            # Character reference
+            if text[2] == "x":
+                code = int(text[3:-1], 16)
+            else:
+                code = int(text[2:-1])
+        else:
+            # Named entity
+            try:
+                code = htmlentitydefs.name2codepoint[text[1:-1]]
+            except KeyError:
+                return text  # leave unchanged
+        try:
+            return chr(code)
+        except (ValueError, OverflowError):
+            return text  # leave unchanged
+
+    return re.sub("&(?:[0-9A-Za-z]+|#(?:[0-9]+|x[0-9A-Fa-f]+));", fixup, text)
+
+
+def literal_destringizer(rep):
+    """Convert a Python literal to the value it represents.
+
+    Parameters
+    ----------
+    rep : string
+        A Python literal.
+
+    Returns
+    -------
+    value : object
+        The value of the Python literal.
+
+    Raises
+    ------
+    ValueError
+        If `rep` is not a Python literal.
+    """
+    msg = "literal_destringizer is deprecated and will be removed in 3.0."
+    warnings.warn(msg, DeprecationWarning)
+    if isinstance(rep, str):
+        orig_rep = rep
+        try:
+            return literal_eval(rep)
+        except SyntaxError as err:
+            raise ValueError(f"{orig_rep!r} is not a valid Python literal") from err
+    else:
+        raise ValueError(f"{rep!r} is not a string")
+
+
+class Pattern(Enum):
+    """encodes the index of each token-matching pattern in `tokenize`."""
+
+    KEYS = 0
+    REALS = 1
+    INTS = 2
+    STRINGS = 3
+    DICT_START = 4
+    DICT_END = 5
+    COMMENT_WHITESPACE = 6
+
+
+class Token(NamedTuple):
+    category: Pattern
+    value: Any
+    line: int
+    position: int
+
+
+def parse_gml(lines, label="label", destringizer=None):
+    """Parse GML graph from a string or iterable.
+
+    Parameters
+    ----------
+    lines : string or iterable of strings
+       Data in GML format.
+
+    label : string, optional
+        If not None, the parsed nodes will be renamed according to node
+        attributes indicated by `label`. Default value: 'label'.
+
+    destringizer : callable, optional
+        A `destringizer` that recovers values stored as strings in GML. If it
+        cannot convert a string to a value, a `ValueError` is raised. Default
+        value : None.
+
+    Returns
+    -------
+    G : EasyGraph graph
+        The parsed graph.
+
+    Raises
+    ------
+    EasyGraphError
+        If the input cannot be parsed.
+
+    See Also
+    --------
+    write_gml, read_gml
+
+    Notes
+    -----
+    This stores nested GML attributes as dictionaries in the EasyGraph graph,
+    node, and edge attribute structures.
+
+    GML files are stored using a 7-bit ASCII encoding with any extended
+    ASCII characters (iso8859-1) appearing as HTML character entities.
+    Without specifying a `stringizer`/`destringizer`, the code is capable of
+    writing `int`/`float`/`str`/`dict`/`list` data as required by the GML
+    specification.  For writing other data types, and for reading data other
+    than `str` you need to explicitly supply a `stringizer`/`destringizer`.
+
+    For additional documentation on the GML file format, please see the
+    `GML url <https://web.archive.org/web/20190207140002/http://www.fim.uni-passau.de/index.php?id=17297&L=1>`_.
+
+    See the module docstring :mod:`easygraph.readwrite.gml` for more details.
+    """
+
+    def decode_line(line):
+        if isinstance(line, bytes):
+            try:
+                line.decode("ascii")
+            except UnicodeDecodeError as err:
+                raise EasyGraphError("input is not ASCII-encoded") from err
+        if not isinstance(line, str):
+            line = str(line)
+        return line
+
+    def filter_lines(lines):
+        if isinstance(lines, str):
+            lines = decode_line(lines)
+            lines = lines.splitlines()
+            yield from lines
+        else:
+            for line in lines:
+                line = decode_line(line)
+                if line and line[-1] == "\n":
+                    line = line[:-1]
+                if line.find("\n") != -1:
+                    raise EasyGraphError("input line contains newline")
+                yield line
+
+    G = parse_gml_lines(filter_lines(lines), label, destringizer)
+    return G
+
+
+def parse_gml_lines(lines, label, destringizer):
+    """Parse GML `lines` into a graph."""
+
+    def tokenize():
+        patterns = [
+            r"[A-Za-z][0-9A-Za-z_]*\b",  # keys
+            # reals
+            r"[+-]?(?:[0-9]*\.[0-9]+|[0-9]+\.[0-9]*|INF)(?:[Ee][+-]?[0-9]+)?",
+            r"[+-]?[0-9]+",  # ints
+            r'".*?"',  # strings
+            r"\[",  # dict start
+            r"\]",  # dict end
+            r"#.*$|\s+",  # comments and whitespaces
+        ]
+        tokens = re.compile("|".join(f"({pattern})" for pattern in patterns))
+        lineno = 0
+        for line in lines:
+            length = len(line)
+            pos = 0
+            while pos < length:
+                match = tokens.match(line, pos)
+                if match is None:
+                    m = f"cannot tokenize {line[pos:]} at ({lineno + 1}, {pos + 1})"
+                    raise EasyGraphError(m)
+                for i in range(len(patterns)):
+                    group = match.group(i + 1)
+                    if group is not None:
+                        if i == 0:  # keys
+                            value = group.rstrip()
+                        elif i == 1:  # reals
+                            value = float(group)
+                        elif i == 2:  # ints
+                            value = int(group)
+                        else:
+                            value = group
+                        if i != 6:  # comments and whitespaces
+                            yield Token(Pattern(i), value, lineno + 1, pos + 1)
+                        pos += len(group)
+                        break
+            lineno += 1
+        yield Token(None, None, lineno + 1, 1)  # EOF
+
+    def unexpected(curr_token, expected):
+        category, value, lineno, pos = curr_token
+        value = repr(value) if value is not None else "EOF"
+        raise EasyGraphError(f"expected {expected}, found {value} at ({lineno}, {pos})")
+
+    def consume(curr_token, category, expected):
+        if curr_token.category == category:
+            return next(tokens)
+        unexpected(curr_token, expected)
+
+    def parse_dict(curr_token):
+        # dict start
+        curr_token = consume(curr_token, Pattern.DICT_START, "'['")
+        # dict contents
+        curr_token, dct = parse_kv(curr_token)
+        # dict end
+        curr_token = consume(curr_token, Pattern.DICT_END, "']'")
+        return curr_token, dct
+
+    def parse_kv(curr_token):
+        dct = defaultdict(list)
+        while curr_token.category == Pattern.KEYS:
+            key = curr_token.value
+            curr_token = next(tokens)
+            category = curr_token.category
+            if category == Pattern.REALS or category == Pattern.INTS:
+                value = curr_token.value
+                curr_token = next(tokens)
+            elif category == Pattern.STRINGS:
+                value = unescape(curr_token.value[1:-1])
+                if destringizer:
+                    try:
+                        value = destringizer(value)
+                    except ValueError:
+                        pass
+                curr_token = next(tokens)
+            elif category == Pattern.DICT_START:
+                curr_token, value = parse_dict(curr_token)
+            else:
+                if key in ("id", "label", "source", "target"):
+                    try:
+                        # String convert the token value
+                        value = unescape(str(curr_token.value))
+                        if destringizer:
+                            try:
+                                value = destringizer(value)
+                            except ValueError:
+                                pass
+                        curr_token = next(tokens)
+                    except Exception:
+                        msg = (
+                            "an int, float, string, '[' or string"
+                            + " convertible ASCII value for node id or label"
+                        )
+                        unexpected(curr_token, msg)
+                elif curr_token.value in {"NAN", "INF"}:
+                    value = float(curr_token.value)
+                    curr_token = next(tokens)
+                else:  # Otherwise error out
+                    unexpected(curr_token, "an int, float, string or '['")
+            dct[key].append(value)
+
+        def clean_dict_value(value):
+            if not isinstance(value, list):
+                return value
+            if len(value) == 1:
+                return value[0]
+            if value[0] == LIST_START_VALUE:
+                return value[1:]
+            return value
+
+        dct = {key: clean_dict_value(value) for key, value in dct.items()}
+        return curr_token, dct
+
+    def parse_graph():
+        curr_token, dct = parse_kv(next(tokens))
+        if curr_token.category is not None:  # EOF
+            unexpected(curr_token, "EOF")
+        if "graph" not in dct:
+            raise EasyGraphError("input contains no graph")
+        graph = dct["graph"]
+        if isinstance(graph, list):
+            raise EasyGraphError("input contains more than one graph")
+        return graph
+
+    tokens = tokenize()
+    graph = parse_graph()
+    directed = graph.pop("directed", False)
+    multigraph = graph.pop("multigraph", False)
+    if not multigraph:
+        G = eg.DiGraph() if directed else eg.Graph()
+    else:
+        G = eg.MultiDiGraph() if directed else eg.MultiGraph()
+    graph_attr = {k: v for k, v in graph.items() if k not in ("node", "edge")}
+    G.graph.update(graph_attr)
+
+    def pop_attr(dct, category, attr, i):
+        try:
+            return dct.pop(attr)
+        except KeyError as err:
+            raise EasyGraphError(f"{category} #{i} has no {attr!r} attribute") from err
+
+    nodes = graph.get("node", [])
+    mapping = {}
+    node_labels = set()
+    for i, node in enumerate(nodes if isinstance(nodes, list) else [nodes]):
+        id = pop_attr(node, "node", "id", i)
+        if id in G:
+            raise EasyGraphError(f"node id {id!r} is duplicated")
+        if label is not None and label != "id":
+            node_label = pop_attr(node, "node", label, i)
+            if node_label in node_labels:
+                raise EasyGraphError(f"node label {node_label!r} is duplicated")
+            node_labels.add(node_label)
+            mapping[id] = node_label
+        G.add_node(id, **node)
+
+    edges = graph.get("edge", [])
+    for i, edge in enumerate(edges if isinstance(edges, list) else [edges]):
+        source = pop_attr(edge, "edge", "source", i)
+        target = pop_attr(edge, "edge", "target", i)
+        if source not in G:
+            raise EasyGraphError(f"edge #{i} has undefined source {source!r}")
+        if target not in G:
+            raise EasyGraphError(f"edge #{i} has undefined target {target!r}")
+        if not multigraph:
+            if not G.has_edge(source, target):
+                G.add_edge(source, target, **edge)
+            else:
+                arrow = "->" if directed else "--"
+                msg = f"edge #{i} ({source!r}{arrow}{target!r}) is duplicated"
+                raise EasyGraphError(msg)
+        else:
+            key = edge.pop("key", None)
+            if key is not None and G.has_edge(source, target, key):
+                arrow = "->" if directed else "--"
+                msg = f"edge #{i} ({source!r}{arrow}{target!r}, {key!r})"
+                msg2 = 'Hint: If multigraph add "multigraph 1" to file header.'
+                raise EasyGraphError(msg + " is duplicated\n" + msg2)
+            G.add_edge(source, target, key, **edge)
+
+    if label is not None and label != "id":
+        G = eg.relabel_nodes(G, mapping)
+    return G
+
+
+def generate_gml(G, stringizer=None):
+    r"""Generate a single entry of the graph `G` in GML format.
+
+    Parameters
+    ----------
+    G : EasyGraph graph
+        The graph to be converted to GML.
+
+    stringizer : callable, optional
+        A `stringizer` which converts non-int/non-float/non-dict values into
+        strings. If it cannot convert a value into a string, it should raise a
+        `ValueError` to indicate that. Default value: None.
+
+    Returns
+    -------
+    lines: generator of strings
+        Lines of GML data. Newlines are not appended.
+
+    Raises
+    ------
+    EasyGraphError
+        If `stringizer` cannot convert a value into a string, or the value to
+        convert is not a string while `stringizer` is None.
+
+    See Also
+    --------
+    literal_stringizer
+
+    Notes
+    -----
+    Graph attributes named 'directed', 'multigraph', 'node' or
+    'edge', node attributes named 'id' or 'label', edge attributes
+    named 'source' or 'target' (or 'key' if `G` is a multigraph)
+    are ignored because these attribute names are used to encode the graph
+    structure.
+
+    GML files are stored using a 7-bit ASCII encoding with any extended
+    ASCII characters (iso8859-1) appearing as HTML character entities.
+    Without specifying a `stringizer`/`destringizer`, the code is capable of
+    writing `int`/`float`/`str`/`dict`/`list` data as required by the GML
+    specification.  For writing other data types, and for reading data other
+    than `str` you need to explicitly supply a `stringizer`/`destringizer`.
+
+    For additional documentation on the GML file format, please see the
+    `GML url <https://web.archive.org/web/20190207140002/http://www.fim.uni-passau.de/index.php?id=17297&L=1>`_.
+
+    See the module docstring :mod:`easygraph.readwrite.gml` for more details.
+
+    Examples
+    --------
+    >>> G = eg.Graph()
+    >>> G.add_node("1")
+    >>> print("\n".join(eg.generate_gml(G)))
+    graph [
+      node [
+        id 0
+        label "1"
+      ]
+    ]
+    """
+    valid_keys = re.compile("^[A-Za-z][0-9A-Za-z_]*$")
+
+    def stringize(key, value, ignored_keys, indent, in_list=False):
+        if not isinstance(key, str):
+            raise EasyGraphError(f"{key!r} is not a string")
+        if not valid_keys.match(key):
+            raise EasyGraphError(f"{key!r} is not a valid key")
+        if not isinstance(key, str):
+            key = str(key)
+        if key not in ignored_keys:
+            if isinstance(value, (int, bool)):
+                if key == "label":
+                    yield indent + key + ' "' + str(value) + '"'
+                elif value is True:
+                    # python bool is an instance of int
+                    yield indent + key + " 1"
+                elif value is False:
+                    yield indent + key + " 0"
+                # GML only supports signed 32-bit integers
+                elif value < -(2**31) or value >= 2**31:
+                    yield indent + key + ' "' + str(value) + '"'
+                else:
+                    yield indent + key + " " + str(value)
+            elif isinstance(value, float):
+                text = repr(value).upper()
+                # GML matches INF to keys, so prepend + to INF. Use repr(float(*))
+                # instead of string literal to future proof against changes to repr.
+                if text == repr(float("inf")).upper():
+                    text = "+" + text
+                else:
+                    # GML requires that a real literal contain a decimal point, but
+                    # repr may not output a decimal point when the mantissa is
+                    # integral and hence needs fixing.
+                    epos = text.rfind("E")
+                    if epos != -1 and text.find(".", 0, epos) == -1:
+                        text = text[:epos] + "." + text[epos:]
+                if key == "label":
+                    yield indent + key + ' "' + text + '"'
+                else:
+                    yield indent + key + " " + text
+            elif isinstance(value, dict):
+                yield indent + key + " ["
+                next_indent = indent + "  "
+                for key, value in value.items():
+                    yield from stringize(key, value, (), next_indent)
+                yield indent + "]"
+            elif (
+                isinstance(value, (list, tuple))
+                and key != "label"
+                and value
+                and not in_list
+            ):
+                if len(value) == 1:
+                    yield indent + key + " " + f'"{LIST_START_VALUE}"'
+                for val in value:
+                    yield from stringize(key, val, (), indent, True)
+            else:
+                if stringizer:
+                    try:
+                        value = stringizer(value)
+                    except ValueError as err:
+                        raise EasyGraphError(
+                            f"{value!r} cannot be converted into a string"
+                        ) from err
+                if not isinstance(value, str):
+                    raise EasyGraphError(f"{value!r} is not a string")
+                yield indent + key + ' "' + escape(value) + '"'
+
+    yield "graph ["
+
+    # Output graph attributes
+    multigraph = G.is_multigraph()
+    if G.is_directed():
+        yield "  directed 1"
+    if multigraph:
+        yield "  multigraph 1"
+    ignored_keys = {"directed", "multigraph", "node", "edge"}
+    for attr, value in G.graph.items():
+        yield from stringize(attr, value, ignored_keys, "  ")
+
+    # Output node data
+    node_id = dict(zip(G, range(len(G))))
+    ignored_keys = {"id", "label"}
+    for node, attrs in G.nodes.items():
+        yield "  node ["
+        yield "    id " + str(node_id[node])
+        yield from stringize("label", node, (), "    ")
+        for attr, value in attrs.items():
+            yield from stringize(attr, value, ignored_keys, "    ")
+        yield "  ]"
+
+    # Output edge data
+    ignored_keys = {"source", "target"}
+    kwargs = {"data": True}
+    if multigraph:
+        ignored_keys.add("key")
+        kwargs["keys"] = True
+    for e in G.edges:
+        yield "  edge ["
+        yield "    source " + str(node_id[e[0]])
+        yield "    target " + str(node_id[e[1]])
+        if multigraph:
+            yield from stringize("key", e[2], (), "    ")
+        for attr, value in e[-1].items():
+            yield from stringize(attr, value, ignored_keys, "    ")
+        yield "  ]"
+    yield "]"
+
+
+@open_file(0, mode="rb")
+def read_gml(path, label="label", destringizer=None):
+    """Read graph in GML format from `path`.
+
+    Parameters
+    ----------
+    path : filename or filehandle
+        The filename or filehandle to read from.
+
+    label : string, optional
+        If not None, the parsed nodes will be renamed according to node
+        attributes indicated by `label`. Default value: 'label'.
+
+    destringizer : callable, optional
+        A `destringizer` that recovers values stored as strings in GML. If it
+        cannot convert a string to a value, a `ValueError` is raised. Default
+        value : None.
+
+    Returns
+    -------
+    G : EasyGraph graph
+        The parsed graph.
+
+    Raises
+    ------
+    EasyGraphError
+        If the input cannot be parsed.
+
+    See Also
+    --------
+    write_gml, parse_gml
+    literal_destringizer
+
+    Notes
+    -----
+    GML files are stored using a 7-bit ASCII encoding with any extended
+    ASCII characters (iso8859-1) appearing as HTML character entities.
+    Without specifying a `stringizer`/`destringizer`, the code is capable of
+    writing `int`/`float`/`str`/`dict`/`list` data as required by the GML
+    specification.  For writing other data types, and for reading data other
+    than `str` you need to explicitly supply a `stringizer`/`destringizer`.
+
+    For additional documentation on the GML file format, please see the
+    `GML url <https://web.archive.org/web/20190207140002/http://www.fim.uni-passau.de/index.php?id=17297&L=1>`_.
+
+    See the module docstring :mod:`easygraph.readwrite.gml` for more details.
+
+    Examples
+    --------
+    >>> G = eg.path_graph(4)
+    >>> eg.write_gml(G, "test.gml")
+
+    GML values are interpreted as strings by default:
+
+    >>> H = eg.read_gml("test.gml")
+    >>> H.nodes
+    NodeView(('0', '1', '2', '3'))
+
+    When a `destringizer` is provided, GML values are converted to the provided type.
+    For example, integer nodes can be recovered as shown below:
+
+    >>> J = eg.read_gml("test.gml", destringizer=int)
+    >>> J.nodes
+    NodeView((0, 1, 2, 3))
+
+    """
+
+    def filter_lines(lines):
+        for line in lines:
+            try:
+                line = line.decode("ascii")
+            except UnicodeDecodeError as err:
+                raise EasyGraphError("input is not ASCII-encoded") from err
+            if not isinstance(line, str):
+                lines = str(lines)
+            if line and line[-1] == "\n":
+                line = line[:-1]
+            yield line
+
+    G = parse_gml_lines(filter_lines(path), label, destringizer)
+    return G
+
+
+@open_file(1, mode="wb")
+def write_gml(G, path, stringizer=None):
+    """Write a graph `G` in GML format to the file or file handle `path`.
+
+    Parameters
+    ----------
+    G : EasyGraph graph
+        The graph to be converted to GML.
+
+    path : filename or filehandle
+        The filename or filehandle to write. Files whose names end with .gz or
+        .bz2 will be compressed.
+
+    stringizer : callable, optional
+        A `stringizer` which converts non-int/non-float/non-dict values into
+        strings. If it cannot convert a value into a string, it should raise a
+        `ValueError` to indicate that. Default value: None.
+
+    Raises
+    ------
+    EasyGraphError
+        If `stringizer` cannot convert a value into a string, or the value to
+        convert is not a string while `stringizer` is None.
+
+    See Also
+    --------
+    read_gml, generate_gml
+    literal_stringizer
+
+    Notes
+    -----
+    Graph attributes named 'directed', 'multigraph', 'node' or
+    'edge', node attributes named 'id' or 'label', edge attributes
+    named 'source' or 'target' (or 'key' if `G` is a multigraph)
+    are ignored because these attribute names are used to encode the graph
+    structure.
+
+    GML files are stored using a 7-bit ASCII encoding with any extended
+    ASCII characters (iso8859-1) appearing as HTML character entities.
+    Without specifying a `stringizer`/`destringizer`, the code is capable of
+    writing `int`/`float`/`str`/`dict`/`list` data as required by the GML
+    specification.  For writing other data types, and for reading data other
+    than `str` you need to explicitly supply a `stringizer`/`destringizer`.
+
+    Note that while we allow non-standard GML to be read from a file, we make
+    sure to write GML format. In particular, underscores are not allowed in
+    attribute names.
+    For additional documentation on the GML file format, please see the
+    `GML url <https://web.archive.org/web/20190207140002/http://www.fim.uni-passau.de/index.php?id=17297&L=1>`_.
+
+    See the module docstring :mod:`easygraph.readwrite.gml` for more details.
+
+    Examples
+    --------
+    >>> G = eg.path_graph(4)
+    >>> eg.write_gml(G, "test.gml")
+
+    Filenames ending in .gz or .bz2 will be compressed.
+
+    >>> eg.write_gml(G, "test.gml.gz")
+    """
+    for line in generate_gml(G, stringizer):
+        path.write((line + "\n").encode("ascii"))
+
+
+def literal_stringizer(value):
+    msg = "literal_stringizer is deprecated and will be removed in 3.0."
+    warnings.warn(msg, DeprecationWarning)
+
+    def stringize(value):
+        if isinstance(value, (int, bool)) or value is None:
+            if value is True:  # GML uses 1/0 for boolean values.
+                buf.write(str(1))
+            elif value is False:
+                buf.write(str(0))
+            else:
+                buf.write(str(value))
+        elif isinstance(value, str):
+            text = repr(value)
+            if text[0] != "u":
+                try:
+                    value.encode("latin1")
+                except UnicodeEncodeError:
+                    text = "u" + text
+            buf.write(text)
+        elif isinstance(value, (float, complex, str, bytes)):
+            buf.write(repr(value))
+        elif isinstance(value, list):
+            buf.write("[")
+            first = True
+            for item in value:
+                if not first:
+                    buf.write(",")
+                else:
+                    first = False
+                stringize(item)
+            buf.write("]")
+        elif isinstance(value, tuple):
+            if len(value) > 1:
+                buf.write("(")
+                first = True
+                for item in value:
+                    if not first:
+                        buf.write(",")
+                    else:
+                        first = False
+                    stringize(item)
+                buf.write(")")
+            elif value:
+                buf.write("(")
+                stringize(value[0])
+                buf.write(",)")
+            else:
+                buf.write("()")
+        elif isinstance(value, dict):
+            buf.write("{")
+            first = True
+            for key, value in value.items():
+                if not first:
+                    buf.write(",")
+                else:
+                    first = False
+                stringize(key)
+                buf.write(":")
+                stringize(value)
+            buf.write("}")
+        elif isinstance(value, set):
+            buf.write("{")
+            first = True
+            for item in value:
+                if not first:
+                    buf.write(",")
+                else:
+                    first = False
+                stringize(item)
+            buf.write("}")
+        else:
+            msg = "{value!r} cannot be converted into a Python literal"
+            raise ValueError(msg)
+
+    buf = StringIO()
+    stringize(value)
+    return buf.getvalue()
```

## easygraph/readwrite/graphviz.py

 * *Ordering differences only*

```diff
@@ -1,180 +1,180 @@
-import easygraph as eg
-
-
-__all__ = ["write_dot", "read_dot", "from_agraph", "to_agraph"]
-
-
-def from_agraph(A, create_using=None):
-    """Returns a EasyGraph Graph or DiGraph from a PyGraphviz graph.
-
-    Parameters
-    ----------
-    A : PyGraphviz AGraph
-      A graph created with PyGraphviz
-
-    create_using : EasyGraph graph constructor, optional (default=None)
-       Graph type to create. If graph instance, then cleared before populated.
-       If `None`, then the appropriate Graph type is inferred from `A`.
-
-    Examples
-    --------
-    >>> K5 = eg.complete_graph(5)
-    >>> A = eg.to_agraph(K5)
-    >>> G = eg.from_agraph(A)
-
-    Notes
-    -----
-    The Graph G will have a dictionary G.graph_attr containing
-    the default graphviz attributes for graphs, nodes and edges.
-
-    Default node attributes will be in the dictionary G.node_attr
-    which is keyed by node.
-
-    Edge attributes will be returned as edge data in G.  With
-    edge_attr=False the edge data will be the Graphviz edge weight
-    attribute or the value 1 if no edge weight attribute is found.
-
-    """
-    if create_using is None:
-        if A.is_directed():
-            if A.is_strict():
-                create_using = eg.DiGraph
-            else:
-                create_using = eg.MultiDiGraph
-        else:
-            if A.is_strict():
-                create_using = eg.Graph
-            else:
-                create_using = eg.MultiGraph
-
-    # assign defaults
-    N = eg.empty_graph(0, create_using)
-    if A.name is not None:
-        N.name = A.name
-
-    # add graph attributes
-    N.graph.update(A.graph_attr)
-
-    # add nodes, attributes to N.node_attr
-    for n in A.nodes():
-        str_attr = {str(k): v for k, v in n.attr.items()}
-        N.add_node(str(n), **str_attr)
-
-    # add edges, assign edge data as dictionary of attributes
-    for e in A.edges():
-        u, v = str(e[0]), str(e[1])
-        attr = dict(e.attr)
-        str_attr = {str(k): v for k, v in attr.items()}
-        if not N.is_multigraph():
-            if e.name is not None:
-                str_attr["key"] = e.name
-            N.add_edge(u, v, **str_attr)
-        else:
-            N.add_edge(u, v, key=e.name, **str_attr)
-
-    # add default attributes for graph, nodes, and edges
-    # hang them on N.graph_attr
-    N.graph["graph"] = dict(A.graph_attr)
-    N.graph["node"] = dict(A.node_attr)
-    N.graph["edge"] = dict(A.edge_attr)
-    return N
-
-
-def to_agraph(N):
-    """Returns a pygraphviz graph from a EasyGraph graph N.
-
-    Parameters
-    ----------
-    N : EasyGraph graph
-      A graph created with EasyGraph
-
-    Examples
-    --------
-    >>> K5 = eg.complete_graph(5)
-    >>> A = eg.to_agraph(K5)
-
-    Notes
-    -----
-    If N has an dict N.graph_attr an attempt will be made first
-    to copy properties attached to the graph (see from_agraph)
-    and then updated with the calling arguments if any.
-
-    """
-    try:
-        import pygraphviz
-    except ImportError as err:
-        raise ImportError("requires pygraphviz http://pygraphviz.github.io/") from err
-    directed = N.is_directed()
-    strict = eg.number_of_selfloops(N) == 0 and not N.is_multigraph()
-    A = pygraphviz.AGraph(name=N.name, strict=strict, directed=directed)
-
-    # default graph attributes
-    A.graph_attr.update(N.graph.get("graph", {}))
-    A.node_attr.update(N.graph.get("node", {}))
-    A.edge_attr.update(N.graph.get("edge", {}))
-
-    A.graph_attr.update(
-        (k, v) for k, v in N.graph.items() if k not in ("graph", "node", "edge")
-    )
-
-    # add nodes
-    for n, nodedata in N.nodes.items():
-        A.add_node(n)
-        # Add node data
-        a = A.get_node(n)
-        a.attr.update({k: str(v) for k, v in nodedata.items()})
-
-    # loop over edges
-    if N.is_multigraph():
-        for u, v, key, edgedata in N.edges:
-            str_edgedata = {k: str(v) for k, v in edgedata.items() if k != "key"}
-            A.add_edge(u, v, key=str(key))
-            # Add edge data
-            a = A.get_edge(u, v)
-            a.attr.update(str_edgedata)
-
-    else:
-        for u, v, edgedata in N.edges:
-            str_edgedata = {k: str(v) for k, v in edgedata.items()}
-            A.add_edge(u, v)
-            # Add edge data
-            a = A.get_edge(u, v)
-            a.attr.update(str_edgedata)
-
-    return A
-
-
-def write_dot(G, path):
-    """Write EasyGraph graph G to Graphviz dot format on path.
-
-    Parameters
-    ----------
-    G : graph
-       A easygraph graph
-    path : filename
-       Filename or file handle to write
-    """
-    A = to_agraph(G)
-    A.write(path)
-    A.clear()
-    return
-
-
-def read_dot(path):
-    """Returns a EasyGraph graph from a dot file on path.
-
-    Parameters
-    ----------
-    path : file or string
-       File name or file handle to read.
-    """
-    try:
-        import pygraphviz
-    except ImportError as err:
-        raise ImportError(
-            "read_dot() requires pygraphviz http://pygraphviz.github.io/"
-        ) from err
-    A = pygraphviz.AGraph(file=path)
-    gr = from_agraph(A)
-    A.clear()
-    return gr
+import easygraph as eg
+
+
+__all__ = ["write_dot", "read_dot", "from_agraph", "to_agraph"]
+
+
+def from_agraph(A, create_using=None):
+    """Returns a EasyGraph Graph or DiGraph from a PyGraphviz graph.
+
+    Parameters
+    ----------
+    A : PyGraphviz AGraph
+      A graph created with PyGraphviz
+
+    create_using : EasyGraph graph constructor, optional (default=None)
+       Graph type to create. If graph instance, then cleared before populated.
+       If `None`, then the appropriate Graph type is inferred from `A`.
+
+    Examples
+    --------
+    >>> K5 = eg.complete_graph(5)
+    >>> A = eg.to_agraph(K5)
+    >>> G = eg.from_agraph(A)
+
+    Notes
+    -----
+    The Graph G will have a dictionary G.graph_attr containing
+    the default graphviz attributes for graphs, nodes and edges.
+
+    Default node attributes will be in the dictionary G.node_attr
+    which is keyed by node.
+
+    Edge attributes will be returned as edge data in G.  With
+    edge_attr=False the edge data will be the Graphviz edge weight
+    attribute or the value 1 if no edge weight attribute is found.
+
+    """
+    if create_using is None:
+        if A.is_directed():
+            if A.is_strict():
+                create_using = eg.DiGraph
+            else:
+                create_using = eg.MultiDiGraph
+        else:
+            if A.is_strict():
+                create_using = eg.Graph
+            else:
+                create_using = eg.MultiGraph
+
+    # assign defaults
+    N = eg.empty_graph(0, create_using)
+    if A.name is not None:
+        N.name = A.name
+
+    # add graph attributes
+    N.graph.update(A.graph_attr)
+
+    # add nodes, attributes to N.node_attr
+    for n in A.nodes():
+        str_attr = {str(k): v for k, v in n.attr.items()}
+        N.add_node(str(n), **str_attr)
+
+    # add edges, assign edge data as dictionary of attributes
+    for e in A.edges():
+        u, v = str(e[0]), str(e[1])
+        attr = dict(e.attr)
+        str_attr = {str(k): v for k, v in attr.items()}
+        if not N.is_multigraph():
+            if e.name is not None:
+                str_attr["key"] = e.name
+            N.add_edge(u, v, **str_attr)
+        else:
+            N.add_edge(u, v, key=e.name, **str_attr)
+
+    # add default attributes for graph, nodes, and edges
+    # hang them on N.graph_attr
+    N.graph["graph"] = dict(A.graph_attr)
+    N.graph["node"] = dict(A.node_attr)
+    N.graph["edge"] = dict(A.edge_attr)
+    return N
+
+
+def to_agraph(N):
+    """Returns a pygraphviz graph from a EasyGraph graph N.
+
+    Parameters
+    ----------
+    N : EasyGraph graph
+      A graph created with EasyGraph
+
+    Examples
+    --------
+    >>> K5 = eg.complete_graph(5)
+    >>> A = eg.to_agraph(K5)
+
+    Notes
+    -----
+    If N has an dict N.graph_attr an attempt will be made first
+    to copy properties attached to the graph (see from_agraph)
+    and then updated with the calling arguments if any.
+
+    """
+    try:
+        import pygraphviz
+    except ImportError as err:
+        raise ImportError("requires pygraphviz http://pygraphviz.github.io/") from err
+    directed = N.is_directed()
+    strict = eg.number_of_selfloops(N) == 0 and not N.is_multigraph()
+    A = pygraphviz.AGraph(name=N.name, strict=strict, directed=directed)
+
+    # default graph attributes
+    A.graph_attr.update(N.graph.get("graph", {}))
+    A.node_attr.update(N.graph.get("node", {}))
+    A.edge_attr.update(N.graph.get("edge", {}))
+
+    A.graph_attr.update(
+        (k, v) for k, v in N.graph.items() if k not in ("graph", "node", "edge")
+    )
+
+    # add nodes
+    for n, nodedata in N.nodes.items():
+        A.add_node(n)
+        # Add node data
+        a = A.get_node(n)
+        a.attr.update({k: str(v) for k, v in nodedata.items()})
+
+    # loop over edges
+    if N.is_multigraph():
+        for u, v, key, edgedata in N.edges:
+            str_edgedata = {k: str(v) for k, v in edgedata.items() if k != "key"}
+            A.add_edge(u, v, key=str(key))
+            # Add edge data
+            a = A.get_edge(u, v)
+            a.attr.update(str_edgedata)
+
+    else:
+        for u, v, edgedata in N.edges:
+            str_edgedata = {k: str(v) for k, v in edgedata.items()}
+            A.add_edge(u, v)
+            # Add edge data
+            a = A.get_edge(u, v)
+            a.attr.update(str_edgedata)
+
+    return A
+
+
+def write_dot(G, path):
+    """Write EasyGraph graph G to Graphviz dot format on path.
+
+    Parameters
+    ----------
+    G : graph
+       A easygraph graph
+    path : filename
+       Filename or file handle to write
+    """
+    A = to_agraph(G)
+    A.write(path)
+    A.clear()
+    return
+
+
+def read_dot(path):
+    """Returns a EasyGraph graph from a dot file on path.
+
+    Parameters
+    ----------
+    path : file or string
+       File name or file handle to read.
+    """
+    try:
+        import pygraphviz
+    except ImportError as err:
+        raise ImportError(
+            "read_dot() requires pygraphviz http://pygraphviz.github.io/"
+        ) from err
+    A = pygraphviz.AGraph(file=path)
+    gr = from_agraph(A)
+    A.clear()
+    return gr
```

## easygraph/readwrite/pickle.py

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-#!/usr/bin/env python3
-
-
-def read_pickle(file_name):
-    import pickle
-
-    with open(file_name, "rb") as f:
-        return pickle.load(f)
-
-
-def write_pickle(file_name, obj):
-    import pickle
-
-    with open(file_name, "wb") as f:
-        pickle.dump(obj, f)
+#!/usr/bin/env python3
+
+
+def read_pickle(file_name):
+    import pickle
+
+    with open(file_name, "rb") as f:
+        return pickle.load(f)
+
+
+def write_pickle(file_name, obj):
+    import pickle
+
+    with open(file_name, "wb") as f:
+        pickle.dump(obj, f)
```

## easygraph/readwrite/pajek.py

 * *Ordering differences only*

```diff
@@ -1,337 +1,337 @@
-# This file is part of the NetworkX distribution.
-
-# NetworkX is distributed with the 3-clause BSD license.
-
-
-# ::
-#    Copyright (C) 2004-2022, NetworkX Developers
-#    Aric Hagberg <hagberg@lanl.gov>
-#    Dan Schult <dschult@colgate.edu>
-#    Pieter Swart <swart@lanl.gov>
-#    All rights reserved.
-
-#    Redistribution and use in source and binary forms, with or without
-#    modification, are permitted provided that the following conditions are
-#    met:
-
-#      * Redistributions of source code must retain the above copyright
-#        notice, this list of conditions and the following disclaimer.
-
-#      * Redistributions in binary form must reproduce the above
-#        copyright notice, this list of conditions and the following
-#        disclaimer in the documentation and/or other materials provided
-#        with the distribution.
-
-#      * Neither the name of the NetworkX Developers nor the names of its
-#        contributors may be used to endorse or promote products derived
-#        from this software without specific prior written permission.
-
-#    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
-#    "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
-#    LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
-#    A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
-#    OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-#    SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
-#    LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
-#    DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
-#    THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-#    (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-#    OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-"""
-*****
-Pajek
-*****
-Read graphs in Pajek format.
-
-This implementation handles directed and undirected graphs including
-those with self loops and parallel edges.
-
-Format
-------
-See http://vlado.fmf.uni-lj.si/pub/networks/pajek/doc/draweps.htm
-for format information.
-
-"""
-
-import warnings
-
-import easygraph as eg
-
-# import networkx as nx
-from easygraph.utils import open_file
-
-
-__all__ = ["read_pajek", "parse_pajek", "generate_pajek", "write_pajek"]
-
-
-def generate_pajek(G):
-    """Generate lines in Pajek graph format.
-
-    Parameters
-    ----------
-    G : graph
-       A EasyGraph graph
-
-    References
-    ----------
-    See http://vlado.fmf.uni-lj.si/pub/networks/pajek/doc/draweps.htm
-    for format information.
-    """
-    if G.name == "":
-        name = "EasyGraph"
-    else:
-        name = G.name
-    # Apparently many Pajek format readers can't process this line
-    # So we'll leave it out for now.
-    # yield '*network %s'%name
-
-    # write nodes with attributes
-    yield f"*vertices {G.order()}"
-    nodes = list(G)
-    # make dictionary mapping nodes to integers
-    nodenumber = dict(zip(nodes, range(1, len(nodes) + 1)))
-    for n in nodes:
-        # copy node attributes and pop mandatory attributes
-        # to avoid duplication.
-        na = G.nodes.get(n, {}).copy()
-        x = na.pop("x", 0.0)
-        y = na.pop("y", 0.0)
-        try:
-            id = int(na.pop("id", nodenumber[n]))
-        except ValueError as err:
-            err.args += (
-                "Pajek format requires 'id' to be an int()."
-                " Refer to the 'Relabeling nodes' section.",
-            )
-            raise
-        nodenumber[n] = id
-        shape = na.pop("shape", "ellipse")
-        s = " ".join(map(make_qstr, (id, n, x, y, shape)))
-        # only optional attributes are left in na.
-        for k, v in na.items():
-            if isinstance(v, str) and v.strip() != "":
-                s += f" {make_qstr(k)} {make_qstr(v)}"
-            else:
-                warnings.warn(
-                    f"Node attribute {k} is not processed."
-                    f" {('Empty attribute' if isinstance(v, str) else 'Non-string attribute')}."
-                )
-        yield s
-
-    # write edges with attributes
-    if G.is_directed():
-        yield "*arcs"
-    else:
-        yield "*edges"
-    # from icecream import ic
-    # ic(G.edges)
-    # if isinstance(G, MultiGraph)
-    for u, v, *edgedata in G.edges:
-        # if len(edgedata) > 1:
-        #     edgedata = edgedata[1]
-        # else:
-        #     edgedata = edgedata[0]
-        edgedata = edgedata[-1]
-        d = edgedata.copy()
-        value = d.pop("weight", 1.0)  # use 1 as default edge value
-        s = " ".join(map(make_qstr, (nodenumber[u], nodenumber[v], value)))
-        for k, v in d.items():
-            if isinstance(v, str) and v.strip() != "":
-                s += f" {make_qstr(k)} {make_qstr(v)}"
-            else:
-                warnings.warn(
-                    f"Edge attribute {k} is not processed."
-                    f" {('Empty attribute' if isinstance(v, str) else 'Non-string attribute')}."
-                )
-        yield s
-
-
-@open_file(1, mode="wb")
-def write_pajek(G, path, encoding="UTF-8"):
-    """Write graph in Pajek format to path.
-
-    Parameters
-    ----------
-    G : graph
-       A EasyGraph graph
-    path : file or string
-       File or filename to write.
-       Filenames ending in .gz or .bz2 will be compressed.
-
-    Examples
-    --------
-    >>> G = eg.path_graph(4)
-    >>> eg.write_pajek(G, "test.net")
-
-    Warnings
-    --------
-    Optional node attributes and edge attributes must be non-empty strings.
-    Otherwise it will not be written into the file. You will need to
-    convert those attributes to strings if you want to keep them.
-
-    References
-    ----------
-    See http://vlado.fmf.uni-lj.si/pub/networks/pajek/doc/draweps.htm
-    for format information.
-    """
-    for line in generate_pajek(G):
-        line += "\n"
-        path.write(line.encode(encoding))
-
-
-@open_file(0, mode="rb")
-def read_pajek(path):
-    """Read graph in Pajek format from path.
-
-    Parameters
-    ----------
-    path : file or string
-       File or filename to write.
-       Filenames ending in .gz or .bz2 will be uncompressed.
-
-    Returns
-    -------
-    G : EasyGraph MultiGraph or MultiDiGraph.
-
-    Examples
-    --------
-    >>> G = eg.path_graph(4)
-    >>> eg.write_pajek(G, "test.net")
-    >>> G = eg.read_pajek("test.net")
-
-    To create a Graph instead of a MultiGraph use
-
-    >>> G1 = eg.Graph(G)
-
-    References
-    ----------
-    See http://vlado.fmf.uni-lj.si/pub/networks/pajek/doc/draweps.htm
-    for format information.
-    """
-    lines = (line.decode() for line in path)
-    # with open(path) as f:
-    #     lines = f.readlines()
-    return parse_pajek(lines)
-
-
-def parse_pajek(lines):
-    """Parse Pajek format graph from string or iterable.
-
-    Parameters
-    ----------
-    lines : string or iterable
-       Data in Pajek format.
-
-    Returns
-    -------
-    G : EasyGraph graph
-
-    See Also
-    --------
-    read_pajek
-
-    """
-    import shlex
-
-    # multigraph=False
-    if isinstance(lines, str):
-        lines = iter(lines.split("\n"))
-    # from itertools import tee
-    # lines, lines2 = tee(lines)
-    # from icecream import ic
-    # ic(next(lines2))
-    lines = iter([line.rstrip("\n") for line in lines])
-    G = eg.MultiDiGraph()  # are multiedges allowed in Pajek? assume yes
-    labels = []  # in the order of the file, needed for matrix
-    while lines:
-        try:
-            l = next(lines)
-        except:  # EOF
-            break
-        if l.lower().startswith("*network"):
-            try:
-                label, name = l.split(None, 1)
-            except ValueError:
-                # Line was not of the form:  *network NAME
-                pass
-            else:
-                G.graph["name"] = name
-        elif l.lower().startswith("*vertices"):
-            nodelabels = {}
-            l, nnodes = l.split()
-            for i in range(int(nnodes)):
-                l = next(lines)
-                try:
-                    splitline = [x for x in shlex.split(str(l))]
-                except AttributeError:
-                    splitline = shlex.split(str(l))
-                id, label = splitline[0:2]
-                labels.append(label)
-                G.add_node(label)
-                nodelabels[id] = label
-                G.nodes[label]["id"] = id
-                try:
-                    x, y, shape = splitline[2:5]
-                    G.nodes[label].update(
-                        {"x": float(x), "y": float(y), "shape": shape}
-                    )
-                except:
-                    pass
-                extra_attr = zip(splitline[5::2], splitline[6::2])
-                G.nodes[label].update(extra_attr)
-        elif l.lower().startswith("*edges") or l.lower().startswith("*arcs"):
-            if l.lower().startswith("*edge"):
-                # switch from multidigraph to multigraph
-                G = eg.MultiGraph(G)
-            if l.lower().startswith("*arcs"):
-                # switch to directed with multiple arcs for each existing edge
-                # G = G.to_directed()
-                pass
-            for l in lines:
-                try:
-                    splitline = [x for x in shlex.split(str(l))]
-                except AttributeError:
-                    splitline = shlex.split(str(l))
-
-                if len(splitline) < 2:
-                    continue
-                ui, vi = splitline[0:2]
-                u = nodelabels.get(ui, ui)
-                v = nodelabels.get(vi, vi)
-                # parse the data attached to this edge and put in a dictionary
-                edge_data = {}
-                try:
-                    # there should always be a single value on the edge?
-                    w = splitline[2:3]
-                    edge_data.update({"weight": float(w[0])})
-                except:
-                    pass
-                    # if there isn't, just assign a 1
-                #                    edge_data.update({'value':1})
-                extra_attr = zip(splitline[3::2], splitline[4::2])
-                edge_data.update(extra_attr)
-                # if G.has_edge(u,v):
-                #     multigraph=True
-                G.add_edge(u, v, **edge_data)
-        elif l.lower().startswith("*matrix"):
-            G = eg.DiGraph(G)
-            adj_list = (
-                (labels[row], labels[col], {"weight": int(data)})
-                for (row, line) in enumerate(lines)
-                for (col, data) in enumerate(line.split())
-                if int(data) != 0
-            )
-            G.add_edges_from(adj_list)
-
-    return G
-
-
-def make_qstr(t):
-    """Returns the string representation of t.
-    Add outer double-quotes if the string has a space.
-    """
-    if not isinstance(t, str):
-        t = str(t)
-    if " " in t:
-        t = f'"{t}"'
-    return t
+# This file is part of the NetworkX distribution.
+
+# NetworkX is distributed with the 3-clause BSD license.
+
+
+# ::
+#    Copyright (C) 2004-2022, NetworkX Developers
+#    Aric Hagberg <hagberg@lanl.gov>
+#    Dan Schult <dschult@colgate.edu>
+#    Pieter Swart <swart@lanl.gov>
+#    All rights reserved.
+
+#    Redistribution and use in source and binary forms, with or without
+#    modification, are permitted provided that the following conditions are
+#    met:
+
+#      * Redistributions of source code must retain the above copyright
+#        notice, this list of conditions and the following disclaimer.
+
+#      * Redistributions in binary form must reproduce the above
+#        copyright notice, this list of conditions and the following
+#        disclaimer in the documentation and/or other materials provided
+#        with the distribution.
+
+#      * Neither the name of the NetworkX Developers nor the names of its
+#        contributors may be used to endorse or promote products derived
+#        from this software without specific prior written permission.
+
+#    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+#    "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+#    LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+#    A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+#    OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#    SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+#    LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+#    DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+#    THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+#    (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+#    OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+"""
+*****
+Pajek
+*****
+Read graphs in Pajek format.
+
+This implementation handles directed and undirected graphs including
+those with self loops and parallel edges.
+
+Format
+------
+See http://vlado.fmf.uni-lj.si/pub/networks/pajek/doc/draweps.htm
+for format information.
+
+"""
+
+import warnings
+
+import easygraph as eg
+
+# import networkx as nx
+from easygraph.utils import open_file
+
+
+__all__ = ["read_pajek", "parse_pajek", "generate_pajek", "write_pajek"]
+
+
+def generate_pajek(G):
+    """Generate lines in Pajek graph format.
+
+    Parameters
+    ----------
+    G : graph
+       A EasyGraph graph
+
+    References
+    ----------
+    See http://vlado.fmf.uni-lj.si/pub/networks/pajek/doc/draweps.htm
+    for format information.
+    """
+    if G.name == "":
+        name = "EasyGraph"
+    else:
+        name = G.name
+    # Apparently many Pajek format readers can't process this line
+    # So we'll leave it out for now.
+    # yield '*network %s'%name
+
+    # write nodes with attributes
+    yield f"*vertices {G.order()}"
+    nodes = list(G)
+    # make dictionary mapping nodes to integers
+    nodenumber = dict(zip(nodes, range(1, len(nodes) + 1)))
+    for n in nodes:
+        # copy node attributes and pop mandatory attributes
+        # to avoid duplication.
+        na = G.nodes.get(n, {}).copy()
+        x = na.pop("x", 0.0)
+        y = na.pop("y", 0.0)
+        try:
+            id = int(na.pop("id", nodenumber[n]))
+        except ValueError as err:
+            err.args += (
+                "Pajek format requires 'id' to be an int()."
+                " Refer to the 'Relabeling nodes' section.",
+            )
+            raise
+        nodenumber[n] = id
+        shape = na.pop("shape", "ellipse")
+        s = " ".join(map(make_qstr, (id, n, x, y, shape)))
+        # only optional attributes are left in na.
+        for k, v in na.items():
+            if isinstance(v, str) and v.strip() != "":
+                s += f" {make_qstr(k)} {make_qstr(v)}"
+            else:
+                warnings.warn(
+                    f"Node attribute {k} is not processed."
+                    f" {('Empty attribute' if isinstance(v, str) else 'Non-string attribute')}."
+                )
+        yield s
+
+    # write edges with attributes
+    if G.is_directed():
+        yield "*arcs"
+    else:
+        yield "*edges"
+    # from icecream import ic
+    # ic(G.edges)
+    # if isinstance(G, MultiGraph)
+    for u, v, *edgedata in G.edges:
+        # if len(edgedata) > 1:
+        #     edgedata = edgedata[1]
+        # else:
+        #     edgedata = edgedata[0]
+        edgedata = edgedata[-1]
+        d = edgedata.copy()
+        value = d.pop("weight", 1.0)  # use 1 as default edge value
+        s = " ".join(map(make_qstr, (nodenumber[u], nodenumber[v], value)))
+        for k, v in d.items():
+            if isinstance(v, str) and v.strip() != "":
+                s += f" {make_qstr(k)} {make_qstr(v)}"
+            else:
+                warnings.warn(
+                    f"Edge attribute {k} is not processed."
+                    f" {('Empty attribute' if isinstance(v, str) else 'Non-string attribute')}."
+                )
+        yield s
+
+
+@open_file(1, mode="wb")
+def write_pajek(G, path, encoding="UTF-8"):
+    """Write graph in Pajek format to path.
+
+    Parameters
+    ----------
+    G : graph
+       A EasyGraph graph
+    path : file or string
+       File or filename to write.
+       Filenames ending in .gz or .bz2 will be compressed.
+
+    Examples
+    --------
+    >>> G = eg.path_graph(4)
+    >>> eg.write_pajek(G, "test.net")
+
+    Warnings
+    --------
+    Optional node attributes and edge attributes must be non-empty strings.
+    Otherwise it will not be written into the file. You will need to
+    convert those attributes to strings if you want to keep them.
+
+    References
+    ----------
+    See http://vlado.fmf.uni-lj.si/pub/networks/pajek/doc/draweps.htm
+    for format information.
+    """
+    for line in generate_pajek(G):
+        line += "\n"
+        path.write(line.encode(encoding))
+
+
+@open_file(0, mode="rb")
+def read_pajek(path):
+    """Read graph in Pajek format from path.
+
+    Parameters
+    ----------
+    path : file or string
+       File or filename to write.
+       Filenames ending in .gz or .bz2 will be uncompressed.
+
+    Returns
+    -------
+    G : EasyGraph MultiGraph or MultiDiGraph.
+
+    Examples
+    --------
+    >>> G = eg.path_graph(4)
+    >>> eg.write_pajek(G, "test.net")
+    >>> G = eg.read_pajek("test.net")
+
+    To create a Graph instead of a MultiGraph use
+
+    >>> G1 = eg.Graph(G)
+
+    References
+    ----------
+    See http://vlado.fmf.uni-lj.si/pub/networks/pajek/doc/draweps.htm
+    for format information.
+    """
+    lines = (line.decode() for line in path)
+    # with open(path) as f:
+    #     lines = f.readlines()
+    return parse_pajek(lines)
+
+
+def parse_pajek(lines):
+    """Parse Pajek format graph from string or iterable.
+
+    Parameters
+    ----------
+    lines : string or iterable
+       Data in Pajek format.
+
+    Returns
+    -------
+    G : EasyGraph graph
+
+    See Also
+    --------
+    read_pajek
+
+    """
+    import shlex
+
+    # multigraph=False
+    if isinstance(lines, str):
+        lines = iter(lines.split("\n"))
+    # from itertools import tee
+    # lines, lines2 = tee(lines)
+    # from icecream import ic
+    # ic(next(lines2))
+    lines = iter([line.rstrip("\n") for line in lines])
+    G = eg.MultiDiGraph()  # are multiedges allowed in Pajek? assume yes
+    labels = []  # in the order of the file, needed for matrix
+    while lines:
+        try:
+            l = next(lines)
+        except:  # EOF
+            break
+        if l.lower().startswith("*network"):
+            try:
+                label, name = l.split(None, 1)
+            except ValueError:
+                # Line was not of the form:  *network NAME
+                pass
+            else:
+                G.graph["name"] = name
+        elif l.lower().startswith("*vertices"):
+            nodelabels = {}
+            l, nnodes = l.split()
+            for i in range(int(nnodes)):
+                l = next(lines)
+                try:
+                    splitline = [x for x in shlex.split(str(l))]
+                except AttributeError:
+                    splitline = shlex.split(str(l))
+                id, label = splitline[0:2]
+                labels.append(label)
+                G.add_node(label)
+                nodelabels[id] = label
+                G.nodes[label]["id"] = id
+                try:
+                    x, y, shape = splitline[2:5]
+                    G.nodes[label].update(
+                        {"x": float(x), "y": float(y), "shape": shape}
+                    )
+                except:
+                    pass
+                extra_attr = zip(splitline[5::2], splitline[6::2])
+                G.nodes[label].update(extra_attr)
+        elif l.lower().startswith("*edges") or l.lower().startswith("*arcs"):
+            if l.lower().startswith("*edge"):
+                # switch from multidigraph to multigraph
+                G = eg.MultiGraph(G)
+            if l.lower().startswith("*arcs"):
+                # switch to directed with multiple arcs for each existing edge
+                # G = G.to_directed()
+                pass
+            for l in lines:
+                try:
+                    splitline = [x for x in shlex.split(str(l))]
+                except AttributeError:
+                    splitline = shlex.split(str(l))
+
+                if len(splitline) < 2:
+                    continue
+                ui, vi = splitline[0:2]
+                u = nodelabels.get(ui, ui)
+                v = nodelabels.get(vi, vi)
+                # parse the data attached to this edge and put in a dictionary
+                edge_data = {}
+                try:
+                    # there should always be a single value on the edge?
+                    w = splitline[2:3]
+                    edge_data.update({"weight": float(w[0])})
+                except:
+                    pass
+                    # if there isn't, just assign a 1
+                #                    edge_data.update({'value':1})
+                extra_attr = zip(splitline[3::2], splitline[4::2])
+                edge_data.update(extra_attr)
+                # if G.has_edge(u,v):
+                #     multigraph=True
+                G.add_edge(u, v, **edge_data)
+        elif l.lower().startswith("*matrix"):
+            G = eg.DiGraph(G)
+            adj_list = (
+                (labels[row], labels[col], {"weight": int(data)})
+                for (row, line) in enumerate(lines)
+                for (col, data) in enumerate(line.split())
+                if int(data) != 0
+            )
+            G.add_edges_from(adj_list)
+
+    return G
+
+
+def make_qstr(t):
+    """Returns the string representation of t.
+    Add outer double-quotes if the string has a space.
+    """
+    if not isinstance(t, str):
+        t = str(t)
+    if " " in t:
+        t = f'"{t}"'
+    return t
```

## easygraph/readwrite/graphml.py

 * *Ordering differences only*

```diff
@@ -1,1056 +1,1056 @@
-"""
-*******
-GraphML
-*******
-Read and write graphs in GraphML format.
-
-.. warning::
-
-    This parser uses the standard xml library present in Python, which is
-    insecure - see :doc:`library/xml` for additional information.
-    Only parse GraphML files you trust.
-
-This implementation does not support mixed graphs (directed and unidirected
-edges together), hyperedges, nested graphs, or ports.
-
-"GraphML is a comprehensive and easy-to-use file format for graphs. It
-consists of a language core to describe the structural properties of a
-graph and a flexible extension mechanism to add application-specific
-data. Its main features include support of
-
-    * directed, undirected, and mixed graphs,
-    * hypergraphs,
-    * hierarchical graphs,
-    * graphical representations,
-    * references to external data,
-    * application-specific attribute data, and
-    * light-weight parsers.
-
-Unlike many other file formats for graphs, GraphML does not use a
-custom syntax. Instead, it is based on XML and hence ideally suited as
-a common denominator for all kinds of services generating, archiving,
-or processing graphs."
-
-http://graphml.graphdrawing.org/
-
-Format
-------
-GraphML is an XML format.  See
-http://graphml.graphdrawing.org/specification.html for the specification and
-http://graphml.graphdrawing.org/primer/graphml-primer.html
-for examples.
-"""
-
-import warnings
-
-from collections import defaultdict
-
-import easygraph as eg
-
-from easygraph.utils import open_file
-from easygraph.utils.exception import EasyGraphError
-
-
-__all__ = [
-    "write_graphml",
-    "read_graphml",
-    "generate_graphml",
-    "write_graphml_xml",
-    "write_graphml_lxml",
-    "parse_graphml",
-    "GraphMLWriter",
-    "GraphMLReader",
-]
-
-
-@open_file(1, mode="wb")
-def write_graphml_xml(
-    G,
-    path,
-    encoding="utf-8",
-    prettyprint=True,
-    infer_numeric_types=False,
-    named_key_ids=False,
-    edge_id_from_attribute=None,
-):
-    """Write G in GraphML XML format to path
-
-    Parameters
-    ----------
-    G : graph
-       A easygraph graph
-    path : file or string
-       File or filename to write.
-       Filenames ending in .gz or .bz2 will be compressed.
-    encoding : string (optional)
-       Encoding for text data.
-    prettyprint : bool (optional)
-       If True use line breaks and indenting in output XML.
-    infer_numeric_types : boolean
-       Determine if numeric types should be generalized.
-       For example, if edges have both int and float 'weight' attributes,
-       we infer in GraphML that both are floats.
-    named_key_ids : bool (optional)
-       If True use attr.name as value for key elements' id attribute.
-    edge_id_from_attribute : dict key (optional)
-        If provided, the graphml edge id is set by looking up the corresponding
-        edge data attribute keyed by this parameter. If `None` or the key does not exist in edge data,
-        the edge id is set by the edge key if `G` is a MultiGraph, else the edge id is left unset.
-
-    Examples
-    --------
-    >>> G = eg.path_graph(4)
-    >>> eg.write_graphml(G, "test.graphml")
-
-    Notes
-    -----
-    This implementation does not support mixed graphs (directed
-    and unidirected edges together) hyperedges, nested graphs, or ports.
-    """
-    writer = GraphMLWriter(
-        encoding=encoding,
-        prettyprint=prettyprint,
-        infer_numeric_types=infer_numeric_types,
-        named_key_ids=named_key_ids,
-        edge_id_from_attribute=edge_id_from_attribute,
-    )
-    writer.add_graph_element(G)
-    writer.dump(path)
-
-
-@open_file(1, mode="wb")
-def write_graphml_lxml(
-    G,
-    path,
-    encoding="utf-8",
-    prettyprint=True,
-    infer_numeric_types=False,
-    named_key_ids=False,
-    edge_id_from_attribute=None,
-):
-    """Write G in GraphML XML format to path
-
-    This function uses the LXML framework and should be faster than
-    the version using the xml library.
-
-    Parameters
-    ----------
-    G : graph
-       A easygraph graph
-    path : file or string
-       File or filename to write.
-       Filenames ending in .gz or .bz2 will be compressed.
-    encoding : string (optional)
-       Encoding for text data.
-    prettyprint : bool (optional)
-       If True use line breaks and indenting in output XML.
-    infer_numeric_types : boolean
-       Determine if numeric types should be generalized.
-       For example, if edges have both int and float 'weight' attributes,
-       we infer in GraphML that both are floats.
-    named_key_ids : bool (optional)
-       If True use attr.name as value for key elements' id attribute.
-    edge_id_from_attribute : dict key (optional)
-        If provided, the graphml edge id is set by looking up the corresponding
-        edge data attribute keyed by this parameter. If `None` or the key does not exist in edge data,
-        the edge id is set by the edge key if `G` is a MultiGraph, else the edge id is left unset.
-
-    Examples
-    --------
-    >>> G = eg.path_graph(4)
-    >>> eg.write_graphml_lxml(G, "fourpath.graphml")
-
-    Notes
-    -----
-    This implementation does not support mixed graphs (directed
-    and unidirected edges together) hyperedges, nested graphs, or ports.
-    """
-    try:
-        import lxml.etree as lxmletree
-    except ImportError:
-        return write_graphml_xml(
-            G,
-            path,
-            encoding,
-            prettyprint,
-            infer_numeric_types,
-            named_key_ids,
-            edge_id_from_attribute,
-        )
-
-    writer = GraphMLWriterLxml(
-        path,
-        graph=G,
-        encoding=encoding,
-        prettyprint=prettyprint,
-        infer_numeric_types=infer_numeric_types,
-        named_key_ids=named_key_ids,
-        edge_id_from_attribute=edge_id_from_attribute,
-    )
-    writer.dump()
-
-
-def generate_graphml(
-    G,
-    encoding="utf-8",
-    prettyprint=True,
-    named_key_ids=False,
-    edge_id_from_attribute=None,
-):
-    """Generate GraphML lines for G
-
-    Parameters
-    ----------
-    G : graph
-       A easygraph graph
-    encoding : string (optional)
-       Encoding for text data.
-    prettyprint : bool (optional)
-       If True use line breaks and indenting in output XML.
-    named_key_ids : bool (optional)
-       If True use attr.name as value for key elements' id attribute.
-    edge_id_from_attribute : dict key (optional)
-        If provided, the graphml edge id is set by looking up the corresponding
-        edge data attribute keyed by this parameter. If `None` or the key does not exist in edge data,
-        the edge id is set by the edge key if `G` is a MultiGraph, else the edge id is left unset.
-
-    Examples
-    --------
-    >>> G = eg.path_graph(4)
-    >>> linefeed = chr(10)  # linefeed = \n
-    >>> s = linefeed.join(eg.generate_graphml(G))
-    >>> for line in eg.generate_graphml(G):  # doctest: +SKIP
-    ...     print(line)
-
-    Notes
-    -----
-    This implementation does not support mixed graphs (directed and unidirected
-    edges together) hyperedges, nested graphs, or ports.
-    """
-    writer = GraphMLWriter(
-        encoding=encoding,
-        prettyprint=prettyprint,
-        named_key_ids=named_key_ids,
-        edge_id_from_attribute=edge_id_from_attribute,
-    )
-    writer.add_graph_element(G)
-    yield from str(writer).splitlines()
-
-
-@open_file(0, mode="rb")
-def read_graphml(path, node_type=str, edge_key_type=int, force_multigraph=False):
-    """Read graph in GraphML format from path.
-
-    Parameters
-    ----------
-    path : file or string
-       File or filename to write.
-       Filenames ending in .gz or .bz2 will be compressed.
-
-    node_type: Python type (default: str)
-       Convert node ids to this type
-
-    edge_key_type: Python type (default: int)
-       Convert graphml edge ids to this type. Multigraphs use id as edge key.
-       Non-multigraphs add to edge attribute dict with name "id".
-
-    force_multigraph : bool (default: False)
-       If True, return a multigraph with edge keys. If False (the default)
-       return a multigraph when multiedges are in the graph.
-
-    Returns
-    -------
-    graph: EasyGraph graph
-        If parallel edges are present or `force_multigraph=True` then
-        a MultiGraph or MultiDiGraph is returned. Otherwise a Graph/DiGraph.
-        The returned graph is directed if the file indicates it should be.
-
-    Notes
-    -----
-    Default node and edge attributes are not propagated to each node and edge.
-    They can be obtained from `G.graph` and applied to node and edge attributes
-    if desired using something like this:
-
-    >>> default_color = G.graph["node_default"]["color"]  # doctest: +SKIP
-    >>> for node, data in G.nodes(data=True):  # doctest: +SKIP
-    ...     if "color" not in data:
-    ...         data["color"] = default_color
-    >>> default_color = G.graph["edge_default"]["color"]  # doctest: +SKIP
-    >>> for u, v, data in G.edges(data=True):  # doctest: +SKIP
-    ...     if "color" not in data:
-    ...         data["color"] = default_color
-
-    This implementation does not support mixed graphs (directed and unidirected
-    edges together), hypergraphs, nested graphs, or ports.
-
-    For multigraphs the GraphML edge "id" will be used as the edge
-    key.  If not specified then they "key" attribute will be used.  If
-    there is no "key" attribute a default EasyGraph multigraph edge key
-    will be provided.
-
-    Files with the yEd "yfiles" extension can be read. The type of the node's
-    shape is preserved in the `shape_type` node attribute.
-
-    yEd compressed files ("file.graphmlz" extension) can be read by renaming
-    the file to "file.graphml.gz".
-
-    """
-    reader = GraphMLReader(node_type, edge_key_type, force_multigraph)
-    # need to check for multiple graphs
-    glist = list(reader(path=path))
-    if len(glist) == 0:
-        # If no graph comes back, try looking for an incomplete header
-        header = b'<graphml xmlns="http://graphml.graphdrawing.org/xmlns">'
-        path.seek(0)
-        old_bytes = path.read()
-        new_bytes = old_bytes.replace(b"<graphml>", header)
-        glist = list(reader(string=new_bytes))
-        if len(glist) == 0:
-            raise EasyGraphError("file not successfully read as graphml")
-    return glist[0]
-
-
-def parse_graphml(
-    graphml_string, node_type=str, edge_key_type=int, force_multigraph=False
-):
-    """Read graph in GraphML format from string.
-
-    Parameters
-    ----------
-    graphml_string : string
-       String containing graphml information
-       (e.g., contents of a graphml file).
-
-    node_type: Python type (default: str)
-       Convert node ids to this type
-
-    edge_key_type: Python type (default: int)
-       Convert graphml edge ids to this type. Multigraphs use id as edge key.
-       Non-multigraphs add to edge attribute dict with name "id".
-
-    force_multigraph : bool (default: False)
-       If True, return a multigraph with edge keys. If False (the default)
-       return a multigraph when multiedges are in the graph.
-
-
-    Returns
-    -------
-    graph: EasyGraph graph
-        If no parallel edges are found a Graph or DiGraph is returned.
-        Otherwise a MultiGraph or MultiDiGraph is returned.
-
-    Examples
-    --------
-    >>> G = eg.path_graph(4)
-    >>> linefeed = chr(10)  # linefeed = \n
-    >>> s = linefeed.join(eg.generate_graphml(G))
-    >>> H = eg.parse_graphml(s)
-
-    Notes
-    -----
-    Default node and edge attributes are not propagated to each node and edge.
-    They can be obtained from `G.graph` and applied to node and edge attributes
-    if desired using something like this:
-
-    >>> default_color = G.graph["node_default"]["color"]  # doctest: +SKIP
-    >>> for node, data in G.nodes(data=True):  # doctest: +SKIP
-    ...     if "color" not in data:
-    ...         data["color"] = default_color
-    >>> default_color = G.graph["edge_default"]["color"]  # doctest: +SKIP
-    >>> for u, v, data in G.edges(data=True):  # doctest: +SKIP
-    ...     if "color" not in data:
-    ...         data["color"] = default_color
-
-    This implementation does not support mixed graphs (directed and unidirected
-    edges together), hypergraphs, nested graphs, or ports.
-
-    For multigraphs the GraphML edge "id" will be used as the edge
-    key.  If not specified then they "key" attribute will be used.  If
-    there is no "key" attribute a default EasyGraph multigraph edge key
-    will be provided.
-
-    """
-    reader = GraphMLReader(node_type, edge_key_type, force_multigraph)
-    # need to check for multiple graphs
-    glist = list(reader(string=graphml_string))
-    if len(glist) == 0:
-        # If no graph comes back, try looking for an incomplete header
-        header = '<graphml xmlns="http://graphml.graphdrawing.org/xmlns">'
-        new_string = graphml_string.replace("<graphml>", header)
-        glist = list(reader(string=new_string))
-        if len(glist) == 0:
-            raise eg.EasyGraphError("file not successfully read as graphml")
-    return glist[0]
-
-
-class GraphML:
-    NS_GRAPHML = "http://graphml.graphdrawing.org/xmlns"
-    NS_XSI = "http://www.w3.org/2001/XMLSchema-instance"
-    # xmlns:y="http://www.yworks.com/xml/graphml"
-    NS_Y = "http://www.yworks.com/xml/graphml"
-    SCHEMALOCATION = " ".join(
-        [
-            "http://graphml.graphdrawing.org/xmlns",
-            "http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd",
-        ]
-    )
-
-    def construct_types(self):
-        types = [
-            (int, "integer"),  # for Gephi GraphML bug
-            (str, "yfiles"),
-            (str, "string"),
-            (int, "int"),
-            (int, "long"),
-            (float, "float"),
-            (float, "double"),
-            (bool, "boolean"),
-        ]
-
-        # These additions to types allow writing numpy types
-        try:
-            import numpy as np
-        except:
-            pass
-        else:
-            # prepend so that python types are created upon read (last entry wins)
-            types = [
-                (np.float64, "float"),
-                (np.float32, "float"),
-                (np.float16, "float"),
-                (np.float_, "float"),
-                (np.int_, "int"),
-                (np.int8, "int"),
-                (np.int16, "int"),
-                (np.int32, "int"),
-                (np.int64, "int"),
-                (np.uint8, "int"),
-                (np.uint16, "int"),
-                (np.uint32, "int"),
-                (np.uint64, "int"),
-                (np.int_, "int"),
-                (np.intc, "int"),
-                (np.intp, "int"),
-            ] + types
-
-        self.xml_type = dict(types)
-        self.python_type = dict(reversed(a) for a in types)
-
-    # This page says that data types in GraphML follow Java(TM).
-    #  http://graphml.graphdrawing.org/primer/graphml-primer.html#AttributesDefinition
-    # true and false are the only boolean literals:
-    #  http://en.wikibooks.org/wiki/Java_Programming/Literals#Boolean_Literals
-    convert_bool = {
-        # We use data.lower() in actual use.
-        "true": True,
-        "false": False,
-        # Include integer strings for convenience.
-        "0": False,
-        0: False,
-        "1": True,
-        1: True,
-    }
-
-    def get_xml_type(self, key):
-        """Wrapper around the xml_type dict that raises a more informative
-        exception message when a user attempts to use data of a type not
-        supported by GraphML."""
-        try:
-            return self.xml_type[key]
-        except KeyError as err:
-            raise TypeError(
-                f"GraphML does not support type {type(key)} as data values."
-            ) from err
-
-
-class GraphMLWriter(GraphML):
-    def __init__(
-        self,
-        graph=None,
-        encoding="utf-8",
-        prettyprint=True,
-        infer_numeric_types=False,
-        named_key_ids=False,
-        edge_id_from_attribute=None,
-    ):
-        self.construct_types()
-        from xml.etree.ElementTree import Element
-
-        self.myElement = Element
-
-        self.infer_numeric_types = infer_numeric_types
-        self.prettyprint = prettyprint
-        self.named_key_ids = named_key_ids
-        self.edge_id_from_attribute = edge_id_from_attribute
-        self.encoding = encoding
-        self.xml = self.myElement(
-            "graphml",
-            {
-                "xmlns": self.NS_GRAPHML,
-                "xmlns:xsi": self.NS_XSI,
-                "xsi:schemaLocation": self.SCHEMALOCATION,
-            },
-        )
-        self.keys = {}
-        self.attributes = defaultdict(list)
-        self.attribute_types = defaultdict(set)
-
-        if graph is not None:
-            self.add_graph_element(graph)
-
-    def __str__(self):
-        from xml.etree.ElementTree import tostring
-
-        if self.prettyprint:
-            self.indent(self.xml)
-        s = tostring(self.xml).decode(self.encoding)
-        return s
-
-    def attr_type(self, name, scope, value):
-        """Infer the attribute type of data named name. Currently this only
-        supports inference of numeric types.
-
-        If self.infer_numeric_types is false, type is used. Otherwise, pick the
-        most general of types found across all values with name and scope. This
-        means edges with data named 'weight' are treated separately from nodes
-        with data named 'weight'.
-        """
-        if self.infer_numeric_types:
-            types = self.attribute_types[(name, scope)]
-
-            if len(types) > 1:
-                types = {self.get_xml_type(t) for t in types}
-                if "string" in types:
-                    return str
-                elif "float" in types or "double" in types:
-                    return float
-                else:
-                    return int
-            else:
-                return list(types)[0]
-        else:
-            return type(value)
-
-    def get_key(self, name, attr_type, scope, default):
-        keys_key = (name, attr_type, scope)
-        try:
-            return self.keys[keys_key]
-        except KeyError:
-            if self.named_key_ids:
-                new_id = name
-            else:
-                new_id = f"d{len(list(self.keys))}"
-
-            self.keys[keys_key] = new_id
-            key_kwargs = {
-                "id": new_id,
-                "for": scope,
-                "attr.name": name,
-                "attr.type": attr_type,
-            }
-            key_element = self.myElement("key", **key_kwargs)
-            # add subelement for data default value if present
-            if default is not None:
-                default_element = self.myElement("default")
-                default_element.text = str(default)
-                key_element.append(default_element)
-            self.xml.insert(0, key_element)
-        return new_id
-
-    def add_data(self, name, element_type, value, scope="all", default=None):
-        """
-        Make a data element for an edge or a node. Keep a log of the
-        type in the keys table.
-        """
-        if element_type not in self.xml_type:
-            raise eg.EasyGraphError(
-                f"GraphML writer does not support {element_type} as data values."
-            )
-        keyid = self.get_key(name, self.get_xml_type(element_type), scope, default)
-        data_element = self.myElement("data", key=keyid)
-        data_element.text = str(value)
-        return data_element
-
-    def add_attributes(self, scope, xml_obj, data, default):
-        """Appends attribute data to edges or nodes, and stores type information
-        to be added later. See add_graph_element.
-        """
-        for k, v in data.items():
-            self.attribute_types[(str(k), scope)].add(type(v))
-            self.attributes[xml_obj].append([k, v, scope, default.get(k)])
-
-    def add_nodes(self, G, graph_element):
-        default = G.graph.get("node_default", {})
-        for node, data in G.nodes.items():
-            node_element = self.myElement("node", id=str(node))
-            self.add_attributes("node", node_element, data, default)
-            graph_element.append(node_element)
-
-    def add_edges(self, G, graph_element):
-        if G.is_multigraph():
-            for u, v, key, data in G.edges:
-                edge_element = self.myElement(
-                    "edge",
-                    source=str(u),
-                    target=str(v),
-                    id=str(data.get(self.edge_id_from_attribute))
-                    if self.edge_id_from_attribute
-                    and self.edge_id_from_attribute in data
-                    else str(key),
-                )
-                default = G.graph.get("edge_default", {})
-                self.add_attributes("edge", edge_element, data, default)
-                graph_element.append(edge_element)
-        else:
-            for u, v, data in G.edges:
-                if self.edge_id_from_attribute and self.edge_id_from_attribute in data:
-                    # select attribute to be edge id
-                    edge_element = self.myElement(
-                        "edge",
-                        source=str(u),
-                        target=str(v),
-                        id=str(data.get(self.edge_id_from_attribute)),
-                    )
-                else:
-                    # default: no edge id
-                    edge_element = self.myElement("edge", source=str(u), target=str(v))
-                default = G.graph.get("edge_default", {})
-                self.add_attributes("edge", edge_element, data, default)
-                graph_element.append(edge_element)
-
-    def add_graph_element(self, G):
-        """
-        Serialize graph G in GraphML to the stream.
-        """
-        if G.is_directed():
-            default_edge_type = "directed"
-        else:
-            default_edge_type = "undirected"
-
-        graphid = G.graph.pop("id", None)
-        if graphid is None:
-            graph_element = self.myElement("graph", edgedefault=default_edge_type)
-        else:
-            graph_element = self.myElement(
-                "graph", edgedefault=default_edge_type, id=graphid
-            )
-        default = {}
-        data = {
-            k: v
-            for (k, v) in G.graph.items()
-            if k not in ["node_default", "edge_default"]
-        }
-        self.add_attributes("graph", graph_element, data, default)
-        self.add_nodes(G, graph_element)
-        self.add_edges(G, graph_element)
-
-        # self.attributes contains a mapping from XML Objects to a list of
-        # data that needs to be added to them.
-        # We postpone processing in order to do type inference/generalization.
-        # See self.attr_type
-        for xml_obj, data in self.attributes.items():
-            for k, v, scope, default in data:
-                xml_obj.append(
-                    self.add_data(
-                        str(k), self.attr_type(k, scope, v), str(v), scope, default
-                    )
-                )
-        self.xml.append(graph_element)
-
-    def add_graphs(self, graph_list):
-        """Add many graphs to this GraphML document."""
-        for G in graph_list:
-            self.add_graph_element(G)
-
-    def dump(self, stream):
-        from xml.etree.ElementTree import ElementTree
-
-        if self.prettyprint:
-            self.indent(self.xml)
-        document = ElementTree(self.xml)
-        document.write(stream, encoding=self.encoding, xml_declaration=True)
-
-    def indent(self, elem, level=0):
-        # in-place prettyprint formatter
-        i = "\n" + level * "  "
-        if len(elem):
-            if not elem.text or not elem.text.strip():
-                elem.text = i + "  "
-            if not elem.tail or not elem.tail.strip():
-                elem.tail = i
-            for elem in elem:
-                self.indent(elem, level + 1)
-            if not elem.tail or not elem.tail.strip():
-                elem.tail = i
-        else:
-            if level and (not elem.tail or not elem.tail.strip()):
-                elem.tail = i
-
-
-class IncrementalElement:
-    """Wrapper for _IncrementalWriter providing an Element like interface.
-
-    This wrapper does not intend to be a complete implementation but rather to
-    deal with those calls used in GraphMLWriter.
-    """
-
-    def __init__(self, xml, prettyprint):
-        self.xml = xml
-        self.prettyprint = prettyprint
-
-    def append(self, element):
-        self.xml.write(element, pretty_print=self.prettyprint)
-
-
-class GraphMLWriterLxml(GraphMLWriter):
-    def __init__(
-        self,
-        path,
-        graph=None,
-        encoding="utf-8",
-        prettyprint=True,
-        infer_numeric_types=False,
-        named_key_ids=False,
-        edge_id_from_attribute=None,
-    ):
-        self.construct_types()
-        import lxml.etree as lxmletree
-
-        self.myElement = lxmletree.Element
-
-        self._encoding = encoding
-        self._prettyprint = prettyprint
-        self.named_key_ids = named_key_ids
-        self.edge_id_from_attribute = edge_id_from_attribute
-        self.infer_numeric_types = infer_numeric_types
-
-        self._xml_base = lxmletree.xmlfile(path, encoding=encoding)
-        self._xml = self._xml_base.__enter__()
-        self._xml.write_declaration()
-
-        # We need to have a xml variable that support insertion. This call is
-        # used for adding the keys to the document.
-        # We will store those keys in a plain list, and then after the graph
-        # element is closed we will add them to the main graphml element.
-        self.xml = []
-        self._keys = self.xml
-        self._graphml = self._xml.element(
-            "graphml",
-            {
-                "xmlns": self.NS_GRAPHML,
-                "xmlns:xsi": self.NS_XSI,
-                "xsi:schemaLocation": self.SCHEMALOCATION,
-            },
-        )
-        self._graphml.__enter__()
-        self.keys = {}
-        self.attribute_types = defaultdict(set)
-
-        if graph is not None:
-            self.add_graph_element(graph)
-
-    def add_graph_element(self, G):
-        """
-        Serialize graph G in GraphML to the stream.
-        """
-        if G.is_directed():
-            default_edge_type = "directed"
-        else:
-            default_edge_type = "undirected"
-
-        graphid = G.graph.pop("id", None)
-        if graphid is None:
-            graph_element = self._xml.element("graph", edgedefault=default_edge_type)
-        else:
-            graph_element = self._xml.element(
-                "graph", edgedefault=default_edge_type, id=graphid
-            )
-
-        # gather attributes types for the whole graph
-        # to find the most general numeric format needed.
-        # Then pass through attributes to create key_id for each.
-        graphdata = {
-            k: v
-            for k, v in G.graph.items()
-            if k not in ("node_default", "edge_default")
-        }
-        node_default = G.graph.get("node_default", {})
-        edge_default = G.graph.get("edge_default", {})
-        # Graph attributes
-        for k, v in graphdata.items():
-            self.attribute_types[(str(k), "graph")].add(type(v))
-        for k, v in graphdata.items():
-            element_type = self.get_xml_type(self.attr_type(k, "graph", v))
-            self.get_key(str(k), element_type, "graph", None)
-        # Nodes and data
-        for node, d in G.nodes.items():
-            for k, v in d.items():
-                self.attribute_types[(str(k), "node")].add(type(v))
-        for node, d in G.nodes.items():
-            for k, v in d.items():
-                T = self.get_xml_type(self.attr_type(k, "node", v))
-                self.get_key(str(k), T, "node", node_default.get(k))
-        # Edges and data
-        if G.is_multigraph():
-            for u, v, ekey, d in G.edges:
-                for k, v in d.items():
-                    self.attribute_types[(str(k), "edge")].add(type(v))
-            for u, v, ekey, d in G.edges:
-                for k, v in d.items():
-                    T = self.get_xml_type(self.attr_type(k, "edge", v))
-                    self.get_key(str(k), T, "edge", edge_default.get(k))
-        else:
-            for u, v, d in G.edges:
-                for k, v in d.items():
-                    self.attribute_types[(str(k), "edge")].add(type(v))
-            for u, v, d in G.edges:
-                for k, v in d.items():
-                    T = self.get_xml_type(self.attr_type(k, "edge", v))
-                    self.get_key(str(k), T, "edge", edge_default.get(k))
-
-        # Now add attribute keys to the xml file
-        for key in self.xml:
-            self._xml.write(key, pretty_print=self._prettyprint)
-
-        # The incremental_writer writes each node/edge as it is created
-        incremental_writer = IncrementalElement(self._xml, self._prettyprint)
-        with graph_element:
-            self.add_attributes("graph", incremental_writer, graphdata, {})
-            self.add_nodes(G, incremental_writer)  # adds attributes too
-            self.add_edges(G, incremental_writer)  # adds attributes too
-
-    def add_attributes(self, scope, xml_obj, data, default):
-        """Appends attribute data."""
-        for k, v in data.items():
-            data_element = self.add_data(
-                str(k), self.attr_type(str(k), scope, v), str(v), scope, default.get(k)
-            )
-            xml_obj.append(data_element)
-
-    def __str__(self):
-        return object.__str__(self)
-
-    def dump(self):
-        self._graphml.__exit__(None, None, None)
-        self._xml_base.__exit__(None, None, None)
-
-
-# default is lxml is present.
-write_graphml = write_graphml_lxml
-
-
-class GraphMLReader(GraphML):
-    """Read a GraphML document.  Produces EasyGraph graph objects."""
-
-    def __init__(self, node_type=str, edge_key_type=int, force_multigraph=False):
-        self.construct_types()
-        self.node_type = node_type
-        self.edge_key_type = edge_key_type
-        self.multigraph = force_multigraph  # If False, test for multiedges
-        self.edge_ids = {}  # dict mapping (u,v) tuples to edge id attributes
-
-    def __call__(self, path=None, string=None):
-        from xml.etree.ElementTree import ElementTree
-        from xml.etree.ElementTree import fromstring
-
-        if path is not None:
-            self.xml = ElementTree(file=path)
-        elif string is not None:
-            self.xml = fromstring(string)
-        else:
-            raise ValueError("Must specify either 'path' or 'string' as kwarg")
-        (keys, defaults) = self.find_graphml_keys(self.xml)
-        for g in self.xml.findall(f"{{{self.NS_GRAPHML}}}graph"):
-            yield self.make_graph(g, keys, defaults)
-
-    def make_graph(self, graph_xml, graphml_keys, defaults, G=None):
-        # set default graph type
-        edgedefault = graph_xml.get("edgedefault", None)
-        if G is None:
-            if edgedefault == "directed":
-                G = eg.MultiDiGraph()
-            else:
-                G = eg.MultiGraph()
-        # set defaults for graph attributes
-        G.graph["node_default"] = {}
-        G.graph["edge_default"] = {}
-        for key_id, value in defaults.items():
-            key_for = graphml_keys[key_id]["for"]
-            name = graphml_keys[key_id]["name"]
-            python_type = graphml_keys[key_id]["type"]
-            if key_for == "node":
-                G.graph["node_default"].update({name: python_type(value)})
-            if key_for == "edge":
-                G.graph["edge_default"].update({name: python_type(value)})
-        # hyperedges are not supported
-        hyperedge = graph_xml.find(f"{{{self.NS_GRAPHML}}}hyperedge")
-        if hyperedge is not None:
-            raise eg.EasyGraphError("GraphML reader doesn't support hyperedges")
-        # add nodes
-        for node_xml in graph_xml.findall(f"{{{self.NS_GRAPHML}}}node"):
-            self.add_node(G, node_xml, graphml_keys, defaults)
-        # add edges
-        for edge_xml in graph_xml.findall(f"{{{self.NS_GRAPHML}}}edge"):
-            self.add_edge(G, edge_xml, graphml_keys)
-        # add graph data
-        data = self.decode_data_elements(graphml_keys, graph_xml)
-        G.graph.update(data)
-
-        # switch to Graph or DiGraph if no parallel edges were found
-        if self.multigraph:
-            return G
-
-        G = eg.DiGraph(G) if G.is_directed() else eg.Graph(G)
-        # add explicit edge "id" from file as attribute in eg graph.
-        eg.set_edge_attributes(G, values=self.edge_ids, name="id")
-        return G
-
-    def add_node(self, G, node_xml, graphml_keys, defaults):
-        """Add a node to the graph."""
-        # warn on finding unsupported ports tag
-        ports = node_xml.find(f"{{{self.NS_GRAPHML}}}port")
-        if ports is not None:
-            warnings.warn("GraphML port tag not supported.")
-        # find the node by id and cast it to the appropriate type
-        node_id = self.node_type(node_xml.get("id"))
-        # get data/attributes for node
-        data = self.decode_data_elements(graphml_keys, node_xml)
-        G.add_node(node_id, **data)
-        # get child nodes
-        if node_xml.attrib.get("yfiles.foldertype") == "group":
-            graph_xml = node_xml.find(f"{{{self.NS_GRAPHML}}}graph")
-            self.make_graph(graph_xml, graphml_keys, defaults, G)
-
-    def add_edge(self, G, edge_element, graphml_keys):
-        """Add an edge to the graph."""
-        # warn on finding unsupported ports tag
-        ports = edge_element.find(f"{{{self.NS_GRAPHML}}}port")
-        if ports is not None:
-            warnings.warn("GraphML port tag not supported.")
-
-        # raise error if we find mixed directed and undirected edges
-        directed = edge_element.get("directed")
-        if G.is_directed() and directed == "false":
-            msg = "directed=false edge found in directed graph."
-            raise eg.EasyGraphError(msg)
-        if (not G.is_directed()) and directed == "true":
-            msg = "directed=true edge found in undi rected graph."
-            raise eg.EasyGraphError(msg)
-
-        source = self.node_type(edge_element.get("source"))
-        target = self.node_type(edge_element.get("target"))
-        data = self.decode_data_elements(graphml_keys, edge_element)
-        # GraphML stores edge ids as an attribute
-        # EasyGraph uses them as keys in multigraphs too if no key
-        # attribute is specified
-        edge_id = edge_element.get("id")
-        if edge_id:
-            # self.edge_ids is used by `make_graph` method for non-multigraphs
-            self.edge_ids[source, target] = edge_id
-            try:
-                edge_id = self.edge_key_type(edge_id)
-            except ValueError:  # Could not convert.
-                pass
-        else:
-            edge_id = data.get("key")
-
-        if G.has_edge(source, target):
-            # mark this as a multigraph
-            self.multigraph = True
-
-        # Use add_edges_from to avoid error with add_edge when `'key' in data`
-        # Note there is only one edge here...
-        G.add_edges_from([(source, target, edge_id, data)])
-
-    def decode_data_elements(self, graphml_keys, obj_xml):
-        """Use the key information to decode the data XML if present."""
-        data = {}
-        for data_element in obj_xml.findall(f"{{{self.NS_GRAPHML}}}data"):
-            key = data_element.get("key")
-            try:
-                data_name = graphml_keys[key]["name"]
-                data_type = graphml_keys[key]["type"]
-            except KeyError as err:
-                raise eg.EasyGraphError(f"Bad GraphML data: no key {key}") from err
-            text = data_element.text
-            # assume anything with subelements is a yfiles extension
-            if text is not None and len(list(data_element)) == 0:
-                if data_type == bool:
-                    # Ignore cases.
-                    # http://docs.oracle.com/javase/6/docs/api/java/lang/
-                    # Boolean.html#parseBoolean%28java.lang.String%29
-                    data[data_name] = self.convert_bool[text.lower()]
-                else:
-                    data[data_name] = data_type(text)
-            elif len(list(data_element)) > 0:
-                # Assume yfiles as subelements, try to extract node_label
-                node_label = None
-                # set GenericNode's configuration as shape type
-                gn = data_element.find(f"{{{self.NS_Y}}}GenericNode")
-                if gn:
-                    data["shape_type"] = gn.get("configuration")
-                for node_type in ["GenericNode", "ShapeNode", "SVGNode", "ImageNode"]:
-                    pref = f"{{{self.NS_Y}}}{node_type}/{{{self.NS_Y}}}"
-                    geometry = data_element.find(f"{pref}Geometry")
-                    if geometry is not None:
-                        data["x"] = geometry.get("x")
-                        data["y"] = geometry.get("y")
-                    if node_label is None:
-                        node_label = data_element.find(f"{pref}NodeLabel")
-                    shape = data_element.find(f"{pref}Shape")
-                    if shape is not None:
-                        data["shape_type"] = shape.get("type")
-                if node_label is not None:
-                    data["label"] = node_label.text
-
-                # check all the different types of edges avaivable in yEd.
-                for edge_type in [
-                    "PolyLineEdge",
-                    "SplineEdge",
-                    "QuadCurveEdge",
-                    "BezierEdge",
-                    "ArcEdge",
-                ]:
-                    pref = f"{{{self.NS_Y}}}{edge_type}/{{{self.NS_Y}}}"
-                    edge_label = data_element.find(f"{pref}EdgeLabel")
-                    if edge_label is not None:
-                        break
-
-                if edge_label is not None:
-                    data["label"] = edge_label.text
-        return data
-
-    def find_graphml_keys(self, graph_element):
-        """Extracts all the keys and key defaults from the xml."""
-        graphml_keys = {}
-        graphml_key_defaults = {}
-        for k in graph_element.findall(f"{{{self.NS_GRAPHML}}}key"):
-            attr_id = k.get("id")
-            attr_type = k.get("attr.type")
-            attr_name = k.get("attr.name")
-            yfiles_type = k.get("yfiles.type")
-            if yfiles_type is not None:
-                attr_name = yfiles_type
-                attr_type = "yfiles"
-            if attr_type is None:
-                attr_type = "string"
-                warnings.warn(f"No key type for id {attr_id}. Using string")
-            if attr_name is None:
-                raise eg.EasyGraphError(f"Unknown key for id {attr_id}.")
-            graphml_keys[attr_id] = {
-                "name": attr_name,
-                "type": self.python_type[attr_type],
-                "for": k.get("for"),
-            }
-            # check for "default" sub-element of key element
-            default = k.find(f"{{{self.NS_GRAPHML}}}default")
-            if default is not None:
-                # Handle default values identically to data element values
-                python_type = graphml_keys[attr_id]["type"]
-                if python_type == bool:
-                    graphml_key_defaults[attr_id] = self.convert_bool[
-                        default.text.lower()
-                    ]
-                else:
-                    graphml_key_defaults[attr_id] = python_type(default.text)
-        return graphml_keys, graphml_key_defaults
+"""
+*******
+GraphML
+*******
+Read and write graphs in GraphML format.
+
+.. warning::
+
+    This parser uses the standard xml library present in Python, which is
+    insecure - see :doc:`library/xml` for additional information.
+    Only parse GraphML files you trust.
+
+This implementation does not support mixed graphs (directed and unidirected
+edges together), hyperedges, nested graphs, or ports.
+
+"GraphML is a comprehensive and easy-to-use file format for graphs. It
+consists of a language core to describe the structural properties of a
+graph and a flexible extension mechanism to add application-specific
+data. Its main features include support of
+
+    * directed, undirected, and mixed graphs,
+    * hypergraphs,
+    * hierarchical graphs,
+    * graphical representations,
+    * references to external data,
+    * application-specific attribute data, and
+    * light-weight parsers.
+
+Unlike many other file formats for graphs, GraphML does not use a
+custom syntax. Instead, it is based on XML and hence ideally suited as
+a common denominator for all kinds of services generating, archiving,
+or processing graphs."
+
+http://graphml.graphdrawing.org/
+
+Format
+------
+GraphML is an XML format.  See
+http://graphml.graphdrawing.org/specification.html for the specification and
+http://graphml.graphdrawing.org/primer/graphml-primer.html
+for examples.
+"""
+
+import warnings
+
+from collections import defaultdict
+
+import easygraph as eg
+
+from easygraph.utils import open_file
+from easygraph.utils.exception import EasyGraphError
+
+
+__all__ = [
+    "write_graphml",
+    "read_graphml",
+    "generate_graphml",
+    "write_graphml_xml",
+    "write_graphml_lxml",
+    "parse_graphml",
+    "GraphMLWriter",
+    "GraphMLReader",
+]
+
+
+@open_file(1, mode="wb")
+def write_graphml_xml(
+    G,
+    path,
+    encoding="utf-8",
+    prettyprint=True,
+    infer_numeric_types=False,
+    named_key_ids=False,
+    edge_id_from_attribute=None,
+):
+    """Write G in GraphML XML format to path
+
+    Parameters
+    ----------
+    G : graph
+       A easygraph graph
+    path : file or string
+       File or filename to write.
+       Filenames ending in .gz or .bz2 will be compressed.
+    encoding : string (optional)
+       Encoding for text data.
+    prettyprint : bool (optional)
+       If True use line breaks and indenting in output XML.
+    infer_numeric_types : boolean
+       Determine if numeric types should be generalized.
+       For example, if edges have both int and float 'weight' attributes,
+       we infer in GraphML that both are floats.
+    named_key_ids : bool (optional)
+       If True use attr.name as value for key elements' id attribute.
+    edge_id_from_attribute : dict key (optional)
+        If provided, the graphml edge id is set by looking up the corresponding
+        edge data attribute keyed by this parameter. If `None` or the key does not exist in edge data,
+        the edge id is set by the edge key if `G` is a MultiGraph, else the edge id is left unset.
+
+    Examples
+    --------
+    >>> G = eg.path_graph(4)
+    >>> eg.write_graphml(G, "test.graphml")
+
+    Notes
+    -----
+    This implementation does not support mixed graphs (directed
+    and unidirected edges together) hyperedges, nested graphs, or ports.
+    """
+    writer = GraphMLWriter(
+        encoding=encoding,
+        prettyprint=prettyprint,
+        infer_numeric_types=infer_numeric_types,
+        named_key_ids=named_key_ids,
+        edge_id_from_attribute=edge_id_from_attribute,
+    )
+    writer.add_graph_element(G)
+    writer.dump(path)
+
+
+@open_file(1, mode="wb")
+def write_graphml_lxml(
+    G,
+    path,
+    encoding="utf-8",
+    prettyprint=True,
+    infer_numeric_types=False,
+    named_key_ids=False,
+    edge_id_from_attribute=None,
+):
+    """Write G in GraphML XML format to path
+
+    This function uses the LXML framework and should be faster than
+    the version using the xml library.
+
+    Parameters
+    ----------
+    G : graph
+       A easygraph graph
+    path : file or string
+       File or filename to write.
+       Filenames ending in .gz or .bz2 will be compressed.
+    encoding : string (optional)
+       Encoding for text data.
+    prettyprint : bool (optional)
+       If True use line breaks and indenting in output XML.
+    infer_numeric_types : boolean
+       Determine if numeric types should be generalized.
+       For example, if edges have both int and float 'weight' attributes,
+       we infer in GraphML that both are floats.
+    named_key_ids : bool (optional)
+       If True use attr.name as value for key elements' id attribute.
+    edge_id_from_attribute : dict key (optional)
+        If provided, the graphml edge id is set by looking up the corresponding
+        edge data attribute keyed by this parameter. If `None` or the key does not exist in edge data,
+        the edge id is set by the edge key if `G` is a MultiGraph, else the edge id is left unset.
+
+    Examples
+    --------
+    >>> G = eg.path_graph(4)
+    >>> eg.write_graphml_lxml(G, "fourpath.graphml")
+
+    Notes
+    -----
+    This implementation does not support mixed graphs (directed
+    and unidirected edges together) hyperedges, nested graphs, or ports.
+    """
+    try:
+        import lxml.etree as lxmletree
+    except ImportError:
+        return write_graphml_xml(
+            G,
+            path,
+            encoding,
+            prettyprint,
+            infer_numeric_types,
+            named_key_ids,
+            edge_id_from_attribute,
+        )
+
+    writer = GraphMLWriterLxml(
+        path,
+        graph=G,
+        encoding=encoding,
+        prettyprint=prettyprint,
+        infer_numeric_types=infer_numeric_types,
+        named_key_ids=named_key_ids,
+        edge_id_from_attribute=edge_id_from_attribute,
+    )
+    writer.dump()
+
+
+def generate_graphml(
+    G,
+    encoding="utf-8",
+    prettyprint=True,
+    named_key_ids=False,
+    edge_id_from_attribute=None,
+):
+    """Generate GraphML lines for G
+
+    Parameters
+    ----------
+    G : graph
+       A easygraph graph
+    encoding : string (optional)
+       Encoding for text data.
+    prettyprint : bool (optional)
+       If True use line breaks and indenting in output XML.
+    named_key_ids : bool (optional)
+       If True use attr.name as value for key elements' id attribute.
+    edge_id_from_attribute : dict key (optional)
+        If provided, the graphml edge id is set by looking up the corresponding
+        edge data attribute keyed by this parameter. If `None` or the key does not exist in edge data,
+        the edge id is set by the edge key if `G` is a MultiGraph, else the edge id is left unset.
+
+    Examples
+    --------
+    >>> G = eg.path_graph(4)
+    >>> linefeed = chr(10)  # linefeed = \n
+    >>> s = linefeed.join(eg.generate_graphml(G))
+    >>> for line in eg.generate_graphml(G):  # doctest: +SKIP
+    ...     print(line)
+
+    Notes
+    -----
+    This implementation does not support mixed graphs (directed and unidirected
+    edges together) hyperedges, nested graphs, or ports.
+    """
+    writer = GraphMLWriter(
+        encoding=encoding,
+        prettyprint=prettyprint,
+        named_key_ids=named_key_ids,
+        edge_id_from_attribute=edge_id_from_attribute,
+    )
+    writer.add_graph_element(G)
+    yield from str(writer).splitlines()
+
+
+@open_file(0, mode="rb")
+def read_graphml(path, node_type=str, edge_key_type=int, force_multigraph=False):
+    """Read graph in GraphML format from path.
+
+    Parameters
+    ----------
+    path : file or string
+       File or filename to write.
+       Filenames ending in .gz or .bz2 will be compressed.
+
+    node_type: Python type (default: str)
+       Convert node ids to this type
+
+    edge_key_type: Python type (default: int)
+       Convert graphml edge ids to this type. Multigraphs use id as edge key.
+       Non-multigraphs add to edge attribute dict with name "id".
+
+    force_multigraph : bool (default: False)
+       If True, return a multigraph with edge keys. If False (the default)
+       return a multigraph when multiedges are in the graph.
+
+    Returns
+    -------
+    graph: EasyGraph graph
+        If parallel edges are present or `force_multigraph=True` then
+        a MultiGraph or MultiDiGraph is returned. Otherwise a Graph/DiGraph.
+        The returned graph is directed if the file indicates it should be.
+
+    Notes
+    -----
+    Default node and edge attributes are not propagated to each node and edge.
+    They can be obtained from `G.graph` and applied to node and edge attributes
+    if desired using something like this:
+
+    >>> default_color = G.graph["node_default"]["color"]  # doctest: +SKIP
+    >>> for node, data in G.nodes(data=True):  # doctest: +SKIP
+    ...     if "color" not in data:
+    ...         data["color"] = default_color
+    >>> default_color = G.graph["edge_default"]["color"]  # doctest: +SKIP
+    >>> for u, v, data in G.edges(data=True):  # doctest: +SKIP
+    ...     if "color" not in data:
+    ...         data["color"] = default_color
+
+    This implementation does not support mixed graphs (directed and unidirected
+    edges together), hypergraphs, nested graphs, or ports.
+
+    For multigraphs the GraphML edge "id" will be used as the edge
+    key.  If not specified then they "key" attribute will be used.  If
+    there is no "key" attribute a default EasyGraph multigraph edge key
+    will be provided.
+
+    Files with the yEd "yfiles" extension can be read. The type of the node's
+    shape is preserved in the `shape_type` node attribute.
+
+    yEd compressed files ("file.graphmlz" extension) can be read by renaming
+    the file to "file.graphml.gz".
+
+    """
+    reader = GraphMLReader(node_type, edge_key_type, force_multigraph)
+    # need to check for multiple graphs
+    glist = list(reader(path=path))
+    if len(glist) == 0:
+        # If no graph comes back, try looking for an incomplete header
+        header = b'<graphml xmlns="http://graphml.graphdrawing.org/xmlns">'
+        path.seek(0)
+        old_bytes = path.read()
+        new_bytes = old_bytes.replace(b"<graphml>", header)
+        glist = list(reader(string=new_bytes))
+        if len(glist) == 0:
+            raise EasyGraphError("file not successfully read as graphml")
+    return glist[0]
+
+
+def parse_graphml(
+    graphml_string, node_type=str, edge_key_type=int, force_multigraph=False
+):
+    """Read graph in GraphML format from string.
+
+    Parameters
+    ----------
+    graphml_string : string
+       String containing graphml information
+       (e.g., contents of a graphml file).
+
+    node_type: Python type (default: str)
+       Convert node ids to this type
+
+    edge_key_type: Python type (default: int)
+       Convert graphml edge ids to this type. Multigraphs use id as edge key.
+       Non-multigraphs add to edge attribute dict with name "id".
+
+    force_multigraph : bool (default: False)
+       If True, return a multigraph with edge keys. If False (the default)
+       return a multigraph when multiedges are in the graph.
+
+
+    Returns
+    -------
+    graph: EasyGraph graph
+        If no parallel edges are found a Graph or DiGraph is returned.
+        Otherwise a MultiGraph or MultiDiGraph is returned.
+
+    Examples
+    --------
+    >>> G = eg.path_graph(4)
+    >>> linefeed = chr(10)  # linefeed = \n
+    >>> s = linefeed.join(eg.generate_graphml(G))
+    >>> H = eg.parse_graphml(s)
+
+    Notes
+    -----
+    Default node and edge attributes are not propagated to each node and edge.
+    They can be obtained from `G.graph` and applied to node and edge attributes
+    if desired using something like this:
+
+    >>> default_color = G.graph["node_default"]["color"]  # doctest: +SKIP
+    >>> for node, data in G.nodes(data=True):  # doctest: +SKIP
+    ...     if "color" not in data:
+    ...         data["color"] = default_color
+    >>> default_color = G.graph["edge_default"]["color"]  # doctest: +SKIP
+    >>> for u, v, data in G.edges(data=True):  # doctest: +SKIP
+    ...     if "color" not in data:
+    ...         data["color"] = default_color
+
+    This implementation does not support mixed graphs (directed and unidirected
+    edges together), hypergraphs, nested graphs, or ports.
+
+    For multigraphs the GraphML edge "id" will be used as the edge
+    key.  If not specified then they "key" attribute will be used.  If
+    there is no "key" attribute a default EasyGraph multigraph edge key
+    will be provided.
+
+    """
+    reader = GraphMLReader(node_type, edge_key_type, force_multigraph)
+    # need to check for multiple graphs
+    glist = list(reader(string=graphml_string))
+    if len(glist) == 0:
+        # If no graph comes back, try looking for an incomplete header
+        header = '<graphml xmlns="http://graphml.graphdrawing.org/xmlns">'
+        new_string = graphml_string.replace("<graphml>", header)
+        glist = list(reader(string=new_string))
+        if len(glist) == 0:
+            raise eg.EasyGraphError("file not successfully read as graphml")
+    return glist[0]
+
+
+class GraphML:
+    NS_GRAPHML = "http://graphml.graphdrawing.org/xmlns"
+    NS_XSI = "http://www.w3.org/2001/XMLSchema-instance"
+    # xmlns:y="http://www.yworks.com/xml/graphml"
+    NS_Y = "http://www.yworks.com/xml/graphml"
+    SCHEMALOCATION = " ".join(
+        [
+            "http://graphml.graphdrawing.org/xmlns",
+            "http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd",
+        ]
+    )
+
+    def construct_types(self):
+        types = [
+            (int, "integer"),  # for Gephi GraphML bug
+            (str, "yfiles"),
+            (str, "string"),
+            (int, "int"),
+            (int, "long"),
+            (float, "float"),
+            (float, "double"),
+            (bool, "boolean"),
+        ]
+
+        # These additions to types allow writing numpy types
+        try:
+            import numpy as np
+        except:
+            pass
+        else:
+            # prepend so that python types are created upon read (last entry wins)
+            types = [
+                (np.float64, "float"),
+                (np.float32, "float"),
+                (np.float16, "float"),
+                (np.float_, "float"),
+                (np.int_, "int"),
+                (np.int8, "int"),
+                (np.int16, "int"),
+                (np.int32, "int"),
+                (np.int64, "int"),
+                (np.uint8, "int"),
+                (np.uint16, "int"),
+                (np.uint32, "int"),
+                (np.uint64, "int"),
+                (np.int_, "int"),
+                (np.intc, "int"),
+                (np.intp, "int"),
+            ] + types
+
+        self.xml_type = dict(types)
+        self.python_type = dict(reversed(a) for a in types)
+
+    # This page says that data types in GraphML follow Java(TM).
+    #  http://graphml.graphdrawing.org/primer/graphml-primer.html#AttributesDefinition
+    # true and false are the only boolean literals:
+    #  http://en.wikibooks.org/wiki/Java_Programming/Literals#Boolean_Literals
+    convert_bool = {
+        # We use data.lower() in actual use.
+        "true": True,
+        "false": False,
+        # Include integer strings for convenience.
+        "0": False,
+        0: False,
+        "1": True,
+        1: True,
+    }
+
+    def get_xml_type(self, key):
+        """Wrapper around the xml_type dict that raises a more informative
+        exception message when a user attempts to use data of a type not
+        supported by GraphML."""
+        try:
+            return self.xml_type[key]
+        except KeyError as err:
+            raise TypeError(
+                f"GraphML does not support type {type(key)} as data values."
+            ) from err
+
+
+class GraphMLWriter(GraphML):
+    def __init__(
+        self,
+        graph=None,
+        encoding="utf-8",
+        prettyprint=True,
+        infer_numeric_types=False,
+        named_key_ids=False,
+        edge_id_from_attribute=None,
+    ):
+        self.construct_types()
+        from xml.etree.ElementTree import Element
+
+        self.myElement = Element
+
+        self.infer_numeric_types = infer_numeric_types
+        self.prettyprint = prettyprint
+        self.named_key_ids = named_key_ids
+        self.edge_id_from_attribute = edge_id_from_attribute
+        self.encoding = encoding
+        self.xml = self.myElement(
+            "graphml",
+            {
+                "xmlns": self.NS_GRAPHML,
+                "xmlns:xsi": self.NS_XSI,
+                "xsi:schemaLocation": self.SCHEMALOCATION,
+            },
+        )
+        self.keys = {}
+        self.attributes = defaultdict(list)
+        self.attribute_types = defaultdict(set)
+
+        if graph is not None:
+            self.add_graph_element(graph)
+
+    def __str__(self):
+        from xml.etree.ElementTree import tostring
+
+        if self.prettyprint:
+            self.indent(self.xml)
+        s = tostring(self.xml).decode(self.encoding)
+        return s
+
+    def attr_type(self, name, scope, value):
+        """Infer the attribute type of data named name. Currently this only
+        supports inference of numeric types.
+
+        If self.infer_numeric_types is false, type is used. Otherwise, pick the
+        most general of types found across all values with name and scope. This
+        means edges with data named 'weight' are treated separately from nodes
+        with data named 'weight'.
+        """
+        if self.infer_numeric_types:
+            types = self.attribute_types[(name, scope)]
+
+            if len(types) > 1:
+                types = {self.get_xml_type(t) for t in types}
+                if "string" in types:
+                    return str
+                elif "float" in types or "double" in types:
+                    return float
+                else:
+                    return int
+            else:
+                return list(types)[0]
+        else:
+            return type(value)
+
+    def get_key(self, name, attr_type, scope, default):
+        keys_key = (name, attr_type, scope)
+        try:
+            return self.keys[keys_key]
+        except KeyError:
+            if self.named_key_ids:
+                new_id = name
+            else:
+                new_id = f"d{len(list(self.keys))}"
+
+            self.keys[keys_key] = new_id
+            key_kwargs = {
+                "id": new_id,
+                "for": scope,
+                "attr.name": name,
+                "attr.type": attr_type,
+            }
+            key_element = self.myElement("key", **key_kwargs)
+            # add subelement for data default value if present
+            if default is not None:
+                default_element = self.myElement("default")
+                default_element.text = str(default)
+                key_element.append(default_element)
+            self.xml.insert(0, key_element)
+        return new_id
+
+    def add_data(self, name, element_type, value, scope="all", default=None):
+        """
+        Make a data element for an edge or a node. Keep a log of the
+        type in the keys table.
+        """
+        if element_type not in self.xml_type:
+            raise eg.EasyGraphError(
+                f"GraphML writer does not support {element_type} as data values."
+            )
+        keyid = self.get_key(name, self.get_xml_type(element_type), scope, default)
+        data_element = self.myElement("data", key=keyid)
+        data_element.text = str(value)
+        return data_element
+
+    def add_attributes(self, scope, xml_obj, data, default):
+        """Appends attribute data to edges or nodes, and stores type information
+        to be added later. See add_graph_element.
+        """
+        for k, v in data.items():
+            self.attribute_types[(str(k), scope)].add(type(v))
+            self.attributes[xml_obj].append([k, v, scope, default.get(k)])
+
+    def add_nodes(self, G, graph_element):
+        default = G.graph.get("node_default", {})
+        for node, data in G.nodes.items():
+            node_element = self.myElement("node", id=str(node))
+            self.add_attributes("node", node_element, data, default)
+            graph_element.append(node_element)
+
+    def add_edges(self, G, graph_element):
+        if G.is_multigraph():
+            for u, v, key, data in G.edges:
+                edge_element = self.myElement(
+                    "edge",
+                    source=str(u),
+                    target=str(v),
+                    id=str(data.get(self.edge_id_from_attribute))
+                    if self.edge_id_from_attribute
+                    and self.edge_id_from_attribute in data
+                    else str(key),
+                )
+                default = G.graph.get("edge_default", {})
+                self.add_attributes("edge", edge_element, data, default)
+                graph_element.append(edge_element)
+        else:
+            for u, v, data in G.edges:
+                if self.edge_id_from_attribute and self.edge_id_from_attribute in data:
+                    # select attribute to be edge id
+                    edge_element = self.myElement(
+                        "edge",
+                        source=str(u),
+                        target=str(v),
+                        id=str(data.get(self.edge_id_from_attribute)),
+                    )
+                else:
+                    # default: no edge id
+                    edge_element = self.myElement("edge", source=str(u), target=str(v))
+                default = G.graph.get("edge_default", {})
+                self.add_attributes("edge", edge_element, data, default)
+                graph_element.append(edge_element)
+
+    def add_graph_element(self, G):
+        """
+        Serialize graph G in GraphML to the stream.
+        """
+        if G.is_directed():
+            default_edge_type = "directed"
+        else:
+            default_edge_type = "undirected"
+
+        graphid = G.graph.pop("id", None)
+        if graphid is None:
+            graph_element = self.myElement("graph", edgedefault=default_edge_type)
+        else:
+            graph_element = self.myElement(
+                "graph", edgedefault=default_edge_type, id=graphid
+            )
+        default = {}
+        data = {
+            k: v
+            for (k, v) in G.graph.items()
+            if k not in ["node_default", "edge_default"]
+        }
+        self.add_attributes("graph", graph_element, data, default)
+        self.add_nodes(G, graph_element)
+        self.add_edges(G, graph_element)
+
+        # self.attributes contains a mapping from XML Objects to a list of
+        # data that needs to be added to them.
+        # We postpone processing in order to do type inference/generalization.
+        # See self.attr_type
+        for xml_obj, data in self.attributes.items():
+            for k, v, scope, default in data:
+                xml_obj.append(
+                    self.add_data(
+                        str(k), self.attr_type(k, scope, v), str(v), scope, default
+                    )
+                )
+        self.xml.append(graph_element)
+
+    def add_graphs(self, graph_list):
+        """Add many graphs to this GraphML document."""
+        for G in graph_list:
+            self.add_graph_element(G)
+
+    def dump(self, stream):
+        from xml.etree.ElementTree import ElementTree
+
+        if self.prettyprint:
+            self.indent(self.xml)
+        document = ElementTree(self.xml)
+        document.write(stream, encoding=self.encoding, xml_declaration=True)
+
+    def indent(self, elem, level=0):
+        # in-place prettyprint formatter
+        i = "\n" + level * "  "
+        if len(elem):
+            if not elem.text or not elem.text.strip():
+                elem.text = i + "  "
+            if not elem.tail or not elem.tail.strip():
+                elem.tail = i
+            for elem in elem:
+                self.indent(elem, level + 1)
+            if not elem.tail or not elem.tail.strip():
+                elem.tail = i
+        else:
+            if level and (not elem.tail or not elem.tail.strip()):
+                elem.tail = i
+
+
+class IncrementalElement:
+    """Wrapper for _IncrementalWriter providing an Element like interface.
+
+    This wrapper does not intend to be a complete implementation but rather to
+    deal with those calls used in GraphMLWriter.
+    """
+
+    def __init__(self, xml, prettyprint):
+        self.xml = xml
+        self.prettyprint = prettyprint
+
+    def append(self, element):
+        self.xml.write(element, pretty_print=self.prettyprint)
+
+
+class GraphMLWriterLxml(GraphMLWriter):
+    def __init__(
+        self,
+        path,
+        graph=None,
+        encoding="utf-8",
+        prettyprint=True,
+        infer_numeric_types=False,
+        named_key_ids=False,
+        edge_id_from_attribute=None,
+    ):
+        self.construct_types()
+        import lxml.etree as lxmletree
+
+        self.myElement = lxmletree.Element
+
+        self._encoding = encoding
+        self._prettyprint = prettyprint
+        self.named_key_ids = named_key_ids
+        self.edge_id_from_attribute = edge_id_from_attribute
+        self.infer_numeric_types = infer_numeric_types
+
+        self._xml_base = lxmletree.xmlfile(path, encoding=encoding)
+        self._xml = self._xml_base.__enter__()
+        self._xml.write_declaration()
+
+        # We need to have a xml variable that support insertion. This call is
+        # used for adding the keys to the document.
+        # We will store those keys in a plain list, and then after the graph
+        # element is closed we will add them to the main graphml element.
+        self.xml = []
+        self._keys = self.xml
+        self._graphml = self._xml.element(
+            "graphml",
+            {
+                "xmlns": self.NS_GRAPHML,
+                "xmlns:xsi": self.NS_XSI,
+                "xsi:schemaLocation": self.SCHEMALOCATION,
+            },
+        )
+        self._graphml.__enter__()
+        self.keys = {}
+        self.attribute_types = defaultdict(set)
+
+        if graph is not None:
+            self.add_graph_element(graph)
+
+    def add_graph_element(self, G):
+        """
+        Serialize graph G in GraphML to the stream.
+        """
+        if G.is_directed():
+            default_edge_type = "directed"
+        else:
+            default_edge_type = "undirected"
+
+        graphid = G.graph.pop("id", None)
+        if graphid is None:
+            graph_element = self._xml.element("graph", edgedefault=default_edge_type)
+        else:
+            graph_element = self._xml.element(
+                "graph", edgedefault=default_edge_type, id=graphid
+            )
+
+        # gather attributes types for the whole graph
+        # to find the most general numeric format needed.
+        # Then pass through attributes to create key_id for each.
+        graphdata = {
+            k: v
+            for k, v in G.graph.items()
+            if k not in ("node_default", "edge_default")
+        }
+        node_default = G.graph.get("node_default", {})
+        edge_default = G.graph.get("edge_default", {})
+        # Graph attributes
+        for k, v in graphdata.items():
+            self.attribute_types[(str(k), "graph")].add(type(v))
+        for k, v in graphdata.items():
+            element_type = self.get_xml_type(self.attr_type(k, "graph", v))
+            self.get_key(str(k), element_type, "graph", None)
+        # Nodes and data
+        for node, d in G.nodes.items():
+            for k, v in d.items():
+                self.attribute_types[(str(k), "node")].add(type(v))
+        for node, d in G.nodes.items():
+            for k, v in d.items():
+                T = self.get_xml_type(self.attr_type(k, "node", v))
+                self.get_key(str(k), T, "node", node_default.get(k))
+        # Edges and data
+        if G.is_multigraph():
+            for u, v, ekey, d in G.edges:
+                for k, v in d.items():
+                    self.attribute_types[(str(k), "edge")].add(type(v))
+            for u, v, ekey, d in G.edges:
+                for k, v in d.items():
+                    T = self.get_xml_type(self.attr_type(k, "edge", v))
+                    self.get_key(str(k), T, "edge", edge_default.get(k))
+        else:
+            for u, v, d in G.edges:
+                for k, v in d.items():
+                    self.attribute_types[(str(k), "edge")].add(type(v))
+            for u, v, d in G.edges:
+                for k, v in d.items():
+                    T = self.get_xml_type(self.attr_type(k, "edge", v))
+                    self.get_key(str(k), T, "edge", edge_default.get(k))
+
+        # Now add attribute keys to the xml file
+        for key in self.xml:
+            self._xml.write(key, pretty_print=self._prettyprint)
+
+        # The incremental_writer writes each node/edge as it is created
+        incremental_writer = IncrementalElement(self._xml, self._prettyprint)
+        with graph_element:
+            self.add_attributes("graph", incremental_writer, graphdata, {})
+            self.add_nodes(G, incremental_writer)  # adds attributes too
+            self.add_edges(G, incremental_writer)  # adds attributes too
+
+    def add_attributes(self, scope, xml_obj, data, default):
+        """Appends attribute data."""
+        for k, v in data.items():
+            data_element = self.add_data(
+                str(k), self.attr_type(str(k), scope, v), str(v), scope, default.get(k)
+            )
+            xml_obj.append(data_element)
+
+    def __str__(self):
+        return object.__str__(self)
+
+    def dump(self):
+        self._graphml.__exit__(None, None, None)
+        self._xml_base.__exit__(None, None, None)
+
+
+# default is lxml is present.
+write_graphml = write_graphml_lxml
+
+
+class GraphMLReader(GraphML):
+    """Read a GraphML document.  Produces EasyGraph graph objects."""
+
+    def __init__(self, node_type=str, edge_key_type=int, force_multigraph=False):
+        self.construct_types()
+        self.node_type = node_type
+        self.edge_key_type = edge_key_type
+        self.multigraph = force_multigraph  # If False, test for multiedges
+        self.edge_ids = {}  # dict mapping (u,v) tuples to edge id attributes
+
+    def __call__(self, path=None, string=None):
+        from xml.etree.ElementTree import ElementTree
+        from xml.etree.ElementTree import fromstring
+
+        if path is not None:
+            self.xml = ElementTree(file=path)
+        elif string is not None:
+            self.xml = fromstring(string)
+        else:
+            raise ValueError("Must specify either 'path' or 'string' as kwarg")
+        (keys, defaults) = self.find_graphml_keys(self.xml)
+        for g in self.xml.findall(f"{{{self.NS_GRAPHML}}}graph"):
+            yield self.make_graph(g, keys, defaults)
+
+    def make_graph(self, graph_xml, graphml_keys, defaults, G=None):
+        # set default graph type
+        edgedefault = graph_xml.get("edgedefault", None)
+        if G is None:
+            if edgedefault == "directed":
+                G = eg.MultiDiGraph()
+            else:
+                G = eg.MultiGraph()
+        # set defaults for graph attributes
+        G.graph["node_default"] = {}
+        G.graph["edge_default"] = {}
+        for key_id, value in defaults.items():
+            key_for = graphml_keys[key_id]["for"]
+            name = graphml_keys[key_id]["name"]
+            python_type = graphml_keys[key_id]["type"]
+            if key_for == "node":
+                G.graph["node_default"].update({name: python_type(value)})
+            if key_for == "edge":
+                G.graph["edge_default"].update({name: python_type(value)})
+        # hyperedges are not supported
+        hyperedge = graph_xml.find(f"{{{self.NS_GRAPHML}}}hyperedge")
+        if hyperedge is not None:
+            raise eg.EasyGraphError("GraphML reader doesn't support hyperedges")
+        # add nodes
+        for node_xml in graph_xml.findall(f"{{{self.NS_GRAPHML}}}node"):
+            self.add_node(G, node_xml, graphml_keys, defaults)
+        # add edges
+        for edge_xml in graph_xml.findall(f"{{{self.NS_GRAPHML}}}edge"):
+            self.add_edge(G, edge_xml, graphml_keys)
+        # add graph data
+        data = self.decode_data_elements(graphml_keys, graph_xml)
+        G.graph.update(data)
+
+        # switch to Graph or DiGraph if no parallel edges were found
+        if self.multigraph:
+            return G
+
+        G = eg.DiGraph(G) if G.is_directed() else eg.Graph(G)
+        # add explicit edge "id" from file as attribute in eg graph.
+        eg.set_edge_attributes(G, values=self.edge_ids, name="id")
+        return G
+
+    def add_node(self, G, node_xml, graphml_keys, defaults):
+        """Add a node to the graph."""
+        # warn on finding unsupported ports tag
+        ports = node_xml.find(f"{{{self.NS_GRAPHML}}}port")
+        if ports is not None:
+            warnings.warn("GraphML port tag not supported.")
+        # find the node by id and cast it to the appropriate type
+        node_id = self.node_type(node_xml.get("id"))
+        # get data/attributes for node
+        data = self.decode_data_elements(graphml_keys, node_xml)
+        G.add_node(node_id, **data)
+        # get child nodes
+        if node_xml.attrib.get("yfiles.foldertype") == "group":
+            graph_xml = node_xml.find(f"{{{self.NS_GRAPHML}}}graph")
+            self.make_graph(graph_xml, graphml_keys, defaults, G)
+
+    def add_edge(self, G, edge_element, graphml_keys):
+        """Add an edge to the graph."""
+        # warn on finding unsupported ports tag
+        ports = edge_element.find(f"{{{self.NS_GRAPHML}}}port")
+        if ports is not None:
+            warnings.warn("GraphML port tag not supported.")
+
+        # raise error if we find mixed directed and undirected edges
+        directed = edge_element.get("directed")
+        if G.is_directed() and directed == "false":
+            msg = "directed=false edge found in directed graph."
+            raise eg.EasyGraphError(msg)
+        if (not G.is_directed()) and directed == "true":
+            msg = "directed=true edge found in undi rected graph."
+            raise eg.EasyGraphError(msg)
+
+        source = self.node_type(edge_element.get("source"))
+        target = self.node_type(edge_element.get("target"))
+        data = self.decode_data_elements(graphml_keys, edge_element)
+        # GraphML stores edge ids as an attribute
+        # EasyGraph uses them as keys in multigraphs too if no key
+        # attribute is specified
+        edge_id = edge_element.get("id")
+        if edge_id:
+            # self.edge_ids is used by `make_graph` method for non-multigraphs
+            self.edge_ids[source, target] = edge_id
+            try:
+                edge_id = self.edge_key_type(edge_id)
+            except ValueError:  # Could not convert.
+                pass
+        else:
+            edge_id = data.get("key")
+
+        if G.has_edge(source, target):
+            # mark this as a multigraph
+            self.multigraph = True
+
+        # Use add_edges_from to avoid error with add_edge when `'key' in data`
+        # Note there is only one edge here...
+        G.add_edges_from([(source, target, edge_id, data)])
+
+    def decode_data_elements(self, graphml_keys, obj_xml):
+        """Use the key information to decode the data XML if present."""
+        data = {}
+        for data_element in obj_xml.findall(f"{{{self.NS_GRAPHML}}}data"):
+            key = data_element.get("key")
+            try:
+                data_name = graphml_keys[key]["name"]
+                data_type = graphml_keys[key]["type"]
+            except KeyError as err:
+                raise eg.EasyGraphError(f"Bad GraphML data: no key {key}") from err
+            text = data_element.text
+            # assume anything with subelements is a yfiles extension
+            if text is not None and len(list(data_element)) == 0:
+                if data_type == bool:
+                    # Ignore cases.
+                    # http://docs.oracle.com/javase/6/docs/api/java/lang/
+                    # Boolean.html#parseBoolean%28java.lang.String%29
+                    data[data_name] = self.convert_bool[text.lower()]
+                else:
+                    data[data_name] = data_type(text)
+            elif len(list(data_element)) > 0:
+                # Assume yfiles as subelements, try to extract node_label
+                node_label = None
+                # set GenericNode's configuration as shape type
+                gn = data_element.find(f"{{{self.NS_Y}}}GenericNode")
+                if gn:
+                    data["shape_type"] = gn.get("configuration")
+                for node_type in ["GenericNode", "ShapeNode", "SVGNode", "ImageNode"]:
+                    pref = f"{{{self.NS_Y}}}{node_type}/{{{self.NS_Y}}}"
+                    geometry = data_element.find(f"{pref}Geometry")
+                    if geometry is not None:
+                        data["x"] = geometry.get("x")
+                        data["y"] = geometry.get("y")
+                    if node_label is None:
+                        node_label = data_element.find(f"{pref}NodeLabel")
+                    shape = data_element.find(f"{pref}Shape")
+                    if shape is not None:
+                        data["shape_type"] = shape.get("type")
+                if node_label is not None:
+                    data["label"] = node_label.text
+
+                # check all the different types of edges avaivable in yEd.
+                for edge_type in [
+                    "PolyLineEdge",
+                    "SplineEdge",
+                    "QuadCurveEdge",
+                    "BezierEdge",
+                    "ArcEdge",
+                ]:
+                    pref = f"{{{self.NS_Y}}}{edge_type}/{{{self.NS_Y}}}"
+                    edge_label = data_element.find(f"{pref}EdgeLabel")
+                    if edge_label is not None:
+                        break
+
+                if edge_label is not None:
+                    data["label"] = edge_label.text
+        return data
+
+    def find_graphml_keys(self, graph_element):
+        """Extracts all the keys and key defaults from the xml."""
+        graphml_keys = {}
+        graphml_key_defaults = {}
+        for k in graph_element.findall(f"{{{self.NS_GRAPHML}}}key"):
+            attr_id = k.get("id")
+            attr_type = k.get("attr.type")
+            attr_name = k.get("attr.name")
+            yfiles_type = k.get("yfiles.type")
+            if yfiles_type is not None:
+                attr_name = yfiles_type
+                attr_type = "yfiles"
+            if attr_type is None:
+                attr_type = "string"
+                warnings.warn(f"No key type for id {attr_id}. Using string")
+            if attr_name is None:
+                raise eg.EasyGraphError(f"Unknown key for id {attr_id}.")
+            graphml_keys[attr_id] = {
+                "name": attr_name,
+                "type": self.python_type[attr_type],
+                "for": k.get("for"),
+            }
+            # check for "default" sub-element of key element
+            default = k.find(f"{{{self.NS_GRAPHML}}}default")
+            if default is not None:
+                # Handle default values identically to data element values
+                python_type = graphml_keys[attr_id]["type"]
+                if python_type == bool:
+                    graphml_key_defaults[attr_id] = self.convert_bool[
+                        default.text.lower()
+                    ]
+                else:
+                    graphml_key_defaults[attr_id] = python_type(default.text)
+        return graphml_keys, graphml_key_defaults
```

## easygraph/readwrite/edgelist.py

 * *Ordering differences only*

```diff
@@ -1,463 +1,463 @@
-import easygraph as eg
-
-from easygraph.utils import open_file
-
-
-__all__ = [
-    "parse_edgelist",
-    "generate_edgelist",
-    "write_edgelist",
-    "read_edgelist",
-    "read_weighted_edgelist",
-    "write_weighted_edgelist",
-]
-
-
-def parse_edgelist(
-    lines, comments="#", delimiter=None, create_using=None, nodetype=None, data=True
-):
-    """Parse lines of an edge list representation of a graph.
-
-    Parameters
-    ----------
-    lines : list or iterator of strings
-        Input data in edgelist format
-    comments : string, optional
-       Marker for comment lines. Default is `'#'`. To specify that no character
-       should be treated as a comment, use ``comments=None``.
-    delimiter : string, optional
-       Separator for node labels. Default is `None`, meaning any whitespace.
-    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
-       Graph type to create. If graph instance, then cleared before populated.
-    nodetype : Python type, optional
-       Convert nodes to this type. Default is `None`, meaning no conversion is
-       performed.
-    data : bool or list of (label,type) tuples
-       If `False` generate no edge data or if `True` use a dictionary
-       representation of edge data or a list tuples specifying dictionary
-       key names and types for edge data.
-
-    Returns
-    -------
-    G: EasyGraph Graph
-        The graph corresponding to lines
-
-    Examples
-    --------
-    Edgelist with no data:
-
-    >>> lines = ["1 2", "2 3", "3 4"]
-    >>> G = eg.parse_edgelist(lines, nodetype=int)
-    >>> list(G)
-    [1, 2, 3, 4]
-    >>> list(G.edges)
-    [(1, 2), (2, 3), (3, 4)]
-
-    Edgelist with data in Python dictionary representation:
-
-    >>> lines = ["1 2 {'weight': 3}", "2 3 {'weight': 27}", "3 4 {'weight': 3.0}"]
-    >>> G = eg.parse_edgelist(lines, nodetype=int)
-    >>> list(G)
-    [1, 2, 3, 4]
-    >>> list(G.edges)
-    [(1, 2, {'weight': 3}), (2, 3, {'weight': 27}), (3, 4, {'weight': 3.0})]
-
-    Edgelist with data in a list:
-
-    >>> lines = ["1 2 3", "2 3 27", "3 4 3.0"]
-    >>> G = eg.parse_edgelist(lines, nodetype=int, data=(("weight", float),))
-    >>> list(G)
-    [1, 2, 3, 4]
-    >>> list(G.edges)
-    [(1, 2, {'weight': 3.0}), (2, 3, {'weight': 27.0}), (3, 4, {'weight': 3.0})]
-
-    See Also
-    --------
-    read_weighted_edgelist
-    """
-    from ast import literal_eval
-
-    G = eg.empty_graph(0, create_using)
-    for line in lines:
-        if comments is not None:
-            p = line.find(comments)
-            if p >= 0:
-                line = line[:p]
-            if not line:
-                continue
-        # split line, should have 2 or more
-        s = line.strip().split(delimiter)
-        if len(s) < 2:
-            continue
-        u = s.pop(0)
-        v = s.pop(0)
-        d = s
-        if nodetype is not None:
-            try:
-                u = nodetype(u)
-                v = nodetype(v)
-            except Exception as err:
-                raise TypeError(
-                    f"Failed to convert nodes {u},{v} to type {nodetype}."
-                ) from err
-
-        if len(d) == 0 or data is False:
-            # no data or data type specified
-            edgedata = {}
-        elif data is True:
-            # no edge types specified
-            try:  # try to evaluate as dictionary
-                if delimiter == ",":
-                    edgedata_str = ",".join(d)
-                else:
-                    edgedata_str = " ".join(d)
-                edgedata = dict(literal_eval(edgedata_str.strip()))
-            except Exception as err:
-                raise TypeError(
-                    f"Failed to convert edge data ({d}) to dictionary."
-                ) from err
-        else:
-            # convert edge data to dictionary with specified keys and type
-            if len(d) != len(data):
-                raise IndexError(
-                    f"Edge data {d} and data_keys {data} are not the same length"
-                )
-            edgedata = {}
-            for (edge_key, edge_type), edge_value in zip(data, d):
-                try:
-                    edge_value = edge_type(edge_value)
-                except Exception as err:
-                    raise TypeError(
-                        f"Failed to convert {edge_key} data {edge_value} "
-                        f"to type {edge_type}."
-                    ) from err
-                edgedata.update({edge_key: edge_value})
-        G.add_edge(u, v, **edgedata)
-    return G
-
-
-def generate_edgelist(G, delimiter=" ", data=True):
-    """Generate a single line of the graph G in edge list format.
-
-    Parameters
-    ----------
-    G : EasyGraph graph
-
-    delimiter : string, optional
-       Separator for node labels
-
-    data : bool or list of keys
-       If False generate no edge data.  If True use a dictionary
-       representation of edge data.  If a list of keys use a list of data
-       values corresponding to the keys.
-
-    Returns
-    -------
-    lines : string
-        Lines of data in adjlist format.
-
-    Examples
-    --------
-    >>> G = eg.lollipop_graph(4, 3)
-    >>> G[1][2]["weight"] = 3
-    >>> G[3][4]["capacity"] = 12
-    >>> for line in eg.generate_edgelist(G, data=False):
-    ...     print(line)
-    0 1
-    0 2
-    0 3
-    1 2
-    1 3
-    2 3
-    3 4
-    4 5
-    5 6
-
-    >>> for line in eg.generate_edgelist(G):
-    ...     print(line)
-    0 1 {}
-    0 2 {}
-    0 3 {}
-    1 2 {'weight': 3}
-    1 3 {}
-    2 3 {}
-    3 4 {'capacity': 12}
-    4 5 {}
-    5 6 {}
-
-    >>> for line in eg.generate_edgelist(G, data=["weight"]):
-    ...     print(line)
-    0 1
-    0 2
-    0 3
-    1 2 3
-    1 3
-    2 3
-    3 4
-    4 5
-    5 6
-
-    See Also
-    --------
-    write_adjlist, read_adjlist
-    """
-    edges = G.edges
-    if edges and len(edges[0]) > 3:
-        # multigraph
-        edges = ((u, v, d) for u, v, _, d in edges)
-    if data is True:
-        for u, v, d in edges:
-            e = u, v, dict(d)
-            yield delimiter.join(map(str, e))
-    elif data is False:
-        for u, v, _ in edges:
-            e = u, v
-            yield delimiter.join(map(str, e))
-    else:
-        for u, v, d in edges:
-            e = [u, v]
-            try:
-                e.extend(d[k] for k in data)
-            except KeyError:
-                pass  # missing data for this edge, should warn?
-            yield delimiter.join(map(str, e))
-
-
-@open_file(1, mode="wb")
-def write_edgelist(G, path, comments="#", delimiter=" ", data=True, encoding="utf-8"):
-    """Write graph as a list of edges.
-
-    Parameters
-    ----------
-    G : graph
-       A EasyGraph graph
-    path : file or string
-       File or filename to write. If a file is provided, it must be
-       opened in 'wb' mode. Filenames ending in .gz or .bz2 will be compressed.
-    comments : string, optional
-       The character used to indicate the start of a comment
-    delimiter : string, optional
-       The string used to separate values.  The default is whitespace.
-    data : bool or list, optional
-       If False write no edge data.
-       If True write a string representation of the edge data dictionary..
-       If a list (or other iterable) is provided, write the  keys specified
-       in the list.
-    encoding: string, optional
-       Specify which encoding to use when writing file.
-
-    Examples
-    --------
-    >>> G = eg.path_graph(4)
-    >>> eg.write_edgelist(G, "test.edgelist")
-    >>> G = eg.path_graph(4)
-    >>> fh = open("test.edgelist", "wb")
-    >>> eg.write_edgelist(G, fh)
-    >>> eg.write_edgelist(G, "test.edgelist.gz")
-    >>> eg.write_edgelist(G, "test.edgelist.gz", data=False)
-
-    >>> G = eg.Graph()
-    >>> G.add_edge(1, 2, weight=7, color="red")
-    >>> eg.write_edgelist(G, "test.edgelist", data=False)
-    >>> eg.write_edgelist(G, "test.edgelist", data=["color"])
-    >>> eg.write_edgelist(G, "test.edgelist", data=["color", "weight"])
-
-    See Also
-    --------
-    read_edgelist
-    write_weighted_edgelist
-    """
-
-    for line in generate_edgelist(G, delimiter, data):
-        line += "\n"
-        path.write(line.encode(encoding))
-
-
-@open_file(0, mode="rb")
-def read_edgelist(
-    path,
-    comments="#",
-    delimiter=None,
-    create_using=None,
-    nodetype=None,
-    data=True,
-    edgetype=None,
-    encoding="utf-8",
-):
-    """Read a graph from a list of edges.
-
-    Parameters
-    ----------
-    path : file or string
-       File or filename to read. If a file is provided, it must be
-       opened in 'rb' mode.
-       Filenames ending in .gz or .bz2 will be uncompressed.
-    comments : string, optional
-       The character used to indicate the start of a comment. To specify that
-       no character should be treated as a comment, use ``comments=None``.
-    delimiter : string, optional
-       The string used to separate values.  The default is whitespace.
-    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
-       Graph type to create. If graph instance, then cleared before populated.
-    nodetype : int, float, str, Python type, optional
-       Convert node data from strings to specified type
-    data : bool or list of (label,type) tuples
-       Tuples specifying dictionary key names and types for edge data
-    edgetype : int, float, str, Python type, optional OBSOLETE
-       Convert edge data from strings to specified type and use as 'weight'
-    encoding: string, optional
-       Specify which encoding to use when reading file.
-
-    Returns
-    -------
-    G : graph
-       A easygraph Graph or other type specified with create_using
-
-    Examples
-    --------
-    >>> eg.write_edgelist(eg.path_graph(4), "test.edgelist")
-    >>> G = eg.read_edgelist("test.edgelist")
-
-    >>> fh = open("test.edgelist", "rb")
-    >>> G = eg.read_edgelist(fh)
-    >>> fh.close()
-
-    >>> G = eg.read_edgelist("test.edgelist", nodetype=int)
-    >>> G = eg.read_edgelist("test.edgelist", create_using=eg.DiGraph)
-
-    Edgelist with data in a list:
-
-    >>> textline = "1 2 3"
-    >>> fh = open("test.edgelist", "w")
-    >>> d = fh.write(textline)
-    >>> fh.close()
-    >>> G = eg.read_edgelist("test.edgelist", nodetype=int, data=(("weight", float),))
-    >>> list(G)
-    [1, 2]
-    >>> list(G.edges)
-    [(1, 2, {'weight': 3.0})]
-
-    See parse_edgelist() for more examples of formatting.
-
-    See Also
-    --------
-    parse_edgelist
-    write_edgelist
-
-    Notes
-    -----
-    Since nodes must be hashable, the function nodetype must return hashable
-    types (e.g. int, float, str, frozenset - or tuples of those, etc.)
-    """
-    lines = (line if isinstance(line, str) else line.decode(encoding) for line in path)
-    return parse_edgelist(
-        lines,
-        comments=comments,
-        delimiter=delimiter,
-        create_using=create_using,
-        nodetype=nodetype,
-        data=data,
-    )
-
-
-def write_weighted_edgelist(G, path, comments="#", delimiter=" ", encoding="utf-8"):
-    """Write graph G as a list of edges with numeric weights.
-
-    Parameters
-    ----------
-    G : graph
-       A EasyGraph graph
-    path : file or string
-       File or filename to write. If a file is provided, it must be
-       opened in 'wb' mode.
-       Filenames ending in .gz or .bz2 will be compressed.
-    comments : string, optional
-       The character used to indicate the start of a comment
-    delimiter : string, optional
-       The string used to separate values.  The default is whitespace.
-    encoding: string, optional
-       Specify which encoding to use when writing file.
-
-    Examples
-    --------
-    >>> G = eg.Graph()
-    >>> G.add_edge(1, 2, weight=7)
-    >>> eg.write_weighted_edgelist(G, "test.weighted.edgelist")
-
-    See Also
-    --------
-    read_edgelist
-    write_edgelist
-    read_weighted_edgelist
-    """
-    write_edgelist(
-        G,
-        path,
-        comments=comments,
-        delimiter=delimiter,
-        data=("weight",),
-        encoding=encoding,
-    )
-
-
-def read_weighted_edgelist(
-    path,
-    comments="#",
-    delimiter=None,
-    create_using=None,
-    nodetype=None,
-    encoding="utf-8",
-):
-    """Read a graph as list of edges with numeric weights.
-
-    Parameters
-    ----------
-    path : file or string
-       File or filename to read. If a file is provided, it must be
-       opened in 'rb' mode.
-       Filenames ending in .gz or .bz2 will be uncompressed.
-    comments : string, optional
-       The character used to indicate the start of a comment.
-    delimiter : string, optional
-       The string used to separate values.  The default is whitespace.
-    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
-       Graph type to create. If graph instance, then cleared before populated.
-    nodetype : int, float, str, Python type, optional
-       Convert node data from strings to specified type
-    encoding: string, optional
-       Specify which encoding to use when reading file.
-
-    Returns
-    -------
-    G : graph
-       A easygraph Graph or other type specified with create_using
-
-    Notes
-    -----
-    Since nodes must be hashable, the function nodetype must return hashable
-    types (e.g. int, float, str, frozenset - or tuples of those, etc.)
-
-    Example edgelist file format.
-
-    With numeric edge data::
-
-     # read with
-     # >>> G=eg.read_weighted_edgelist(fh)
-     # source target data
-     a b 1
-     a c 3.14159
-     d e 42
-
-    See Also
-    --------
-    write_weighted_edgelist
-    """
-    return read_edgelist(
-        path,
-        comments=comments,
-        delimiter=delimiter,
-        create_using=create_using,
-        nodetype=nodetype,
-        data=(("weight", float),),
-        encoding=encoding,
-    )
+import easygraph as eg
+
+from easygraph.utils import open_file
+
+
+__all__ = [
+    "parse_edgelist",
+    "generate_edgelist",
+    "write_edgelist",
+    "read_edgelist",
+    "read_weighted_edgelist",
+    "write_weighted_edgelist",
+]
+
+
+def parse_edgelist(
+    lines, comments="#", delimiter=None, create_using=None, nodetype=None, data=True
+):
+    """Parse lines of an edge list representation of a graph.
+
+    Parameters
+    ----------
+    lines : list or iterator of strings
+        Input data in edgelist format
+    comments : string, optional
+       Marker for comment lines. Default is `'#'`. To specify that no character
+       should be treated as a comment, use ``comments=None``.
+    delimiter : string, optional
+       Separator for node labels. Default is `None`, meaning any whitespace.
+    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
+       Graph type to create. If graph instance, then cleared before populated.
+    nodetype : Python type, optional
+       Convert nodes to this type. Default is `None`, meaning no conversion is
+       performed.
+    data : bool or list of (label,type) tuples
+       If `False` generate no edge data or if `True` use a dictionary
+       representation of edge data or a list tuples specifying dictionary
+       key names and types for edge data.
+
+    Returns
+    -------
+    G: EasyGraph Graph
+        The graph corresponding to lines
+
+    Examples
+    --------
+    Edgelist with no data:
+
+    >>> lines = ["1 2", "2 3", "3 4"]
+    >>> G = eg.parse_edgelist(lines, nodetype=int)
+    >>> list(G)
+    [1, 2, 3, 4]
+    >>> list(G.edges)
+    [(1, 2), (2, 3), (3, 4)]
+
+    Edgelist with data in Python dictionary representation:
+
+    >>> lines = ["1 2 {'weight': 3}", "2 3 {'weight': 27}", "3 4 {'weight': 3.0}"]
+    >>> G = eg.parse_edgelist(lines, nodetype=int)
+    >>> list(G)
+    [1, 2, 3, 4]
+    >>> list(G.edges)
+    [(1, 2, {'weight': 3}), (2, 3, {'weight': 27}), (3, 4, {'weight': 3.0})]
+
+    Edgelist with data in a list:
+
+    >>> lines = ["1 2 3", "2 3 27", "3 4 3.0"]
+    >>> G = eg.parse_edgelist(lines, nodetype=int, data=(("weight", float),))
+    >>> list(G)
+    [1, 2, 3, 4]
+    >>> list(G.edges)
+    [(1, 2, {'weight': 3.0}), (2, 3, {'weight': 27.0}), (3, 4, {'weight': 3.0})]
+
+    See Also
+    --------
+    read_weighted_edgelist
+    """
+    from ast import literal_eval
+
+    G = eg.empty_graph(0, create_using)
+    for line in lines:
+        if comments is not None:
+            p = line.find(comments)
+            if p >= 0:
+                line = line[:p]
+            if not line:
+                continue
+        # split line, should have 2 or more
+        s = line.strip().split(delimiter)
+        if len(s) < 2:
+            continue
+        u = s.pop(0)
+        v = s.pop(0)
+        d = s
+        if nodetype is not None:
+            try:
+                u = nodetype(u)
+                v = nodetype(v)
+            except Exception as err:
+                raise TypeError(
+                    f"Failed to convert nodes {u},{v} to type {nodetype}."
+                ) from err
+
+        if len(d) == 0 or data is False:
+            # no data or data type specified
+            edgedata = {}
+        elif data is True:
+            # no edge types specified
+            try:  # try to evaluate as dictionary
+                if delimiter == ",":
+                    edgedata_str = ",".join(d)
+                else:
+                    edgedata_str = " ".join(d)
+                edgedata = dict(literal_eval(edgedata_str.strip()))
+            except Exception as err:
+                raise TypeError(
+                    f"Failed to convert edge data ({d}) to dictionary."
+                ) from err
+        else:
+            # convert edge data to dictionary with specified keys and type
+            if len(d) != len(data):
+                raise IndexError(
+                    f"Edge data {d} and data_keys {data} are not the same length"
+                )
+            edgedata = {}
+            for (edge_key, edge_type), edge_value in zip(data, d):
+                try:
+                    edge_value = edge_type(edge_value)
+                except Exception as err:
+                    raise TypeError(
+                        f"Failed to convert {edge_key} data {edge_value} "
+                        f"to type {edge_type}."
+                    ) from err
+                edgedata.update({edge_key: edge_value})
+        G.add_edge(u, v, **edgedata)
+    return G
+
+
+def generate_edgelist(G, delimiter=" ", data=True):
+    """Generate a single line of the graph G in edge list format.
+
+    Parameters
+    ----------
+    G : EasyGraph graph
+
+    delimiter : string, optional
+       Separator for node labels
+
+    data : bool or list of keys
+       If False generate no edge data.  If True use a dictionary
+       representation of edge data.  If a list of keys use a list of data
+       values corresponding to the keys.
+
+    Returns
+    -------
+    lines : string
+        Lines of data in adjlist format.
+
+    Examples
+    --------
+    >>> G = eg.lollipop_graph(4, 3)
+    >>> G[1][2]["weight"] = 3
+    >>> G[3][4]["capacity"] = 12
+    >>> for line in eg.generate_edgelist(G, data=False):
+    ...     print(line)
+    0 1
+    0 2
+    0 3
+    1 2
+    1 3
+    2 3
+    3 4
+    4 5
+    5 6
+
+    >>> for line in eg.generate_edgelist(G):
+    ...     print(line)
+    0 1 {}
+    0 2 {}
+    0 3 {}
+    1 2 {'weight': 3}
+    1 3 {}
+    2 3 {}
+    3 4 {'capacity': 12}
+    4 5 {}
+    5 6 {}
+
+    >>> for line in eg.generate_edgelist(G, data=["weight"]):
+    ...     print(line)
+    0 1
+    0 2
+    0 3
+    1 2 3
+    1 3
+    2 3
+    3 4
+    4 5
+    5 6
+
+    See Also
+    --------
+    write_adjlist, read_adjlist
+    """
+    edges = G.edges
+    if edges and len(edges[0]) > 3:
+        # multigraph
+        edges = ((u, v, d) for u, v, _, d in edges)
+    if data is True:
+        for u, v, d in edges:
+            e = u, v, dict(d)
+            yield delimiter.join(map(str, e))
+    elif data is False:
+        for u, v, _ in edges:
+            e = u, v
+            yield delimiter.join(map(str, e))
+    else:
+        for u, v, d in edges:
+            e = [u, v]
+            try:
+                e.extend(d[k] for k in data)
+            except KeyError:
+                pass  # missing data for this edge, should warn?
+            yield delimiter.join(map(str, e))
+
+
+@open_file(1, mode="wb")
+def write_edgelist(G, path, comments="#", delimiter=" ", data=True, encoding="utf-8"):
+    """Write graph as a list of edges.
+
+    Parameters
+    ----------
+    G : graph
+       A EasyGraph graph
+    path : file or string
+       File or filename to write. If a file is provided, it must be
+       opened in 'wb' mode. Filenames ending in .gz or .bz2 will be compressed.
+    comments : string, optional
+       The character used to indicate the start of a comment
+    delimiter : string, optional
+       The string used to separate values.  The default is whitespace.
+    data : bool or list, optional
+       If False write no edge data.
+       If True write a string representation of the edge data dictionary..
+       If a list (or other iterable) is provided, write the  keys specified
+       in the list.
+    encoding: string, optional
+       Specify which encoding to use when writing file.
+
+    Examples
+    --------
+    >>> G = eg.path_graph(4)
+    >>> eg.write_edgelist(G, "test.edgelist")
+    >>> G = eg.path_graph(4)
+    >>> fh = open("test.edgelist", "wb")
+    >>> eg.write_edgelist(G, fh)
+    >>> eg.write_edgelist(G, "test.edgelist.gz")
+    >>> eg.write_edgelist(G, "test.edgelist.gz", data=False)
+
+    >>> G = eg.Graph()
+    >>> G.add_edge(1, 2, weight=7, color="red")
+    >>> eg.write_edgelist(G, "test.edgelist", data=False)
+    >>> eg.write_edgelist(G, "test.edgelist", data=["color"])
+    >>> eg.write_edgelist(G, "test.edgelist", data=["color", "weight"])
+
+    See Also
+    --------
+    read_edgelist
+    write_weighted_edgelist
+    """
+
+    for line in generate_edgelist(G, delimiter, data):
+        line += "\n"
+        path.write(line.encode(encoding))
+
+
+@open_file(0, mode="rb")
+def read_edgelist(
+    path,
+    comments="#",
+    delimiter=None,
+    create_using=None,
+    nodetype=None,
+    data=True,
+    edgetype=None,
+    encoding="utf-8",
+):
+    """Read a graph from a list of edges.
+
+    Parameters
+    ----------
+    path : file or string
+       File or filename to read. If a file is provided, it must be
+       opened in 'rb' mode.
+       Filenames ending in .gz or .bz2 will be uncompressed.
+    comments : string, optional
+       The character used to indicate the start of a comment. To specify that
+       no character should be treated as a comment, use ``comments=None``.
+    delimiter : string, optional
+       The string used to separate values.  The default is whitespace.
+    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
+       Graph type to create. If graph instance, then cleared before populated.
+    nodetype : int, float, str, Python type, optional
+       Convert node data from strings to specified type
+    data : bool or list of (label,type) tuples
+       Tuples specifying dictionary key names and types for edge data
+    edgetype : int, float, str, Python type, optional OBSOLETE
+       Convert edge data from strings to specified type and use as 'weight'
+    encoding: string, optional
+       Specify which encoding to use when reading file.
+
+    Returns
+    -------
+    G : graph
+       A easygraph Graph or other type specified with create_using
+
+    Examples
+    --------
+    >>> eg.write_edgelist(eg.path_graph(4), "test.edgelist")
+    >>> G = eg.read_edgelist("test.edgelist")
+
+    >>> fh = open("test.edgelist", "rb")
+    >>> G = eg.read_edgelist(fh)
+    >>> fh.close()
+
+    >>> G = eg.read_edgelist("test.edgelist", nodetype=int)
+    >>> G = eg.read_edgelist("test.edgelist", create_using=eg.DiGraph)
+
+    Edgelist with data in a list:
+
+    >>> textline = "1 2 3"
+    >>> fh = open("test.edgelist", "w")
+    >>> d = fh.write(textline)
+    >>> fh.close()
+    >>> G = eg.read_edgelist("test.edgelist", nodetype=int, data=(("weight", float),))
+    >>> list(G)
+    [1, 2]
+    >>> list(G.edges)
+    [(1, 2, {'weight': 3.0})]
+
+    See parse_edgelist() for more examples of formatting.
+
+    See Also
+    --------
+    parse_edgelist
+    write_edgelist
+
+    Notes
+    -----
+    Since nodes must be hashable, the function nodetype must return hashable
+    types (e.g. int, float, str, frozenset - or tuples of those, etc.)
+    """
+    lines = (line if isinstance(line, str) else line.decode(encoding) for line in path)
+    return parse_edgelist(
+        lines,
+        comments=comments,
+        delimiter=delimiter,
+        create_using=create_using,
+        nodetype=nodetype,
+        data=data,
+    )
+
+
+def write_weighted_edgelist(G, path, comments="#", delimiter=" ", encoding="utf-8"):
+    """Write graph G as a list of edges with numeric weights.
+
+    Parameters
+    ----------
+    G : graph
+       A EasyGraph graph
+    path : file or string
+       File or filename to write. If a file is provided, it must be
+       opened in 'wb' mode.
+       Filenames ending in .gz or .bz2 will be compressed.
+    comments : string, optional
+       The character used to indicate the start of a comment
+    delimiter : string, optional
+       The string used to separate values.  The default is whitespace.
+    encoding: string, optional
+       Specify which encoding to use when writing file.
+
+    Examples
+    --------
+    >>> G = eg.Graph()
+    >>> G.add_edge(1, 2, weight=7)
+    >>> eg.write_weighted_edgelist(G, "test.weighted.edgelist")
+
+    See Also
+    --------
+    read_edgelist
+    write_edgelist
+    read_weighted_edgelist
+    """
+    write_edgelist(
+        G,
+        path,
+        comments=comments,
+        delimiter=delimiter,
+        data=("weight",),
+        encoding=encoding,
+    )
+
+
+def read_weighted_edgelist(
+    path,
+    comments="#",
+    delimiter=None,
+    create_using=None,
+    nodetype=None,
+    encoding="utf-8",
+):
+    """Read a graph as list of edges with numeric weights.
+
+    Parameters
+    ----------
+    path : file or string
+       File or filename to read. If a file is provided, it must be
+       opened in 'rb' mode.
+       Filenames ending in .gz or .bz2 will be uncompressed.
+    comments : string, optional
+       The character used to indicate the start of a comment.
+    delimiter : string, optional
+       The string used to separate values.  The default is whitespace.
+    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
+       Graph type to create. If graph instance, then cleared before populated.
+    nodetype : int, float, str, Python type, optional
+       Convert node data from strings to specified type
+    encoding: string, optional
+       Specify which encoding to use when reading file.
+
+    Returns
+    -------
+    G : graph
+       A easygraph Graph or other type specified with create_using
+
+    Notes
+    -----
+    Since nodes must be hashable, the function nodetype must return hashable
+    types (e.g. int, float, str, frozenset - or tuples of those, etc.)
+
+    Example edgelist file format.
+
+    With numeric edge data::
+
+     # read with
+     # >>> G=eg.read_weighted_edgelist(fh)
+     # source target data
+     a b 1
+     a c 3.14159
+     d e 42
+
+    See Also
+    --------
+    write_weighted_edgelist
+    """
+    return read_edgelist(
+        path,
+        comments=comments,
+        delimiter=delimiter,
+        create_using=create_using,
+        nodetype=nodetype,
+        data=(("weight", float),),
+        encoding=encoding,
+    )
```

## easygraph/readwrite/__init__.py

 * *Ordering differences only*

```diff
@@ -1,9 +1,9 @@
-from easygraph.readwrite.edgelist import *
-from easygraph.readwrite.gexf import *
-from easygraph.readwrite.gml import *
-from easygraph.readwrite.graphml import *
-from easygraph.readwrite.graphviz import *
-from easygraph.readwrite.json_graph import *
-from easygraph.readwrite.pajek import *
-from easygraph.readwrite.pickle import *
-from easygraph.readwrite.ucinet import *
+from easygraph.readwrite.edgelist import *
+from easygraph.readwrite.gexf import *
+from easygraph.readwrite.gml import *
+from easygraph.readwrite.graphml import *
+from easygraph.readwrite.graphviz import *
+from easygraph.readwrite.json_graph import *
+from easygraph.readwrite.pajek import *
+from easygraph.readwrite.pickle import *
+from easygraph.readwrite.ucinet import *
```

## easygraph/readwrite/tests/test_gml.py

 * *Ordering differences only*

```diff
@@ -1,589 +1,589 @@
-import codecs
-import io
-import os
-import tempfile
-
-from ast import literal_eval
-from contextlib import contextmanager
-from textwrap import dedent
-
-import easygraph as eg
-import pytest
-
-from easygraph.readwrite.gml import literal_destringizer
-from easygraph.readwrite.gml import literal_stringizer
-
-
-class TestGraph:
-    @classmethod
-    def setup_class(cls):
-        cls.simple_data = """Creator "me"
-Version "xx"
-graph [
- comment "This is a sample graph"
- directed 1
- IsPlanar 1
- pos  [ x 0 y 1 ]
- node [
-   id 1
-   label "Node 1"
-   pos [ x 1 y 1 ]
- ]
- node [
-    id 2
-    pos [ x 1 y 2 ]
-    label "Node 2"
-    ]
-  node [
-    id 3
-    label "Node 3"
-    pos [ x 1 y 3 ]
-  ]
-  edge [
-    source 1
-    target 2
-    label "Edge from node 1 to node 2"
-    color [line "blue" thickness 3]
-
-  ]
-  edge [
-    source 2
-    target 3
-    label "Edge from node 2 to node 3"
-  ]
-  edge [
-    source 3
-    target 1
-    label "Edge from node 3 to node 1"
-  ]
-]
-"""
-
-    def test_parse_gml_cytoscape_bug(self):
-        # example from issue #321, originally #324 in trac
-        cytoscape_example = """
-Creator "Cytoscape"
-Version 1.0
-graph   [
-    node    [
-        root_index  -3
-        id  -3
-        graphics    [
-            x   -96.0
-            y   -67.0
-            w   40.0
-            h   40.0
-            fill    "#ff9999"
-            type    "ellipse"
-            outline "#666666"
-            outline_width   1.5
-        ]
-        label   "node2"
-    ]
-    node    [
-        root_index  -2
-        id  -2
-        graphics    [
-            x   63.0
-            y   37.0
-            w   40.0
-            h   40.0
-            fill    "#ff9999"
-            type    "ellipse"
-            outline "#666666"
-            outline_width   1.5
-        ]
-        label   "node1"
-    ]
-    node    [
-        root_index  -1
-        id  -1
-        graphics    [
-            x   -31.0
-            y   -17.0
-            w   40.0
-            h   40.0
-            fill    "#ff9999"
-            type    "ellipse"
-            outline "#666666"
-            outline_width   1.5
-        ]
-        label   "node0"
-    ]
-    edge    [
-        root_index  -2
-        target  -2
-        source  -1
-        graphics    [
-            width   1.5
-            fill    "#0000ff"
-            type    "line"
-            Line    [
-            ]
-            source_arrow    0
-            target_arrow    3
-        ]
-        label   "DirectedEdge"
-    ]
-    edge    [
-        root_index  -1
-        target  -1
-        source  -3
-        graphics    [
-            width   1.5
-            fill    "#0000ff"
-            type    "line"
-            Line    [
-            ]
-            source_arrow    0
-            target_arrow    3
-        ]
-        label   "DirectedEdge"
-    ]
-]
-"""
-        eg.parse_gml(cytoscape_example)
-
-    def test_parse_gml(self):
-        G = eg.parse_gml(self.simple_data, label="label")
-        assert sorted(G.nodes) == ["Node 1", "Node 2", "Node 3"]
-        assert [e[:2] for e in sorted(G.edges)] == [
-            ("Node 1", "Node 2"),
-            ("Node 2", "Node 3"),
-            ("Node 3", "Node 1"),
-        ]
-
-        assert [e for e in sorted(G.edges)] == [
-            (
-                "Node 1",
-                "Node 2",
-                {
-                    "color": {"line": "blue", "thickness": 3},
-                    "label": "Edge from node 1 to node 2",
-                },
-            ),
-            ("Node 2", "Node 3", {"label": "Edge from node 2 to node 3"}),
-            ("Node 3", "Node 1", {"label": "Edge from node 3 to node 1"}),
-        ]
-
-    def test_read_gml(self):
-        (fd, fname) = tempfile.mkstemp()
-        fh = open(fname, "w")
-        fh.write(self.simple_data)
-        fh.close()
-        Gin = eg.read_gml(fname, label="label")
-        G = eg.parse_gml(self.simple_data, label="label")
-        assert sorted(G.nodes) == sorted(Gin.nodes)
-        assert sorted(G.edges) == sorted(Gin.edges)
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_labels_are_strings(self):
-        # GML requires labels to be strings (i.e., in quotes)
-        answer = """graph [
-  node [
-    id 0
-    label "1203"
-  ]
-]"""
-        G = eg.Graph()
-        G.add_node(1203)
-        data = "\n".join(eg.generate_gml(G, stringizer=literal_stringizer))
-        assert data == answer
-
-    def test_relabel_duplicate(self):
-        data = """
-graph
-[
-        label   ""
-        directed        1
-        node
-        [
-                id      0
-                label   "same"
-        ]
-        node
-        [
-                id      1
-                label   "same"
-        ]
-]
-"""
-        fh = io.BytesIO(data.encode("UTF-8"))
-        fh.seek(0)
-        pytest.raises(eg.EasyGraphError, eg.read_gml, fh, label="label")
-
-    def test_quotes(self):
-        G = eg.path_graph(1)
-        G.name = "path_graph(1)"
-        attr = 'This is "quoted" and this is a copyright: ' + chr(169)
-        G.nodes[0]["demo"] = attr
-        fobj = tempfile.NamedTemporaryFile()
-        eg.write_gml(G, fobj)
-        fobj.seek(0)
-        # Should be bytes in 2.x and 3.x
-        data = fobj.read().strip().decode("ascii")
-        answer = """graph [
-  name "path_graph(1)"
-  node [
-    id 0
-    label "0"
-    demo "This is &#34;quoted&#34; and this is a copyright: &#169;"
-  ]
-]"""
-        assert data == answer
-
-    def test_unicode_node(self):
-        node = "node" + chr(169)
-        G = eg.Graph()
-        G.add_node(node)
-        fobj = tempfile.NamedTemporaryFile()
-        eg.write_gml(G, fobj)
-        fobj.seek(0)
-        # Should be bytes in 2.x and 3.x
-        data = fobj.read().strip().decode("ascii")
-        answer = """graph [
-  node [
-    id 0
-    label "node&#169;"
-  ]
-]"""
-        assert data == answer
-
-    def test_float_label(self):
-        node = 1.0
-        G = eg.Graph()
-        G.add_node(node)
-        fobj = tempfile.NamedTemporaryFile()
-        eg.write_gml(G, fobj)
-        fobj.seek(0)
-        # Should be bytes in 2.x and 3.x
-        data = fobj.read().strip().decode("ascii")
-        answer = """graph [
-  node [
-    id 0
-    label "1.0"
-  ]
-]"""
-        assert data == answer
-
-    def test_name(self):
-        G = eg.parse_gml('graph [ name "x" node [ id 0 label "x" ] ]')
-        assert "x" == G.graph["name"]
-        G = eg.parse_gml('graph [ node [ id 0 label "x" ] ]')
-        assert "" == G.name
-        assert "name" not in G.graph
-
-    def test_graph_types(self):
-        for directed in [None, False, True]:
-            for multigraph in [None, False, True]:
-                gml = "graph ["
-                if directed is not None:
-                    gml += " directed " + str(int(directed))
-                if multigraph is not None:
-                    gml += " multigraph " + str(int(multigraph))
-                gml += ' node [ id 0 label "0" ]'
-                gml += " edge [ source 0 target 0 ]"
-                gml += " ]"
-                G = eg.parse_gml(gml)
-                assert bool(directed) == G.is_directed()
-                assert bool(multigraph) == G.is_multigraph()
-                gml = "graph [\n"
-                if directed is True:
-                    gml += "  directed 1\n"
-                if multigraph is True:
-                    gml += "  multigraph 1\n"
-                gml += """  node [
-    id 0
-    label "0"
-  ]
-  edge [
-    source 0
-    target 0
-"""
-                if multigraph:
-                    gml += "    key 0\n"
-                gml += "  ]\n]"
-                assert gml == "\n".join(eg.generate_gml(G))
-
-    def test_data_types(self):
-        data = [
-            True,
-            False,
-            10**20,
-            -2e33,
-            "'",
-            '"&&amp;&&#34;"',
-            [{(b"\xfd",): "\x7f", chr(0x4444): (1, 2)}, (2, "3")],
-        ]
-        data.append(chr(0x14444))
-        data.append(literal_eval("{2.3j, 1 - 2.3j, ()}"))
-        G = eg.Graph()
-        G.name = data
-        G.graph["data"] = data
-        G.add_node(0, int=-1, data=dict(data=data))
-        G.add_edge(0, 0, float=-2.5, data=data)
-        gml = "\n".join(eg.generate_gml(G, stringizer=literal_stringizer))
-        G = eg.parse_gml(gml, destringizer=literal_destringizer)
-        assert data == G.name
-        assert {"name": data, "data": data} == G.graph
-        assert G.nodes == {0: dict(int=-1, data=dict(data=data))}
-        assert list(G.edges) == [(0, 0, dict(float=-2.5, data=data))]
-        G = eg.Graph()
-        G.graph["data"] = "frozenset([1, 2, 3])"
-        G = eg.parse_gml(eg.generate_gml(G), destringizer=literal_eval)
-        assert G.graph["data"] == "frozenset([1, 2, 3])"
-
-    def test_escape_unescape(self):
-        gml = """graph [
-  name "&amp;&#34;&#xf;&#x4444;&#1234567890;&#x1234567890abcdef;&unknown;"
-]"""
-        G = eg.parse_gml(gml)
-        assert (
-            '&"\x0f' + chr(0x4444) + "&#1234567890;&#x1234567890abcdef;&unknown;"
-            == G.name
-        )
-        gml = "\n".join(eg.generate_gml(G))
-        alnu = "#1234567890;&#38;#x1234567890abcdef"
-        answer = (
-            """graph [
-  name "&#38;&#34;&#15;&#17476;&#38;"""
-            + alnu
-            + """;&#38;unknown;"
-]"""
-        )
-        assert answer == gml
-
-    def test_exceptions(self):
-        pytest.raises(ValueError, literal_destringizer, "(")
-        pytest.raises(ValueError, literal_destringizer, "frozenset([1, 2, 3])")
-        pytest.raises(ValueError, literal_destringizer, literal_destringizer)
-        pytest.raises(ValueError, literal_stringizer, frozenset([1, 2, 3]))
-        pytest.raises(ValueError, literal_stringizer, literal_stringizer)
-        with tempfile.TemporaryFile() as f:
-            f.write(codecs.BOM_UTF8 + b"graph[]")
-            f.seek(0)
-            pytest.raises(eg.EasyGraphError, eg.read_gml, f)
-
-        def assert_parse_error(gml):
-            pytest.raises(eg.EasyGraphError, eg.parse_gml, gml)
-
-        assert_parse_error(["graph [\n\n", "]"])
-        assert_parse_error("")
-        assert_parse_error('Creator ""')
-        assert_parse_error("0")
-        assert_parse_error("graph ]")
-        assert_parse_error("graph [ 1 ]")
-        assert_parse_error("graph [ 1.E+2 ]")
-        assert_parse_error('graph [ "A" ]')
-        assert_parse_error("graph [ ] graph ]")
-        assert_parse_error("graph [ ] graph [ ]")
-        assert_parse_error("graph [ data [1, 2, 3] ]")
-        assert_parse_error("graph [ node [ ] ]")
-        assert_parse_error("graph [ node [ id 0 ] ]")
-        eg.parse_gml('graph [ node [ id "a" ] ]', label="id")
-        assert_parse_error("graph [ node [ id 0 label 0 ] node [ id 0 label 1 ] ]")
-        assert_parse_error("graph [ node [ id 0 label 0 ] node [ id 1 label 0 ] ]")
-        assert_parse_error("graph [ node [ id 0 label 0 ] edge [ ] ]")
-        assert_parse_error("graph [ node [ id 0 label 0 ] edge [ source 0 ] ]")
-        eg.parse_gml("graph [edge [ source 0 target 0 ] node [ id 0 label 0 ] ]")
-        assert_parse_error("graph [ node [ id 0 label 0 ] edge [ source 1 target 0 ] ]")
-        assert_parse_error("graph [ node [ id 0 label 0 ] edge [ source 0 target 1 ] ]")
-        assert_parse_error(
-            "graph [ node [ id 0 label 0 ] node [ id 1 label 1 ] "
-            "edge [ source 0 target 1 ] edge [ source 1 target 0 ] ]"
-        )
-        eg.parse_gml(
-            "graph [ node [ id 0 label 0 ] node [ id 1 label 1 ] "
-            "edge [ source 0 target 1 ] edge [ source 1 target 0 ] "
-            "directed 1 ]"
-        )
-        eg.parse_gml(
-            "graph [ node [ id 0 label 0 ] node [ id 1 label 1 ] "
-            "edge [ source 0 target 1 ] edge [ source 0 target 1 ]"
-            "multigraph 1 ]"
-        )
-        eg.parse_gml(
-            "graph [ node [ id 0 label 0 ] node [ id 1 label 1 ] "
-            "edge [ source 0 target 1 key 0 ] edge [ source 0 target 1 ]"
-            "multigraph 1 ]"
-        )
-        assert_parse_error(
-            "graph [ node [ id 0 label 0 ] node [ id 1 label 1 ] "
-            "edge [ source 0 target 1 key 0 ] edge [ source 0 target 1 key 0 ]"
-            "multigraph 1 ]"
-        )
-        eg.parse_gml(
-            "graph [ node [ id 0 label 0 ] node [ id 1 label 1 ] "
-            "edge [ source 0 target 1 key 0 ] edge [ source 1 target 0 key 0 ]"
-            "directed 1 multigraph 1 ]"
-        )
-
-        # Tests for string convertible alphanumeric id and label values
-        eg.parse_gml("graph [edge [ source a target a ] node [ id a label b ] ]")
-        eg.parse_gml(
-            "graph [ node [ id n42 label 0 ] node [ id x43 label 1 ]"
-            "edge [ source n42 target x43 key 0 ]"
-            "edge [ source x43 target n42 key 0 ]"
-            "directed 1 multigraph 1 ]"
-        )
-        assert_parse_error(
-            "graph [edge [ source u'u\4200' target u'u\4200' ] "
-            + "node [ id u'u\4200' label b ] ]"
-        )
-
-        def assert_generate_error(*args, **kwargs):
-            pytest.raises(
-                eg.EasyGraphError, lambda: list(eg.generate_gml(*args, **kwargs))
-            )
-
-        G = eg.Graph()
-        G.graph[3] = 3
-        assert_generate_error(G)
-        G = eg.Graph()
-        G.graph["3"] = 3
-        assert_generate_error(G)
-        G = eg.Graph()
-        G.graph["data"] = frozenset([1, 2, 3])
-        assert_generate_error(G, stringizer=literal_stringizer)
-        G = eg.Graph()
-        G.graph["data"] = []
-        assert_generate_error(G)
-        assert_generate_error(G, stringizer=len)
-
-    def test_label_kwarg(self):
-        G = eg.parse_gml(self.simple_data, label="id")
-        assert sorted(G.nodes) == [1, 2, 3]
-        labels = [G.nodes[n]["label"] for n in sorted(G.nodes)]
-        assert labels == ["Node 1", "Node 2", "Node 3"]
-
-        G = eg.parse_gml(self.simple_data, label=None)
-        assert sorted(G.nodes) == [1, 2, 3]
-        labels = [G.nodes[n]["label"] for n in sorted(G.nodes)]
-        assert labels == ["Node 1", "Node 2", "Node 3"]
-
-    def test_outofrange_integers(self):
-        # GML restricts integers to 32 signed bits.
-        # Check that we honor this restriction on export
-        G = eg.Graph()
-        # Test export for numbers that barely fit or don't fit into 32 bits,
-        # and 3 numbers in the middle
-        numbers = {
-            "toosmall": (-(2**31)) - 1,
-            "small": -(2**31),
-            "med1": -4,
-            "med2": 0,
-            "med3": 17,
-            "big": (2**31) - 1,
-            "toobig": 2**31,
-        }
-        G.add_node("Node", **numbers)
-
-        fd, fname = tempfile.mkstemp()
-        try:
-            eg.write_gml(G, fname)
-            # Check that the export wrote the nonfitting numbers as strings
-            G2 = eg.read_gml(fname)
-            for attr, value in G2.nodes["Node"].items():
-                if attr == "toosmall" or attr == "toobig":
-                    assert type(value) == str
-                else:
-                    assert type(value) == int
-        finally:
-            os.close(fd)
-            os.unlink(fname)
-
-
-@contextmanager
-def byte_file():
-    _file_handle = io.BytesIO()
-    yield _file_handle
-    _file_handle.seek(0)
-
-
-class TestPropertyLists:
-    def test_writing_graph_with_multi_element_property_list(self):
-        g = eg.Graph()
-        g.add_node("n1", properties=["element", 0, 1, 2.5, True, False])
-        with byte_file() as f:
-            eg.write_gml(g, f)
-        result = f.read().decode()
-
-        assert result == dedent(
-            """\
-            graph [
-              node [
-                id 0
-                label "n1"
-                properties "element"
-                properties 0
-                properties 1
-                properties 2.5
-                properties 1
-                properties 0
-              ]
-            ]
-        """
-        )
-
-    def test_writing_graph_with_one_element_property_list(self):
-        g = eg.Graph()
-        g.add_node("n1", properties=["element"])
-        with byte_file() as f:
-            eg.write_gml(g, f)
-        result = f.read().decode()
-
-        assert result == dedent(
-            """\
-            graph [
-              node [
-                id 0
-                label "n1"
-                properties "_easygraph_list_start"
-                properties "element"
-              ]
-            ]
-        """
-        )
-
-    def test_reading_graph_with_list_property(self):
-        with byte_file() as f:
-            f.write(
-                dedent(
-                    """
-              graph [
-                node [
-                  id 0
-                  label "n1"
-                  properties "element"
-                  properties 0
-                  properties 1
-                  properties 2.5
-                ]
-              ]
-            """
-                ).encode("ascii")
-            )
-            f.seek(0)
-            graph = eg.read_gml(f)
-        assert graph.nodes["n1"] == {"properties": ["element", 0, 1, 2.5]}
-
-    def test_reading_graph_with_single_element_list_property(self):
-        with byte_file() as f:
-            f.write(
-                dedent(
-                    """
-              graph [
-                node [
-                  id 0
-                  label "n1"
-                  properties "_easygraph_list_start"
-                  properties "element"
-                ]
-              ]
-            """
-                ).encode("ascii")
-            )
-            f.seek(0)
-            graph = eg.read_gml(f)
-        assert graph.nodes["n1"] == {"properties": ["element"]}
+import codecs
+import io
+import os
+import tempfile
+
+from ast import literal_eval
+from contextlib import contextmanager
+from textwrap import dedent
+
+import easygraph as eg
+import pytest
+
+from easygraph.readwrite.gml import literal_destringizer
+from easygraph.readwrite.gml import literal_stringizer
+
+
+class TestGraph:
+    @classmethod
+    def setup_class(cls):
+        cls.simple_data = """Creator "me"
+Version "xx"
+graph [
+ comment "This is a sample graph"
+ directed 1
+ IsPlanar 1
+ pos  [ x 0 y 1 ]
+ node [
+   id 1
+   label "Node 1"
+   pos [ x 1 y 1 ]
+ ]
+ node [
+    id 2
+    pos [ x 1 y 2 ]
+    label "Node 2"
+    ]
+  node [
+    id 3
+    label "Node 3"
+    pos [ x 1 y 3 ]
+  ]
+  edge [
+    source 1
+    target 2
+    label "Edge from node 1 to node 2"
+    color [line "blue" thickness 3]
+
+  ]
+  edge [
+    source 2
+    target 3
+    label "Edge from node 2 to node 3"
+  ]
+  edge [
+    source 3
+    target 1
+    label "Edge from node 3 to node 1"
+  ]
+]
+"""
+
+    def test_parse_gml_cytoscape_bug(self):
+        # example from issue #321, originally #324 in trac
+        cytoscape_example = """
+Creator "Cytoscape"
+Version 1.0
+graph   [
+    node    [
+        root_index  -3
+        id  -3
+        graphics    [
+            x   -96.0
+            y   -67.0
+            w   40.0
+            h   40.0
+            fill    "#ff9999"
+            type    "ellipse"
+            outline "#666666"
+            outline_width   1.5
+        ]
+        label   "node2"
+    ]
+    node    [
+        root_index  -2
+        id  -2
+        graphics    [
+            x   63.0
+            y   37.0
+            w   40.0
+            h   40.0
+            fill    "#ff9999"
+            type    "ellipse"
+            outline "#666666"
+            outline_width   1.5
+        ]
+        label   "node1"
+    ]
+    node    [
+        root_index  -1
+        id  -1
+        graphics    [
+            x   -31.0
+            y   -17.0
+            w   40.0
+            h   40.0
+            fill    "#ff9999"
+            type    "ellipse"
+            outline "#666666"
+            outline_width   1.5
+        ]
+        label   "node0"
+    ]
+    edge    [
+        root_index  -2
+        target  -2
+        source  -1
+        graphics    [
+            width   1.5
+            fill    "#0000ff"
+            type    "line"
+            Line    [
+            ]
+            source_arrow    0
+            target_arrow    3
+        ]
+        label   "DirectedEdge"
+    ]
+    edge    [
+        root_index  -1
+        target  -1
+        source  -3
+        graphics    [
+            width   1.5
+            fill    "#0000ff"
+            type    "line"
+            Line    [
+            ]
+            source_arrow    0
+            target_arrow    3
+        ]
+        label   "DirectedEdge"
+    ]
+]
+"""
+        eg.parse_gml(cytoscape_example)
+
+    def test_parse_gml(self):
+        G = eg.parse_gml(self.simple_data, label="label")
+        assert sorted(G.nodes) == ["Node 1", "Node 2", "Node 3"]
+        assert [e[:2] for e in sorted(G.edges)] == [
+            ("Node 1", "Node 2"),
+            ("Node 2", "Node 3"),
+            ("Node 3", "Node 1"),
+        ]
+
+        assert [e for e in sorted(G.edges)] == [
+            (
+                "Node 1",
+                "Node 2",
+                {
+                    "color": {"line": "blue", "thickness": 3},
+                    "label": "Edge from node 1 to node 2",
+                },
+            ),
+            ("Node 2", "Node 3", {"label": "Edge from node 2 to node 3"}),
+            ("Node 3", "Node 1", {"label": "Edge from node 3 to node 1"}),
+        ]
+
+    def test_read_gml(self):
+        (fd, fname) = tempfile.mkstemp()
+        fh = open(fname, "w")
+        fh.write(self.simple_data)
+        fh.close()
+        Gin = eg.read_gml(fname, label="label")
+        G = eg.parse_gml(self.simple_data, label="label")
+        assert sorted(G.nodes) == sorted(Gin.nodes)
+        assert sorted(G.edges) == sorted(Gin.edges)
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_labels_are_strings(self):
+        # GML requires labels to be strings (i.e., in quotes)
+        answer = """graph [
+  node [
+    id 0
+    label "1203"
+  ]
+]"""
+        G = eg.Graph()
+        G.add_node(1203)
+        data = "\n".join(eg.generate_gml(G, stringizer=literal_stringizer))
+        assert data == answer
+
+    def test_relabel_duplicate(self):
+        data = """
+graph
+[
+        label   ""
+        directed        1
+        node
+        [
+                id      0
+                label   "same"
+        ]
+        node
+        [
+                id      1
+                label   "same"
+        ]
+]
+"""
+        fh = io.BytesIO(data.encode("UTF-8"))
+        fh.seek(0)
+        pytest.raises(eg.EasyGraphError, eg.read_gml, fh, label="label")
+
+    def test_quotes(self):
+        G = eg.path_graph(1)
+        G.name = "path_graph(1)"
+        attr = 'This is "quoted" and this is a copyright: ' + chr(169)
+        G.nodes[0]["demo"] = attr
+        fobj = tempfile.NamedTemporaryFile()
+        eg.write_gml(G, fobj)
+        fobj.seek(0)
+        # Should be bytes in 2.x and 3.x
+        data = fobj.read().strip().decode("ascii")
+        answer = """graph [
+  name "path_graph(1)"
+  node [
+    id 0
+    label "0"
+    demo "This is &#34;quoted&#34; and this is a copyright: &#169;"
+  ]
+]"""
+        assert data == answer
+
+    def test_unicode_node(self):
+        node = "node" + chr(169)
+        G = eg.Graph()
+        G.add_node(node)
+        fobj = tempfile.NamedTemporaryFile()
+        eg.write_gml(G, fobj)
+        fobj.seek(0)
+        # Should be bytes in 2.x and 3.x
+        data = fobj.read().strip().decode("ascii")
+        answer = """graph [
+  node [
+    id 0
+    label "node&#169;"
+  ]
+]"""
+        assert data == answer
+
+    def test_float_label(self):
+        node = 1.0
+        G = eg.Graph()
+        G.add_node(node)
+        fobj = tempfile.NamedTemporaryFile()
+        eg.write_gml(G, fobj)
+        fobj.seek(0)
+        # Should be bytes in 2.x and 3.x
+        data = fobj.read().strip().decode("ascii")
+        answer = """graph [
+  node [
+    id 0
+    label "1.0"
+  ]
+]"""
+        assert data == answer
+
+    def test_name(self):
+        G = eg.parse_gml('graph [ name "x" node [ id 0 label "x" ] ]')
+        assert "x" == G.graph["name"]
+        G = eg.parse_gml('graph [ node [ id 0 label "x" ] ]')
+        assert "" == G.name
+        assert "name" not in G.graph
+
+    def test_graph_types(self):
+        for directed in [None, False, True]:
+            for multigraph in [None, False, True]:
+                gml = "graph ["
+                if directed is not None:
+                    gml += " directed " + str(int(directed))
+                if multigraph is not None:
+                    gml += " multigraph " + str(int(multigraph))
+                gml += ' node [ id 0 label "0" ]'
+                gml += " edge [ source 0 target 0 ]"
+                gml += " ]"
+                G = eg.parse_gml(gml)
+                assert bool(directed) == G.is_directed()
+                assert bool(multigraph) == G.is_multigraph()
+                gml = "graph [\n"
+                if directed is True:
+                    gml += "  directed 1\n"
+                if multigraph is True:
+                    gml += "  multigraph 1\n"
+                gml += """  node [
+    id 0
+    label "0"
+  ]
+  edge [
+    source 0
+    target 0
+"""
+                if multigraph:
+                    gml += "    key 0\n"
+                gml += "  ]\n]"
+                assert gml == "\n".join(eg.generate_gml(G))
+
+    def test_data_types(self):
+        data = [
+            True,
+            False,
+            10**20,
+            -2e33,
+            "'",
+            '"&&amp;&&#34;"',
+            [{(b"\xfd",): "\x7f", chr(0x4444): (1, 2)}, (2, "3")],
+        ]
+        data.append(chr(0x14444))
+        data.append(literal_eval("{2.3j, 1 - 2.3j, ()}"))
+        G = eg.Graph()
+        G.name = data
+        G.graph["data"] = data
+        G.add_node(0, int=-1, data=dict(data=data))
+        G.add_edge(0, 0, float=-2.5, data=data)
+        gml = "\n".join(eg.generate_gml(G, stringizer=literal_stringizer))
+        G = eg.parse_gml(gml, destringizer=literal_destringizer)
+        assert data == G.name
+        assert {"name": data, "data": data} == G.graph
+        assert G.nodes == {0: dict(int=-1, data=dict(data=data))}
+        assert list(G.edges) == [(0, 0, dict(float=-2.5, data=data))]
+        G = eg.Graph()
+        G.graph["data"] = "frozenset([1, 2, 3])"
+        G = eg.parse_gml(eg.generate_gml(G), destringizer=literal_eval)
+        assert G.graph["data"] == "frozenset([1, 2, 3])"
+
+    def test_escape_unescape(self):
+        gml = """graph [
+  name "&amp;&#34;&#xf;&#x4444;&#1234567890;&#x1234567890abcdef;&unknown;"
+]"""
+        G = eg.parse_gml(gml)
+        assert (
+            '&"\x0f' + chr(0x4444) + "&#1234567890;&#x1234567890abcdef;&unknown;"
+            == G.name
+        )
+        gml = "\n".join(eg.generate_gml(G))
+        alnu = "#1234567890;&#38;#x1234567890abcdef"
+        answer = (
+            """graph [
+  name "&#38;&#34;&#15;&#17476;&#38;"""
+            + alnu
+            + """;&#38;unknown;"
+]"""
+        )
+        assert answer == gml
+
+    def test_exceptions(self):
+        pytest.raises(ValueError, literal_destringizer, "(")
+        pytest.raises(ValueError, literal_destringizer, "frozenset([1, 2, 3])")
+        pytest.raises(ValueError, literal_destringizer, literal_destringizer)
+        pytest.raises(ValueError, literal_stringizer, frozenset([1, 2, 3]))
+        pytest.raises(ValueError, literal_stringizer, literal_stringizer)
+        with tempfile.TemporaryFile() as f:
+            f.write(codecs.BOM_UTF8 + b"graph[]")
+            f.seek(0)
+            pytest.raises(eg.EasyGraphError, eg.read_gml, f)
+
+        def assert_parse_error(gml):
+            pytest.raises(eg.EasyGraphError, eg.parse_gml, gml)
+
+        assert_parse_error(["graph [\n\n", "]"])
+        assert_parse_error("")
+        assert_parse_error('Creator ""')
+        assert_parse_error("0")
+        assert_parse_error("graph ]")
+        assert_parse_error("graph [ 1 ]")
+        assert_parse_error("graph [ 1.E+2 ]")
+        assert_parse_error('graph [ "A" ]')
+        assert_parse_error("graph [ ] graph ]")
+        assert_parse_error("graph [ ] graph [ ]")
+        assert_parse_error("graph [ data [1, 2, 3] ]")
+        assert_parse_error("graph [ node [ ] ]")
+        assert_parse_error("graph [ node [ id 0 ] ]")
+        eg.parse_gml('graph [ node [ id "a" ] ]', label="id")
+        assert_parse_error("graph [ node [ id 0 label 0 ] node [ id 0 label 1 ] ]")
+        assert_parse_error("graph [ node [ id 0 label 0 ] node [ id 1 label 0 ] ]")
+        assert_parse_error("graph [ node [ id 0 label 0 ] edge [ ] ]")
+        assert_parse_error("graph [ node [ id 0 label 0 ] edge [ source 0 ] ]")
+        eg.parse_gml("graph [edge [ source 0 target 0 ] node [ id 0 label 0 ] ]")
+        assert_parse_error("graph [ node [ id 0 label 0 ] edge [ source 1 target 0 ] ]")
+        assert_parse_error("graph [ node [ id 0 label 0 ] edge [ source 0 target 1 ] ]")
+        assert_parse_error(
+            "graph [ node [ id 0 label 0 ] node [ id 1 label 1 ] "
+            "edge [ source 0 target 1 ] edge [ source 1 target 0 ] ]"
+        )
+        eg.parse_gml(
+            "graph [ node [ id 0 label 0 ] node [ id 1 label 1 ] "
+            "edge [ source 0 target 1 ] edge [ source 1 target 0 ] "
+            "directed 1 ]"
+        )
+        eg.parse_gml(
+            "graph [ node [ id 0 label 0 ] node [ id 1 label 1 ] "
+            "edge [ source 0 target 1 ] edge [ source 0 target 1 ]"
+            "multigraph 1 ]"
+        )
+        eg.parse_gml(
+            "graph [ node [ id 0 label 0 ] node [ id 1 label 1 ] "
+            "edge [ source 0 target 1 key 0 ] edge [ source 0 target 1 ]"
+            "multigraph 1 ]"
+        )
+        assert_parse_error(
+            "graph [ node [ id 0 label 0 ] node [ id 1 label 1 ] "
+            "edge [ source 0 target 1 key 0 ] edge [ source 0 target 1 key 0 ]"
+            "multigraph 1 ]"
+        )
+        eg.parse_gml(
+            "graph [ node [ id 0 label 0 ] node [ id 1 label 1 ] "
+            "edge [ source 0 target 1 key 0 ] edge [ source 1 target 0 key 0 ]"
+            "directed 1 multigraph 1 ]"
+        )
+
+        # Tests for string convertible alphanumeric id and label values
+        eg.parse_gml("graph [edge [ source a target a ] node [ id a label b ] ]")
+        eg.parse_gml(
+            "graph [ node [ id n42 label 0 ] node [ id x43 label 1 ]"
+            "edge [ source n42 target x43 key 0 ]"
+            "edge [ source x43 target n42 key 0 ]"
+            "directed 1 multigraph 1 ]"
+        )
+        assert_parse_error(
+            "graph [edge [ source u'u\4200' target u'u\4200' ] "
+            + "node [ id u'u\4200' label b ] ]"
+        )
+
+        def assert_generate_error(*args, **kwargs):
+            pytest.raises(
+                eg.EasyGraphError, lambda: list(eg.generate_gml(*args, **kwargs))
+            )
+
+        G = eg.Graph()
+        G.graph[3] = 3
+        assert_generate_error(G)
+        G = eg.Graph()
+        G.graph["3"] = 3
+        assert_generate_error(G)
+        G = eg.Graph()
+        G.graph["data"] = frozenset([1, 2, 3])
+        assert_generate_error(G, stringizer=literal_stringizer)
+        G = eg.Graph()
+        G.graph["data"] = []
+        assert_generate_error(G)
+        assert_generate_error(G, stringizer=len)
+
+    def test_label_kwarg(self):
+        G = eg.parse_gml(self.simple_data, label="id")
+        assert sorted(G.nodes) == [1, 2, 3]
+        labels = [G.nodes[n]["label"] for n in sorted(G.nodes)]
+        assert labels == ["Node 1", "Node 2", "Node 3"]
+
+        G = eg.parse_gml(self.simple_data, label=None)
+        assert sorted(G.nodes) == [1, 2, 3]
+        labels = [G.nodes[n]["label"] for n in sorted(G.nodes)]
+        assert labels == ["Node 1", "Node 2", "Node 3"]
+
+    def test_outofrange_integers(self):
+        # GML restricts integers to 32 signed bits.
+        # Check that we honor this restriction on export
+        G = eg.Graph()
+        # Test export for numbers that barely fit or don't fit into 32 bits,
+        # and 3 numbers in the middle
+        numbers = {
+            "toosmall": (-(2**31)) - 1,
+            "small": -(2**31),
+            "med1": -4,
+            "med2": 0,
+            "med3": 17,
+            "big": (2**31) - 1,
+            "toobig": 2**31,
+        }
+        G.add_node("Node", **numbers)
+
+        fd, fname = tempfile.mkstemp()
+        try:
+            eg.write_gml(G, fname)
+            # Check that the export wrote the nonfitting numbers as strings
+            G2 = eg.read_gml(fname)
+            for attr, value in G2.nodes["Node"].items():
+                if attr == "toosmall" or attr == "toobig":
+                    assert type(value) == str
+                else:
+                    assert type(value) == int
+        finally:
+            os.close(fd)
+            os.unlink(fname)
+
+
+@contextmanager
+def byte_file():
+    _file_handle = io.BytesIO()
+    yield _file_handle
+    _file_handle.seek(0)
+
+
+class TestPropertyLists:
+    def test_writing_graph_with_multi_element_property_list(self):
+        g = eg.Graph()
+        g.add_node("n1", properties=["element", 0, 1, 2.5, True, False])
+        with byte_file() as f:
+            eg.write_gml(g, f)
+        result = f.read().decode()
+
+        assert result == dedent(
+            """\
+            graph [
+              node [
+                id 0
+                label "n1"
+                properties "element"
+                properties 0
+                properties 1
+                properties 2.5
+                properties 1
+                properties 0
+              ]
+            ]
+        """
+        )
+
+    def test_writing_graph_with_one_element_property_list(self):
+        g = eg.Graph()
+        g.add_node("n1", properties=["element"])
+        with byte_file() as f:
+            eg.write_gml(g, f)
+        result = f.read().decode()
+
+        assert result == dedent(
+            """\
+            graph [
+              node [
+                id 0
+                label "n1"
+                properties "_easygraph_list_start"
+                properties "element"
+              ]
+            ]
+        """
+        )
+
+    def test_reading_graph_with_list_property(self):
+        with byte_file() as f:
+            f.write(
+                dedent(
+                    """
+              graph [
+                node [
+                  id 0
+                  label "n1"
+                  properties "element"
+                  properties 0
+                  properties 1
+                  properties 2.5
+                ]
+              ]
+            """
+                ).encode("ascii")
+            )
+            f.seek(0)
+            graph = eg.read_gml(f)
+        assert graph.nodes["n1"] == {"properties": ["element", 0, 1, 2.5]}
+
+    def test_reading_graph_with_single_element_list_property(self):
+        with byte_file() as f:
+            f.write(
+                dedent(
+                    """
+              graph [
+                node [
+                  id 0
+                  label "n1"
+                  properties "_easygraph_list_start"
+                  properties "element"
+                ]
+              ]
+            """
+                ).encode("ascii")
+            )
+            f.seek(0)
+            graph = eg.read_gml(f)
+        assert graph.nodes["n1"] == {"properties": ["element"]}
```

## easygraph/readwrite/tests/test_graphviz.py

 * *Ordering differences only*

```diff
@@ -1,58 +1,58 @@
-import os
-import tempfile
-
-import pytest
-
-
-pygraphviz = pytest.importorskip("pygraphviz")
-
-import easygraph as eg
-
-from easygraph.utils import edges_equal
-from easygraph.utils import nodes_equal
-
-
-class TestAGraph:
-    def build_graph(self, G):
-        edges = [("A", "B"), ("A", "C"), ("A", "C"), ("B", "C"), ("A", "D")]
-        G.add_edges_from(edges)
-        G.add_node("E")
-        G.graph["metal"] = "bronze"
-        return G
-
-    def assert_equal(self, G1, G2):
-        assert nodes_equal(G1.nodes, G2.nodes)
-        assert edges_equal(G1.edges, G2.edges)
-        assert G1.graph["metal"] == G2.graph["metal"]
-
-    def agraph_checks(self, G):
-        G = self.build_graph(G)
-        A = eg.to_agraph(G)
-        H = eg.from_agraph(A)
-        self.assert_equal(G, H)
-
-        fd, fname = tempfile.mkstemp()
-        eg.write_dot(H, fname)
-        Hin = eg.read_dot(fname)
-        self.assert_equal(H, Hin)
-        os.close(fd)
-        os.unlink(fname)
-
-        (fd, fname) = tempfile.mkstemp()
-        with open(fname, "w") as fh:
-            eg.write_dot(H, fh)
-
-        with open(fname) as fh:
-            Hin = eg.read_dot(fh)
-        os.close(fd)
-        os.unlink(fname)
-        self.assert_equal(H, Hin)
-
-    def test_from_agraph_name(self):
-        G = eg.Graph(name="test")
-        A = eg.to_agraph(G)
-        H = eg.from_agraph(A)
-        assert G.name == "test"
-
-    def test_undirected(self):
-        self.agraph_checks(eg.Graph())
+import os
+import tempfile
+
+import pytest
+
+
+pygraphviz = pytest.importorskip("pygraphviz")
+
+import easygraph as eg
+
+from easygraph.utils import edges_equal
+from easygraph.utils import nodes_equal
+
+
+class TestAGraph:
+    def build_graph(self, G):
+        edges = [("A", "B"), ("A", "C"), ("A", "C"), ("B", "C"), ("A", "D")]
+        G.add_edges_from(edges)
+        G.add_node("E")
+        G.graph["metal"] = "bronze"
+        return G
+
+    def assert_equal(self, G1, G2):
+        assert nodes_equal(G1.nodes, G2.nodes)
+        assert edges_equal(G1.edges, G2.edges)
+        assert G1.graph["metal"] == G2.graph["metal"]
+
+    def agraph_checks(self, G):
+        G = self.build_graph(G)
+        A = eg.to_agraph(G)
+        H = eg.from_agraph(A)
+        self.assert_equal(G, H)
+
+        fd, fname = tempfile.mkstemp()
+        eg.write_dot(H, fname)
+        Hin = eg.read_dot(fname)
+        self.assert_equal(H, Hin)
+        os.close(fd)
+        os.unlink(fname)
+
+        (fd, fname) = tempfile.mkstemp()
+        with open(fname, "w") as fh:
+            eg.write_dot(H, fh)
+
+        with open(fname) as fh:
+            Hin = eg.read_dot(fh)
+        os.close(fd)
+        os.unlink(fname)
+        self.assert_equal(H, Hin)
+
+    def test_from_agraph_name(self):
+        G = eg.Graph(name="test")
+        A = eg.to_agraph(G)
+        H = eg.from_agraph(A)
+        assert G.name == "test"
+
+    def test_undirected(self):
+        self.agraph_checks(eg.Graph())
```

## easygraph/readwrite/tests/test_gexf.py

 * *Ordering differences only*

```diff
@@ -1,447 +1,447 @@
-import io
-import sys
-import time
-
-import easygraph as eg
-import pytest
-
-
-class TestGEXF:
-    @classmethod
-    def setup_class(cls):
-        cls.simple_directed_data = """<?xml version="1.0" encoding="UTF-8"?>
-<gexf xmlns="http://www.gexf.net/1.2draft" version="1.2">
-    <graph mode="static" defaultedgetype="directed">
-        <nodes>
-            <node id="0" label="Hello" />
-            <node id="1" label="Word" />
-        </nodes>
-        <edges>
-            <edge id="0" source="0" target="1" />
-        </edges>
-    </graph>
-</gexf>
-"""
-        cls.simple_directed_graph = eg.DiGraph()
-        cls.simple_directed_graph.add_node("0", label="Hello")
-        cls.simple_directed_graph.add_node("1", label="World")
-        cls.simple_directed_graph.add_edge("0", "1", id="0")
-
-        cls.simple_directed_fh = io.BytesIO(cls.simple_directed_data.encode("UTF-8"))
-
-        cls.attribute_data = """<?xml version="1.0" encoding="UTF-8"?>\
-<gexf xmlns="http://www.gexf.net/1.2draft" xmlns:xsi="http://www.w3.\
-org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.gexf.net/\
-1.2draft http://www.gexf.net/1.2draft/gexf.xsd" version="1.2">
-  <meta lastmodifieddate="2009-03-20">
-    <creator>Gephi.org</creator>
-    <description>A Web network</description>
-  </meta>
-  <graph defaultedgetype="directed">
-    <attributes class="node">
-      <attribute id="0" title="url" type="string"/>
-      <attribute id="1" title="indegree" type="integer"/>
-      <attribute id="2" title="frog" type="boolean">
-        <default>true</default>
-      </attribute>
-    </attributes>
-    <nodes>
-      <node id="0" label="Gephi">
-        <attvalues>
-          <attvalue for="0" value="https://gephi.org"/>
-          <attvalue for="1" value="1"/>
-          <attvalue for="2" value="false"/>
-        </attvalues>
-      </node>
-      <node id="1" label="Webatlas">
-        <attvalues>
-          <attvalue for="0" value="http://webatlas.fr"/>
-          <attvalue for="1" value="2"/>
-          <attvalue for="2" value="false"/>
-        </attvalues>
-      </node>
-      <node id="2" label="RTGI">
-        <attvalues>
-          <attvalue for="0" value="http://rtgi.fr"/>
-          <attvalue for="1" value="1"/>
-          <attvalue for="2" value="true"/>
-        </attvalues>
-      </node>
-      <node id="3" label="BarabasiLab">
-        <attvalues>
-          <attvalue for="0" value="http://barabasilab.com"/>
-          <attvalue for="1" value="1"/>
-          <attvalue for="2" value="true"/>
-        </attvalues>
-      </node>
-    </nodes>
-    <edges>
-      <edge id="0" source="0" target="1" label="foo"/>
-      <edge id="1" source="0" target="2"/>
-      <edge id="2" source="1" target="0"/>
-      <edge id="3" source="2" target="1"/>
-      <edge id="4" source="0" target="3"/>
-    </edges>
-  </graph>
-</gexf>
-"""
-        cls.attribute_graph = eg.DiGraph()
-        cls.attribute_graph.graph["node_default"] = {"frog": True}
-        cls.attribute_graph.add_node(
-            "0", label="Gephi", url="https://gephi.org", indegree=1, frog=False
-        )
-        cls.attribute_graph.add_node(
-            "1", label="Webatlas", url="http://webatlas.fr", indegree=2, frog=False
-        )
-        cls.attribute_graph.add_node(
-            "2", label="RTGI", url="http://rtgi.fr", indegree=1, frog=True
-        )
-        cls.attribute_graph.add_node(
-            "3",
-            label="BarabasiLab",
-            url="http://barabasilab.com",
-            indegree=1,
-            frog=True,
-        )
-        cls.attribute_graph.add_edge("0", "1", id="0", label="foo")
-        cls.attribute_graph.add_edge("0", "2", id="1")
-        cls.attribute_graph.add_edge("1", "0", id="2")
-        cls.attribute_graph.add_edge("2", "1", id="3")
-        cls.attribute_graph.add_edge("0", "3", id="4")
-        cls.attribute_fh = io.BytesIO(cls.attribute_data.encode("UTF-8"))
-
-        cls.simple_undirected_data = """<?xml version="1.0" encoding="UTF-8"?>
-<gexf xmlns="http://www.gexf.net/1.2draft" version="1.2">
-    <graph mode="static" defaultedgetype="undirected">
-        <nodes>
-            <node id="0" label="Hello" />
-            <node id="1" label="Word" />
-        </nodes>
-        <edges>
-            <edge id="0" source="0" target="1" />
-        </edges>
-    </graph>
-</gexf>
-"""
-        cls.simple_undirected_graph = eg.Graph()
-        cls.simple_undirected_graph.add_node("0", label="Hello")
-        cls.simple_undirected_graph.add_node("1", label="World")
-        cls.simple_undirected_graph.add_edge("0", "1", id="0")
-
-        cls.simple_undirected_fh = io.BytesIO(
-            cls.simple_undirected_data.encode("UTF-8")
-        )
-
-    def test_read_simple_directed_graphml(self):
-        G = self.simple_directed_graph
-        H = eg.read_gexf(self.simple_directed_fh)
-        assert sorted(G.nodes) == sorted(H.nodes)
-        assert sorted(G.edges) == sorted(H.edges)
-        self.simple_directed_fh.seek(0)
-
-    def test_write_read_simple_directed_graphml(self):
-        G = self.simple_directed_graph
-        fh = io.BytesIO()
-        eg.write_gexf(G, fh)
-        fh.seek(0)
-        H = eg.read_gexf(fh)
-        assert sorted(G.nodes) == sorted(H.nodes)
-        assert sorted(G.edges) == sorted(H.edges)
-        self.simple_directed_fh.seek(0)
-
-    def test_read_simple_undirected_graphml(self):
-        G = self.simple_undirected_graph
-        H = eg.read_gexf(self.simple_undirected_fh)
-        assert sorted(G.nodes) == sorted(H.nodes)
-        assert sorted(G.edges) == sorted(H.edges)
-        self.simple_undirected_fh.seek(0)
-
-    def test_read_attribute_graphml(self):
-        G = self.attribute_graph
-        H = eg.read_gexf(self.attribute_fh)
-        assert sorted(G.nodes) == sorted(H.nodes)
-        ge = sorted(G.edges)
-        he = sorted(H.edges)
-        for a, b in zip(ge, he):
-            assert a == b
-        self.attribute_fh.seek(0)
-
-    def test_directed_edge_in_undirected(self):
-        s = """<?xml version="1.0" encoding="UTF-8"?>
-<gexf xmlns="http://www.gexf.net/1.2draft" version='1.2'>
-    <graph mode="static" defaultedgetype="undirected" name="">
-        <nodes>
-            <node id="0" label="Hello" />
-            <node id="1" label="Word" />
-        </nodes>
-        <edges>
-            <edge id="0" source="0" target="1" type="directed"/>
-        </edges>
-    </graph>
-</gexf>
-"""
-        fh = io.BytesIO(s.encode("UTF-8"))
-        pytest.raises(eg.EasyGraphError, eg.read_gexf, fh)
-
-    def test_undirected_edge_in_directed(self):
-        s = """<?xml version="1.0" encoding="UTF-8"?>
-<gexf xmlns="http://www.gexf.net/1.2draft" version='1.2'>
-    <graph mode="static" defaultedgetype="directed" name="">
-        <nodes>
-            <node id="0" label="Hello" />
-            <node id="1" label="Word" />
-        </nodes>
-        <edges>
-            <edge id="0" source="0" target="1" type="undirected"/>
-        </edges>
-    </graph>
-</gexf>
-"""
-        fh = io.BytesIO(s.encode("UTF-8"))
-        pytest.raises(eg.EasyGraphError, eg.read_gexf, fh)
-
-    def test_key_raises(self):
-        s = """<?xml version="1.0" encoding="UTF-8"?>
-<gexf xmlns="http://www.gexf.net/1.2draft" version='1.2'>
-    <graph mode="static" defaultedgetype="directed" name="">
-        <nodes>
-            <node id="0" label="Hello">
-              <attvalues>
-                <attvalue for='0' value='1'/>
-              </attvalues>
-            </node>
-            <node id="1" label="Word" />
-        </nodes>
-        <edges>
-            <edge id="0" source="0" target="1" type="undirected"/>
-        </edges>
-    </graph>
-</gexf>
-"""
-        fh = io.BytesIO(s.encode("UTF-8"))
-        pytest.raises(eg.EasyGraphError, eg.read_gexf, fh)
-
-    def test_relabel(self):
-        s = """<?xml version="1.0" encoding="UTF-8"?>
-<gexf xmlns="http://www.gexf.net/1.2draft" version='1.2'>
-    <graph mode="static" defaultedgetype="directed" name="">
-        <nodes>
-            <node id="0" label="Hello" />
-            <node id="1" label="Word" />
-        </nodes>
-        <edges>
-            <edge id="0" source="0" target="1"/>
-        </edges>
-    </graph>
-</gexf>
-"""
-        fh = io.BytesIO(s.encode("UTF-8"))
-        G = eg.read_gexf(fh, relabel=True)
-        assert sorted(G.nodes) == ["Hello", "Word"]
-
-    def test_default_attribute(self):
-        G = eg.Graph()
-        G.add_node(1, label="1", color="green")
-        eg.add_path(G, [0, 1, 2, 3])
-        G.add_edge(1, 2, foo=3)
-        G.graph["node_default"] = {"color": "yellow"}
-        G.graph["edge_default"] = {"foo": 7}
-        fh = io.BytesIO()
-        eg.write_gexf(G, fh)
-        fh.seek(0)
-        H = eg.read_gexf(fh, node_type=int)
-        assert sorted(G.nodes) == sorted(H.nodes)
-        # Reading a gexf graph always sets mode attribute to either
-        # 'static' or 'dynamic'. Remove the mode attribute from the
-        # read graph for the sake of comparing remaining attributes.
-        del H.graph["mode"]
-        assert G.graph == H.graph
-
-    def test_serialize_ints_to_strings(self):
-        G = eg.Graph()
-        G.add_node(1, id=7, label=77)
-        fh = io.BytesIO()
-        eg.write_gexf(G, fh)
-        fh.seek(0)
-        H = eg.read_gexf(fh, node_type=int)
-        assert list(H) == [7]
-        assert H.nodes[7]["label"] == "77"
-
-    @pytest.mark.skipif(sys.version_info < (3, 8), reason="requires >= python3.8")
-    def test_edge_id_construct(self):
-        G = eg.Graph()
-        G.add_edges_from([(0, 1, {"id": 0}), (1, 2, {"id": 2}), (2, 3)])
-
-        expected = f"""<gexf xmlns="http://www.gexf.net/1.2draft" xmlns:xsi\
-="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.\
-gexf.net/1.2draft http://www.gexf.net/1.2draft/gexf.xsd" version="1.2">
-  <meta lastmodifieddate="{time.strftime('%Y-%m-%d')}">
-    <creator>EasyGraph</creator>
-  </meta>
-  <graph defaultedgetype="undirected" mode="static" name="">
-    <nodes>
-      <node id="0" label="0" />
-      <node id="1" label="1" />
-      <node id="2" label="2" />
-      <node id="3" label="3" />
-    </nodes>
-    <edges>
-      <edge source="0" target="1" id="0" />
-      <edge source="1" target="2" id="2" />
-      <edge source="2" target="3" id="1" />
-    </edges>
-  </graph>
-</gexf>"""
-
-        obtained = "\n".join(eg.generate_gexf(G))
-        assert expected == obtained
-
-    @pytest.mark.skipif(sys.version_info < (3, 8), reason="requires >= python3.8")
-    def test_numpy_type(self):
-        np = pytest.importorskip("numpy")
-        G = eg.path_graph(4)
-        eg.set_node_attributes(G, {n: n for n in np.arange(4)}, "number")
-        G[0][1]["edge-number"] = np.float64(1.1)
-        expected = f"""<gexf xmlns="http://www.gexf.net/1.2draft"\
- xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation\
-="http://www.gexf.net/1.2draft http://www.gexf.net/1.2draft/gexf.xsd"\
- version="1.2">
-  <meta lastmodifieddate="{time.strftime('%Y-%m-%d')}">
-    <creator>EasyGraph</creator>
-  </meta>
-  <graph defaultedgetype="undirected" mode="static" name="">
-    <attributes mode="static" class="edge">
-      <attribute id="1" title="edge-number" type="float" />
-    </attributes>
-    <attributes mode="static" class="node">
-      <attribute id="0" title="number" type="int" />
-    </attributes>
-    <nodes>
-      <node id="0" label="0">
-        <attvalues>
-          <attvalue for="0" value="0" />
-        </attvalues>
-      </node>
-      <node id="1" label="1">
-        <attvalues>
-          <attvalue for="0" value="1" />
-        </attvalues>
-      </node>
-      <node id="2" label="2">
-        <attvalues>
-          <attvalue for="0" value="2" />
-        </attvalues>
-      </node>
-      <node id="3" label="3">
-        <attvalues>
-          <attvalue for="0" value="3" />
-        </attvalues>
-      </node>
-    </nodes>
-    <edges>
-      <edge source="0" target="1" id="0">
-        <attvalues>
-          <attvalue for="1" value="1.1" />
-        </attvalues>
-      </edge>
-      <edge source="1" target="2" id="1" />
-      <edge source="2" target="3" id="2" />
-    </edges>
-  </graph>
-</gexf>"""
-        obtained = "\n".join(eg.generate_gexf(G))
-        assert expected == obtained
-
-    def test_bool(self):
-        G = eg.Graph()
-        G.add_node(1, testattr=True)
-        fh = io.BytesIO()
-        eg.write_gexf(G, fh)
-        fh.seek(0)
-        H = eg.read_gexf(fh, node_type=int)
-        assert H.nodes[1]["testattr"]
-
-    def test_specials(self):
-        from math import isnan
-
-        inf, nan = float("inf"), float("nan")
-        G = eg.Graph()
-        G.add_node(1, testattr=inf, strdata="inf", key="a")
-        G.add_node(2, testattr=nan, strdata="nan", key="b")
-        G.add_node(3, testattr=-inf, strdata="-inf", key="c")
-
-        fh = io.BytesIO()
-        eg.write_gexf(G, fh)
-        fh.seek(0)
-        filetext = fh.read()
-        fh.seek(0)
-        H = eg.read_gexf(fh, node_type=int)
-
-        assert b"INF" in filetext
-        assert b"NaN" in filetext
-        assert b"-INF" in filetext
-
-        assert H.nodes[1]["testattr"] == inf
-        assert isnan(H.nodes[2]["testattr"])
-        assert H.nodes[3]["testattr"] == -inf
-
-        assert H.nodes[1]["strdata"] == "inf"
-        assert H.nodes[2]["strdata"] == "nan"
-        assert H.nodes[3]["strdata"] == "-inf"
-
-        assert H.nodes[1]["easygraph_key"] == "a"
-        assert H.nodes[2]["easygraph_key"] == "b"
-        assert H.nodes[3]["easygraph_key"] == "c"
-
-    def test_simple_list(self):
-        G = eg.Graph()
-        list_value = [(1, 2, 3), (9, 1, 2)]
-        G.add_node(1, key=list_value)
-        fh = io.BytesIO()
-        eg.write_gexf(G, fh)
-        fh.seek(0)
-        H = eg.read_gexf(fh, node_type=int)
-        assert H.nodes[1]["easygraph_key"] == list_value
-
-    def test_dynamic_mode(self):
-        G = eg.Graph()
-        G.add_node(1, label="1", color="green")
-        G.graph["mode"] = "dynamic"
-        fh = io.BytesIO()
-        eg.write_gexf(G, fh)
-        fh.seek(0)
-        H = eg.read_gexf(fh, node_type=int)
-        assert sorted(G.nodes) == sorted(H.nodes)
-        assert sorted(sorted(e) for e in G.edges) == sorted(sorted(e) for e in H.edges)
-
-    def test_slice_and_spell(self):
-        # Test spell first, so version = 1.2
-        G = eg.Graph()
-        G.add_node(0, label="1", color="green")
-        G.nodes[0]["spells"] = [(1, 2)]
-        fh = io.BytesIO()
-        eg.write_gexf(G, fh)
-        fh.seek(0)
-        H = eg.read_gexf(fh, node_type=int)
-        assert sorted(G.nodes) == sorted(H.nodes)
-        assert sorted(sorted(e) for e in G.edges) == sorted(sorted(e) for e in H.edges)
-
-        G = eg.Graph()
-        G.add_node(0, label="1", color="green")
-        G.nodes[0]["slices"] = [(1, 2)]
-        fh = io.BytesIO()
-        eg.write_gexf(G, fh, version="1.1draft")
-        fh.seek(0)
-        H = eg.read_gexf(fh, node_type=int)
-        assert sorted(G.nodes) == sorted(H.nodes)
-        assert sorted(sorted(e) for e in G.edges) == sorted(sorted(e) for e in H.edges)
-
-    def test_add_parent(self):
-        G = eg.Graph()
-        G.add_node(0, label="1", color="green", parents=[1, 2])
-        fh = io.BytesIO()
-        eg.write_gexf(G, fh)
-        fh.seek(0)
-        H = eg.read_gexf(fh, node_type=int)
-        assert sorted(G.nodes) == sorted(H.nodes)
-        assert sorted(sorted(e) for e in G.edges) == sorted(sorted(e) for e in H.edges)
+import io
+import sys
+import time
+
+import easygraph as eg
+import pytest
+
+
+class TestGEXF:
+    @classmethod
+    def setup_class(cls):
+        cls.simple_directed_data = """<?xml version="1.0" encoding="UTF-8"?>
+<gexf xmlns="http://www.gexf.net/1.2draft" version="1.2">
+    <graph mode="static" defaultedgetype="directed">
+        <nodes>
+            <node id="0" label="Hello" />
+            <node id="1" label="Word" />
+        </nodes>
+        <edges>
+            <edge id="0" source="0" target="1" />
+        </edges>
+    </graph>
+</gexf>
+"""
+        cls.simple_directed_graph = eg.DiGraph()
+        cls.simple_directed_graph.add_node("0", label="Hello")
+        cls.simple_directed_graph.add_node("1", label="World")
+        cls.simple_directed_graph.add_edge("0", "1", id="0")
+
+        cls.simple_directed_fh = io.BytesIO(cls.simple_directed_data.encode("UTF-8"))
+
+        cls.attribute_data = """<?xml version="1.0" encoding="UTF-8"?>\
+<gexf xmlns="http://www.gexf.net/1.2draft" xmlns:xsi="http://www.w3.\
+org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.gexf.net/\
+1.2draft http://www.gexf.net/1.2draft/gexf.xsd" version="1.2">
+  <meta lastmodifieddate="2009-03-20">
+    <creator>Gephi.org</creator>
+    <description>A Web network</description>
+  </meta>
+  <graph defaultedgetype="directed">
+    <attributes class="node">
+      <attribute id="0" title="url" type="string"/>
+      <attribute id="1" title="indegree" type="integer"/>
+      <attribute id="2" title="frog" type="boolean">
+        <default>true</default>
+      </attribute>
+    </attributes>
+    <nodes>
+      <node id="0" label="Gephi">
+        <attvalues>
+          <attvalue for="0" value="https://gephi.org"/>
+          <attvalue for="1" value="1"/>
+          <attvalue for="2" value="false"/>
+        </attvalues>
+      </node>
+      <node id="1" label="Webatlas">
+        <attvalues>
+          <attvalue for="0" value="http://webatlas.fr"/>
+          <attvalue for="1" value="2"/>
+          <attvalue for="2" value="false"/>
+        </attvalues>
+      </node>
+      <node id="2" label="RTGI">
+        <attvalues>
+          <attvalue for="0" value="http://rtgi.fr"/>
+          <attvalue for="1" value="1"/>
+          <attvalue for="2" value="true"/>
+        </attvalues>
+      </node>
+      <node id="3" label="BarabasiLab">
+        <attvalues>
+          <attvalue for="0" value="http://barabasilab.com"/>
+          <attvalue for="1" value="1"/>
+          <attvalue for="2" value="true"/>
+        </attvalues>
+      </node>
+    </nodes>
+    <edges>
+      <edge id="0" source="0" target="1" label="foo"/>
+      <edge id="1" source="0" target="2"/>
+      <edge id="2" source="1" target="0"/>
+      <edge id="3" source="2" target="1"/>
+      <edge id="4" source="0" target="3"/>
+    </edges>
+  </graph>
+</gexf>
+"""
+        cls.attribute_graph = eg.DiGraph()
+        cls.attribute_graph.graph["node_default"] = {"frog": True}
+        cls.attribute_graph.add_node(
+            "0", label="Gephi", url="https://gephi.org", indegree=1, frog=False
+        )
+        cls.attribute_graph.add_node(
+            "1", label="Webatlas", url="http://webatlas.fr", indegree=2, frog=False
+        )
+        cls.attribute_graph.add_node(
+            "2", label="RTGI", url="http://rtgi.fr", indegree=1, frog=True
+        )
+        cls.attribute_graph.add_node(
+            "3",
+            label="BarabasiLab",
+            url="http://barabasilab.com",
+            indegree=1,
+            frog=True,
+        )
+        cls.attribute_graph.add_edge("0", "1", id="0", label="foo")
+        cls.attribute_graph.add_edge("0", "2", id="1")
+        cls.attribute_graph.add_edge("1", "0", id="2")
+        cls.attribute_graph.add_edge("2", "1", id="3")
+        cls.attribute_graph.add_edge("0", "3", id="4")
+        cls.attribute_fh = io.BytesIO(cls.attribute_data.encode("UTF-8"))
+
+        cls.simple_undirected_data = """<?xml version="1.0" encoding="UTF-8"?>
+<gexf xmlns="http://www.gexf.net/1.2draft" version="1.2">
+    <graph mode="static" defaultedgetype="undirected">
+        <nodes>
+            <node id="0" label="Hello" />
+            <node id="1" label="Word" />
+        </nodes>
+        <edges>
+            <edge id="0" source="0" target="1" />
+        </edges>
+    </graph>
+</gexf>
+"""
+        cls.simple_undirected_graph = eg.Graph()
+        cls.simple_undirected_graph.add_node("0", label="Hello")
+        cls.simple_undirected_graph.add_node("1", label="World")
+        cls.simple_undirected_graph.add_edge("0", "1", id="0")
+
+        cls.simple_undirected_fh = io.BytesIO(
+            cls.simple_undirected_data.encode("UTF-8")
+        )
+
+    def test_read_simple_directed_graphml(self):
+        G = self.simple_directed_graph
+        H = eg.read_gexf(self.simple_directed_fh)
+        assert sorted(G.nodes) == sorted(H.nodes)
+        assert sorted(G.edges) == sorted(H.edges)
+        self.simple_directed_fh.seek(0)
+
+    def test_write_read_simple_directed_graphml(self):
+        G = self.simple_directed_graph
+        fh = io.BytesIO()
+        eg.write_gexf(G, fh)
+        fh.seek(0)
+        H = eg.read_gexf(fh)
+        assert sorted(G.nodes) == sorted(H.nodes)
+        assert sorted(G.edges) == sorted(H.edges)
+        self.simple_directed_fh.seek(0)
+
+    def test_read_simple_undirected_graphml(self):
+        G = self.simple_undirected_graph
+        H = eg.read_gexf(self.simple_undirected_fh)
+        assert sorted(G.nodes) == sorted(H.nodes)
+        assert sorted(G.edges) == sorted(H.edges)
+        self.simple_undirected_fh.seek(0)
+
+    def test_read_attribute_graphml(self):
+        G = self.attribute_graph
+        H = eg.read_gexf(self.attribute_fh)
+        assert sorted(G.nodes) == sorted(H.nodes)
+        ge = sorted(G.edges)
+        he = sorted(H.edges)
+        for a, b in zip(ge, he):
+            assert a == b
+        self.attribute_fh.seek(0)
+
+    def test_directed_edge_in_undirected(self):
+        s = """<?xml version="1.0" encoding="UTF-8"?>
+<gexf xmlns="http://www.gexf.net/1.2draft" version='1.2'>
+    <graph mode="static" defaultedgetype="undirected" name="">
+        <nodes>
+            <node id="0" label="Hello" />
+            <node id="1" label="Word" />
+        </nodes>
+        <edges>
+            <edge id="0" source="0" target="1" type="directed"/>
+        </edges>
+    </graph>
+</gexf>
+"""
+        fh = io.BytesIO(s.encode("UTF-8"))
+        pytest.raises(eg.EasyGraphError, eg.read_gexf, fh)
+
+    def test_undirected_edge_in_directed(self):
+        s = """<?xml version="1.0" encoding="UTF-8"?>
+<gexf xmlns="http://www.gexf.net/1.2draft" version='1.2'>
+    <graph mode="static" defaultedgetype="directed" name="">
+        <nodes>
+            <node id="0" label="Hello" />
+            <node id="1" label="Word" />
+        </nodes>
+        <edges>
+            <edge id="0" source="0" target="1" type="undirected"/>
+        </edges>
+    </graph>
+</gexf>
+"""
+        fh = io.BytesIO(s.encode("UTF-8"))
+        pytest.raises(eg.EasyGraphError, eg.read_gexf, fh)
+
+    def test_key_raises(self):
+        s = """<?xml version="1.0" encoding="UTF-8"?>
+<gexf xmlns="http://www.gexf.net/1.2draft" version='1.2'>
+    <graph mode="static" defaultedgetype="directed" name="">
+        <nodes>
+            <node id="0" label="Hello">
+              <attvalues>
+                <attvalue for='0' value='1'/>
+              </attvalues>
+            </node>
+            <node id="1" label="Word" />
+        </nodes>
+        <edges>
+            <edge id="0" source="0" target="1" type="undirected"/>
+        </edges>
+    </graph>
+</gexf>
+"""
+        fh = io.BytesIO(s.encode("UTF-8"))
+        pytest.raises(eg.EasyGraphError, eg.read_gexf, fh)
+
+    def test_relabel(self):
+        s = """<?xml version="1.0" encoding="UTF-8"?>
+<gexf xmlns="http://www.gexf.net/1.2draft" version='1.2'>
+    <graph mode="static" defaultedgetype="directed" name="">
+        <nodes>
+            <node id="0" label="Hello" />
+            <node id="1" label="Word" />
+        </nodes>
+        <edges>
+            <edge id="0" source="0" target="1"/>
+        </edges>
+    </graph>
+</gexf>
+"""
+        fh = io.BytesIO(s.encode("UTF-8"))
+        G = eg.read_gexf(fh, relabel=True)
+        assert sorted(G.nodes) == ["Hello", "Word"]
+
+    def test_default_attribute(self):
+        G = eg.Graph()
+        G.add_node(1, label="1", color="green")
+        eg.add_path(G, [0, 1, 2, 3])
+        G.add_edge(1, 2, foo=3)
+        G.graph["node_default"] = {"color": "yellow"}
+        G.graph["edge_default"] = {"foo": 7}
+        fh = io.BytesIO()
+        eg.write_gexf(G, fh)
+        fh.seek(0)
+        H = eg.read_gexf(fh, node_type=int)
+        assert sorted(G.nodes) == sorted(H.nodes)
+        # Reading a gexf graph always sets mode attribute to either
+        # 'static' or 'dynamic'. Remove the mode attribute from the
+        # read graph for the sake of comparing remaining attributes.
+        del H.graph["mode"]
+        assert G.graph == H.graph
+
+    def test_serialize_ints_to_strings(self):
+        G = eg.Graph()
+        G.add_node(1, id=7, label=77)
+        fh = io.BytesIO()
+        eg.write_gexf(G, fh)
+        fh.seek(0)
+        H = eg.read_gexf(fh, node_type=int)
+        assert list(H) == [7]
+        assert H.nodes[7]["label"] == "77"
+
+    @pytest.mark.skipif(sys.version_info < (3, 8), reason="requires >= python3.8")
+    def test_edge_id_construct(self):
+        G = eg.Graph()
+        G.add_edges_from([(0, 1, {"id": 0}), (1, 2, {"id": 2}), (2, 3)])
+
+        expected = f"""<gexf xmlns="http://www.gexf.net/1.2draft" xmlns:xsi\
+="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.\
+gexf.net/1.2draft http://www.gexf.net/1.2draft/gexf.xsd" version="1.2">
+  <meta lastmodifieddate="{time.strftime('%Y-%m-%d')}">
+    <creator>EasyGraph</creator>
+  </meta>
+  <graph defaultedgetype="undirected" mode="static" name="">
+    <nodes>
+      <node id="0" label="0" />
+      <node id="1" label="1" />
+      <node id="2" label="2" />
+      <node id="3" label="3" />
+    </nodes>
+    <edges>
+      <edge source="0" target="1" id="0" />
+      <edge source="1" target="2" id="2" />
+      <edge source="2" target="3" id="1" />
+    </edges>
+  </graph>
+</gexf>"""
+
+        obtained = "\n".join(eg.generate_gexf(G))
+        assert expected == obtained
+
+    @pytest.mark.skipif(sys.version_info < (3, 8), reason="requires >= python3.8")
+    def test_numpy_type(self):
+        np = pytest.importorskip("numpy")
+        G = eg.path_graph(4)
+        eg.set_node_attributes(G, {n: n for n in np.arange(4)}, "number")
+        G[0][1]["edge-number"] = np.float64(1.1)
+        expected = f"""<gexf xmlns="http://www.gexf.net/1.2draft"\
+ xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation\
+="http://www.gexf.net/1.2draft http://www.gexf.net/1.2draft/gexf.xsd"\
+ version="1.2">
+  <meta lastmodifieddate="{time.strftime('%Y-%m-%d')}">
+    <creator>EasyGraph</creator>
+  </meta>
+  <graph defaultedgetype="undirected" mode="static" name="">
+    <attributes mode="static" class="edge">
+      <attribute id="1" title="edge-number" type="float" />
+    </attributes>
+    <attributes mode="static" class="node">
+      <attribute id="0" title="number" type="int" />
+    </attributes>
+    <nodes>
+      <node id="0" label="0">
+        <attvalues>
+          <attvalue for="0" value="0" />
+        </attvalues>
+      </node>
+      <node id="1" label="1">
+        <attvalues>
+          <attvalue for="0" value="1" />
+        </attvalues>
+      </node>
+      <node id="2" label="2">
+        <attvalues>
+          <attvalue for="0" value="2" />
+        </attvalues>
+      </node>
+      <node id="3" label="3">
+        <attvalues>
+          <attvalue for="0" value="3" />
+        </attvalues>
+      </node>
+    </nodes>
+    <edges>
+      <edge source="0" target="1" id="0">
+        <attvalues>
+          <attvalue for="1" value="1.1" />
+        </attvalues>
+      </edge>
+      <edge source="1" target="2" id="1" />
+      <edge source="2" target="3" id="2" />
+    </edges>
+  </graph>
+</gexf>"""
+        obtained = "\n".join(eg.generate_gexf(G))
+        assert expected == obtained
+
+    def test_bool(self):
+        G = eg.Graph()
+        G.add_node(1, testattr=True)
+        fh = io.BytesIO()
+        eg.write_gexf(G, fh)
+        fh.seek(0)
+        H = eg.read_gexf(fh, node_type=int)
+        assert H.nodes[1]["testattr"]
+
+    def test_specials(self):
+        from math import isnan
+
+        inf, nan = float("inf"), float("nan")
+        G = eg.Graph()
+        G.add_node(1, testattr=inf, strdata="inf", key="a")
+        G.add_node(2, testattr=nan, strdata="nan", key="b")
+        G.add_node(3, testattr=-inf, strdata="-inf", key="c")
+
+        fh = io.BytesIO()
+        eg.write_gexf(G, fh)
+        fh.seek(0)
+        filetext = fh.read()
+        fh.seek(0)
+        H = eg.read_gexf(fh, node_type=int)
+
+        assert b"INF" in filetext
+        assert b"NaN" in filetext
+        assert b"-INF" in filetext
+
+        assert H.nodes[1]["testattr"] == inf
+        assert isnan(H.nodes[2]["testattr"])
+        assert H.nodes[3]["testattr"] == -inf
+
+        assert H.nodes[1]["strdata"] == "inf"
+        assert H.nodes[2]["strdata"] == "nan"
+        assert H.nodes[3]["strdata"] == "-inf"
+
+        assert H.nodes[1]["easygraph_key"] == "a"
+        assert H.nodes[2]["easygraph_key"] == "b"
+        assert H.nodes[3]["easygraph_key"] == "c"
+
+    def test_simple_list(self):
+        G = eg.Graph()
+        list_value = [(1, 2, 3), (9, 1, 2)]
+        G.add_node(1, key=list_value)
+        fh = io.BytesIO()
+        eg.write_gexf(G, fh)
+        fh.seek(0)
+        H = eg.read_gexf(fh, node_type=int)
+        assert H.nodes[1]["easygraph_key"] == list_value
+
+    def test_dynamic_mode(self):
+        G = eg.Graph()
+        G.add_node(1, label="1", color="green")
+        G.graph["mode"] = "dynamic"
+        fh = io.BytesIO()
+        eg.write_gexf(G, fh)
+        fh.seek(0)
+        H = eg.read_gexf(fh, node_type=int)
+        assert sorted(G.nodes) == sorted(H.nodes)
+        assert sorted(sorted(e) for e in G.edges) == sorted(sorted(e) for e in H.edges)
+
+    def test_slice_and_spell(self):
+        # Test spell first, so version = 1.2
+        G = eg.Graph()
+        G.add_node(0, label="1", color="green")
+        G.nodes[0]["spells"] = [(1, 2)]
+        fh = io.BytesIO()
+        eg.write_gexf(G, fh)
+        fh.seek(0)
+        H = eg.read_gexf(fh, node_type=int)
+        assert sorted(G.nodes) == sorted(H.nodes)
+        assert sorted(sorted(e) for e in G.edges) == sorted(sorted(e) for e in H.edges)
+
+        G = eg.Graph()
+        G.add_node(0, label="1", color="green")
+        G.nodes[0]["slices"] = [(1, 2)]
+        fh = io.BytesIO()
+        eg.write_gexf(G, fh, version="1.1draft")
+        fh.seek(0)
+        H = eg.read_gexf(fh, node_type=int)
+        assert sorted(G.nodes) == sorted(H.nodes)
+        assert sorted(sorted(e) for e in G.edges) == sorted(sorted(e) for e in H.edges)
+
+    def test_add_parent(self):
+        G = eg.Graph()
+        G.add_node(0, label="1", color="green", parents=[1, 2])
+        fh = io.BytesIO()
+        eg.write_gexf(G, fh)
+        fh.seek(0)
+        H = eg.read_gexf(fh, node_type=int)
+        assert sorted(G.nodes) == sorted(H.nodes)
+        assert sorted(sorted(e) for e in G.edges) == sorted(sorted(e) for e in H.edges)
```

## easygraph/readwrite/tests/test_ucinet.py

```diff
@@ -1,338 +1,340 @@
-"""
-UCINET tests
-"""
-
-import io
-
-import easygraph as eg
-
-from nose import SkipTest
-from nose.tools import *
-
-
-def filterEdges(edges):
-    return [e[:3] for e in edges]
-
-
-class TestUcinet:
-    @classmethod
-    def setup_class(self):
-        self.G = eg.MultiDiGraph()
-        self.G.add_nodes_from(["a", "b", "c", "d", "e"])
-        self.G.add_edges_from(
-            [
-                ("a", "b"),
-                ("a", "c"),
-                ("a", "d"),
-                ("a", "e"),
-                ("b", "a"),
-                ("b", "c"),
-                ("b", "d"),
-                ("c", "a"),
-                ("c", "b"),
-                ("d", "a"),
-                ("d", "b"),
-                ("e", "a"),
-            ]
-        )
-        try:
-            pass
-        except ImportError:
-            raise SkipTest("NumPy not available.")
-
-    def test_generate_ucinet(self):
-        Gout = eg.generate_ucinet(self.G)
-        s = ""
-        for line in Gout:
-            s += line + "\n"
-        G_generated = eg.parse_ucinet(s)
-
-        data = """\
-dl n=5 format=fullmatrix
-labels:
-a,b,c,d,e
-data:
-0 1 1 1 1
-1 0 1 1 0
-1 1 0 0 0
-1 1 0 0 0
-1 0 0 0 0"""
-        G = eg.parse_ucinet(data)
-        assert sorted(G.nodes) == sorted(G_generated.nodes)
-        assert sorted(G.edges) == sorted(G_generated.edges)
-
-    def test_parse_ucinet(self):
-        data = """
-DL N = 5
-Data:
-0 1 1 1 1
-1 0 1 0 0
-1 1 0 0 1
-1 0 0 0 0
-1 0 1 0 0
-        """
-        graph = eg.MultiDiGraph()
-        graph.add_nodes_from([0, 1, 2, 3, 4])
-        graph.add_edges_from(
-            [
-                (0, 1),
-                (0, 2),
-                (0, 3),
-                (0, 4),
-                (1, 0),
-                (1, 2),
-                (2, 0),
-                (2, 1),
-                (2, 4),
-                (3, 0),
-                (4, 0),
-                (4, 2),
-            ]
-        )
-        G = eg.parse_ucinet(data)
-        assert sorted(G.nodes) == sorted(graph.nodes)
-        assert sorted(filterEdges(G.edges)) == sorted(filterEdges(graph.edges))
-        # print [n for n in G.nodes(data=True)]
-        # print [e for e in G.edges]
-
-    def test_parse_ucinet_labels(self):
-        """
-        Test parsing of labels : single line (data1), multiple lines (data2), embedded (data3)
-        Labels must be separated by spaces, carriage returns, equal signs or commas.
-        Labels with embedded spaces are not advisable, but can be entered by
-        surrounding the label in quotes (e.g., "Humpty Dumpty").
-        """
-        data1 = """
-dl n=5
-format = fullmatrix
-labels:
-barry,david,lin,pat,russ
-data:
-0 1 1 1 0
-1 0 0 0 1
-1 0 0 1 0
-1 0 1 0 1
-0 1 0 1 0
-                """
-        data2 = """
-dl n=5
-format = fullmatrix
-labels:
-barry,david
-lin,pat
-russ
-data:
-0 1 1 1 0
-1 0 0 0 1
-1 0 0 1 0
-1 0 1 0 1
-0 1 0 1 0
-        """
-        data3 = """\
-dl n=5
-format = fullmatrix
-labels embedded
-data:
-barry david lin pat russ
-Barry 0 1 1 1 0
-david 1 0 0 0 1
-Lin 1 0 0 1 0
-Pat 1 0 1 0 1
-Russ 0 1 0 1 0
-        """
-        G = eg.MultiDiGraph()
-        G.add_nodes_from(["russ", "barry", "lin", "pat", "david"])
-        G.add_edges_from(
-            [
-                ("russ", "pat"),
-                ("russ", "david"),
-                ("barry", "lin"),
-                ("barry", "pat"),
-                ("barry", "david"),
-                ("lin", "barry"),
-                ("lin", "pat"),
-                ("pat", "barry"),
-                ("pat", "lin"),
-                ("pat", "russ"),
-                ("david", "barry"),
-                ("david", "russ"),
-            ]
-        )
-        G1 = eg.parse_ucinet(data1)
-        G2 = eg.parse_ucinet(data2)
-        G3 = eg.parse_ucinet(data3)
-        assert sorted(G1.nodes) == sorted(G.nodes)
-        assert sorted(G2.nodes) == sorted(G.nodes)
-        assert sorted(G3.nodes) == sorted(G.nodes)
-        assert sorted(e[:3] for e in G1.edges) == sorted(e[:3] for e in G.edges)
-        assert sorted(e[:3] for e in G2.edges) == sorted(e[:3] for e in G.edges)
-        assert sorted(e[:3] for e in G3.edges) == sorted(e[:3] for e in G.edges)
-        # print [n for n in G.nodes]
-        # print [e for e in G.edges]
-
-    def test_parse_ucinet_nodelist1(self):
-        data1 = """
-DL n=4
-format = nodelist1
-data:
-  1  3 2 1
-  4  1 4
-  2  2 4 1
-        """
-        data2 = """
-DL n=4
-format = nodelist1b
-data:
-  3  1 2 3
-  3  1 2 4
-  0
-  2  1 4
-        """
-        G = eg.MultiDiGraph()
-        G.add_nodes_from([0, 1, 2, 3])
-        G.add_edges_from(
-            [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 3), (3, 0), (3, 3)]
-        )
-        G1 = eg.parse_ucinet(data1)
-        G2 = eg.parse_ucinet(data2)
-        assert sorted(G1.nodes) == sorted(G.nodes)
-        assert sorted(G2.nodes) == sorted(G.nodes)
-        assert sorted(filterEdges(G1.edges)) == sorted(filterEdges(G.edges))
-        assert sorted(filterEdges(G2.edges)) == sorted(filterEdges(G.edges))
-
-    def test_parse_ucinet_nodelist1_labels(self):
-        data1 = """
-DL n=5
-format = nodelist1
-labels:
-george, sally, jim, billy, jane
-data:
-1 2 3
-2 3
-4 1
-5 3
-        """
-        data2 = """
-DL n=5
-format = nodelist1
-labels embedded:
-data:
-george sally jim
-sally jim
-billy george
-jane jim
-        """
-        G = eg.MultiDiGraph()
-        G.add_nodes_from(["george", "sally", "jim", "billy", "jane"])
-        G.add_edges_from(
-            [
-                ("billy", "george"),
-                ("jane", "jim"),
-                ("sally", "jim"),
-                ("george", "jim"),
-                ("george", "sally"),
-            ]
-        )
-        G1 = eg.parse_ucinet(data1)
-        G2 = eg.parse_ucinet(data2)
-        assert sorted(G1.nodes) == sorted(G.nodes)
-        assert sorted(G2.nodes) == sorted(G.nodes)
-        assert sorted(G1.edges) == sorted(G.edges)
-        assert sorted(G2.edges) == sorted(G.edges)
-
-    def test_read_ucinet(self):
-        fh = io.BytesIO()
-        data = """
-DL N = 5
-Data:
-0 1 1 1 1
-1 0 1 0 0
-1 1 0 0 1
-1 0 0 0 0
-1 0 1 0 0
-        """
-        Gin = eg.parse_ucinet(data)
-        fh.write(data.encode("UTF-8"))
-        fh.seek(0)
-        Gout = eg.read_ucinet(fh)
-        assert sorted(Gout.nodes) == sorted(Gin.nodes)
-        assert sorted(e[:3] for e in Gout.edges) == sorted(e[:3] for e in Gin.edges)
-
-    def test_write_ucinet(self):
-        fh = io.BytesIO()
-        data = """\
-dl n=5 format=fullmatrix
-data:
-0 1 1 1 1
-1 0 1 0 0
-1 1 0 0 1
-1 0 0 0 0
-1 0 1 0 0
-"""
-        graph = eg.MultiDiGraph()
-        graph.add_nodes_from([0, 1, 2, 3, 4])
-        graph.add_edges_from(
-            [
-                (0, 1),
-                (0, 2),
-                (0, 3),
-                (0, 4),
-                (1, 0),
-                (1, 2),
-                (2, 0),
-                (2, 1),
-                (2, 4),
-                (3, 0),
-                (4, 0),
-                (4, 2),
-            ]
-        )
-
-        eg.write_ucinet(graph, fh)
-        fh.seek(0)
-        G = eg.parse_ucinet(fh.readlines())
-        assert sorted(G.nodes) == sorted(graph.nodes)
-        assert sorted(e[:3] for e in G.edges) == sorted(e[:3] for e in graph.edges)
-
-    def test_parse_ucinet_edgelist1(self):
-        data1 = """
-DL n=5
-format = edgelist1
-labels:
-george, sally, jim, billy, jane
-data:
-1 2
-1 3
-2 3
-3 1
-5 4
-        """
-        data2 = """
-DL n=5
-format = edgelist1
-labels embedded:
-data:
-george sally
-george jim
-sally jim
-jim george
-jane billy
-        """
-        G = eg.MultiDiGraph()
-        G.add_nodes_from(["george", "sally", "jim", "billy", "jane"])
-        G.add_edges_from(
-            [
-                ("jim", "george"),
-                ("jane", "billy"),
-                ("sally", "jim"),
-                ("george", "jim"),
-                ("george", "sally"),
-            ]
-        )
-
-        G1 = eg.parse_ucinet(data1)
-        G2 = eg.parse_ucinet(data2)
-        assert sorted(G1.nodes) == sorted(G.nodes)
-        assert sorted(G2.nodes) == sorted(G.nodes)
-        assert sorted(G1.edges) == sorted(G.edges)
-        assert sorted(G2.edges) == sorted(G.edges)
+"""
+UCINET tests
+"""
+
+import io
+
+import easygraph as eg
+
+
+# from nose import SkipTest
+# from nose.tools import *
+
+
+def filterEdges(edges):
+    return [e[:3] for e in edges]
+
+
+class TestUcinet:
+    @classmethod
+    def setup_class(self):
+        self.G = eg.MultiDiGraph()
+        self.G.add_nodes_from(["a", "b", "c", "d", "e"])
+        self.G.add_edges_from(
+            [
+                ("a", "b"),
+                ("a", "c"),
+                ("a", "d"),
+                ("a", "e"),
+                ("b", "a"),
+                ("b", "c"),
+                ("b", "d"),
+                ("c", "a"),
+                ("c", "b"),
+                ("d", "a"),
+                ("d", "b"),
+                ("e", "a"),
+            ]
+        )
+        try:
+            pass
+        except ImportError:
+            print("NumPy not available.")
+            # raise SkipTest("NumPy not available.")
+
+    def test_generate_ucinet(self):
+        Gout = eg.generate_ucinet(self.G)
+        s = ""
+        for line in Gout:
+            s += line + "\n"
+        G_generated = eg.parse_ucinet(s)
+
+        data = """\
+dl n=5 format=fullmatrix
+labels:
+a,b,c,d,e
+data:
+0 1 1 1 1
+1 0 1 1 0
+1 1 0 0 0
+1 1 0 0 0
+1 0 0 0 0"""
+        G = eg.parse_ucinet(data)
+        assert sorted(G.nodes) == sorted(G_generated.nodes)
+        assert sorted(G.edges) == sorted(G_generated.edges)
+
+    def test_parse_ucinet(self):
+        data = """
+DL N = 5
+Data:
+0 1 1 1 1
+1 0 1 0 0
+1 1 0 0 1
+1 0 0 0 0
+1 0 1 0 0
+        """
+        graph = eg.MultiDiGraph()
+        graph.add_nodes_from([0, 1, 2, 3, 4])
+        graph.add_edges_from(
+            [
+                (0, 1),
+                (0, 2),
+                (0, 3),
+                (0, 4),
+                (1, 0),
+                (1, 2),
+                (2, 0),
+                (2, 1),
+                (2, 4),
+                (3, 0),
+                (4, 0),
+                (4, 2),
+            ]
+        )
+        G = eg.parse_ucinet(data)
+        assert sorted(G.nodes) == sorted(graph.nodes)
+        assert sorted(filterEdges(G.edges)) == sorted(filterEdges(graph.edges))
+        # print [n for n in G.nodes(data=True)]
+        # print [e for e in G.edges]
+
+    def test_parse_ucinet_labels(self):
+        """
+        Test parsing of labels : single line (data1), multiple lines (data2), embedded (data3)
+        Labels must be separated by spaces, carriage returns, equal signs or commas.
+        Labels with embedded spaces are not advisable, but can be entered by
+        surrounding the label in quotes (e.g., "Humpty Dumpty").
+        """
+        data1 = """
+dl n=5
+format = fullmatrix
+labels:
+barry,david,lin,pat,russ
+data:
+0 1 1 1 0
+1 0 0 0 1
+1 0 0 1 0
+1 0 1 0 1
+0 1 0 1 0
+                """
+        data2 = """
+dl n=5
+format = fullmatrix
+labels:
+barry,david
+lin,pat
+russ
+data:
+0 1 1 1 0
+1 0 0 0 1
+1 0 0 1 0
+1 0 1 0 1
+0 1 0 1 0
+        """
+        data3 = """\
+dl n=5
+format = fullmatrix
+labels embedded
+data:
+barry david lin pat russ
+Barry 0 1 1 1 0
+david 1 0 0 0 1
+Lin 1 0 0 1 0
+Pat 1 0 1 0 1
+Russ 0 1 0 1 0
+        """
+        G = eg.MultiDiGraph()
+        G.add_nodes_from(["russ", "barry", "lin", "pat", "david"])
+        G.add_edges_from(
+            [
+                ("russ", "pat"),
+                ("russ", "david"),
+                ("barry", "lin"),
+                ("barry", "pat"),
+                ("barry", "david"),
+                ("lin", "barry"),
+                ("lin", "pat"),
+                ("pat", "barry"),
+                ("pat", "lin"),
+                ("pat", "russ"),
+                ("david", "barry"),
+                ("david", "russ"),
+            ]
+        )
+        G1 = eg.parse_ucinet(data1)
+        G2 = eg.parse_ucinet(data2)
+        G3 = eg.parse_ucinet(data3)
+        assert sorted(G1.nodes) == sorted(G.nodes)
+        assert sorted(G2.nodes) == sorted(G.nodes)
+        assert sorted(G3.nodes) == sorted(G.nodes)
+        assert sorted(e[:3] for e in G1.edges) == sorted(e[:3] for e in G.edges)
+        assert sorted(e[:3] for e in G2.edges) == sorted(e[:3] for e in G.edges)
+        assert sorted(e[:3] for e in G3.edges) == sorted(e[:3] for e in G.edges)
+        # print [n for n in G.nodes]
+        # print [e for e in G.edges]
+
+    def test_parse_ucinet_nodelist1(self):
+        data1 = """
+DL n=4
+format = nodelist1
+data:
+  1  3 2 1
+  4  1 4
+  2  2 4 1
+        """
+        data2 = """
+DL n=4
+format = nodelist1b
+data:
+  3  1 2 3
+  3  1 2 4
+  0
+  2  1 4
+        """
+        G = eg.MultiDiGraph()
+        G.add_nodes_from([0, 1, 2, 3])
+        G.add_edges_from(
+            [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 3), (3, 0), (3, 3)]
+        )
+        G1 = eg.parse_ucinet(data1)
+        G2 = eg.parse_ucinet(data2)
+        assert sorted(G1.nodes) == sorted(G.nodes)
+        assert sorted(G2.nodes) == sorted(G.nodes)
+        assert sorted(filterEdges(G1.edges)) == sorted(filterEdges(G.edges))
+        assert sorted(filterEdges(G2.edges)) == sorted(filterEdges(G.edges))
+
+    def test_parse_ucinet_nodelist1_labels(self):
+        data1 = """
+DL n=5
+format = nodelist1
+labels:
+george, sally, jim, billy, jane
+data:
+1 2 3
+2 3
+4 1
+5 3
+        """
+        data2 = """
+DL n=5
+format = nodelist1
+labels embedded:
+data:
+george sally jim
+sally jim
+billy george
+jane jim
+        """
+        G = eg.MultiDiGraph()
+        G.add_nodes_from(["george", "sally", "jim", "billy", "jane"])
+        G.add_edges_from(
+            [
+                ("billy", "george"),
+                ("jane", "jim"),
+                ("sally", "jim"),
+                ("george", "jim"),
+                ("george", "sally"),
+            ]
+        )
+        G1 = eg.parse_ucinet(data1)
+        G2 = eg.parse_ucinet(data2)
+        assert sorted(G1.nodes) == sorted(G.nodes)
+        assert sorted(G2.nodes) == sorted(G.nodes)
+        assert sorted(G1.edges) == sorted(G.edges)
+        assert sorted(G2.edges) == sorted(G.edges)
+
+    def test_read_ucinet(self):
+        fh = io.BytesIO()
+        data = """
+DL N = 5
+Data:
+0 1 1 1 1
+1 0 1 0 0
+1 1 0 0 1
+1 0 0 0 0
+1 0 1 0 0
+        """
+        Gin = eg.parse_ucinet(data)
+        fh.write(data.encode("UTF-8"))
+        fh.seek(0)
+        Gout = eg.read_ucinet(fh)
+        assert sorted(Gout.nodes) == sorted(Gin.nodes)
+        assert sorted(e[:3] for e in Gout.edges) == sorted(e[:3] for e in Gin.edges)
+
+    def test_write_ucinet(self):
+        fh = io.BytesIO()
+        data = """\
+dl n=5 format=fullmatrix
+data:
+0 1 1 1 1
+1 0 1 0 0
+1 1 0 0 1
+1 0 0 0 0
+1 0 1 0 0
+"""
+        graph = eg.MultiDiGraph()
+        graph.add_nodes_from([0, 1, 2, 3, 4])
+        graph.add_edges_from(
+            [
+                (0, 1),
+                (0, 2),
+                (0, 3),
+                (0, 4),
+                (1, 0),
+                (1, 2),
+                (2, 0),
+                (2, 1),
+                (2, 4),
+                (3, 0),
+                (4, 0),
+                (4, 2),
+            ]
+        )
+
+        eg.write_ucinet(graph, fh)
+        fh.seek(0)
+        G = eg.parse_ucinet(fh.readlines())
+        assert sorted(G.nodes) == sorted(graph.nodes)
+        assert sorted(e[:3] for e in G.edges) == sorted(e[:3] for e in graph.edges)
+
+    def test_parse_ucinet_edgelist1(self):
+        data1 = """
+DL n=5
+format = edgelist1
+labels:
+george, sally, jim, billy, jane
+data:
+1 2
+1 3
+2 3
+3 1
+5 4
+        """
+        data2 = """
+DL n=5
+format = edgelist1
+labels embedded:
+data:
+george sally
+george jim
+sally jim
+jim george
+jane billy
+        """
+        G = eg.MultiDiGraph()
+        G.add_nodes_from(["george", "sally", "jim", "billy", "jane"])
+        G.add_edges_from(
+            [
+                ("jim", "george"),
+                ("jane", "billy"),
+                ("sally", "jim"),
+                ("george", "jim"),
+                ("george", "sally"),
+            ]
+        )
+
+        G1 = eg.parse_ucinet(data1)
+        G2 = eg.parse_ucinet(data2)
+        assert sorted(G1.nodes) == sorted(G.nodes)
+        assert sorted(G2.nodes) == sorted(G.nodes)
+        assert sorted(G1.edges) == sorted(G.edges)
+        assert sorted(G2.edges) == sorted(G.edges)
```

## easygraph/readwrite/tests/test_pickle.py

 * *Ordering differences only*

```diff
@@ -1,53 +1,53 @@
-#!/usr/bin/env python3
-"""
-pickle read / write tests
-"""
-
-import os
-import pickle
-import tempfile
-
-import easygraph as eg
-
-from easygraph.utils import edges_equal
-
-
-class TestPickle:
-    @classmethod
-    def setup_class(cls):
-        cls.data = """*network Tralala\n*vertices 4\n   1 "A1"         0.0938 0.0896   ellipse x_fact 1 y_fact 1\n   2 "Bb"         0.8188 0.2458   ellipse x_fact 1 y_fact 1\n   3 "C"          0.3688 0.7792   ellipse x_fact 1\n   4 "D2"         0.9583 0.8563   ellipse x_fact 1\n*arcs\n1 1 1  h2 0 w 3 c Blue s 3 a1 -130 k1 0.6 a2 -130 k2 0.6 ap 0.5 l "Bezier loop" lc BlueViolet fos 20 lr 58 lp 0.3 la 360\n2 1 1  h2 0 a1 120 k1 1.3 a2 -120 k2 0.3 ap 25 l "Bezier arc" lphi 270 la 180 lr 19 lp 0.5\n1 2 1  h2 0 a1 40 k1 2.8 a2 30 k2 0.8 ap 25 l "Bezier arc" lphi 90 la 0 lp 0.65\n4 2 -1  h2 0 w 1 k1 -2 k2 250 ap 25 l "Circular arc" c Red lc OrangeRed\n3 4 1  p Dashed h2 0 w 2 c OliveGreen ap 25 l "Straight arc" lc PineGreen\n1 3 1  p Dashed h2 0 w 5 k1 -1 k2 -20 ap 25 l "Oval arc" c Brown lc Black\n3 3 -1  h1 6 w 1 h2 12 k1 -2 k2 -15 ap 0.5 l "Circular loop" c Red lc OrangeRed lphi 270 la 180"""
-        cls.G = eg.MultiDiGraph()
-        cls.G.add_nodes_from(["A1", "Bb", "C", "D2"])
-        cls.G.add_edges_from(
-            [
-                ("A1", "A1"),
-                ("A1", "Bb"),
-                ("A1", "C"),
-                ("Bb", "A1"),
-                ("C", "C"),
-                ("C", "D2"),
-                ("D2", "Bb"),
-            ]
-        )
-
-        cls.G.graph["name"] = "Tralala"
-        (fd, cls.fname) = tempfile.mkstemp()
-        with os.fdopen(fd, "wb") as fh:
-            fh.write(pickle.dumps(cls.G))
-
-    @classmethod
-    def teardown_class(cls):
-        os.unlink(cls.fname)
-
-    def test_read_pickle(self):
-        G = eg.read_pickle(self.fname)
-        assert G.nodes == self.G.nodes
-        assert G.edges == self.G.edges
-
-    def test_write_pickle(self):
-        G = eg.parse_pajek(self.data)
-        eg.write_pickle(self.fname, G)
-        Gin = eg.read_pickle(self.fname)
-        assert sorted(G.nodes) == sorted(Gin.nodes)
-        assert edges_equal(G.edges, Gin.edges)
-        assert self.G.graph == Gin.graph
+#!/usr/bin/env python3
+"""
+pickle read / write tests
+"""
+
+import os
+import pickle
+import tempfile
+
+import easygraph as eg
+
+from easygraph.utils import edges_equal
+
+
+class TestPickle:
+    @classmethod
+    def setup_class(cls):
+        cls.data = """*network Tralala\n*vertices 4\n   1 "A1"         0.0938 0.0896   ellipse x_fact 1 y_fact 1\n   2 "Bb"         0.8188 0.2458   ellipse x_fact 1 y_fact 1\n   3 "C"          0.3688 0.7792   ellipse x_fact 1\n   4 "D2"         0.9583 0.8563   ellipse x_fact 1\n*arcs\n1 1 1  h2 0 w 3 c Blue s 3 a1 -130 k1 0.6 a2 -130 k2 0.6 ap 0.5 l "Bezier loop" lc BlueViolet fos 20 lr 58 lp 0.3 la 360\n2 1 1  h2 0 a1 120 k1 1.3 a2 -120 k2 0.3 ap 25 l "Bezier arc" lphi 270 la 180 lr 19 lp 0.5\n1 2 1  h2 0 a1 40 k1 2.8 a2 30 k2 0.8 ap 25 l "Bezier arc" lphi 90 la 0 lp 0.65\n4 2 -1  h2 0 w 1 k1 -2 k2 250 ap 25 l "Circular arc" c Red lc OrangeRed\n3 4 1  p Dashed h2 0 w 2 c OliveGreen ap 25 l "Straight arc" lc PineGreen\n1 3 1  p Dashed h2 0 w 5 k1 -1 k2 -20 ap 25 l "Oval arc" c Brown lc Black\n3 3 -1  h1 6 w 1 h2 12 k1 -2 k2 -15 ap 0.5 l "Circular loop" c Red lc OrangeRed lphi 270 la 180"""
+        cls.G = eg.MultiDiGraph()
+        cls.G.add_nodes_from(["A1", "Bb", "C", "D2"])
+        cls.G.add_edges_from(
+            [
+                ("A1", "A1"),
+                ("A1", "Bb"),
+                ("A1", "C"),
+                ("Bb", "A1"),
+                ("C", "C"),
+                ("C", "D2"),
+                ("D2", "Bb"),
+            ]
+        )
+
+        cls.G.graph["name"] = "Tralala"
+        (fd, cls.fname) = tempfile.mkstemp()
+        with os.fdopen(fd, "wb") as fh:
+            fh.write(pickle.dumps(cls.G))
+
+    @classmethod
+    def teardown_class(cls):
+        os.unlink(cls.fname)
+
+    def test_read_pickle(self):
+        G = eg.read_pickle(self.fname)
+        assert G.nodes == self.G.nodes
+        assert G.edges == self.G.edges
+
+    def test_write_pickle(self):
+        G = eg.parse_pajek(self.data)
+        eg.write_pickle(self.fname, G)
+        Gin = eg.read_pickle(self.fname)
+        assert sorted(G.nodes) == sorted(Gin.nodes)
+        assert edges_equal(G.edges, Gin.edges)
+        assert self.G.graph == Gin.graph
```

## easygraph/readwrite/tests/test_edgelist.py

 * *Ordering differences only*

```diff
@@ -1,318 +1,318 @@
-"""
-    Unit tests for edgelists.
-"""
-import io
-import os
-import tempfile
-import textwrap
-
-import easygraph as eg
-import pytest
-
-from easygraph.utils import edges_equal
-from easygraph.utils import graphs_equal
-from easygraph.utils import nodes_equal
-
-
-edges_no_data = textwrap.dedent(
-    """
-    # comment line
-    1 2
-    # comment line
-    2 3
-    """
-)
-
-
-edges_with_values = textwrap.dedent(
-    """
-    # comment line
-    1 2 2.0
-    # comment line
-    2 3 3.0
-    """
-)
-
-
-edges_with_weight = textwrap.dedent(
-    """
-    # comment line
-    1 2 {'weight':2.0}
-    # comment line
-    2 3 {'weight':3.0}
-    """
-)
-
-
-edges_with_multiple_attrs = textwrap.dedent(
-    """
-    # comment line
-    1 2 {'weight':2.0, 'color':'green'}
-    # comment line
-    2 3 {'weight':3.0, 'color':'red'}
-    """
-)
-
-
-edges_with_multiple_attrs_csv = textwrap.dedent(
-    """
-    # comment line
-    1, 2, {'weight':2.0, 'color':'green'}
-    # comment line
-    2, 3, {'weight':3.0, 'color':'red'}
-    """
-)
-
-
-_expected_edges_weights = [(1, 2, {"weight": 2.0}), (2, 3, {"weight": 3.0})]
-_expected_edges_multiattr = [
-    (1, 2, {"weight": 2.0, "color": "green"}),
-    (2, 3, {"weight": 3.0, "color": "red"}),
-]
-
-
-@pytest.mark.parametrize(
-    ("data", "extra_kwargs"),
-    (
-        (edges_no_data, {}),
-        (edges_with_values, {}),
-        (edges_with_weight, {}),
-        (edges_with_multiple_attrs, {}),
-        (edges_with_multiple_attrs_csv, {"delimiter": ","}),
-    ),
-)
-def test_read_edgelist_no_data(data, extra_kwargs):
-    bytesIO = io.BytesIO(data.encode("utf-8"))
-    G = eg.read_edgelist(bytesIO, nodetype=int, data=False, **extra_kwargs)
-    assert edges_equal(G.edges, [(1, 2, {}), (2, 3, {})])
-
-
-def test_read_weighted_edgelist():
-    bytesIO = io.BytesIO(edges_with_values.encode("utf-8"))
-    G = eg.read_weighted_edgelist(bytesIO, nodetype=int)
-    assert edges_equal(G.edges, _expected_edges_weights)
-
-
-@pytest.mark.parametrize(
-    ("data", "extra_kwargs", "expected"),
-    (
-        (edges_with_weight, {}, _expected_edges_weights),
-        (edges_with_multiple_attrs, {}, _expected_edges_multiattr),
-        (edges_with_multiple_attrs_csv, {"delimiter": ","}, _expected_edges_multiattr),
-    ),
-)
-def test_read_edgelist_with_data(data, extra_kwargs, expected):
-    bytesIO = io.BytesIO(data.encode("utf-8"))
-    G = eg.read_edgelist(bytesIO, nodetype=int, **extra_kwargs)
-    assert edges_equal(G.edges, expected)
-
-
-@pytest.fixture
-def example_graph():
-    G = eg.Graph()
-    G.add_weighted_edges_from([(1, 2, 3.0), (2, 3, 27.0), (3, 4, 3.0)])
-    return G
-
-
-def test_parse_edgelist_no_data(example_graph):
-    G = example_graph
-    H = eg.parse_edgelist(["1 2", "2 3", "3 4"], nodetype=int)
-    assert nodes_equal(G.nodes, H.nodes)
-    assert edges_equal(G.edges, H.edges, need_data=False)
-
-
-def test_parse_edgelist_with_data_dict(example_graph):
-    G = example_graph
-    H = eg.parse_edgelist(
-        ["1 2 {'weight': 3}", "2 3 {'weight': 27}", "3 4 {'weight': 3.0}"], nodetype=int
-    )
-    assert nodes_equal(G.nodes, H.nodes)
-    assert edges_equal(G.edges, H.edges)
-
-
-def test_parse_edgelist_with_data_list(example_graph):
-    G = example_graph
-    H = eg.parse_edgelist(
-        ["1 2 3", "2 3 27", "3 4 3.0"], nodetype=int, data=(("weight", float),)
-    )
-    assert nodes_equal(G.nodes, H.nodes)
-    assert edges_equal(G.edges, H.edges)
-
-
-def test_parse_edgelist():
-    # ignore lines with less than 2 nodes
-    lines = ["1;2", "2 3", "3 4"]
-    G = eg.parse_edgelist(lines, nodetype=int)
-    # assert list(G.edges) == [(2, 3), (3, 4)]
-    assert edges_equal(G.edges, [(2, 3), (3, 4)], need_data=False)
-    # unknown nodetype
-    with pytest.raises(TypeError, match="Failed to convert nodes"):
-        lines = ["1 2", "2 3", "3 4"]
-        eg.parse_edgelist(lines, nodetype="nope")
-    # lines have invalid edge format
-    with pytest.raises(TypeError, match="Failed to convert edge data"):
-        lines = ["1 2 3", "2 3", "3 4"]
-        eg.parse_edgelist(lines, nodetype=int)
-    # edge data and data_keys not the same length
-    with pytest.raises(IndexError, match="not the same length"):
-        lines = ["1 2 3", "2 3 27", "3 4 3.0"]
-        eg.parse_edgelist(
-            lines, nodetype=int, data=(("weight", float), ("capacity", int))
-        )
-    # edge data can't be converted to edge type
-    with pytest.raises(TypeError, match="Failed to convert"):
-        lines = ["1 2 't1'", "2 3 't3'", "3 4 't3'"]
-        eg.parse_edgelist(lines, nodetype=int, data=(("weight", float),))
-
-
-def test_comments_None():
-    edgelist = ["node#1 node#2", "node#2 node#3"]
-    # comments=None supported to ignore all comment characters
-    G = eg.parse_edgelist(edgelist, comments=None)
-    H = eg.Graph([e.split(" ") for e in edgelist])
-    assert edges_equal(G.edges, H.edges)
-
-
-class TestEdgelist:
-    @classmethod
-    def setup_class(cls):
-        cls.G = eg.Graph(name="test")
-        e = [("a", "b"), ("b", "c"), ("c", "d"), ("d", "e"), ("e", "f"), ("a", "f")]
-        cls.G.add_edges_from(e)
-        cls.G.add_node("g")
-        cls.DG = eg.DiGraph(cls.G)
-        cls.XG = eg.MultiGraph()
-        cls.XG.add_weighted_edges_from([(1, 2, 5), (1, 2, 5), (1, 2, 1), (3, 3, 42)])
-        cls.XDG = eg.MultiDiGraph(cls.XG)
-
-    def test_write_edgelist_1(self):
-        fh = io.BytesIO()
-        G = eg.Graph()
-        G.add_edges_from([(1, 2), (2, 3)])
-        eg.write_edgelist(G, fh, data=False)
-        fh.seek(0)
-        assert fh.read() == b"1 2\n2 3\n"
-
-    def test_write_edgelist_2(self):
-        fh = io.BytesIO()
-        G = eg.Graph()
-        G.add_edges_from([(1, 2), (2, 3)])
-        eg.write_edgelist(G, fh, data=True)
-        fh.seek(0)
-        assert fh.read() == b"1 2 {}\n2 3 {}\n"
-
-    def test_write_edgelist_3(self):
-        fh = io.BytesIO()
-        G = eg.Graph()
-        G.add_edge(1, 2, weight=2.0)
-        G.add_edge(2, 3, weight=3.0)
-        eg.write_edgelist(G, fh, data=True)
-        fh.seek(0)
-        assert fh.read() == b"1 2 {'weight': 2.0}\n2 3 {'weight': 3.0}\n"
-
-    def test_write_edgelist_4(self):
-        fh = io.BytesIO()
-        G = eg.Graph()
-        G.add_edge(1, 2, weight=2.0)
-        G.add_edge(2, 3, weight=3.0)
-        eg.write_edgelist(G, fh, data=["weight"])
-        fh.seek(0)
-        assert fh.read() == b"1 2 2.0\n2 3 3.0\n"
-
-    def test_unicode(self):
-        G = eg.Graph()
-        name1 = chr(2344) + chr(123) + chr(6543)
-        name2 = chr(5543) + chr(1543) + chr(324)
-        G.add_edge(name1, "Radiohead", **{name2: 3})
-        fd, fname = tempfile.mkstemp()
-        eg.write_edgelist(G, fname)
-        H = eg.read_edgelist(fname)
-        assert graphs_equal(G, H)
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_latin1_issue(self):
-        G = eg.Graph()
-        name1 = chr(2344) + chr(123) + chr(6543)
-        name2 = chr(5543) + chr(1543) + chr(324)
-        G.add_edge(name1, "Radiohead", **{name2: 3})
-        fd, fname = tempfile.mkstemp()
-        pytest.raises(
-            UnicodeEncodeError, eg.write_edgelist, G, fname, encoding="latin-1"
-        )
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_latin1(self):
-        G = eg.Graph()
-        name1 = "Bj" + chr(246) + "rk"
-        name2 = chr(220) + "ber"
-        G.add_edge(name1, "Radiohead", **{name2: 3})
-        fd, fname = tempfile.mkstemp()
-        eg.write_edgelist(G, fname, encoding="latin-1")
-        H = eg.read_edgelist(fname, encoding="latin-1")
-        assert graphs_equal(G, H)
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_edgelist_graph(self):
-        G = self.G
-        (fd, fname) = tempfile.mkstemp()
-        eg.write_edgelist(G, fname)
-        H = eg.read_edgelist(fname)
-        H2 = eg.read_edgelist(fname)
-        assert H is not H2  # they should be different graphs
-        G.remove_node("g")  # isolated nodes are not written in edgelist
-        assert nodes_equal(list(H), list(G))
-        assert edges_equal(list(H.edges), list(G.edges))
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_edgelist_digraph(self):
-        G = self.DG
-        (fd, fname) = tempfile.mkstemp()
-        eg.write_edgelist(G, fname)
-        H = eg.read_edgelist(fname, create_using=eg.DiGraph())
-        H2 = eg.read_edgelist(fname, create_using=eg.DiGraph())
-        assert H is not H2  # they should be different graphs
-        G.remove_node("g")  # isolated nodes are not written in edgelist
-        assert nodes_equal(list(H), list(G))
-        assert edges_equal(list(H.edges), list(G.edges))
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_edgelist_integers(self):
-        G = eg.convert_node_labels_to_integers(self.G)
-        (fd, fname) = tempfile.mkstemp()
-        eg.write_edgelist(G, fname)
-        H = eg.read_edgelist(fname, nodetype=int)
-        # isolated nodes are not written in edgelist
-        G.remove_nodes_from(list(eg.isolates(G)))
-        assert nodes_equal(list(H), list(G))
-        assert edges_equal(list(H.edges), list(G.edges))
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_edgelist_multigraph(self):
-        G = self.XG
-        (fd, fname) = tempfile.mkstemp()
-        eg.write_edgelist(G, fname)
-        H = eg.read_edgelist(fname, nodetype=int, create_using=eg.MultiGraph())
-        H2 = eg.read_edgelist(fname, nodetype=int, create_using=eg.MultiGraph())
-        assert H is not H2  # they should be different graphs
-        assert nodes_equal(list(H), list(G))
-        assert edges_equal(list(H.edges), list(G.edges), need_data=False)
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_edgelist_multidigraph(self):
-        G = self.XDG
-        (fd, fname) = tempfile.mkstemp()
-        eg.write_edgelist(G, fname)
-        H = eg.read_edgelist(fname, nodetype=int, create_using=eg.MultiDiGraph())
-        H2 = eg.read_edgelist(fname, nodetype=int, create_using=eg.MultiDiGraph())
-        assert H is not H2  # they should be different graphs
-        assert nodes_equal(list(H), list(G))
-        assert edges_equal(list(H.edges), list(G.edges), need_data=False)
-        os.close(fd)
-        os.unlink(fname)
+"""
+    Unit tests for edgelists.
+"""
+import io
+import os
+import tempfile
+import textwrap
+
+import easygraph as eg
+import pytest
+
+from easygraph.utils import edges_equal
+from easygraph.utils import graphs_equal
+from easygraph.utils import nodes_equal
+
+
+edges_no_data = textwrap.dedent(
+    """
+    # comment line
+    1 2
+    # comment line
+    2 3
+    """
+)
+
+
+edges_with_values = textwrap.dedent(
+    """
+    # comment line
+    1 2 2.0
+    # comment line
+    2 3 3.0
+    """
+)
+
+
+edges_with_weight = textwrap.dedent(
+    """
+    # comment line
+    1 2 {'weight':2.0}
+    # comment line
+    2 3 {'weight':3.0}
+    """
+)
+
+
+edges_with_multiple_attrs = textwrap.dedent(
+    """
+    # comment line
+    1 2 {'weight':2.0, 'color':'green'}
+    # comment line
+    2 3 {'weight':3.0, 'color':'red'}
+    """
+)
+
+
+edges_with_multiple_attrs_csv = textwrap.dedent(
+    """
+    # comment line
+    1, 2, {'weight':2.0, 'color':'green'}
+    # comment line
+    2, 3, {'weight':3.0, 'color':'red'}
+    """
+)
+
+
+_expected_edges_weights = [(1, 2, {"weight": 2.0}), (2, 3, {"weight": 3.0})]
+_expected_edges_multiattr = [
+    (1, 2, {"weight": 2.0, "color": "green"}),
+    (2, 3, {"weight": 3.0, "color": "red"}),
+]
+
+
+@pytest.mark.parametrize(
+    ("data", "extra_kwargs"),
+    (
+        (edges_no_data, {}),
+        (edges_with_values, {}),
+        (edges_with_weight, {}),
+        (edges_with_multiple_attrs, {}),
+        (edges_with_multiple_attrs_csv, {"delimiter": ","}),
+    ),
+)
+def test_read_edgelist_no_data(data, extra_kwargs):
+    bytesIO = io.BytesIO(data.encode("utf-8"))
+    G = eg.read_edgelist(bytesIO, nodetype=int, data=False, **extra_kwargs)
+    assert edges_equal(G.edges, [(1, 2, {}), (2, 3, {})])
+
+
+def test_read_weighted_edgelist():
+    bytesIO = io.BytesIO(edges_with_values.encode("utf-8"))
+    G = eg.read_weighted_edgelist(bytesIO, nodetype=int)
+    assert edges_equal(G.edges, _expected_edges_weights)
+
+
+@pytest.mark.parametrize(
+    ("data", "extra_kwargs", "expected"),
+    (
+        (edges_with_weight, {}, _expected_edges_weights),
+        (edges_with_multiple_attrs, {}, _expected_edges_multiattr),
+        (edges_with_multiple_attrs_csv, {"delimiter": ","}, _expected_edges_multiattr),
+    ),
+)
+def test_read_edgelist_with_data(data, extra_kwargs, expected):
+    bytesIO = io.BytesIO(data.encode("utf-8"))
+    G = eg.read_edgelist(bytesIO, nodetype=int, **extra_kwargs)
+    assert edges_equal(G.edges, expected)
+
+
+@pytest.fixture
+def example_graph():
+    G = eg.Graph()
+    G.add_weighted_edges_from([(1, 2, 3.0), (2, 3, 27.0), (3, 4, 3.0)])
+    return G
+
+
+def test_parse_edgelist_no_data(example_graph):
+    G = example_graph
+    H = eg.parse_edgelist(["1 2", "2 3", "3 4"], nodetype=int)
+    assert nodes_equal(G.nodes, H.nodes)
+    assert edges_equal(G.edges, H.edges, need_data=False)
+
+
+def test_parse_edgelist_with_data_dict(example_graph):
+    G = example_graph
+    H = eg.parse_edgelist(
+        ["1 2 {'weight': 3}", "2 3 {'weight': 27}", "3 4 {'weight': 3.0}"], nodetype=int
+    )
+    assert nodes_equal(G.nodes, H.nodes)
+    assert edges_equal(G.edges, H.edges)
+
+
+def test_parse_edgelist_with_data_list(example_graph):
+    G = example_graph
+    H = eg.parse_edgelist(
+        ["1 2 3", "2 3 27", "3 4 3.0"], nodetype=int, data=(("weight", float),)
+    )
+    assert nodes_equal(G.nodes, H.nodes)
+    assert edges_equal(G.edges, H.edges)
+
+
+def test_parse_edgelist():
+    # ignore lines with less than 2 nodes
+    lines = ["1;2", "2 3", "3 4"]
+    G = eg.parse_edgelist(lines, nodetype=int)
+    # assert list(G.edges) == [(2, 3), (3, 4)]
+    assert edges_equal(G.edges, [(2, 3), (3, 4)], need_data=False)
+    # unknown nodetype
+    with pytest.raises(TypeError, match="Failed to convert nodes"):
+        lines = ["1 2", "2 3", "3 4"]
+        eg.parse_edgelist(lines, nodetype="nope")
+    # lines have invalid edge format
+    with pytest.raises(TypeError, match="Failed to convert edge data"):
+        lines = ["1 2 3", "2 3", "3 4"]
+        eg.parse_edgelist(lines, nodetype=int)
+    # edge data and data_keys not the same length
+    with pytest.raises(IndexError, match="not the same length"):
+        lines = ["1 2 3", "2 3 27", "3 4 3.0"]
+        eg.parse_edgelist(
+            lines, nodetype=int, data=(("weight", float), ("capacity", int))
+        )
+    # edge data can't be converted to edge type
+    with pytest.raises(TypeError, match="Failed to convert"):
+        lines = ["1 2 't1'", "2 3 't3'", "3 4 't3'"]
+        eg.parse_edgelist(lines, nodetype=int, data=(("weight", float),))
+
+
+def test_comments_None():
+    edgelist = ["node#1 node#2", "node#2 node#3"]
+    # comments=None supported to ignore all comment characters
+    G = eg.parse_edgelist(edgelist, comments=None)
+    H = eg.Graph([e.split(" ") for e in edgelist])
+    assert edges_equal(G.edges, H.edges)
+
+
+class TestEdgelist:
+    @classmethod
+    def setup_class(cls):
+        cls.G = eg.Graph(name="test")
+        e = [("a", "b"), ("b", "c"), ("c", "d"), ("d", "e"), ("e", "f"), ("a", "f")]
+        cls.G.add_edges_from(e)
+        cls.G.add_node("g")
+        cls.DG = eg.DiGraph(cls.G)
+        cls.XG = eg.MultiGraph()
+        cls.XG.add_weighted_edges_from([(1, 2, 5), (1, 2, 5), (1, 2, 1), (3, 3, 42)])
+        cls.XDG = eg.MultiDiGraph(cls.XG)
+
+    def test_write_edgelist_1(self):
+        fh = io.BytesIO()
+        G = eg.Graph()
+        G.add_edges_from([(1, 2), (2, 3)])
+        eg.write_edgelist(G, fh, data=False)
+        fh.seek(0)
+        assert fh.read() == b"1 2\n2 3\n"
+
+    def test_write_edgelist_2(self):
+        fh = io.BytesIO()
+        G = eg.Graph()
+        G.add_edges_from([(1, 2), (2, 3)])
+        eg.write_edgelist(G, fh, data=True)
+        fh.seek(0)
+        assert fh.read() == b"1 2 {}\n2 3 {}\n"
+
+    def test_write_edgelist_3(self):
+        fh = io.BytesIO()
+        G = eg.Graph()
+        G.add_edge(1, 2, weight=2.0)
+        G.add_edge(2, 3, weight=3.0)
+        eg.write_edgelist(G, fh, data=True)
+        fh.seek(0)
+        assert fh.read() == b"1 2 {'weight': 2.0}\n2 3 {'weight': 3.0}\n"
+
+    def test_write_edgelist_4(self):
+        fh = io.BytesIO()
+        G = eg.Graph()
+        G.add_edge(1, 2, weight=2.0)
+        G.add_edge(2, 3, weight=3.0)
+        eg.write_edgelist(G, fh, data=["weight"])
+        fh.seek(0)
+        assert fh.read() == b"1 2 2.0\n2 3 3.0\n"
+
+    def test_unicode(self):
+        G = eg.Graph()
+        name1 = chr(2344) + chr(123) + chr(6543)
+        name2 = chr(5543) + chr(1543) + chr(324)
+        G.add_edge(name1, "Radiohead", **{name2: 3})
+        fd, fname = tempfile.mkstemp()
+        eg.write_edgelist(G, fname)
+        H = eg.read_edgelist(fname)
+        assert graphs_equal(G, H)
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_latin1_issue(self):
+        G = eg.Graph()
+        name1 = chr(2344) + chr(123) + chr(6543)
+        name2 = chr(5543) + chr(1543) + chr(324)
+        G.add_edge(name1, "Radiohead", **{name2: 3})
+        fd, fname = tempfile.mkstemp()
+        pytest.raises(
+            UnicodeEncodeError, eg.write_edgelist, G, fname, encoding="latin-1"
+        )
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_latin1(self):
+        G = eg.Graph()
+        name1 = "Bj" + chr(246) + "rk"
+        name2 = chr(220) + "ber"
+        G.add_edge(name1, "Radiohead", **{name2: 3})
+        fd, fname = tempfile.mkstemp()
+        eg.write_edgelist(G, fname, encoding="latin-1")
+        H = eg.read_edgelist(fname, encoding="latin-1")
+        assert graphs_equal(G, H)
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_edgelist_graph(self):
+        G = self.G
+        (fd, fname) = tempfile.mkstemp()
+        eg.write_edgelist(G, fname)
+        H = eg.read_edgelist(fname)
+        H2 = eg.read_edgelist(fname)
+        assert H is not H2  # they should be different graphs
+        G.remove_node("g")  # isolated nodes are not written in edgelist
+        assert nodes_equal(list(H), list(G))
+        assert edges_equal(list(H.edges), list(G.edges))
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_edgelist_digraph(self):
+        G = self.DG
+        (fd, fname) = tempfile.mkstemp()
+        eg.write_edgelist(G, fname)
+        H = eg.read_edgelist(fname, create_using=eg.DiGraph())
+        H2 = eg.read_edgelist(fname, create_using=eg.DiGraph())
+        assert H is not H2  # they should be different graphs
+        G.remove_node("g")  # isolated nodes are not written in edgelist
+        assert nodes_equal(list(H), list(G))
+        assert edges_equal(list(H.edges), list(G.edges))
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_edgelist_integers(self):
+        G = eg.convert_node_labels_to_integers(self.G)
+        (fd, fname) = tempfile.mkstemp()
+        eg.write_edgelist(G, fname)
+        H = eg.read_edgelist(fname, nodetype=int)
+        # isolated nodes are not written in edgelist
+        G.remove_nodes_from(list(eg.isolates(G)))
+        assert nodes_equal(list(H), list(G))
+        assert edges_equal(list(H.edges), list(G.edges))
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_edgelist_multigraph(self):
+        G = self.XG
+        (fd, fname) = tempfile.mkstemp()
+        eg.write_edgelist(G, fname)
+        H = eg.read_edgelist(fname, nodetype=int, create_using=eg.MultiGraph())
+        H2 = eg.read_edgelist(fname, nodetype=int, create_using=eg.MultiGraph())
+        assert H is not H2  # they should be different graphs
+        assert nodes_equal(list(H), list(G))
+        assert edges_equal(list(H.edges), list(G.edges), need_data=False)
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_edgelist_multidigraph(self):
+        G = self.XDG
+        (fd, fname) = tempfile.mkstemp()
+        eg.write_edgelist(G, fname)
+        H = eg.read_edgelist(fname, nodetype=int, create_using=eg.MultiDiGraph())
+        H2 = eg.read_edgelist(fname, nodetype=int, create_using=eg.MultiDiGraph())
+        assert H is not H2  # they should be different graphs
+        assert nodes_equal(list(H), list(G))
+        assert edges_equal(list(H.edges), list(G.edges), need_data=False)
+        os.close(fd)
+        os.unlink(fname)
```

## easygraph/readwrite/tests/test_pajek.py

 * *Ordering differences only*

```diff
@@ -1,307 +1,307 @@
-# # This file is part of the NetworkX distribution.
-
-# NetworkX is distributed with the 3-clause BSD license.
-
-
-# ::
-#    Copyright (C) 2004-2022, NetworkX Developers
-#    Aric Hagberg <hagberg@lanl.gov>
-#    Dan Schult <dschult@colgate.edu>
-#    Pieter Swart <swart@lanl.gov>
-#    All rights reserved.
-
-#    Redistribution and use in source and binary forms, with or without
-#    modification, are permitted provided that the following conditions are
-#    met:
-
-#      * Redistributions of source code must retain the above copyright
-#        notice, this list of conditions and the following disclaimer.
-
-#      * Redistributions in binary form must reproduce the above
-#        copyright notice, this list of conditions and the following
-#        disclaimer in the documentation and/or other materials provided
-#        with the distribution.
-
-#      * Neither the name of the NetworkX Developers nor the names of its
-#        contributors may be used to endorse or promote products derived
-#        from this software without specific prior written permission.
-
-#    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
-#    "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
-#    LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
-#    A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
-#    OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-#    SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
-#    LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
-#    DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
-#    THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-#    (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-#    OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-"""
-Pajek tests
-"""
-import easygraph as eg
-
-
-print(eg)
-import os
-import tempfile
-
-from easygraph.utils import edges_equal
-from easygraph.utils import nodes_equal
-
-
-# from rich import print
-
-test_parse_pajek_edges = [
-    (
-        "A1",
-        "A1",
-        0,
-        {
-            "weight": 1.0,
-            "h2": "0",
-            "w": "3",
-            "c": "Blue",
-            "s": "3",
-            "a1": "-130",
-            "k1": "0.6",
-            "a2": "-130",
-            "k2": "0.6",
-            "ap": "0.5",
-            "l": "Bezier loop",
-            "lc": "BlueViolet",
-            "fos": "20",
-            "lr": "58",
-            "lp": "0.3",
-            "la": "360",
-        },
-    ),
-    (
-        "A1",
-        "Bb",
-        0,
-        {
-            "weight": 1.0,
-            "h2": "0",
-            "a1": "40",
-            "k1": "2.8",
-            "a2": "30",
-            "k2": "0.8",
-            "ap": "25",
-            "l": "Bezier arc",
-            "lphi": "90",
-            "la": "0",
-            "lp": "0.65",
-        },
-    ),
-    (
-        "A1",
-        "C",
-        0,
-        {
-            "weight": 1.0,
-            "p": "Dashed",
-            "h2": "0",
-            "w": "5",
-            "k1": "-1",
-            "k2": "-20",
-            "ap": "25",
-            "l": "Oval arc",
-            "c": "Brown",
-            "lc": "Black",
-        },
-    ),
-    (
-        "Bb",
-        "A1",
-        0,
-        {
-            "weight": 1.0,
-            "h2": "0",
-            "a1": "120",
-            "k1": "1.3",
-            "a2": "-120",
-            "k2": "0.3",
-            "ap": "25",
-            "l": "Bezier arc",
-            "lphi": "270",
-            "la": "180",
-            "lr": "19",
-            "lp": "0.5",
-        },
-    ),
-    (
-        "C",
-        "D2",
-        0,
-        {
-            "weight": 1.0,
-            "p": "Dashed",
-            "h2": "0",
-            "w": "2",
-            "c": "OliveGreen",
-            "ap": "25",
-            "l": "Straight arc",
-            "lc": "PineGreen",
-        },
-    ),
-    (
-        "C",
-        "C",
-        0,
-        {
-            "weight": -1.0,
-            "h1": "6",
-            "w": "1",
-            "h2": "12",
-            "k1": "-2",
-            "k2": "-15",
-            "ap": "0.5",
-            "l": "Circular loop",
-            "c": "Red",
-            "lc": "OrangeRed",
-            "lphi": "270",
-            "la": "180",
-        },
-    ),
-    (
-        "D2",
-        "Bb",
-        0,
-        {
-            "weight": -1.0,
-            "h2": "0",
-            "w": "1",
-            "k1": "-2",
-            "k2": "250",
-            "ap": "25",
-            "l": "Circular arc",
-            "c": "Red",
-            "lc": "OrangeRed",
-        },
-    ),
-]
-
-
-class TestPajek:
-    @classmethod
-    def setup_class(cls):
-        cls.data = """*network Tralala\n*vertices 4\n   1 "A1"         0.0938 0.0896   ellipse x_fact 1 y_fact 1\n   2 "Bb"         0.8188 0.2458   ellipse x_fact 1 y_fact 1\n   3 "C"          0.3688 0.7792   ellipse x_fact 1\n   4 "D2"         0.9583 0.8563   ellipse x_fact 1\n*arcs\n1 1 1  h2 0 w 3 c Blue s 3 a1 -130 k1 0.6 a2 -130 k2 0.6 ap 0.5 l "Bezier loop" lc BlueViolet fos 20 lr 58 lp 0.3 la 360\n2 1 1  h2 0 a1 120 k1 1.3 a2 -120 k2 0.3 ap 25 l "Bezier arc" lphi 270 la 180 lr 19 lp 0.5\n1 2 1  h2 0 a1 40 k1 2.8 a2 30 k2 0.8 ap 25 l "Bezier arc" lphi 90 la 0 lp 0.65\n4 2 -1  h2 0 w 1 k1 -2 k2 250 ap 25 l "Circular arc" c Red lc OrangeRed\n3 4 1  p Dashed h2 0 w 2 c OliveGreen ap 25 l "Straight arc" lc PineGreen\n1 3 1  p Dashed h2 0 w 5 k1 -1 k2 -20 ap 25 l "Oval arc" c Brown lc Black\n3 3 -1  h1 6 w 1 h2 12 k1 -2 k2 -15 ap 0.5 l "Circular loop" c Red lc OrangeRed lphi 270 la 180"""
-        cls.G = eg.MultiDiGraph()
-        cls.G.add_nodes_from(["A1", "Bb", "C", "D2"])
-        cls.G.add_edges_from(
-            [
-                ("A1", "A1"),
-                ("A1", "Bb"),
-                ("A1", "C"),
-                ("Bb", "A1"),
-                ("C", "C"),
-                ("C", "D2"),
-                ("D2", "Bb"),
-            ]
-        )
-
-        cls.G.graph["name"] = "Tralala"
-        (fd, cls.fname) = tempfile.mkstemp()
-        with os.fdopen(fd, "wb") as fh:
-            fh.write(cls.data.encode("UTF-8"))
-
-    @classmethod
-    def teardown_class(cls):
-        os.unlink(cls.fname)
-
-    def test_parse_pajek_simple(self):
-        # Example without node positions or shape
-        data = """*Vertices 2\n1 "1"\n2 "2"\n*Edges\n1 2\n2 1"""
-        G = eg.parse_pajek(data)
-        assert sorted(G.nodes) == ["1", "2"]
-        assert edges_equal(G.edges, [("1", "2", 0, {}), ("1", "2", 1, {})])
-
-    def test_parse_pajek(self):
-        G = eg.parse_pajek(self.data)
-        assert sorted(G.nodes) == ["A1", "Bb", "C", "D2"]
-        # print(G.edges)
-        assert edges_equal(G.edges, test_parse_pajek_edges)
-
-    def test_parse_pajek_mat(self):
-        data = """*Vertices 3\n1 "one"\n2 "two"\n3 "three"\n*Matrix\n1 1 0\n0 1 0\n0 1 0\n"""
-        G = eg.parse_pajek(data)
-        assert set(G.nodes) == {"one", "two", "three"}
-        assert G.nodes["two"] == {"id": "2"}
-        assert edges_equal(
-            # set(G.edges),
-            G.edges,
-            [
-                ("one", "one", {"weight": 1}),
-                ("one", "two", {"weight": 1}),
-                ("two", "two", {"weight": 1}),
-                ("three", "two", {"weight": 1}),
-            ],
-        )
-
-    def test_read_pajek(self):
-        G = eg.parse_pajek(self.data)
-        Gin = eg.read_pajek(self.fname)
-        assert sorted(G.nodes) == sorted(Gin.nodes)
-        assert edges_equal(G.edges, Gin.edges)
-        assert self.G.graph == Gin.graph
-        for n in G:
-            assert G.nodes[n] == Gin.nodes[n]
-
-    def test_write_pajek(self):
-        import io
-
-        G = eg.parse_pajek(self.data)
-        fh = io.BytesIO()
-        eg.write_pajek(G, fh)
-        fh.seek(0)
-        H = eg.read_pajek(fh)
-        assert nodes_equal(G.nodes, list(H))
-        assert edges_equal(G.edges, list(H.edges))
-        # Graph name is left out for now, therefore it is not tested.
-        # assert_equal(G.graph, H.graph)
-
-    def test_ignored_attribute(self):
-        import io
-
-        G = eg.Graph()
-        fh = io.BytesIO()
-        G.add_node(1, int_attr=1)
-        G.add_node(2, empty_attr="  ")
-        G.add_edge(1, 2, int_attr=2)
-        G.add_edge(2, 3, empty_attr="  ")
-
-        import warnings
-
-        with warnings.catch_warnings(record=True) as w:
-            eg.write_pajek(G, fh)
-            assert len(w) == 4
-
-    def test_noname(self):
-        # Make sure we can parse a line such as:  *network
-        # Issue #952
-        line = "*network\n"
-        other_lines = self.data.split("\n")[1:]
-        data = line + "\n".join(other_lines)
-        G = eg.parse_pajek(data)
-
-    def test_unicode(self):
-        import io
-
-        G = eg.Graph()
-        name1 = chr(2344) + chr(123) + chr(6543)
-        name2 = chr(5543) + chr(1543) + chr(324)
-        G.add_edge(name1, "Radiohead", foo=name2)
-        fh = io.BytesIO()
-        eg.write_pajek(G, fh)
-        fh.seek(0)
-        H = eg.read_pajek(fh)
-        assert nodes_equal(list(G), list(H))
-        # from icecream import ic
-        # ic(G.edges)
-        # ic(H.edges)
-        # ic(G.graph)
-        # ic(H.graph)
-        # assert edges_equal(list(G.edges), list(H.edges))
-        assert G.graph == H.graph
+# # This file is part of the NetworkX distribution.
+
+# NetworkX is distributed with the 3-clause BSD license.
+
+
+# ::
+#    Copyright (C) 2004-2022, NetworkX Developers
+#    Aric Hagberg <hagberg@lanl.gov>
+#    Dan Schult <dschult@colgate.edu>
+#    Pieter Swart <swart@lanl.gov>
+#    All rights reserved.
+
+#    Redistribution and use in source and binary forms, with or without
+#    modification, are permitted provided that the following conditions are
+#    met:
+
+#      * Redistributions of source code must retain the above copyright
+#        notice, this list of conditions and the following disclaimer.
+
+#      * Redistributions in binary form must reproduce the above
+#        copyright notice, this list of conditions and the following
+#        disclaimer in the documentation and/or other materials provided
+#        with the distribution.
+
+#      * Neither the name of the NetworkX Developers nor the names of its
+#        contributors may be used to endorse or promote products derived
+#        from this software without specific prior written permission.
+
+#    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+#    "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+#    LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+#    A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+#    OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#    SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+#    LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+#    DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+#    THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+#    (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+#    OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+"""
+Pajek tests
+"""
+import easygraph as eg
+
+
+print(eg)
+import os
+import tempfile
+
+from easygraph.utils import edges_equal
+from easygraph.utils import nodes_equal
+
+
+# from rich import print
+
+test_parse_pajek_edges = [
+    (
+        "A1",
+        "A1",
+        0,
+        {
+            "weight": 1.0,
+            "h2": "0",
+            "w": "3",
+            "c": "Blue",
+            "s": "3",
+            "a1": "-130",
+            "k1": "0.6",
+            "a2": "-130",
+            "k2": "0.6",
+            "ap": "0.5",
+            "l": "Bezier loop",
+            "lc": "BlueViolet",
+            "fos": "20",
+            "lr": "58",
+            "lp": "0.3",
+            "la": "360",
+        },
+    ),
+    (
+        "A1",
+        "Bb",
+        0,
+        {
+            "weight": 1.0,
+            "h2": "0",
+            "a1": "40",
+            "k1": "2.8",
+            "a2": "30",
+            "k2": "0.8",
+            "ap": "25",
+            "l": "Bezier arc",
+            "lphi": "90",
+            "la": "0",
+            "lp": "0.65",
+        },
+    ),
+    (
+        "A1",
+        "C",
+        0,
+        {
+            "weight": 1.0,
+            "p": "Dashed",
+            "h2": "0",
+            "w": "5",
+            "k1": "-1",
+            "k2": "-20",
+            "ap": "25",
+            "l": "Oval arc",
+            "c": "Brown",
+            "lc": "Black",
+        },
+    ),
+    (
+        "Bb",
+        "A1",
+        0,
+        {
+            "weight": 1.0,
+            "h2": "0",
+            "a1": "120",
+            "k1": "1.3",
+            "a2": "-120",
+            "k2": "0.3",
+            "ap": "25",
+            "l": "Bezier arc",
+            "lphi": "270",
+            "la": "180",
+            "lr": "19",
+            "lp": "0.5",
+        },
+    ),
+    (
+        "C",
+        "D2",
+        0,
+        {
+            "weight": 1.0,
+            "p": "Dashed",
+            "h2": "0",
+            "w": "2",
+            "c": "OliveGreen",
+            "ap": "25",
+            "l": "Straight arc",
+            "lc": "PineGreen",
+        },
+    ),
+    (
+        "C",
+        "C",
+        0,
+        {
+            "weight": -1.0,
+            "h1": "6",
+            "w": "1",
+            "h2": "12",
+            "k1": "-2",
+            "k2": "-15",
+            "ap": "0.5",
+            "l": "Circular loop",
+            "c": "Red",
+            "lc": "OrangeRed",
+            "lphi": "270",
+            "la": "180",
+        },
+    ),
+    (
+        "D2",
+        "Bb",
+        0,
+        {
+            "weight": -1.0,
+            "h2": "0",
+            "w": "1",
+            "k1": "-2",
+            "k2": "250",
+            "ap": "25",
+            "l": "Circular arc",
+            "c": "Red",
+            "lc": "OrangeRed",
+        },
+    ),
+]
+
+
+class TestPajek:
+    @classmethod
+    def setup_class(cls):
+        cls.data = """*network Tralala\n*vertices 4\n   1 "A1"         0.0938 0.0896   ellipse x_fact 1 y_fact 1\n   2 "Bb"         0.8188 0.2458   ellipse x_fact 1 y_fact 1\n   3 "C"          0.3688 0.7792   ellipse x_fact 1\n   4 "D2"         0.9583 0.8563   ellipse x_fact 1\n*arcs\n1 1 1  h2 0 w 3 c Blue s 3 a1 -130 k1 0.6 a2 -130 k2 0.6 ap 0.5 l "Bezier loop" lc BlueViolet fos 20 lr 58 lp 0.3 la 360\n2 1 1  h2 0 a1 120 k1 1.3 a2 -120 k2 0.3 ap 25 l "Bezier arc" lphi 270 la 180 lr 19 lp 0.5\n1 2 1  h2 0 a1 40 k1 2.8 a2 30 k2 0.8 ap 25 l "Bezier arc" lphi 90 la 0 lp 0.65\n4 2 -1  h2 0 w 1 k1 -2 k2 250 ap 25 l "Circular arc" c Red lc OrangeRed\n3 4 1  p Dashed h2 0 w 2 c OliveGreen ap 25 l "Straight arc" lc PineGreen\n1 3 1  p Dashed h2 0 w 5 k1 -1 k2 -20 ap 25 l "Oval arc" c Brown lc Black\n3 3 -1  h1 6 w 1 h2 12 k1 -2 k2 -15 ap 0.5 l "Circular loop" c Red lc OrangeRed lphi 270 la 180"""
+        cls.G = eg.MultiDiGraph()
+        cls.G.add_nodes_from(["A1", "Bb", "C", "D2"])
+        cls.G.add_edges_from(
+            [
+                ("A1", "A1"),
+                ("A1", "Bb"),
+                ("A1", "C"),
+                ("Bb", "A1"),
+                ("C", "C"),
+                ("C", "D2"),
+                ("D2", "Bb"),
+            ]
+        )
+
+        cls.G.graph["name"] = "Tralala"
+        (fd, cls.fname) = tempfile.mkstemp()
+        with os.fdopen(fd, "wb") as fh:
+            fh.write(cls.data.encode("UTF-8"))
+
+    @classmethod
+    def teardown_class(cls):
+        os.unlink(cls.fname)
+
+    def test_parse_pajek_simple(self):
+        # Example without node positions or shape
+        data = """*Vertices 2\n1 "1"\n2 "2"\n*Edges\n1 2\n2 1"""
+        G = eg.parse_pajek(data)
+        assert sorted(G.nodes) == ["1", "2"]
+        assert edges_equal(G.edges, [("1", "2", 0, {}), ("1", "2", 1, {})])
+
+    def test_parse_pajek(self):
+        G = eg.parse_pajek(self.data)
+        assert sorted(G.nodes) == ["A1", "Bb", "C", "D2"]
+        # print(G.edges)
+        assert edges_equal(G.edges, test_parse_pajek_edges)
+
+    def test_parse_pajek_mat(self):
+        data = """*Vertices 3\n1 "one"\n2 "two"\n3 "three"\n*Matrix\n1 1 0\n0 1 0\n0 1 0\n"""
+        G = eg.parse_pajek(data)
+        assert set(G.nodes) == {"one", "two", "three"}
+        assert G.nodes["two"] == {"id": "2"}
+        assert edges_equal(
+            # set(G.edges),
+            G.edges,
+            [
+                ("one", "one", {"weight": 1}),
+                ("one", "two", {"weight": 1}),
+                ("two", "two", {"weight": 1}),
+                ("three", "two", {"weight": 1}),
+            ],
+        )
+
+    def test_read_pajek(self):
+        G = eg.parse_pajek(self.data)
+        Gin = eg.read_pajek(self.fname)
+        assert sorted(G.nodes) == sorted(Gin.nodes)
+        assert edges_equal(G.edges, Gin.edges)
+        assert self.G.graph == Gin.graph
+        for n in G:
+            assert G.nodes[n] == Gin.nodes[n]
+
+    def test_write_pajek(self):
+        import io
+
+        G = eg.parse_pajek(self.data)
+        fh = io.BytesIO()
+        eg.write_pajek(G, fh)
+        fh.seek(0)
+        H = eg.read_pajek(fh)
+        assert nodes_equal(G.nodes, list(H))
+        assert edges_equal(G.edges, list(H.edges))
+        # Graph name is left out for now, therefore it is not tested.
+        # assert_equal(G.graph, H.graph)
+
+    def test_ignored_attribute(self):
+        import io
+
+        G = eg.Graph()
+        fh = io.BytesIO()
+        G.add_node(1, int_attr=1)
+        G.add_node(2, empty_attr="  ")
+        G.add_edge(1, 2, int_attr=2)
+        G.add_edge(2, 3, empty_attr="  ")
+
+        import warnings
+
+        with warnings.catch_warnings(record=True) as w:
+            eg.write_pajek(G, fh)
+            assert len(w) == 4
+
+    def test_noname(self):
+        # Make sure we can parse a line such as:  *network
+        # Issue #952
+        line = "*network\n"
+        other_lines = self.data.split("\n")[1:]
+        data = line + "\n".join(other_lines)
+        G = eg.parse_pajek(data)
+
+    def test_unicode(self):
+        import io
+
+        G = eg.Graph()
+        name1 = chr(2344) + chr(123) + chr(6543)
+        name2 = chr(5543) + chr(1543) + chr(324)
+        G.add_edge(name1, "Radiohead", foo=name2)
+        fh = io.BytesIO()
+        eg.write_pajek(G, fh)
+        fh.seek(0)
+        H = eg.read_pajek(fh)
+        assert nodes_equal(list(G), list(H))
+        # from icecream import ic
+        # ic(G.edges)
+        # ic(H.edges)
+        # ic(G.graph)
+        # ic(H.graph)
+        # assert edges_equal(list(G.edges), list(H.edges))
+        assert G.graph == H.graph
```

## easygraph/readwrite/tests/test_graphml.py

 * *Ordering differences only*

```diff
@@ -1,1489 +1,1489 @@
-import io
-import os
-import tempfile
-
-import easygraph as eg
-import pytest
-
-from easygraph.readwrite.graphml import GraphMLWriter
-from easygraph.utils import edges_equal
-from easygraph.utils import nodes_equal
-
-
-class BaseGraphML:
-    @classmethod
-    def setup_class(cls):
-        cls.simple_directed_data = """<?xml version="1.0" encoding="UTF-8"?>
-<!-- This file was written by the JAVA GraphML Library.-->
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <graph id="G" edgedefault="directed">
-    <node id="n0"/>
-    <node id="n1"/>
-    <node id="n2"/>
-    <node id="n3"/>
-    <node id="n4"/>
-    <node id="n5"/>
-    <node id="n6"/>
-    <node id="n7"/>
-    <node id="n8"/>
-    <node id="n9"/>
-    <node id="n10"/>
-    <edge id="foo" source="n0" target="n2"/>
-    <edge source="n1" target="n2"/>
-    <edge source="n2" target="n3"/>
-    <edge source="n3" target="n5"/>
-    <edge source="n3" target="n4"/>
-    <edge source="n4" target="n6"/>
-    <edge source="n6" target="n5"/>
-    <edge source="n5" target="n7"/>
-    <edge source="n6" target="n8"/>
-    <edge source="n8" target="n7"/>
-    <edge source="n8" target="n9"/>
-  </graph>
-</graphml>"""
-        cls.simple_directed_graph = eg.DiGraph()
-        cls.simple_directed_graph.add_node("n10")
-        cls.simple_directed_graph.add_edge("n0", "n2", id="foo")
-        cls.simple_directed_graph.add_edge("n0", "n2")
-        cls.simple_directed_graph.add_edges_from(
-            [
-                ("n1", "n2"),
-                ("n2", "n3"),
-                ("n3", "n5"),
-                ("n3", "n4"),
-                ("n4", "n6"),
-                ("n6", "n5"),
-                ("n5", "n7"),
-                ("n6", "n8"),
-                ("n8", "n7"),
-                ("n8", "n9"),
-            ]
-        )
-        cls.simple_directed_fh = io.BytesIO(cls.simple_directed_data.encode("UTF-8"))
-
-        cls.attribute_data = """<?xml version="1.0" encoding="UTF-8"?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-      xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-        http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <key id="d0" for="node" attr.name="color" attr.type="string">
-    <default>yellow</default>
-  </key>
-  <key id="d1" for="edge" attr.name="weight" attr.type="double"/>
-  <graph id="G" edgedefault="directed">
-    <node id="n0">
-      <data key="d0">green</data>
-    </node>
-    <node id="n1"/>
-    <node id="n2">
-      <data key="d0">blue</data>
-    </node>
-    <node id="n3">
-      <data key="d0">red</data>
-    </node>
-    <node id="n4"/>
-    <node id="n5">
-      <data key="d0">turquoise</data>
-    </node>
-    <edge id="e0" source="n0" target="n2">
-      <data key="d1">1.0</data>
-    </edge>
-    <edge id="e1" source="n0" target="n1">
-      <data key="d1">1.0</data>
-    </edge>
-    <edge id="e2" source="n1" target="n3">
-      <data key="d1">2.0</data>
-    </edge>
-    <edge id="e3" source="n3" target="n2"/>
-    <edge id="e4" source="n2" target="n4"/>
-    <edge id="e5" source="n3" target="n5"/>
-    <edge id="e6" source="n5" target="n4">
-      <data key="d1">1.1</data>
-    </edge>
-  </graph>
-</graphml>
-"""
-        cls.attribute_graph = eg.DiGraph(id="G")
-        cls.attribute_graph.graph["node_default"] = {"color": "yellow"}
-        cls.attribute_graph.add_node("n0", color="green")
-        cls.attribute_graph.add_node("n2", color="blue")
-        cls.attribute_graph.add_node("n3", color="red")
-        cls.attribute_graph.add_node("n4")
-        cls.attribute_graph.add_node("n5", color="turquoise")
-        cls.attribute_graph.add_edge("n0", "n2", id="e0", weight=1.0)
-        cls.attribute_graph.add_edge("n0", "n1", id="e1", weight=1.0)
-        cls.attribute_graph.add_edge("n1", "n3", id="e2", weight=2.0)
-        cls.attribute_graph.add_edge("n3", "n2", id="e3")
-        cls.attribute_graph.add_edge("n2", "n4", id="e4")
-        cls.attribute_graph.add_edge("n3", "n5", id="e5")
-        cls.attribute_graph.add_edge("n5", "n4", id="e6", weight=1.1)
-        cls.attribute_fh = io.BytesIO(cls.attribute_data.encode("UTF-8"))
-
-        cls.node_attribute_default_data = """<?xml version="1.0" encoding="UTF-8"?>
-        <graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-              xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-              xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-                http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-          <key id="d0" for="node" attr.name="boolean_attribute" attr.type="boolean"><default>false</default></key>
-          <key id="d1" for="node" attr.name="int_attribute" attr.type="int"><default>0</default></key>
-          <key id="d2" for="node" attr.name="long_attribute" attr.type="long"><default>0</default></key>
-          <key id="d3" for="node" attr.name="float_attribute" attr.type="float"><default>0.0</default></key>
-          <key id="d4" for="node" attr.name="double_attribute" attr.type="double"><default>0.0</default></key>
-          <key id="d5" for="node" attr.name="string_attribute" attr.type="string"><default>Foo</default></key>
-          <graph id="G" edgedefault="directed">
-            <node id="n0"/>
-            <node id="n1"/>
-            <edge id="e0" source="n0" target="n1"/>
-          </graph>
-        </graphml>
-        """
-        cls.node_attribute_default_graph = eg.DiGraph(id="G")
-        cls.node_attribute_default_graph.graph["node_default"] = {
-            "boolean_attribute": False,
-            "int_attribute": 0,
-            "long_attribute": 0,
-            "float_attribute": 0.0,
-            "double_attribute": 0.0,
-            "string_attribute": "Foo",
-        }
-        cls.node_attribute_default_graph.add_node("n0")
-        cls.node_attribute_default_graph.add_node("n1")
-        cls.node_attribute_default_graph.add_edge("n0", "n1", id="e0")
-        cls.node_attribute_default_fh = io.BytesIO(
-            cls.node_attribute_default_data.encode("UTF-8")
-        )
-
-        cls.attribute_named_key_ids_data = """<?xml version='1.0' encoding='utf-8'?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <key id="edge_prop" for="edge" attr.name="edge_prop" attr.type="string"/>
-  <key id="prop2" for="node" attr.name="prop2" attr.type="string"/>
-  <key id="prop1" for="node" attr.name="prop1" attr.type="string"/>
-  <graph edgedefault="directed">
-    <node id="0">
-      <data key="prop1">val1</data>
-      <data key="prop2">val2</data>
-    </node>
-    <node id="1">
-      <data key="prop1">val_one</data>
-      <data key="prop2">val2</data>
-    </node>
-    <edge source="0" target="1">
-      <data key="edge_prop">edge_value</data>
-    </edge>
-  </graph>
-</graphml>
-"""
-        cls.attribute_named_key_ids_graph = eg.DiGraph()
-        cls.attribute_named_key_ids_graph.add_node("0", prop1="val1", prop2="val2")
-        cls.attribute_named_key_ids_graph.add_node("1", prop1="val_one", prop2="val2")
-        cls.attribute_named_key_ids_graph.add_edge("0", "1", edge_prop="edge_value")
-        fh = io.BytesIO(cls.attribute_named_key_ids_data.encode("UTF-8"))
-        cls.attribute_named_key_ids_fh = fh
-
-        cls.attribute_numeric_type_data = """<?xml version='1.0' encoding='utf-8'?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <key attr.name="weight" attr.type="double" for="node" id="d1" />
-  <key attr.name="weight" attr.type="double" for="edge" id="d0" />
-  <graph edgedefault="directed">
-    <node id="n0">
-      <data key="d1">1</data>
-    </node>
-    <node id="n1">
-      <data key="d1">2.0</data>
-    </node>
-    <edge source="n0" target="n1">
-      <data key="d0">1</data>
-    </edge>
-    <edge source="n1" target="n0">
-      <data key="d0">k</data>
-    </edge>
-    <edge source="n1" target="n1">
-      <data key="d0">1.0</data>
-    </edge>
-  </graph>
-</graphml>
-"""
-        cls.attribute_numeric_type_graph = eg.DiGraph()
-        cls.attribute_numeric_type_graph.add_node("n0", weight=1)
-        cls.attribute_numeric_type_graph.add_node("n1", weight=2.0)
-        cls.attribute_numeric_type_graph.add_edge("n0", "n1", weight=1)
-        cls.attribute_numeric_type_graph.add_edge("n1", "n1", weight=1.0)
-        fh = io.BytesIO(cls.attribute_numeric_type_data.encode("UTF-8"))
-        cls.attribute_numeric_type_fh = fh
-
-        cls.simple_undirected_data = """<?xml version="1.0" encoding="UTF-8"?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <graph id="G">
-    <node id="n0"/>
-    <node id="n1"/>
-    <node id="n2"/>
-    <node id="n10"/>
-    <edge id="foo" source="n0" target="n2"/>
-    <edge source="n1" target="n2"/>
-    <edge source="n2" target="n3"/>
-  </graph>
-</graphml>"""
-        #    <edge source="n8" target="n10" directed="false"/>
-        cls.simple_undirected_graph = eg.Graph()
-        cls.simple_undirected_graph.add_node("n10")
-        cls.simple_undirected_graph.add_edge("n0", "n2", id="foo")
-        cls.simple_undirected_graph.add_edges_from([("n1", "n2"), ("n2", "n3")])
-        fh = io.BytesIO(cls.simple_undirected_data.encode("UTF-8"))
-        cls.simple_undirected_fh = fh
-
-        cls.undirected_multigraph_data = """<?xml version="1.0" encoding="UTF-8"?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <graph id="G">
-    <node id="n0"/>
-    <node id="n1"/>
-    <node id="n2"/>
-    <node id="n10"/>
-    <edge id="e0" source="n0" target="n2"/>
-    <edge id="e1" source="n1" target="n2"/>
-    <edge id="e2" source="n2" target="n1"/>
-  </graph>
-</graphml>"""
-        cls.undirected_multigraph = eg.MultiGraph()
-        cls.undirected_multigraph.add_node("n10")
-        cls.undirected_multigraph.add_edge("n0", "n2", id="e0")
-        cls.undirected_multigraph.add_edge("n1", "n2", id="e1")
-        cls.undirected_multigraph.add_edge("n2", "n1", id="e2")
-        fh = io.BytesIO(cls.undirected_multigraph_data.encode("UTF-8"))
-        cls.undirected_multigraph_fh = fh
-
-        cls.undirected_multigraph_no_multiedge_data = """<?xml version="1.0" encoding="UTF-8"?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <graph id="G">
-    <node id="n0"/>
-    <node id="n1"/>
-    <node id="n2"/>
-    <node id="n10"/>
-    <edge id="e0" source="n0" target="n2"/>
-    <edge id="e1" source="n1" target="n2"/>
-    <edge id="e2" source="n2" target="n3"/>
-  </graph>
-</graphml>"""
-        cls.undirected_multigraph_no_multiedge = eg.MultiGraph()
-        cls.undirected_multigraph_no_multiedge.add_node("n10")
-        cls.undirected_multigraph_no_multiedge.add_edge("n0", "n2", id="e0")
-        cls.undirected_multigraph_no_multiedge.add_edge("n1", "n2", id="e1")
-        cls.undirected_multigraph_no_multiedge.add_edge("n2", "n3", id="e2")
-        fh = io.BytesIO(cls.undirected_multigraph_no_multiedge_data.encode("UTF-8"))
-        cls.undirected_multigraph_no_multiedge_fh = fh
-
-        cls.multigraph_only_ids_for_multiedges_data = """<?xml version="1.0" encoding="UTF-8"?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <graph id="G">
-    <node id="n0"/>
-    <node id="n1"/>
-    <node id="n2"/>
-    <node id="n10"/>
-    <edge source="n0" target="n2"/>
-    <edge id="e1" source="n1" target="n2"/>
-    <edge id="e2" source="n2" target="n1"/>
-  </graph>
-</graphml>"""
-        cls.multigraph_only_ids_for_multiedges = eg.MultiGraph()
-        cls.multigraph_only_ids_for_multiedges.add_node("n10")
-        cls.multigraph_only_ids_for_multiedges.add_edge("n0", "n2")
-        cls.multigraph_only_ids_for_multiedges.add_edge("n1", "n2", id="e1")
-        cls.multigraph_only_ids_for_multiedges.add_edge("n2", "n1", id="e2")
-        fh = io.BytesIO(cls.multigraph_only_ids_for_multiedges_data.encode("UTF-8"))
-        cls.multigraph_only_ids_for_multiedges_fh = fh
-
-
-class TestReadGraphML(BaseGraphML):
-    def test_read_simple_directed_graphml(self):
-        G = self.simple_directed_graph
-        H = eg.read_graphml(self.simple_directed_fh)
-        assert sorted(G.nodes) == sorted(H.nodes)
-        assert sorted(G.edges) == sorted(H.edges)
-        assert sorted(G.edges) == sorted(H.edges)
-        self.simple_directed_fh.seek(0)
-
-        PG = eg.parse_graphml(self.simple_directed_data)
-        assert sorted(G.nodes) == sorted(PG.nodes)
-        assert sorted(G.edges) == sorted(PG.edges)
-        assert sorted(G.edges) == sorted(PG.edges)
-
-    def test_read_simple_undirected_graphml(self):
-        G = self.simple_undirected_graph
-        H = eg.read_graphml(self.simple_undirected_fh)
-        assert nodes_equal(G.nodes, H.nodes)
-        assert edges_equal(G.edges, H.edges)
-        self.simple_undirected_fh.seek(0)
-
-        PG = eg.parse_graphml(self.simple_undirected_data)
-        assert nodes_equal(G.nodes, PG.nodes)
-        assert edges_equal(G.edges, PG.edges)
-
-    def test_read_undirected_multigraph_graphml(self):
-        G = self.undirected_multigraph
-        H = eg.read_graphml(self.undirected_multigraph_fh)
-        assert nodes_equal(G.nodes, H.nodes)
-        # assert edges_equal(G.edges, H.edges)
-        self.undirected_multigraph_fh.seek(0)
-
-        PG = eg.parse_graphml(self.undirected_multigraph_data)
-        assert nodes_equal(G.nodes, PG.nodes)
-        # assert edges_equal(G.edges, PG.edges)
-
-    def test_read_undirected_multigraph_no_multiedge_graphml(self):
-        G = self.undirected_multigraph_no_multiedge
-        H = eg.read_graphml(self.undirected_multigraph_no_multiedge_fh)
-        assert nodes_equal(G.nodes, H.nodes)
-        # assert edges_equal(G.edges, H.edges)
-        self.undirected_multigraph_no_multiedge_fh.seek(0)
-
-        PG = eg.parse_graphml(self.undirected_multigraph_no_multiedge_data)
-        assert nodes_equal(G.nodes, PG.nodes)
-        # assert edges_equal(G.edges, PG.edges)
-
-    def test_read_undirected_multigraph_only_ids_for_multiedges_graphml(self):
-        G = self.multigraph_only_ids_for_multiedges
-        H = eg.read_graphml(self.multigraph_only_ids_for_multiedges_fh)
-        assert nodes_equal(G.nodes, H.nodes)
-        # assert edges_equal(G.edges, H.edges)
-        self.multigraph_only_ids_for_multiedges_fh.seek(0)
-
-        PG = eg.parse_graphml(self.multigraph_only_ids_for_multiedges_data)
-        assert nodes_equal(G.nodes, PG.nodes)
-        # assert edges_equal(G.edges, PG.edges)
-
-    def test_read_attribute_graphml(self):
-        G = self.attribute_graph
-        H = eg.read_graphml(self.attribute_fh)
-        assert nodes_equal(sorted(G.nodes), sorted(H.nodes))
-        ge = sorted(G.edges)
-        he = sorted(H.edges)
-        for a, b in zip(ge, he):
-            assert a == b
-        self.attribute_fh.seek(0)
-
-        PG = eg.parse_graphml(self.attribute_data)
-        assert sorted(G.nodes) == sorted(PG.nodes)
-        ge = sorted(G.edges)
-        he = sorted(PG.edges)
-        for a, b in zip(ge, he):
-            assert a == b
-
-    def test_node_default_attribute_graphml(self):
-        G = self.node_attribute_default_graph
-        H = eg.read_graphml(self.node_attribute_default_fh)
-        assert G.graph["node_default"] == H.graph["node_default"]
-
-    def test_directed_edge_in_undirected(self):
-        s = """<?xml version="1.0" encoding="UTF-8"?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <graph id="G">
-    <node id="n0"/>
-    <node id="n1"/>
-    <node id="n2"/>
-    <edge source="n0" target="n1"/>
-    <edge source="n1" target="n2" directed='true'/>
-  </graph>
-</graphml>"""
-        fh = io.BytesIO(s.encode("UTF-8"))
-        pytest.raises(eg.EasyGraphError, eg.read_graphml, fh)
-        pytest.raises(eg.EasyGraphError, eg.parse_graphml, s)
-
-    def test_undirected_edge_in_directed(self):
-        s = """<?xml version="1.0" encoding="UTF-8"?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <graph id="G" edgedefault='directed'>
-    <node id="n0"/>
-    <node id="n1"/>
-    <node id="n2"/>
-    <edge source="n0" target="n1"/>
-    <edge source="n1" target="n2" directed='false'/>
-  </graph>
-</graphml>"""
-        fh = io.BytesIO(s.encode("UTF-8"))
-        pytest.raises(eg.EasyGraphError, eg.read_graphml, fh)
-        pytest.raises(eg.EasyGraphError, eg.parse_graphml, s)
-
-    def test_key_raise(self):
-        s = """<?xml version="1.0" encoding="UTF-8"?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <key id="d0" for="node" attr.name="color" attr.type="string">
-    <default>yellow</default>
-  </key>
-  <key id="d1" for="edge" attr.name="weight" attr.type="double"/>
-  <graph id="G" edgedefault="directed">
-    <node id="n0">
-      <data key="d0">green</data>
-    </node>
-    <node id="n1"/>
-    <node id="n2">
-      <data key="d0">blue</data>
-    </node>
-    <edge id="e0" source="n0" target="n2">
-      <data key="d2">1.0</data>
-    </edge>
-  </graph>
-</graphml>
-"""
-        fh = io.BytesIO(s.encode("UTF-8"))
-        pytest.raises(eg.EasyGraphError, eg.read_graphml, fh)
-        pytest.raises(eg.EasyGraphError, eg.parse_graphml, s)
-
-    def test_hyperedge_raise(self):
-        s = """<?xml version="1.0" encoding="UTF-8"?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <key id="d0" for="node" attr.name="color" attr.type="string">
-    <default>yellow</default>
-  </key>
-  <key id="d1" for="edge" attr.name="weight" attr.type="double"/>
-  <graph id="G" edgedefault="directed">
-    <node id="n0">
-      <data key="d0">green</data>
-    </node>
-    <node id="n1"/>
-    <node id="n2">
-      <data key="d0">blue</data>
-    </node>
-    <hyperedge id="e0" source="n0" target="n2">
-       <endpoint node="n0"/>
-       <endpoint node="n1"/>
-       <endpoint node="n2"/>
-    </hyperedge>
-  </graph>
-</graphml>
-"""
-        fh = io.BytesIO(s.encode("UTF-8"))
-        pytest.raises(eg.EasyGraphError, eg.read_graphml, fh)
-        pytest.raises(eg.EasyGraphError, eg.parse_graphml, s)
-
-    def test_multigraph_keys(self):
-        # Test that reading multigraphs uses edge id attributes as keys
-        s = """<?xml version="1.0" encoding="UTF-8"?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <graph id="G" edgedefault="directed">
-    <node id="n0"/>
-    <node id="n1"/>
-    <edge id="e0" source="n0" target="n1"/>
-    <edge id="e1" source="n0" target="n1"/>
-  </graph>
-</graphml>
-"""
-        fh = io.BytesIO(s.encode("UTF-8"))
-        G = eg.read_graphml(fh)
-        expected = [("n0", "n1", "e0"), ("n0", "n1", "e1")]
-        # assert sorted(G.edges) == expected
-        fh.seek(0)
-        H = eg.parse_graphml(s)
-        # assert sorted(H.edges) == expected
-
-    def test_preserve_multi_edge_data(self):
-        """
-        Test that data and keys of edges are preserved on consequent
-        write and reads
-        """
-        G = eg.MultiGraph()
-        G.add_node(1)
-        G.add_node(2)
-        G.add_edges_from(
-            [
-                # edges with no data, no keys:
-                (1, 2),
-                # edges with only data:
-                (1, 2, dict(key="data_key1")),
-                (1, 2, dict(id="data_id2")),
-                (1, 2, dict(key="data_key3", id="data_id3")),
-                # edges with both data and keys:
-                (1, 2, 103, dict(key="data_key4")),
-                (1, 2, 104, dict(id="data_id5")),
-                (1, 2, 105, dict(key="data_key6", id="data_id7")),
-            ]
-        )
-        fh = io.BytesIO()
-        eg.write_graphml(G, fh)
-        fh.seek(0)
-        H = eg.read_graphml(fh, node_type=int)
-        assert edges_equal(G.edges, H.edges)
-        assert G._adj == H._adj
-
-        Gadj = {
-            str(node): {
-                str(nbr): {str(ekey): dd for ekey, dd in key_dict.items()}
-                for nbr, key_dict in nbr_dict.items()
-            }
-            for node, nbr_dict in G._adj.items()
-        }
-        fh.seek(0)
-        HH = eg.read_graphml(fh, node_type=str, edge_key_type=str)
-        assert Gadj == HH._adj
-
-        fh.seek(0)
-        string_fh = fh.read()
-        HH = eg.parse_graphml(string_fh, node_type=str, edge_key_type=str)
-        assert Gadj == HH._adj
-
-    def test_yfiles_extension(self):
-        data = """<?xml version="1.0" encoding="UTF-8" standalone="no"?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xmlns:y="http://www.yworks.com/xml/graphml"
-         xmlns:yed="http://www.yworks.com/xml/yed/3"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <!--Created by yFiles for Java 2.7-->
-  <key for="graphml" id="d0" yfiles.type="resources"/>
-  <key attr.name="url" attr.type="string" for="node" id="d1"/>
-  <key attr.name="description" attr.type="string" for="node" id="d2"/>
-  <key for="node" id="d3" yfiles.type="nodegraphics"/>
-  <key attr.name="Description" attr.type="string" for="graph" id="d4">
-    <default/>
-  </key>
-  <key attr.name="url" attr.type="string" for="edge" id="d5"/>
-  <key attr.name="description" attr.type="string" for="edge" id="d6"/>
-  <key for="edge" id="d7" yfiles.type="edgegraphics"/>
-  <graph edgedefault="directed" id="G">
-    <node id="n0">
-      <data key="d3">
-        <y:ShapeNode>
-          <y:Geometry height="30.0" width="30.0" x="125.0" y="100.0"/>
-          <y:Fill color="#FFCC00" transparent="false"/>
-          <y:BorderStyle color="#000000" type="line" width="1.0"/>
-          <y:NodeLabel alignment="center" autoSizePolicy="content"
-           borderDistance="0.0" fontFamily="Dialog" fontSize="13"
-           fontStyle="plain" hasBackgroundColor="false" hasLineColor="false"
-           height="19.1328125" modelName="internal" modelPosition="c"
-           textColor="#000000" visible="true" width="12.27099609375"
-           x="8.864501953125" y="5.43359375">1</y:NodeLabel>
-          <y:Shape type="rectangle"/>
-        </y:ShapeNode>
-      </data>
-    </node>
-    <node id="n1">
-      <data key="d3">
-        <y:ShapeNode>
-          <y:Geometry height="30.0" width="30.0" x="183.0" y="205.0"/>
-          <y:Fill color="#FFCC00" transparent="false"/>
-          <y:BorderStyle color="#000000" type="line" width="1.0"/>
-          <y:NodeLabel alignment="center" autoSizePolicy="content"
-          borderDistance="0.0" fontFamily="Dialog" fontSize="13"
-          fontStyle="plain" hasBackgroundColor="false" hasLineColor="false"
-          height="19.1328125" modelName="internal" modelPosition="c"
-          textColor="#000000" visible="true" width="12.27099609375"
-          x="8.864501953125" y="5.43359375">2</y:NodeLabel>
-          <y:Shape type="rectangle"/>
-        </y:ShapeNode>
-      </data>
-    </node>
-    <node id="n2">
-      <data key="d6" xml:space="preserve"><![CDATA[description
-line1
-line2]]></data>
-      <data key="d3">
-        <y:GenericNode configuration="com.yworks.flowchart.terminator">
-          <y:Geometry height="40.0" width="80.0" x="950.0" y="286.0"/>
-          <y:Fill color="#E8EEF7" color2="#B7C9E3" transparent="false"/>
-          <y:BorderStyle color="#000000" type="line" width="1.0"/>
-          <y:NodeLabel alignment="center" autoSizePolicy="content"
-          fontFamily="Dialog" fontSize="12" fontStyle="plain"
-          hasBackgroundColor="false" hasLineColor="false" height="17.96875"
-          horizontalTextPosition="center" iconTextGap="4" modelName="custom"
-          textColor="#000000" verticalTextPosition="bottom" visible="true"
-          width="67.984375" x="6.0078125" xml:space="preserve"
-          y="11.015625">3<y:LabelModel>
-          <y:SmartNodeLabelModel distance="4.0"/></y:LabelModel>
-          <y:ModelParameter><y:SmartNodeLabelModelParameter labelRatioX="0.0"
-          labelRatioY="0.0" nodeRatioX="0.0" nodeRatioY="0.0" offsetX="0.0"
-          offsetY="0.0" upX="0.0" upY="-1.0"/></y:ModelParameter></y:NodeLabel>
-        </y:GenericNode>
-      </data>
-    </node>
-    <edge id="e0" source="n0" target="n1">
-      <data key="d7">
-        <y:PolyLineEdge>
-          <y:Path sx="0.0" sy="0.0" tx="0.0" ty="0.0"/>
-          <y:LineStyle color="#000000" type="line" width="1.0"/>
-          <y:Arrows source="none" target="standard"/>
-          <y:BendStyle smoothed="false"/>
-        </y:PolyLineEdge>
-      </data>
-    </edge>
-  </graph>
-  <data key="d0">
-    <y:Resources/>
-  </data>
-</graphml>
-"""
-        fh = io.BytesIO(data.encode("UTF-8"))
-        G = eg.read_graphml(fh, force_multigraph=True)
-        # assert list(G.edges) == [("n0", "n1")]
-        assert G.has_edge("n0", "n1", key="e0")
-        assert G.nodes["n0"]["label"] == "1"
-        assert G.nodes["n1"]["label"] == "2"
-        assert G.nodes["n2"]["label"] == "3"
-        assert G.nodes["n0"]["shape_type"] == "rectangle"
-        assert G.nodes["n1"]["shape_type"] == "rectangle"
-        assert G.nodes["n2"]["shape_type"] == "com.yworks.flowchart.terminator"
-        assert G.nodes["n2"]["description"] == "description\nline1\nline2"
-        fh.seek(0)
-        G = eg.read_graphml(fh)
-        # assert list(G.edges) == [("n0", "n1")]
-        assert G["n0"]["n1"]["id"] == "e0"
-        assert G.nodes["n0"]["label"] == "1"
-        assert G.nodes["n1"]["label"] == "2"
-        assert G.nodes["n2"]["label"] == "3"
-        assert G.nodes["n0"]["shape_type"] == "rectangle"
-        assert G.nodes["n1"]["shape_type"] == "rectangle"
-        assert G.nodes["n2"]["shape_type"] == "com.yworks.flowchart.terminator"
-        assert G.nodes["n2"]["description"] == "description\nline1\nline2"
-
-        H = eg.parse_graphml(data, force_multigraph=True)
-        # assert list(H.edges) == [("n0", "n1")]
-        assert H.has_edge("n0", "n1", key="e0")
-        assert H.nodes["n0"]["label"] == "1"
-        assert H.nodes["n1"]["label"] == "2"
-        assert H.nodes["n2"]["label"] == "3"
-
-        H = eg.parse_graphml(data)
-        # assert list(H.edges) == [("n0", "n1")]
-        assert H["n0"]["n1"]["id"] == "e0"
-        assert H.nodes["n0"]["label"] == "1"
-        assert H.nodes["n1"]["label"] == "2"
-        assert H.nodes["n2"]["label"] == "3"
-
-    def test_bool(self):
-        s = """<?xml version="1.0" encoding="UTF-8"?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <key id="d0" for="node" attr.name="test" attr.type="boolean">
-    <default>false</default>
-  </key>
-  <graph id="G" edgedefault="directed">
-    <node id="n0">
-      <data key="d0">true</data>
-    </node>
-    <node id="n1"/>
-    <node id="n2">
-      <data key="d0">false</data>
-    </node>
-    <node id="n3">
-      <data key="d0">FaLsE</data>
-    </node>
-    <node id="n4">
-      <data key="d0">True</data>
-    </node>
-    <node id="n5">
-      <data key="d0">0</data>
-    </node>
-    <node id="n6">
-      <data key="d0">1</data>
-    </node>
-  </graph>
-</graphml>
-"""
-        fh = io.BytesIO(s.encode("UTF-8"))
-        G = eg.read_graphml(fh)
-        H = eg.parse_graphml(s)
-        for graph in [G, H]:
-            assert graph.nodes["n0"]["test"]
-            assert not graph.nodes["n2"]["test"]
-            assert not graph.nodes["n3"]["test"]
-            assert graph.nodes["n4"]["test"]
-            assert not graph.nodes["n5"]["test"]
-            assert graph.nodes["n6"]["test"]
-
-    def test_graphml_header_line(self):
-        good = """<?xml version="1.0" encoding="UTF-8" standalone="no"?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <key id="d0" for="node" attr.name="test" attr.type="boolean">
-    <default>false</default>
-  </key>
-  <graph id="G">
-    <node id="n0">
-      <data key="d0">true</data>
-    </node>
-  </graph>
-</graphml>
-"""
-        bad = """<?xml version="1.0" encoding="UTF-8" standalone="no"?>
-<graphml>
-  <key id="d0" for="node" attr.name="test" attr.type="boolean">
-    <default>false</default>
-  </key>
-  <graph id="G">
-    <node id="n0">
-      <data key="d0">true</data>
-    </node>
-  </graph>
-</graphml>
-"""
-        ugly = """<?xml version="1.0" encoding="UTF-8" standalone="no"?>
-<graphml xmlns="https://ghghgh">
-  <key id="d0" for="node" attr.name="test" attr.type="boolean">
-    <default>false</default>
-  </key>
-  <graph id="G">
-    <node id="n0">
-      <data key="d0">true</data>
-    </node>
-  </graph>
-</graphml>
-"""
-        for s in (good, bad):
-            fh = io.BytesIO(s.encode("UTF-8"))
-            G = eg.read_graphml(fh)
-            H = eg.parse_graphml(s)
-            for graph in [G, H]:
-                assert graph.nodes["n0"]["test"]
-
-        fh = io.BytesIO(ugly.encode("UTF-8"))
-        pytest.raises(eg.EasyGraphError, eg.read_graphml, fh)
-        pytest.raises(eg.EasyGraphError, eg.parse_graphml, ugly)
-
-    def test_read_attributes_with_groups(self):
-        data = """\
-<?xml version="1.0" encoding="UTF-8" standalone="no"?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:java="http://www.yworks.com/xml/yfiles-common/1.0/java" xmlns:sys="http://www.yworks.com/xml/yfiles-common/markup/primitives/2.0" xmlns:x="http://www.yworks.com/xml/yfiles-common/markup/2.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:y="http://www.yworks.com/xml/graphml" xmlns:yed="http://www.yworks.com/xml/yed/3" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://www.yworks.com/xml/schema/graphml/1.1/ygraphml.xsd">
-  <!--Created by yEd 3.17-->
-  <key attr.name="Description" attr.type="string" for="graph" id="d0"/>
-  <key for="port" id="d1" yfiles.type="portgraphics"/>
-  <key for="port" id="d2" yfiles.type="portgeometry"/>
-  <key for="port" id="d3" yfiles.type="portuserdata"/>
-  <key attr.name="CustomProperty" attr.type="string" for="node" id="d4">
-    <default/>
-  </key>
-  <key attr.name="url" attr.type="string" for="node" id="d5"/>
-  <key attr.name="description" attr.type="string" for="node" id="d6"/>
-  <key for="node" id="d7" yfiles.type="nodegraphics"/>
-  <key for="graphml" id="d8" yfiles.type="resources"/>
-  <key attr.name="url" attr.type="string" for="edge" id="d9"/>
-  <key attr.name="description" attr.type="string" for="edge" id="d10"/>
-  <key for="edge" id="d11" yfiles.type="edgegraphics"/>
-  <graph edgedefault="directed" id="G">
-    <data key="d0"/>
-    <node id="n0">
-      <data key="d4"><![CDATA[CustomPropertyValue]]></data>
-      <data key="d6"/>
-      <data key="d7">
-        <y:ShapeNode>
-          <y:Geometry height="30.0" width="30.0" x="125.0" y="-255.4611111111111"/>
-          <y:Fill color="#FFCC00" transparent="false"/>
-          <y:BorderStyle color="#000000" raised="false" type="line" width="1.0"/>
-          <y:NodeLabel alignment="center" autoSizePolicy="content" fontFamily="Dialog" fontSize="12" fontStyle="plain" hasBackgroundColor="false" hasLineColor="false" height="17.96875" horizontalTextPosition="center" iconTextGap="4" modelName="custom" textColor="#000000" verticalTextPosition="bottom" visible="true" width="11.634765625" x="9.1826171875" y="6.015625">2<y:LabelModel>
-              <y:SmartNodeLabelModel distance="4.0"/>
-            </y:LabelModel>
-            <y:ModelParameter>
-              <y:SmartNodeLabelModelParameter labelRatioX="0.0" labelRatioY="0.0" nodeRatioX="0.0" nodeRatioY="0.0" offsetX="0.0" offsetY="0.0" upX="0.0" upY="-1.0"/>
-            </y:ModelParameter>
-          </y:NodeLabel>
-          <y:Shape type="rectangle"/>
-        </y:ShapeNode>
-      </data>
-    </node>
-    <node id="n1" yfiles.foldertype="group">
-      <data key="d4"><![CDATA[CustomPropertyValue]]></data>
-      <data key="d5"/>
-      <data key="d6"/>
-      <data key="d7">
-        <y:ProxyAutoBoundsNode>
-          <y:Realizers active="0">
-            <y:GroupNode>
-              <y:Geometry height="250.38333333333333" width="140.0" x="-30.0" y="-330.3833333333333"/>
-              <y:Fill color="#F5F5F5" transparent="false"/>
-              <y:BorderStyle color="#000000" type="dashed" width="1.0"/>
-              <y:NodeLabel alignment="right" autoSizePolicy="node_width" backgroundColor="#EBEBEB" borderDistance="0.0" fontFamily="Dialog" fontSize="15" fontStyle="plain" hasLineColor="false" height="21.4609375" horizontalTextPosition="center" iconTextGap="4" modelName="internal" modelPosition="t" textColor="#000000" verticalTextPosition="bottom" visible="true" width="140.0" x="0.0" y="0.0">Group 3</y:NodeLabel>
-              <y:Shape type="roundrectangle"/>
-              <y:State closed="false" closedHeight="50.0" closedWidth="50.0" innerGraphDisplayEnabled="false"/>
-              <y:Insets bottom="15" bottomF="15.0" left="15" leftF="15.0" right="15" rightF="15.0" top="15" topF="15.0"/>
-              <y:BorderInsets bottom="1" bottomF="1.0" left="0" leftF="0.0" right="0" rightF="0.0" top="1" topF="1.0001736111111086"/>
-            </y:GroupNode>
-            <y:GroupNode>
-              <y:Geometry height="50.0" width="50.0" x="0.0" y="60.0"/>
-              <y:Fill color="#F5F5F5" transparent="false"/>
-              <y:BorderStyle color="#000000" type="dashed" width="1.0"/>
-              <y:NodeLabel alignment="right" autoSizePolicy="node_width" backgroundColor="#EBEBEB" borderDistance="0.0" fontFamily="Dialog" fontSize="15" fontStyle="plain" hasLineColor="false" height="21.4609375" horizontalTextPosition="center" iconTextGap="4" modelName="internal" modelPosition="t" textColor="#000000" verticalTextPosition="bottom" visible="true" width="65.201171875" x="-7.6005859375" y="0.0">Folder 3</y:NodeLabel>
-              <y:Shape type="roundrectangle"/>
-              <y:State closed="true" closedHeight="50.0" closedWidth="50.0" innerGraphDisplayEnabled="false"/>
-              <y:Insets bottom="5" bottomF="5.0" left="5" leftF="5.0" right="5" rightF="5.0" top="5" topF="5.0"/>
-              <y:BorderInsets bottom="0" bottomF="0.0" left="0" leftF="0.0" right="0" rightF="0.0" top="0" topF="0.0"/>
-            </y:GroupNode>
-          </y:Realizers>
-        </y:ProxyAutoBoundsNode>
-      </data>
-      <graph edgedefault="directed" id="n1:">
-        <node id="n1::n0" yfiles.foldertype="group">
-          <data key="d4"><![CDATA[CustomPropertyValue]]></data>
-          <data key="d5"/>
-          <data key="d6"/>
-          <data key="d7">
-            <y:ProxyAutoBoundsNode>
-              <y:Realizers active="0">
-                <y:GroupNode>
-                  <y:Geometry height="83.46111111111111" width="110.0" x="-15.0" y="-292.9222222222222"/>
-                  <y:Fill color="#F5F5F5" transparent="false"/>
-                  <y:BorderStyle color="#000000" type="dashed" width="1.0"/>
-                  <y:NodeLabel alignment="right" autoSizePolicy="node_width" backgroundColor="#EBEBEB" borderDistance="0.0" fontFamily="Dialog" fontSize="15" fontStyle="plain" hasLineColor="false" height="21.4609375" horizontalTextPosition="center" iconTextGap="4" modelName="internal" modelPosition="t" textColor="#000000" verticalTextPosition="bottom" visible="true" width="110.0" x="0.0" y="0.0">Group 1</y:NodeLabel>
-                  <y:Shape type="roundrectangle"/>
-                  <y:State closed="false" closedHeight="50.0" closedWidth="50.0" innerGraphDisplayEnabled="false"/>
-                  <y:Insets bottom="15" bottomF="15.0" left="15" leftF="15.0" right="15" rightF="15.0" top="15" topF="15.0"/>
-                  <y:BorderInsets bottom="1" bottomF="1.0" left="0" leftF="0.0" right="0" rightF="0.0" top="1" topF="1.0001736111111086"/>
-                </y:GroupNode>
-                <y:GroupNode>
-                  <y:Geometry height="50.0" width="50.0" x="0.0" y="60.0"/>
-                  <y:Fill color="#F5F5F5" transparent="false"/>
-                  <y:BorderStyle color="#000000" type="dashed" width="1.0"/>
-                  <y:NodeLabel alignment="right" autoSizePolicy="node_width" backgroundColor="#EBEBEB" borderDistance="0.0" fontFamily="Dialog" fontSize="15" fontStyle="plain" hasLineColor="false" height="21.4609375" horizontalTextPosition="center" iconTextGap="4" modelName="internal" modelPosition="t" textColor="#000000" verticalTextPosition="bottom" visible="true" width="65.201171875" x="-7.6005859375" y="0.0">Folder 1</y:NodeLabel>
-                  <y:Shape type="roundrectangle"/>
-                  <y:State closed="true" closedHeight="50.0" closedWidth="50.0" innerGraphDisplayEnabled="false"/>
-                  <y:Insets bottom="5" bottomF="5.0" left="5" leftF="5.0" right="5" rightF="5.0" top="5" topF="5.0"/>
-                  <y:BorderInsets bottom="0" bottomF="0.0" left="0" leftF="0.0" right="0" rightF="0.0" top="0" topF="0.0"/>
-                </y:GroupNode>
-              </y:Realizers>
-            </y:ProxyAutoBoundsNode>
-          </data>
-          <graph edgedefault="directed" id="n1::n0:">
-            <node id="n1::n0::n0">
-              <data key="d4"><![CDATA[CustomPropertyValue]]></data>
-              <data key="d6"/>
-              <data key="d7">
-                <y:ShapeNode>
-                  <y:Geometry height="30.0" width="30.0" x="50.0" y="-255.4611111111111"/>
-                  <y:Fill color="#FFCC00" transparent="false"/>
-                  <y:BorderStyle color="#000000" raised="false" type="line" width="1.0"/>
-                  <y:NodeLabel alignment="center" autoSizePolicy="content" fontFamily="Dialog" fontSize="12" fontStyle="plain" hasBackgroundColor="false" hasLineColor="false" height="17.96875" horizontalTextPosition="center" iconTextGap="4" modelName="custom" textColor="#000000" verticalTextPosition="bottom" visible="true" width="11.634765625" x="9.1826171875" y="6.015625">1<y:LabelModel>
-                      <y:SmartNodeLabelModel distance="4.0"/>
-                    </y:LabelModel>
-                    <y:ModelParameter>
-                      <y:SmartNodeLabelModelParameter labelRatioX="0.0" labelRatioY="0.0" nodeRatioX="0.0" nodeRatioY="0.0" offsetX="0.0" offsetY="0.0" upX="0.0" upY="-1.0"/>
-                    </y:ModelParameter>
-                  </y:NodeLabel>
-                  <y:Shape type="rectangle"/>
-                </y:ShapeNode>
-              </data>
-            </node>
-            <node id="n1::n0::n1">
-              <data key="d4"><![CDATA[CustomPropertyValue]]></data>
-              <data key="d6"/>
-              <data key="d7">
-                <y:ShapeNode>
-                  <y:Geometry height="30.0" width="30.0" x="0.0" y="-255.4611111111111"/>
-                  <y:Fill color="#FFCC00" transparent="false"/>
-                  <y:BorderStyle color="#000000" raised="false" type="line" width="1.0"/>
-                  <y:NodeLabel alignment="center" autoSizePolicy="content" fontFamily="Dialog" fontSize="12" fontStyle="plain" hasBackgroundColor="false" hasLineColor="false" height="17.96875" horizontalTextPosition="center" iconTextGap="4" modelName="custom" textColor="#000000" verticalTextPosition="bottom" visible="true" width="11.634765625" x="9.1826171875" y="6.015625">3<y:LabelModel>
-                      <y:SmartNodeLabelModel distance="4.0"/>
-                    </y:LabelModel>
-                    <y:ModelParameter>
-                      <y:SmartNodeLabelModelParameter labelRatioX="0.0" labelRatioY="0.0" nodeRatioX="0.0" nodeRatioY="0.0" offsetX="0.0" offsetY="0.0" upX="0.0" upY="-1.0"/>
-                    </y:ModelParameter>
-                  </y:NodeLabel>
-                  <y:Shape type="rectangle"/>
-                </y:ShapeNode>
-              </data>
-            </node>
-          </graph>
-        </node>
-        <node id="n1::n1" yfiles.foldertype="group">
-          <data key="d4"><![CDATA[CustomPropertyValue]]></data>
-          <data key="d5"/>
-          <data key="d6"/>
-          <data key="d7">
-            <y:ProxyAutoBoundsNode>
-              <y:Realizers active="0">
-                <y:GroupNode>
-                  <y:Geometry height="83.46111111111111" width="110.0" x="-15.0" y="-179.4611111111111"/>
-                  <y:Fill color="#F5F5F5" transparent="false"/>
-                  <y:BorderStyle color="#000000" type="dashed" width="1.0"/>
-                  <y:NodeLabel alignment="right" autoSizePolicy="node_width" backgroundColor="#EBEBEB" borderDistance="0.0" fontFamily="Dialog" fontSize="15" fontStyle="plain" hasLineColor="false" height="21.4609375" horizontalTextPosition="center" iconTextGap="4" modelName="internal" modelPosition="t" textColor="#000000" verticalTextPosition="bottom" visible="true" width="110.0" x="0.0" y="0.0">Group 2</y:NodeLabel>
-                  <y:Shape type="roundrectangle"/>
-                  <y:State closed="false" closedHeight="50.0" closedWidth="50.0" innerGraphDisplayEnabled="false"/>
-                  <y:Insets bottom="15" bottomF="15.0" left="15" leftF="15.0" right="15" rightF="15.0" top="15" topF="15.0"/>
-                  <y:BorderInsets bottom="1" bottomF="1.0" left="0" leftF="0.0" right="0" rightF="0.0" top="1" topF="1.0001736111111086"/>
-                </y:GroupNode>
-                <y:GroupNode>
-                  <y:Geometry height="50.0" width="50.0" x="0.0" y="60.0"/>
-                  <y:Fill color="#F5F5F5" transparent="false"/>
-                  <y:BorderStyle color="#000000" type="dashed" width="1.0"/>
-                  <y:NodeLabel alignment="right" autoSizePolicy="node_width" backgroundColor="#EBEBEB" borderDistance="0.0" fontFamily="Dialog" fontSize="15" fontStyle="plain" hasLineColor="false" height="21.4609375" horizontalTextPosition="center" iconTextGap="4" modelName="internal" modelPosition="t" textColor="#000000" verticalTextPosition="bottom" visible="true" width="65.201171875" x="-7.6005859375" y="0.0">Folder 2</y:NodeLabel>
-                  <y:Shape type="roundrectangle"/>
-                  <y:State closed="true" closedHeight="50.0" closedWidth="50.0" innerGraphDisplayEnabled="false"/>
-                  <y:Insets bottom="5" bottomF="5.0" left="5" leftF="5.0" right="5" rightF="5.0" top="5" topF="5.0"/>
-                  <y:BorderInsets bottom="0" bottomF="0.0" left="0" leftF="0.0" right="0" rightF="0.0" top="0" topF="0.0"/>
-                </y:GroupNode>
-              </y:Realizers>
-            </y:ProxyAutoBoundsNode>
-          </data>
-          <graph edgedefault="directed" id="n1::n1:">
-            <node id="n1::n1::n0">
-              <data key="d4"><![CDATA[CustomPropertyValue]]></data>
-              <data key="d6"/>
-              <data key="d7">
-                <y:ShapeNode>
-                  <y:Geometry height="30.0" width="30.0" x="0.0" y="-142.0"/>
-                  <y:Fill color="#FFCC00" transparent="false"/>
-                  <y:BorderStyle color="#000000" raised="false" type="line" width="1.0"/>
-                  <y:NodeLabel alignment="center" autoSizePolicy="content" fontFamily="Dialog" fontSize="12" fontStyle="plain" hasBackgroundColor="false" hasLineColor="false" height="17.96875" horizontalTextPosition="center" iconTextGap="4" modelName="custom" textColor="#000000" verticalTextPosition="bottom" visible="true" width="11.634765625" x="9.1826171875" y="6.015625">5<y:LabelModel>
-                      <y:SmartNodeLabelModel distance="4.0"/>
-                    </y:LabelModel>
-                    <y:ModelParameter>
-                      <y:SmartNodeLabelModelParameter labelRatioX="0.0" labelRatioY="0.0" nodeRatioX="0.0" nodeRatioY="0.0" offsetX="0.0" offsetY="0.0" upX="0.0" upY="-1.0"/>
-                    </y:ModelParameter>
-                  </y:NodeLabel>
-                  <y:Shape type="rectangle"/>
-                </y:ShapeNode>
-              </data>
-            </node>
-            <node id="n1::n1::n1">
-              <data key="d4"><![CDATA[CustomPropertyValue]]></data>
-              <data key="d6"/>
-              <data key="d7">
-                <y:ShapeNode>
-                  <y:Geometry height="30.0" width="30.0" x="50.0" y="-142.0"/>
-                  <y:Fill color="#FFCC00" transparent="false"/>
-                  <y:BorderStyle color="#000000" raised="false" type="line" width="1.0"/>
-                  <y:NodeLabel alignment="center" autoSizePolicy="content" fontFamily="Dialog" fontSize="12" fontStyle="plain" hasBackgroundColor="false" hasLineColor="false" height="17.96875" horizontalTextPosition="center" iconTextGap="4" modelName="custom" textColor="#000000" verticalTextPosition="bottom" visible="true" width="11.634765625" x="9.1826171875" y="6.015625">6<y:LabelModel>
-                      <y:SmartNodeLabelModel distance="4.0"/>
-                    </y:LabelModel>
-                    <y:ModelParameter>
-                      <y:SmartNodeLabelModelParameter labelRatioX="0.0" labelRatioY="0.0" nodeRatioX="0.0" nodeRatioY="0.0" offsetX="0.0" offsetY="0.0" upX="0.0" upY="-1.0"/>
-                    </y:ModelParameter>
-                  </y:NodeLabel>
-                  <y:Shape type="rectangle"/>
-                </y:ShapeNode>
-              </data>
-            </node>
-          </graph>
-        </node>
-      </graph>
-    </node>
-    <node id="n2">
-      <data key="d4"><![CDATA[CustomPropertyValue]]></data>
-      <data key="d6"/>
-      <data key="d7">
-        <y:ShapeNode>
-          <y:Geometry height="30.0" width="30.0" x="125.0" y="-142.0"/>
-          <y:Fill color="#FFCC00" transparent="false"/>
-          <y:BorderStyle color="#000000" raised="false" type="line" width="1.0"/>
-          <y:NodeLabel alignment="center" autoSizePolicy="content" fontFamily="Dialog" fontSize="12" fontStyle="plain" hasBackgroundColor="false" hasLineColor="false" height="17.96875" horizontalTextPosition="center" iconTextGap="4" modelName="custom" textColor="#000000" verticalTextPosition="bottom" visible="true" width="11.634765625" x="9.1826171875" y="6.015625">9<y:LabelModel>
-              <y:SmartNodeLabelModel distance="4.0"/>
-            </y:LabelModel>
-            <y:ModelParameter>
-              <y:SmartNodeLabelModelParameter labelRatioX="0.0" labelRatioY="0.0" nodeRatioX="0.0" nodeRatioY="0.0" offsetX="0.0" offsetY="0.0" upX="0.0" upY="-1.0"/>
-            </y:ModelParameter>
-          </y:NodeLabel>
-          <y:Shape type="rectangle"/>
-        </y:ShapeNode>
-      </data>
-    </node>
-    <edge id="n1::n1::e0" source="n1::n1::n0" target="n1::n1::n1">
-      <data key="d10"/>
-      <data key="d11">
-        <y:PolyLineEdge>
-          <y:Path sx="15.0" sy="-0.0" tx="-15.0" ty="-0.0"/>
-          <y:LineStyle color="#000000" type="line" width="1.0"/>
-          <y:Arrows source="none" target="standard"/>
-          <y:BendStyle smoothed="false"/>
-        </y:PolyLineEdge>
-      </data>
-    </edge>
-    <edge id="n1::n0::e0" source="n1::n0::n1" target="n1::n0::n0">
-      <data key="d10"/>
-      <data key="d11">
-        <y:PolyLineEdge>
-          <y:Path sx="15.0" sy="-0.0" tx="-15.0" ty="-0.0"/>
-          <y:LineStyle color="#000000" type="line" width="1.0"/>
-          <y:Arrows source="none" target="standard"/>
-          <y:BendStyle smoothed="false"/>
-        </y:PolyLineEdge>
-      </data>
-    </edge>
-    <edge id="e0" source="n1::n0::n0" target="n0">
-      <data key="d10"/>
-      <data key="d11">
-        <y:PolyLineEdge>
-          <y:Path sx="15.0" sy="-0.0" tx="-15.0" ty="-0.0"/>
-          <y:LineStyle color="#000000" type="line" width="1.0"/>
-          <y:Arrows source="none" target="standard"/>
-          <y:BendStyle smoothed="false"/>
-        </y:PolyLineEdge>
-      </data>
-    </edge>
-    <edge id="e1" source="n1::n1::n1" target="n2">
-      <data key="d10"/>
-      <data key="d11">
-        <y:PolyLineEdge>
-          <y:Path sx="15.0" sy="-0.0" tx="-15.0" ty="-0.0"/>
-          <y:LineStyle color="#000000" type="line" width="1.0"/>
-          <y:Arrows source="none" target="standard"/>
-          <y:BendStyle smoothed="false"/>
-        </y:PolyLineEdge>
-      </data>
-    </edge>
-  </graph>
-  <data key="d8">
-    <y:Resources/>
-  </data>
-</graphml>
-"""
-        # verify that nodes / attributes are correctly read when part of a group
-        fh = io.BytesIO(data.encode("UTF-8"))
-        G = eg.read_graphml(fh)
-        data = [x for _, x in G.nodes.items()]
-        assert len(data) == 9
-        for node_data in data:
-            assert node_data["CustomProperty"] != ""
-
-    def test_long_attribute_type(self):
-        # test that graphs with attr.type="long" (as produced by botch and
-        # dose3) can be parsed
-        s = """<?xml version='1.0' encoding='utf-8'?>
-<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
-         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
-  <key attr.name="cudfversion" attr.type="long" for="node" id="d6" />
-  <graph edgedefault="directed">
-    <node id="n1">
-      <data key="d6">4284</data>
-    </node>
-  </graph>
-</graphml>"""
-        fh = io.BytesIO(s.encode("UTF-8"))
-        G = eg.read_graphml(fh)
-        expected = [("n1", {"cudfversion": 4284})]
-        assert sorted(G.nodes.items()) == expected
-        fh.seek(0)
-        H = eg.parse_graphml(s)
-        assert sorted(H.nodes.items()) == expected
-
-
-class TestWriteGraphML(BaseGraphML):
-    writer = staticmethod(eg.write_graphml_lxml)
-
-    @classmethod
-    def setup_class(cls):
-        BaseGraphML.setup_class()
-        _ = pytest.importorskip("lxml.etree")
-
-    def test_write_interface(self):
-        try:
-            assert eg.write_graphml == eg.write_graphml_lxml
-        except ImportError:
-            assert eg.write_graphml == eg.write_graphml_xml
-
-    def test_write_read_simple_directed_graphml(self):
-        G = self.simple_directed_graph
-        G.graph["hi"] = "there"
-        fh = io.BytesIO()
-        self.writer(G, fh)
-        fh.seek(0)
-        H = eg.read_graphml(fh)
-        assert sorted(G.nodes) == sorted(H.nodes)
-        assert sorted(G.edges) == sorted(H.edges)
-        assert sorted(G.edges) == sorted(H.edges)
-        self.simple_directed_fh.seek(0)
-
-    def test_GraphMLWriter_add_graphs(self):
-        gmlw = GraphMLWriter()
-        G = self.simple_directed_graph
-        H = G.copy()
-        gmlw.add_graphs([G, H])
-
-    def test_write_read_simple_no_prettyprint(self):
-        G = self.simple_directed_graph
-        G.graph["hi"] = "there"
-        G.graph["id"] = "1"
-        fh = io.BytesIO()
-        self.writer(G, fh, prettyprint=False)
-        fh.seek(0)
-        H = eg.read_graphml(fh)
-        assert sorted(G.nodes) == sorted(H.nodes)
-        assert sorted(G.edges) == sorted(H.edges)
-        assert sorted(G.edges) == sorted(H.edges)
-        self.simple_directed_fh.seek(0)
-
-    def test_write_read_attribute_named_key_ids_graphml(self):
-        from xml.etree.ElementTree import parse
-
-        G = self.attribute_named_key_ids_graph
-        fh = io.BytesIO()
-        self.writer(G, fh, named_key_ids=True)
-        fh.seek(0)
-        H = eg.read_graphml(fh)
-        fh.seek(0)
-
-        assert nodes_equal(G.nodes, H.nodes)
-        assert edges_equal(G.edges, H.edges)
-        assert edges_equal(G.edges, H.edges)
-        self.attribute_named_key_ids_fh.seek(0)
-
-        xml = parse(fh)
-        # Children are the key elements, and the graph element
-        children = list(xml.getroot())
-        assert len(children) == 4
-
-        keys = [child.items() for child in children[:3]]
-
-        assert len(keys) == 3
-        assert ("id", "edge_prop") in keys[0]
-        assert ("attr.name", "edge_prop") in keys[0]
-        assert ("id", "prop2") in keys[1]
-        assert ("attr.name", "prop2") in keys[1]
-        assert ("id", "prop1") in keys[2]
-        assert ("attr.name", "prop1") in keys[2]
-
-        # Confirm the read graph nodes/edge are identical when compared to
-        # default writing behavior.
-        default_behavior_fh = io.BytesIO()
-        eg.write_graphml(G, default_behavior_fh)
-        default_behavior_fh.seek(0)
-        H = eg.read_graphml(default_behavior_fh)
-
-        named_key_ids_behavior_fh = io.BytesIO()
-        eg.write_graphml(G, named_key_ids_behavior_fh, named_key_ids=True)
-        named_key_ids_behavior_fh.seek(0)
-        J = eg.read_graphml(named_key_ids_behavior_fh)
-
-        assert all(n1 == n2 for (n1, n2) in zip(H.nodes, J.nodes))
-        assert all(e1 == e2 for (e1, e2) in zip(H.edges, J.edges))
-
-    def test_write_read_attribute_numeric_type_graphml(self):
-        from xml.etree.ElementTree import parse
-
-        G = self.attribute_numeric_type_graph
-        fh = io.BytesIO()
-        self.writer(G, fh, infer_numeric_types=True)
-        fh.seek(0)
-        H = eg.read_graphml(fh)
-        fh.seek(0)
-
-        assert nodes_equal(G.nodes, H.nodes)
-        assert edges_equal(G.edges, H.edges)
-        assert edges_equal(G.edges, H.edges)
-        self.attribute_numeric_type_fh.seek(0)
-
-        xml = parse(fh)
-        # Children are the key elements, and the graph element
-        children = list(xml.getroot())
-        assert len(children) == 3
-
-        keys = [child.items() for child in children[:2]]
-
-        assert len(keys) == 2
-        assert ("attr.type", "double") in keys[0]
-        assert ("attr.type", "double") in keys[1]
-
-    def test_more_multigraph_keys(self):
-        """Writing keys as edge id attributes means keys become strings.
-        The original keys are stored as data, so read them back in
-        if `str(key) == edge_id`
-        This allows the adjacency to remain the same.
-        """
-        G = eg.MultiGraph()
-        G.add_edges_from([("a", "b", 2), ("a", "b", 3)])
-        fd, fname = tempfile.mkstemp()
-        self.writer(G, fname)
-        H = eg.read_graphml(fname)
-        assert H.is_multigraph()
-        assert edges_equal(G.edges, H.edges)
-        assert G._adj == H._adj
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_default_attribute(self):
-        G = eg.Graph(name="Fred")
-        G.add_node(1, label=1, color="green")
-        eg.add_path(G, [0, 1, 2, 3])
-        G.add_edge(1, 2, weight=3)
-        G.graph["node_default"] = {"color": "yellow"}
-        G.graph["edge_default"] = {"weight": 7}
-        fh = io.BytesIO()
-        self.writer(G, fh)
-        fh.seek(0)
-        H = eg.read_graphml(fh, node_type=int)
-        assert nodes_equal(G.nodes, H.nodes)
-        assert edges_equal(G.edges, H.edges)
-        assert G.graph == H.graph
-
-    def test_mixed_type_attributes(self):
-        G = eg.MultiGraph()
-        G.add_node("n0", special=False)
-        G.add_node("n1", special=0)
-        G.add_edge("n0", "n1", special=False)
-        G.add_edge("n0", "n1", special=0)
-        fh = io.BytesIO()
-        self.writer(G, fh)
-        fh.seek(0)
-        H = eg.read_graphml(fh)
-        assert not H.nodes["n0"]["special"]
-        assert H.nodes["n1"]["special"] == 0
-
-    def test_str_number_mixed_type_attributes(self):
-        G = eg.MultiGraph()
-        G.add_node("n0", special="hello")
-        G.add_node("n1", special=0)
-        G.add_edge("n0", "n1", special="hello")
-        G.add_edge("n0", "n1", special=0)
-        fh = io.BytesIO()
-        self.writer(G, fh)
-        fh.seek(0)
-        H = eg.read_graphml(fh)
-        assert H.nodes["n0"]["special"] == "hello"
-        assert H.nodes["n1"]["special"] == 0
-
-    def test_mixed_int_type_number_attributes(self):
-        np = pytest.importorskip("numpy")
-        G = eg.MultiGraph()
-        G.add_node("n0", special=np.int64(0))
-        G.add_node("n1", special=1)
-        G.add_edge("n0", "n1", special=np.int64(2))
-        G.add_edge("n0", "n1", special=3)
-        fh = io.BytesIO()
-        self.writer(G, fh)
-        fh.seek(0)
-        H = eg.read_graphml(fh)
-        assert H.nodes["n0"]["special"] == 0
-        assert H.nodes["n1"]["special"] == 1
-
-    def test_numpy_float(self):
-        np = pytest.importorskip("numpy")
-        wt = np.float_(3.4)
-        G = eg.Graph([(1, 2, {"weight": wt})])
-        fd, fname = tempfile.mkstemp()
-        self.writer(G, fname)
-        H = eg.read_graphml(fname, node_type=int)
-        assert G._adj == H._adj
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_multigraph_to_graph(self):
-        # test converting multigraph to graph if no parallel edges found
-        G = eg.MultiGraph()
-        G.add_edges_from([("a", "b", 2), ("b", "c", 3)])  # no multiedges
-        fd, fname = tempfile.mkstemp()
-        self.writer(G, fname)
-        H = eg.read_graphml(fname)
-        assert not H.is_multigraph()
-        H = eg.read_graphml(fname, force_multigraph=True)
-        assert H.is_multigraph()
-        os.close(fd)
-        os.unlink(fname)
-
-        # add a multiedge
-        G.add_edge("a", "b", "e-id")
-        fd, fname = tempfile.mkstemp()
-        self.writer(G, fname)
-        H = eg.read_graphml(fname)
-        assert H.is_multigraph()
-        H = eg.read_graphml(fname, force_multigraph=True)
-        assert H.is_multigraph()
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_write_generate_edge_id_from_attribute(self):
-        from xml.etree.ElementTree import parse
-
-        G = eg.Graph()
-        G.add_edges_from([("a", "b"), ("b", "c"), ("a", "c")])
-        edge_attributes = {e[:2]: str(e[:2]) for e in G.edges}
-        eg.set_edge_attributes(G, edge_attributes, "eid")
-        fd, fname = tempfile.mkstemp()
-        # set edge_id_from_attribute e.g. "eid" for write_graphml()
-        self.writer(G, fname, edge_id_from_attribute="eid")
-        # set edge_id_from_attribute e.g. "eid" for generate_graphml()
-        generator = eg.generate_graphml(G, edge_id_from_attribute="eid")
-
-        H = eg.read_graphml(fname)
-        assert nodes_equal(G.nodes, H.nodes)
-
-        # EasyGraph adds explicit edge "id" from file as attribute
-        eg.set_edge_attributes(G, edge_attributes, "id")
-        assert edges_equal(G.edges, H.edges)
-
-        tree = parse(fname)
-        children = list(tree.getroot())
-        assert len(children) == 2
-        edge_ids = [
-            edge.attrib["id"]
-            for edge in tree.getroot().findall(
-                ".//{http://graphml.graphdrawing.org/xmlns}edge"
-            )
-        ]
-        # verify edge id value is equal to specified attribute value
-        assert sorted(edge_ids) == sorted(edge_attributes.values())
-
-        # check graphml generated from generate_graphml()
-        data = "".join(generator)
-        J = eg.parse_graphml(data)
-        assert sorted(G.nodes) == sorted(J.nodes)
-        assert sorted(G.edges) == sorted(J.edges)
-        # EasyGraph adds explicit edge "id" from file as attribute
-        eg.set_edge_attributes(G, edge_attributes, "id")
-        assert edges_equal(G.edges, J.edges)
-
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_multigraph_write_generate_edge_id_from_attribute(self):
-        from xml.etree.ElementTree import parse
-
-        G = eg.MultiGraph()
-        G.add_edges_from([("a", "b"), ("b", "c"), ("a", "c"), ("a", "b")])
-        edge_attributes = {e[:3]: str(e[:3]) for e in G.edges}
-        eg.set_edge_attributes(G, edge_attributes, "eid")
-        fd, fname = tempfile.mkstemp()
-        # set edge_id_from_attribute e.g. "eid" for write_graphml()
-        self.writer(G, fname, edge_id_from_attribute="eid")
-        # set edge_id_from_attribute e.g. "eid" for generate_graphml()
-        generator = eg.generate_graphml(G, edge_id_from_attribute="eid")
-
-        H = eg.read_graphml(fname)
-        assert H.is_multigraph()
-        H = eg.read_graphml(fname, force_multigraph=True)
-        assert H.is_multigraph()
-
-        assert nodes_equal(G.nodes, H.nodes)
-        # assert edges_equal(G.edges, H.edges)x
-        x = [data.get("eid") for u, v, _, data in H.edges]
-        assert sorted(data.get("eid") for u, v, _, data in H.edges) == sorted(
-            edge_attributes.values()
-        )
-        # EasyGraph uses edge_ids as keys in multigraphs if no key
-        assert sorted(key for u, v, key, _ in H.edges) == sorted(
-            edge_attributes.values()
-        )
-
-        tree = parse(fname)
-        children = list(tree.getroot())
-        assert len(children) == 2
-        edge_ids = [
-            edge.attrib["id"]
-            for edge in tree.getroot().findall(
-                ".//{http://graphml.graphdrawing.org/xmlns}edge"
-            )
-        ]
-        # verify edge id value is equal to specified attribute value
-        assert sorted(edge_ids) == sorted(edge_attributes.values())
-
-        # check graphml generated from generate_graphml()
-        graphml_data = "".join(generator)
-        J = eg.parse_graphml(graphml_data)
-        assert J.is_multigraph()
-
-        assert nodes_equal(G.nodes, J.nodes)
-        # assert edges_equal(G.edges, J.edges)
-        assert sorted(data.get("eid") for u, v, _, data in J.edges) == sorted(
-            edge_attributes.values()
-        )
-        # EasyGraph uses edge_ids as keys in multigraphs if no key
-        assert sorted(key for u, v, key, _ in J.edges) == sorted(
-            edge_attributes.values()
-        )
-
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_numpy_float64(self):
-        np = pytest.importorskip("numpy")
-        wt = np.float64(3.4)
-        G = eg.Graph([(1, 2, {"weight": wt})])
-        fd, fname = tempfile.mkstemp()
-        self.writer(G, fname)
-        H = eg.read_graphml(fname, node_type=int)
-        assert G.edges == H.edges
-        wtG = G[1][2]["weight"]
-        wtH = H[1][2]["weight"]
-        assert wtG == pytest.approx(wtH, abs=1e-6)
-        assert type(wtG) == np.float64
-        assert type(wtH) == float
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_numpy_float32(self):
-        np = pytest.importorskip("numpy")
-        wt = np.float32(3.4)
-        G = eg.Graph([(1, 2, {"weight": wt})])
-        fd, fname = tempfile.mkstemp()
-        self.writer(G, fname)
-        H = eg.read_graphml(fname, node_type=int)
-        # assert G.edges == H.edges
-        wtG = G[1][2]["weight"]
-        wtH = H[1][2]["weight"]
-        assert wtG == pytest.approx(wtH, abs=1e-6)
-        assert type(wtG) == np.float32
-        assert type(wtH) == float
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_numpy_float64_inference(self):
-        np = pytest.importorskip("numpy")
-        G = self.attribute_numeric_type_graph
-        fd, fname = tempfile.mkstemp()
-        self.writer(G, fname, infer_numeric_types=True)
-        H = eg.read_graphml(fname)
-        assert G._adj == H._adj
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_unicode_attributes(self):
-        G = eg.Graph()
-        name1 = chr(2344) + chr(123) + chr(6543)
-        name2 = chr(5543) + chr(1543) + chr(324)
-        node_type = str
-        G.add_edge(name1, "Radiohead", foo=name2)
-        fd, fname = tempfile.mkstemp()
-        self.writer(G, fname)
-        H = eg.read_graphml(fname, node_type=node_type)
-        assert G._adj == H._adj
-        os.close(fd)
-        os.unlink(fname)
-
-    def test_unicode_escape(self):
-        # test for handling json escaped stings in python 2 Issue #1880
-        import json
-
-        a = dict(a='{"a": "123"}')  # an object with many chars to escape
-        sa = json.dumps(a)
-        G = eg.Graph()
-        G.graph["test"] = sa
-        fh = io.BytesIO()
-        self.writer(G, fh)
-        fh.seek(0)
-        H = eg.read_graphml(fh)
-        assert G.graph["test"] == H.graph["test"]
+import io
+import os
+import tempfile
+
+import easygraph as eg
+import pytest
+
+from easygraph.readwrite.graphml import GraphMLWriter
+from easygraph.utils import edges_equal
+from easygraph.utils import nodes_equal
+
+
+class BaseGraphML:
+    @classmethod
+    def setup_class(cls):
+        cls.simple_directed_data = """<?xml version="1.0" encoding="UTF-8"?>
+<!-- This file was written by the JAVA GraphML Library.-->
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <graph id="G" edgedefault="directed">
+    <node id="n0"/>
+    <node id="n1"/>
+    <node id="n2"/>
+    <node id="n3"/>
+    <node id="n4"/>
+    <node id="n5"/>
+    <node id="n6"/>
+    <node id="n7"/>
+    <node id="n8"/>
+    <node id="n9"/>
+    <node id="n10"/>
+    <edge id="foo" source="n0" target="n2"/>
+    <edge source="n1" target="n2"/>
+    <edge source="n2" target="n3"/>
+    <edge source="n3" target="n5"/>
+    <edge source="n3" target="n4"/>
+    <edge source="n4" target="n6"/>
+    <edge source="n6" target="n5"/>
+    <edge source="n5" target="n7"/>
+    <edge source="n6" target="n8"/>
+    <edge source="n8" target="n7"/>
+    <edge source="n8" target="n9"/>
+  </graph>
+</graphml>"""
+        cls.simple_directed_graph = eg.DiGraph()
+        cls.simple_directed_graph.add_node("n10")
+        cls.simple_directed_graph.add_edge("n0", "n2", id="foo")
+        cls.simple_directed_graph.add_edge("n0", "n2")
+        cls.simple_directed_graph.add_edges_from(
+            [
+                ("n1", "n2"),
+                ("n2", "n3"),
+                ("n3", "n5"),
+                ("n3", "n4"),
+                ("n4", "n6"),
+                ("n6", "n5"),
+                ("n5", "n7"),
+                ("n6", "n8"),
+                ("n8", "n7"),
+                ("n8", "n9"),
+            ]
+        )
+        cls.simple_directed_fh = io.BytesIO(cls.simple_directed_data.encode("UTF-8"))
+
+        cls.attribute_data = """<?xml version="1.0" encoding="UTF-8"?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+      xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+        http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <key id="d0" for="node" attr.name="color" attr.type="string">
+    <default>yellow</default>
+  </key>
+  <key id="d1" for="edge" attr.name="weight" attr.type="double"/>
+  <graph id="G" edgedefault="directed">
+    <node id="n0">
+      <data key="d0">green</data>
+    </node>
+    <node id="n1"/>
+    <node id="n2">
+      <data key="d0">blue</data>
+    </node>
+    <node id="n3">
+      <data key="d0">red</data>
+    </node>
+    <node id="n4"/>
+    <node id="n5">
+      <data key="d0">turquoise</data>
+    </node>
+    <edge id="e0" source="n0" target="n2">
+      <data key="d1">1.0</data>
+    </edge>
+    <edge id="e1" source="n0" target="n1">
+      <data key="d1">1.0</data>
+    </edge>
+    <edge id="e2" source="n1" target="n3">
+      <data key="d1">2.0</data>
+    </edge>
+    <edge id="e3" source="n3" target="n2"/>
+    <edge id="e4" source="n2" target="n4"/>
+    <edge id="e5" source="n3" target="n5"/>
+    <edge id="e6" source="n5" target="n4">
+      <data key="d1">1.1</data>
+    </edge>
+  </graph>
+</graphml>
+"""
+        cls.attribute_graph = eg.DiGraph(id="G")
+        cls.attribute_graph.graph["node_default"] = {"color": "yellow"}
+        cls.attribute_graph.add_node("n0", color="green")
+        cls.attribute_graph.add_node("n2", color="blue")
+        cls.attribute_graph.add_node("n3", color="red")
+        cls.attribute_graph.add_node("n4")
+        cls.attribute_graph.add_node("n5", color="turquoise")
+        cls.attribute_graph.add_edge("n0", "n2", id="e0", weight=1.0)
+        cls.attribute_graph.add_edge("n0", "n1", id="e1", weight=1.0)
+        cls.attribute_graph.add_edge("n1", "n3", id="e2", weight=2.0)
+        cls.attribute_graph.add_edge("n3", "n2", id="e3")
+        cls.attribute_graph.add_edge("n2", "n4", id="e4")
+        cls.attribute_graph.add_edge("n3", "n5", id="e5")
+        cls.attribute_graph.add_edge("n5", "n4", id="e6", weight=1.1)
+        cls.attribute_fh = io.BytesIO(cls.attribute_data.encode("UTF-8"))
+
+        cls.node_attribute_default_data = """<?xml version="1.0" encoding="UTF-8"?>
+        <graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+              xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+              xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+                http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+          <key id="d0" for="node" attr.name="boolean_attribute" attr.type="boolean"><default>false</default></key>
+          <key id="d1" for="node" attr.name="int_attribute" attr.type="int"><default>0</default></key>
+          <key id="d2" for="node" attr.name="long_attribute" attr.type="long"><default>0</default></key>
+          <key id="d3" for="node" attr.name="float_attribute" attr.type="float"><default>0.0</default></key>
+          <key id="d4" for="node" attr.name="double_attribute" attr.type="double"><default>0.0</default></key>
+          <key id="d5" for="node" attr.name="string_attribute" attr.type="string"><default>Foo</default></key>
+          <graph id="G" edgedefault="directed">
+            <node id="n0"/>
+            <node id="n1"/>
+            <edge id="e0" source="n0" target="n1"/>
+          </graph>
+        </graphml>
+        """
+        cls.node_attribute_default_graph = eg.DiGraph(id="G")
+        cls.node_attribute_default_graph.graph["node_default"] = {
+            "boolean_attribute": False,
+            "int_attribute": 0,
+            "long_attribute": 0,
+            "float_attribute": 0.0,
+            "double_attribute": 0.0,
+            "string_attribute": "Foo",
+        }
+        cls.node_attribute_default_graph.add_node("n0")
+        cls.node_attribute_default_graph.add_node("n1")
+        cls.node_attribute_default_graph.add_edge("n0", "n1", id="e0")
+        cls.node_attribute_default_fh = io.BytesIO(
+            cls.node_attribute_default_data.encode("UTF-8")
+        )
+
+        cls.attribute_named_key_ids_data = """<?xml version='1.0' encoding='utf-8'?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <key id="edge_prop" for="edge" attr.name="edge_prop" attr.type="string"/>
+  <key id="prop2" for="node" attr.name="prop2" attr.type="string"/>
+  <key id="prop1" for="node" attr.name="prop1" attr.type="string"/>
+  <graph edgedefault="directed">
+    <node id="0">
+      <data key="prop1">val1</data>
+      <data key="prop2">val2</data>
+    </node>
+    <node id="1">
+      <data key="prop1">val_one</data>
+      <data key="prop2">val2</data>
+    </node>
+    <edge source="0" target="1">
+      <data key="edge_prop">edge_value</data>
+    </edge>
+  </graph>
+</graphml>
+"""
+        cls.attribute_named_key_ids_graph = eg.DiGraph()
+        cls.attribute_named_key_ids_graph.add_node("0", prop1="val1", prop2="val2")
+        cls.attribute_named_key_ids_graph.add_node("1", prop1="val_one", prop2="val2")
+        cls.attribute_named_key_ids_graph.add_edge("0", "1", edge_prop="edge_value")
+        fh = io.BytesIO(cls.attribute_named_key_ids_data.encode("UTF-8"))
+        cls.attribute_named_key_ids_fh = fh
+
+        cls.attribute_numeric_type_data = """<?xml version='1.0' encoding='utf-8'?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <key attr.name="weight" attr.type="double" for="node" id="d1" />
+  <key attr.name="weight" attr.type="double" for="edge" id="d0" />
+  <graph edgedefault="directed">
+    <node id="n0">
+      <data key="d1">1</data>
+    </node>
+    <node id="n1">
+      <data key="d1">2.0</data>
+    </node>
+    <edge source="n0" target="n1">
+      <data key="d0">1</data>
+    </edge>
+    <edge source="n1" target="n0">
+      <data key="d0">k</data>
+    </edge>
+    <edge source="n1" target="n1">
+      <data key="d0">1.0</data>
+    </edge>
+  </graph>
+</graphml>
+"""
+        cls.attribute_numeric_type_graph = eg.DiGraph()
+        cls.attribute_numeric_type_graph.add_node("n0", weight=1)
+        cls.attribute_numeric_type_graph.add_node("n1", weight=2.0)
+        cls.attribute_numeric_type_graph.add_edge("n0", "n1", weight=1)
+        cls.attribute_numeric_type_graph.add_edge("n1", "n1", weight=1.0)
+        fh = io.BytesIO(cls.attribute_numeric_type_data.encode("UTF-8"))
+        cls.attribute_numeric_type_fh = fh
+
+        cls.simple_undirected_data = """<?xml version="1.0" encoding="UTF-8"?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <graph id="G">
+    <node id="n0"/>
+    <node id="n1"/>
+    <node id="n2"/>
+    <node id="n10"/>
+    <edge id="foo" source="n0" target="n2"/>
+    <edge source="n1" target="n2"/>
+    <edge source="n2" target="n3"/>
+  </graph>
+</graphml>"""
+        #    <edge source="n8" target="n10" directed="false"/>
+        cls.simple_undirected_graph = eg.Graph()
+        cls.simple_undirected_graph.add_node("n10")
+        cls.simple_undirected_graph.add_edge("n0", "n2", id="foo")
+        cls.simple_undirected_graph.add_edges_from([("n1", "n2"), ("n2", "n3")])
+        fh = io.BytesIO(cls.simple_undirected_data.encode("UTF-8"))
+        cls.simple_undirected_fh = fh
+
+        cls.undirected_multigraph_data = """<?xml version="1.0" encoding="UTF-8"?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <graph id="G">
+    <node id="n0"/>
+    <node id="n1"/>
+    <node id="n2"/>
+    <node id="n10"/>
+    <edge id="e0" source="n0" target="n2"/>
+    <edge id="e1" source="n1" target="n2"/>
+    <edge id="e2" source="n2" target="n1"/>
+  </graph>
+</graphml>"""
+        cls.undirected_multigraph = eg.MultiGraph()
+        cls.undirected_multigraph.add_node("n10")
+        cls.undirected_multigraph.add_edge("n0", "n2", id="e0")
+        cls.undirected_multigraph.add_edge("n1", "n2", id="e1")
+        cls.undirected_multigraph.add_edge("n2", "n1", id="e2")
+        fh = io.BytesIO(cls.undirected_multigraph_data.encode("UTF-8"))
+        cls.undirected_multigraph_fh = fh
+
+        cls.undirected_multigraph_no_multiedge_data = """<?xml version="1.0" encoding="UTF-8"?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <graph id="G">
+    <node id="n0"/>
+    <node id="n1"/>
+    <node id="n2"/>
+    <node id="n10"/>
+    <edge id="e0" source="n0" target="n2"/>
+    <edge id="e1" source="n1" target="n2"/>
+    <edge id="e2" source="n2" target="n3"/>
+  </graph>
+</graphml>"""
+        cls.undirected_multigraph_no_multiedge = eg.MultiGraph()
+        cls.undirected_multigraph_no_multiedge.add_node("n10")
+        cls.undirected_multigraph_no_multiedge.add_edge("n0", "n2", id="e0")
+        cls.undirected_multigraph_no_multiedge.add_edge("n1", "n2", id="e1")
+        cls.undirected_multigraph_no_multiedge.add_edge("n2", "n3", id="e2")
+        fh = io.BytesIO(cls.undirected_multigraph_no_multiedge_data.encode("UTF-8"))
+        cls.undirected_multigraph_no_multiedge_fh = fh
+
+        cls.multigraph_only_ids_for_multiedges_data = """<?xml version="1.0" encoding="UTF-8"?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <graph id="G">
+    <node id="n0"/>
+    <node id="n1"/>
+    <node id="n2"/>
+    <node id="n10"/>
+    <edge source="n0" target="n2"/>
+    <edge id="e1" source="n1" target="n2"/>
+    <edge id="e2" source="n2" target="n1"/>
+  </graph>
+</graphml>"""
+        cls.multigraph_only_ids_for_multiedges = eg.MultiGraph()
+        cls.multigraph_only_ids_for_multiedges.add_node("n10")
+        cls.multigraph_only_ids_for_multiedges.add_edge("n0", "n2")
+        cls.multigraph_only_ids_for_multiedges.add_edge("n1", "n2", id="e1")
+        cls.multigraph_only_ids_for_multiedges.add_edge("n2", "n1", id="e2")
+        fh = io.BytesIO(cls.multigraph_only_ids_for_multiedges_data.encode("UTF-8"))
+        cls.multigraph_only_ids_for_multiedges_fh = fh
+
+
+class TestReadGraphML(BaseGraphML):
+    def test_read_simple_directed_graphml(self):
+        G = self.simple_directed_graph
+        H = eg.read_graphml(self.simple_directed_fh)
+        assert sorted(G.nodes) == sorted(H.nodes)
+        assert sorted(G.edges) == sorted(H.edges)
+        assert sorted(G.edges) == sorted(H.edges)
+        self.simple_directed_fh.seek(0)
+
+        PG = eg.parse_graphml(self.simple_directed_data)
+        assert sorted(G.nodes) == sorted(PG.nodes)
+        assert sorted(G.edges) == sorted(PG.edges)
+        assert sorted(G.edges) == sorted(PG.edges)
+
+    def test_read_simple_undirected_graphml(self):
+        G = self.simple_undirected_graph
+        H = eg.read_graphml(self.simple_undirected_fh)
+        assert nodes_equal(G.nodes, H.nodes)
+        assert edges_equal(G.edges, H.edges)
+        self.simple_undirected_fh.seek(0)
+
+        PG = eg.parse_graphml(self.simple_undirected_data)
+        assert nodes_equal(G.nodes, PG.nodes)
+        assert edges_equal(G.edges, PG.edges)
+
+    def test_read_undirected_multigraph_graphml(self):
+        G = self.undirected_multigraph
+        H = eg.read_graphml(self.undirected_multigraph_fh)
+        assert nodes_equal(G.nodes, H.nodes)
+        # assert edges_equal(G.edges, H.edges)
+        self.undirected_multigraph_fh.seek(0)
+
+        PG = eg.parse_graphml(self.undirected_multigraph_data)
+        assert nodes_equal(G.nodes, PG.nodes)
+        # assert edges_equal(G.edges, PG.edges)
+
+    def test_read_undirected_multigraph_no_multiedge_graphml(self):
+        G = self.undirected_multigraph_no_multiedge
+        H = eg.read_graphml(self.undirected_multigraph_no_multiedge_fh)
+        assert nodes_equal(G.nodes, H.nodes)
+        # assert edges_equal(G.edges, H.edges)
+        self.undirected_multigraph_no_multiedge_fh.seek(0)
+
+        PG = eg.parse_graphml(self.undirected_multigraph_no_multiedge_data)
+        assert nodes_equal(G.nodes, PG.nodes)
+        # assert edges_equal(G.edges, PG.edges)
+
+    def test_read_undirected_multigraph_only_ids_for_multiedges_graphml(self):
+        G = self.multigraph_only_ids_for_multiedges
+        H = eg.read_graphml(self.multigraph_only_ids_for_multiedges_fh)
+        assert nodes_equal(G.nodes, H.nodes)
+        # assert edges_equal(G.edges, H.edges)
+        self.multigraph_only_ids_for_multiedges_fh.seek(0)
+
+        PG = eg.parse_graphml(self.multigraph_only_ids_for_multiedges_data)
+        assert nodes_equal(G.nodes, PG.nodes)
+        # assert edges_equal(G.edges, PG.edges)
+
+    def test_read_attribute_graphml(self):
+        G = self.attribute_graph
+        H = eg.read_graphml(self.attribute_fh)
+        assert nodes_equal(sorted(G.nodes), sorted(H.nodes))
+        ge = sorted(G.edges)
+        he = sorted(H.edges)
+        for a, b in zip(ge, he):
+            assert a == b
+        self.attribute_fh.seek(0)
+
+        PG = eg.parse_graphml(self.attribute_data)
+        assert sorted(G.nodes) == sorted(PG.nodes)
+        ge = sorted(G.edges)
+        he = sorted(PG.edges)
+        for a, b in zip(ge, he):
+            assert a == b
+
+    def test_node_default_attribute_graphml(self):
+        G = self.node_attribute_default_graph
+        H = eg.read_graphml(self.node_attribute_default_fh)
+        assert G.graph["node_default"] == H.graph["node_default"]
+
+    def test_directed_edge_in_undirected(self):
+        s = """<?xml version="1.0" encoding="UTF-8"?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <graph id="G">
+    <node id="n0"/>
+    <node id="n1"/>
+    <node id="n2"/>
+    <edge source="n0" target="n1"/>
+    <edge source="n1" target="n2" directed='true'/>
+  </graph>
+</graphml>"""
+        fh = io.BytesIO(s.encode("UTF-8"))
+        pytest.raises(eg.EasyGraphError, eg.read_graphml, fh)
+        pytest.raises(eg.EasyGraphError, eg.parse_graphml, s)
+
+    def test_undirected_edge_in_directed(self):
+        s = """<?xml version="1.0" encoding="UTF-8"?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <graph id="G" edgedefault='directed'>
+    <node id="n0"/>
+    <node id="n1"/>
+    <node id="n2"/>
+    <edge source="n0" target="n1"/>
+    <edge source="n1" target="n2" directed='false'/>
+  </graph>
+</graphml>"""
+        fh = io.BytesIO(s.encode("UTF-8"))
+        pytest.raises(eg.EasyGraphError, eg.read_graphml, fh)
+        pytest.raises(eg.EasyGraphError, eg.parse_graphml, s)
+
+    def test_key_raise(self):
+        s = """<?xml version="1.0" encoding="UTF-8"?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <key id="d0" for="node" attr.name="color" attr.type="string">
+    <default>yellow</default>
+  </key>
+  <key id="d1" for="edge" attr.name="weight" attr.type="double"/>
+  <graph id="G" edgedefault="directed">
+    <node id="n0">
+      <data key="d0">green</data>
+    </node>
+    <node id="n1"/>
+    <node id="n2">
+      <data key="d0">blue</data>
+    </node>
+    <edge id="e0" source="n0" target="n2">
+      <data key="d2">1.0</data>
+    </edge>
+  </graph>
+</graphml>
+"""
+        fh = io.BytesIO(s.encode("UTF-8"))
+        pytest.raises(eg.EasyGraphError, eg.read_graphml, fh)
+        pytest.raises(eg.EasyGraphError, eg.parse_graphml, s)
+
+    def test_hyperedge_raise(self):
+        s = """<?xml version="1.0" encoding="UTF-8"?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <key id="d0" for="node" attr.name="color" attr.type="string">
+    <default>yellow</default>
+  </key>
+  <key id="d1" for="edge" attr.name="weight" attr.type="double"/>
+  <graph id="G" edgedefault="directed">
+    <node id="n0">
+      <data key="d0">green</data>
+    </node>
+    <node id="n1"/>
+    <node id="n2">
+      <data key="d0">blue</data>
+    </node>
+    <hyperedge id="e0" source="n0" target="n2">
+       <endpoint node="n0"/>
+       <endpoint node="n1"/>
+       <endpoint node="n2"/>
+    </hyperedge>
+  </graph>
+</graphml>
+"""
+        fh = io.BytesIO(s.encode("UTF-8"))
+        pytest.raises(eg.EasyGraphError, eg.read_graphml, fh)
+        pytest.raises(eg.EasyGraphError, eg.parse_graphml, s)
+
+    def test_multigraph_keys(self):
+        # Test that reading multigraphs uses edge id attributes as keys
+        s = """<?xml version="1.0" encoding="UTF-8"?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <graph id="G" edgedefault="directed">
+    <node id="n0"/>
+    <node id="n1"/>
+    <edge id="e0" source="n0" target="n1"/>
+    <edge id="e1" source="n0" target="n1"/>
+  </graph>
+</graphml>
+"""
+        fh = io.BytesIO(s.encode("UTF-8"))
+        G = eg.read_graphml(fh)
+        expected = [("n0", "n1", "e0"), ("n0", "n1", "e1")]
+        # assert sorted(G.edges) == expected
+        fh.seek(0)
+        H = eg.parse_graphml(s)
+        # assert sorted(H.edges) == expected
+
+    def test_preserve_multi_edge_data(self):
+        """
+        Test that data and keys of edges are preserved on consequent
+        write and reads
+        """
+        G = eg.MultiGraph()
+        G.add_node(1)
+        G.add_node(2)
+        G.add_edges_from(
+            [
+                # edges with no data, no keys:
+                (1, 2),
+                # edges with only data:
+                (1, 2, dict(key="data_key1")),
+                (1, 2, dict(id="data_id2")),
+                (1, 2, dict(key="data_key3", id="data_id3")),
+                # edges with both data and keys:
+                (1, 2, 103, dict(key="data_key4")),
+                (1, 2, 104, dict(id="data_id5")),
+                (1, 2, 105, dict(key="data_key6", id="data_id7")),
+            ]
+        )
+        fh = io.BytesIO()
+        eg.write_graphml(G, fh)
+        fh.seek(0)
+        H = eg.read_graphml(fh, node_type=int)
+        assert edges_equal(G.edges, H.edges)
+        assert G._adj == H._adj
+
+        Gadj = {
+            str(node): {
+                str(nbr): {str(ekey): dd for ekey, dd in key_dict.items()}
+                for nbr, key_dict in nbr_dict.items()
+            }
+            for node, nbr_dict in G._adj.items()
+        }
+        fh.seek(0)
+        HH = eg.read_graphml(fh, node_type=str, edge_key_type=str)
+        assert Gadj == HH._adj
+
+        fh.seek(0)
+        string_fh = fh.read()
+        HH = eg.parse_graphml(string_fh, node_type=str, edge_key_type=str)
+        assert Gadj == HH._adj
+
+    def test_yfiles_extension(self):
+        data = """<?xml version="1.0" encoding="UTF-8" standalone="no"?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xmlns:y="http://www.yworks.com/xml/graphml"
+         xmlns:yed="http://www.yworks.com/xml/yed/3"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <!--Created by yFiles for Java 2.7-->
+  <key for="graphml" id="d0" yfiles.type="resources"/>
+  <key attr.name="url" attr.type="string" for="node" id="d1"/>
+  <key attr.name="description" attr.type="string" for="node" id="d2"/>
+  <key for="node" id="d3" yfiles.type="nodegraphics"/>
+  <key attr.name="Description" attr.type="string" for="graph" id="d4">
+    <default/>
+  </key>
+  <key attr.name="url" attr.type="string" for="edge" id="d5"/>
+  <key attr.name="description" attr.type="string" for="edge" id="d6"/>
+  <key for="edge" id="d7" yfiles.type="edgegraphics"/>
+  <graph edgedefault="directed" id="G">
+    <node id="n0">
+      <data key="d3">
+        <y:ShapeNode>
+          <y:Geometry height="30.0" width="30.0" x="125.0" y="100.0"/>
+          <y:Fill color="#FFCC00" transparent="false"/>
+          <y:BorderStyle color="#000000" type="line" width="1.0"/>
+          <y:NodeLabel alignment="center" autoSizePolicy="content"
+           borderDistance="0.0" fontFamily="Dialog" fontSize="13"
+           fontStyle="plain" hasBackgroundColor="false" hasLineColor="false"
+           height="19.1328125" modelName="internal" modelPosition="c"
+           textColor="#000000" visible="true" width="12.27099609375"
+           x="8.864501953125" y="5.43359375">1</y:NodeLabel>
+          <y:Shape type="rectangle"/>
+        </y:ShapeNode>
+      </data>
+    </node>
+    <node id="n1">
+      <data key="d3">
+        <y:ShapeNode>
+          <y:Geometry height="30.0" width="30.0" x="183.0" y="205.0"/>
+          <y:Fill color="#FFCC00" transparent="false"/>
+          <y:BorderStyle color="#000000" type="line" width="1.0"/>
+          <y:NodeLabel alignment="center" autoSizePolicy="content"
+          borderDistance="0.0" fontFamily="Dialog" fontSize="13"
+          fontStyle="plain" hasBackgroundColor="false" hasLineColor="false"
+          height="19.1328125" modelName="internal" modelPosition="c"
+          textColor="#000000" visible="true" width="12.27099609375"
+          x="8.864501953125" y="5.43359375">2</y:NodeLabel>
+          <y:Shape type="rectangle"/>
+        </y:ShapeNode>
+      </data>
+    </node>
+    <node id="n2">
+      <data key="d6" xml:space="preserve"><![CDATA[description
+line1
+line2]]></data>
+      <data key="d3">
+        <y:GenericNode configuration="com.yworks.flowchart.terminator">
+          <y:Geometry height="40.0" width="80.0" x="950.0" y="286.0"/>
+          <y:Fill color="#E8EEF7" color2="#B7C9E3" transparent="false"/>
+          <y:BorderStyle color="#000000" type="line" width="1.0"/>
+          <y:NodeLabel alignment="center" autoSizePolicy="content"
+          fontFamily="Dialog" fontSize="12" fontStyle="plain"
+          hasBackgroundColor="false" hasLineColor="false" height="17.96875"
+          horizontalTextPosition="center" iconTextGap="4" modelName="custom"
+          textColor="#000000" verticalTextPosition="bottom" visible="true"
+          width="67.984375" x="6.0078125" xml:space="preserve"
+          y="11.015625">3<y:LabelModel>
+          <y:SmartNodeLabelModel distance="4.0"/></y:LabelModel>
+          <y:ModelParameter><y:SmartNodeLabelModelParameter labelRatioX="0.0"
+          labelRatioY="0.0" nodeRatioX="0.0" nodeRatioY="0.0" offsetX="0.0"
+          offsetY="0.0" upX="0.0" upY="-1.0"/></y:ModelParameter></y:NodeLabel>
+        </y:GenericNode>
+      </data>
+    </node>
+    <edge id="e0" source="n0" target="n1">
+      <data key="d7">
+        <y:PolyLineEdge>
+          <y:Path sx="0.0" sy="0.0" tx="0.0" ty="0.0"/>
+          <y:LineStyle color="#000000" type="line" width="1.0"/>
+          <y:Arrows source="none" target="standard"/>
+          <y:BendStyle smoothed="false"/>
+        </y:PolyLineEdge>
+      </data>
+    </edge>
+  </graph>
+  <data key="d0">
+    <y:Resources/>
+  </data>
+</graphml>
+"""
+        fh = io.BytesIO(data.encode("UTF-8"))
+        G = eg.read_graphml(fh, force_multigraph=True)
+        # assert list(G.edges) == [("n0", "n1")]
+        assert G.has_edge("n0", "n1", key="e0")
+        assert G.nodes["n0"]["label"] == "1"
+        assert G.nodes["n1"]["label"] == "2"
+        assert G.nodes["n2"]["label"] == "3"
+        assert G.nodes["n0"]["shape_type"] == "rectangle"
+        assert G.nodes["n1"]["shape_type"] == "rectangle"
+        assert G.nodes["n2"]["shape_type"] == "com.yworks.flowchart.terminator"
+        assert G.nodes["n2"]["description"] == "description\nline1\nline2"
+        fh.seek(0)
+        G = eg.read_graphml(fh)
+        # assert list(G.edges) == [("n0", "n1")]
+        assert G["n0"]["n1"]["id"] == "e0"
+        assert G.nodes["n0"]["label"] == "1"
+        assert G.nodes["n1"]["label"] == "2"
+        assert G.nodes["n2"]["label"] == "3"
+        assert G.nodes["n0"]["shape_type"] == "rectangle"
+        assert G.nodes["n1"]["shape_type"] == "rectangle"
+        assert G.nodes["n2"]["shape_type"] == "com.yworks.flowchart.terminator"
+        assert G.nodes["n2"]["description"] == "description\nline1\nline2"
+
+        H = eg.parse_graphml(data, force_multigraph=True)
+        # assert list(H.edges) == [("n0", "n1")]
+        assert H.has_edge("n0", "n1", key="e0")
+        assert H.nodes["n0"]["label"] == "1"
+        assert H.nodes["n1"]["label"] == "2"
+        assert H.nodes["n2"]["label"] == "3"
+
+        H = eg.parse_graphml(data)
+        # assert list(H.edges) == [("n0", "n1")]
+        assert H["n0"]["n1"]["id"] == "e0"
+        assert H.nodes["n0"]["label"] == "1"
+        assert H.nodes["n1"]["label"] == "2"
+        assert H.nodes["n2"]["label"] == "3"
+
+    def test_bool(self):
+        s = """<?xml version="1.0" encoding="UTF-8"?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <key id="d0" for="node" attr.name="test" attr.type="boolean">
+    <default>false</default>
+  </key>
+  <graph id="G" edgedefault="directed">
+    <node id="n0">
+      <data key="d0">true</data>
+    </node>
+    <node id="n1"/>
+    <node id="n2">
+      <data key="d0">false</data>
+    </node>
+    <node id="n3">
+      <data key="d0">FaLsE</data>
+    </node>
+    <node id="n4">
+      <data key="d0">True</data>
+    </node>
+    <node id="n5">
+      <data key="d0">0</data>
+    </node>
+    <node id="n6">
+      <data key="d0">1</data>
+    </node>
+  </graph>
+</graphml>
+"""
+        fh = io.BytesIO(s.encode("UTF-8"))
+        G = eg.read_graphml(fh)
+        H = eg.parse_graphml(s)
+        for graph in [G, H]:
+            assert graph.nodes["n0"]["test"]
+            assert not graph.nodes["n2"]["test"]
+            assert not graph.nodes["n3"]["test"]
+            assert graph.nodes["n4"]["test"]
+            assert not graph.nodes["n5"]["test"]
+            assert graph.nodes["n6"]["test"]
+
+    def test_graphml_header_line(self):
+        good = """<?xml version="1.0" encoding="UTF-8" standalone="no"?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <key id="d0" for="node" attr.name="test" attr.type="boolean">
+    <default>false</default>
+  </key>
+  <graph id="G">
+    <node id="n0">
+      <data key="d0">true</data>
+    </node>
+  </graph>
+</graphml>
+"""
+        bad = """<?xml version="1.0" encoding="UTF-8" standalone="no"?>
+<graphml>
+  <key id="d0" for="node" attr.name="test" attr.type="boolean">
+    <default>false</default>
+  </key>
+  <graph id="G">
+    <node id="n0">
+      <data key="d0">true</data>
+    </node>
+  </graph>
+</graphml>
+"""
+        ugly = """<?xml version="1.0" encoding="UTF-8" standalone="no"?>
+<graphml xmlns="https://ghghgh">
+  <key id="d0" for="node" attr.name="test" attr.type="boolean">
+    <default>false</default>
+  </key>
+  <graph id="G">
+    <node id="n0">
+      <data key="d0">true</data>
+    </node>
+  </graph>
+</graphml>
+"""
+        for s in (good, bad):
+            fh = io.BytesIO(s.encode("UTF-8"))
+            G = eg.read_graphml(fh)
+            H = eg.parse_graphml(s)
+            for graph in [G, H]:
+                assert graph.nodes["n0"]["test"]
+
+        fh = io.BytesIO(ugly.encode("UTF-8"))
+        pytest.raises(eg.EasyGraphError, eg.read_graphml, fh)
+        pytest.raises(eg.EasyGraphError, eg.parse_graphml, ugly)
+
+    def test_read_attributes_with_groups(self):
+        data = """\
+<?xml version="1.0" encoding="UTF-8" standalone="no"?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:java="http://www.yworks.com/xml/yfiles-common/1.0/java" xmlns:sys="http://www.yworks.com/xml/yfiles-common/markup/primitives/2.0" xmlns:x="http://www.yworks.com/xml/yfiles-common/markup/2.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:y="http://www.yworks.com/xml/graphml" xmlns:yed="http://www.yworks.com/xml/yed/3" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://www.yworks.com/xml/schema/graphml/1.1/ygraphml.xsd">
+  <!--Created by yEd 3.17-->
+  <key attr.name="Description" attr.type="string" for="graph" id="d0"/>
+  <key for="port" id="d1" yfiles.type="portgraphics"/>
+  <key for="port" id="d2" yfiles.type="portgeometry"/>
+  <key for="port" id="d3" yfiles.type="portuserdata"/>
+  <key attr.name="CustomProperty" attr.type="string" for="node" id="d4">
+    <default/>
+  </key>
+  <key attr.name="url" attr.type="string" for="node" id="d5"/>
+  <key attr.name="description" attr.type="string" for="node" id="d6"/>
+  <key for="node" id="d7" yfiles.type="nodegraphics"/>
+  <key for="graphml" id="d8" yfiles.type="resources"/>
+  <key attr.name="url" attr.type="string" for="edge" id="d9"/>
+  <key attr.name="description" attr.type="string" for="edge" id="d10"/>
+  <key for="edge" id="d11" yfiles.type="edgegraphics"/>
+  <graph edgedefault="directed" id="G">
+    <data key="d0"/>
+    <node id="n0">
+      <data key="d4"><![CDATA[CustomPropertyValue]]></data>
+      <data key="d6"/>
+      <data key="d7">
+        <y:ShapeNode>
+          <y:Geometry height="30.0" width="30.0" x="125.0" y="-255.4611111111111"/>
+          <y:Fill color="#FFCC00" transparent="false"/>
+          <y:BorderStyle color="#000000" raised="false" type="line" width="1.0"/>
+          <y:NodeLabel alignment="center" autoSizePolicy="content" fontFamily="Dialog" fontSize="12" fontStyle="plain" hasBackgroundColor="false" hasLineColor="false" height="17.96875" horizontalTextPosition="center" iconTextGap="4" modelName="custom" textColor="#000000" verticalTextPosition="bottom" visible="true" width="11.634765625" x="9.1826171875" y="6.015625">2<y:LabelModel>
+              <y:SmartNodeLabelModel distance="4.0"/>
+            </y:LabelModel>
+            <y:ModelParameter>
+              <y:SmartNodeLabelModelParameter labelRatioX="0.0" labelRatioY="0.0" nodeRatioX="0.0" nodeRatioY="0.0" offsetX="0.0" offsetY="0.0" upX="0.0" upY="-1.0"/>
+            </y:ModelParameter>
+          </y:NodeLabel>
+          <y:Shape type="rectangle"/>
+        </y:ShapeNode>
+      </data>
+    </node>
+    <node id="n1" yfiles.foldertype="group">
+      <data key="d4"><![CDATA[CustomPropertyValue]]></data>
+      <data key="d5"/>
+      <data key="d6"/>
+      <data key="d7">
+        <y:ProxyAutoBoundsNode>
+          <y:Realizers active="0">
+            <y:GroupNode>
+              <y:Geometry height="250.38333333333333" width="140.0" x="-30.0" y="-330.3833333333333"/>
+              <y:Fill color="#F5F5F5" transparent="false"/>
+              <y:BorderStyle color="#000000" type="dashed" width="1.0"/>
+              <y:NodeLabel alignment="right" autoSizePolicy="node_width" backgroundColor="#EBEBEB" borderDistance="0.0" fontFamily="Dialog" fontSize="15" fontStyle="plain" hasLineColor="false" height="21.4609375" horizontalTextPosition="center" iconTextGap="4" modelName="internal" modelPosition="t" textColor="#000000" verticalTextPosition="bottom" visible="true" width="140.0" x="0.0" y="0.0">Group 3</y:NodeLabel>
+              <y:Shape type="roundrectangle"/>
+              <y:State closed="false" closedHeight="50.0" closedWidth="50.0" innerGraphDisplayEnabled="false"/>
+              <y:Insets bottom="15" bottomF="15.0" left="15" leftF="15.0" right="15" rightF="15.0" top="15" topF="15.0"/>
+              <y:BorderInsets bottom="1" bottomF="1.0" left="0" leftF="0.0" right="0" rightF="0.0" top="1" topF="1.0001736111111086"/>
+            </y:GroupNode>
+            <y:GroupNode>
+              <y:Geometry height="50.0" width="50.0" x="0.0" y="60.0"/>
+              <y:Fill color="#F5F5F5" transparent="false"/>
+              <y:BorderStyle color="#000000" type="dashed" width="1.0"/>
+              <y:NodeLabel alignment="right" autoSizePolicy="node_width" backgroundColor="#EBEBEB" borderDistance="0.0" fontFamily="Dialog" fontSize="15" fontStyle="plain" hasLineColor="false" height="21.4609375" horizontalTextPosition="center" iconTextGap="4" modelName="internal" modelPosition="t" textColor="#000000" verticalTextPosition="bottom" visible="true" width="65.201171875" x="-7.6005859375" y="0.0">Folder 3</y:NodeLabel>
+              <y:Shape type="roundrectangle"/>
+              <y:State closed="true" closedHeight="50.0" closedWidth="50.0" innerGraphDisplayEnabled="false"/>
+              <y:Insets bottom="5" bottomF="5.0" left="5" leftF="5.0" right="5" rightF="5.0" top="5" topF="5.0"/>
+              <y:BorderInsets bottom="0" bottomF="0.0" left="0" leftF="0.0" right="0" rightF="0.0" top="0" topF="0.0"/>
+            </y:GroupNode>
+          </y:Realizers>
+        </y:ProxyAutoBoundsNode>
+      </data>
+      <graph edgedefault="directed" id="n1:">
+        <node id="n1::n0" yfiles.foldertype="group">
+          <data key="d4"><![CDATA[CustomPropertyValue]]></data>
+          <data key="d5"/>
+          <data key="d6"/>
+          <data key="d7">
+            <y:ProxyAutoBoundsNode>
+              <y:Realizers active="0">
+                <y:GroupNode>
+                  <y:Geometry height="83.46111111111111" width="110.0" x="-15.0" y="-292.9222222222222"/>
+                  <y:Fill color="#F5F5F5" transparent="false"/>
+                  <y:BorderStyle color="#000000" type="dashed" width="1.0"/>
+                  <y:NodeLabel alignment="right" autoSizePolicy="node_width" backgroundColor="#EBEBEB" borderDistance="0.0" fontFamily="Dialog" fontSize="15" fontStyle="plain" hasLineColor="false" height="21.4609375" horizontalTextPosition="center" iconTextGap="4" modelName="internal" modelPosition="t" textColor="#000000" verticalTextPosition="bottom" visible="true" width="110.0" x="0.0" y="0.0">Group 1</y:NodeLabel>
+                  <y:Shape type="roundrectangle"/>
+                  <y:State closed="false" closedHeight="50.0" closedWidth="50.0" innerGraphDisplayEnabled="false"/>
+                  <y:Insets bottom="15" bottomF="15.0" left="15" leftF="15.0" right="15" rightF="15.0" top="15" topF="15.0"/>
+                  <y:BorderInsets bottom="1" bottomF="1.0" left="0" leftF="0.0" right="0" rightF="0.0" top="1" topF="1.0001736111111086"/>
+                </y:GroupNode>
+                <y:GroupNode>
+                  <y:Geometry height="50.0" width="50.0" x="0.0" y="60.0"/>
+                  <y:Fill color="#F5F5F5" transparent="false"/>
+                  <y:BorderStyle color="#000000" type="dashed" width="1.0"/>
+                  <y:NodeLabel alignment="right" autoSizePolicy="node_width" backgroundColor="#EBEBEB" borderDistance="0.0" fontFamily="Dialog" fontSize="15" fontStyle="plain" hasLineColor="false" height="21.4609375" horizontalTextPosition="center" iconTextGap="4" modelName="internal" modelPosition="t" textColor="#000000" verticalTextPosition="bottom" visible="true" width="65.201171875" x="-7.6005859375" y="0.0">Folder 1</y:NodeLabel>
+                  <y:Shape type="roundrectangle"/>
+                  <y:State closed="true" closedHeight="50.0" closedWidth="50.0" innerGraphDisplayEnabled="false"/>
+                  <y:Insets bottom="5" bottomF="5.0" left="5" leftF="5.0" right="5" rightF="5.0" top="5" topF="5.0"/>
+                  <y:BorderInsets bottom="0" bottomF="0.0" left="0" leftF="0.0" right="0" rightF="0.0" top="0" topF="0.0"/>
+                </y:GroupNode>
+              </y:Realizers>
+            </y:ProxyAutoBoundsNode>
+          </data>
+          <graph edgedefault="directed" id="n1::n0:">
+            <node id="n1::n0::n0">
+              <data key="d4"><![CDATA[CustomPropertyValue]]></data>
+              <data key="d6"/>
+              <data key="d7">
+                <y:ShapeNode>
+                  <y:Geometry height="30.0" width="30.0" x="50.0" y="-255.4611111111111"/>
+                  <y:Fill color="#FFCC00" transparent="false"/>
+                  <y:BorderStyle color="#000000" raised="false" type="line" width="1.0"/>
+                  <y:NodeLabel alignment="center" autoSizePolicy="content" fontFamily="Dialog" fontSize="12" fontStyle="plain" hasBackgroundColor="false" hasLineColor="false" height="17.96875" horizontalTextPosition="center" iconTextGap="4" modelName="custom" textColor="#000000" verticalTextPosition="bottom" visible="true" width="11.634765625" x="9.1826171875" y="6.015625">1<y:LabelModel>
+                      <y:SmartNodeLabelModel distance="4.0"/>
+                    </y:LabelModel>
+                    <y:ModelParameter>
+                      <y:SmartNodeLabelModelParameter labelRatioX="0.0" labelRatioY="0.0" nodeRatioX="0.0" nodeRatioY="0.0" offsetX="0.0" offsetY="0.0" upX="0.0" upY="-1.0"/>
+                    </y:ModelParameter>
+                  </y:NodeLabel>
+                  <y:Shape type="rectangle"/>
+                </y:ShapeNode>
+              </data>
+            </node>
+            <node id="n1::n0::n1">
+              <data key="d4"><![CDATA[CustomPropertyValue]]></data>
+              <data key="d6"/>
+              <data key="d7">
+                <y:ShapeNode>
+                  <y:Geometry height="30.0" width="30.0" x="0.0" y="-255.4611111111111"/>
+                  <y:Fill color="#FFCC00" transparent="false"/>
+                  <y:BorderStyle color="#000000" raised="false" type="line" width="1.0"/>
+                  <y:NodeLabel alignment="center" autoSizePolicy="content" fontFamily="Dialog" fontSize="12" fontStyle="plain" hasBackgroundColor="false" hasLineColor="false" height="17.96875" horizontalTextPosition="center" iconTextGap="4" modelName="custom" textColor="#000000" verticalTextPosition="bottom" visible="true" width="11.634765625" x="9.1826171875" y="6.015625">3<y:LabelModel>
+                      <y:SmartNodeLabelModel distance="4.0"/>
+                    </y:LabelModel>
+                    <y:ModelParameter>
+                      <y:SmartNodeLabelModelParameter labelRatioX="0.0" labelRatioY="0.0" nodeRatioX="0.0" nodeRatioY="0.0" offsetX="0.0" offsetY="0.0" upX="0.0" upY="-1.0"/>
+                    </y:ModelParameter>
+                  </y:NodeLabel>
+                  <y:Shape type="rectangle"/>
+                </y:ShapeNode>
+              </data>
+            </node>
+          </graph>
+        </node>
+        <node id="n1::n1" yfiles.foldertype="group">
+          <data key="d4"><![CDATA[CustomPropertyValue]]></data>
+          <data key="d5"/>
+          <data key="d6"/>
+          <data key="d7">
+            <y:ProxyAutoBoundsNode>
+              <y:Realizers active="0">
+                <y:GroupNode>
+                  <y:Geometry height="83.46111111111111" width="110.0" x="-15.0" y="-179.4611111111111"/>
+                  <y:Fill color="#F5F5F5" transparent="false"/>
+                  <y:BorderStyle color="#000000" type="dashed" width="1.0"/>
+                  <y:NodeLabel alignment="right" autoSizePolicy="node_width" backgroundColor="#EBEBEB" borderDistance="0.0" fontFamily="Dialog" fontSize="15" fontStyle="plain" hasLineColor="false" height="21.4609375" horizontalTextPosition="center" iconTextGap="4" modelName="internal" modelPosition="t" textColor="#000000" verticalTextPosition="bottom" visible="true" width="110.0" x="0.0" y="0.0">Group 2</y:NodeLabel>
+                  <y:Shape type="roundrectangle"/>
+                  <y:State closed="false" closedHeight="50.0" closedWidth="50.0" innerGraphDisplayEnabled="false"/>
+                  <y:Insets bottom="15" bottomF="15.0" left="15" leftF="15.0" right="15" rightF="15.0" top="15" topF="15.0"/>
+                  <y:BorderInsets bottom="1" bottomF="1.0" left="0" leftF="0.0" right="0" rightF="0.0" top="1" topF="1.0001736111111086"/>
+                </y:GroupNode>
+                <y:GroupNode>
+                  <y:Geometry height="50.0" width="50.0" x="0.0" y="60.0"/>
+                  <y:Fill color="#F5F5F5" transparent="false"/>
+                  <y:BorderStyle color="#000000" type="dashed" width="1.0"/>
+                  <y:NodeLabel alignment="right" autoSizePolicy="node_width" backgroundColor="#EBEBEB" borderDistance="0.0" fontFamily="Dialog" fontSize="15" fontStyle="plain" hasLineColor="false" height="21.4609375" horizontalTextPosition="center" iconTextGap="4" modelName="internal" modelPosition="t" textColor="#000000" verticalTextPosition="bottom" visible="true" width="65.201171875" x="-7.6005859375" y="0.0">Folder 2</y:NodeLabel>
+                  <y:Shape type="roundrectangle"/>
+                  <y:State closed="true" closedHeight="50.0" closedWidth="50.0" innerGraphDisplayEnabled="false"/>
+                  <y:Insets bottom="5" bottomF="5.0" left="5" leftF="5.0" right="5" rightF="5.0" top="5" topF="5.0"/>
+                  <y:BorderInsets bottom="0" bottomF="0.0" left="0" leftF="0.0" right="0" rightF="0.0" top="0" topF="0.0"/>
+                </y:GroupNode>
+              </y:Realizers>
+            </y:ProxyAutoBoundsNode>
+          </data>
+          <graph edgedefault="directed" id="n1::n1:">
+            <node id="n1::n1::n0">
+              <data key="d4"><![CDATA[CustomPropertyValue]]></data>
+              <data key="d6"/>
+              <data key="d7">
+                <y:ShapeNode>
+                  <y:Geometry height="30.0" width="30.0" x="0.0" y="-142.0"/>
+                  <y:Fill color="#FFCC00" transparent="false"/>
+                  <y:BorderStyle color="#000000" raised="false" type="line" width="1.0"/>
+                  <y:NodeLabel alignment="center" autoSizePolicy="content" fontFamily="Dialog" fontSize="12" fontStyle="plain" hasBackgroundColor="false" hasLineColor="false" height="17.96875" horizontalTextPosition="center" iconTextGap="4" modelName="custom" textColor="#000000" verticalTextPosition="bottom" visible="true" width="11.634765625" x="9.1826171875" y="6.015625">5<y:LabelModel>
+                      <y:SmartNodeLabelModel distance="4.0"/>
+                    </y:LabelModel>
+                    <y:ModelParameter>
+                      <y:SmartNodeLabelModelParameter labelRatioX="0.0" labelRatioY="0.0" nodeRatioX="0.0" nodeRatioY="0.0" offsetX="0.0" offsetY="0.0" upX="0.0" upY="-1.0"/>
+                    </y:ModelParameter>
+                  </y:NodeLabel>
+                  <y:Shape type="rectangle"/>
+                </y:ShapeNode>
+              </data>
+            </node>
+            <node id="n1::n1::n1">
+              <data key="d4"><![CDATA[CustomPropertyValue]]></data>
+              <data key="d6"/>
+              <data key="d7">
+                <y:ShapeNode>
+                  <y:Geometry height="30.0" width="30.0" x="50.0" y="-142.0"/>
+                  <y:Fill color="#FFCC00" transparent="false"/>
+                  <y:BorderStyle color="#000000" raised="false" type="line" width="1.0"/>
+                  <y:NodeLabel alignment="center" autoSizePolicy="content" fontFamily="Dialog" fontSize="12" fontStyle="plain" hasBackgroundColor="false" hasLineColor="false" height="17.96875" horizontalTextPosition="center" iconTextGap="4" modelName="custom" textColor="#000000" verticalTextPosition="bottom" visible="true" width="11.634765625" x="9.1826171875" y="6.015625">6<y:LabelModel>
+                      <y:SmartNodeLabelModel distance="4.0"/>
+                    </y:LabelModel>
+                    <y:ModelParameter>
+                      <y:SmartNodeLabelModelParameter labelRatioX="0.0" labelRatioY="0.0" nodeRatioX="0.0" nodeRatioY="0.0" offsetX="0.0" offsetY="0.0" upX="0.0" upY="-1.0"/>
+                    </y:ModelParameter>
+                  </y:NodeLabel>
+                  <y:Shape type="rectangle"/>
+                </y:ShapeNode>
+              </data>
+            </node>
+          </graph>
+        </node>
+      </graph>
+    </node>
+    <node id="n2">
+      <data key="d4"><![CDATA[CustomPropertyValue]]></data>
+      <data key="d6"/>
+      <data key="d7">
+        <y:ShapeNode>
+          <y:Geometry height="30.0" width="30.0" x="125.0" y="-142.0"/>
+          <y:Fill color="#FFCC00" transparent="false"/>
+          <y:BorderStyle color="#000000" raised="false" type="line" width="1.0"/>
+          <y:NodeLabel alignment="center" autoSizePolicy="content" fontFamily="Dialog" fontSize="12" fontStyle="plain" hasBackgroundColor="false" hasLineColor="false" height="17.96875" horizontalTextPosition="center" iconTextGap="4" modelName="custom" textColor="#000000" verticalTextPosition="bottom" visible="true" width="11.634765625" x="9.1826171875" y="6.015625">9<y:LabelModel>
+              <y:SmartNodeLabelModel distance="4.0"/>
+            </y:LabelModel>
+            <y:ModelParameter>
+              <y:SmartNodeLabelModelParameter labelRatioX="0.0" labelRatioY="0.0" nodeRatioX="0.0" nodeRatioY="0.0" offsetX="0.0" offsetY="0.0" upX="0.0" upY="-1.0"/>
+            </y:ModelParameter>
+          </y:NodeLabel>
+          <y:Shape type="rectangle"/>
+        </y:ShapeNode>
+      </data>
+    </node>
+    <edge id="n1::n1::e0" source="n1::n1::n0" target="n1::n1::n1">
+      <data key="d10"/>
+      <data key="d11">
+        <y:PolyLineEdge>
+          <y:Path sx="15.0" sy="-0.0" tx="-15.0" ty="-0.0"/>
+          <y:LineStyle color="#000000" type="line" width="1.0"/>
+          <y:Arrows source="none" target="standard"/>
+          <y:BendStyle smoothed="false"/>
+        </y:PolyLineEdge>
+      </data>
+    </edge>
+    <edge id="n1::n0::e0" source="n1::n0::n1" target="n1::n0::n0">
+      <data key="d10"/>
+      <data key="d11">
+        <y:PolyLineEdge>
+          <y:Path sx="15.0" sy="-0.0" tx="-15.0" ty="-0.0"/>
+          <y:LineStyle color="#000000" type="line" width="1.0"/>
+          <y:Arrows source="none" target="standard"/>
+          <y:BendStyle smoothed="false"/>
+        </y:PolyLineEdge>
+      </data>
+    </edge>
+    <edge id="e0" source="n1::n0::n0" target="n0">
+      <data key="d10"/>
+      <data key="d11">
+        <y:PolyLineEdge>
+          <y:Path sx="15.0" sy="-0.0" tx="-15.0" ty="-0.0"/>
+          <y:LineStyle color="#000000" type="line" width="1.0"/>
+          <y:Arrows source="none" target="standard"/>
+          <y:BendStyle smoothed="false"/>
+        </y:PolyLineEdge>
+      </data>
+    </edge>
+    <edge id="e1" source="n1::n1::n1" target="n2">
+      <data key="d10"/>
+      <data key="d11">
+        <y:PolyLineEdge>
+          <y:Path sx="15.0" sy="-0.0" tx="-15.0" ty="-0.0"/>
+          <y:LineStyle color="#000000" type="line" width="1.0"/>
+          <y:Arrows source="none" target="standard"/>
+          <y:BendStyle smoothed="false"/>
+        </y:PolyLineEdge>
+      </data>
+    </edge>
+  </graph>
+  <data key="d8">
+    <y:Resources/>
+  </data>
+</graphml>
+"""
+        # verify that nodes / attributes are correctly read when part of a group
+        fh = io.BytesIO(data.encode("UTF-8"))
+        G = eg.read_graphml(fh)
+        data = [x for _, x in G.nodes.items()]
+        assert len(data) == 9
+        for node_data in data:
+            assert node_data["CustomProperty"] != ""
+
+    def test_long_attribute_type(self):
+        # test that graphs with attr.type="long" (as produced by botch and
+        # dose3) can be parsed
+        s = """<?xml version='1.0' encoding='utf-8'?>
+<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
+         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
+  <key attr.name="cudfversion" attr.type="long" for="node" id="d6" />
+  <graph edgedefault="directed">
+    <node id="n1">
+      <data key="d6">4284</data>
+    </node>
+  </graph>
+</graphml>"""
+        fh = io.BytesIO(s.encode("UTF-8"))
+        G = eg.read_graphml(fh)
+        expected = [("n1", {"cudfversion": 4284})]
+        assert sorted(G.nodes.items()) == expected
+        fh.seek(0)
+        H = eg.parse_graphml(s)
+        assert sorted(H.nodes.items()) == expected
+
+
+class TestWriteGraphML(BaseGraphML):
+    writer = staticmethod(eg.write_graphml_lxml)
+
+    @classmethod
+    def setup_class(cls):
+        BaseGraphML.setup_class()
+        _ = pytest.importorskip("lxml.etree")
+
+    def test_write_interface(self):
+        try:
+            assert eg.write_graphml == eg.write_graphml_lxml
+        except ImportError:
+            assert eg.write_graphml == eg.write_graphml_xml
+
+    def test_write_read_simple_directed_graphml(self):
+        G = self.simple_directed_graph
+        G.graph["hi"] = "there"
+        fh = io.BytesIO()
+        self.writer(G, fh)
+        fh.seek(0)
+        H = eg.read_graphml(fh)
+        assert sorted(G.nodes) == sorted(H.nodes)
+        assert sorted(G.edges) == sorted(H.edges)
+        assert sorted(G.edges) == sorted(H.edges)
+        self.simple_directed_fh.seek(0)
+
+    def test_GraphMLWriter_add_graphs(self):
+        gmlw = GraphMLWriter()
+        G = self.simple_directed_graph
+        H = G.copy()
+        gmlw.add_graphs([G, H])
+
+    def test_write_read_simple_no_prettyprint(self):
+        G = self.simple_directed_graph
+        G.graph["hi"] = "there"
+        G.graph["id"] = "1"
+        fh = io.BytesIO()
+        self.writer(G, fh, prettyprint=False)
+        fh.seek(0)
+        H = eg.read_graphml(fh)
+        assert sorted(G.nodes) == sorted(H.nodes)
+        assert sorted(G.edges) == sorted(H.edges)
+        assert sorted(G.edges) == sorted(H.edges)
+        self.simple_directed_fh.seek(0)
+
+    def test_write_read_attribute_named_key_ids_graphml(self):
+        from xml.etree.ElementTree import parse
+
+        G = self.attribute_named_key_ids_graph
+        fh = io.BytesIO()
+        self.writer(G, fh, named_key_ids=True)
+        fh.seek(0)
+        H = eg.read_graphml(fh)
+        fh.seek(0)
+
+        assert nodes_equal(G.nodes, H.nodes)
+        assert edges_equal(G.edges, H.edges)
+        assert edges_equal(G.edges, H.edges)
+        self.attribute_named_key_ids_fh.seek(0)
+
+        xml = parse(fh)
+        # Children are the key elements, and the graph element
+        children = list(xml.getroot())
+        assert len(children) == 4
+
+        keys = [child.items() for child in children[:3]]
+
+        assert len(keys) == 3
+        assert ("id", "edge_prop") in keys[0]
+        assert ("attr.name", "edge_prop") in keys[0]
+        assert ("id", "prop2") in keys[1]
+        assert ("attr.name", "prop2") in keys[1]
+        assert ("id", "prop1") in keys[2]
+        assert ("attr.name", "prop1") in keys[2]
+
+        # Confirm the read graph nodes/edge are identical when compared to
+        # default writing behavior.
+        default_behavior_fh = io.BytesIO()
+        eg.write_graphml(G, default_behavior_fh)
+        default_behavior_fh.seek(0)
+        H = eg.read_graphml(default_behavior_fh)
+
+        named_key_ids_behavior_fh = io.BytesIO()
+        eg.write_graphml(G, named_key_ids_behavior_fh, named_key_ids=True)
+        named_key_ids_behavior_fh.seek(0)
+        J = eg.read_graphml(named_key_ids_behavior_fh)
+
+        assert all(n1 == n2 for (n1, n2) in zip(H.nodes, J.nodes))
+        assert all(e1 == e2 for (e1, e2) in zip(H.edges, J.edges))
+
+    def test_write_read_attribute_numeric_type_graphml(self):
+        from xml.etree.ElementTree import parse
+
+        G = self.attribute_numeric_type_graph
+        fh = io.BytesIO()
+        self.writer(G, fh, infer_numeric_types=True)
+        fh.seek(0)
+        H = eg.read_graphml(fh)
+        fh.seek(0)
+
+        assert nodes_equal(G.nodes, H.nodes)
+        assert edges_equal(G.edges, H.edges)
+        assert edges_equal(G.edges, H.edges)
+        self.attribute_numeric_type_fh.seek(0)
+
+        xml = parse(fh)
+        # Children are the key elements, and the graph element
+        children = list(xml.getroot())
+        assert len(children) == 3
+
+        keys = [child.items() for child in children[:2]]
+
+        assert len(keys) == 2
+        assert ("attr.type", "double") in keys[0]
+        assert ("attr.type", "double") in keys[1]
+
+    def test_more_multigraph_keys(self):
+        """Writing keys as edge id attributes means keys become strings.
+        The original keys are stored as data, so read them back in
+        if `str(key) == edge_id`
+        This allows the adjacency to remain the same.
+        """
+        G = eg.MultiGraph()
+        G.add_edges_from([("a", "b", 2), ("a", "b", 3)])
+        fd, fname = tempfile.mkstemp()
+        self.writer(G, fname)
+        H = eg.read_graphml(fname)
+        assert H.is_multigraph()
+        assert edges_equal(G.edges, H.edges)
+        assert G._adj == H._adj
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_default_attribute(self):
+        G = eg.Graph(name="Fred")
+        G.add_node(1, label=1, color="green")
+        eg.add_path(G, [0, 1, 2, 3])
+        G.add_edge(1, 2, weight=3)
+        G.graph["node_default"] = {"color": "yellow"}
+        G.graph["edge_default"] = {"weight": 7}
+        fh = io.BytesIO()
+        self.writer(G, fh)
+        fh.seek(0)
+        H = eg.read_graphml(fh, node_type=int)
+        assert nodes_equal(G.nodes, H.nodes)
+        assert edges_equal(G.edges, H.edges)
+        assert G.graph == H.graph
+
+    def test_mixed_type_attributes(self):
+        G = eg.MultiGraph()
+        G.add_node("n0", special=False)
+        G.add_node("n1", special=0)
+        G.add_edge("n0", "n1", special=False)
+        G.add_edge("n0", "n1", special=0)
+        fh = io.BytesIO()
+        self.writer(G, fh)
+        fh.seek(0)
+        H = eg.read_graphml(fh)
+        assert not H.nodes["n0"]["special"]
+        assert H.nodes["n1"]["special"] == 0
+
+    def test_str_number_mixed_type_attributes(self):
+        G = eg.MultiGraph()
+        G.add_node("n0", special="hello")
+        G.add_node("n1", special=0)
+        G.add_edge("n0", "n1", special="hello")
+        G.add_edge("n0", "n1", special=0)
+        fh = io.BytesIO()
+        self.writer(G, fh)
+        fh.seek(0)
+        H = eg.read_graphml(fh)
+        assert H.nodes["n0"]["special"] == "hello"
+        assert H.nodes["n1"]["special"] == 0
+
+    def test_mixed_int_type_number_attributes(self):
+        np = pytest.importorskip("numpy")
+        G = eg.MultiGraph()
+        G.add_node("n0", special=np.int64(0))
+        G.add_node("n1", special=1)
+        G.add_edge("n0", "n1", special=np.int64(2))
+        G.add_edge("n0", "n1", special=3)
+        fh = io.BytesIO()
+        self.writer(G, fh)
+        fh.seek(0)
+        H = eg.read_graphml(fh)
+        assert H.nodes["n0"]["special"] == 0
+        assert H.nodes["n1"]["special"] == 1
+
+    def test_numpy_float(self):
+        np = pytest.importorskip("numpy")
+        wt = np.float_(3.4)
+        G = eg.Graph([(1, 2, {"weight": wt})])
+        fd, fname = tempfile.mkstemp()
+        self.writer(G, fname)
+        H = eg.read_graphml(fname, node_type=int)
+        assert G._adj == H._adj
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_multigraph_to_graph(self):
+        # test converting multigraph to graph if no parallel edges found
+        G = eg.MultiGraph()
+        G.add_edges_from([("a", "b", 2), ("b", "c", 3)])  # no multiedges
+        fd, fname = tempfile.mkstemp()
+        self.writer(G, fname)
+        H = eg.read_graphml(fname)
+        assert not H.is_multigraph()
+        H = eg.read_graphml(fname, force_multigraph=True)
+        assert H.is_multigraph()
+        os.close(fd)
+        os.unlink(fname)
+
+        # add a multiedge
+        G.add_edge("a", "b", "e-id")
+        fd, fname = tempfile.mkstemp()
+        self.writer(G, fname)
+        H = eg.read_graphml(fname)
+        assert H.is_multigraph()
+        H = eg.read_graphml(fname, force_multigraph=True)
+        assert H.is_multigraph()
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_write_generate_edge_id_from_attribute(self):
+        from xml.etree.ElementTree import parse
+
+        G = eg.Graph()
+        G.add_edges_from([("a", "b"), ("b", "c"), ("a", "c")])
+        edge_attributes = {e[:2]: str(e[:2]) for e in G.edges}
+        eg.set_edge_attributes(G, edge_attributes, "eid")
+        fd, fname = tempfile.mkstemp()
+        # set edge_id_from_attribute e.g. "eid" for write_graphml()
+        self.writer(G, fname, edge_id_from_attribute="eid")
+        # set edge_id_from_attribute e.g. "eid" for generate_graphml()
+        generator = eg.generate_graphml(G, edge_id_from_attribute="eid")
+
+        H = eg.read_graphml(fname)
+        assert nodes_equal(G.nodes, H.nodes)
+
+        # EasyGraph adds explicit edge "id" from file as attribute
+        eg.set_edge_attributes(G, edge_attributes, "id")
+        assert edges_equal(G.edges, H.edges)
+
+        tree = parse(fname)
+        children = list(tree.getroot())
+        assert len(children) == 2
+        edge_ids = [
+            edge.attrib["id"]
+            for edge in tree.getroot().findall(
+                ".//{http://graphml.graphdrawing.org/xmlns}edge"
+            )
+        ]
+        # verify edge id value is equal to specified attribute value
+        assert sorted(edge_ids) == sorted(edge_attributes.values())
+
+        # check graphml generated from generate_graphml()
+        data = "".join(generator)
+        J = eg.parse_graphml(data)
+        assert sorted(G.nodes) == sorted(J.nodes)
+        assert sorted(G.edges) == sorted(J.edges)
+        # EasyGraph adds explicit edge "id" from file as attribute
+        eg.set_edge_attributes(G, edge_attributes, "id")
+        assert edges_equal(G.edges, J.edges)
+
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_multigraph_write_generate_edge_id_from_attribute(self):
+        from xml.etree.ElementTree import parse
+
+        G = eg.MultiGraph()
+        G.add_edges_from([("a", "b"), ("b", "c"), ("a", "c"), ("a", "b")])
+        edge_attributes = {e[:3]: str(e[:3]) for e in G.edges}
+        eg.set_edge_attributes(G, edge_attributes, "eid")
+        fd, fname = tempfile.mkstemp()
+        # set edge_id_from_attribute e.g. "eid" for write_graphml()
+        self.writer(G, fname, edge_id_from_attribute="eid")
+        # set edge_id_from_attribute e.g. "eid" for generate_graphml()
+        generator = eg.generate_graphml(G, edge_id_from_attribute="eid")
+
+        H = eg.read_graphml(fname)
+        assert H.is_multigraph()
+        H = eg.read_graphml(fname, force_multigraph=True)
+        assert H.is_multigraph()
+
+        assert nodes_equal(G.nodes, H.nodes)
+        # assert edges_equal(G.edges, H.edges)x
+        x = [data.get("eid") for u, v, _, data in H.edges]
+        assert sorted(data.get("eid") for u, v, _, data in H.edges) == sorted(
+            edge_attributes.values()
+        )
+        # EasyGraph uses edge_ids as keys in multigraphs if no key
+        assert sorted(key for u, v, key, _ in H.edges) == sorted(
+            edge_attributes.values()
+        )
+
+        tree = parse(fname)
+        children = list(tree.getroot())
+        assert len(children) == 2
+        edge_ids = [
+            edge.attrib["id"]
+            for edge in tree.getroot().findall(
+                ".//{http://graphml.graphdrawing.org/xmlns}edge"
+            )
+        ]
+        # verify edge id value is equal to specified attribute value
+        assert sorted(edge_ids) == sorted(edge_attributes.values())
+
+        # check graphml generated from generate_graphml()
+        graphml_data = "".join(generator)
+        J = eg.parse_graphml(graphml_data)
+        assert J.is_multigraph()
+
+        assert nodes_equal(G.nodes, J.nodes)
+        # assert edges_equal(G.edges, J.edges)
+        assert sorted(data.get("eid") for u, v, _, data in J.edges) == sorted(
+            edge_attributes.values()
+        )
+        # EasyGraph uses edge_ids as keys in multigraphs if no key
+        assert sorted(key for u, v, key, _ in J.edges) == sorted(
+            edge_attributes.values()
+        )
+
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_numpy_float64(self):
+        np = pytest.importorskip("numpy")
+        wt = np.float64(3.4)
+        G = eg.Graph([(1, 2, {"weight": wt})])
+        fd, fname = tempfile.mkstemp()
+        self.writer(G, fname)
+        H = eg.read_graphml(fname, node_type=int)
+        assert G.edges == H.edges
+        wtG = G[1][2]["weight"]
+        wtH = H[1][2]["weight"]
+        assert wtG == pytest.approx(wtH, abs=1e-6)
+        assert type(wtG) == np.float64
+        assert type(wtH) == float
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_numpy_float32(self):
+        np = pytest.importorskip("numpy")
+        wt = np.float32(3.4)
+        G = eg.Graph([(1, 2, {"weight": wt})])
+        fd, fname = tempfile.mkstemp()
+        self.writer(G, fname)
+        H = eg.read_graphml(fname, node_type=int)
+        # assert G.edges == H.edges
+        wtG = G[1][2]["weight"]
+        wtH = H[1][2]["weight"]
+        assert wtG == pytest.approx(wtH, abs=1e-6)
+        assert type(wtG) == np.float32
+        assert type(wtH) == float
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_numpy_float64_inference(self):
+        np = pytest.importorskip("numpy")
+        G = self.attribute_numeric_type_graph
+        fd, fname = tempfile.mkstemp()
+        self.writer(G, fname, infer_numeric_types=True)
+        H = eg.read_graphml(fname)
+        assert G._adj == H._adj
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_unicode_attributes(self):
+        G = eg.Graph()
+        name1 = chr(2344) + chr(123) + chr(6543)
+        name2 = chr(5543) + chr(1543) + chr(324)
+        node_type = str
+        G.add_edge(name1, "Radiohead", foo=name2)
+        fd, fname = tempfile.mkstemp()
+        self.writer(G, fname)
+        H = eg.read_graphml(fname, node_type=node_type)
+        assert G._adj == H._adj
+        os.close(fd)
+        os.unlink(fname)
+
+    def test_unicode_escape(self):
+        # test for handling json escaped stings in python 2 Issue #1880
+        import json
+
+        a = dict(a='{"a": "123"}')  # an object with many chars to escape
+        sa = json.dumps(a)
+        G = eg.Graph()
+        G.graph["test"] = sa
+        fh = io.BytesIO()
+        self.writer(G, fh)
+        fh.seek(0)
+        H = eg.read_graphml(fh)
+        assert G.graph["test"] == H.graph["test"]
```

## easygraph/readwrite/json_graph/node_link.py

 * *Ordering differences only*

```diff
@@ -1,111 +1,111 @@
-from itertools import chain
-from itertools import count
-
-import easygraph as eg
-
-
-__all__ = ["node_link_graph"]
-
-
-_attrs = dict(source="source", target="target", name="id", key="key", link="links")
-
-
-def _to_tuple(x):
-    """Converts lists to tuples, including nested lists.
-
-    All other non-list inputs are passed through unmodified. This function is
-    intended to be used to convert potentially nested lists from json files
-    into valid nodes.
-
-    Examples
-    --------
-    >>> _to_tuple([1, 2, [3, 4]])
-    (1, 2, (3, 4))
-    """
-    if not isinstance(x, (tuple, list)):
-        return x
-    return tuple(map(_to_tuple, x))
-
-
-def node_link_graph(data, directed=False, multigraph=True, attrs=None):
-    """Returns graph from node-link data format.
-
-    Parameters
-    ----------
-    data : dict
-        node-link formatted graph data
-
-    directed : bool
-        If True, and direction not specified in data, return a directed graph.
-
-    multigraph : bool
-        If True, and multigraph not specified in data, return a multigraph.
-
-    attrs : dict
-        A dictionary that contains five keys 'source', 'target', 'name',
-        'key' and 'link'.  The corresponding values provide the attribute
-        names for storing NetworkX-internal graph data.  Default value:
-
-            dict(source='source', target='target', name='id',
-                key='key', link='links')
-
-    Returns
-    -------
-    G : EasyGraph graph
-        A EasyGraph graph object
-
-    Examples
-    --------
-    >>> from easygraph.readwrite import json_graph
-    >>> G = eg.Graph([("A", "B")])
-    >>> data = json_graph.node_link_data(G)
-    >>> H = json_graph.node_link_graph(data)
-
-    Notes
-    -----
-    Attribute 'key' is only used for multigraphs.
-
-    See Also
-    --------
-    node_link_data, adjacency_data, tree_data
-    """
-    # Allow 'attrs' to keep default values.
-    if attrs is None:
-        attrs = _attrs
-    else:
-        attrs.update({k: v for k, v in _attrs.items() if k not in attrs})
-    multigraph = data.get("multigraph", multigraph)
-    directed = data.get("directed", directed)
-    if multigraph:
-        graph = eg.MultiGraph()
-    else:
-        graph = eg.Graph()
-    if directed:
-        graph = graph.to_directed()
-    name = attrs["name"]
-    source = attrs["source"]
-    target = attrs["target"]
-    links = attrs["link"]
-    # Allow 'key' to be omitted from attrs if the graph is not a multigraph.
-    key = None if not multigraph else attrs["key"]
-    graph.graph = data.get("graph", {})
-    c = count()
-    for d in data["nodes"]:
-        node = _to_tuple(d.get(name, next(c)))
-        nodedata = {str(k): v for k, v in d.items() if k != name}
-        graph.add_node(node, **nodedata)
-    for d in data[links]:
-        src = tuple(d[source]) if isinstance(d[source], list) else d[source]
-        tgt = tuple(d[target]) if isinstance(d[target], list) else d[target]
-        if not multigraph:
-            edgedata = {str(k): v for k, v in d.items() if k != source and k != target}
-            graph.add_edge(src, tgt, **edgedata)
-        else:
-            ky = d.get(key, None)
-            edgedata = {
-                str(k): v
-                for k, v in d.items()
-                if k != source and k != target and k != key
-            }
-            graph.add_edge(src, tgt, ky, **edgedata)
-    return graph
+from itertools import chain
+from itertools import count
+
+import easygraph as eg
+
+
+__all__ = ["node_link_graph"]
+
+
+_attrs = dict(source="source", target="target", name="id", key="key", link="links")
+
+
+def _to_tuple(x):
+    """Converts lists to tuples, including nested lists.
+
+    All other non-list inputs are passed through unmodified. This function is
+    intended to be used to convert potentially nested lists from json files
+    into valid nodes.
+
+    Examples
+    --------
+    >>> _to_tuple([1, 2, [3, 4]])
+    (1, 2, (3, 4))
+    """
+    if not isinstance(x, (tuple, list)):
+        return x
+    return tuple(map(_to_tuple, x))
+
+
+def node_link_graph(data, directed=False, multigraph=True, attrs=None):
+    """Returns graph from node-link data format.
+
+    Parameters
+    ----------
+    data : dict
+        node-link formatted graph data
+
+    directed : bool
+        If True, and direction not specified in data, return a directed graph.
+
+    multigraph : bool
+        If True, and multigraph not specified in data, return a multigraph.
+
+    attrs : dict
+        A dictionary that contains five keys 'source', 'target', 'name',
+        'key' and 'link'.  The corresponding values provide the attribute
+        names for storing NetworkX-internal graph data.  Default value:
+
+            dict(source='source', target='target', name='id',
+                key='key', link='links')
+
+    Returns
+    -------
+    G : EasyGraph graph
+        A EasyGraph graph object
+
+    Examples
+    --------
+    >>> from easygraph.readwrite import json_graph
+    >>> G = eg.Graph([("A", "B")])
+    >>> data = json_graph.node_link_data(G)
+    >>> H = json_graph.node_link_graph(data)
+
+    Notes
+    -----
+    Attribute 'key' is only used for multigraphs.
+
+    See Also
+    --------
+    node_link_data, adjacency_data, tree_data
+    """
+    # Allow 'attrs' to keep default values.
+    if attrs is None:
+        attrs = _attrs
+    else:
+        attrs.update({k: v for k, v in _attrs.items() if k not in attrs})
+    multigraph = data.get("multigraph", multigraph)
+    directed = data.get("directed", directed)
+    if multigraph:
+        graph = eg.MultiGraph()
+    else:
+        graph = eg.Graph()
+    if directed:
+        graph = graph.to_directed()
+    name = attrs["name"]
+    source = attrs["source"]
+    target = attrs["target"]
+    links = attrs["link"]
+    # Allow 'key' to be omitted from attrs if the graph is not a multigraph.
+    key = None if not multigraph else attrs["key"]
+    graph.graph = data.get("graph", {})
+    c = count()
+    for d in data["nodes"]:
+        node = _to_tuple(d.get(name, next(c)))
+        nodedata = {str(k): v for k, v in d.items() if k != name}
+        graph.add_node(node, **nodedata)
+    for d in data[links]:
+        src = tuple(d[source]) if isinstance(d[source], list) else d[source]
+        tgt = tuple(d[target]) if isinstance(d[target], list) else d[target]
+        if not multigraph:
+            edgedata = {str(k): v for k, v in d.items() if k != source and k != target}
+            graph.add_edge(src, tgt, **edgedata)
+        else:
+            ky = d.get(key, None)
+            edgedata = {
+                str(k): v
+                for k, v in d.items()
+                if k != source and k != target and k != key
+            }
+            graph.add_edge(src, tgt, ky, **edgedata)
+    return graph
```

## easygraph/readwrite/json_graph/__init__.py

 * *Ordering differences only*

```diff
@@ -1,16 +1,16 @@
-"""
-*********
-JSON data
-*********
-Generate and parse JSON serializable data for NetworkX graphs.
-
-These formats are suitable for use with the d3.js examples https://d3js.org/
-
-The three formats that you can generate with NetworkX are:
-
- - node-link like in the d3.js example https://bl.ocks.org/mbostock/4062045
- - tree like in the d3.js example https://bl.ocks.org/mbostock/4063550
- - adjacency like in the d3.js example https://bost.ocks.org/mike/miserables/
-"""
-
-from easygraph.readwrite.json_graph.node_link import *
+"""
+*********
+JSON data
+*********
+Generate and parse JSON serializable data for NetworkX graphs.
+
+These formats are suitable for use with the d3.js examples https://d3js.org/
+
+The three formats that you can generate with NetworkX are:
+
+ - node-link like in the d3.js example https://bl.ocks.org/mbostock/4062045
+ - tree like in the d3.js example https://bl.ocks.org/mbostock/4063550
+ - adjacency like in the d3.js example https://bost.ocks.org/mike/miserables/
+"""
+
+from easygraph.readwrite.json_graph.node_link import *
```

## easygraph/ml_metrics/base.py

 * *Ordering differences only*

```diff
@@ -1,202 +1,202 @@
-import abc
-
-from collections import defaultdict
-from functools import partial
-from typing import Dict
-from typing import List
-from typing import Union
-
-import torch
-
-from easygraph._global import AUTHOR_EMAIL
-
-
-def format_metric_configs(task: str, metric_configs: List[Union[str, Dict[str, dict]]]):
-    r"""Format metric_configs.
-
-    Args:
-        ``task`` (``str``): The type of the task. The supported types include: ``classification``, ``retrieval`` and ``recommender``.
-        ``metric_configs`` (``Dict[str, Dict[str, Union[str, int]]]``): The metric configs.
-    """
-    task = task.lower()
-    if task == "classification":
-        import easygraph.ml_metrics.classification as module
-
-        available_metrics = module.available_classification_metrics()
-    else:
-        raise ValueError(
-            f"Task {task} is not supported yet. Please email '{AUTHOR_EMAIL}' to"
-            " add it."
-        )
-    metric_list = []
-    for metric in metric_configs:
-        if isinstance(metric, str):
-            marker, func_name = metric, metric
-            assert func_name in available_metrics, (
-                f"{func_name} is not supported yet. Please email '{AUTHOR_EMAIL}' to"
-                " add it."
-            )
-            func = getattr(module, func_name)
-        elif isinstance(metric, dict):
-            assert len(metric) == 1
-            func_name = list(metric.keys())[0]
-            assert func_name in available_metrics, (
-                f"{func_name} is not supported yet. Please email '{AUTHOR_EMAIL}' to"
-                " add it."
-            )
-            params = metric[func_name]
-            func = getattr(module, func_name)
-            func = partial(func, **params)
-            markder_list = []
-            for k, v in params.items():
-                _m = f"{k}@"
-                if isinstance(v, str):
-                    _m += v
-                elif isinstance(v, int):
-                    _m += str(v)
-                elif isinstance(v, float):
-                    _m += f"{v:.4f}"
-                elif isinstance(v, list) or isinstance(v, tuple) or isinstance(v, set):
-                    _m += "_".join([str(_v) for _v in v])
-                else:
-                    _m += str(v)
-                markder_list.append(_m)
-            marker = f"{func_name} -> {' | '.join(markder_list)}"
-        else:
-            raise ValueError
-        metric_list.append({"marker": marker, "func": func, "func_name": func_name})
-    return metric_list
-
-
-class BaseEvaluator:
-    r"""The base class for task-specified metric evaluators.
-
-    Args:
-        ``task`` (``str``): The type of the task. The supported types include: ``classification``, ``retrieval`` and ``recommender``.
-        ``metric_configs`` (``List[Union[str, Dict[str, dict]]]``): The metric configurations. The key is the metric name and the value is the metric parameters.
-        ``validate_index`` (``int``): The specified metric index used for validation. Defaults to ``0``.
-    """
-
-    def __init__(
-        self,
-        task: str,
-        metric_configs: List[Union[str, Dict[str, dict]]],
-        validate_index: int = 0,
-    ):
-        self.validate_index = validate_index
-        metric_configs = format_metric_configs(task, metric_configs)
-        assert validate_index >= 0 and validate_index < len(
-            metric_configs
-        ), "The specified validate metric index is out of range."
-        self.marker_list, self.func_list = [], []
-        for metric in metric_configs:
-            self.marker_list.append(metric["marker"])
-            self.func_list.append(metric["func"])
-        # init batch data containers
-        self.validate_res = []
-        self.test_res_dict = defaultdict(list)
-        self.last_validate_res, self.last_test_res = None, {}
-
-    @abc.abstractmethod
-    def __repr__(self) -> str:
-        r"""Print the Evaluator information."""
-
-    def validate_add_batch(
-        self, batch_y_true: torch.Tensor, batch_y_pred: torch.Tensor
-    ):
-        import numpy as np
-
-        r"""Add batch data for validation.
-
-        Args:
-            ``batch_y_true`` (``torch.Tensor``): The ground truth data. Size :math:`(N_{batch}, -)`.
-            ``batch_y_pred`` (``torch.Tensor``): The predicted data. Size :math:`(N_{batch}, -)`.
-        """
-        batch_res = self.func_list[self.validate_index](
-            batch_y_true, batch_y_pred, ret_batch=True
-        )
-        batch_res = np.array(batch_res)
-        if len(batch_res.shape) == 1:
-            batch_res = batch_res[:, np.newaxis]
-        self.validate_res.append(batch_res)
-
-    def validate_epoch_res(self):
-        r"""For all added batch data, return the result of the evaluation on the specified ``validate_index``-th metric.
-        """
-        import numpy as np
-
-        if self.validate_res == [] and self.last_validate_res is not None:
-            return self.last_validate_res
-        assert self.validate_res != [], "No batch data added for validation."
-        self.last_validate_res = np.vstack(self.validate_res).mean(0).item()
-        # clear batch cache
-        self.validate_res = []
-        return self.last_validate_res
-
-    def test_add_batch(self, batch_y_true: torch.Tensor, batch_y_pred: torch.Tensor):
-        r"""Add batch data for testing.
-
-        Args:
-            ``batch_y_true`` (``torch.Tensor``): The ground truth data. Size :math:`(N_{batch}, -)`.
-            ``batch_y_pred`` (``torch.Tensor``): The predicted data. Size :math:`(N_{batch}, -)`.
-        """
-        import numpy as np
-
-        for name, func in zip(self.marker_list, self.func_list):
-            batch_res = func(batch_y_true, batch_y_pred, ret_batch=True)
-            if not isinstance(batch_res, tuple):
-                batch_res = np.array(batch_res)
-                if len(batch_res.shape) == 1:
-                    batch_res = batch_res[:, np.newaxis]
-                self.test_res_dict[name].append(batch_res)
-            else:
-                if self.test_res_dict[name] == []:
-                    self.test_res_dict[name] = [list() for _ in range(len(batch_res))]
-                for idx, batch_sub_res in enumerate(batch_res):
-                    batch_sub_res = np.array(batch_sub_res)
-                    if len(batch_sub_res.shape) == 1:
-                        batch_sub_res = batch_sub_res[:, np.newaxis]
-                    self.test_res_dict[name][idx].append(batch_sub_res)
-
-    def test_epoch_res(self):
-        r"""For all added batch data, return results of the evaluation on all the ml_metrics in ``metric_configs``.
-        """
-        import numpy as np
-
-        if self.test_res_dict == {} and self.last_test_res is not None:
-            return self.last_test_res
-        assert self.test_res_dict != {}, "No batch data added for testing."
-        for name, res_list in self.test_res_dict.items():
-            if not isinstance(res_list[0], list):
-                self.last_test_res[name] = (
-                    np.vstack(res_list).mean(0).squeeze().tolist()
-                )
-            else:
-                self.last_test_res[name] = [
-                    np.vstack(sub_res_list).mean(0).squeeze().tolist()
-                    for sub_res_list in res_list
-                ]
-        # clear batch cache
-        self.test_res_dict = defaultdict(list)
-        return self.last_test_res
-
-    def validate(self, y_true: torch.LongTensor, y_pred: torch.Tensor):
-        r"""Return the result of the evaluation on the specified ``validate_index``-th metric.
-
-        Args:
-            ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, -)`.
-            ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, -)`.
-        """
-        return self.func_list[self.validate_index](y_true, y_pred)
-
-    def test(self, y_true: torch.LongTensor, y_pred: torch.Tensor):
-        r"""Return results of the evaluation on all the ml_metrics in ``metric_configs``.
-
-        Args:
-            ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, -)`.
-            ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, -)`.
-        """
-        return {
-            name: func(y_true, y_pred)
-            for name, func in zip(self.marker_list, self.func_list)
-        }
+import abc
+
+from collections import defaultdict
+from functools import partial
+from typing import Dict
+from typing import List
+from typing import Union
+
+import torch
+
+from easygraph._global import AUTHOR_EMAIL
+
+
+def format_metric_configs(task: str, metric_configs: List[Union[str, Dict[str, dict]]]):
+    r"""Format metric_configs.
+
+    Args:
+        ``task`` (``str``): The type of the task. The supported types include: ``classification``, ``retrieval`` and ``recommender``.
+        ``metric_configs`` (``Dict[str, Dict[str, Union[str, int]]]``): The metric configs.
+    """
+    task = task.lower()
+    if task == "classification":
+        import easygraph.ml_metrics.classification as module
+
+        available_metrics = module.available_classification_metrics()
+    else:
+        raise ValueError(
+            f"Task {task} is not supported yet. Please email '{AUTHOR_EMAIL}' to"
+            " add it."
+        )
+    metric_list = []
+    for metric in metric_configs:
+        if isinstance(metric, str):
+            marker, func_name = metric, metric
+            assert func_name in available_metrics, (
+                f"{func_name} is not supported yet. Please email '{AUTHOR_EMAIL}' to"
+                " add it."
+            )
+            func = getattr(module, func_name)
+        elif isinstance(metric, dict):
+            assert len(metric) == 1
+            func_name = list(metric.keys())[0]
+            assert func_name in available_metrics, (
+                f"{func_name} is not supported yet. Please email '{AUTHOR_EMAIL}' to"
+                " add it."
+            )
+            params = metric[func_name]
+            func = getattr(module, func_name)
+            func = partial(func, **params)
+            markder_list = []
+            for k, v in params.items():
+                _m = f"{k}@"
+                if isinstance(v, str):
+                    _m += v
+                elif isinstance(v, int):
+                    _m += str(v)
+                elif isinstance(v, float):
+                    _m += f"{v:.4f}"
+                elif isinstance(v, list) or isinstance(v, tuple) or isinstance(v, set):
+                    _m += "_".join([str(_v) for _v in v])
+                else:
+                    _m += str(v)
+                markder_list.append(_m)
+            marker = f"{func_name} -> {' | '.join(markder_list)}"
+        else:
+            raise ValueError
+        metric_list.append({"marker": marker, "func": func, "func_name": func_name})
+    return metric_list
+
+
+class BaseEvaluator:
+    r"""The base class for task-specified metric evaluators.
+
+    Args:
+        ``task`` (``str``): The type of the task. The supported types include: ``classification``, ``retrieval`` and ``recommender``.
+        ``metric_configs`` (``List[Union[str, Dict[str, dict]]]``): The metric configurations. The key is the metric name and the value is the metric parameters.
+        ``validate_index`` (``int``): The specified metric index used for validation. Defaults to ``0``.
+    """
+
+    def __init__(
+        self,
+        task: str,
+        metric_configs: List[Union[str, Dict[str, dict]]],
+        validate_index: int = 0,
+    ):
+        self.validate_index = validate_index
+        metric_configs = format_metric_configs(task, metric_configs)
+        assert validate_index >= 0 and validate_index < len(
+            metric_configs
+        ), "The specified validate metric index is out of range."
+        self.marker_list, self.func_list = [], []
+        for metric in metric_configs:
+            self.marker_list.append(metric["marker"])
+            self.func_list.append(metric["func"])
+        # init batch data containers
+        self.validate_res = []
+        self.test_res_dict = defaultdict(list)
+        self.last_validate_res, self.last_test_res = None, {}
+
+    @abc.abstractmethod
+    def __repr__(self) -> str:
+        r"""Print the Evaluator information."""
+
+    def validate_add_batch(
+        self, batch_y_true: torch.Tensor, batch_y_pred: torch.Tensor
+    ):
+        import numpy as np
+
+        r"""Add batch data for validation.
+
+        Args:
+            ``batch_y_true`` (``torch.Tensor``): The ground truth data. Size :math:`(N_{batch}, -)`.
+            ``batch_y_pred`` (``torch.Tensor``): The predicted data. Size :math:`(N_{batch}, -)`.
+        """
+        batch_res = self.func_list[self.validate_index](
+            batch_y_true, batch_y_pred, ret_batch=True
+        )
+        batch_res = np.array(batch_res)
+        if len(batch_res.shape) == 1:
+            batch_res = batch_res[:, np.newaxis]
+        self.validate_res.append(batch_res)
+
+    def validate_epoch_res(self):
+        r"""For all added batch data, return the result of the evaluation on the specified ``validate_index``-th metric.
+        """
+        import numpy as np
+
+        if self.validate_res == [] and self.last_validate_res is not None:
+            return self.last_validate_res
+        assert self.validate_res != [], "No batch data added for validation."
+        self.last_validate_res = np.vstack(self.validate_res).mean(0).item()
+        # clear batch cache
+        self.validate_res = []
+        return self.last_validate_res
+
+    def test_add_batch(self, batch_y_true: torch.Tensor, batch_y_pred: torch.Tensor):
+        r"""Add batch data for testing.
+
+        Args:
+            ``batch_y_true`` (``torch.Tensor``): The ground truth data. Size :math:`(N_{batch}, -)`.
+            ``batch_y_pred`` (``torch.Tensor``): The predicted data. Size :math:`(N_{batch}, -)`.
+        """
+        import numpy as np
+
+        for name, func in zip(self.marker_list, self.func_list):
+            batch_res = func(batch_y_true, batch_y_pred, ret_batch=True)
+            if not isinstance(batch_res, tuple):
+                batch_res = np.array(batch_res)
+                if len(batch_res.shape) == 1:
+                    batch_res = batch_res[:, np.newaxis]
+                self.test_res_dict[name].append(batch_res)
+            else:
+                if self.test_res_dict[name] == []:
+                    self.test_res_dict[name] = [list() for _ in range(len(batch_res))]
+                for idx, batch_sub_res in enumerate(batch_res):
+                    batch_sub_res = np.array(batch_sub_res)
+                    if len(batch_sub_res.shape) == 1:
+                        batch_sub_res = batch_sub_res[:, np.newaxis]
+                    self.test_res_dict[name][idx].append(batch_sub_res)
+
+    def test_epoch_res(self):
+        r"""For all added batch data, return results of the evaluation on all the ml_metrics in ``metric_configs``.
+        """
+        import numpy as np
+
+        if self.test_res_dict == {} and self.last_test_res is not None:
+            return self.last_test_res
+        assert self.test_res_dict != {}, "No batch data added for testing."
+        for name, res_list in self.test_res_dict.items():
+            if not isinstance(res_list[0], list):
+                self.last_test_res[name] = (
+                    np.vstack(res_list).mean(0).squeeze().tolist()
+                )
+            else:
+                self.last_test_res[name] = [
+                    np.vstack(sub_res_list).mean(0).squeeze().tolist()
+                    for sub_res_list in res_list
+                ]
+        # clear batch cache
+        self.test_res_dict = defaultdict(list)
+        return self.last_test_res
+
+    def validate(self, y_true: torch.LongTensor, y_pred: torch.Tensor):
+        r"""Return the result of the evaluation on the specified ``validate_index``-th metric.
+
+        Args:
+            ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, -)`.
+            ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, -)`.
+        """
+        return self.func_list[self.validate_index](y_true, y_pred)
+
+    def test(self, y_true: torch.LongTensor, y_pred: torch.Tensor):
+        r"""Return results of the evaluation on all the ml_metrics in ``metric_configs``.
+
+        Args:
+            ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, -)`.
+            ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, -)`.
+        """
+        return {
+            name: func(y_true, y_pred)
+            for name, func in zip(self.marker_list, self.func_list)
+        }
```

## easygraph/ml_metrics/classification.py

 * *Ordering differences only*

```diff
@@ -1,162 +1,162 @@
-from typing import Dict
-from typing import List
-from typing import Union
-
-import sklearn.metrics as sm
-import torch
-
-from .base import BaseEvaluator
-
-
-__all__ = [
-    "available_classification_metrics",
-    "accuracy",
-    "f1_score",
-    "confusion_matrix",
-    "VertexClassificationEvaluator",
-]
-
-
-def available_classification_metrics():
-    r"""Return available ml_metrics for the classification task.
-
-    The available ml_metrics are: ``accuracy``, ``f1_score``, ``confusion_matrix``.
-    """
-    return ("accuracy", "f1_score", "confusion_matrix")
-
-
-def _format_inputs(y_true: torch.LongTensor, y_pred: torch.Tensor):
-    r"""Format the inputs.
-
-    Args:
-        ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, )`.
-        ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, N_{class})` or :math:`(N_{samples}, )`.
-    """
-    assert y_true.dim() == 1, "y_true must be 1D torch.LongTensor."
-    assert y_pred.dim() in (1, 2), "y_pred must be 1D or 2D torch.Tensor."
-    y_true = y_true.cpu().detach()
-    if y_pred.dim() == 2:
-        y_pred = y_pred.argmax(dim=1)
-    y_pred = y_pred.cpu().detach()
-    assert y_true.shape == y_pred.shape, "y_true and y_pred must have the same length."
-    return (y_true, y_pred)
-
-
-def accuracy(y_true: torch.LongTensor, y_pred: torch.Tensor):
-    r"""Calculate the accuracy score for the classification task.
-
-    .. math::
-        \text{Accuracy} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{I}(y_i, \hat{y}_i),
-
-    where :math:`\mathcal{I}(\cdot, \cdot)` is the indicator function, which is 1 if the two inputs are equal, and 0 otherwise.
-    :math:`y_i` and :math:`\hat{y}_i` are the ground truth and predicted labels for the i-th sample.
-
-    Args:
-        ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, )`.
-        ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, N_{class})` or :math:`(N_{samples}, )`.
-
-    Examples:
-        >>> import torch
-        >>> import easygraph.ml_metrics as dm
-        >>> y_true = torch.tensor([3, 2, 4])
-        >>> y_pred = torch.tensor([
-                [0.2, 0.3, 0.5, 0.4, 0.3],
-                [0.8, 0.2, 0.3, 0.5, 0.4],
-                [0.2, 0.4, 0.5, 0.2, 0.8],
-            ])
-        >>> dm.classification.accuracy(y_true, y_pred)
-        0.3333333432674408
-    """
-    y_true, y_pred = _format_inputs(y_true, y_pred)
-    return (y_true == y_pred).float().mean().item()
-
-
-def f1_score(y_true: torch.LongTensor, y_pred: torch.Tensor, average: str = "macro"):
-    r"""Calculate the F1 score for the classification task.
-
-    Args:
-        ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, )`.
-        ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, N_{class})` or :math:`(N_{samples}, )`.
-        ``average`` (``str``): The average method. Must be one of "macro", "micro", "weighted".
-
-    Examples:
-        >>> import torch
-        >>> import easygraph.ml_metrics as dm
-        >>> y_true = torch.tensor([3, 2, 4, 0])
-        >>> y_pred = torch.tensor([
-                [0.2, 0.3, 0.5, 0.4, 0.3],
-                [0.8, 0.2, 0.3, 0.5, 0.4],
-                [0.2, 0.4, 0.5, 0.2, 0.8],
-                [0.8, 0.4, 0.5, 0.2, 0.8]
-            ])
-        >>> dm.classification.f1_score(y_true, y_pred, "macro")
-        0.41666666666666663
-        >>> dm.classification.f1_score(y_true, y_pred, "micro")
-        0.5
-        >>> dm.classification.f1_score(y_true, y_pred, "weighted")
-        0.41666666666666663
-    """
-    y_true, y_pred = _format_inputs(y_true, y_pred)
-    return sm.f1_score(y_true, y_pred, average=average)
-
-
-def confusion_matrix(y_true: torch.LongTensor, y_pred: torch.Tensor):
-    r"""Calculate the confusion matrix for the classification task.
-
-    Args:
-        ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, )`.
-        ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, N_{class})` or :math:`(N_{samples}, )`.
-
-    Examples:
-        >>> import torch
-        >>> import easygraph.ml_metrics as dm
-        >>> y_true = torch.tensor([3, 2, 4, 0])
-        >>> y_pred = torch.tensor([
-                [0.2, 0.3, 0.5, 0.4, 0.3],
-                [0.8, 0.2, 0.3, 0.5, 0.4],
-                [0.2, 0.4, 0.5, 0.2, 0.8],
-                [0.8, 0.4, 0.5, 0.2, 0.8]
-            ])
-        >>> dm.classification.confusion_matrix(y_true, y_pred)
-        array([[1, 0, 0, 0],
-               [1, 0, 0, 0],
-               [0, 1, 0, 0],
-               [0, 0, 0, 1]])
-    """
-    y_true, y_pred = _format_inputs(y_true, y_pred)
-    return sm.confusion_matrix(y_true, y_pred)
-
-
-# Vertex Classification Evaluator
-class VertexClassificationEvaluator(BaseEvaluator):
-    r"""Return the metric evaluator for vertex classification task. The supported ml_metrics includes: ``accuracy``, ``f1_score``, ``confusion_matrix``.
-
-    Args:
-        ``metric_configs`` (``List[Union[str, Dict[str, dict]]]``): The metric configurations. The key is the metric name and the value is the metric parameters.
-        ``validate_index`` (``int``): The specified metric index used for validation. Defaults to ``0``.
-    """
-
-    def __init__(
-        self,
-        metric_configs: List[Union[str, Dict[str, dict]]],
-        validate_index: int = 0,
-    ):
-        super().__init__("classification", metric_configs, validate_index)
-
-    def validate(self, y_true: torch.LongTensor, y_pred: torch.Tensor):
-        r"""Return the result of the evaluation on the specified ``validate_index``-th metric.
-
-        Args:
-            ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, )`.
-            ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, N_{class})` or :math:`(N_{samples}, )`.
-        """
-        return super().validate(y_true, y_pred)
-
-    def test(self, y_true: torch.LongTensor, y_pred: torch.Tensor):
-        r"""Return results of the evaluation on all the ml_metrics in ``metric_configs``.
-
-        Args:
-            ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, )`.
-            ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, N_{class})` or :math:`(N_{samples}, )`.
-        """
-        return super().test(y_true, y_pred)
+from typing import Dict
+from typing import List
+from typing import Union
+
+import sklearn.metrics as sm
+import torch
+
+from .base import BaseEvaluator
+
+
+__all__ = [
+    "available_classification_metrics",
+    "accuracy",
+    "f1_score",
+    "confusion_matrix",
+    "VertexClassificationEvaluator",
+]
+
+
+def available_classification_metrics():
+    r"""Return available ml_metrics for the classification task.
+
+    The available ml_metrics are: ``accuracy``, ``f1_score``, ``confusion_matrix``.
+    """
+    return ("accuracy", "f1_score", "confusion_matrix")
+
+
+def _format_inputs(y_true: torch.LongTensor, y_pred: torch.Tensor):
+    r"""Format the inputs.
+
+    Args:
+        ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, )`.
+        ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, N_{class})` or :math:`(N_{samples}, )`.
+    """
+    assert y_true.dim() == 1, "y_true must be 1D torch.LongTensor."
+    assert y_pred.dim() in (1, 2), "y_pred must be 1D or 2D torch.Tensor."
+    y_true = y_true.cpu().detach()
+    if y_pred.dim() == 2:
+        y_pred = y_pred.argmax(dim=1)
+    y_pred = y_pred.cpu().detach()
+    assert y_true.shape == y_pred.shape, "y_true and y_pred must have the same length."
+    return (y_true, y_pred)
+
+
+def accuracy(y_true: torch.LongTensor, y_pred: torch.Tensor):
+    r"""Calculate the accuracy score for the classification task.
+
+    .. math::
+        \text{Accuracy} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{I}(y_i, \hat{y}_i),
+
+    where :math:`\mathcal{I}(\cdot, \cdot)` is the indicator function, which is 1 if the two inputs are equal, and 0 otherwise.
+    :math:`y_i` and :math:`\hat{y}_i` are the ground truth and predicted labels for the i-th sample.
+
+    Args:
+        ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, )`.
+        ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, N_{class})` or :math:`(N_{samples}, )`.
+
+    Examples:
+        >>> import torch
+        >>> import easygraph.ml_metrics as dm
+        >>> y_true = torch.tensor([3, 2, 4])
+        >>> y_pred = torch.tensor([
+                [0.2, 0.3, 0.5, 0.4, 0.3],
+                [0.8, 0.2, 0.3, 0.5, 0.4],
+                [0.2, 0.4, 0.5, 0.2, 0.8],
+            ])
+        >>> dm.classification.accuracy(y_true, y_pred)
+        0.3333333432674408
+    """
+    y_true, y_pred = _format_inputs(y_true, y_pred)
+    return (y_true == y_pred).float().mean().item()
+
+
+def f1_score(y_true: torch.LongTensor, y_pred: torch.Tensor, average: str = "macro"):
+    r"""Calculate the F1 score for the classification task.
+
+    Args:
+        ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, )`.
+        ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, N_{class})` or :math:`(N_{samples}, )`.
+        ``average`` (``str``): The average method. Must be one of "macro", "micro", "weighted".
+
+    Examples:
+        >>> import torch
+        >>> import easygraph.ml_metrics as dm
+        >>> y_true = torch.tensor([3, 2, 4, 0])
+        >>> y_pred = torch.tensor([
+                [0.2, 0.3, 0.5, 0.4, 0.3],
+                [0.8, 0.2, 0.3, 0.5, 0.4],
+                [0.2, 0.4, 0.5, 0.2, 0.8],
+                [0.8, 0.4, 0.5, 0.2, 0.8]
+            ])
+        >>> dm.classification.f1_score(y_true, y_pred, "macro")
+        0.41666666666666663
+        >>> dm.classification.f1_score(y_true, y_pred, "micro")
+        0.5
+        >>> dm.classification.f1_score(y_true, y_pred, "weighted")
+        0.41666666666666663
+    """
+    y_true, y_pred = _format_inputs(y_true, y_pred)
+    return sm.f1_score(y_true, y_pred, average=average)
+
+
+def confusion_matrix(y_true: torch.LongTensor, y_pred: torch.Tensor):
+    r"""Calculate the confusion matrix for the classification task.
+
+    Args:
+        ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, )`.
+        ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, N_{class})` or :math:`(N_{samples}, )`.
+
+    Examples:
+        >>> import torch
+        >>> import easygraph.ml_metrics as dm
+        >>> y_true = torch.tensor([3, 2, 4, 0])
+        >>> y_pred = torch.tensor([
+                [0.2, 0.3, 0.5, 0.4, 0.3],
+                [0.8, 0.2, 0.3, 0.5, 0.4],
+                [0.2, 0.4, 0.5, 0.2, 0.8],
+                [0.8, 0.4, 0.5, 0.2, 0.8]
+            ])
+        >>> dm.classification.confusion_matrix(y_true, y_pred)
+        array([[1, 0, 0, 0],
+               [1, 0, 0, 0],
+               [0, 1, 0, 0],
+               [0, 0, 0, 1]])
+    """
+    y_true, y_pred = _format_inputs(y_true, y_pred)
+    return sm.confusion_matrix(y_true, y_pred)
+
+
+# Vertex Classification Evaluator
+class VertexClassificationEvaluator(BaseEvaluator):
+    r"""Return the metric evaluator for vertex classification task. The supported ml_metrics includes: ``accuracy``, ``f1_score``, ``confusion_matrix``.
+
+    Args:
+        ``metric_configs`` (``List[Union[str, Dict[str, dict]]]``): The metric configurations. The key is the metric name and the value is the metric parameters.
+        ``validate_index`` (``int``): The specified metric index used for validation. Defaults to ``0``.
+    """
+
+    def __init__(
+        self,
+        metric_configs: List[Union[str, Dict[str, dict]]],
+        validate_index: int = 0,
+    ):
+        super().__init__("classification", metric_configs, validate_index)
+
+    def validate(self, y_true: torch.LongTensor, y_pred: torch.Tensor):
+        r"""Return the result of the evaluation on the specified ``validate_index``-th metric.
+
+        Args:
+            ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, )`.
+            ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, N_{class})` or :math:`(N_{samples}, )`.
+        """
+        return super().validate(y_true, y_pred)
+
+    def test(self, y_true: torch.LongTensor, y_pred: torch.Tensor):
+        r"""Return results of the evaluation on all the ml_metrics in ``metric_configs``.
+
+        Args:
+            ``y_true`` (``torch.LongTensor``): The ground truth labels. Size :math:`(N_{samples}, )`.
+            ``y_pred`` (``torch.Tensor``): The predicted labels. Size :math:`(N_{samples}, N_{class})` or :math:`(N_{samples}, )`.
+        """
+        return super().test(y_true, y_pred)
```

## easygraph/ml_metrics/__init__.py

 * *Ordering differences only*

```diff
@@ -1,45 +1,45 @@
-try:
-    from typing import Dict
-    from typing import List
-    from typing import Union
-
-    from easygraph._global import AUTHOR_EMAIL
-
-    from .base import BaseEvaluator
-    from .classification import VertexClassificationEvaluator
-    from .classification import available_classification_metrics
-    from .hypergraphs import HypergraphVertexClassificationEvaluator
-except:
-    print(
-        "Warning raise in module:ml_metrics. Please install Pytorch before you use"
-        " functions related to nueral network"
-    )
-
-
-def build_evaluator(
-    task: str,
-    metric_configs: List[Union[str, Dict[str, dict]]],
-    validate_index: int = 0,
-):
-    r"""Return the metric evaluator for the given task.
-
-    Args:
-        ``task`` (``str``): The type of the task. The supported types include: ``graph_vertex_classification``, ``hypergraph_vertex_classification``, and ``user_item_recommender``.
-        ``metric_configs`` (``List[Union[str, Dict[str, dict]]]``): The list of metric names.
-        ``validate_index`` (``int``): The specified metric index used for validation. Defaults to ``0``.
-    """
-    if task == "hypergraph_vertex_classification":
-        return HypergraphVertexClassificationEvaluator(metric_configs, validate_index)
-    else:
-        raise ValueError(
-            f"{task} is not supported yet. Please email '{AUTHOR_EMAIL}' to add it."
-        )
-
-
-# __all__ = [
-#     "BaseEvaluator",
-#     "build_evaluator",
-#     "available_classification_metrics",
-#     "VertexClassificationEvaluator",
-#     "HypergraphVertexClassificationEvaluator",
-# ]
+try:
+    from typing import Dict
+    from typing import List
+    from typing import Union
+
+    from easygraph._global import AUTHOR_EMAIL
+
+    from .base import BaseEvaluator
+    from .classification import VertexClassificationEvaluator
+    from .classification import available_classification_metrics
+    from .hypergraphs import HypergraphVertexClassificationEvaluator
+except:
+    print(
+        "Warning raise in module:ml_metrics. Please install Pytorch before you use"
+        " functions related to nueral network"
+    )
+
+
+def build_evaluator(
+    task: str,
+    metric_configs: List[Union[str, Dict[str, dict]]],
+    validate_index: int = 0,
+):
+    r"""Return the metric evaluator for the given task.
+
+    Args:
+        ``task`` (``str``): The type of the task. The supported types include: ``graph_vertex_classification``, ``hypergraph_vertex_classification``, and ``user_item_recommender``.
+        ``metric_configs`` (``List[Union[str, Dict[str, dict]]]``): The list of metric names.
+        ``validate_index`` (``int``): The specified metric index used for validation. Defaults to ``0``.
+    """
+    if task == "hypergraph_vertex_classification":
+        return HypergraphVertexClassificationEvaluator(metric_configs, validate_index)
+    else:
+        raise ValueError(
+            f"{task} is not supported yet. Please email '{AUTHOR_EMAIL}' to add it."
+        )
+
+
+# __all__ = [
+#     "BaseEvaluator",
+#     "build_evaluator",
+#     "available_classification_metrics",
+#     "VertexClassificationEvaluator",
+#     "HypergraphVertexClassificationEvaluator",
+# ]
```

## easygraph/ml_metrics/hypergraphs/hypergraph.py

 * *Ordering differences only*

```diff
@@ -1,47 +1,47 @@
-from typing import Dict
-from typing import List
-from typing import Union
-
-import torch
-
-from ..classification import VertexClassificationEvaluator
-
-
-class HypergraphVertexClassificationEvaluator(VertexClassificationEvaluator):
-    r"""Return the metric evaluator for vertex classification task on the hypergraph structure. The supported ml_metrics includes: ``accuracy``, ``f1_score``, ``confusion_matrix``.
-
-    Args:
-        ``metric_configs`` (``List[Union[str, Dict[str, dict]]]``): The metric configurations. The key is the metric name and the value is the metric parameters.
-        ``validate_index`` (``int``): The specified metric index used for validation. Defaults to ``0``.
-
-    Examples:
-        >>> import torch
-        >>> import easygraph.ml_metrics as dm
-        >>> evaluator = dm.HypergraphVertexClassificationEvaluator(
-                [
-                    "accuracy",
-                    {"f1_score": {"average": "macro"}},
-                ],
-                0
-            )
-        >>> y_true = torch.tensor([0, 0, 1, 1, 2, 2])
-        >>> y_pred = torch.tensor([0, 2, 1, 2, 1, 2])
-        >>> evaluator.validate(y_true, y_pred)
-        0.5
-        >>> evaluator.test(y_true, y_pred)
-        {
-            'accuracy': 0.5,
-            'f1_score -> average@macro': 0.5222222222222221
-        }
-    """
-
-    def __init__(
-        self, metric_configs: List[Union[str, Dict[str, dict]]], validate_index: int = 0
-    ):
-        super().__init__(metric_configs, validate_index)
-
-    def validate(self, y_true: torch.LongTensor, y_pred: torch.Tensor):
-        return super().validate(y_true, y_pred)
-
-    def test(self, y_true: torch.LongTensor, y_pred: torch.Tensor):
-        return super().test(y_true, y_pred)
+from typing import Dict
+from typing import List
+from typing import Union
+
+import torch
+
+from ..classification import VertexClassificationEvaluator
+
+
+class HypergraphVertexClassificationEvaluator(VertexClassificationEvaluator):
+    r"""Return the metric evaluator for vertex classification task on the hypergraph structure. The supported ml_metrics includes: ``accuracy``, ``f1_score``, ``confusion_matrix``.
+
+    Args:
+        ``metric_configs`` (``List[Union[str, Dict[str, dict]]]``): The metric configurations. The key is the metric name and the value is the metric parameters.
+        ``validate_index`` (``int``): The specified metric index used for validation. Defaults to ``0``.
+
+    Examples:
+        >>> import torch
+        >>> import easygraph.ml_metrics as dm
+        >>> evaluator = dm.HypergraphVertexClassificationEvaluator(
+                [
+                    "accuracy",
+                    {"f1_score": {"average": "macro"}},
+                ],
+                0
+            )
+        >>> y_true = torch.tensor([0, 0, 1, 1, 2, 2])
+        >>> y_pred = torch.tensor([0, 2, 1, 2, 1, 2])
+        >>> evaluator.validate(y_true, y_pred)
+        0.5
+        >>> evaluator.test(y_true, y_pred)
+        {
+            'accuracy': 0.5,
+            'f1_score -> average@macro': 0.5222222222222221
+        }
+    """
+
+    def __init__(
+        self, metric_configs: List[Union[str, Dict[str, dict]]], validate_index: int = 0
+    ):
+        super().__init__(metric_configs, validate_index)
+
+    def validate(self, y_true: torch.LongTensor, y_pred: torch.Tensor):
+        return super().validate(y_true, y_pred)
+
+    def test(self, y_true: torch.LongTensor, y_pred: torch.Tensor):
+        return super().test(y_true, y_pred)
```

## easygraph/ml_metrics/hypergraphs/__init__.py

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-from .hypergraph import HypergraphVertexClassificationEvaluator
+from .hypergraph import HypergraphVertexClassificationEvaluator
```

## easygraph/model/__init__.py

 * *Ordering differences only*

```diff
@@ -1,16 +1,16 @@
-try:
-    from .hypergraphs import DHCF
-    from .hypergraphs import HGNN
-    from .hypergraphs import HGNNP
-    from .hypergraphs import HNHN
-    from .hypergraphs import HyperGCN
-    from .hypergraphs import SetGNN
-    from .hypergraphs import UniGAT
-    from .hypergraphs import UniGCN
-    from .hypergraphs import UniGIN
-    from .hypergraphs import UniSAGE
-except:
-    print(
-        "Warning raise in module:model.Please install Pytorch before you use functions"
-        " related to Hypergraph"
-    )
+try:
+    from .hypergraphs import DHCF
+    from .hypergraphs import HGNN
+    from .hypergraphs import HGNNP
+    from .hypergraphs import HNHN
+    from .hypergraphs import HyperGCN
+    from .hypergraphs import SetGNN
+    from .hypergraphs import UniGAT
+    from .hypergraphs import UniGCN
+    from .hypergraphs import UniGIN
+    from .hypergraphs import UniSAGE
+except:
+    print(
+        "Warning raise in module:model.Please install Pytorch before you use functions"
+        " related to Hypergraph"
+    )
```

## easygraph/model/hypergraphs/setgnn.py

 * *Ordering differences only*

```diff
@@ -1,888 +1,888 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-from easygraph.nn.convs.common import MLP
-from easygraph.nn.convs.hypergraphs.halfnlh_conv import HalfNLHconv
-from torch.nn import Linear
-
-
-__all__ = ["SetGNN"]
-
-
-class SetGNN(nn.Module):
-    r"""The SetGNN model proposed in `YOU ARE ALLSET: A MULTISET LEARNING FRAMEWORK FOR HYPERGRAPH NEURAL NETWORKS <https://openreview.net/pdf?id=hpBTIv2uy_E>`_ paper (ICLR 2022).
-
-    Parameters:
-        ``num_features`` (``int``): : The dimension of node features.
-        ``num_classes`` (``int``): The Number of class of the classification task.
-        ``Classifier_hidden`` (``int``): Decoder hidden units.
-        ``Classifier_num_layers`` (``int``): Layers of decoder.
-        ``MLP_hidden`` (``int``): Encoder hidden units.
-        ``MLP_num_layers`` (``int``): Layers of encoder.
-         ``dropout`` (``float``, optional): Dropout ratio. Defaults to 0.5.
-        ``aggregate`` (``str``): The aggregation method. Defaults to ``add``
-        ``normalization`` (``str``): The normalization method. Defaults to ``ln``
-        ``deepset_input_norm`` (``bool``):  Defaults to True.
-        ``heads`` (``int``):  Defaults to 1
-        `PMA`` (``bool``):  Defaults to True
-        `GPR`` (``bool``):  Defaults to False
-        `LearnMask`` (``bool``):  Defaults to False
-        `norm`` (``Tensor``):  The weight for edges in bipartite graphs, correspond to data.edge_index
-
-    """
-
-    def __init__(
-        self,
-        num_features,
-        num_classes,
-        Classifier_hidden=64,
-        Classifier_num_layers=2,
-        MLP_hidden=64,
-        MLP_num_layers=2,
-        All_num_layers=2,
-        dropout=0.5,
-        aggregate="mean",
-        normalization="ln",
-        deepset_input_norm=True,
-        heads=1,
-        PMA=True,
-        GPR=False,
-        LearnMask=False,
-        norm=None,
-    ):
-        super(SetGNN, self).__init__()
-        """
-        args should contain the following:
-        V_in_dim, V_enc_hid_dim, V_dec_hid_dim, V_out_dim, V_enc_num_layers, V_dec_num_layers
-        E_in_dim, E_enc_hid_dim, E_dec_hid_dim, E_out_dim, E_enc_num_layers, E_dec_num_layers
-        All_num_layers,dropout
-        !!! V_in_dim should be the dimension of node features
-        !!! E_out_dim should be the number of classes (for classification)
-        """
-
-        #         Now set all dropout the same, but can be different
-        self.All_num_layers = All_num_layers
-        self.dropout = dropout
-        self.aggr = aggregate
-        self.NormLayer = normalization
-        self.InputNorm = deepset_input_norm
-        self.GPR = GPR
-        self.LearnMask = LearnMask
-        #         Now define V2EConvs[i], V2EConvs[i] for ith layers
-        #         Currently we assume there's no hyperedge features, which means V_out_dim = E_in_dim
-        #         If there's hyperedge features, concat with Vpart decoder output features [V_feat||E_feat]
-        self.V2EConvs = nn.ModuleList()
-        self.E2VConvs = nn.ModuleList()
-        self.bnV2Es = nn.ModuleList()
-        self.bnE2Vs = nn.ModuleList()
-
-        if self.LearnMask:
-            self.Importance = nn.Parameter(torch.ones(norm.size()))
-
-        if self.All_num_layers == 0:
-            self.classifier = MLP(
-                in_channels=num_features,
-                hidden_channels=Classifier_hidden,
-                out_channels=num_classes,
-                num_layers=Classifier_num_layers,
-                dropout=self.dropout,
-                normalization=self.NormLayer,
-                InputNorm=False,
-            )
-        else:
-            self.V2EConvs.append(
-                HalfNLHconv(
-                    in_dim=num_features,
-                    hid_dim=MLP_hidden,
-                    out_dim=MLP_hidden,
-                    num_layers=MLP_num_layers,
-                    dropout=self.dropout,
-                    normalization=self.NormLayer,
-                    InputNorm=self.InputNorm,
-                    heads=heads,
-                    attention=PMA,
-                )
-            )
-            self.bnV2Es.append(nn.BatchNorm1d(MLP_hidden))
-            self.E2VConvs.append(
-                HalfNLHconv(
-                    in_dim=MLP_hidden,
-                    hid_dim=MLP_hidden,
-                    out_dim=MLP_hidden,
-                    num_layers=MLP_num_layers,
-                    dropout=self.dropout,
-                    normalization=self.NormLayer,
-                    InputNorm=self.InputNorm,
-                    heads=heads,
-                    attention=PMA,
-                )
-            )
-            self.bnE2Vs.append(nn.BatchNorm1d(MLP_hidden))
-            for _ in range(self.All_num_layers - 1):
-                self.V2EConvs.append(
-                    HalfNLHconv(
-                        in_dim=MLP_hidden,
-                        hid_dim=MLP_hidden,
-                        out_dim=MLP_hidden,
-                        num_layers=MLP_num_layers,
-                        dropout=self.dropout,
-                        normalization=self.NormLayer,
-                        InputNorm=self.InputNorm,
-                        heads=heads,
-                        attention=PMA,
-                    )
-                )
-                self.bnV2Es.append(nn.BatchNorm1d(MLP_hidden))
-                self.E2VConvs.append(
-                    HalfNLHconv(
-                        in_dim=MLP_hidden,
-                        hid_dim=MLP_hidden,
-                        out_dim=MLP_hidden,
-                        num_layers=MLP_num_layers,
-                        dropout=self.dropout,
-                        normalization=self.NormLayer,
-                        InputNorm=self.InputNorm,
-                        heads=heads,
-                        attention=PMA,
-                    )
-                )
-                self.bnE2Vs.append(nn.BatchNorm1d(MLP_hidden))
-
-            if self.GPR:
-                self.MLP = MLP(
-                    in_channels=num_features,
-                    hidden_channels=MLP_hidden,
-                    out_channels=MLP_hidden,
-                    num_layers=MLP_num_layers,
-                    dropout=self.dropout,
-                    normalization=self.NormLayer,
-                    InputNorm=False,
-                )
-                self.GPRweights = Linear(self.All_num_layers + 1, 1, bias=False)
-                self.classifier = MLP(
-                    in_channels=MLP_hidden,
-                    hidden_channels=Classifier_hidden,
-                    out_channels=num_classes,
-                    num_layers=Classifier_num_layers,
-                    dropout=self.dropout,
-                    normalization=self.NormLayer,
-                    InputNorm=False,
-                )
-            else:
-                self.classifier = MLP(
-                    in_channels=MLP_hidden,
-                    hidden_channels=Classifier_hidden,
-                    out_channels=num_classes,
-                    num_layers=Classifier_num_layers,
-                    dropout=self.dropout,
-                    normalization=self.NormLayer,
-                    InputNorm=False,
-                )
-
-    def reset_parameters(self):
-        for layer in self.V2EConvs:
-            layer.reset_parameters()
-        for layer in self.E2VConvs:
-            layer.reset_parameters()
-        for layer in self.bnV2Es:
-            layer.reset_parameters()
-        for layer in self.bnE2Vs:
-            layer.reset_parameters()
-        self.classifier.reset_parameters()
-        if self.GPR:
-            self.MLP.reset_parameters()
-            self.GPRweights.reset_parameters()
-        if self.LearnMask:
-            nn.init.ones_(self.Importance)
-
-    def forward(self, data):
-        """
-        The data should contain the follows
-        data.x: node features
-        data.edge_index: edge list (of size (2,|E|)) where data.edge_index[0] contains nodes and data.edge_index[1] contains hyperedges
-        !!! Note that self loop should be assigned to a new (hyper)edge id!!!
-        !!! Also note that the (hyper)edge id should start at 0 (akin to node id)
-        data.norm: The weight for edges in bipartite graphs, correspond to data.edge_index
-        !!! Note that we output final node representation. Loss should be defined outside.
-        """
-
-        x, edge_index, norm = data.x, data.edge_index, data.norm
-        if self.LearnMask:
-            norm = self.Importance * norm
-        cidx = edge_index[1].min()
-        edge_index[1] -= cidx  # make sure we do not waste memory
-        reversed_edge_index = torch.stack([edge_index[1], edge_index[0]], dim=0)
-        if self.GPR:
-            xs = []
-            xs.append(F.relu(self.MLP(x)))
-            for i, _ in enumerate(self.V2EConvs):
-                x = F.relu(self.V2EConvs[i](x, edge_index, norm, self.aggr))
-                #                 x = self.bnV2Es[i](x)
-                x = F.dropout(x, p=self.dropout, training=self.training)
-                x = self.E2VConvs[i](x, reversed_edge_index, norm, self.aggr)
-                x = F.relu(x)
-                xs.append(x)
-                #                 x = self.bnE2Vs[i](x)
-                x = F.dropout(x, p=self.dropout, training=self.training)
-            x = torch.stack(xs, dim=-1)
-            x = self.GPRweights(x).squeeze()
-            x = self.classifier(x)
-        else:
-            x = F.dropout(x, p=0.2, training=self.training)  # Input dropout
-            for i, _ in enumerate(self.V2EConvs):
-                x = F.relu(self.V2EConvs[i](x, edge_index, norm, self.aggr))
-                #                 x = self.bnV2Es[i](x)
-                x = F.dropout(x, p=self.dropout, training=self.training)
-                x = F.relu(self.E2VConvs[i](x, reversed_edge_index, norm, self.aggr))
-                #                 x = self.bnE2Vs[i](x)
-                x = F.dropout(x, p=self.dropout, training=self.training)
-            x = self.classifier(x)
-
-        return x
-
-
-# import os
-# import time
-# import torch
-# import argparse
-#
-# import numpy as np
-# import os.path as osp
-
-# import matplotlib.pyplot as plt
-#
-# from tqdm import tqdm
-#
-#
-# from easygraph.model.data.convert_datasets_to_pygDataset import dataset_Hypergraph
-# from torch_scatter import scatter_add
-#
-# def parse_method(args, data):
-#     #     Currently we don't set hyperparameters w.r.t. different dataset
-#     if args.method == 'AllSetTransformer':
-#         if args.LearnMask:
-#             model = SetGNN(args, data.norm)
-#         else:
-#             model = SetGNN(num_classes=args.num_classes,num_features=args.num_features)
-#
-#     elif args.method == 'AllDeepSets':
-#         args.PMA = False
-#         args.aggregate = 'add'
-#         if args.LearnMask:
-#             model = SetGNN(args, data.norm)
-#         else:
-#             model = SetGNN(args)
-#
-#     #     elif args.method == 'SetGPRGNN':
-#     #         model = SetGPRGNN(args)
-#
-#
-#     return model
-#
-#
-# class Logger(object):
-#     """ Adapted from https://github.com/snap-stanford/ogb/ """
-#
-#     def __init__(self, runs, info=None):
-#         self.info = info
-#         self.results = [[] for _ in range(runs)]
-#
-#     def add_result(self, run, result):
-#         assert len(result) == 3
-#         assert run >= 0 and run < len(self.results)
-#         self.results[run].append(result)
-#
-#     def print_statistics(self, run=None):
-#         if run is not None:
-#             result = 100 * torch.tensor(self.results[run])
-#             argmax = result[:, 1].argmax().item()
-#             print(f'Run {run + 1:02d}:')
-#             print(f'Highest Train: {result[:, 0].max():.2f}')
-#             print(f'Highest Valid: {result[:, 1].max():.2f}')
-#             print(f'  Final Train: {result[argmax, 0]:.2f}')
-#             print(f'   Final Test: {result[argmax, 2]:.2f}')
-#         else:
-#             result = 100 * torch.tensor(self.results)
-#
-#             best_results = []
-#             for r in result:
-#                 train1 = r[:, 0].max().item()
-#                 valid = r[:, 1].max().item()
-#                 train2 = r[r[:, 1].argmax(), 0].item()
-#                 test = r[r[:, 1].argmax(), 2].item()
-#                 best_results.append((train1, valid, train2, test))
-#
-#             best_result = torch.tensor(best_results)
-#
-#             print(f'All runs:')
-#             r = best_result[:, 0]
-#             print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')
-#             r = best_result[:, 1]
-#             print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')
-#             r = best_result[:, 2]
-#             print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')
-#             r = best_result[:, 3]
-#             print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')
-#
-#             return best_result[:, 1], best_result[:, 3]
-#
-#     def plot_result(self, run=None):
-#         plt.style.use('seaborn')
-#         if run is not None:
-#             result = 100 * torch.tensor(self.results[run])
-#             x = torch.arange(result.shape[0])
-#             plt.figure()
-#             print(f'Run {run + 1:02d}:')
-#             plt.plot(x, result[:, 0], x, result[:, 1], x, result[:, 2])
-#             plt.legend(['Train', 'Valid', 'Test'])
-#         else:
-#             result = 100 * torch.tensor(self.results[0])
-#             x = torch.arange(result.shape[0])
-#             plt.figure()
-#             #             print(f'Run {run + 1:02d}:')
-#             plt.plot(x, result[:, 0], x, result[:, 1], x, result[:, 2])
-#             plt.legend(['Train', 'Valid', 'Test'])
-#
-#
-# @torch.no_grad()
-# def evaluate(model, data, split_idx, eval_func, result=None):
-#     if result is not None:
-#         out = result
-#     else:
-#         model.eval()
-#         out = model(data)
-#         out = F.log_softmax(out, dim=1)
-#
-#     train_acc = eval_func(
-#         data.y[split_idx['train']], out[split_idx['train']])
-#     valid_acc = eval_func(
-#         data.y[split_idx['valid']], out[split_idx['valid']])
-#     test_acc = eval_func(
-#         data.y[split_idx['test']], out[split_idx['test']])
-#
-#     #     Also keep track of losses
-#     train_loss = F.nll_loss(
-#         out[split_idx['train']], data.y[split_idx['train']])
-#     valid_loss = F.nll_loss(
-#         out[split_idx['valid']], data.y[split_idx['valid']])
-#     test_loss = F.nll_loss(
-#         out[split_idx['test']], data.y[split_idx['test']])
-#     return train_acc, valid_acc, test_acc, train_loss, valid_loss, test_loss, out
-#
-#
-# def eval_acc(y_true, y_pred):
-#     acc_list = []
-#     y_true = y_true.detach().cpu().numpy()
-#     y_pred = y_pred.argmax(dim=-1, keepdim=False).detach().cpu().numpy()
-#
-#     #     ipdb.set_trace()
-#     #     for i in range(y_true.shape[1]):
-#     is_labeled = y_true == y_true
-#     correct = y_true[is_labeled] == y_pred[is_labeled]
-#     acc_list.append(float(np.sum(correct)) / len(correct))
-#
-#     return sum(acc_list) / len(acc_list)
-#
-#
-# def count_parameters(model):
-#     return sum(p.numel() for p in model.parameters() if p.requires_grad)
-#
-#
-# # --- Main part of the training ---
-# # # Part 0: Parse arguments
-#
-#
-# """
-#
-# """
-# def ExtractV2E(data):
-#     # Assume edge_index = [V|E;E|V]
-#     edge_index = data.edge_index
-# #     First, ensure the sorting is correct (increasing along edge_index[0])
-#     _, sorted_idx = torch.sort(edge_index[0])
-#     edge_index = edge_index[:, sorted_idx].type(torch.LongTensor)
-#     print("data.n_x:",data.n_x)
-#     # num_nodes = data.n_x[0]
-#     num_nodes = data.n_x
-#     num_hyperedges = data.num_hyperedges
-#     if not ((data.n_x+data.num_hyperedges-1) == data.edge_index[0].max().item()):
-#         print('num_hyperedges does not match! 1')
-#         return
-#     cidx = torch.where(edge_index[0] == num_nodes)[
-#         0].min()  # cidx: [V...|cidx E...]
-#     data.edge_index = edge_index[:, :cidx].type(torch.LongTensor)
-#     return data
-#
-# def Add_Self_Loops(data):
-#     # update so we dont jump on some indices
-#     # Assume edge_index = [V;E]. If not, use ExtractV2E()
-#     edge_index = data.edge_index
-#     num_nodes = data.n_x
-#     num_hyperedges = data.num_hyperedges
-#
-#     if not ((data.n_x + data.num_hyperedges - 1) == data.edge_index[1].max().item()):
-#         print('num_hyperedges does not match! 2')
-#         return
-#
-#     hyperedge_appear_fre = Counter(edge_index[1].numpy())
-#     # store the nodes that already have self-loops
-#     skip_node_lst = []
-#     for edge in hyperedge_appear_fre:
-#         if hyperedge_appear_fre[edge] == 1:
-#             skip_node = edge_index[0][torch.where(
-#                 edge_index[1] == edge)[0].item()]
-#             skip_node_lst.append(skip_node.item())
-#
-#     new_edge_idx = edge_index[1].max() + 1
-#     new_edges = torch.zeros(
-#         (2, num_nodes - len(skip_node_lst)), dtype=edge_index.dtype)
-#     tmp_count = 0
-#     for i in range(num_nodes):
-#         if i not in skip_node_lst:
-#             new_edges[0][tmp_count] = i
-#             new_edges[1][tmp_count] = new_edge_idx
-#             new_edge_idx += 1
-#             tmp_count += 1
-#
-#     data.totedges = num_hyperedges + num_nodes - len(skip_node_lst)
-#     edge_index = torch.cat((edge_index, new_edges), dim=1)
-#     # Sort along w.r.t. nodes
-#     _, sorted_idx = torch.sort(edge_index[0])
-#     data.edge_index = edge_index[:, sorted_idx].type(torch.LongTensor)
-#     return data
-#
-# def expand_edge_index(data, edge_th=0):
-#     '''
-#     args:
-#         num_nodes: regular nodes. i.e. x.shape[0]
-#         num_edges: number of hyperedges. not the star expansion edges.
-#
-#     this function will expand each n2he relations, [[n_1, n_2, n_3],
-#                                                     [e_7, e_7, e_7]]
-#     to :
-#         [[n_1,   n_1,   n_2,   n_2,   n_3,   n_3],
-#          [e_7_2, e_7_3, e_7_1, e_7_3, e_7_1, e_7_2]]
-#
-#     and each he2n relations:   [[e_7, e_7, e_7],
-#                                 [n_1, n_2, n_3]]
-#     to :
-#         [[e_7_1, e_7_2, e_7_3],
-#          [n_1,   n_2,   n_3]]
-#
-#     and repeated for every hyperedge.
-#     '''
-#     edge_index = data.edge_index
-#     num_nodes = data.n_x[0].item()
-#     if hasattr(data, 'totedges'):
-#         num_edges = data.totedges
-#     else:
-#         num_edges = data.num_hyperedges[0]
-#
-#     expanded_n2he_index = []
-# #     n2he_with_same_heid = []
-#
-# #     expanded_he2n_index = []
-# #     he2n_with_same_heid = []
-#
-#     # start edge_id from the largest node_id + 1.
-#     cur_he_id = num_nodes
-#     # keep an mapping of new_edge_id to original edge_id for edge_size query.
-#     new_edge_id_2_original_edge_id = {}
-#
-#     # do the expansion for all annotated he_id in the original edge_index
-# #     ipdb.set_trace()
-#     for he_idx in range(num_nodes, num_edges + num_nodes):
-#         # find all nodes within the same hyperedge.
-#         selected_he = edge_index[:, edge_index[1] == he_idx]
-#         size_of_he = selected_he.shape[1]
-#
-# #         Trim a hyperedge if its size>edge_th
-#         if edge_th > 0:
-#             if size_of_he > edge_th:
-#                 continue
-#
-#         if size_of_he == 1:
-#             # there is only one node in this hyperedge -> self-loop node. add to graph.
-#             #             n2he_with_same_heid.append(selected_he)
-#
-#             new_n2he = selected_he.clone()
-#             new_n2he[1] = cur_he_id
-#             expanded_n2he_index.append(new_n2he)
-#
-#             # ====
-# #             new_he2n_same_heid = torch.flip(selected_he, dims = [0])
-# #             he2n_with_same_heid.append(new_he2n_same_heid)
-#
-# #             new_he2n = torch.flip(selected_he, dims = [0])
-# #             new_he2n[0] = cur_he_id
-# #             expanded_he2n_index.append(new_he2n)
-#
-#             cur_he_id += 1
-#             continue
-#
-#         # -------------------------------
-# #         # new_n2he_same_heid uses same he id for all nodes.
-# #         new_n2he_same_heid = selected_he.repeat_interleave(size_of_he - 1, dim = 1)
-# #         n2he_with_same_heid.append(new_n2he_same_heid)
-#
-#         # for new_n2he mapping. connect the nodes to all repeated he first.
-#         # then remove those connection that corresponding to the node itself.
-#         new_n2he = selected_he.repeat_interleave(size_of_he, dim=1)
-#
-#         # new_edge_ids start from the he_id from previous iteration (cur_he_id).
-#         new_edge_ids = torch.LongTensor(
-#             np.arange(cur_he_id, cur_he_id + size_of_he)).repeat(size_of_he)
-#         new_n2he[1] = new_edge_ids
-#
-#         # build a mapping between node and it's corresponding edge.
-#         # e.g. {n_1: e_7_1, n_2: e_7_2}
-#         tmp_node_id_2_he_id_dict = {}
-#         for idx in range(size_of_he):
-#             new_edge_id_2_original_edge_id[cur_he_id] = he_idx
-#             cur_node_id = selected_he[0][idx].item()
-#             tmp_node_id_2_he_id_dict[cur_node_id] = cur_he_id
-#             cur_he_id += 1
-#
-#         # create n2he by deleting the self-product edge.
-#         new_he_select_mask = torch.BoolTensor([True] * new_n2he.shape[1])
-#         for col_idx in range(new_n2he.shape[1]):
-#             tmp_node_id, tmp_edge_id = new_n2he[0, col_idx].item(
-#             ), new_n2he[1, col_idx].item()
-#             if tmp_node_id_2_he_id_dict[tmp_node_id] == tmp_edge_id:
-#                 new_he_select_mask[col_idx] = False
-#         new_n2he = new_n2he[:, new_he_select_mask]
-#         expanded_n2he_index.append(new_n2he)
-#
-#
-# #         # ---------------------------
-# #         # create he2n from mapping.
-# #         new_he2n = np.array([[he_id, node_id] for node_id, he_id in tmp_node_id_2_he_id_dict.items()])
-# #         new_he2n = torch.from_numpy(new_he2n.T).to(device = edge_index.device)
-# #         expanded_he2n_index.append(new_he2n)
-#
-# #         # create he2n with same heid as input edge_index.
-# #         new_he2n_same_heid = torch.zeros_like(new_he2n, device = edge_index.device)
-# #         new_he2n_same_heid[1] = new_he2n[1]
-# #         new_he2n_same_heid[0] = torch.ones_like(new_he2n[0]) * he_idx
-# #         he2n_with_same_heid.append(new_he2n_same_heid)
-#
-#     new_edge_index = torch.cat(expanded_n2he_index, dim=1)
-# #     new_he2n_index = torch.cat(expanded_he2n_index, dim = 1)
-# #     new_edge_index = torch.cat([new_n2he_index, new_he2n_index], dim = 1)
-#     # sort the new_edge_index by first row. (node_ids)
-#     new_order = new_edge_index[0].argsort()
-#     data.edge_index = new_edge_index[:, new_order]
-#
-#     return data
-#
-# def rand_train_test_idx(label, train_prop=.5, valid_prop=.25, ignore_negative=True, balance=False):
-#     """ Adapted from https://github.com/CUAI/Non-Homophily-Benchmarks"""
-#     """ randomly splits label into train/valid/test splits """
-#     if not balance:
-#         if ignore_negative:
-#             labeled_nodes = torch.where(label != -1)[0]
-#         else:
-#             labeled_nodes = label
-#
-#         n = labeled_nodes.shape[0]
-#         train_num = int(n * train_prop)
-#         valid_num = int(n * valid_prop)
-#
-#         perm = torch.as_tensor(np.random.permutation(n))
-#
-#         train_indices = perm[:train_num]
-#         val_indices = perm[train_num:train_num + valid_num]
-#         test_indices = perm[train_num + valid_num:]
-#
-#         if not ignore_negative:
-#             return train_indices, val_indices, test_indices
-#
-#         train_idx = labeled_nodes[train_indices]
-#         valid_idx = labeled_nodes[val_indices]
-#         test_idx = labeled_nodes[test_indices]
-#
-#         split_idx = {'train': train_idx,
-#                      'valid': valid_idx,
-#                      'test': test_idx}
-#     else:
-#         #         ipdb.set_trace()
-#         indices = []
-#         for i in range(label.max()+1):
-#             index = torch.where((label == i))[0].view(-1)
-#             index = index[torch.randperm(index.size(0))]
-#             indices.append(index)
-#
-#         percls_trn = int(train_prop/(label.max()+1)*len(label))
-#         val_lb = int(valid_prop*len(label))
-#         train_idx = torch.cat([i[:percls_trn] for i in indices], dim=0)
-#         rest_index = torch.cat([i[percls_trn:] for i in indices], dim=0)
-#         rest_index = rest_index[torch.randperm(rest_index.size(0))]
-#         valid_idx = rest_index[:val_lb]
-#         test_idx = rest_index[val_lb:]
-#         split_idx = {'train': train_idx,
-#                      'valid': valid_idx,
-#                      'test': test_idx}
-#     return split_idx
-#
-# def norm_contruction(data, option='all_one', TYPE='V2E'):
-#     from torch_geometric.nn.conv.gcn_conv import gcn_norm
-#     if TYPE == 'V2E':
-#         if option == 'all_one':
-#             data.norm = torch.ones_like(data.edge_index[0])
-#
-#         elif option == 'deg_half_sym':
-#             edge_weight = torch.ones_like(data.edge_index[0])
-#             cidx = data.edge_index[1].min()
-#             Vdeg = scatter_add(edge_weight, data.edge_index[0], dim=0)
-#             HEdeg = scatter_add(edge_weight, data.edge_index[1]-cidx, dim=0)
-#             V_norm = Vdeg**(-1/2)
-#             E_norm = HEdeg**(-1/2)
-#             data.norm = V_norm[data.edge_index[0]] * \
-#                 E_norm[data.edge_index[1]-cidx]
-#
-#     elif TYPE == 'V2V':
-#         data.edge_index, data.norm = gcn_norm(
-#             data.edge_index, data.norm, add_self_loops=True)
-#     return data
-#
-# if __name__ == '__main__':
-#     parser = argparse.ArgumentParser()
-#     parser.add_argument('--train_prop', type=float, default=0.5)
-#     parser.add_argument('--valid_prop', type=float, default=0.25)
-#     parser.add_argument('--dname', default='cora')
-#     # method in ['SetGNN','CEGCN','CEGAT','HyperGCN','HGNN','HCHA']
-#     parser.add_argument('--method', default='AllSetTransformer')
-#     parser.add_argument('--epochs', default=500, type=int)
-#     # Number of runs for each split (test fix, only shuffle train/val)
-#     parser.add_argument('--runs', default=20, type=int)
-#     parser.add_argument('--cuda', default=0, choices=[-1, 0, 1], type=int)
-#     parser.add_argument('--dropout', default=0.5, type=float)
-#     parser.add_argument('--lr', default=0.001, type=float)
-#     parser.add_argument('--wd', default=0.0, type=float)
-#     # How many layers of full NLConvs
-#     parser.add_argument('--All_num_layers', default=2, type=int)
-#     parser.add_argument('--MLP_num_layers', default=2,
-#                         type=int)  # How many layers of encoder
-#     parser.add_argument('--MLP_hidden', default=64,
-#                         type=int)  # Encoder hidden units
-#     parser.add_argument('--Classifier_num_layers', default=2,
-#                         type=int)  # How many layers of decoder
-#     parser.add_argument('--Classifier_hidden', default=64,
-#                         type=int)  # Decoder hidden units
-#     parser.add_argument('--display_step', type=int, default=-1)
-#     parser.add_argument('--aggregate', default='mean', choices=['sum', 'mean'])
-#     # ['all_one','deg_half_sym']
-#     parser.add_argument('--normtype', default='all_one')
-#     parser.add_argument('--add_self_loop', action='store_false')
-#     # NormLayer for MLP. ['bn','ln','None']
-#     parser.add_argument('--normalization', default='ln')
-#     parser.add_argument('--deepset_input_norm', default=True)
-#     parser.add_argument('--GPR', action='store_false')  # skip all but last dec
-#     # skip all but last dec
-#     parser.add_argument('--LearnMask', action='store_false')
-#     parser.add_argument('--num_features', default=0, type=int)  # Placeholder
-#     parser.add_argument('--num_classes', default=0, type=int)  # Placeholder
-#     # Choose std for synthetic feature noise
-#     parser.add_argument('--feature_noise', default='1', type=str)
-#     # whether the he contain self node or not
-#     parser.add_argument('--exclude_self', action='store_true')
-#     parser.add_argument('--PMA', action='store_true')
-#     #     Args for HyperGCN
-#     parser.add_argument('--HyperGCN_mediators', action='store_true')
-#     parser.add_argument('--HyperGCN_fast', action='store_true')
-#     #     Args for Attentions: GAT and SetGNN
-#     parser.add_argument('--heads', default=1, type=int)  # Placeholder
-#     parser.add_argument('--output_heads', default=1, type=int)  # Placeholder
-#     #     Args for HNHN
-#     parser.add_argument('--HNHN_alpha', default=-1.5, type=float)
-#     parser.add_argument('--HNHN_beta', default=-0.5, type=float)
-#     parser.add_argument('--HNHN_nonlinear_inbetween', default=True, type=bool)
-#     #     Args for HCHA
-#     parser.add_argument('--HCHA_symdegnorm', action='store_true')
-#     #     Args for UniGNN
-#     parser.add_argument('--UniGNN_use-norm', action="store_true", help='use norm in the final layer')
-#     parser.add_argument('--UniGNN_degV', default=0)
-#     parser.add_argument('--UniGNN_degE', default=0)
-#
-#     parser.set_defaults(PMA=True)  # True: Use PMA. False: Use Deepsets.
-#     parser.set_defaults(add_self_loop=True)
-#     parser.set_defaults(exclude_self=False)
-#     parser.set_defaults(GPR=False)
-#     parser.set_defaults(LearnMask=False)
-#     parser.set_defaults(HyperGCN_mediators=True)
-#     parser.set_defaults(HyperGCN_fast=True)
-#     parser.set_defaults(HCHA_symdegnorm=False)
-#
-#     #     Use the line below for .py file
-#     args = parser.parse_args()
-#     #     Use the line below for notebook
-#     # args = parser.parse_args([])
-#     # args, _ = parser.parse_known_args()
-#
-#     # # Part 1: Load data
-#
-#     ### Load and preprocess data ###
-#     existing_dataset = ['20newsW100', 'ModelNet40', 'zoo',
-#                         'NTU2012', 'Mushroom',
-#                         'coauthor_cora', 'coauthor_dblp',
-#                         'yelp', 'amazon-reviews', 'walmart-trips', 'house-committees',
-#                         'walmart-trips-100', 'house-committees-100',
-#                         'cora', 'citeseer', 'pubmed']
-#
-#     synthetic_list = ['amazon-reviews', 'walmart-trips', 'house-committees', 'walmart-trips-100',
-#                       'house-committees-100']
-#
-#     if args.dname in existing_dataset:
-#         dname = args.dname
-#         f_noise = args.feature_noise
-#         if (f_noise is not None) and dname in synthetic_list:
-#             p2raw = '../data/raw_data/AllSet_all_raw_data/'
-#             dataset = dataset_Hypergraph(name=dname,
-#                                          feature_noise=f_noise,
-#                                          p2raw=p2raw)
-#         else:
-#             if dname in ['cora', 'citeseer', 'pubmed']:
-#                 p2raw = '../data/cocitation/'
-#             elif dname in ['coauthor_cora', 'coauthor_dblp']:
-#                 p2raw = '../data/AllSet_all_raw_data/coauthorship/'
-#             elif dname in ['yelp']:
-#                 p2raw = '../data/AllSet_all_raw_data/yelp/'
-#             else:
-#                 p2raw = '../data/AllSet_all_raw_data/'
-#             dataset = dataset_Hypergraph(name=dname, root='../data/pyg_data/hypergraph_dataset_updated/',
-#                                          p2raw=p2raw)
-#         data = dataset.data
-#         args.num_features = dataset.num_features
-#         args.num_classes = dataset.num_classes
-#         if args.dname in ['yelp', 'walmart-trips', 'house-committees', 'walmart-trips-100', 'house-committees-100']:
-#             #         Shift the y label to start with 0
-#             args.num_classes = len(data.y.unique())
-#             data.y = data.y - data.y.min()
-#         if not hasattr(data, 'n_x'):
-#             data.n_x = torch.tensor([data.x.shape[0]])
-#         if not hasattr(data, 'num_hyperedges'):
-#             # note that we assume the he_id is consecutive.
-#             data.num_hyperedges = torch.tensor(
-#                 [data.edge_index[0].max() - data.n_x[0] + 1])
-#
-#     # ipdb.set_trace()
-#     #     Preprocessing
-#     # if args.method in ['SetGNN', 'SetGPRGNN', 'SetGNN-DeepSet']:
-#     if args.method in ['AllSetTransformer', 'AllDeepSets']:
-#         data = ExtractV2E(data)
-#         if args.add_self_loop:
-#             data = Add_Self_Loops(data)
-#         if args.exclude_self:
-#             data = expand_edge_index(data)
-#
-#         data = norm_contruction(data, option=args.normtype)
-#
-#
-#         #     Get splits
-#     split_idx_lst = []
-#     for run in range(args.runs):
-#         split_idx = rand_train_test_idx(
-#             data.y, train_prop=args.train_prop, valid_prop=args.valid_prop)
-#         split_idx_lst.append(split_idx)
-#
-#     # # Part 2: Load model
-#
-#     model = parse_method(args, data)
-#     # put things to device
-#     if args.cuda in [0, 1]:
-#         device = torch.device('cuda:' + str(args.cuda)
-#                               if torch.cuda.is_available() else 'cpu')
-#     else:
-#         device = torch.device('cpu')
-#
-#     model, data = model.to(device), data.to(device)
-#     if args.method == 'UniGCNII':
-#         args.UniGNN_degV = args.UniGNN_degV.to(device)
-#         args.UniGNN_degE = args.UniGNN_degE.to(device)
-#
-#     num_params = count_parameters(model)
-#
-#     # # Part 3: Main. Training + Evaluation
-#
-#     logger = Logger(args.runs, args)
-#
-#     criterion = nn.NLLLoss()
-#     eval_func = eval_acc
-#
-#     model.train()
-#     # print('MODEL:', model)
-#
-#     ### Training loop ###
-#     runtime_list = []
-#     for run in tqdm(range(args.runs)):
-#         start_time = time.time()
-#         split_idx = split_idx_lst[run]
-#         train_idx = split_idx['train'].to(device)
-#         model.reset_parameters()
-#         if args.method == 'UniGCNII':
-#             optimizer = torch.optim.Adam([
-#                 dict(params=model.reg_params, weight_decay=0.01),
-#                 dict(params=model.non_reg_params, weight_decay=5e-4)
-#             ], lr=0.01)
-#         else:
-#             optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)
-#         #     This is for HNHN only
-#         #     if args.method == 'HNHN':
-#         #         scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=100, gamma=0.51)
-#         best_val = float('-inf')
-#         for epoch in range(args.epochs):
-#             #         Training part
-#             model.train()
-#             optimizer.zero_grad()
-#             out = model(data)
-#             out = F.log_softmax(out, dim=1)
-#             loss = criterion(out[train_idx], data.y[train_idx])
-#             loss.backward()
-#             optimizer.step()
-#             #         if args.method == 'HNHN':
-#             #             scheduler.step()
-#             #         Evaluation part
-#             result = evaluate(model, data, split_idx, eval_func)
-#             logger.add_result(run, result[:3])
-#
-#             if epoch % args.display_step == 0 and args.display_step > 0:
-#                 print(f'Epoch: {epoch:02d}, '
-#                       f'Train Loss: {loss:.4f}, '
-#                       f'Valid Loss: {result[4]:.4f}, '
-#                       f'Test  Loss: {result[5]:.4f}, '
-#                       f'Train Acc: {100 * result[0]:.2f}%, '
-#                       f'Valid Acc: {100 * result[1]:.2f}%, '
-#                       f'Test  Acc: {100 * result[2]:.2f}%')
-#
-#         end_time = time.time()
-#         runtime_list.append(end_time - start_time)
-#
-#         # logger.print_statistics(run)
-#
-#     ### Save results ###
-#     avg_time, std_time = np.mean(runtime_list), np.std(runtime_list)
-#
-#     best_val, best_test = logger.print_statistics()
-#     res_root = 'hyperparameter_tunning'
-#     if not osp.isdir(res_root):
-#         os.makedirs(res_root)
-#
-#     filename = f'{res_root}/{args.dname}_noise_{args.feature_noise}.csv'
-#     print(f"Saving results to {filename}")
-#     with open(filename, 'a+') as write_obj:
-#         cur_line = f'{args.method}_{args.lr}_{args.wd}_{args.heads}'
-#         cur_line += f',{best_val.mean():.3f} ± {best_val.std():.3f}'
-#         cur_line += f',{best_test.mean():.3f} ± {best_test.std():.3f}'
-#         cur_line += f',{num_params}, {avg_time:.2f}s, {std_time:.2f}s'
-#         cur_line += f',{avg_time // 60}min{(avg_time % 60):.2f}s'
-#         cur_line += f'\n'
-#         write_obj.write(cur_line)
-#
-#     all_args_file = f'{res_root}/all_args_{args.dname}_noise_{args.feature_noise}.csv'
-#     with open(all_args_file, 'a+') as f:
-#         f.write(str(args))
-#         f.write('\n')
-#
-#     print('All done! Exit python code')
-#     quit()
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+from easygraph.nn.convs.common import MLP
+from easygraph.nn.convs.hypergraphs.halfnlh_conv import HalfNLHconv
+from torch.nn import Linear
+
+
+__all__ = ["SetGNN"]
+
+
+class SetGNN(nn.Module):
+    r"""The SetGNN model proposed in `YOU ARE ALLSET: A MULTISET LEARNING FRAMEWORK FOR HYPERGRAPH NEURAL NETWORKS <https://openreview.net/pdf?id=hpBTIv2uy_E>`_ paper (ICLR 2022).
+
+    Parameters:
+        ``num_features`` (``int``): : The dimension of node features.
+        ``num_classes`` (``int``): The Number of class of the classification task.
+        ``Classifier_hidden`` (``int``): Decoder hidden units.
+        ``Classifier_num_layers`` (``int``): Layers of decoder.
+        ``MLP_hidden`` (``int``): Encoder hidden units.
+        ``MLP_num_layers`` (``int``): Layers of encoder.
+         ``dropout`` (``float``, optional): Dropout ratio. Defaults to 0.5.
+        ``aggregate`` (``str``): The aggregation method. Defaults to ``add``
+        ``normalization`` (``str``): The normalization method. Defaults to ``ln``
+        ``deepset_input_norm`` (``bool``):  Defaults to True.
+        ``heads`` (``int``):  Defaults to 1
+        `PMA`` (``bool``):  Defaults to True
+        `GPR`` (``bool``):  Defaults to False
+        `LearnMask`` (``bool``):  Defaults to False
+        `norm`` (``Tensor``):  The weight for edges in bipartite graphs, correspond to data.edge_index
+
+    """
+
+    def __init__(
+        self,
+        num_features,
+        num_classes,
+        Classifier_hidden=64,
+        Classifier_num_layers=2,
+        MLP_hidden=64,
+        MLP_num_layers=2,
+        All_num_layers=2,
+        dropout=0.5,
+        aggregate="mean",
+        normalization="ln",
+        deepset_input_norm=True,
+        heads=1,
+        PMA=True,
+        GPR=False,
+        LearnMask=False,
+        norm=None,
+    ):
+        super(SetGNN, self).__init__()
+        """
+        args should contain the following:
+        V_in_dim, V_enc_hid_dim, V_dec_hid_dim, V_out_dim, V_enc_num_layers, V_dec_num_layers
+        E_in_dim, E_enc_hid_dim, E_dec_hid_dim, E_out_dim, E_enc_num_layers, E_dec_num_layers
+        All_num_layers,dropout
+        !!! V_in_dim should be the dimension of node features
+        !!! E_out_dim should be the number of classes (for classification)
+        """
+
+        #         Now set all dropout the same, but can be different
+        self.All_num_layers = All_num_layers
+        self.dropout = dropout
+        self.aggr = aggregate
+        self.NormLayer = normalization
+        self.InputNorm = deepset_input_norm
+        self.GPR = GPR
+        self.LearnMask = LearnMask
+        #         Now define V2EConvs[i], V2EConvs[i] for ith layers
+        #         Currently we assume there's no hyperedge features, which means V_out_dim = E_in_dim
+        #         If there's hyperedge features, concat with Vpart decoder output features [V_feat||E_feat]
+        self.V2EConvs = nn.ModuleList()
+        self.E2VConvs = nn.ModuleList()
+        self.bnV2Es = nn.ModuleList()
+        self.bnE2Vs = nn.ModuleList()
+
+        if self.LearnMask:
+            self.Importance = nn.Parameter(torch.ones(norm.size()))
+
+        if self.All_num_layers == 0:
+            self.classifier = MLP(
+                in_channels=num_features,
+                hidden_channels=Classifier_hidden,
+                out_channels=num_classes,
+                num_layers=Classifier_num_layers,
+                dropout=self.dropout,
+                normalization=self.NormLayer,
+                InputNorm=False,
+            )
+        else:
+            self.V2EConvs.append(
+                HalfNLHconv(
+                    in_dim=num_features,
+                    hid_dim=MLP_hidden,
+                    out_dim=MLP_hidden,
+                    num_layers=MLP_num_layers,
+                    dropout=self.dropout,
+                    normalization=self.NormLayer,
+                    InputNorm=self.InputNorm,
+                    heads=heads,
+                    attention=PMA,
+                )
+            )
+            self.bnV2Es.append(nn.BatchNorm1d(MLP_hidden))
+            self.E2VConvs.append(
+                HalfNLHconv(
+                    in_dim=MLP_hidden,
+                    hid_dim=MLP_hidden,
+                    out_dim=MLP_hidden,
+                    num_layers=MLP_num_layers,
+                    dropout=self.dropout,
+                    normalization=self.NormLayer,
+                    InputNorm=self.InputNorm,
+                    heads=heads,
+                    attention=PMA,
+                )
+            )
+            self.bnE2Vs.append(nn.BatchNorm1d(MLP_hidden))
+            for _ in range(self.All_num_layers - 1):
+                self.V2EConvs.append(
+                    HalfNLHconv(
+                        in_dim=MLP_hidden,
+                        hid_dim=MLP_hidden,
+                        out_dim=MLP_hidden,
+                        num_layers=MLP_num_layers,
+                        dropout=self.dropout,
+                        normalization=self.NormLayer,
+                        InputNorm=self.InputNorm,
+                        heads=heads,
+                        attention=PMA,
+                    )
+                )
+                self.bnV2Es.append(nn.BatchNorm1d(MLP_hidden))
+                self.E2VConvs.append(
+                    HalfNLHconv(
+                        in_dim=MLP_hidden,
+                        hid_dim=MLP_hidden,
+                        out_dim=MLP_hidden,
+                        num_layers=MLP_num_layers,
+                        dropout=self.dropout,
+                        normalization=self.NormLayer,
+                        InputNorm=self.InputNorm,
+                        heads=heads,
+                        attention=PMA,
+                    )
+                )
+                self.bnE2Vs.append(nn.BatchNorm1d(MLP_hidden))
+
+            if self.GPR:
+                self.MLP = MLP(
+                    in_channels=num_features,
+                    hidden_channels=MLP_hidden,
+                    out_channels=MLP_hidden,
+                    num_layers=MLP_num_layers,
+                    dropout=self.dropout,
+                    normalization=self.NormLayer,
+                    InputNorm=False,
+                )
+                self.GPRweights = Linear(self.All_num_layers + 1, 1, bias=False)
+                self.classifier = MLP(
+                    in_channels=MLP_hidden,
+                    hidden_channels=Classifier_hidden,
+                    out_channels=num_classes,
+                    num_layers=Classifier_num_layers,
+                    dropout=self.dropout,
+                    normalization=self.NormLayer,
+                    InputNorm=False,
+                )
+            else:
+                self.classifier = MLP(
+                    in_channels=MLP_hidden,
+                    hidden_channels=Classifier_hidden,
+                    out_channels=num_classes,
+                    num_layers=Classifier_num_layers,
+                    dropout=self.dropout,
+                    normalization=self.NormLayer,
+                    InputNorm=False,
+                )
+
+    def reset_parameters(self):
+        for layer in self.V2EConvs:
+            layer.reset_parameters()
+        for layer in self.E2VConvs:
+            layer.reset_parameters()
+        for layer in self.bnV2Es:
+            layer.reset_parameters()
+        for layer in self.bnE2Vs:
+            layer.reset_parameters()
+        self.classifier.reset_parameters()
+        if self.GPR:
+            self.MLP.reset_parameters()
+            self.GPRweights.reset_parameters()
+        if self.LearnMask:
+            nn.init.ones_(self.Importance)
+
+    def forward(self, data):
+        """
+        The data should contain the follows
+        data.x: node features
+        data.edge_index: edge list (of size (2,|E|)) where data.edge_index[0] contains nodes and data.edge_index[1] contains hyperedges
+        !!! Note that self loop should be assigned to a new (hyper)edge id!!!
+        !!! Also note that the (hyper)edge id should start at 0 (akin to node id)
+        data.norm: The weight for edges in bipartite graphs, correspond to data.edge_index
+        !!! Note that we output final node representation. Loss should be defined outside.
+        """
+
+        x, edge_index, norm = data.x, data.edge_index, data.norm
+        if self.LearnMask:
+            norm = self.Importance * norm
+        cidx = edge_index[1].min()
+        edge_index[1] -= cidx  # make sure we do not waste memory
+        reversed_edge_index = torch.stack([edge_index[1], edge_index[0]], dim=0)
+        if self.GPR:
+            xs = []
+            xs.append(F.relu(self.MLP(x)))
+            for i, _ in enumerate(self.V2EConvs):
+                x = F.relu(self.V2EConvs[i](x, edge_index, norm, self.aggr))
+                #                 x = self.bnV2Es[i](x)
+                x = F.dropout(x, p=self.dropout, training=self.training)
+                x = self.E2VConvs[i](x, reversed_edge_index, norm, self.aggr)
+                x = F.relu(x)
+                xs.append(x)
+                #                 x = self.bnE2Vs[i](x)
+                x = F.dropout(x, p=self.dropout, training=self.training)
+            x = torch.stack(xs, dim=-1)
+            x = self.GPRweights(x).squeeze()
+            x = self.classifier(x)
+        else:
+            x = F.dropout(x, p=0.2, training=self.training)  # Input dropout
+            for i, _ in enumerate(self.V2EConvs):
+                x = F.relu(self.V2EConvs[i](x, edge_index, norm, self.aggr))
+                #                 x = self.bnV2Es[i](x)
+                x = F.dropout(x, p=self.dropout, training=self.training)
+                x = F.relu(self.E2VConvs[i](x, reversed_edge_index, norm, self.aggr))
+                #                 x = self.bnE2Vs[i](x)
+                x = F.dropout(x, p=self.dropout, training=self.training)
+            x = self.classifier(x)
+
+        return x
+
+
+# import os
+# import time
+# import torch
+# import argparse
+#
+# import numpy as np
+# import os.path as osp
+
+# import matplotlib.pyplot as plt
+#
+# from tqdm import tqdm
+#
+#
+# from easygraph.model.data.convert_datasets_to_pygDataset import dataset_Hypergraph
+# from torch_scatter import scatter_add
+#
+# def parse_method(args, data):
+#     #     Currently we don't set hyperparameters w.r.t. different dataset
+#     if args.method == 'AllSetTransformer':
+#         if args.LearnMask:
+#             model = SetGNN(args, data.norm)
+#         else:
+#             model = SetGNN(num_classes=args.num_classes,num_features=args.num_features)
+#
+#     elif args.method == 'AllDeepSets':
+#         args.PMA = False
+#         args.aggregate = 'add'
+#         if args.LearnMask:
+#             model = SetGNN(args, data.norm)
+#         else:
+#             model = SetGNN(args)
+#
+#     #     elif args.method == 'SetGPRGNN':
+#     #         model = SetGPRGNN(args)
+#
+#
+#     return model
+#
+#
+# class Logger(object):
+#     """ Adapted from https://github.com/snap-stanford/ogb/ """
+#
+#     def __init__(self, runs, info=None):
+#         self.info = info
+#         self.results = [[] for _ in range(runs)]
+#
+#     def add_result(self, run, result):
+#         assert len(result) == 3
+#         assert run >= 0 and run < len(self.results)
+#         self.results[run].append(result)
+#
+#     def print_statistics(self, run=None):
+#         if run is not None:
+#             result = 100 * torch.tensor(self.results[run])
+#             argmax = result[:, 1].argmax().item()
+#             print(f'Run {run + 1:02d}:')
+#             print(f'Highest Train: {result[:, 0].max():.2f}')
+#             print(f'Highest Valid: {result[:, 1].max():.2f}')
+#             print(f'  Final Train: {result[argmax, 0]:.2f}')
+#             print(f'   Final Test: {result[argmax, 2]:.2f}')
+#         else:
+#             result = 100 * torch.tensor(self.results)
+#
+#             best_results = []
+#             for r in result:
+#                 train1 = r[:, 0].max().item()
+#                 valid = r[:, 1].max().item()
+#                 train2 = r[r[:, 1].argmax(), 0].item()
+#                 test = r[r[:, 1].argmax(), 2].item()
+#                 best_results.append((train1, valid, train2, test))
+#
+#             best_result = torch.tensor(best_results)
+#
+#             print(f'All runs:')
+#             r = best_result[:, 0]
+#             print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')
+#             r = best_result[:, 1]
+#             print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')
+#             r = best_result[:, 2]
+#             print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')
+#             r = best_result[:, 3]
+#             print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')
+#
+#             return best_result[:, 1], best_result[:, 3]
+#
+#     def plot_result(self, run=None):
+#         plt.style.use('seaborn')
+#         if run is not None:
+#             result = 100 * torch.tensor(self.results[run])
+#             x = torch.arange(result.shape[0])
+#             plt.figure()
+#             print(f'Run {run + 1:02d}:')
+#             plt.plot(x, result[:, 0], x, result[:, 1], x, result[:, 2])
+#             plt.legend(['Train', 'Valid', 'Test'])
+#         else:
+#             result = 100 * torch.tensor(self.results[0])
+#             x = torch.arange(result.shape[0])
+#             plt.figure()
+#             #             print(f'Run {run + 1:02d}:')
+#             plt.plot(x, result[:, 0], x, result[:, 1], x, result[:, 2])
+#             plt.legend(['Train', 'Valid', 'Test'])
+#
+#
+# @torch.no_grad()
+# def evaluate(model, data, split_idx, eval_func, result=None):
+#     if result is not None:
+#         out = result
+#     else:
+#         model.eval()
+#         out = model(data)
+#         out = F.log_softmax(out, dim=1)
+#
+#     train_acc = eval_func(
+#         data.y[split_idx['train']], out[split_idx['train']])
+#     valid_acc = eval_func(
+#         data.y[split_idx['valid']], out[split_idx['valid']])
+#     test_acc = eval_func(
+#         data.y[split_idx['test']], out[split_idx['test']])
+#
+#     #     Also keep track of losses
+#     train_loss = F.nll_loss(
+#         out[split_idx['train']], data.y[split_idx['train']])
+#     valid_loss = F.nll_loss(
+#         out[split_idx['valid']], data.y[split_idx['valid']])
+#     test_loss = F.nll_loss(
+#         out[split_idx['test']], data.y[split_idx['test']])
+#     return train_acc, valid_acc, test_acc, train_loss, valid_loss, test_loss, out
+#
+#
+# def eval_acc(y_true, y_pred):
+#     acc_list = []
+#     y_true = y_true.detach().cpu().numpy()
+#     y_pred = y_pred.argmax(dim=-1, keepdim=False).detach().cpu().numpy()
+#
+#     #     ipdb.set_trace()
+#     #     for i in range(y_true.shape[1]):
+#     is_labeled = y_true == y_true
+#     correct = y_true[is_labeled] == y_pred[is_labeled]
+#     acc_list.append(float(np.sum(correct)) / len(correct))
+#
+#     return sum(acc_list) / len(acc_list)
+#
+#
+# def count_parameters(model):
+#     return sum(p.numel() for p in model.parameters() if p.requires_grad)
+#
+#
+# # --- Main part of the training ---
+# # # Part 0: Parse arguments
+#
+#
+# """
+#
+# """
+# def ExtractV2E(data):
+#     # Assume edge_index = [V|E;E|V]
+#     edge_index = data.edge_index
+# #     First, ensure the sorting is correct (increasing along edge_index[0])
+#     _, sorted_idx = torch.sort(edge_index[0])
+#     edge_index = edge_index[:, sorted_idx].type(torch.LongTensor)
+#     print("data.n_x:",data.n_x)
+#     # num_nodes = data.n_x[0]
+#     num_nodes = data.n_x
+#     num_hyperedges = data.num_hyperedges
+#     if not ((data.n_x+data.num_hyperedges-1) == data.edge_index[0].max().item()):
+#         print('num_hyperedges does not match! 1')
+#         return
+#     cidx = torch.where(edge_index[0] == num_nodes)[
+#         0].min()  # cidx: [V...|cidx E...]
+#     data.edge_index = edge_index[:, :cidx].type(torch.LongTensor)
+#     return data
+#
+# def Add_Self_Loops(data):
+#     # update so we dont jump on some indices
+#     # Assume edge_index = [V;E]. If not, use ExtractV2E()
+#     edge_index = data.edge_index
+#     num_nodes = data.n_x
+#     num_hyperedges = data.num_hyperedges
+#
+#     if not ((data.n_x + data.num_hyperedges - 1) == data.edge_index[1].max().item()):
+#         print('num_hyperedges does not match! 2')
+#         return
+#
+#     hyperedge_appear_fre = Counter(edge_index[1].numpy())
+#     # store the nodes that already have self-loops
+#     skip_node_lst = []
+#     for edge in hyperedge_appear_fre:
+#         if hyperedge_appear_fre[edge] == 1:
+#             skip_node = edge_index[0][torch.where(
+#                 edge_index[1] == edge)[0].item()]
+#             skip_node_lst.append(skip_node.item())
+#
+#     new_edge_idx = edge_index[1].max() + 1
+#     new_edges = torch.zeros(
+#         (2, num_nodes - len(skip_node_lst)), dtype=edge_index.dtype)
+#     tmp_count = 0
+#     for i in range(num_nodes):
+#         if i not in skip_node_lst:
+#             new_edges[0][tmp_count] = i
+#             new_edges[1][tmp_count] = new_edge_idx
+#             new_edge_idx += 1
+#             tmp_count += 1
+#
+#     data.totedges = num_hyperedges + num_nodes - len(skip_node_lst)
+#     edge_index = torch.cat((edge_index, new_edges), dim=1)
+#     # Sort along w.r.t. nodes
+#     _, sorted_idx = torch.sort(edge_index[0])
+#     data.edge_index = edge_index[:, sorted_idx].type(torch.LongTensor)
+#     return data
+#
+# def expand_edge_index(data, edge_th=0):
+#     '''
+#     args:
+#         num_nodes: regular nodes. i.e. x.shape[0]
+#         num_edges: number of hyperedges. not the star expansion edges.
+#
+#     this function will expand each n2he relations, [[n_1, n_2, n_3],
+#                                                     [e_7, e_7, e_7]]
+#     to :
+#         [[n_1,   n_1,   n_2,   n_2,   n_3,   n_3],
+#          [e_7_2, e_7_3, e_7_1, e_7_3, e_7_1, e_7_2]]
+#
+#     and each he2n relations:   [[e_7, e_7, e_7],
+#                                 [n_1, n_2, n_3]]
+#     to :
+#         [[e_7_1, e_7_2, e_7_3],
+#          [n_1,   n_2,   n_3]]
+#
+#     and repeated for every hyperedge.
+#     '''
+#     edge_index = data.edge_index
+#     num_nodes = data.n_x[0].item()
+#     if hasattr(data, 'totedges'):
+#         num_edges = data.totedges
+#     else:
+#         num_edges = data.num_hyperedges[0]
+#
+#     expanded_n2he_index = []
+# #     n2he_with_same_heid = []
+#
+# #     expanded_he2n_index = []
+# #     he2n_with_same_heid = []
+#
+#     # start edge_id from the largest node_id + 1.
+#     cur_he_id = num_nodes
+#     # keep an mapping of new_edge_id to original edge_id for edge_size query.
+#     new_edge_id_2_original_edge_id = {}
+#
+#     # do the expansion for all annotated he_id in the original edge_index
+# #     ipdb.set_trace()
+#     for he_idx in range(num_nodes, num_edges + num_nodes):
+#         # find all nodes within the same hyperedge.
+#         selected_he = edge_index[:, edge_index[1] == he_idx]
+#         size_of_he = selected_he.shape[1]
+#
+# #         Trim a hyperedge if its size>edge_th
+#         if edge_th > 0:
+#             if size_of_he > edge_th:
+#                 continue
+#
+#         if size_of_he == 1:
+#             # there is only one node in this hyperedge -> self-loop node. add to graph.
+#             #             n2he_with_same_heid.append(selected_he)
+#
+#             new_n2he = selected_he.clone()
+#             new_n2he[1] = cur_he_id
+#             expanded_n2he_index.append(new_n2he)
+#
+#             # ====
+# #             new_he2n_same_heid = torch.flip(selected_he, dims = [0])
+# #             he2n_with_same_heid.append(new_he2n_same_heid)
+#
+# #             new_he2n = torch.flip(selected_he, dims = [0])
+# #             new_he2n[0] = cur_he_id
+# #             expanded_he2n_index.append(new_he2n)
+#
+#             cur_he_id += 1
+#             continue
+#
+#         # -------------------------------
+# #         # new_n2he_same_heid uses same he id for all nodes.
+# #         new_n2he_same_heid = selected_he.repeat_interleave(size_of_he - 1, dim = 1)
+# #         n2he_with_same_heid.append(new_n2he_same_heid)
+#
+#         # for new_n2he mapping. connect the nodes to all repeated he first.
+#         # then remove those connection that corresponding to the node itself.
+#         new_n2he = selected_he.repeat_interleave(size_of_he, dim=1)
+#
+#         # new_edge_ids start from the he_id from previous iteration (cur_he_id).
+#         new_edge_ids = torch.LongTensor(
+#             np.arange(cur_he_id, cur_he_id + size_of_he)).repeat(size_of_he)
+#         new_n2he[1] = new_edge_ids
+#
+#         # build a mapping between node and it's corresponding edge.
+#         # e.g. {n_1: e_7_1, n_2: e_7_2}
+#         tmp_node_id_2_he_id_dict = {}
+#         for idx in range(size_of_he):
+#             new_edge_id_2_original_edge_id[cur_he_id] = he_idx
+#             cur_node_id = selected_he[0][idx].item()
+#             tmp_node_id_2_he_id_dict[cur_node_id] = cur_he_id
+#             cur_he_id += 1
+#
+#         # create n2he by deleting the self-product edge.
+#         new_he_select_mask = torch.BoolTensor([True] * new_n2he.shape[1])
+#         for col_idx in range(new_n2he.shape[1]):
+#             tmp_node_id, tmp_edge_id = new_n2he[0, col_idx].item(
+#             ), new_n2he[1, col_idx].item()
+#             if tmp_node_id_2_he_id_dict[tmp_node_id] == tmp_edge_id:
+#                 new_he_select_mask[col_idx] = False
+#         new_n2he = new_n2he[:, new_he_select_mask]
+#         expanded_n2he_index.append(new_n2he)
+#
+#
+# #         # ---------------------------
+# #         # create he2n from mapping.
+# #         new_he2n = np.array([[he_id, node_id] for node_id, he_id in tmp_node_id_2_he_id_dict.items()])
+# #         new_he2n = torch.from_numpy(new_he2n.T).to(device = edge_index.device)
+# #         expanded_he2n_index.append(new_he2n)
+#
+# #         # create he2n with same heid as input edge_index.
+# #         new_he2n_same_heid = torch.zeros_like(new_he2n, device = edge_index.device)
+# #         new_he2n_same_heid[1] = new_he2n[1]
+# #         new_he2n_same_heid[0] = torch.ones_like(new_he2n[0]) * he_idx
+# #         he2n_with_same_heid.append(new_he2n_same_heid)
+#
+#     new_edge_index = torch.cat(expanded_n2he_index, dim=1)
+# #     new_he2n_index = torch.cat(expanded_he2n_index, dim = 1)
+# #     new_edge_index = torch.cat([new_n2he_index, new_he2n_index], dim = 1)
+#     # sort the new_edge_index by first row. (node_ids)
+#     new_order = new_edge_index[0].argsort()
+#     data.edge_index = new_edge_index[:, new_order]
+#
+#     return data
+#
+# def rand_train_test_idx(label, train_prop=.5, valid_prop=.25, ignore_negative=True, balance=False):
+#     """ Adapted from https://github.com/CUAI/Non-Homophily-Benchmarks"""
+#     """ randomly splits label into train/valid/test splits """
+#     if not balance:
+#         if ignore_negative:
+#             labeled_nodes = torch.where(label != -1)[0]
+#         else:
+#             labeled_nodes = label
+#
+#         n = labeled_nodes.shape[0]
+#         train_num = int(n * train_prop)
+#         valid_num = int(n * valid_prop)
+#
+#         perm = torch.as_tensor(np.random.permutation(n))
+#
+#         train_indices = perm[:train_num]
+#         val_indices = perm[train_num:train_num + valid_num]
+#         test_indices = perm[train_num + valid_num:]
+#
+#         if not ignore_negative:
+#             return train_indices, val_indices, test_indices
+#
+#         train_idx = labeled_nodes[train_indices]
+#         valid_idx = labeled_nodes[val_indices]
+#         test_idx = labeled_nodes[test_indices]
+#
+#         split_idx = {'train': train_idx,
+#                      'valid': valid_idx,
+#                      'test': test_idx}
+#     else:
+#         #         ipdb.set_trace()
+#         indices = []
+#         for i in range(label.max()+1):
+#             index = torch.where((label == i))[0].view(-1)
+#             index = index[torch.randperm(index.size(0))]
+#             indices.append(index)
+#
+#         percls_trn = int(train_prop/(label.max()+1)*len(label))
+#         val_lb = int(valid_prop*len(label))
+#         train_idx = torch.cat([i[:percls_trn] for i in indices], dim=0)
+#         rest_index = torch.cat([i[percls_trn:] for i in indices], dim=0)
+#         rest_index = rest_index[torch.randperm(rest_index.size(0))]
+#         valid_idx = rest_index[:val_lb]
+#         test_idx = rest_index[val_lb:]
+#         split_idx = {'train': train_idx,
+#                      'valid': valid_idx,
+#                      'test': test_idx}
+#     return split_idx
+#
+# def norm_contruction(data, option='all_one', TYPE='V2E'):
+#     from torch_geometric.nn.conv.gcn_conv import gcn_norm
+#     if TYPE == 'V2E':
+#         if option == 'all_one':
+#             data.norm = torch.ones_like(data.edge_index[0])
+#
+#         elif option == 'deg_half_sym':
+#             edge_weight = torch.ones_like(data.edge_index[0])
+#             cidx = data.edge_index[1].min()
+#             Vdeg = scatter_add(edge_weight, data.edge_index[0], dim=0)
+#             HEdeg = scatter_add(edge_weight, data.edge_index[1]-cidx, dim=0)
+#             V_norm = Vdeg**(-1/2)
+#             E_norm = HEdeg**(-1/2)
+#             data.norm = V_norm[data.edge_index[0]] * \
+#                 E_norm[data.edge_index[1]-cidx]
+#
+#     elif TYPE == 'V2V':
+#         data.edge_index, data.norm = gcn_norm(
+#             data.edge_index, data.norm, add_self_loops=True)
+#     return data
+#
+# if __name__ == '__main__':
+#     parser = argparse.ArgumentParser()
+#     parser.add_argument('--train_prop', type=float, default=0.5)
+#     parser.add_argument('--valid_prop', type=float, default=0.25)
+#     parser.add_argument('--dname', default='cora')
+#     # method in ['SetGNN','CEGCN','CEGAT','HyperGCN','HGNN','HCHA']
+#     parser.add_argument('--method', default='AllSetTransformer')
+#     parser.add_argument('--epochs', default=500, type=int)
+#     # Number of runs for each split (test fix, only shuffle train/val)
+#     parser.add_argument('--runs', default=20, type=int)
+#     parser.add_argument('--cuda', default=0, choices=[-1, 0, 1], type=int)
+#     parser.add_argument('--dropout', default=0.5, type=float)
+#     parser.add_argument('--lr', default=0.001, type=float)
+#     parser.add_argument('--wd', default=0.0, type=float)
+#     # How many layers of full NLConvs
+#     parser.add_argument('--All_num_layers', default=2, type=int)
+#     parser.add_argument('--MLP_num_layers', default=2,
+#                         type=int)  # How many layers of encoder
+#     parser.add_argument('--MLP_hidden', default=64,
+#                         type=int)  # Encoder hidden units
+#     parser.add_argument('--Classifier_num_layers', default=2,
+#                         type=int)  # How many layers of decoder
+#     parser.add_argument('--Classifier_hidden', default=64,
+#                         type=int)  # Decoder hidden units
+#     parser.add_argument('--display_step', type=int, default=-1)
+#     parser.add_argument('--aggregate', default='mean', choices=['sum', 'mean'])
+#     # ['all_one','deg_half_sym']
+#     parser.add_argument('--normtype', default='all_one')
+#     parser.add_argument('--add_self_loop', action='store_false')
+#     # NormLayer for MLP. ['bn','ln','None']
+#     parser.add_argument('--normalization', default='ln')
+#     parser.add_argument('--deepset_input_norm', default=True)
+#     parser.add_argument('--GPR', action='store_false')  # skip all but last dec
+#     # skip all but last dec
+#     parser.add_argument('--LearnMask', action='store_false')
+#     parser.add_argument('--num_features', default=0, type=int)  # Placeholder
+#     parser.add_argument('--num_classes', default=0, type=int)  # Placeholder
+#     # Choose std for synthetic feature noise
+#     parser.add_argument('--feature_noise', default='1', type=str)
+#     # whether the he contain self node or not
+#     parser.add_argument('--exclude_self', action='store_true')
+#     parser.add_argument('--PMA', action='store_true')
+#     #     Args for HyperGCN
+#     parser.add_argument('--HyperGCN_mediators', action='store_true')
+#     parser.add_argument('--HyperGCN_fast', action='store_true')
+#     #     Args for Attentions: GAT and SetGNN
+#     parser.add_argument('--heads', default=1, type=int)  # Placeholder
+#     parser.add_argument('--output_heads', default=1, type=int)  # Placeholder
+#     #     Args for HNHN
+#     parser.add_argument('--HNHN_alpha', default=-1.5, type=float)
+#     parser.add_argument('--HNHN_beta', default=-0.5, type=float)
+#     parser.add_argument('--HNHN_nonlinear_inbetween', default=True, type=bool)
+#     #     Args for HCHA
+#     parser.add_argument('--HCHA_symdegnorm', action='store_true')
+#     #     Args for UniGNN
+#     parser.add_argument('--UniGNN_use-norm', action="store_true", help='use norm in the final layer')
+#     parser.add_argument('--UniGNN_degV', default=0)
+#     parser.add_argument('--UniGNN_degE', default=0)
+#
+#     parser.set_defaults(PMA=True)  # True: Use PMA. False: Use Deepsets.
+#     parser.set_defaults(add_self_loop=True)
+#     parser.set_defaults(exclude_self=False)
+#     parser.set_defaults(GPR=False)
+#     parser.set_defaults(LearnMask=False)
+#     parser.set_defaults(HyperGCN_mediators=True)
+#     parser.set_defaults(HyperGCN_fast=True)
+#     parser.set_defaults(HCHA_symdegnorm=False)
+#
+#     #     Use the line below for .py file
+#     args = parser.parse_args()
+#     #     Use the line below for notebook
+#     # args = parser.parse_args([])
+#     # args, _ = parser.parse_known_args()
+#
+#     # # Part 1: Load data
+#
+#     ### Load and preprocess data ###
+#     existing_dataset = ['20newsW100', 'ModelNet40', 'zoo',
+#                         'NTU2012', 'Mushroom',
+#                         'coauthor_cora', 'coauthor_dblp',
+#                         'yelp', 'amazon-reviews', 'walmart-trips', 'house-committees',
+#                         'walmart-trips-100', 'house-committees-100',
+#                         'cora', 'citeseer', 'pubmed']
+#
+#     synthetic_list = ['amazon-reviews', 'walmart-trips', 'house-committees', 'walmart-trips-100',
+#                       'house-committees-100']
+#
+#     if args.dname in existing_dataset:
+#         dname = args.dname
+#         f_noise = args.feature_noise
+#         if (f_noise is not None) and dname in synthetic_list:
+#             p2raw = '../data/raw_data/AllSet_all_raw_data/'
+#             dataset = dataset_Hypergraph(name=dname,
+#                                          feature_noise=f_noise,
+#                                          p2raw=p2raw)
+#         else:
+#             if dname in ['cora', 'citeseer', 'pubmed']:
+#                 p2raw = '../data/cocitation/'
+#             elif dname in ['coauthor_cora', 'coauthor_dblp']:
+#                 p2raw = '../data/AllSet_all_raw_data/coauthorship/'
+#             elif dname in ['yelp']:
+#                 p2raw = '../data/AllSet_all_raw_data/yelp/'
+#             else:
+#                 p2raw = '../data/AllSet_all_raw_data/'
+#             dataset = dataset_Hypergraph(name=dname, root='../data/pyg_data/hypergraph_dataset_updated/',
+#                                          p2raw=p2raw)
+#         data = dataset.data
+#         args.num_features = dataset.num_features
+#         args.num_classes = dataset.num_classes
+#         if args.dname in ['yelp', 'walmart-trips', 'house-committees', 'walmart-trips-100', 'house-committees-100']:
+#             #         Shift the y label to start with 0
+#             args.num_classes = len(data.y.unique())
+#             data.y = data.y - data.y.min()
+#         if not hasattr(data, 'n_x'):
+#             data.n_x = torch.tensor([data.x.shape[0]])
+#         if not hasattr(data, 'num_hyperedges'):
+#             # note that we assume the he_id is consecutive.
+#             data.num_hyperedges = torch.tensor(
+#                 [data.edge_index[0].max() - data.n_x[0] + 1])
+#
+#     # ipdb.set_trace()
+#     #     Preprocessing
+#     # if args.method in ['SetGNN', 'SetGPRGNN', 'SetGNN-DeepSet']:
+#     if args.method in ['AllSetTransformer', 'AllDeepSets']:
+#         data = ExtractV2E(data)
+#         if args.add_self_loop:
+#             data = Add_Self_Loops(data)
+#         if args.exclude_self:
+#             data = expand_edge_index(data)
+#
+#         data = norm_contruction(data, option=args.normtype)
+#
+#
+#         #     Get splits
+#     split_idx_lst = []
+#     for run in range(args.runs):
+#         split_idx = rand_train_test_idx(
+#             data.y, train_prop=args.train_prop, valid_prop=args.valid_prop)
+#         split_idx_lst.append(split_idx)
+#
+#     # # Part 2: Load model
+#
+#     model = parse_method(args, data)
+#     # put things to device
+#     if args.cuda in [0, 1]:
+#         device = torch.device('cuda:' + str(args.cuda)
+#                               if torch.cuda.is_available() else 'cpu')
+#     else:
+#         device = torch.device('cpu')
+#
+#     model, data = model.to(device), data.to(device)
+#     if args.method == 'UniGCNII':
+#         args.UniGNN_degV = args.UniGNN_degV.to(device)
+#         args.UniGNN_degE = args.UniGNN_degE.to(device)
+#
+#     num_params = count_parameters(model)
+#
+#     # # Part 3: Main. Training + Evaluation
+#
+#     logger = Logger(args.runs, args)
+#
+#     criterion = nn.NLLLoss()
+#     eval_func = eval_acc
+#
+#     model.train()
+#     # print('MODEL:', model)
+#
+#     ### Training loop ###
+#     runtime_list = []
+#     for run in tqdm(range(args.runs)):
+#         start_time = time.time()
+#         split_idx = split_idx_lst[run]
+#         train_idx = split_idx['train'].to(device)
+#         model.reset_parameters()
+#         if args.method == 'UniGCNII':
+#             optimizer = torch.optim.Adam([
+#                 dict(params=model.reg_params, weight_decay=0.01),
+#                 dict(params=model.non_reg_params, weight_decay=5e-4)
+#             ], lr=0.01)
+#         else:
+#             optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)
+#         #     This is for HNHN only
+#         #     if args.method == 'HNHN':
+#         #         scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=100, gamma=0.51)
+#         best_val = float('-inf')
+#         for epoch in range(args.epochs):
+#             #         Training part
+#             model.train()
+#             optimizer.zero_grad()
+#             out = model(data)
+#             out = F.log_softmax(out, dim=1)
+#             loss = criterion(out[train_idx], data.y[train_idx])
+#             loss.backward()
+#             optimizer.step()
+#             #         if args.method == 'HNHN':
+#             #             scheduler.step()
+#             #         Evaluation part
+#             result = evaluate(model, data, split_idx, eval_func)
+#             logger.add_result(run, result[:3])
+#
+#             if epoch % args.display_step == 0 and args.display_step > 0:
+#                 print(f'Epoch: {epoch:02d}, '
+#                       f'Train Loss: {loss:.4f}, '
+#                       f'Valid Loss: {result[4]:.4f}, '
+#                       f'Test  Loss: {result[5]:.4f}, '
+#                       f'Train Acc: {100 * result[0]:.2f}%, '
+#                       f'Valid Acc: {100 * result[1]:.2f}%, '
+#                       f'Test  Acc: {100 * result[2]:.2f}%')
+#
+#         end_time = time.time()
+#         runtime_list.append(end_time - start_time)
+#
+#         # logger.print_statistics(run)
+#
+#     ### Save results ###
+#     avg_time, std_time = np.mean(runtime_list), np.std(runtime_list)
+#
+#     best_val, best_test = logger.print_statistics()
+#     res_root = 'hyperparameter_tunning'
+#     if not osp.isdir(res_root):
+#         os.makedirs(res_root)
+#
+#     filename = f'{res_root}/{args.dname}_noise_{args.feature_noise}.csv'
+#     print(f"Saving results to {filename}")
+#     with open(filename, 'a+') as write_obj:
+#         cur_line = f'{args.method}_{args.lr}_{args.wd}_{args.heads}'
+#         cur_line += f',{best_val.mean():.3f} ± {best_val.std():.3f}'
+#         cur_line += f',{best_test.mean():.3f} ± {best_test.std():.3f}'
+#         cur_line += f',{num_params}, {avg_time:.2f}s, {std_time:.2f}s'
+#         cur_line += f',{avg_time // 60}min{(avg_time % 60):.2f}s'
+#         cur_line += f'\n'
+#         write_obj.write(cur_line)
+#
+#     all_args_file = f'{res_root}/all_args_{args.dname}_noise_{args.feature_noise}.csv'
+#     with open(all_args_file, 'a+') as f:
+#         f.write(str(args))
+#         f.write('\n')
+#
+#     print('All done! Exit python code')
+#     quit()
```

## easygraph/model/hypergraphs/hgnn.py

 * *Ordering differences only*

```diff
@@ -1,53 +1,53 @@
-import torch
-import torch.nn as nn
-
-from easygraph.nn import HGNNConv
-
-
-class HGNN(nn.Module):
-    r"""The HGNN model proposed in `Hypergraph Neural Networks <https://arxiv.org/pdf/1809.09401>`_ paper (AAAI 2019).
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
-        ``num_classes`` (``int``): The Number of class of the classification task.
-        ``use_bn`` (``bool``): If set to ``True``, use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``, optional): Dropout ratio. Defaults to 0.5.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        hid_channels: int,
-        num_classes: int,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-    ) -> None:
-        super().__init__()
-        self.layers = nn.ModuleList()
-        self.layers.append(
-            HGNNConv(in_channels, hid_channels, use_bn=use_bn, drop_rate=drop_rate)
-        )
-        self.layers.append(
-            HGNNConv(hid_channels, num_classes, use_bn=use_bn, is_last=True)
-        )
-
-    def forward(self, X: torch.Tensor, hg: "Hypergraph") -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
-            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
-        """
-        # import time
-        # start = time.time()
-        # X = self.laplacian @ self.Theta1(self.dropout(X))
-        # end = time.time()
-        # # print("lal:",end-start)
-        # X = self.act(X)
-        # X = self.laplacian @ self.Theta2(X)
-        # return X
-
-        for layer in self.layers:
-            X = layer(X, hg)
-        return X
+import torch
+import torch.nn as nn
+
+from easygraph.nn import HGNNConv
+
+
+class HGNN(nn.Module):
+    r"""The HGNN model proposed in `Hypergraph Neural Networks <https://arxiv.org/pdf/1809.09401>`_ paper (AAAI 2019).
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
+        ``num_classes`` (``int``): The Number of class of the classification task.
+        ``use_bn`` (``bool``): If set to ``True``, use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``, optional): Dropout ratio. Defaults to 0.5.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        hid_channels: int,
+        num_classes: int,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+    ) -> None:
+        super().__init__()
+        self.layers = nn.ModuleList()
+        self.layers.append(
+            HGNNConv(in_channels, hid_channels, use_bn=use_bn, drop_rate=drop_rate)
+        )
+        self.layers.append(
+            HGNNConv(hid_channels, num_classes, use_bn=use_bn, is_last=True)
+        )
+
+    def forward(self, X: torch.Tensor, hg: "Hypergraph") -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
+            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
+        """
+        # import time
+        # start = time.time()
+        # X = self.laplacian @ self.Theta1(self.dropout(X))
+        # end = time.time()
+        # # print("lal:",end-start)
+        # X = self.act(X)
+        # X = self.laplacian @ self.Theta2(X)
+        # return X
+
+        for layer in self.layers:
+            X = layer(X, hg)
+        return X
```

## easygraph/model/hypergraphs/hypergcn.py

 * *Ordering differences only*

```diff
@@ -1,67 +1,67 @@
-import torch
-import torch.nn as nn
-
-from easygraph.classes import Graph
-from easygraph.nn import HyperGCNConv
-
-
-class HyperGCN(nn.Module):
-    r"""The HyperGCN model proposed in `HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs <https://papers.nips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf>`_ paper (NeurIPS 2019).
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
-        ``num_classes`` (``int``): The Number of class of the classification task.
-        ``use_mediator`` (``str``): Whether to use mediator to transform the hyperedges to edges in the graph. Defaults to ``False``.
-        ``fast`` (``bool``): If set to ``True``, the transformed graph structure will be computed once from the input hypergraph and vertex features, and cached for future use. Defaults to ``True``.
-        ``drop_rate`` (``float``, optional): Dropout ratio. Defaults to 0.5.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        hid_channels: int,
-        num_classes: int,
-        use_mediator: bool = False,
-        use_bn: bool = False,
-        fast: bool = True,
-        drop_rate: float = 0.5,
-    ) -> None:
-        super().__init__()
-        self.fast = fast
-        self.cached_g = None
-        self.with_mediator = use_mediator
-        self.layers = nn.ModuleList()
-        self.layers.append(
-            HyperGCNConv(
-                in_channels,
-                hid_channels,
-                use_mediator,
-                use_bn=use_bn,
-                drop_rate=drop_rate,
-            )
-        )
-        self.layers.append(
-            HyperGCNConv(
-                hid_channels, num_classes, use_mediator, use_bn=use_bn, is_last=True
-            )
-        )
-
-    def forward(self, X: torch.Tensor, hg: "eg.Hypergraph") -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
-            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
-        """
-        if self.fast:
-            if self.cached_g is None:
-                self.cached_g = Graph.from_hypergraph_hypergcn(
-                    hg, X, self.with_mediator
-                )
-            for layer in self.layers:
-                X = layer(X, hg, self.cached_g)
-        else:
-            for layer in self.layers:
-                X = layer(X, hg)
-        return X
+import torch
+import torch.nn as nn
+
+from easygraph.classes import Graph
+from easygraph.nn import HyperGCNConv
+
+
+class HyperGCN(nn.Module):
+    r"""The HyperGCN model proposed in `HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs <https://papers.nips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf>`_ paper (NeurIPS 2019).
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
+        ``num_classes`` (``int``): The Number of class of the classification task.
+        ``use_mediator`` (``str``): Whether to use mediator to transform the hyperedges to edges in the graph. Defaults to ``False``.
+        ``fast`` (``bool``): If set to ``True``, the transformed graph structure will be computed once from the input hypergraph and vertex features, and cached for future use. Defaults to ``True``.
+        ``drop_rate`` (``float``, optional): Dropout ratio. Defaults to 0.5.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        hid_channels: int,
+        num_classes: int,
+        use_mediator: bool = False,
+        use_bn: bool = False,
+        fast: bool = True,
+        drop_rate: float = 0.5,
+    ) -> None:
+        super().__init__()
+        self.fast = fast
+        self.cached_g = None
+        self.with_mediator = use_mediator
+        self.layers = nn.ModuleList()
+        self.layers.append(
+            HyperGCNConv(
+                in_channels,
+                hid_channels,
+                use_mediator,
+                use_bn=use_bn,
+                drop_rate=drop_rate,
+            )
+        )
+        self.layers.append(
+            HyperGCNConv(
+                hid_channels, num_classes, use_mediator, use_bn=use_bn, is_last=True
+            )
+        )
+
+    def forward(self, X: torch.Tensor, hg: "eg.Hypergraph") -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
+            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
+        """
+        if self.fast:
+            if self.cached_g is None:
+                self.cached_g = Graph.from_hypergraph_hypergcn(
+                    hg, X, self.with_mediator
+                )
+            for layer in self.layers:
+                X = layer(X, hg, self.cached_g)
+        else:
+            for layer in self.layers:
+                X = layer(X, hg)
+        return X
```

## easygraph/model/hypergraphs/dhne.py

 * *Ordering differences only*

```diff
@@ -1,70 +1,70 @@
-import torch
-import torch.nn as nn
-
-
-class DHNE(nn.Module):
-    r"""The DHNE model proposed in `Structural Deep Embedding for Hyper-Networks <https://arxiv.org/abs/1711.10146>`_ paper (AAAI 2018).
-
-    Parameters:
-        ``dim_feature`` (``int``): : feature dimension list ( len = 3)
-        ``embedding_size`` (``int``): :The embedding dimension size
-        ``hidden_size`` (``int``): The hidden full connected layer size.
-
-    """
-
-    def __init__(self, dim_feature, embedding_size, hidden_size):
-        super(DHNE, self).__init__()
-        self.dim_feature = dim_feature
-        self.embedding_size = embedding_size
-        self.hidden_size = hidden_size
-        self.encode0 = nn.Sequential(
-            nn.Linear(
-                in_features=self.dim_feature[0], out_features=self.embedding_size[0]
-            )
-        )
-        self.encode1 = nn.Sequential(
-            nn.Linear(
-                in_features=self.dim_feature[1], out_features=self.embedding_size[1]
-            )
-        )
-        self.encode2 = nn.Sequential(
-            nn.Linear(
-                in_features=self.dim_feature[2], out_features=self.embedding_size[2]
-            )
-        )
-        self.decode_layer0 = nn.Linear(
-            in_features=self.embedding_size[0], out_features=self.dim_feature[0]
-        )
-        self.decode_layer1 = nn.Linear(
-            in_features=self.embedding_size[1], out_features=self.dim_feature[1]
-        )
-        self.decode_layer2 = nn.Linear(
-            in_features=self.embedding_size[2], out_features=self.dim_feature[2]
-        )
-
-        self.hidden_layer = nn.Linear(
-            in_features=sum(self.embedding_size), out_features=self.hidden_size
-        )
-        self.ouput_layer = nn.Linear(in_features=self.hidden_size, out_features=1)
-
-    def forward(self, input0, input1, input2):
-        input0 = self.encode0(input0)
-        input0 = torch.tanh(input0)
-        decode0 = self.decode_layer0(input0)
-        decode0 = torch.sigmoid(decode0)
-
-        input1 = self.encode1(input1)
-        input1 = torch.tanh(input1)
-        decode1 = self.decode_layer1(input1)
-        decode1 = torch.sigmoid(decode1)
-
-        input2 = self.encode2(input2)
-        input2 = torch.tanh(input2)
-        decode2 = self.decode_layer2(input2)
-        decode2 = torch.sigmoid(decode2)
-
-        merged = torch.tanh(torch.cat((input0, input1, input2), dim=1))
-        merged = self.hidden_layer(merged)
-        merged = self.ouput_layer(merged)
-        merged = torch.sigmoid(merged)
-        return [decode0, decode1, decode2, merged]
+import torch
+import torch.nn as nn
+
+
+class DHNE(nn.Module):
+    r"""The DHNE model proposed in `Structural Deep Embedding for Hyper-Networks <https://arxiv.org/abs/1711.10146>`_ paper (AAAI 2018).
+
+    Parameters:
+        ``dim_feature`` (``int``): : feature dimension list ( len = 3)
+        ``embedding_size`` (``int``): :The embedding dimension size
+        ``hidden_size`` (``int``): The hidden full connected layer size.
+
+    """
+
+    def __init__(self, dim_feature, embedding_size, hidden_size):
+        super(DHNE, self).__init__()
+        self.dim_feature = dim_feature
+        self.embedding_size = embedding_size
+        self.hidden_size = hidden_size
+        self.encode0 = nn.Sequential(
+            nn.Linear(
+                in_features=self.dim_feature[0], out_features=self.embedding_size[0]
+            )
+        )
+        self.encode1 = nn.Sequential(
+            nn.Linear(
+                in_features=self.dim_feature[1], out_features=self.embedding_size[1]
+            )
+        )
+        self.encode2 = nn.Sequential(
+            nn.Linear(
+                in_features=self.dim_feature[2], out_features=self.embedding_size[2]
+            )
+        )
+        self.decode_layer0 = nn.Linear(
+            in_features=self.embedding_size[0], out_features=self.dim_feature[0]
+        )
+        self.decode_layer1 = nn.Linear(
+            in_features=self.embedding_size[1], out_features=self.dim_feature[1]
+        )
+        self.decode_layer2 = nn.Linear(
+            in_features=self.embedding_size[2], out_features=self.dim_feature[2]
+        )
+
+        self.hidden_layer = nn.Linear(
+            in_features=sum(self.embedding_size), out_features=self.hidden_size
+        )
+        self.ouput_layer = nn.Linear(in_features=self.hidden_size, out_features=1)
+
+    def forward(self, input0, input1, input2):
+        input0 = self.encode0(input0)
+        input0 = torch.tanh(input0)
+        decode0 = self.decode_layer0(input0)
+        decode0 = torch.sigmoid(decode0)
+
+        input1 = self.encode1(input1)
+        input1 = torch.tanh(input1)
+        decode1 = self.decode_layer1(input1)
+        decode1 = torch.sigmoid(decode1)
+
+        input2 = self.encode2(input2)
+        input2 = torch.tanh(input2)
+        decode2 = self.decode_layer2(input2)
+        decode2 = torch.sigmoid(decode2)
+
+        merged = torch.tanh(torch.cat((input0, input1, input2), dim=1))
+        merged = self.hidden_layer(merged)
+        merged = self.ouput_layer(merged)
+        merged = torch.sigmoid(merged)
+        return [decode0, decode1, decode2, merged]
```

## easygraph/model/hypergraphs/dhcf.py

 * *Ordering differences only*

```diff
@@ -1,95 +1,95 @@
-from typing import Tuple
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-from easygraph.classes import Hypergraph
-
-
-class DHCF(nn.Module):
-    r"""The DHCF model proposed in `Dual Channel Hypergraph Collaborative Filtering <https://dl.acm.org/doi/10.1145/3394486.3403253>`_ paper (KDD 2020).
-
-    .. note::
-
-        The user and item embeddings and trainable parameters are initialized with xavier_uniform distribution.
-
-    Parameters:
-        ``num_users`` (``int``): The Number of users.
-        ``num_items`` (``int``): The Number of items.
-        ``emb_dim`` (``int``): Embedding dimension.
-        ``num_layers`` (``int``): The Number of layers. Defaults to ``3``.
-        ``drop_rate`` (``float``): The dropout probability. Defaults to ``0.5``.
-    """
-
-    def __init__(
-        self,
-        num_users: int,
-        num_items: int,
-        emb_dim: int,
-        num_layers: int = 3,
-        drop_rate: float = 0.5,
-    ) -> None:
-        super().__init__()
-        self.num_users, self.num_items = num_users, num_items
-        self.num_layers = num_layers
-        self.drop_rate = drop_rate
-        self.u_embedding = nn.Embedding(num_users, emb_dim)
-        self.i_embedding = nn.Embedding(num_items, emb_dim)
-        self.W_gc, self.W_bi = nn.ModuleList(), nn.ModuleList()
-        for _ in range(self.num_layers):
-            self.W_gc.append(nn.Linear(emb_dim, emb_dim))
-            self.W_bi.append(nn.Linear(emb_dim, emb_dim))
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        r"""Initialize learnable parameters."""
-        nn.init.xavier_uniform_(self.u_embedding.weight)
-        nn.init.xavier_uniform_(self.i_embedding.weight)
-        for W_gc, W_bi in zip(self.W_gc, self.W_bi):
-            nn.init.xavier_uniform_(W_gc.weight)
-            nn.init.xavier_uniform_(W_bi.weight)
-            nn.init.constant_(W_gc.bias, 0)
-            nn.init.constant_(W_bi.bias, 0)
-
-    def forward(
-        self, hg_ui: Hypergraph, hg_iu: Hypergraph
-    ) -> Tuple[torch.Tensor, torch.Tensor]:
-        r"""The forward function.
-
-        Parameters:
-            ``hg_ui`` (``eg.Hypergraph``): The hypergraph structure that users as vertices.
-            ``hg_iu`` (``eg.Hypergraph``): The hypergraph structure that items as vertices.
-        """
-        u_embs = self.u_embedding.weight
-        i_embs = self.i_embedding.weight
-        all_embs = torch.cat([u_embs, i_embs], dim=0)
-
-        embs_list = [all_embs]
-        for _idx in range(self.num_layers):
-            u_embs, i_embs = torch.split(
-                all_embs, [self.num_users, self.num_items], dim=0
-            )
-            # ==========================================================================================
-            # Two JHConv Layers for users and items, respectively.
-            u_embs = hg_ui.smoothing_with_HGNN(u_embs)
-            i_embs = hg_iu.smoothing_with_HGNN(i_embs)
-            g_embs = torch.cat([u_embs, i_embs], dim=0)
-            sum_embs = F.leaky_relu(
-                self.W_gc[_idx](g_embs) + g_embs, negative_slope=0.2
-            )
-            # ==========================================================================================
-
-            bi_embs = all_embs * g_embs
-            bi_embs = F.leaky_relu(self.W_bi[_idx](bi_embs), negative_slope=0.2)
-
-            all_embs = sum_embs + bi_embs
-            all_embs = F.dropout(all_embs, p=self.drop_rate, training=self.training)
-            all_embs = F.normalize(all_embs, p=2, dim=1)
-
-            embs_list.append(all_embs)
-        embs = torch.stack(embs_list, dim=1)
-        embs = torch.mean(embs, dim=1)
-
-        u_embs, i_embs = torch.split(embs, [self.num_users, self.num_items], dim=0)
-        return u_embs, i_embs
+from typing import Tuple
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+from easygraph.classes import Hypergraph
+
+
+class DHCF(nn.Module):
+    r"""The DHCF model proposed in `Dual Channel Hypergraph Collaborative Filtering <https://dl.acm.org/doi/10.1145/3394486.3403253>`_ paper (KDD 2020).
+
+    .. note::
+
+        The user and item embeddings and trainable parameters are initialized with xavier_uniform distribution.
+
+    Parameters:
+        ``num_users`` (``int``): The Number of users.
+        ``num_items`` (``int``): The Number of items.
+        ``emb_dim`` (``int``): Embedding dimension.
+        ``num_layers`` (``int``): The Number of layers. Defaults to ``3``.
+        ``drop_rate`` (``float``): The dropout probability. Defaults to ``0.5``.
+    """
+
+    def __init__(
+        self,
+        num_users: int,
+        num_items: int,
+        emb_dim: int,
+        num_layers: int = 3,
+        drop_rate: float = 0.5,
+    ) -> None:
+        super().__init__()
+        self.num_users, self.num_items = num_users, num_items
+        self.num_layers = num_layers
+        self.drop_rate = drop_rate
+        self.u_embedding = nn.Embedding(num_users, emb_dim)
+        self.i_embedding = nn.Embedding(num_items, emb_dim)
+        self.W_gc, self.W_bi = nn.ModuleList(), nn.ModuleList()
+        for _ in range(self.num_layers):
+            self.W_gc.append(nn.Linear(emb_dim, emb_dim))
+            self.W_bi.append(nn.Linear(emb_dim, emb_dim))
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        r"""Initialize learnable parameters."""
+        nn.init.xavier_uniform_(self.u_embedding.weight)
+        nn.init.xavier_uniform_(self.i_embedding.weight)
+        for W_gc, W_bi in zip(self.W_gc, self.W_bi):
+            nn.init.xavier_uniform_(W_gc.weight)
+            nn.init.xavier_uniform_(W_bi.weight)
+            nn.init.constant_(W_gc.bias, 0)
+            nn.init.constant_(W_bi.bias, 0)
+
+    def forward(
+        self, hg_ui: Hypergraph, hg_iu: Hypergraph
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        r"""The forward function.
+
+        Parameters:
+            ``hg_ui`` (``eg.Hypergraph``): The hypergraph structure that users as vertices.
+            ``hg_iu`` (``eg.Hypergraph``): The hypergraph structure that items as vertices.
+        """
+        u_embs = self.u_embedding.weight
+        i_embs = self.i_embedding.weight
+        all_embs = torch.cat([u_embs, i_embs], dim=0)
+
+        embs_list = [all_embs]
+        for _idx in range(self.num_layers):
+            u_embs, i_embs = torch.split(
+                all_embs, [self.num_users, self.num_items], dim=0
+            )
+            # ==========================================================================================
+            # Two JHConv Layers for users and items, respectively.
+            u_embs = hg_ui.smoothing_with_HGNN(u_embs)
+            i_embs = hg_iu.smoothing_with_HGNN(i_embs)
+            g_embs = torch.cat([u_embs, i_embs], dim=0)
+            sum_embs = F.leaky_relu(
+                self.W_gc[_idx](g_embs) + g_embs, negative_slope=0.2
+            )
+            # ==========================================================================================
+
+            bi_embs = all_embs * g_embs
+            bi_embs = F.leaky_relu(self.W_bi[_idx](bi_embs), negative_slope=0.2)
+
+            all_embs = sum_embs + bi_embs
+            all_embs = F.dropout(all_embs, p=self.drop_rate, training=self.training)
+            all_embs = F.normalize(all_embs, p=2, dim=1)
+
+            embs_list.append(all_embs)
+        embs = torch.stack(embs_list, dim=1)
+        embs = torch.mean(embs, dim=1)
+
+        u_embs, i_embs = torch.split(embs, [self.num_users, self.num_items], dim=0)
+        return u_embs, i_embs
```

## easygraph/model/hypergraphs/unignn.py

 * *Ordering differences only*

```diff
@@ -1,214 +1,214 @@
-import torch
-import torch.nn as nn
-
-from easygraph.nn import MultiHeadWrapper
-from easygraph.nn import UniGATConv
-from easygraph.nn import UniGCNConv
-from easygraph.nn import UniGINConv
-from easygraph.nn import UniSAGEConv
-
-
-__all__ = [
-    "UniGCN",
-    "UniGAT",
-    "UniSAGE",
-    "UniGIN",
-]
-
-
-class UniGCN(nn.Module):
-    r"""The UniGCN model proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
-        ``num_classes`` (``int``): The Number of class of the classification task.
-        ``use_bn`` (``bool``): If set to ``True``, use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``, optional): Dropout ratio. Defaults to ``0.5``.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        hid_channels: int,
-        num_classes: int,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-    ) -> None:
-        super().__init__()
-        self.layers = nn.ModuleList()
-        self.layers.append(
-            UniGCNConv(in_channels, hid_channels, use_bn=use_bn, drop_rate=drop_rate)
-        )
-        self.layers.append(
-            UniGCNConv(hid_channels, num_classes, use_bn=use_bn, is_last=True)
-        )
-
-    def forward(self, X: torch.Tensor, hg: "eg.Hypergraph") -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
-            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
-        """
-        for layer in self.layers:
-            X = layer(X, hg)
-        return X
-
-
-class UniGAT(nn.Module):
-    r"""The UniGAT model proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
-        ``num_classes`` (``int``): The Number of class of the classification task.
-        ``num_heads`` (``int``): The Number of attention head in each layer.
-        ``use_bn`` (``bool``): If set to ``True``, use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``): The dropout probability. Defaults to ``0.5``.
-        ``atten_neg_slope`` (``float``): Hyper-parameter of the ``LeakyReLU`` activation of edge attention. Defaults to 0.2.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        hid_channels: int,
-        num_classes: int,
-        num_heads: int,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-        atten_neg_slope: float = 0.2,
-    ) -> None:
-        super().__init__()
-        self.drop_layer = nn.Dropout(drop_rate)
-        self.multi_head_layer = MultiHeadWrapper(
-            num_heads,
-            "concat",
-            UniGATConv,
-            in_channels=in_channels,
-            out_channels=hid_channels,
-            use_bn=use_bn,
-            drop_rate=drop_rate,
-            atten_neg_slope=atten_neg_slope,
-        )
-        # The original implementation has applied activation layer after the final layer.
-        # Thus, we donot set ``is_last`` to ``True``.
-        self.out_layer = UniGATConv(
-            hid_channels * num_heads,
-            num_classes,
-            use_bn=use_bn,
-            drop_rate=drop_rate,
-            atten_neg_slope=atten_neg_slope,
-            is_last=False,
-        )
-
-    def forward(self, X: torch.Tensor, hg: "eg.Hypergraph") -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
-            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
-        """
-        X = self.drop_layer(X)
-        X = self.multi_head_layer(X=X, hg=hg)
-        X = self.drop_layer(X)
-        X = self.out_layer(X, hg)
-        return X
-
-
-class UniSAGE(nn.Module):
-    r"""The UniSAGE model proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
-        ``num_classes`` (``int``): The Number of class of the classification task.
-        ``use_bn`` (``bool``): If set to ``True``, use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``, optional): Dropout ratio. Defaults to ``0.5``.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        hid_channels: int,
-        num_classes: int,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-    ) -> None:
-        super().__init__()
-        self.layers = nn.ModuleList()
-        self.layers.append(
-            UniSAGEConv(in_channels, hid_channels, use_bn=use_bn, drop_rate=drop_rate)
-        )
-        self.layers.append(
-            UniSAGEConv(hid_channels, num_classes, use_bn=use_bn, is_last=True)
-        )
-
-    def forward(self, X: torch.Tensor, hg: "eg.Hypergraph") -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
-            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
-        """
-        for layer in self.layers:
-            X = layer(X, hg)
-        return X
-
-
-class UniGIN(nn.Module):
-    r"""The UniGIN model proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
-        ``num_classes`` (``int``): The Number of class of the classification task.
-        ``eps`` (``float``): The epsilon value. Defaults to ``0.0``.
-        ``train_eps`` (``bool``): If set to ``True``, the epsilon value will be trainable. Defaults to ``False``.
-        ``use_bn`` (``bool``): If set to ``True``, use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``, optional): Dropout ratio. Defaults to ``0.5``.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        hid_channels: int,
-        num_classes: int,
-        eps: float = 0.0,
-        train_eps: bool = False,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-    ) -> None:
-        super().__init__()
-        self.layers = nn.ModuleList()
-        self.layers.append(
-            UniGINConv(
-                in_channels,
-                hid_channels,
-                eps=eps,
-                train_eps=train_eps,
-                use_bn=use_bn,
-                drop_rate=drop_rate,
-            )
-        )
-        self.layers.append(
-            UniGINConv(
-                hid_channels,
-                num_classes,
-                eps=eps,
-                train_eps=train_eps,
-                use_bn=use_bn,
-                is_last=True,
-            )
-        )
-
-    def forward(self, X: torch.Tensor, hg: "eg.Hypergraph") -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
-            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
-        """
-        for layer in self.layers:
-            X = layer(X, hg)
-        return X
+import torch
+import torch.nn as nn
+
+from easygraph.nn import MultiHeadWrapper
+from easygraph.nn import UniGATConv
+from easygraph.nn import UniGCNConv
+from easygraph.nn import UniGINConv
+from easygraph.nn import UniSAGEConv
+
+
+__all__ = [
+    "UniGCN",
+    "UniGAT",
+    "UniSAGE",
+    "UniGIN",
+]
+
+
+class UniGCN(nn.Module):
+    r"""The UniGCN model proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
+        ``num_classes`` (``int``): The Number of class of the classification task.
+        ``use_bn`` (``bool``): If set to ``True``, use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``, optional): Dropout ratio. Defaults to ``0.5``.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        hid_channels: int,
+        num_classes: int,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+    ) -> None:
+        super().__init__()
+        self.layers = nn.ModuleList()
+        self.layers.append(
+            UniGCNConv(in_channels, hid_channels, use_bn=use_bn, drop_rate=drop_rate)
+        )
+        self.layers.append(
+            UniGCNConv(hid_channels, num_classes, use_bn=use_bn, is_last=True)
+        )
+
+    def forward(self, X: torch.Tensor, hg: "eg.Hypergraph") -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
+            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
+        """
+        for layer in self.layers:
+            X = layer(X, hg)
+        return X
+
+
+class UniGAT(nn.Module):
+    r"""The UniGAT model proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
+        ``num_classes`` (``int``): The Number of class of the classification task.
+        ``num_heads`` (``int``): The Number of attention head in each layer.
+        ``use_bn`` (``bool``): If set to ``True``, use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``): The dropout probability. Defaults to ``0.5``.
+        ``atten_neg_slope`` (``float``): Hyper-parameter of the ``LeakyReLU`` activation of edge attention. Defaults to 0.2.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        hid_channels: int,
+        num_classes: int,
+        num_heads: int,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+        atten_neg_slope: float = 0.2,
+    ) -> None:
+        super().__init__()
+        self.drop_layer = nn.Dropout(drop_rate)
+        self.multi_head_layer = MultiHeadWrapper(
+            num_heads,
+            "concat",
+            UniGATConv,
+            in_channels=in_channels,
+            out_channels=hid_channels,
+            use_bn=use_bn,
+            drop_rate=drop_rate,
+            atten_neg_slope=atten_neg_slope,
+        )
+        # The original implementation has applied activation layer after the final layer.
+        # Thus, we donot set ``is_last`` to ``True``.
+        self.out_layer = UniGATConv(
+            hid_channels * num_heads,
+            num_classes,
+            use_bn=use_bn,
+            drop_rate=drop_rate,
+            atten_neg_slope=atten_neg_slope,
+            is_last=False,
+        )
+
+    def forward(self, X: torch.Tensor, hg: "eg.Hypergraph") -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
+            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
+        """
+        X = self.drop_layer(X)
+        X = self.multi_head_layer(X=X, hg=hg)
+        X = self.drop_layer(X)
+        X = self.out_layer(X, hg)
+        return X
+
+
+class UniSAGE(nn.Module):
+    r"""The UniSAGE model proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
+        ``num_classes`` (``int``): The Number of class of the classification task.
+        ``use_bn`` (``bool``): If set to ``True``, use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``, optional): Dropout ratio. Defaults to ``0.5``.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        hid_channels: int,
+        num_classes: int,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+    ) -> None:
+        super().__init__()
+        self.layers = nn.ModuleList()
+        self.layers.append(
+            UniSAGEConv(in_channels, hid_channels, use_bn=use_bn, drop_rate=drop_rate)
+        )
+        self.layers.append(
+            UniSAGEConv(hid_channels, num_classes, use_bn=use_bn, is_last=True)
+        )
+
+    def forward(self, X: torch.Tensor, hg: "eg.Hypergraph") -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
+            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
+        """
+        for layer in self.layers:
+            X = layer(X, hg)
+        return X
+
+
+class UniGIN(nn.Module):
+    r"""The UniGIN model proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
+        ``num_classes`` (``int``): The Number of class of the classification task.
+        ``eps`` (``float``): The epsilon value. Defaults to ``0.0``.
+        ``train_eps`` (``bool``): If set to ``True``, the epsilon value will be trainable. Defaults to ``False``.
+        ``use_bn`` (``bool``): If set to ``True``, use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``, optional): Dropout ratio. Defaults to ``0.5``.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        hid_channels: int,
+        num_classes: int,
+        eps: float = 0.0,
+        train_eps: bool = False,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+    ) -> None:
+        super().__init__()
+        self.layers = nn.ModuleList()
+        self.layers.append(
+            UniGINConv(
+                in_channels,
+                hid_channels,
+                eps=eps,
+                train_eps=train_eps,
+                use_bn=use_bn,
+                drop_rate=drop_rate,
+            )
+        )
+        self.layers.append(
+            UniGINConv(
+                hid_channels,
+                num_classes,
+                eps=eps,
+                train_eps=train_eps,
+                use_bn=use_bn,
+                is_last=True,
+            )
+        )
+
+    def forward(self, X: torch.Tensor, hg: "eg.Hypergraph") -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
+            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
+        """
+        for layer in self.layers:
+            X = layer(X, hg)
+        return X
```

## easygraph/model/hypergraphs/__init__.py

 * *Ordering differences only*

```diff
@@ -1,10 +1,10 @@
-from .dhcf import DHCF
-from .hgnn import HGNN
-from .hgnnp import HGNNP
-from .hnhn import HNHN
-from .hypergcn import HyperGCN
-from .setgnn import SetGNN
-from .unignn import UniGAT
-from .unignn import UniGCN
-from .unignn import UniGIN
-from .unignn import UniSAGE
+from .dhcf import DHCF
+from .hgnn import HGNN
+from .hgnnp import HGNNP
+from .hnhn import HNHN
+from .hypergcn import HyperGCN
+from .setgnn import SetGNN
+from .unignn import UniGAT
+from .unignn import UniGCN
+from .unignn import UniGIN
+from .unignn import UniSAGE
```

## easygraph/model/hypergraphs/hgnnp.py

 * *Ordering differences only*

```diff
@@ -1,44 +1,44 @@
-import torch
-import torch.nn as nn
-
-from easygraph.nn import HGNNPConv
-
-
-class HGNNP(nn.Module):
-    r"""The HGNN :sup:`+` model proposed in `HGNN+: General Hypergraph Neural Networks <https://ieeexplore.ieee.org/document/9795251>`_ paper (IEEE T-PAMI 2022).
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
-        ``num_classes`` (``int``): The Number of class of the classification task.
-        ``use_bn`` (``bool``): If set to ``True``, use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``, optional): Dropout ratio. Defaults to ``0.5``.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        hid_channels: int,
-        num_classes: int,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-    ) -> None:
-        super().__init__()
-        self.layers = nn.ModuleList()
-        self.layers.append(
-            HGNNPConv(in_channels, hid_channels, use_bn=use_bn, drop_rate=drop_rate)
-        )
-        self.layers.append(
-            HGNNPConv(hid_channels, num_classes, use_bn=use_bn, is_last=True)
-        )
-
-    def forward(self, X: torch.Tensor, hg: "eg.Hypergraph") -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
-            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
-        """
-        for layer in self.layers:
-            X = layer(X, hg)
-        return X
+import torch
+import torch.nn as nn
+
+from easygraph.nn import HGNNPConv
+
+
+class HGNNP(nn.Module):
+    r"""The HGNN :sup:`+` model proposed in `HGNN+: General Hypergraph Neural Networks <https://ieeexplore.ieee.org/document/9795251>`_ paper (IEEE T-PAMI 2022).
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
+        ``num_classes`` (``int``): The Number of class of the classification task.
+        ``use_bn`` (``bool``): If set to ``True``, use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``, optional): Dropout ratio. Defaults to ``0.5``.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        hid_channels: int,
+        num_classes: int,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+    ) -> None:
+        super().__init__()
+        self.layers = nn.ModuleList()
+        self.layers.append(
+            HGNNPConv(in_channels, hid_channels, use_bn=use_bn, drop_rate=drop_rate)
+        )
+        self.layers.append(
+            HGNNPConv(hid_channels, num_classes, use_bn=use_bn, is_last=True)
+        )
+
+    def forward(self, X: torch.Tensor, hg: "eg.Hypergraph") -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
+            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
+        """
+        for layer in self.layers:
+            X = layer(X, hg)
+        return X
```

## easygraph/model/hypergraphs/hnhn.py

 * *Ordering differences only*

```diff
@@ -1,44 +1,44 @@
-import torch
-import torch.nn as nn
-
-from easygraph.nn import HNHNConv
-
-
-class HNHN(nn.Module):
-    r"""The HNHN model proposed in `HNHN: Hypergraph Networks with Hyperedge Neurons <https://arxiv.org/pdf/2006.12278.pdf>`_ paper (ICML 2020).
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
-        ``num_classes`` (``int``): The Number of class of the classification task.
-        ``use_bn`` (``bool``): If set to ``True``, use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``, optional): Dropout ratio. Defaults to ``0.5``.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        hid_channels: int,
-        num_classes: int,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-    ) -> None:
-        super().__init__()
-        self.layers = nn.ModuleList()
-        self.layers.append(
-            HNHNConv(in_channels, hid_channels, use_bn=use_bn, drop_rate=drop_rate)
-        )
-        self.layers.append(
-            HNHNConv(hid_channels, num_classes, use_bn=use_bn, is_last=True)
-        )
-
-    def forward(self, X: torch.Tensor, hg: "eg.Hypergraph") -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
-            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
-        """
-        for layer in self.layers:
-            X = layer(X, hg)
-        return X
+import torch
+import torch.nn as nn
+
+from easygraph.nn import HNHNConv
+
+
+class HNHN(nn.Module):
+    r"""The HNHN model proposed in `HNHN: Hypergraph Networks with Hyperedge Neurons <https://arxiv.org/pdf/2006.12278.pdf>`_ paper (ICML 2020).
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``hid_channels`` (``int``): :math:`C_{hid}` is the number of hidden channels.
+        ``num_classes`` (``int``): The Number of class of the classification task.
+        ``use_bn`` (``bool``): If set to ``True``, use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``, optional): Dropout ratio. Defaults to ``0.5``.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        hid_channels: int,
+        num_classes: int,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+    ) -> None:
+        super().__init__()
+        self.layers = nn.ModuleList()
+        self.layers.append(
+            HNHNConv(in_channels, hid_channels, use_bn=use_bn, drop_rate=drop_rate)
+        )
+        self.layers.append(
+            HNHNConv(hid_channels, num_classes, use_bn=use_bn, is_last=True)
+        )
+
+    def forward(self, X: torch.Tensor, hg: "eg.Hypergraph") -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
+            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
+        """
+        for layer in self.layers:
+            X = layer(X, hg)
+        return X
```

## easygraph/tests/test_convert.py

 * *Ordering differences only*

```diff
@@ -1,99 +1,99 @@
-import sys
-
-import pytest
-
-
-np = pytest.importorskip("numpy")
-pd = pytest.importorskip("pandas")
-sp = pytest.importorskip("scipy")
-
-import easygraph as eg
-
-from easygraph.utils.misc import *
-
-
-class TestConvertNumpyArray:
-    def setup_method(self):
-        self.G1 = eg.complete_graph(5)
-
-    def assert_equal(self, G1, G2):
-        assert nodes_equal(G1.nodes, G2.nodes)
-        assert edges_equal(G1.edges, G2.edges, need_data=False)
-
-    def identity_conversion(self, G, A, create_using):
-        assert A.sum() > 0
-        GG = eg.from_numpy_array(A, create_using=create_using)
-        self.assert_equal(G, GG)
-        GW = eg.to_easygraph_graph(A, create_using=create_using)
-        self.assert_equal(G, GW)
-
-    def test_identity_graph_array(self):
-        "Conversion from graph to array to graph."
-        A = eg.to_numpy_array(self.G1)
-        self.identity_conversion(self.G1, A, eg.Graph())
-
-
-class TestConvertPandas:
-    def setup_method(self):
-        self.rng = np.random.RandomState(seed=5)
-        ints = self.rng.randint(1, 11, size=(3, 2))
-        a = ["A", "B", "C"]
-        b = ["D", "A", "E"]
-        df = pd.DataFrame(ints, columns=["weight", "cost"])
-        df[0] = a  # Column label 0 (int)
-        df["b"] = b  # Column label 'b' (str)
-        self.df = df
-
-        mdf = pd.DataFrame([[4, 16, "A", "D"]], columns=["weight", "cost", 0, "b"])
-        # self.mdf = df.append(mdf)
-        self.mdf = pd.concat([df, mdf])
-
-    def assert_equal(self, G1, G2):
-        assert nodes_equal(G1.nodes, G2.nodes)
-        assert edges_equal(G1.edges, G2.edges, need_data=False)
-
-    def test_from_edgelist_multi_attr(self):
-        Gtrue = eg.Graph(
-            [
-                ("E", "C", {"cost": 9, "weight": 10}),
-                ("B", "A", {"cost": 1, "weight": 7}),
-                ("A", "D", {"cost": 7, "weight": 4}),
-            ]
-        )
-        G = eg.from_pandas_edgelist(self.df, 0, "b", ["weight", "cost"])
-        self.assert_equal(G, Gtrue)
-
-    def test_from_adjacency(self):
-        Gtrue = eg.DiGraph(
-            [
-                ("A", "B"),
-                ("B", "C"),
-            ]
-        )
-        data = {
-            "A": {"A": 0, "B": 0, "C": 0},
-            "B": {"A": 1, "B": 0, "C": 0},
-            "C": {"A": 0, "B": 1, "C": 0},
-        }
-        dftrue = pd.DataFrame(data, dtype=np.intp)
-        df = dftrue[["A", "C", "B"]]
-        G = eg.from_pandas_adjacency(df, create_using=eg.DiGraph())
-        self.assert_equal(G, Gtrue)
-
-
-class TestConvertScipy:
-    def setup_method(self):
-        self.G1 = eg.complete_graph(3)
-
-    def assert_equal(self, G1, G2):
-        assert nodes_equal(G1.nodes, G2.nodes)
-        assert edges_equal(G1.edges, G2.edges, need_data=False)
-
-    # skip if on python < 3.8
-    @pytest.mark.skipif(
-        sys.version_info < (3, 8), reason="requires python3.8 or higher"
-    )
-    def test_from_scipy(self):
-        data = sp.sparse.csr_matrix([[0, 1, 1], [1, 0, 1], [1, 1, 0]])
-        G = eg.from_scipy_sparse_matrix(data)
-        self.assert_equal(self.G1, G)
+import sys
+
+import pytest
+
+
+np = pytest.importorskip("numpy")
+pd = pytest.importorskip("pandas")
+sp = pytest.importorskip("scipy")
+
+import easygraph as eg
+
+from easygraph.utils.misc import *
+
+
+class TestConvertNumpyArray:
+    def setup_method(self):
+        self.G1 = eg.complete_graph(5)
+
+    def assert_equal(self, G1, G2):
+        assert nodes_equal(G1.nodes, G2.nodes)
+        assert edges_equal(G1.edges, G2.edges, need_data=False)
+
+    def identity_conversion(self, G, A, create_using):
+        assert A.sum() > 0
+        GG = eg.from_numpy_array(A, create_using=create_using)
+        self.assert_equal(G, GG)
+        GW = eg.to_easygraph_graph(A, create_using=create_using)
+        self.assert_equal(G, GW)
+
+    def test_identity_graph_array(self):
+        "Conversion from graph to array to graph."
+        A = eg.to_numpy_array(self.G1)
+        self.identity_conversion(self.G1, A, eg.Graph())
+
+
+class TestConvertPandas:
+    def setup_method(self):
+        self.rng = np.random.RandomState(seed=5)
+        ints = self.rng.randint(1, 11, size=(3, 2))
+        a = ["A", "B", "C"]
+        b = ["D", "A", "E"]
+        df = pd.DataFrame(ints, columns=["weight", "cost"])
+        df[0] = a  # Column label 0 (int)
+        df["b"] = b  # Column label 'b' (str)
+        self.df = df
+
+        mdf = pd.DataFrame([[4, 16, "A", "D"]], columns=["weight", "cost", 0, "b"])
+        # self.mdf = df.append(mdf)
+        self.mdf = pd.concat([df, mdf])
+
+    def assert_equal(self, G1, G2):
+        assert nodes_equal(G1.nodes, G2.nodes)
+        assert edges_equal(G1.edges, G2.edges, need_data=False)
+
+    def test_from_edgelist_multi_attr(self):
+        Gtrue = eg.Graph(
+            [
+                ("E", "C", {"cost": 9, "weight": 10}),
+                ("B", "A", {"cost": 1, "weight": 7}),
+                ("A", "D", {"cost": 7, "weight": 4}),
+            ]
+        )
+        G = eg.from_pandas_edgelist(self.df, 0, "b", ["weight", "cost"])
+        self.assert_equal(G, Gtrue)
+
+    def test_from_adjacency(self):
+        Gtrue = eg.DiGraph(
+            [
+                ("A", "B"),
+                ("B", "C"),
+            ]
+        )
+        data = {
+            "A": {"A": 0, "B": 0, "C": 0},
+            "B": {"A": 1, "B": 0, "C": 0},
+            "C": {"A": 0, "B": 1, "C": 0},
+        }
+        dftrue = pd.DataFrame(data, dtype=np.intp)
+        df = dftrue[["A", "C", "B"]]
+        G = eg.from_pandas_adjacency(df, create_using=eg.DiGraph())
+        self.assert_equal(G, Gtrue)
+
+
+class TestConvertScipy:
+    def setup_method(self):
+        self.G1 = eg.complete_graph(3)
+
+    def assert_equal(self, G1, G2):
+        assert nodes_equal(G1.nodes, G2.nodes)
+        assert edges_equal(G1.edges, G2.edges, need_data=False)
+
+    # skip if on python < 3.8
+    @pytest.mark.skipif(
+        sys.version_info < (3, 8), reason="requires python3.8 or higher"
+    )
+    def test_from_scipy(self):
+        data = sp.sparse.csr_matrix([[0, 1, 1], [1, 0, 1], [1, 1, 0]])
+        G = eg.from_scipy_sparse_matrix(data)
+        self.assert_equal(self.G1, G)
```

## easygraph/tests/test_cpp_easygraph.py

 * *Ordering differences only*

```diff
@@ -1,12 +1,12 @@
-#!/usr/bin/env python3
-#
-# from pathlib import Path
-# from subprocess import run
-#
-#
-# script_test_cpp_easygraph_path = Path(__file__).parent / "script_test_cpp_easygraph.py"
-#
-#
-# def test_cpp_easygraph():
-#     p = run(["python3", str(script_test_cpp_easygraph_path)])
-#     assert p.returncode == 0
+#!/usr/bin/env python3
+#
+# from pathlib import Path
+# from subprocess import run
+#
+#
+# script_test_cpp_easygraph_path = Path(__file__).parent / "script_test_cpp_easygraph.py"
+#
+#
+# def test_cpp_easygraph():
+#     p = run(["python3", str(script_test_cpp_easygraph_path)])
+#     assert p.returncode == 0
```

## easygraph/classes/hypergraph.py

 * *Ordering differences only*

```diff
@@ -1,2463 +1,2463 @@
-import pickle
-import random
-
-from copy import deepcopy
-from pathlib import Path
-from typing import TYPE_CHECKING
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Tuple
-from typing import Union
-
-import easygraph as eg
-import numpy as np
-import torch
-
-from easygraph.classes.base import BaseHypergraph
-from easygraph.functions.drawing import draw_hypergraph
-from easygraph.utils.exception import EasyGraphError
-from easygraph.utils.sparse import sparse_dropout
-from scipy.sparse import csr_array
-from scipy.sparse import csr_matrix
-
-
-# from torch_sparse import spmm
-
-
-if TYPE_CHECKING:
-    from easygraph import Graph
-
-__all__ = ["Hypergraph"]
-
-
-class Hypergraph(BaseHypergraph):
-
-    """
-    The ``Hypergraph`` class is developed for hypergraph structures.
-    Please notice that node id in hypergraph is in [0, num_v)
-
-    Parameters
-    ----------
-        num_v  : (int) The number of vertices in the hypergraph
-        e_list : (Union[List[int], List[List[int]]], optional) A list of hyperedges describes how the vertices point to the hyperedges. Defaults to ``None``
-        e_weight : (Union[float, List[float]], optional)  A list of weights for hyperedges. If set to None, the value ``1`` is used for all hyperedges. Defaults to None
-        merge_op : (str) The operation to merge those conflicting hyperedges in the same hyperedge group, which can be ``'mean'``, ``'sum'`` or ``'max'``. Defaults to ``'mean'``
-        device : (torch.device, optional) The device to store the hypergraph. Defaults to torch.device('cpu')
-
-
-    """
-
-    gnn_data_dict_factory = dict
-    degree_data_dict = dict
-
-    def __init__(
-        self,
-        num_v: int,
-        v_property: Optional[Union[Dict, List[Dict]]] = None,
-        e_list: Optional[Union[List[int], List[List[int]]]] = None,
-        e_weight: Optional[Union[float, List[float]]] = None,
-        e_property: Optional[Union[Dict, List[Dict]]] = None,
-        merge_op: str = "mean",
-        device: torch.device = torch.device("cpu"),
-    ):
-        super().__init__(
-            num_v,
-            e_list=e_list,
-            v_property=v_property,
-            e_property=e_property,
-            device=device,
-        )
-
-        self._ndata = self.gnn_data_dict_factory()
-        self.deg_v_dict = self.degree_data_dict()
-        self.n_e_dict = {}
-        self.edge_index = -1
-
-        for i in range(num_v):
-            self.deg_v_dict[i] = 0
-            self.n_e_dict[i] = []
-
-        if e_list is not None:
-            self.add_hyperedges(
-                e_list=e_list,
-                e_weight=e_weight,
-                e_property=e_property,
-                merge_op=merge_op,
-            )
-        edges_col = []
-        indptr_list = []
-        ptr = 0
-        for v in self.n_e_dict.values():
-            edges_col.extend(v)
-            indptr_list.append(ptr)
-            ptr += len(v)
-        indptr_list.append(ptr)
-
-        e_idx, v_idx = [], []
-        for n, e in self.n_e_dict.items():
-            v_idx.extend([n] * len(e))
-            e_idx.extend(e)
-        self.cache["e_idx"] = e_idx
-        self.cache["v_idx"] = v_idx
-
-        self.cache["edges_col"] = np.array(edges_col)
-        self.cache["indptr_list"] = np.array(indptr_list)
-
-    def __repr__(self) -> str:
-        r"""Print the hypergraph information."""
-        return f"Hypergraph(num_vertex={self.num_v}, num_hyperedge={self.num_e})"
-
-    @property
-    def ndata(self):
-        return self._ndata
-
-    @property
-    def state_dict(self) -> Dict[str, Any]:
-        r"""Get the state dict of the hypergraph."""
-        return {
-            "num_v": self.num_v,
-            "v_property": self.v_property,
-            "e_property": self.e_property,
-            "raw_groups": self._raw_groups,
-            "deg_v_dict": self.deg_v_dict,
-        }
-
-    def unique_edge_sizes(self):
-        """A function that returns the unique edge sizes.
-
-        Returns
-        -------
-        list()
-            The unique edge sizes in ascending order by size.
-        """
-        edge_size_set = set()
-        edge_lst = self.e[0]
-        for e in edge_lst:
-            edge_size_set.add(len(e))
-
-        return sorted(edge_size_set)
-
-    def is_uniform(self):
-        """Order of uniformity if the hypergraph is uniform, or False.
-
-        A hypergraph is uniform if all its edges have the same order.
-
-        Returns d if the hypergraph is d-uniform, that is if all edges
-        in the hypergraph (excluding singletons) have the same degree d.
-        Returns False if not uniform.
-
-        Returns
-        -------
-        d : int or False
-            If the hypergraph is d-uniform, return d, or False otherwise.
-
-        Examples
-        --------
-        This function can be used as a boolean check:
-
-        >>> import easygraph as eg
-        >>> H = eg.Hypergraph(v_num = 5, e_list = [(0, 1, 2), (1, 2, 3), (2, 3, 4)])
-        >>> H.is_uniform()
-        2
-        """
-        edge_sizes = self.unique_edge_sizes()
-        if 1 in edge_sizes:
-            edge_sizes.remove(1)
-
-        if edge_sizes is None or len(edge_sizes) != 1:
-            return False
-
-        # order of all edges
-        return edge_sizes.pop()
-
-    def save(self, file_path: Union[str, Path]):
-        r"""Save the EasyGraph's hypergraph structure a file.
-
-        Parameters:
-            ``file_path`` (``Union[str, Path]``): The file path to store the EasyGraph's hypergraph structure.
-        """
-        file_path = Path(file_path)
-        assert file_path.parent.exists(), "The directory does not exist."
-        data = {
-            "class": "Hypergraph",
-            "state_dict": self.state_dict,
-        }
-        with open(file_path, "wb") as fp:
-            pickle.dump(data, fp)
-
-    @staticmethod
-    def load(file_path: Union[str, Path]):
-        r"""Load the EasyGraph's hypergraph structure from a file.
-
-        Parameters:
-            ``file_path`` (``Union[str, Path]``): The file path to load the EasyGraph's hypergraph structure.
-        """
-        file_path = Path(file_path)
-        assert file_path.exists(), "The file does not exist."
-        with open(file_path, "rb") as fp:
-            data = pickle.load(fp)
-        assert (
-            data["class"] == "Hypergraph"
-        ), "The file is not a EasyGraph's hypergraph file."
-        return Hypergraph.from_state_dict(data["state_dict"])
-
-    def draw(
-        self,
-        e_style: str = "circle",
-        v_label: Optional[List[str]] = None,
-        v_size: Union[float, list] = 1.0,
-        v_color: Union[str, list] = "r",
-        v_line_width: Union[str, list] = 1.0,
-        e_color: Union[str, list] = "gray",
-        e_fill_color: Union[str, list] = "whitesmoke",
-        e_line_width: Union[str, list] = 1.0,
-        font_size: float = 1.0,
-        font_family: str = "sans-serif",
-        push_v_strength: float = 1.0,
-        push_e_strength: float = 1.0,
-        pull_e_strength: float = 1.0,
-        pull_center_strength: float = 1.0,
-    ):
-        r"""Draw the hypergraph structure.
-
-        Parameters:
-            ``e_style`` (``str``): The style of hyperedges. The available styles are only ``'circle'``. Defaults to ``'circle'``.
-            ``v_label`` (``list``): The labels of vertices. Defaults to ``None``.
-            ``v_size`` (``float`` or ``list``): The size of vertices. Defaults to ``1.0``.
-            ``v_color`` (``str`` or ``list``): The `color <https://matplotlib.org/stable/gallery/color/named_colors.html>`_ of vertices. Defaults to ``'r'``.
-            ``v_line_width`` (``float`` or ``list``): The line width of vertices. Defaults to ``1.0``.
-            ``e_color`` (``str`` or ``list``): The `color <https://matplotlib.org/stable/gallery/color/named_colors.html>`_ of hyperedges. Defaults to ``'gray'``.
-            ``e_fill_color`` (``str`` or ``list``): The fill `color <https://matplotlib.org/stable/gallery/color/named_colors.html>`_ of hyperedges. Defaults to ``'whitesmoke'``.
-            ``e_line_width`` (``float`` or ``list``): The line width of hyperedges. Defaults to ``1.0``.
-            ``font_size`` (``float``): The font size of labels. Defaults to ``1.0``.
-            ``font_family`` (``str``): The font family of labels. Defaults to ``'sans-serif'``.
-            ``push_v_strength`` (``float``): The strength of pushing vertices. Defaults to ``1.0``.
-            ``push_e_strength`` (``float``): The strength of pushing hyperedges. Defaults to ``1.0``.
-            ``pull_e_strength`` (``float``): The strength of pulling hyperedges. Defaults to ``1.0``.
-            ``pull_center_strength`` (``float``): The strength of pulling vertices to the center. Defaults to ``1.0``.
-        """
-        draw_hypergraph(
-            self,
-            e_style,
-            v_label,
-            v_size,
-            v_color,
-            v_line_width,
-            e_color,
-            e_fill_color,
-            e_line_width,
-            font_size,
-            font_family,
-            push_v_strength,
-            push_e_strength,
-            pull_e_strength,
-            pull_center_strength,
-        )
-
-    def clear(self):
-        r"""Clear all hyperedges and caches from the hypergraph."""
-
-        super().clear()
-        self.deg_v_dict = {}
-        self._ndata = {}
-
-    def clone(self) -> "Hypergraph":
-        r"""Return a copy of the hypergraph."""
-        hg = Hypergraph(self.num_v, device=self.device)
-        hg._raw_groups = deepcopy(self._raw_groups)
-        hg.cache = deepcopy(self.cache)
-        hg.group_cache = deepcopy(self.group_cache)
-        hg.deg_v_dict = deepcopy(self.deg_v_dict)
-        return hg
-
-    def to(self, device: torch.device):
-        r"""Move the hypergraph to the specified device.
-
-        Parameters:
-            ``device`` (``torch.device``): The target device.
-        """
-        return super().to(device)
-
-    # =====================================================================================
-    # some construction functions
-    @staticmethod
-    def from_state_dict(state_dict: dict):
-        r"""Load the hypergraph from the state dict.
-
-        Parameters:
-            ``state_dict`` (``dict``): The state dict to load the hypergraph.
-        """
-        _hg = Hypergraph(state_dict["num_v"])
-        _hg._raw_groups = deepcopy(state_dict["raw_groups"])
-        _hg._e_property = deepcopy(state_dict["e_property"])
-        _hg._v_property = deepcopy(state_dict["v_property"])
-        _hg.deg_v_dict = deepcopy(state_dict["deg_v_dict"])
-        return _hg
-
-    @staticmethod
-    def _e_list_from_feature_kNN(features: torch.Tensor, k: int):
-        import scipy
-
-        r"""Construct hyperedges from the feature matrix. Each hyperedge in the hypergraph is constructed by the central vertex and its :math:`k-1` neighbor vertices.
-
-        Parameters:
-            ``features`` (``torch.Tensor``): The feature matrix.
-            ``k`` (``int``): The number of nearest neighbors.
-        """
-        features = features.cpu().numpy()
-        assert features.ndim == 2, "The feature matrix should be 2-D."
-        assert k <= features.shape[0], (
-            "The number of nearest neighbors should be less than or equal to the number"
-            " of vertices."
-        )
-        tree = scipy.spatial.cKDTree(features)
-        _, nbr_array = tree.query(features, k=k)
-        return nbr_array.tolist()
-
-    @staticmethod
-    def from_feature_kNN(
-        features: torch.Tensor, k: int, device: torch.device = torch.device("cpu")
-    ):
-        r"""Construct the hypergraph from the feature matrix. Each hyperedge in the hypergraph is constructed by the central vertex and its :math:`k-1` neighbor vertices.
-
-        .. note::
-            The constructed hypergraph is a k-uniform hypergraph. If the feature matrix has the size :math:`N \times C`, the number of vertices and hyperedges of the constructed hypergraph are both :math:`N`.
-
-        Parameters:
-            ``features`` (``torch.Tensor``): The feature matrix.
-            ``k`` (``int``): The number of nearest neighbors.
-            ``device`` (``torch.device``, optional): The device to store the hypergraph. Defaults to ``torch.device('cpu')``.
-        """
-        e_list = Hypergraph._e_list_from_feature_kNN(features, k)
-        hg = Hypergraph(num_v=features.shape[0], e_list=e_list, device=device)
-        return hg
-
-    @staticmethod
-    def from_graph(graph, device: torch.device = torch.device("cpu")) -> "Hypergraph":
-        r"""Construct the hypergraph from the graph. Each edge in the graph is treated as a hyperedge in the constructed hypergraph.
-
-        .. note::
-            The constructed hypergraph is a 2-uniform hypergraph, and has the same number of vertices and edges/hyperedges as the graph.
-
-        Parameters:
-            ``graph`` (``eg.Graph``): The graph to construct the hypergraph.
-            ``device`` (``torch.device``, optional): The device to store the hypergraph. Defaults to ``torch.device('cpu')``.
-        """
-        e_list, e_weight, v_property, e_property = graph.e
-        hg = Hypergraph(
-            num_v=len(graph.nodes),
-            e_list=e_list,
-            e_weight=e_weight,
-            v_property=v_property,
-            e_property=e_property,
-            device=device,
-        )
-        return hg
-
-    @staticmethod
-    def _e_list_from_graph_kHop(
-        graph,
-        k: int,
-        only_kHop: bool = False,
-    ) -> List[tuple]:
-        r"""Construct the hyperedge list from the graph by k-Hop neighbors. Each hyperedge in the hypergraph is constructed by the central vertex and its :math:`k`-Hop neighbor vertices.
-
-        .. note::
-            If the graph have :math:`|\mathcal{V}|` vertices, the constructed hypergraph will have :math:`|\mathcal{V}|` vertices and equal to or less than :math:`|\mathcal{V}|` hyperedges.
-
-        Parameters:
-            ``graph`` (``eg.Graph``): The graph to construct the hypergraph.
-            ``k`` (``int``): The number of hop neighbors.
-            ``only_kHop`` (``bool``, optional): If set to ``True``, only the central vertex and its :math:`k`-th Hop neighbors are used to construct the hyperedges. By default, the constructed hyperedge will include the central vertex and its [ :math:`1`-th, :math:`2`-th, :math:`\cdots`, :math:`k`-th ] Hop neighbors. Defaults to ``False``.
-        """
-        assert (
-            k >= 1
-        ), "The number of hop neighbors should be larger than or equal to 1."
-        A_1, A_k = graph.A.clone(), graph.A.clone()
-        A_history = []
-        for _ in range(k - 1):
-            A_k = torch.sparse.mm(A_k, A_1)
-            if not only_kHop:
-                A_history.append(A_k.clone())
-        if not only_kHop:
-            A_k = A_1
-            for A_ in A_history:
-                A_k = A_k + A_
-        e_list = [
-            tuple(set([v_idx] + A_k[v_idx]._indices().cpu().squeeze(0).tolist()))
-            for v_idx in range(len(graph.nodes))
-        ]
-        return e_list
-
-    @staticmethod
-    def from_graph_kHop(
-        graph,
-        k: int,
-        only_kHop: bool = False,
-        device: torch.device = torch.device("cpu"),
-    ) -> "Hypergraph":
-        r"""Construct the hypergraph from the graph by k-Hop neighbors. Each hyperedge in the hypergraph is constructed by the central vertex and its :math:`k`-Hop neighbor vertices.
-
-        .. note::
-            If the graph have :math:`|\mathcal{V}|` vertices, the constructed hypergraph will have :math:`|\mathcal{V}|` vertices and equal to or less than :math:`|\mathcal{V}|` hyperedges.
-
-        Parameters:
-            ``graph`` (``eg.Graph``): The graph to construct the hypergraph.
-            ``k`` (``int``): The number of hop neighbors.
-            ``only_kHop`` (``bool``): If set to ``True``, only the central vertex and its :math:`k`-th Hop neighbors are used to construct the hyperedges. By default, the constructed hyperedge will include the central vertex and its [ :math:`1`-th, :math:`2`-th, :math:`\cdots`, :math:`k`-th ] Hop neighbors. Defaults to ``False``.
-            ``device`` (``torch.device``, optional): The device to store the hypergraph. Defaults to ``torch.device('cpu')``.
-        """
-        e_list = Hypergraph._e_list_from_graph_kHop(graph, k, only_kHop)
-        hg = Hypergraph(num_v=len(graph.nodes), e_list=e_list, device=device)
-        return hg
-
-    def isOutRange(self, id):
-        if id >= self.num_v or id < 0:
-            return False
-        return True
-
-    def add_hyperedges(
-        self,
-        e_list: Union[List[int], List[List[int]]],
-        e_weight: Optional[Union[float, List[float]]] = None,
-        e_property: Optional[Union[Dict, List[Dict]]] = None,
-        merge_op: str = "sum",
-        group_name: str = "main",
-    ):
-        r"""Add hyperedges to the hypergraph. If the ``group_name`` is not specified, the hyperedges will be added to the default ``main`` hyperedge group.
-
-        Parameters:
-            ``e_list`` (``Union[List[int], List[List[int]]]``): A list of hyperedges describes how the vertices point to the hyperedges.
-            ``e_weight`` (``Union[float, List[float]]``, optional): A list of weights for hyperedges. If set to ``None``, the value ``1`` is used for all hyperedges. Defaults to ``None``.
-            ``merge_op`` (``str``): The merge operation for the conflicting hyperedges. The possible values are ``"mean"``, ``"sum"``, and ``"max"``. Defaults to ``"mean"``.
-            ``group_name`` (``str``, optional): The target hyperedge group to add these hyperedges. Defaults to the ``main`` hyperedge group.
-        """
-        e_list = self._format_e_list(e_list)
-        if e_weight is None:
-            e_weight = [1.0] * len(e_list)
-        elif type(e_weight) in (int, float):
-            e_weight = [e_weight]
-        elif type(e_weight) is list:
-            pass
-        else:
-            raise TypeError(
-                "The type of e_weight should be float or list, but got"
-                f" {type(e_weight)}"
-            )
-        assert len(e_list) == len(
-            e_weight
-        ), "The number of hyperedges and the number of weights are not equal."
-
-        for _idx in range(len(e_list)):
-            flag = True
-
-            if (
-                group_name not in self._raw_groups
-                or self._hyperedge_code(e_list[_idx], e_list[_idx])
-                not in self._raw_groups[group_name]
-            ):
-                flag = False
-                self.edge_index += 1
-            for n_id in e_list[_idx]:
-                if self.isOutRange(n_id) == False:
-                    raise EasyGraphError(
-                        "The node id:"
-                        + str(n_id)
-                        + " in hyperedge is out of range, please ensure that"
-                        " the node is in [0,n)"
-                    )
-                self.deg_v_dict[n_id] += 1
-                if flag is False:
-                    self.n_e_dict[n_id].append(self.edge_index)
-            if e_property != None:
-                if type(e_property) == dict:
-                    e_property = [e_property]
-                e_property[_idx].update({"w_e": float(e_weight[_idx])})
-                self._add_hyperedge(
-                    self._hyperedge_code(e_list[_idx], e_list[_idx]),
-                    e_property[_idx],
-                    merge_op,
-                    group_name,
-                )
-            else:
-                self._add_hyperedge(
-                    self._hyperedge_code(e_list[_idx], e_list[_idx]),
-                    {"w_e": float(e_weight[_idx])},
-                    merge_op,
-                    group_name,
-                )
-
-        self._clear_cache(group_name)
-
-    def add_hyperedges_from_feature_kNN(
-        self, feature: torch.Tensor, k: int, group_name: str = "main"
-    ):
-        r"""Add hyperedges from the feature matrix by k-NN. Each hyperedge is constructed by the central vertex and its :math:`k`-Nearest Neighbor vertices.
-
-        Parameters:
-            ``features`` (``torch.Tensor``): The feature matrix.
-            ``k`` (``int``): The number of nearest neighbors.
-            ``group_name`` (``str``, optional): The target hyperedge group to add these hyperedges. Defaults to the ``main`` hyperedge group.
-        """
-        assert feature.shape[0] == self.num_v, (
-            "The number of vertices in the feature matrix is not equal to the number of"
-            " vertices in the hypergraph."
-        )
-        e_list = Hypergraph._e_list_from_feature_kNN(feature, k)
-        self.add_hyperedges(e_list, group_name=group_name)
-
-    def add_hyperedges_from_graph(self, graph, group_name: str = "main"):
-        r"""Add hyperedges from edges in the graph. Each edge in the graph is treated as a hyperedge.
-
-        Parameters:
-            ``graph`` (``eg.Graph``): The graph to join the hypergraph.
-            ``group_name`` (``str``, optional): The target hyperedge group to add these hyperedges. Defaults to the ``main`` hyperedge group.
-        """
-        assert self.num_v == len(
-            graph.nodes
-        ), "The number of vertices in the hypergraph and the graph are not equal."
-        e_list, e_weight = graph.e_both_side
-        self.add_hyperedges(e_list, e_weight=e_weight, group_name=group_name)
-
-    def add_hyperedges_from_graph_kHop(
-        self, graph, k: int, only_kHop: bool = False, group_name: str = "main"
-    ):
-        r"""Add hyperedges from vertices and its k-Hop neighbors in the graph. Each hyperedge in the hypergraph is constructed by the central vertex and its :math:`k`-Hop neighbor vertices.
-
-        .. note::
-            If the graph have :math:`|\mathcal{V}|` vertices, the constructed hypergraph will have :math:`|\mathcal{V}|` vertices and equal to or less than :math:`|\mathcal{V}|` hyperedges.
-
-        Parameters:
-            ``graph`` (``eg.Graph``): The graph to join the hypergraph.
-            ``k`` (``int``): The number of hop neighbors.
-            ``only_kHop`` (``bool``): If set to ``True``, only the central vertex and its :math:`k`-th Hop neighbors are used to construct the hyperedges. By default, the constructed hyperedge will include the central vertex and its [ :math:`1`-th, :math:`2`-th, :math:`\cdots`, :math:`k`-th ] Hop neighbors. Defaults to ``False``.
-            ``group_name`` (``str``, optional): The target hyperedge group to add these hyperedges. Defaults to the ``main`` hyperedge group.
-        """
-        assert self.num_v == len(
-            graph.nodes
-        ), "The number of vertices in the hypergraph and the graph are not equal."
-        e_list = Hypergraph._e_list_from_graph_kHop(graph, k, only_kHop=only_kHop)
-        self.add_hyperedges(e_list, group_name=group_name)
-
-    def remove_hyperedges(
-        self,
-        e_list: Union[List[int], List[List[int]]],
-        group_name: Optional[str] = None,
-    ):
-        r"""Remove the specified hyperedges from the hypergraph.
-
-        Parameters:
-            ``e_list`` (``Union[List[int], List[List[int]]]``): A list of hyperedges describes how the vertices point to the hyperedges.
-            ``group_name`` (``str``, optional): Remove these hyperedges from the specified hyperedge group. If not specified, the function will
-                remove those hyperedges from all hyperedge groups. Defaults to the ``None``.
-        """
-        assert (
-            group_name is None or group_name in self.group_names
-        ), "The specified group_name is not in existing hyperedge groups."
-        e_list = self._format_e_list(e_list)
-        if group_name is None:
-            for _idx in range(len(e_list)):
-                for n_id in e_list[_idx]:
-                    self.deg_v_dict[n_id] -= 1
-                    if self.isOutRange(n_id) == False:
-                        raise EasyGraphError(
-                            "The node id in hyperedge is out of range, please ensure"
-                            " that the node is in [1,n)"
-                        )
-                e_code = self._hyperedge_code(e_list[_idx], e_list[_idx])
-                for name in self.group_names:
-                    self._raw_groups[name].pop(e_code, None)
-        else:
-            for _idx in range(len(e_list)):
-                for n_id in e_list[_idx]:
-                    self.deg_v_dict[n_id] -= 1
-                    if self.isOutRange(n_id) == False:
-                        raise EasyGraphError(
-                            "The node id in hyperedge is out of range, please ensure"
-                            " that the node is in [1,n)"
-                        )
-                e_code = self._hyperedge_code(e_list[_idx], e_list[_idx])
-                self._raw_groups[group_name].pop(e_code, None)
-        self._clear_cache(group_name)
-
-    def remove_group(self, group_name: str):
-        r"""Remove the specified hyperedge group from the hypergraph.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the hyperedge group to remove.
-        """
-        for e_code, e in self._raw_groups[group_name].items():
-            e = e_code[0]
-            for n_id in e:
-                self.deg_v_dict[n_id] -= 1
-        self._raw_groups.pop(group_name, None)
-        self._clear_cache(group_name)
-
-    def drop_hyperedges(self, drop_rate: float, ord="uniform"):
-        r"""Randomly drop hyperedges from the hypergraph. This function will return a new hypergraph with non-dropped hyperedges.
-
-        Parameters:
-            ``drop_rate`` (``float``): The drop rate of hyperedges.
-            ``ord`` (``str``): The order of dropping edges. Currently, only ``'uniform'`` is supported. Defaults to ``uniform``.
-        """
-        if ord == "uniform":
-            _raw_groups = {}
-            for name in self.group_names:
-                _raw_groups[name] = {
-                    k: v
-                    for k, v in self._raw_groups[name].items()
-                    if random.random() > drop_rate
-                }
-            state_dict = {
-                "num_v": self.num_v,
-                "raw_groups": _raw_groups,
-                "e_property": self._e_property,
-                "v_property": self._v_property,
-            }
-            _hg = Hypergraph.from_state_dict(state_dict)
-            _hg = _hg.to(self.device)
-        else:
-            raise ValueError(f"Unknown drop order: {ord}.")
-        return _hg
-
-    def drop_hyperedges_of_group(
-        self, group_name: str, drop_rate: float, ord="uniform"
-    ):
-        r"""Randomly drop hyperedges from the specified hyperedge group. This function will return a new hypergraph with non-dropped hyperedges.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the hyperedge group.
-            ``drop_rate`` (``float``): The drop rate of hyperedges.
-            ``ord`` (``str``): The order of dropping edges. Currently, only ``'uniform'`` is supported. Defaults to ``uniform``.
-        """
-        if ord == "uniform":
-            _raw_groups = {}
-            for name in self.group_names:
-                if name == group_name:
-                    _raw_groups[name] = {
-                        k: v
-                        for k, v in self._raw_groups[name].items()
-                        if random.random() > drop_rate
-                    }
-                else:
-                    _raw_groups[name] = self._raw_groups[name]
-            state_dict = {
-                "num_v": self.num_v,
-                "raw_groups": self._raw_groups,
-                "e_property": self._e_property,
-                "v_property": self._v_property,
-            }
-            _hg = Hypergraph.from_state_dict(state_dict)
-            _hg = _hg.to(self.device)
-        else:
-            raise ValueError(f"Unknown drop order: {ord}.")
-        return _hg
-
-    # =====================================================================================
-    # properties for representation
-    @property
-    def v(self) -> List[int]:
-        r"""Return the list of vertices."""
-        return super().v
-
-    @property
-    def e(self) -> Tuple[List[List[int]], List[float]]:
-        r"""Return all hyperedges and weights in the hypergraph."""
-        if self.cache.get("e", None) is None:
-            e_list, e_weight, e_property = [], [], []
-            for name in self.group_names:
-                _e = self.e_of_group(name)
-                e_list.extend(_e[0])
-                e_weight.extend(_e[1])
-                e_property.extend(_e[2])
-            self.cache["e"] = (e_list, e_weight, e_property)
-        return self.cache["e"]
-
-    def e_of_group(self, group_name: str) -> Tuple[List[List[int]], List[float]]:
-        r"""Return all hyperedges and weights of the specified hyperedge group.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("e", None) is None:
-            e_list = [e_code[0] for e_code in self._raw_groups[group_name].keys()]
-            e_weight = [
-                e_content["w_e"] for e_content in self._raw_groups[group_name].values()
-            ]
-
-            e_property = []
-            for e_content in self._raw_groups[group_name].values():
-                properties = {}
-                for k, v in e_content.items():
-                    if k != "w_e":
-                        properties[k] = v
-                e_property.append(properties)
-            self.group_cache[group_name]["e"] = (e_list, e_weight, e_property)
-        return self.group_cache[group_name]["e"]
-
-    @property
-    def num_v(self) -> int:
-        r"""Return the number of vertices in the hypergraph."""
-        return super().num_v
-
-    @property
-    def num_e(self) -> int:
-        r"""Return the number of hyperedges in the hypergraph."""
-        return super().num_e
-
-    def num_e_of_group(self, group_name: str) -> int:
-        r"""Return the number of hyperedges of the specified hyperedge group.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        return super().num_e_of_group(group_name)
-
-    @property
-    def deg_v(self) -> List[int]:
-        r"""Return the degree list of each vertex."""
-        return self.D_v.to_sparse_coo()._values().cpu().view(-1).numpy().tolist()
-
-    def deg_v_of_group(self, group_name: str) -> List[int]:
-        r"""Return the degree list of each vertex of the specified hyperedge group.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        return self.D_v_of_group(group_name)._values().cpu().view(-1).numpy().tolist()
-
-    @property
-    def deg_e(self) -> List[int]:
-        r"""Return the degree list of each hyperedge."""
-        return self.D_e.to_sparse_coo()._values().cpu().view(-1).numpy().tolist()
-
-    def deg_e_of_group(self, group_name: str) -> List[int]:
-        r"""Return the degree list of each hyperedge of the specified hyperedge group.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        return self.D_e_of_group(group_name)._values().cpu().view(-1).numpy().tolist()
-
-    def nbr_e(self, v_idx: int) -> List[int]:
-        r"""Return the neighbor hyperedge list of the specified vertex.
-
-        Parameters:
-            ``v_idx`` (``int``): The index of the vertex.
-        """
-        return self.N_e(v_idx).cpu().numpy().tolist()
-
-    def nbr_e_of_group(self, v_idx: int, group_name: str) -> List[int]:
-        r"""Return the neighbor hyperedge list of the specified vertex of the specified hyperedge group.
-
-        Parameters:
-            ``v_idx`` (``int``): The index of the vertex.
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        return self.N_e_of_group(v_idx, group_name).cpu().numpy().tolist()
-
-    def nbr_v(self, e_idx: int) -> List[int]:
-        r"""Return the neighbor vertex list of the specified hyperedge.
-
-        Parameters:
-            ``e_idx`` (``int``): The index of the hyperedge.
-        """
-        return self.N_v(e_idx).cpu().numpy().tolist()
-
-    def nbr_v_of_group(self, e_idx: int, group_name: str) -> List[int]:
-        r"""Return the neighbor vertex list of the specified hyperedge of the specified hyperedge group.
-
-        Parameters:
-            ``e_idx`` (``int``): The index of the hyperedge.
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        return self.N_v_of_group(e_idx, group_name).cpu().numpy().tolist()
-
-    @property
-    def num_groups(self) -> int:
-        r"""Return the number of hyperedge groups in the hypergraph."""
-        return super().num_groups
-
-    @property
-    def group_names(self) -> List[str]:
-        r"""Return the names of all hyperedge groups in the hypergraph."""
-        return super().group_names
-
-    # =====================================================================================
-    # properties for deep learning
-    @property
-    def vars_for_DL(self) -> List[str]:
-        r"""Return a name list of available variables for deep learning in the hypergraph including
-
-        Sparse Matrices:
-
-        .. math::
-            \mathbf{H}, \mathbf{H}^\top, \mathcal{L}_{sym}, \mathcal{L}_{rw} \mathcal{L}_{HGNN},
-
-        Sparse Diagnal Matrices:
-
-        .. math::
-            \mathbf{W}_e, \mathbf{D}_v, \mathbf{D}_v^{-1}, \mathbf{D}_v^{-\frac{1}{2}}, \mathbf{D}_e, \mathbf{D}_e^{-1},
-
-        Vectors:
-
-        .. math::
-            \overrightarrow{v2e}_{src}, \overrightarrow{v2e}_{dst}, \overrightarrow{v2e}_{weight},\\
-            \overrightarrow{e2v}_{src}, \overrightarrow{e2v}_{dst}, \overrightarrow{e2v}_{weight}
-
-        """
-        return [
-            "H",
-            "H_T",
-            "L_sym",
-            "L_rw",
-            "L_HGNN",
-            "W_e",
-            "D_v",
-            "D_v_neg_1",
-            "D_v_neg_1_2",
-            "D_e",
-            "D_e_neg_1",
-            "v2e_src",
-            "v2e_dst",
-            "v2e_weighte2v_src",
-            "e2v_dst",
-            "e2v_weight",
-        ]
-
-    @property
-    def v2e_src(self) -> torch.Tensor:
-        r"""Return the source vertex index vector :math:`\overrightarrow{v2e}_{src}` of the connections (vertices point to hyperedges) in the hypergraph.
-        """
-        return self.H_T._indices()[1].clone()
-
-    def v2e_src_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the source vertex index vector :math:`\overrightarrow{v2e}_{src}` of the connections (vertices point to hyperedges) in the specified hyperedge group.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        return self.H_T_of_group(group_name)._indices()[1].clone()
-
-    @property
-    def v2e_dst(self) -> torch.Tensor:
-        r"""Return the destination hyperedge index vector :math:`\overrightarrow{v2e}_{dst}` of the connections (vertices point to hyperedges) in the hypergraph.
-        """
-        if self.cache.get("v2e_dst") is None:
-            self.cache["v2e_dst"] = self.H_T._indices()[0]
-        return self.cache["v2e_dst"]
-
-    def v2e_dst_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the destination hyperedge index vector :math:`\overrightarrow{v2e}_{dst}` of the connections (vertices point to hyperedges) in the specified hyperedge group.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        return self.H_T_of_group(group_name)._indices()[0].clone()
-
-    @property
-    def v2e_weight(self) -> torch.Tensor:
-        r"""Return the weight vector :math:`\overrightarrow{v2e}_{weight}` of the connections (vertices point to hyperedges) in the hypergraph.
-        """
-        return self.H_T._values().clone()
-
-    def v2e_weight_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the weight vector :math:`\overrightarrow{v2e}_{weight}` of the connections (vertices point to hyperedges) in the specified hyperedge group.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        return self.H_T_of_group(group_name)._values().clone()
-
-    @property
-    def e2v_src(self) -> torch.Tensor:
-        r"""Return the source hyperedge index vector :math:`\overrightarrow{e2v}_{src}` of the connections (hyperedges point to vertices) in the hypergraph.
-        """
-        return self.H._indices()[1]
-
-    def e2v_src_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the source hyperedge index vector :math:`\overrightarrow{e2v}_{src}` of the connections (hyperedges point to vertices) in the specified hyperedge group.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        return self.H_of_group(group_name)._indices()[1].clone()
-
-    @property
-    def e2v_dst(self) -> torch.Tensor:
-        r"""Return the destination vertex index vector :math:`\overrightarrow{e2v}_{dst}` of the connections (hyperedges point to vertices) in the hypergraph.
-        """
-        return self.H._indices()[0].clone()
-
-    def e2v_dst_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the destination vertex index vector :math:`\overrightarrow{e2v}_{dst}` of the connections (hyperedges point to vertices) in the specified hyperedge group.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        return self.H_of_group(group_name)._indices()[0].clone()
-
-    @property
-    def e2v_weight(self) -> torch.Tensor:
-        r"""Return the weight vector :math:`\overrightarrow{e2v}_{weight}` of the connections (hyperedges point to vertices) in the hypergraph.
-        """
-        return self.H._values()
-
-    def e2v_weight_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the weight vector :math:`\overrightarrow{e2v}_{weight}` of the connections (hyperedges point to vertices) in the specified hyperedge group.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        return self.H_of_group(group_name)._values().clone()
-
-    @property
-    def H(self) -> torch.Tensor:
-        r"""Return the hypergraph incidence matrix :math:`\mathbf{H}` with ``torch.Tensor`` format.
-        """
-
-        if self.cache.get("H") is None:
-            num_e = len(self._raw_groups["main"])
-            if self.cache.get("v_idx") is None or self.cache.get("e_idx") is None:
-                e_idx, v_idx = [], []
-                for n, e in self.n_e_dict.items():
-                    v_idx.extend([n] * len(e))
-                    e_idx.extend(e)
-                self.cache["e_idx"] = e_idx
-                self.cache["v_idx"] = v_idx
-            self.cache["H"] = torch.sparse_coo_tensor(
-                torch.tensor(
-                    [self.cache["v_idx"], self.cache["e_idx"]], dtype=torch.long
-                ),
-                torch.ones(len(self.cache["v_idx"])),
-                torch.Size([self.num_v, num_e]),
-                device=self.device,
-            ).coalesce()
-
-        return self.cache["H"]
-
-    @property
-    def e_set(self):
-        if self.cache.get("e_set") is None:
-            e_lst = []
-            for name in self.group_names:
-                _e = self.e_of_group(name)
-                e_lst.extend(_e[0])
-            self.cache["e_set"] = e_lst
-        return self.cache["e_set"]
-
-    @property
-    def incidence_matrix(self):
-        if self.cache.get("incidence_matrix") is None:
-            if (
-                self.cache.get("edges_col") is None
-                or self.cache.get("indptr_list") is None
-            ):
-                edges_col = []
-                indptr_list = []
-                ptr = 0
-                for v in self.n_e_dict.values():
-                    edges_col.extend(v)
-                    indptr_list.append(ptr)
-                    ptr += len(v)
-                indptr_list.append(ptr)
-                self.cache["edges_col"] = np.array(edges_col)
-                self.cache["indptr_list"] = np.array(indptr_list)
-
-            H = csr_matrix(
-                (
-                    [1] * len(self.cache["edges_col"]),
-                    self.cache["edges_col"],
-                    self.cache["indptr_list"],
-                ),
-                shape=(self.num_v, self.num_e),
-                dtype=int,
-            )
-            self.cache["incidence_matrix"] = H
-        return self.cache["incidence_matrix"]
-
-    def get_star_expansion(self):
-        r"""
-        The star expansion algorithm creates a graph  G*(V*, E*) for every hypergraph G(V, E).
-        The graph G*(V*, E*) introduces a node e∈E for each hyperedge in G(V, E), where V* = V ∪ E.
-        Each node e is connected to all the nodes belonging to the hyperedge it originates from, i.e., E* = {(u, e): u∈e, e∈E}.
-        It is worth noting that each hyperedge in the set E corresponds to a star-shaped structure in the graph G*(V*, E*),
-        and G* is a bipartite graph. The star expansion redistributes the weights of hyperedges to their corresponding ordinary pairwise graph edges.
-
-        $ \omega ^{*}(u,e)=\frac{\omega(e)}{\delta(e)} $
-
-        References
-        ----------
-        Antelmi, Alessia, et al. "A survey on hypergraph representation learning." ACM Computing Surveys 56.1 (2023): 1-38.
-
-        """
-        star_expansion_graph = eg.Graph()
-        for node in self.v:
-            star_expansion_graph.add_node(node, type="node")
-        e_index = len(self.v)
-        hyperedge_edge_list = self.e[0]
-        hyperedge_weight_list = self.e[1]
-        hyperedge_property_list = self.e[2]
-        for hyperedge_index, e in enumerate(hyperedge_edge_list):
-            hyperedge_weight = hyperedge_weight_list[hyperedge_index]
-            star_expansion_graph.add_node(e_index, type="hyperedge")
-            for index, node in enumerate(e):
-                star_expansion_graph.add_edge(
-                    e_index,
-                    node,
-                    weight=hyperedge_weight / len(e),
-                    hyperedge_index=hyperedge_index,
-                    edge_property=hyperedge_property_list[index],
-                )
-            e_index = e_index + 1
-        return star_expansion_graph
-
-    def neighbor_of_node(self, node):
-        neighbor_lst = list()
-        node_adj = self.adjacency_matrix()
-        if (
-            self.cache.get("neighbor") is None
-            or self.cache["neighbor"].get(node) is None
-        ):
-            start = node_adj.indptr[node]
-            end = node_adj.indptr[node + 1]
-
-            for j in range(start, end):
-                neighbor_lst.append(node_adj.indices[j])
-
-            if self.cache.get("neighbor") is None:
-                self.cache["neighbor"] = {}
-                self.cache["neighbor"][node] = neighbor_lst
-            else:
-                self.cache["neighbor"][node] = neighbor_lst
-
-        return self.cache["neighbor"][node]
-
-    def adjacency_matrix(self, s=1, weight=False):
-        r"""
-        The :term:`s-adjacency matrix` for the dual hypergraph.
-
-        Parameters
-        ----------
-        s : int, optional, default 1
-
-        Returns
-        -------
-        adjacency_matrix : scipy.sparse.csr.csr_matrix
-
-        """
-        if self.cache.get("adjacency_matrix") == None:
-            tmp_H = self.incidence_matrix
-            A = tmp_H @ (tmp_H.T)
-            A[np.diag_indices_from(A)] = 0
-            if not weight:
-                A = (A >= s) * 1
-            self.cache["adjacency_matrix"] = csr_matrix(A)
-        return self.cache["adjacency_matrix"]
-
-    def edge_adjacency_matrix(self, s=1, weight=False):
-        r"""
-        The :term:`s-adjacency matrix` for the dual hypergraph.
-
-        Parameters
-        ----------
-        s : int, optional, default 1
-
-        Returns
-        -------
-        adjacency_matrix : scipy.sparse.csr.csr_matrix
-
-        """
-        tmp_H = self.incidence_matrix
-        A = (tmp_H.T) @ (tmp_H)
-        A[np.diag_indices_from(A)] = 0
-        if not weight:
-            A = (A >= s) * 1
-        return csr_array(A)
-        # return A
-
-    def H_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the hypergraph incidence matrix :math:`\mathbf{H}` of the specified hyperedge group with ``torch.Tensor`` format.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("H") is None:
-            self.group_cache[group_name]["H"] = self._fetch_H()
-        return self.group_cache[group_name]["H"]
-
-    def edge_distance(self, source, target, s=1):
-        """
-
-        Parameters
-        ----------
-        source
-        target
-        s
-
-        Returns
-        -------
-        s- walk distance : the shortest s-walk edge distance
-
-        Notes
-        -----
-            The s-distance is the shortest s-walk length between the edges.
-            An s-walk between edges is a sequence of edges such that
-            consecutive pairwise edges intersect in at least s nodes. The
-            length of the shortest s-walk is 1 less than the number of edges
-            in the path sequence.
-
-        """
-        l_graph = self.get_clique_expansion(s=s, edge=True)
-        if source not in l_graph.nodes:
-            raise EasyGraphError("Please make sure source exist!")
-        dist = eg.Dijkstra(l_graph, source)
-        if target in dist:
-            return dist[target]
-        raise EasyGraphError("Please make sure target exist!")
-
-    def distance(self, source, target=None, s=1):
-        """
-
-        Parameters
-        ----------
-        source : node in the hypergraph
-        target : node in the hypergraph
-        s : positive integer
-            the number of edges
-
-        Returns
-        -------
-        s-walk distance : int
-
-        Notes
-        -----
-        The s-distance is the shortest s-walk length between the nodes.
-        An s-walk between nodes is a sequence of nodes that pairwise share
-        at least s edges. The length of the shortest s-walk is 1 less than
-        the number of nodes in the path sequence.
-
-        Uses the EasyGraph's Dijkstra method on the graph
-        generated by the s-adjacency matrix.
-
-        """
-
-        l_graph = self.get_clique_expansion(s=s)
-        if source not in l_graph.nodes:
-            raise EasyGraphError("Please make sure source exist!")
-        if target is not None and target not in l_graph.nodes:
-            raise EasyGraphError("Please make sure target exist!")
-        dist = eg.single_source_dijkstra(G=l_graph, source=source, target=target)
-        return dist[target] if target != None else dist
-
-    def edge_diameter(self, s=1):
-        """
-        Returns the length of the longest shortest s-walk between edges in
-        hypergraph
-
-        Parameters
-        ----------
-        s : int, optional, default 1
-
-        Return
-        ------
-        edge_diameter : int
-
-        Raises
-        ------
-        EasyGraphXError
-            If hypergraph is not s-edge-connected
-
-        Notes
-        -----
-        Two edges are s-adjacent if they share s nodes.
-        Two nodes e_start and e_end are s-walk connected if there is a
-        sequence of edges e_start, e_1, e_2, ... e_n-1, e_end such that
-        consecutive edges are s-adjacent. If the graph is not connected, an
-        error will be raised.
-
-        """
-        l_graph = self.get_clique_expansion(s=s, edge=True)
-        if eg.is_connected(l_graph):
-            return eg.diameter(l_graph)
-        raise EasyGraphError(f"Hypergraph is not s-connected. s={s}")
-
-    def diameter(self, s=1):
-        """
-        Returns the length of the longest shortest s-walk between nodes in
-        hypergraph
-
-        Parameters
-        ----------
-        s : int, optional, default 1
-
-        Returns
-        -------
-        diameter : int
-        Raises
-        ------
-        EasyGraphError
-            If hypergraph is not s-edge-connected
-
-        Notes
-        -----
-        Two nodes are s-adjacent if they share s edges.
-        Two nodes v_start and v_end are s-walk connected if there is a
-        sequence of nodes v_start, v_1, v_2, ... v_n-1, v_end such that
-        consecutive nodes are s-adjacent. If the graph is not connected,
-        an error will be raised.
-        """
-        l_graph = self.get_clique_expansion(s=s)
-        if eg.is_connected(l_graph):
-            return eg.diameter(l_graph)
-        raise EasyGraphError(f"Hypergraph is not s-connected. s={s}")
-
-    @property
-    def H_T(self) -> torch.Tensor:
-        r"""Return the transpose of the hypergraph incidence matrix :math:`\mathbf{H}^\top` with ``torch.Tensor`` format.
-        """
-        if self.cache.get("H_T") is None:
-            self.cache["H_T"] = self.H.t()
-        return self.cache["H_T"]
-
-    def H_T_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the transpose of the hypergraph incidence matrix :math:`\mathbf{H}^\top` of the specified hyperedge group with ``torch.Tensor`` format.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("H_T") is None:
-            self.group_cache[group_name]["H_T"] = self.H_of_group(group_name).t()
-        return self.group_cache[group_name]["H_T"]
-
-    @property
-    def W_e(self) -> torch.Tensor:
-        r"""Return the weight matrix :math:`\mathbf{W}_e` of hyperedges with ``torch.Tensor`` format.
-        """
-        if self.cache.get("W_e") is None:
-            _tmp = torch.tensor(self.e[1])
-            _num_e = _tmp.size(0)
-            self.cache["W_e"] = torch.sparse_coo_tensor(
-                torch.arange(0, _num_e).view(1, -1).repeat(2, 1),
-                _tmp,
-                torch.Size([_num_e, _num_e]),
-                device=self.device,
-            ).coalesce()
-
-        return self.cache["W_e"]
-
-    def W_e_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the weight matrix :math:`\mathbf{W}_e` of hyperedges of the specified hyperedge group with ``torch.Tensor`` format.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("W_e") is None:
-            w_list = [1.0] * len(self._raw_groups["main"])
-            _tmp = torch.tensor(w_list, device=self.device).view((-1, 1)).view(-1)
-            _num_e = _tmp.size(0)
-            self.group_cache[group_name]["W_e"] = torch.sparse_coo_tensor(
-                torch.arange(0, _num_e).view(1, -1).repeat(2, 1),
-                _tmp,
-                torch.Size([_num_e, _num_e]),
-                device=self.device,
-            ).coalesce()
-        return self.group_cache[group_name]["W_e"]
-
-    @property
-    def degree_node(self):
-        return self.deg_v_dict
-
-    @property
-    def D_v(self) -> torch.Tensor:
-        r"""Return the vertex degree matrix :math:`\mathbf{D}_v` with ``torch.sparse_coo_tensor`` format.
-        """
-        if self.cache.get("D_v") is None:
-            if self.cache.get("D_v_value") is None:
-                self.cache["D_v_value"] = (
-                    torch.sparse.sum(self.H, dim=1).to_dense().view(-1)
-                )
-
-            self.cache["D_v"] = torch.sparse_coo_tensor(
-                torch.arange(0, self.num_v).view(1, -1).repeat(2, 1),
-                self.cache["D_v_value"],
-                torch.Size([self.num_v, self.num_v]),
-                device=self.device,
-            ).coalesce()
-        return self.cache["D_v"]
-
-    def D_v_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the vertex degree matrix :math:`\mathbf{D}_v` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("D_v") is None:
-            _tmp = (
-                torch.sparse.sum(self.H_of_group(group_name), dim=1)
-                .to_dense()
-                .clone()
-                .view(-1)
-            )
-            _num_v = _tmp.size(0)
-            self.group_cache[group_name]["D_v"] = torch.sparse_coo_tensor(
-                torch.arange(0, _num_v).view(1, -1).repeat(2, 1),
-                _tmp,
-                torch.Size([_num_v, _num_v]),
-                device=self.device,
-            ).coalesce()
-        return self.group_cache[group_name]["D_v"]
-
-    @property
-    def D_v_neg_1(self) -> torch.Tensor:
-        r"""Return the vertex degree matrix :math:`\mathbf{D}_v^{-1}` with ``torch.sparse_coo_tensor`` format.
-        """
-        if self.cache.get("D_v_neg_1") is None:
-            if self.cache.get("D_v_value") is None:
-                self.cache["D_v_value"] = (
-                    torch.sparse.sum(self.H, dim=1).to_dense().view(-1)
-                )
-            _tmp = self.cache["D_v_value"]
-            _num_v = _tmp.size(0)
-            _val = _tmp**-1
-            _val[torch.isinf(_val)] = 0
-            self.cache["D_v_neg_1"] = torch.sparse_csr_tensor(
-                torch.arange(0, _num_v + 1),
-                torch.arange(0, _num_v),
-                _val,
-                torch.Size([_num_v, _num_v]),
-                device=self.device,
-            )
-
-        return self.cache["D_v_neg_1"]
-
-    def D_v_neg_1_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the vertex degree matrix :math:`\mathbf{D}_v^{-1}` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("D_v_neg_1") is None:
-            _mat = self.D_v_of_group(group_name).clone()
-            _val = _mat._values() ** -1
-            _val[torch.isinf(_val)] = 0
-            self.group_cache[group_name]["D_v_neg_1"] = torch.sparse_coo_tensor(
-                _mat._indices(), _val, _mat.size(), device=self.device
-            ).coalesce()
-        return self.group_cache[group_name]["D_v_neg_1"]
-
-    @property
-    def D_v_neg_1_2(self) -> torch.Tensor:
-        r"""Return the vertex degree matrix :math:`\mathbf{D}_v^{-\frac{1}{2}}` with ``torch.sparse_coo_tensor`` format.
-        """
-        if self.cache.get("D_v_neg_1_2") is None:
-            if self.cache.get("D_v_value") is None:
-                self.cache["D_v_value"] = (
-                    torch.sparse.sum(self.H, dim=1).to_dense().view(-1)
-                )
-            _mat = self.cache["D_v_value"]
-            _mat = _mat**-0.5
-            _mat[torch.isinf(_mat)] = 0
-            self.cache["D_v_neg_1_2"] = torch.sparse_csr_tensor(
-                torch.arange(0, self.num_v + 1),
-                torch.arange(0, self.num_v),
-                _mat,
-                torch.Size([self.num_v, self.num_v]),
-                device=self.device,
-            )
-
-        return self.cache["D_v_neg_1_2"]
-
-    def D_v_neg_1_2_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the vertex degree matrix :math:`\mathbf{D}_v^{-\frac{1}{2}}` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("D_v_neg_1_2") is None:
-            _mat = self.D_v_of_group(group_name).clone()
-            _val = _mat._values() ** -0.5
-            _val[torch.isinf(_val)] = 0
-            self.group_cache[group_name]["D_v_neg_1_2"] = torch.sparse_coo_tensor(
-                _mat._indices(), _val, _mat.size(), device=self.device
-            ).coalesce()
-        return self.group_cache[group_name]["D_v_neg_1_2"]
-
-    @property
-    def D_e(self) -> torch.Tensor:
-        r"""Return the hyperedge degree matrix :math:`\mathbf{D}_e` with ``torch.sparse_coo_tensor`` format.
-        """
-        if self.cache.get("D_e") is None:
-            _tmp = torch.sparse.sum(self.H_T, dim=1).to_dense().view(-1)
-            _num_e = _tmp.size(0)
-            self.cache["D_e"] = torch.sparse_csr_tensor(
-                torch.arange(0, _num_e + 1),
-                torch.arange(0, _num_e),
-                _tmp,
-                torch.Size([_num_e, _num_e]),
-                device=self.device,
-            )
-
-        return self.cache["D_e"]
-
-    def D_e_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the hyperedge degree matrix :math:`\mathbf{D}_e` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("D_e") is None:
-            _tmp = (
-                torch.sparse.sum(self._fetch_H().t(), dim=1).to_dense().clone().view(-1)
-            )
-            _num_e = _tmp.size(0)
-            self.group_cache[group_name]["D_e"] = torch.sparse_coo_tensor(
-                torch.arange(0, _num_e).view(1, -1).repeat(2, 1),
-                _tmp,
-                torch.Size([_num_e, _num_e]),
-                device=self.device,
-            ).coalesce()
-        return self.group_cache[group_name]["D_e"]
-
-    @property
-    def D_e_neg_1(self) -> torch.Tensor:
-        r"""Return the hyperedge degree matrix :math:`\mathbf{D}_e^{-1}` with ``torch.sparse_coo_tensor`` format.
-        """
-        if self.cache.get("D_e_neg_1") is None:
-            _tmp = torch.sparse.sum(self.H_T, dim=1).to_dense().view(-1)
-            _num_e = _tmp.size(0)
-            _val = _tmp**-1
-            _val[torch.isinf(_val)] = 0
-
-            self.cache["D_e_neg_1"] = torch.sparse_csr_tensor(
-                torch.arange(0, _num_e + 1),
-                torch.arange(0, _num_e),
-                _val,
-                torch.Size([_num_e, _num_e]),
-                device=self.device,
-            )
-
-        return self.cache["D_e_neg_1"]
-
-    def D_e_neg_1_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the hyperedge degree matrix :math:`\mathbf{D}_e^{-1}` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("D_e_neg_1") is None:
-            _mat = self.D_e_of_group(group_name).clone()
-            _val = _mat._values() ** -1
-            _val[torch.isinf(_val)] = 0
-            self.group_cache[group_name]["D_e_neg_1"] = torch.sparse_coo_tensor(
-                _mat._indices(), _val, _mat.size(), device=self.device
-            ).coalesce()
-        return self.group_cache[group_name]["D_e_neg_1"]
-
-    def _fetch_H(self):
-        r"""Fetch the H matrix of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
-
-        Args:
-            ``direction`` (``str``): The direction of hyperedges can be either ``'v2e'`` or ``'e2v'``.
-            ``group_name`` (``str``): The name of the group.
-        """
-        # assert (
-        #     group_name in self.group_names
-        # ), f"The specified {group_name} is not in existing hyperedge groups."
-        # assert direction in ["v2e", "e2v"], "direction must be one of ['v2e', 'e2v']"
-        # if direction == "v2e":
-        #     select_idx = 0
-        # else:
-        #     select_idx = 1
-        if self.cache.get("main_H") is None:
-            num_e = len(self._raw_groups["main"])
-            self.cache["main_H"] = torch.sparse_coo_tensor(
-                ([self.cache["v_idx"], self.cache["e_idx"]]),
-                torch.ones(len(self.cache["v_idx"])),
-                torch.Size([self.num_v, num_e]),
-                device=self.device,
-            ).coalesce()
-
-        return self.cache["main_H"]
-
-    def N_e(self, v_idx: int) -> torch.Tensor:
-        r"""Return the neighbor hyperedges of the specified vertex with ``torch.Tensor`` format.
-
-        .. note::
-            The ``v_idx`` must be in the range of [0, :attr:`num_v`).
-
-        Parameters:
-            ``v_idx`` (``int``): The index of the vertex.
-        """
-        assert v_idx < self.num_v
-        _tmp, e_bias = [], 0
-        for name in self.group_names:
-            _tmp.append(self.N_e_of_group(v_idx, name) + e_bias)
-            e_bias += self.num_e_of_group(name)
-        return torch.cat(_tmp, dim=0)
-
-    def N_e_of_group(self, v_idx: int, group_name: str) -> torch.Tensor:
-        r"""Return the neighbor hyperedges of the specified vertex of the specified hyperedge group with ``torch.Tensor`` format.
-
-        .. note::
-            The ``v_idx`` must be in the range of [0, :attr:`num_v`).
-
-        Parameters:
-            ``v_idx`` (``int``): The index of the vertex.
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        assert v_idx < self.num_v
-        e_indices = self.H_of_group(group_name)[v_idx]._indices()[0]
-        return e_indices.clone()
-
-    def N_v(self, e_idx: int) -> torch.Tensor:
-        r"""Return the neighbor vertices of the specified hyperedge with ``torch.Tensor`` format.
-
-        .. note::
-            The ``e_idx`` must be in the range of [0, :attr:`num_e`).
-
-        Parameters:
-            ``e_idx`` (``int``): The index of the hyperedge.
-        """
-        assert e_idx < self.num_e
-        for name in self.group_names:
-            if e_idx < self.num_e_of_group(name):
-                return self.N_v_of_group(e_idx, name)
-            else:
-                e_idx -= self.num_e_of_group(name)
-
-    def N_v_of_group(self, e_idx: int, group_name: str) -> torch.Tensor:
-        r"""Return the neighbor vertices of the specified hyperedge of the specified hyperedge group with ``torch.Tensor`` format.
-
-        .. note::
-            The ``e_idx`` must be in the range of [0, :func:`num_e_of_group`).
-
-        Parameters:
-            ``e_idx`` (``int``): The index of the hyperedge.
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        assert e_idx < self.num_e_of_group(group_name)
-        v_indices = self.H_T_of_group(group_name)[e_idx]._indices()[0]
-        return v_indices.clone()
-
-    # =====================================================================================
-    # spectral-based convolution/smoothing
-    def smoothing(self, X: torch.Tensor, L: torch.Tensor, lamb: float) -> torch.Tensor:
-        return super().smoothing(X, L, lamb)
-
-    @property
-    def L_sym(self) -> torch.Tensor:
-        r"""Return the symmetric Laplacian matrix :math:`\mathcal{L}_{sym}` of the hypergraph with ``torch.sparse_coo_tensor`` format.
-
-        .. math::
-            \mathcal{L}_{sym} = \mathbf{I} - \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}}
-        """
-        if self.cache.get("L_sym") is None:
-            L_HGNN = self.L_HGNN.clone()
-            self.cache["L_sym"] = torch.sparse_coo_tensor(
-                torch.hstack(
-                    [
-                        torch.arange(0, self.num_v).view(1, -1).repeat(2, 1),
-                        L_HGNN.to_sparse_coo()._indices(),
-                    ]
-                ),
-                torch.hstack(
-                    [torch.ones(self.num_v), -L_HGNN.to_sparse_coo()._values()]
-                ),
-                torch.Size([self.num_v, self.num_v]),
-                device=self.device,
-            ).coalesce()
-        return self.cache["L_sym"]
-
-    def L_sym_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the symmetric Laplacian matrix :math:`\mathcal{L}_{sym}` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
-
-        .. math::
-            \mathcal{L}_{sym} = \mathbf{I} - \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}}
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("L_sym") is None:
-            L_HGNN = self.L_HGNN_of_group(group_name).clone()
-            self.group_cache[group_name]["L_sym"] = torch.sparse_coo_tensor(
-                torch.hstack(
-                    [
-                        torch.arange(0, self.num_v).view(1, -1).repeat(2, 1),
-                        L_HGNN._indices(),
-                    ]
-                ),
-                torch.hstack([torch.ones(self.num_v), -L_HGNN._values()]),
-                torch.Size([self.num_v, self.num_v]),
-                device=self.device,
-            ).coalesce()
-        return self.group_cache[group_name]["L_sym"]
-
-    @property
-    def L_rw(self) -> torch.Tensor:
-        r"""Return the random walk Laplacian matrix :math:`\mathcal{L}_{rw}` of the hypergraph with ``torch.sparse_coo_tensor`` format.
-
-        .. math::
-            \mathcal{L}_{rw} = \mathbf{I} - \mathbf{D}_v^{-1} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top
-        """
-        if self.cache.get("L_rw") is None:
-            _tmp = (
-                self.D_v_neg_1.mm(self.H).mm(self.W_e).mm(self.D_e_neg_1).mm(self.H_T)
-            )
-            self.cache["L_rw"] = (
-                torch.sparse_coo_tensor(
-                    torch.hstack(
-                        [
-                            torch.arange(0, self.num_v).view(1, -1).repeat(2, 1),
-                            _tmp._indices(),
-                        ]
-                    ),
-                    torch.hstack([torch.ones(self.num_v), -_tmp._values()]),
-                    torch.Size([self.num_v, self.num_v]),
-                    device=self.device,
-                )
-                .coalesce()
-                .clone()
-            )
-        return self.cache["L_rw"]
-
-    def L_rw_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the random walk Laplacian matrix :math:`\mathcal{L}_{rw}` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
-
-        .. math::
-            \mathcal{L}_{rw} = \mathbf{I} - \mathbf{D}_v^{-1} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("L_rw") is None:
-            _tmp = (
-                self.D_v_neg_1_of_group(group_name)
-                .mm(self.H_of_group(group_name))
-                .mm(
-                    self.W_e_of_group(group_name),
-                )
-                .mm(
-                    self.D_e_neg_1_of_group(group_name),
-                )
-                .mm(
-                    self.H_T_of_group(group_name),
-                )
-            )
-            self.group_cache[group_name]["L_rw"] = (
-                torch.sparse_coo_tensor(
-                    torch.hstack(
-                        [
-                            torch.arange(0, self.num_v).view(1, -1).repeat(2, 1),
-                            _tmp._indices(),
-                        ]
-                    ),
-                    torch.hstack([torch.ones(self.num_v), -_tmp._values()]),
-                    torch.Size([self.num_v, self.num_v]),
-                    device=self.device,
-                )
-                .coalesce()
-                .clone()
-            )
-        return self.group_cache[group_name]["L_rw"]
-
-    ## HGNN Laplacian smoothing
-    @property
-    def L_HGNN(self) -> torch.Tensor:
-        r"""Return the HGNN Laplacian matrix :math:`\mathcal{L}_{HGNN}` of the hypergraph with ``torch.sparse_coo_tensor`` format.
-
-        .. math::
-            \mathcal{L}_{HGNN} = \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}}
-        """
-        if self.cache.get("L_HGNN") is None:
-            _d_v_neg_1_2 = self.D_v_neg_1_2.to_sparse_coo()
-            _tmp = (
-                _d_v_neg_1_2
-                @ self.H
-                @ self.W_e
-                @ self.D_e_neg_1.to_sparse_coo()
-                @ self.H_T
-                @ _d_v_neg_1_2
-            )
-            self.cache["L_HGNN"] = _tmp.to_sparse_csr()
-        return self.cache["L_HGNN"]
-
-    def L_HGNN_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the HGNN Laplacian matrix :math:`\mathcal{L}_{HGNN}` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
-
-        .. math::
-            \mathcal{L}_{HGNN} = \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}}
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("L_HGNN") is None:
-            _tmp = (
-                self.D_v_neg_1_2_of_group(group_name)
-                .mm(self.H_of_group(group_name))
-                .mm(self.W_e_of_group(group_name))
-                .mm(
-                    self.D_e_neg_1_of_group(group_name),
-                )
-                .mm(
-                    self.H_T_of_group(group_name),
-                )
-                .mm(
-                    self.D_v_neg_1_2_of_group(group_name),
-                )
-            )
-            self.group_cache[group_name]["L_HGNN"] = _tmp.coalesce()
-        return self.group_cache[group_name]["L_HGNN"]
-
-    def smoothing_with_HGNN(
-        self, X: torch.Tensor, drop_rate: float = 0.0
-    ) -> torch.Tensor:
-        r"""Return the smoothed feature matrix with the HGNN Laplacian matrix :math:`\mathcal{L}_{HGNN}`.
-
-            .. math::
-                \mathbf{X} = \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}} \mathbf{X}
-
-        Parameters:
-            ``X`` (``torch.Tensor``): The feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
-        """
-        if self.device != X.device:
-            X = X.to(self.device)
-
-        if drop_rate > 0.0:
-            L_HGNN = sparse_dropout(self.L_HGNN, drop_rate)
-        else:
-            L_HGNN = self.L_HGNN
-        return L_HGNN.mm(X)
-
-    def smoothing_with_HGNN_of_group(
-        self, group_name: str, X: torch.Tensor, drop_rate: float = 0.0
-    ) -> torch.Tensor:
-        r"""Return the smoothed feature matrix with the HGNN Laplacian matrix :math:`\mathcal{L}_{HGNN}`.
-
-            .. math::
-                \mathbf{X} = \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}} \mathbf{X}
-
-        Parameters:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-            ``X`` (``torch.Tensor``): The feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.device != X.device:
-            X = X.to(self.device)
-        if drop_rate > 0.0:
-            L_HGNN = sparse_dropout(self.L_HGNN_of_group(group_name), drop_rate)
-        else:
-            L_HGNN = self.L_HGNN_of_group(group_name)
-        return L_HGNN.mm(X)
-
-    # =====================================================================================
-    # spatial-based convolution/message-passing
-    # general message passing functions
-    def v2e_aggregation(
-        self,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        v2e_weight: Optional[torch.Tensor] = None,
-        drop_rate: float = 0.0,
-    ):
-        r"""Message aggregation step of ``vertices to hyperedges``.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyperedges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
-        """
-        if self.device != X.device:
-            self.to(X.device)
-        if v2e_weight is None:
-            if drop_rate > 0.0:
-                P = sparse_dropout(self.H_T, drop_rate)
-            else:
-                P = self.H_T
-
-            if aggr == "mean":
-                X = torch.sparse.mm(P, X)
-                X = torch.sparse.mm(self.D_e_neg_1, X)
-            elif aggr == "sum":
-                X = torch.sparse.mm(P, X)
-            elif aggr == "softmax_then_sum":
-                P = torch.sparse.softmax(P, dim=1)
-                X = torch.sparse.mm(P, X)
-            else:
-                raise ValueError(f"Unknown aggregation method {aggr}.")
-        else:
-            # init message path
-            assert (
-                v2e_weight.shape[0] == self.v2e_weight.shape[0]
-            ), "The size of v2e_weight must be equal to the size of self.v2e_weight."
-            P = torch.sparse_coo_tensor(
-                self.H_T._indices(), v2e_weight, self.H_T.shape, device=self.device
-            )
-
-            if drop_rate > 0.0:
-                P = sparse_dropout(P, drop_rate)
-            # message passing
-            if aggr == "mean":
-                X = torch.sparse.mm(P, X)
-                D_e_neg_1 = torch.sparse.sum(P, dim=1).to_dense().view(-1, 1)
-                D_e_neg_1[torch.isinf(D_e_neg_1)] = 0
-                X = torch.sparse.mm(D_e_neg_1, X)
-            elif aggr == "sum":
-                X = torch.sparse.mm(P, X)
-            elif aggr == "softmax_then_sum":
-                P = torch.sparse.softmax(P, dim=1)
-                X = torch.sparse.mm(P, X)
-            else:
-                raise ValueError(f"Unknown aggregation method {aggr}.")
-        return X
-
-    def v2e_aggregation_of_group(
-        self,
-        group_name: str,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        v2e_weight: Optional[torch.Tensor] = None,
-        drop_rate: float = 0.0,
-    ):
-        r"""Message aggregation step of ``vertices to hyperedges`` in specified hyperedge group.
-
-        Parameters:
-            ``group_name`` (``str``): The specified hyperedge group.
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyperedges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        assert aggr in ["mean", "sum", "softmax_then_sum"]
-        if self.device != X.device:
-            self.to(X.device)
-        if v2e_weight is None:
-            if drop_rate > 0.0:
-                P = sparse_dropout(self.H_T_of_group(group_name), drop_rate)
-            else:
-                P = self.H_T_of_group(group_name)
-            if aggr == "mean":
-                X = torch.sparse.mm(P, X)
-                X = torch.sparse.mm(self.D_e_neg_1_of_group(group_name), X)
-            elif aggr == "sum":
-                X = torch.sparse.mm(P, X)
-            elif aggr == "softmax_then_sum":
-                P = torch.sparse.softmax(P, dim=1)
-                X = torch.sparse.mm(P, X)
-            else:
-                raise ValueError(f"Unknown aggregation method {aggr}.")
-        else:
-            # init message path
-            assert (
-                v2e_weight.shape[0] == self.v2e_weight_of_group(group_name).shape[0]
-            ), (
-                "The size of v2e_weight must be equal to the size of"
-                f" self.v2e_weight_of_group('{group_name}')."
-            )
-            P = torch.sparse_coo_tensor(
-                self.H_T_of_group(group_name)._indices(),
-                v2e_weight,
-                self.H_T_of_group(group_name).shape,
-                device=self.device,
-            )
-            if drop_rate > 0.0:
-                P = sparse_dropout(P, drop_rate)
-            # message passing
-            if aggr == "mean":
-                X = torch.sparse.mm(P, X)
-                D_e_neg_1 = torch.sparse.sum(P, dim=1).to_dense().view(-1, 1)
-                D_e_neg_1[torch.isinf(D_e_neg_1)] = 0
-                X = D_e_neg_1 * X
-            elif aggr == "sum":
-                X = torch.sparse.mm(P, X)
-            elif aggr == "softmax_then_sum":
-                P = torch.sparse.softmax(P, dim=1)
-                X = torch.sparse.mm(P, X)
-            else:
-                raise ValueError(f"Unknown aggregation method {aggr}.")
-        return X
-
-    def v2e_update(self, X: torch.Tensor, e_weight: Optional[torch.Tensor] = None):
-        r"""Message update step of ``vertices to hyperedges``.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
-            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-        if self.device != X.device:
-            self.to(X.device)
-        if e_weight is None:
-            X = torch.sparse.mm(self.W_e, X)
-        else:
-            e_weight = e_weight.view(-1, 1)
-            assert (
-                e_weight.shape[0] == self.num_e
-            ), "The size of e_weight must be equal to the size of self.num_e."
-            X = e_weight * X
-        return X
-
-    def v2e_update_of_group(
-        self, group_name: str, X: torch.Tensor, e_weight: Optional[torch.Tensor] = None
-    ):
-        r"""Message update step of ``vertices to hyperedges`` in specified hyperedge group.
-
-        Parameters:
-            ``group_name`` (``str``): The specified hyperedge group.
-            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
-            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.device != X.device:
-            self.to(X.device)
-        if e_weight is None:
-            X = torch.sparse.mm(self.W_e_of_group(group_name), X)
-        else:
-            e_weight = e_weight.view(-1, 1)
-            assert e_weight.shape[0] == self.num_e_of_group(group_name), (
-                "The size of e_weight must be equal to the size of"
-                f" self.num_e_of_group('{group_name}')."
-            )
-            X = e_weight * X
-        return X
-
-    def v2e(
-        self,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        v2e_weight: Optional[torch.Tensor] = None,
-        e_weight: Optional[torch.Tensor] = None,
-        drop_rate: float = 0.0,
-    ):
-        r"""Message passing of ``vertices to hyperedges``. The combination of ``v2e_aggregation`` and ``v2e_update``.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyperedges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
-        """
-
-        X = self.v2e_aggregation(X, aggr, v2e_weight, drop_rate=drop_rate)
-        X = self.v2e_update(X, e_weight)
-        return X
-
-    def v2e_of_group(
-        self,
-        group_name: str,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        v2e_weight: Optional[torch.Tensor] = None,
-        e_weight: Optional[torch.Tensor] = None,
-        drop_rate: float = 0.0,
-    ):
-        r"""Message passing of ``vertices to hyperedges`` in specified hyperedge group. The combination of ``e2v_aggregation_of_group`` and ``e2v_update_of_group``.
-
-        Parameters:
-            ``group_name`` (``str``): The specified hyperedge group.
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyperedges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        X = self.v2e_aggregation_of_group(
-            group_name, X, aggr, v2e_weight, drop_rate=drop_rate
-        )
-        X = self.v2e_update_of_group(group_name, X, e_weight)
-        return X
-
-    def e2v_aggregation(
-        self,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        e2v_weight: Optional[torch.Tensor] = None,
-        drop_rate: float = 0.0,
-    ):
-        r"""Message aggregation step of ``hyperedges to vertices``.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
-        """
-        if self.device != X.device:
-            self.to(X.device)
-        if e2v_weight is None:
-            if drop_rate > 0.0:
-                P = sparse_dropout(self.H, drop_rate)
-            else:
-                P = self.H
-            if aggr == "mean":
-                X = torch.sparse.mm(P, X)
-                X = torch.sparse.mm(self.D_v_neg_1, X)
-            elif aggr == "sum":
-                X = torch.sparse.mm(P, X)
-            elif aggr == "softmax_then_sum":
-                P = torch.sparse.softmax(P, dim=1)
-                X = torch.sparse.mm(P, X)
-            else:
-                raise ValueError(f"Unknown aggregation method: {aggr}")
-        else:
-            # init message path
-            assert (
-                e2v_weight.shape[0] == self.e2v_weight.shape[0]
-            ), "The size of e2v_weight must be equal to the size of self.e2v_weight."
-            P = torch.sparse_coo_tensor(
-                self.H._indices(), e2v_weight, self.H.shape, device=self.device
-            ).coalesce()
-
-            if drop_rate > 0.0:
-                P = sparse_dropout(P, drop_rate)
-            # message passing
-            if aggr == "mean":
-                X = torch.sparse.mm(P, X)
-                D_v_neg_1 = torch.sparse.sum(P, dim=1).to_dense().view(-1, 1)
-                D_v_neg_1[torch.isinf(D_v_neg_1)] = 0
-                X = D_v_neg_1 * X
-            elif aggr == "sum":
-                X = torch.sparse.mm(P, X)
-            elif aggr == "softmax_then_sum":
-                P = torch.sparse.softmax(P, dim=1)
-                X = torch.sparse.mm(P, X)
-            else:
-                raise ValueError(f"Unknown aggregation method: {aggr}")
-        return X
-
-    def e2v_aggregation_of_group(
-        self,
-        group_name: str,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        e2v_weight: Optional[torch.Tensor] = None,
-        drop_rate: float = 0.0,
-    ):
-        r"""Message aggregation step of ``hyperedges to vertices`` in specified hyperedge group.
-
-        Parameters:
-            ``group_name`` (``str``): The specified hyperedge group.
-            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        assert aggr in ["mean", "sum", "softmax_then_sum"]
-        if self.device != X.device:
-            self.to(X.device)
-        if e2v_weight is None:
-            if drop_rate > 0.0:
-                P = sparse_dropout(self.H_of_group(group_name), drop_rate)
-            else:
-                P = self.H_of_group(group_name)
-            if aggr == "mean":
-                X = torch.sparse.mm(P, X)
-                X = torch.sparse.mm(self.D_v_neg_1_of_group[group_name], X)
-            elif aggr == "sum":
-                X = torch.sparse.mm(P, X)
-            elif aggr == "softmax_then_sum":
-                P = torch.sparse.softmax(P, dim=1)
-                X = torch.sparse.mm(P, X)
-            else:
-                raise ValueError(f"Unknown aggregation method: {aggr}")
-        else:
-            # init message path
-            assert (
-                e2v_weight.shape[0] == self.e2v_weight_of_group[group_name].shape[0]
-            ), (
-                "The size of e2v_weight must be equal to the size of"
-                f" self.e2v_weight_of_group('{group_name}')."
-            )
-            P = torch.sparse_coo_tensor(
-                self.H_of_group[group_name]._indices(),
-                e2v_weight,
-                self.H_of_group[group_name].shape,
-                device=self.device,
-            )
-            if drop_rate > 0.0:
-                P = sparse_dropout(P, drop_rate)
-            # message passing
-            if aggr == "mean":
-                X = torch.sparse.mm(P, X)
-                D_v_neg_1 = torch.sparse.sum(P, dim=1).to_dense().view(-1, 1)
-                D_v_neg_1[torch.isinf(D_v_neg_1)] = 0
-                X = D_v_neg_1 * X
-            elif aggr == "sum":
-                X = torch.sparse.mm(P, X)
-            elif aggr == "softmax_then_sum":
-                P = torch.sparse.softmax(P, dim=1)
-                X = torch.sparse.mm(P, X)
-            else:
-                raise ValueError(f"Unknown aggregation method: {aggr}")
-        return X
-
-    def e2v_update(self, X: torch.Tensor):
-        r"""Message update step of ``hyperedges to vertices``.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-        """
-        if self.device != X.device:
-            self.to(X.device)
-        return X
-
-    def e2v_update_of_group(self, group_name: str, X: torch.Tensor):
-        r"""Message update step of ``hyperedges to vertices`` in specified hyperedge group.
-
-        Parameters:
-            ``group_name`` (``str``): The specified hyperedge group.
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.device != X.device:
-            self.to(X.device)
-        return X
-
-    def e2v(
-        self,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        e2v_weight: Optional[torch.Tensor] = None,
-        drop_rate: float = 0.0,
-    ):
-        r"""Message passing of ``hyperedges to vertices``. The combination of ``e2v_aggregation`` and ``e2v_update``.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
-        """
-        X = self.e2v_aggregation(X, aggr, e2v_weight, drop_rate=drop_rate)
-        X = self.e2v_update(X)
-        return X
-
-    def e2v_of_group(
-        self,
-        group_name: str,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        e2v_weight: Optional[torch.Tensor] = None,
-        drop_rate: float = 0.0,
-    ):
-        r"""Message passing of ``hyperedges to vertices`` in specified hyperedge group. The combination of ``e2v_aggregation_of_group`` and ``e2v_update_of_group``.
-
-        Parameters:
-            ``group_name`` (``str``): The specified hyperedge group.
-            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        X = self.e2v_aggregation_of_group(
-            group_name, X, aggr, e2v_weight, drop_rate=drop_rate
-        )
-        X = self.e2v_update_of_group(group_name, X)
-        return X
-
-    def v2v(
-        self,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        drop_rate: float = 0.0,
-        v2e_aggr: Optional[str] = None,
-        v2e_weight: Optional[torch.Tensor] = None,
-        v2e_drop_rate: Optional[float] = None,
-        e_weight: Optional[torch.Tensor] = None,
-        e2v_aggr: Optional[str] = None,
-        e2v_weight: Optional[torch.Tensor] = None,
-        e2v_drop_rate: Optional[float] = None,
-    ):
-        r"""Message passing of ``vertices to vertices``. The combination of ``v2e`` and ``e2v``.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, this ``aggr`` will be used to both ``v2e`` and ``e2v``.
-            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
-            ``v2e_aggr`` (``str``, optional): The aggregation method for hyperedges to vertices. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``e2v``.
-            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyperedges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``v2e_drop_rate`` (``float``, optional): Dropout rate for hyperedges to vertices. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. If specified, it will override the ``drop_rate`` in ``e2v``. Default: ``None``.
-            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``e2v_aggr`` (``str``, optional): The aggregation method for vertices to hyperedges. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``v2e``.
-            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``e2v_drop_rate`` (``float``, optional): Dropout rate for vertices to hyperedges. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. If specified, it will override the ``drop_rate`` in ``v2e``. Default: ``None``.
-        """
-        if v2e_aggr is None:
-            v2e_aggr = aggr
-        if e2v_aggr is None:
-            e2v_aggr = aggr
-        if v2e_drop_rate is None:
-            v2e_drop_rate = drop_rate
-        if e2v_drop_rate is None:
-            e2v_drop_rate = drop_rate
-
-        X = self.v2e(X, v2e_aggr, v2e_weight, e_weight, drop_rate=v2e_drop_rate)
-        X = self.e2v(X, e2v_aggr, e2v_weight, drop_rate=e2v_drop_rate)
-
-        return X
-
-    def v2v_of_group(
-        self,
-        group_name: str,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        drop_rate: float = 0.0,
-        v2e_aggr: Optional[str] = None,
-        v2e_weight: Optional[torch.Tensor] = None,
-        v2e_drop_rate: Optional[float] = None,
-        e_weight: Optional[torch.Tensor] = None,
-        e2v_aggr: Optional[str] = None,
-        e2v_weight: Optional[torch.Tensor] = None,
-        e2v_drop_rate: Optional[float] = None,
-    ):
-        r"""Message passing of ``vertices to vertices`` in specified hyperedge group. The combination of ``v2e_of_group`` and ``e2v_of_group``.
-
-        Parameters:
-            ``group_name`` (``str``): The specified hyperedge group.
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, this ``aggr`` will be used to both ``v2e_of_group`` and ``e2v_of_group``.
-            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
-            ``v2e_aggr`` (``str``, optional): The aggregation method for hyperedges to vertices. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``e2v_of_group``.
-            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyperedges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``v2e_drop_rate`` (``float``, optional): Dropout rate for hyperedges to vertices. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. If specified, it will override the ``drop_rate`` in ``e2v_of_group``. Default: ``None``.
-            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``e2v_aggr`` (``str``, optional): The aggregation method for vertices to hyperedges. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``v2e_of_group``.
-            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``e2v_drop_rate`` (``float``, optional): Dropout rate for vertices to hyperedges. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. If specified, it will override the ``drop_rate`` in ``v2e_of_group``. Default: ``None``.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if v2e_aggr is None:
-            v2e_aggr = aggr
-        if e2v_aggr is None:
-            e2v_aggr = aggr
-        if v2e_drop_rate is None:
-            v2e_drop_rate = drop_rate
-        if e2v_drop_rate is None:
-            e2v_drop_rate = drop_rate
-        X = self.v2e_of_group(
-            group_name, X, v2e_aggr, v2e_weight, e_weight, drop_rate=v2e_drop_rate
-        )
-        X = self.e2v_of_group(
-            group_name, X, e2v_aggr, e2v_weight, drop_rate=e2v_drop_rate
-        )
-        return X
-
-    def get_linegraph(self, s=1, weight=True):
-        """
-        Get the linegraph of the hypergraph based on the clique expansion.
-        The edges will be the vertices of the line
-        graph. Two vertices are connected by an s-line-graph edge if the
-        corresponding hypergraph edges intersect in at least s hypergraph nodes.
-
-
-        Parameters
-        ----------
-        s : Two vertices are connected if the nodes they correspond to share
-        at least s incident hyper edges.
-        edge : If edges=True (default)then the edges will be the vertices of the line
-        graph. Two vertices are connected by an s-line-graph edge if the
-        corresponding hypergraph edges intersect in at least s hypergraph nodes.
-        If edges=False, the hypergraph nodes will be the vertices of the line
-        graph.
-        weight :
-
-        Returns
-        -------
-            Graph: easygraph.Graph, the linegraph of the hypergraph.
-
-        """
-        edge_adjacency = self.edge_adjacency_matrix(s=s, weight=weight)
-        graph = eg.from_scipy_sparse_matrix(edge_adjacency)
-        return graph
-
-    def get_clique_expansion(self, s=1, weight=True):
-        """
-        Get the linegraph of the hypergraph based on the clique expansion.
-        The hypergraph nodes will be the vertices of the line
-        graph. Two vertices are connected if the nodes they correspond to share
-        at least s incident hyper edges.
-
-        Parameters
-        ----------
-        s : Two vertices are connected if the nodes they correspond to share
-        at least s incident hyper edges.
-        edge : If edges=True (default)then the edges will be the vertices of the line
-        graph. Two vertices are connected by an s-line-graph edge if the
-        corresponding hypergraph edges intersect in at least s hypergraph nodes.
-        If edges=False, the hypergraph nodes will be the vertices of the line
-        graph.
-        weight :
-
-        Returns
-        -------
-            Graph: easygraph.Graph, the clique expansion of the hypergraph.
-
-        """
-
-        if self.cache.get("clique_expansion") is None:
-            A = self.adjacency_matrix(s=s, weight=weight)
-            graph = eg.Graph()
-            A = np.array(np.nonzero(A))
-            e1 = np.array([idx for idx in A[0]])
-            e2 = np.array([idx for idx in A[1]])
-            A = np.array([e1, e2]).T
-            graph.add_edges_from(A)
-            graph.add_nodes(list(range(0, self.num_v)))
-            self.cache["clique_expansion"] = graph
-
-        return self.cache["clique_expansion"]
-
-    def s_connected_components(self, s=1, edges=True, return_singletons=False):
-        """
-        Returns a generator for the :term:`s-edge-connected components
-        <s-edge-connected component>`
-        or the :term:`s-node-connected components <s-connected component,
-        s-node-connected component>` of the hypergraph.
-
-        Parameters
-        ----------
-        s : int, optional, default 1
-
-        edges : boolean, optional, default = True
-            If True will return edge components, if False will return node
-            components
-        return_singletons : bool, optional, default = False
-
-        Notes
-        -----
-        If edges=True, this method returns the s-edge-connected components as
-        lists of lists of edge uids.
-        An s-edge-component has the property that for any two edges e1 and e2
-        there is a sequence of edges starting with e1 and ending with e2
-        such that pairwise adjacent edges in the sequence intersect in at least
-        s nodes. If s=1 these are the path components of the hypergraph.
-
-        If edges=False this method returns s-node-connected components.
-        A list of sets of uids of the nodes which are s-walk connected.
-        Two nodes v1 and v2 are s-walk-connected if there is a
-        sequence of nodes starting with v1 and ending with v2 such that
-        pairwise adjacent nodes in the sequence share s edges. If s=1 these
-        are the path components of the hypergraph.
-
-        Example
-        -------
-            >>> S = {'A':{1,2,3},'B':{2,3,4},'C':{5,6},'D':{6}}
-            >>> H = Hypergraph(S)
-
-            >>> list(H.s_components(edges=True))
-            [{'C', 'D'}, {'A', 'B'}]
-            >>> list(H.s_components(edges=False))
-            [{1, 2, 3, 4}, {5, 6}]
-
-        Yields
-        ------
-        s_connected_components : iterator
-            Iterator returns sets of uids of the edges (or nodes) in the
-            s-edge(node) components of hypergraph.
-
-        """
-        if not edges:
-            g = self.get_clique_expansion()
-        else:
-            g = self.get_linegraph(s)
-        for c in eg.connected_components(g):
-            if not return_singletons and len(c) == 1:
-                continue
-            yield c
+import pickle
+import random
+
+from copy import deepcopy
+from pathlib import Path
+from typing import TYPE_CHECKING
+from typing import Any
+from typing import Dict
+from typing import List
+from typing import Optional
+from typing import Tuple
+from typing import Union
+
+import easygraph as eg
+import numpy as np
+import torch
+
+from easygraph.classes.base import BaseHypergraph
+from easygraph.functions.drawing import draw_hypergraph
+from easygraph.utils.exception import EasyGraphError
+from easygraph.utils.sparse import sparse_dropout
+from scipy.sparse import csr_array
+from scipy.sparse import csr_matrix
+
+
+# from torch_sparse import spmm
+
+
+if TYPE_CHECKING:
+    from easygraph import Graph
+
+__all__ = ["Hypergraph"]
+
+
+class Hypergraph(BaseHypergraph):
+
+    """
+    The ``Hypergraph`` class is developed for hypergraph structures.
+    Please notice that node id in hypergraph is in [0, num_v)
+
+    Parameters
+    ----------
+        num_v  : (int) The number of vertices in the hypergraph
+        e_list : (Union[List[int], List[List[int]]], optional) A list of hyperedges describes how the vertices point to the hyperedges. Defaults to ``None``
+        e_weight : (Union[float, List[float]], optional)  A list of weights for hyperedges. If set to None, the value ``1`` is used for all hyperedges. Defaults to None
+        merge_op : (str) The operation to merge those conflicting hyperedges in the same hyperedge group, which can be ``'mean'``, ``'sum'`` or ``'max'``. Defaults to ``'mean'``
+        device : (torch.device, optional) The device to store the hypergraph. Defaults to torch.device('cpu')
+
+
+    """
+
+    gnn_data_dict_factory = dict
+    degree_data_dict = dict
+
+    def __init__(
+        self,
+        num_v: int,
+        v_property: Optional[Union[Dict, List[Dict]]] = None,
+        e_list: Optional[Union[List[int], List[List[int]]]] = None,
+        e_weight: Optional[Union[float, List[float]]] = None,
+        e_property: Optional[Union[Dict, List[Dict]]] = None,
+        merge_op: str = "mean",
+        device: torch.device = torch.device("cpu"),
+    ):
+        super().__init__(
+            num_v,
+            e_list=e_list,
+            v_property=v_property,
+            e_property=e_property,
+            device=device,
+        )
+
+        self._ndata = self.gnn_data_dict_factory()
+        self.deg_v_dict = self.degree_data_dict()
+        self.n_e_dict = {}
+        self.edge_index = -1
+
+        for i in range(num_v):
+            self.deg_v_dict[i] = 0
+            self.n_e_dict[i] = []
+
+        if e_list is not None:
+            self.add_hyperedges(
+                e_list=e_list,
+                e_weight=e_weight,
+                e_property=e_property,
+                merge_op=merge_op,
+            )
+        edges_col = []
+        indptr_list = []
+        ptr = 0
+        for v in self.n_e_dict.values():
+            edges_col.extend(v)
+            indptr_list.append(ptr)
+            ptr += len(v)
+        indptr_list.append(ptr)
+
+        e_idx, v_idx = [], []
+        for n, e in self.n_e_dict.items():
+            v_idx.extend([n] * len(e))
+            e_idx.extend(e)
+        self.cache["e_idx"] = e_idx
+        self.cache["v_idx"] = v_idx
+
+        self.cache["edges_col"] = np.array(edges_col)
+        self.cache["indptr_list"] = np.array(indptr_list)
+
+    def __repr__(self) -> str:
+        r"""Print the hypergraph information."""
+        return f"Hypergraph(num_vertex={self.num_v}, num_hyperedge={self.num_e})"
+
+    @property
+    def ndata(self):
+        return self._ndata
+
+    @property
+    def state_dict(self) -> Dict[str, Any]:
+        r"""Get the state dict of the hypergraph."""
+        return {
+            "num_v": self.num_v,
+            "v_property": self.v_property,
+            "e_property": self.e_property,
+            "raw_groups": self._raw_groups,
+            "deg_v_dict": self.deg_v_dict,
+        }
+
+    def unique_edge_sizes(self):
+        """A function that returns the unique edge sizes.
+
+        Returns
+        -------
+        list()
+            The unique edge sizes in ascending order by size.
+        """
+        edge_size_set = set()
+        edge_lst = self.e[0]
+        for e in edge_lst:
+            edge_size_set.add(len(e))
+
+        return sorted(edge_size_set)
+
+    def is_uniform(self):
+        """Order of uniformity if the hypergraph is uniform, or False.
+
+        A hypergraph is uniform if all its edges have the same order.
+
+        Returns d if the hypergraph is d-uniform, that is if all edges
+        in the hypergraph (excluding singletons) have the same degree d.
+        Returns False if not uniform.
+
+        Returns
+        -------
+        d : int or False
+            If the hypergraph is d-uniform, return d, or False otherwise.
+
+        Examples
+        --------
+        This function can be used as a boolean check:
+
+        >>> import easygraph as eg
+        >>> H = eg.Hypergraph(v_num = 5, e_list = [(0, 1, 2), (1, 2, 3), (2, 3, 4)])
+        >>> H.is_uniform()
+        2
+        """
+        edge_sizes = self.unique_edge_sizes()
+        if 1 in edge_sizes:
+            edge_sizes.remove(1)
+
+        if edge_sizes is None or len(edge_sizes) != 1:
+            return False
+
+        # order of all edges
+        return edge_sizes.pop()
+
+    def save(self, file_path: Union[str, Path]):
+        r"""Save the EasyGraph's hypergraph structure a file.
+
+        Parameters:
+            ``file_path`` (``Union[str, Path]``): The file path to store the EasyGraph's hypergraph structure.
+        """
+        file_path = Path(file_path)
+        assert file_path.parent.exists(), "The directory does not exist."
+        data = {
+            "class": "Hypergraph",
+            "state_dict": self.state_dict,
+        }
+        with open(file_path, "wb") as fp:
+            pickle.dump(data, fp)
+
+    @staticmethod
+    def load(file_path: Union[str, Path]):
+        r"""Load the EasyGraph's hypergraph structure from a file.
+
+        Parameters:
+            ``file_path`` (``Union[str, Path]``): The file path to load the EasyGraph's hypergraph structure.
+        """
+        file_path = Path(file_path)
+        assert file_path.exists(), "The file does not exist."
+        with open(file_path, "rb") as fp:
+            data = pickle.load(fp)
+        assert (
+            data["class"] == "Hypergraph"
+        ), "The file is not a EasyGraph's hypergraph file."
+        return Hypergraph.from_state_dict(data["state_dict"])
+
+    def draw(
+        self,
+        e_style: str = "circle",
+        v_label: Optional[List[str]] = None,
+        v_size: Union[float, list] = 1.0,
+        v_color: Union[str, list] = "r",
+        v_line_width: Union[str, list] = 1.0,
+        e_color: Union[str, list] = "gray",
+        e_fill_color: Union[str, list] = "whitesmoke",
+        e_line_width: Union[str, list] = 1.0,
+        font_size: float = 1.0,
+        font_family: str = "sans-serif",
+        push_v_strength: float = 1.0,
+        push_e_strength: float = 1.0,
+        pull_e_strength: float = 1.0,
+        pull_center_strength: float = 1.0,
+    ):
+        r"""Draw the hypergraph structure.
+
+        Parameters:
+            ``e_style`` (``str``): The style of hyperedges. The available styles are only ``'circle'``. Defaults to ``'circle'``.
+            ``v_label`` (``list``): The labels of vertices. Defaults to ``None``.
+            ``v_size`` (``float`` or ``list``): The size of vertices. Defaults to ``1.0``.
+            ``v_color`` (``str`` or ``list``): The `color <https://matplotlib.org/stable/gallery/color/named_colors.html>`_ of vertices. Defaults to ``'r'``.
+            ``v_line_width`` (``float`` or ``list``): The line width of vertices. Defaults to ``1.0``.
+            ``e_color`` (``str`` or ``list``): The `color <https://matplotlib.org/stable/gallery/color/named_colors.html>`_ of hyperedges. Defaults to ``'gray'``.
+            ``e_fill_color`` (``str`` or ``list``): The fill `color <https://matplotlib.org/stable/gallery/color/named_colors.html>`_ of hyperedges. Defaults to ``'whitesmoke'``.
+            ``e_line_width`` (``float`` or ``list``): The line width of hyperedges. Defaults to ``1.0``.
+            ``font_size`` (``float``): The font size of labels. Defaults to ``1.0``.
+            ``font_family`` (``str``): The font family of labels. Defaults to ``'sans-serif'``.
+            ``push_v_strength`` (``float``): The strength of pushing vertices. Defaults to ``1.0``.
+            ``push_e_strength`` (``float``): The strength of pushing hyperedges. Defaults to ``1.0``.
+            ``pull_e_strength`` (``float``): The strength of pulling hyperedges. Defaults to ``1.0``.
+            ``pull_center_strength`` (``float``): The strength of pulling vertices to the center. Defaults to ``1.0``.
+        """
+        draw_hypergraph(
+            self,
+            e_style,
+            v_label,
+            v_size,
+            v_color,
+            v_line_width,
+            e_color,
+            e_fill_color,
+            e_line_width,
+            font_size,
+            font_family,
+            push_v_strength,
+            push_e_strength,
+            pull_e_strength,
+            pull_center_strength,
+        )
+
+    def clear(self):
+        r"""Clear all hyperedges and caches from the hypergraph."""
+
+        super().clear()
+        self.deg_v_dict = {}
+        self._ndata = {}
+
+    def clone(self) -> "Hypergraph":
+        r"""Return a copy of the hypergraph."""
+        hg = Hypergraph(self.num_v, device=self.device)
+        hg._raw_groups = deepcopy(self._raw_groups)
+        hg.cache = deepcopy(self.cache)
+        hg.group_cache = deepcopy(self.group_cache)
+        hg.deg_v_dict = deepcopy(self.deg_v_dict)
+        return hg
+
+    def to(self, device: torch.device):
+        r"""Move the hypergraph to the specified device.
+
+        Parameters:
+            ``device`` (``torch.device``): The target device.
+        """
+        return super().to(device)
+
+    # =====================================================================================
+    # some construction functions
+    @staticmethod
+    def from_state_dict(state_dict: dict):
+        r"""Load the hypergraph from the state dict.
+
+        Parameters:
+            ``state_dict`` (``dict``): The state dict to load the hypergraph.
+        """
+        _hg = Hypergraph(state_dict["num_v"])
+        _hg._raw_groups = deepcopy(state_dict["raw_groups"])
+        _hg._e_property = deepcopy(state_dict["e_property"])
+        _hg._v_property = deepcopy(state_dict["v_property"])
+        _hg.deg_v_dict = deepcopy(state_dict["deg_v_dict"])
+        return _hg
+
+    @staticmethod
+    def _e_list_from_feature_kNN(features: torch.Tensor, k: int):
+        import scipy
+
+        r"""Construct hyperedges from the feature matrix. Each hyperedge in the hypergraph is constructed by the central vertex and its :math:`k-1` neighbor vertices.
+
+        Parameters:
+            ``features`` (``torch.Tensor``): The feature matrix.
+            ``k`` (``int``): The number of nearest neighbors.
+        """
+        features = features.cpu().numpy()
+        assert features.ndim == 2, "The feature matrix should be 2-D."
+        assert k <= features.shape[0], (
+            "The number of nearest neighbors should be less than or equal to the number"
+            " of vertices."
+        )
+        tree = scipy.spatial.cKDTree(features)
+        _, nbr_array = tree.query(features, k=k)
+        return nbr_array.tolist()
+
+    @staticmethod
+    def from_feature_kNN(
+        features: torch.Tensor, k: int, device: torch.device = torch.device("cpu")
+    ):
+        r"""Construct the hypergraph from the feature matrix. Each hyperedge in the hypergraph is constructed by the central vertex and its :math:`k-1` neighbor vertices.
+
+        .. note::
+            The constructed hypergraph is a k-uniform hypergraph. If the feature matrix has the size :math:`N \times C`, the number of vertices and hyperedges of the constructed hypergraph are both :math:`N`.
+
+        Parameters:
+            ``features`` (``torch.Tensor``): The feature matrix.
+            ``k`` (``int``): The number of nearest neighbors.
+            ``device`` (``torch.device``, optional): The device to store the hypergraph. Defaults to ``torch.device('cpu')``.
+        """
+        e_list = Hypergraph._e_list_from_feature_kNN(features, k)
+        hg = Hypergraph(num_v=features.shape[0], e_list=e_list, device=device)
+        return hg
+
+    @staticmethod
+    def from_graph(graph, device: torch.device = torch.device("cpu")) -> "Hypergraph":
+        r"""Construct the hypergraph from the graph. Each edge in the graph is treated as a hyperedge in the constructed hypergraph.
+
+        .. note::
+            The constructed hypergraph is a 2-uniform hypergraph, and has the same number of vertices and edges/hyperedges as the graph.
+
+        Parameters:
+            ``graph`` (``eg.Graph``): The graph to construct the hypergraph.
+            ``device`` (``torch.device``, optional): The device to store the hypergraph. Defaults to ``torch.device('cpu')``.
+        """
+        e_list, e_weight, v_property, e_property = graph.e
+        hg = Hypergraph(
+            num_v=len(graph.nodes),
+            e_list=e_list,
+            e_weight=e_weight,
+            v_property=v_property,
+            e_property=e_property,
+            device=device,
+        )
+        return hg
+
+    @staticmethod
+    def _e_list_from_graph_kHop(
+        graph,
+        k: int,
+        only_kHop: bool = False,
+    ) -> List[tuple]:
+        r"""Construct the hyperedge list from the graph by k-Hop neighbors. Each hyperedge in the hypergraph is constructed by the central vertex and its :math:`k`-Hop neighbor vertices.
+
+        .. note::
+            If the graph have :math:`|\mathcal{V}|` vertices, the constructed hypergraph will have :math:`|\mathcal{V}|` vertices and equal to or less than :math:`|\mathcal{V}|` hyperedges.
+
+        Parameters:
+            ``graph`` (``eg.Graph``): The graph to construct the hypergraph.
+            ``k`` (``int``): The number of hop neighbors.
+            ``only_kHop`` (``bool``, optional): If set to ``True``, only the central vertex and its :math:`k`-th Hop neighbors are used to construct the hyperedges. By default, the constructed hyperedge will include the central vertex and its [ :math:`1`-th, :math:`2`-th, :math:`\cdots`, :math:`k`-th ] Hop neighbors. Defaults to ``False``.
+        """
+        assert (
+            k >= 1
+        ), "The number of hop neighbors should be larger than or equal to 1."
+        A_1, A_k = graph.A.clone(), graph.A.clone()
+        A_history = []
+        for _ in range(k - 1):
+            A_k = torch.sparse.mm(A_k, A_1)
+            if not only_kHop:
+                A_history.append(A_k.clone())
+        if not only_kHop:
+            A_k = A_1
+            for A_ in A_history:
+                A_k = A_k + A_
+        e_list = [
+            tuple(set([v_idx] + A_k[v_idx]._indices().cpu().squeeze(0).tolist()))
+            for v_idx in range(len(graph.nodes))
+        ]
+        return e_list
+
+    @staticmethod
+    def from_graph_kHop(
+        graph,
+        k: int,
+        only_kHop: bool = False,
+        device: torch.device = torch.device("cpu"),
+    ) -> "Hypergraph":
+        r"""Construct the hypergraph from the graph by k-Hop neighbors. Each hyperedge in the hypergraph is constructed by the central vertex and its :math:`k`-Hop neighbor vertices.
+
+        .. note::
+            If the graph have :math:`|\mathcal{V}|` vertices, the constructed hypergraph will have :math:`|\mathcal{V}|` vertices and equal to or less than :math:`|\mathcal{V}|` hyperedges.
+
+        Parameters:
+            ``graph`` (``eg.Graph``): The graph to construct the hypergraph.
+            ``k`` (``int``): The number of hop neighbors.
+            ``only_kHop`` (``bool``): If set to ``True``, only the central vertex and its :math:`k`-th Hop neighbors are used to construct the hyperedges. By default, the constructed hyperedge will include the central vertex and its [ :math:`1`-th, :math:`2`-th, :math:`\cdots`, :math:`k`-th ] Hop neighbors. Defaults to ``False``.
+            ``device`` (``torch.device``, optional): The device to store the hypergraph. Defaults to ``torch.device('cpu')``.
+        """
+        e_list = Hypergraph._e_list_from_graph_kHop(graph, k, only_kHop)
+        hg = Hypergraph(num_v=len(graph.nodes), e_list=e_list, device=device)
+        return hg
+
+    def isOutRange(self, id):
+        if id >= self.num_v or id < 0:
+            return False
+        return True
+
+    def add_hyperedges(
+        self,
+        e_list: Union[List[int], List[List[int]]],
+        e_weight: Optional[Union[float, List[float]]] = None,
+        e_property: Optional[Union[Dict, List[Dict]]] = None,
+        merge_op: str = "sum",
+        group_name: str = "main",
+    ):
+        r"""Add hyperedges to the hypergraph. If the ``group_name`` is not specified, the hyperedges will be added to the default ``main`` hyperedge group.
+
+        Parameters:
+            ``e_list`` (``Union[List[int], List[List[int]]]``): A list of hyperedges describes how the vertices point to the hyperedges.
+            ``e_weight`` (``Union[float, List[float]]``, optional): A list of weights for hyperedges. If set to ``None``, the value ``1`` is used for all hyperedges. Defaults to ``None``.
+            ``merge_op`` (``str``): The merge operation for the conflicting hyperedges. The possible values are ``"mean"``, ``"sum"``, and ``"max"``. Defaults to ``"mean"``.
+            ``group_name`` (``str``, optional): The target hyperedge group to add these hyperedges. Defaults to the ``main`` hyperedge group.
+        """
+        e_list = self._format_e_list(e_list)
+        if e_weight is None:
+            e_weight = [1.0] * len(e_list)
+        elif type(e_weight) in (int, float):
+            e_weight = [e_weight]
+        elif type(e_weight) is list:
+            pass
+        else:
+            raise TypeError(
+                "The type of e_weight should be float or list, but got"
+                f" {type(e_weight)}"
+            )
+        assert len(e_list) == len(
+            e_weight
+        ), "The number of hyperedges and the number of weights are not equal."
+
+        for _idx in range(len(e_list)):
+            flag = True
+
+            if (
+                group_name not in self._raw_groups
+                or self._hyperedge_code(e_list[_idx], e_list[_idx])
+                not in self._raw_groups[group_name]
+            ):
+                flag = False
+                self.edge_index += 1
+            for n_id in e_list[_idx]:
+                if self.isOutRange(n_id) == False:
+                    raise EasyGraphError(
+                        "The node id:"
+                        + str(n_id)
+                        + " in hyperedge is out of range, please ensure that"
+                        " the node is in [0,n)"
+                    )
+                self.deg_v_dict[n_id] += 1
+                if flag is False:
+                    self.n_e_dict[n_id].append(self.edge_index)
+            if e_property != None:
+                if type(e_property) == dict:
+                    e_property = [e_property]
+                e_property[_idx].update({"w_e": float(e_weight[_idx])})
+                self._add_hyperedge(
+                    self._hyperedge_code(e_list[_idx], e_list[_idx]),
+                    e_property[_idx],
+                    merge_op,
+                    group_name,
+                )
+            else:
+                self._add_hyperedge(
+                    self._hyperedge_code(e_list[_idx], e_list[_idx]),
+                    {"w_e": float(e_weight[_idx])},
+                    merge_op,
+                    group_name,
+                )
+
+        self._clear_cache(group_name)
+
+    def add_hyperedges_from_feature_kNN(
+        self, feature: torch.Tensor, k: int, group_name: str = "main"
+    ):
+        r"""Add hyperedges from the feature matrix by k-NN. Each hyperedge is constructed by the central vertex and its :math:`k`-Nearest Neighbor vertices.
+
+        Parameters:
+            ``features`` (``torch.Tensor``): The feature matrix.
+            ``k`` (``int``): The number of nearest neighbors.
+            ``group_name`` (``str``, optional): The target hyperedge group to add these hyperedges. Defaults to the ``main`` hyperedge group.
+        """
+        assert feature.shape[0] == self.num_v, (
+            "The number of vertices in the feature matrix is not equal to the number of"
+            " vertices in the hypergraph."
+        )
+        e_list = Hypergraph._e_list_from_feature_kNN(feature, k)
+        self.add_hyperedges(e_list, group_name=group_name)
+
+    def add_hyperedges_from_graph(self, graph, group_name: str = "main"):
+        r"""Add hyperedges from edges in the graph. Each edge in the graph is treated as a hyperedge.
+
+        Parameters:
+            ``graph`` (``eg.Graph``): The graph to join the hypergraph.
+            ``group_name`` (``str``, optional): The target hyperedge group to add these hyperedges. Defaults to the ``main`` hyperedge group.
+        """
+        assert self.num_v == len(
+            graph.nodes
+        ), "The number of vertices in the hypergraph and the graph are not equal."
+        e_list, e_weight = graph.e_both_side
+        self.add_hyperedges(e_list, e_weight=e_weight, group_name=group_name)
+
+    def add_hyperedges_from_graph_kHop(
+        self, graph, k: int, only_kHop: bool = False, group_name: str = "main"
+    ):
+        r"""Add hyperedges from vertices and its k-Hop neighbors in the graph. Each hyperedge in the hypergraph is constructed by the central vertex and its :math:`k`-Hop neighbor vertices.
+
+        .. note::
+            If the graph have :math:`|\mathcal{V}|` vertices, the constructed hypergraph will have :math:`|\mathcal{V}|` vertices and equal to or less than :math:`|\mathcal{V}|` hyperedges.
+
+        Parameters:
+            ``graph`` (``eg.Graph``): The graph to join the hypergraph.
+            ``k`` (``int``): The number of hop neighbors.
+            ``only_kHop`` (``bool``): If set to ``True``, only the central vertex and its :math:`k`-th Hop neighbors are used to construct the hyperedges. By default, the constructed hyperedge will include the central vertex and its [ :math:`1`-th, :math:`2`-th, :math:`\cdots`, :math:`k`-th ] Hop neighbors. Defaults to ``False``.
+            ``group_name`` (``str``, optional): The target hyperedge group to add these hyperedges. Defaults to the ``main`` hyperedge group.
+        """
+        assert self.num_v == len(
+            graph.nodes
+        ), "The number of vertices in the hypergraph and the graph are not equal."
+        e_list = Hypergraph._e_list_from_graph_kHop(graph, k, only_kHop=only_kHop)
+        self.add_hyperedges(e_list, group_name=group_name)
+
+    def remove_hyperedges(
+        self,
+        e_list: Union[List[int], List[List[int]]],
+        group_name: Optional[str] = None,
+    ):
+        r"""Remove the specified hyperedges from the hypergraph.
+
+        Parameters:
+            ``e_list`` (``Union[List[int], List[List[int]]]``): A list of hyperedges describes how the vertices point to the hyperedges.
+            ``group_name`` (``str``, optional): Remove these hyperedges from the specified hyperedge group. If not specified, the function will
+                remove those hyperedges from all hyperedge groups. Defaults to the ``None``.
+        """
+        assert (
+            group_name is None or group_name in self.group_names
+        ), "The specified group_name is not in existing hyperedge groups."
+        e_list = self._format_e_list(e_list)
+        if group_name is None:
+            for _idx in range(len(e_list)):
+                for n_id in e_list[_idx]:
+                    self.deg_v_dict[n_id] -= 1
+                    if self.isOutRange(n_id) == False:
+                        raise EasyGraphError(
+                            "The node id in hyperedge is out of range, please ensure"
+                            " that the node is in [1,n)"
+                        )
+                e_code = self._hyperedge_code(e_list[_idx], e_list[_idx])
+                for name in self.group_names:
+                    self._raw_groups[name].pop(e_code, None)
+        else:
+            for _idx in range(len(e_list)):
+                for n_id in e_list[_idx]:
+                    self.deg_v_dict[n_id] -= 1
+                    if self.isOutRange(n_id) == False:
+                        raise EasyGraphError(
+                            "The node id in hyperedge is out of range, please ensure"
+                            " that the node is in [1,n)"
+                        )
+                e_code = self._hyperedge_code(e_list[_idx], e_list[_idx])
+                self._raw_groups[group_name].pop(e_code, None)
+        self._clear_cache(group_name)
+
+    def remove_group(self, group_name: str):
+        r"""Remove the specified hyperedge group from the hypergraph.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the hyperedge group to remove.
+        """
+        for e_code, e in self._raw_groups[group_name].items():
+            e = e_code[0]
+            for n_id in e:
+                self.deg_v_dict[n_id] -= 1
+        self._raw_groups.pop(group_name, None)
+        self._clear_cache(group_name)
+
+    def drop_hyperedges(self, drop_rate: float, ord="uniform"):
+        r"""Randomly drop hyperedges from the hypergraph. This function will return a new hypergraph with non-dropped hyperedges.
+
+        Parameters:
+            ``drop_rate`` (``float``): The drop rate of hyperedges.
+            ``ord`` (``str``): The order of dropping edges. Currently, only ``'uniform'`` is supported. Defaults to ``uniform``.
+        """
+        if ord == "uniform":
+            _raw_groups = {}
+            for name in self.group_names:
+                _raw_groups[name] = {
+                    k: v
+                    for k, v in self._raw_groups[name].items()
+                    if random.random() > drop_rate
+                }
+            state_dict = {
+                "num_v": self.num_v,
+                "raw_groups": _raw_groups,
+                "e_property": self._e_property,
+                "v_property": self._v_property,
+            }
+            _hg = Hypergraph.from_state_dict(state_dict)
+            _hg = _hg.to(self.device)
+        else:
+            raise ValueError(f"Unknown drop order: {ord}.")
+        return _hg
+
+    def drop_hyperedges_of_group(
+        self, group_name: str, drop_rate: float, ord="uniform"
+    ):
+        r"""Randomly drop hyperedges from the specified hyperedge group. This function will return a new hypergraph with non-dropped hyperedges.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the hyperedge group.
+            ``drop_rate`` (``float``): The drop rate of hyperedges.
+            ``ord`` (``str``): The order of dropping edges. Currently, only ``'uniform'`` is supported. Defaults to ``uniform``.
+        """
+        if ord == "uniform":
+            _raw_groups = {}
+            for name in self.group_names:
+                if name == group_name:
+                    _raw_groups[name] = {
+                        k: v
+                        for k, v in self._raw_groups[name].items()
+                        if random.random() > drop_rate
+                    }
+                else:
+                    _raw_groups[name] = self._raw_groups[name]
+            state_dict = {
+                "num_v": self.num_v,
+                "raw_groups": self._raw_groups,
+                "e_property": self._e_property,
+                "v_property": self._v_property,
+            }
+            _hg = Hypergraph.from_state_dict(state_dict)
+            _hg = _hg.to(self.device)
+        else:
+            raise ValueError(f"Unknown drop order: {ord}.")
+        return _hg
+
+    # =====================================================================================
+    # properties for representation
+    @property
+    def v(self) -> List[int]:
+        r"""Return the list of vertices."""
+        return super().v
+
+    @property
+    def e(self) -> Tuple[List[List[int]], List[float]]:
+        r"""Return all hyperedges and weights in the hypergraph."""
+        if self.cache.get("e", None) is None:
+            e_list, e_weight, e_property = [], [], []
+            for name in self.group_names:
+                _e = self.e_of_group(name)
+                e_list.extend(_e[0])
+                e_weight.extend(_e[1])
+                e_property.extend(_e[2])
+            self.cache["e"] = (e_list, e_weight, e_property)
+        return self.cache["e"]
+
+    def e_of_group(self, group_name: str) -> Tuple[List[List[int]], List[float]]:
+        r"""Return all hyperedges and weights of the specified hyperedge group.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("e", None) is None:
+            e_list = [e_code[0] for e_code in self._raw_groups[group_name].keys()]
+            e_weight = [
+                e_content["w_e"] for e_content in self._raw_groups[group_name].values()
+            ]
+
+            e_property = []
+            for e_content in self._raw_groups[group_name].values():
+                properties = {}
+                for k, v in e_content.items():
+                    if k != "w_e":
+                        properties[k] = v
+                e_property.append(properties)
+            self.group_cache[group_name]["e"] = (e_list, e_weight, e_property)
+        return self.group_cache[group_name]["e"]
+
+    @property
+    def num_v(self) -> int:
+        r"""Return the number of vertices in the hypergraph."""
+        return super().num_v
+
+    @property
+    def num_e(self) -> int:
+        r"""Return the number of hyperedges in the hypergraph."""
+        return super().num_e
+
+    def num_e_of_group(self, group_name: str) -> int:
+        r"""Return the number of hyperedges of the specified hyperedge group.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        return super().num_e_of_group(group_name)
+
+    @property
+    def deg_v(self) -> List[int]:
+        r"""Return the degree list of each vertex."""
+        return self.D_v.to_sparse_coo()._values().cpu().view(-1).numpy().tolist()
+
+    def deg_v_of_group(self, group_name: str) -> List[int]:
+        r"""Return the degree list of each vertex of the specified hyperedge group.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        return self.D_v_of_group(group_name)._values().cpu().view(-1).numpy().tolist()
+
+    @property
+    def deg_e(self) -> List[int]:
+        r"""Return the degree list of each hyperedge."""
+        return self.D_e.to_sparse_coo()._values().cpu().view(-1).numpy().tolist()
+
+    def deg_e_of_group(self, group_name: str) -> List[int]:
+        r"""Return the degree list of each hyperedge of the specified hyperedge group.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        return self.D_e_of_group(group_name)._values().cpu().view(-1).numpy().tolist()
+
+    def nbr_e(self, v_idx: int) -> List[int]:
+        r"""Return the neighbor hyperedge list of the specified vertex.
+
+        Parameters:
+            ``v_idx`` (``int``): The index of the vertex.
+        """
+        return self.N_e(v_idx).cpu().numpy().tolist()
+
+    def nbr_e_of_group(self, v_idx: int, group_name: str) -> List[int]:
+        r"""Return the neighbor hyperedge list of the specified vertex of the specified hyperedge group.
+
+        Parameters:
+            ``v_idx`` (``int``): The index of the vertex.
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        return self.N_e_of_group(v_idx, group_name).cpu().numpy().tolist()
+
+    def nbr_v(self, e_idx: int) -> List[int]:
+        r"""Return the neighbor vertex list of the specified hyperedge.
+
+        Parameters:
+            ``e_idx`` (``int``): The index of the hyperedge.
+        """
+        return self.N_v(e_idx).cpu().numpy().tolist()
+
+    def nbr_v_of_group(self, e_idx: int, group_name: str) -> List[int]:
+        r"""Return the neighbor vertex list of the specified hyperedge of the specified hyperedge group.
+
+        Parameters:
+            ``e_idx`` (``int``): The index of the hyperedge.
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        return self.N_v_of_group(e_idx, group_name).cpu().numpy().tolist()
+
+    @property
+    def num_groups(self) -> int:
+        r"""Return the number of hyperedge groups in the hypergraph."""
+        return super().num_groups
+
+    @property
+    def group_names(self) -> List[str]:
+        r"""Return the names of all hyperedge groups in the hypergraph."""
+        return super().group_names
+
+    # =====================================================================================
+    # properties for deep learning
+    @property
+    def vars_for_DL(self) -> List[str]:
+        r"""Return a name list of available variables for deep learning in the hypergraph including
+
+        Sparse Matrices:
+
+        .. math::
+            \mathbf{H}, \mathbf{H}^\top, \mathcal{L}_{sym}, \mathcal{L}_{rw} \mathcal{L}_{HGNN},
+
+        Sparse Diagnal Matrices:
+
+        .. math::
+            \mathbf{W}_e, \mathbf{D}_v, \mathbf{D}_v^{-1}, \mathbf{D}_v^{-\frac{1}{2}}, \mathbf{D}_e, \mathbf{D}_e^{-1},
+
+        Vectors:
+
+        .. math::
+            \overrightarrow{v2e}_{src}, \overrightarrow{v2e}_{dst}, \overrightarrow{v2e}_{weight},\\
+            \overrightarrow{e2v}_{src}, \overrightarrow{e2v}_{dst}, \overrightarrow{e2v}_{weight}
+
+        """
+        return [
+            "H",
+            "H_T",
+            "L_sym",
+            "L_rw",
+            "L_HGNN",
+            "W_e",
+            "D_v",
+            "D_v_neg_1",
+            "D_v_neg_1_2",
+            "D_e",
+            "D_e_neg_1",
+            "v2e_src",
+            "v2e_dst",
+            "v2e_weighte2v_src",
+            "e2v_dst",
+            "e2v_weight",
+        ]
+
+    @property
+    def v2e_src(self) -> torch.Tensor:
+        r"""Return the source vertex index vector :math:`\overrightarrow{v2e}_{src}` of the connections (vertices point to hyperedges) in the hypergraph.
+        """
+        return self.H_T._indices()[1].clone()
+
+    def v2e_src_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the source vertex index vector :math:`\overrightarrow{v2e}_{src}` of the connections (vertices point to hyperedges) in the specified hyperedge group.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        return self.H_T_of_group(group_name)._indices()[1].clone()
+
+    @property
+    def v2e_dst(self) -> torch.Tensor:
+        r"""Return the destination hyperedge index vector :math:`\overrightarrow{v2e}_{dst}` of the connections (vertices point to hyperedges) in the hypergraph.
+        """
+        if self.cache.get("v2e_dst") is None:
+            self.cache["v2e_dst"] = self.H_T._indices()[0]
+        return self.cache["v2e_dst"]
+
+    def v2e_dst_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the destination hyperedge index vector :math:`\overrightarrow{v2e}_{dst}` of the connections (vertices point to hyperedges) in the specified hyperedge group.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        return self.H_T_of_group(group_name)._indices()[0].clone()
+
+    @property
+    def v2e_weight(self) -> torch.Tensor:
+        r"""Return the weight vector :math:`\overrightarrow{v2e}_{weight}` of the connections (vertices point to hyperedges) in the hypergraph.
+        """
+        return self.H_T._values().clone()
+
+    def v2e_weight_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the weight vector :math:`\overrightarrow{v2e}_{weight}` of the connections (vertices point to hyperedges) in the specified hyperedge group.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        return self.H_T_of_group(group_name)._values().clone()
+
+    @property
+    def e2v_src(self) -> torch.Tensor:
+        r"""Return the source hyperedge index vector :math:`\overrightarrow{e2v}_{src}` of the connections (hyperedges point to vertices) in the hypergraph.
+        """
+        return self.H._indices()[1]
+
+    def e2v_src_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the source hyperedge index vector :math:`\overrightarrow{e2v}_{src}` of the connections (hyperedges point to vertices) in the specified hyperedge group.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        return self.H_of_group(group_name)._indices()[1].clone()
+
+    @property
+    def e2v_dst(self) -> torch.Tensor:
+        r"""Return the destination vertex index vector :math:`\overrightarrow{e2v}_{dst}` of the connections (hyperedges point to vertices) in the hypergraph.
+        """
+        return self.H._indices()[0].clone()
+
+    def e2v_dst_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the destination vertex index vector :math:`\overrightarrow{e2v}_{dst}` of the connections (hyperedges point to vertices) in the specified hyperedge group.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        return self.H_of_group(group_name)._indices()[0].clone()
+
+    @property
+    def e2v_weight(self) -> torch.Tensor:
+        r"""Return the weight vector :math:`\overrightarrow{e2v}_{weight}` of the connections (hyperedges point to vertices) in the hypergraph.
+        """
+        return self.H._values()
+
+    def e2v_weight_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the weight vector :math:`\overrightarrow{e2v}_{weight}` of the connections (hyperedges point to vertices) in the specified hyperedge group.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        return self.H_of_group(group_name)._values().clone()
+
+    @property
+    def H(self) -> torch.Tensor:
+        r"""Return the hypergraph incidence matrix :math:`\mathbf{H}` with ``torch.Tensor`` format.
+        """
+
+        if self.cache.get("H") is None:
+            num_e = len(self._raw_groups["main"])
+            if self.cache.get("v_idx") is None or self.cache.get("e_idx") is None:
+                e_idx, v_idx = [], []
+                for n, e in self.n_e_dict.items():
+                    v_idx.extend([n] * len(e))
+                    e_idx.extend(e)
+                self.cache["e_idx"] = e_idx
+                self.cache["v_idx"] = v_idx
+            self.cache["H"] = torch.sparse_coo_tensor(
+                torch.tensor(
+                    [self.cache["v_idx"], self.cache["e_idx"]], dtype=torch.long
+                ),
+                torch.ones(len(self.cache["v_idx"])),
+                torch.Size([self.num_v, num_e]),
+                device=self.device,
+            ).coalesce()
+
+        return self.cache["H"]
+
+    @property
+    def e_set(self):
+        if self.cache.get("e_set") is None:
+            e_lst = []
+            for name in self.group_names:
+                _e = self.e_of_group(name)
+                e_lst.extend(_e[0])
+            self.cache["e_set"] = e_lst
+        return self.cache["e_set"]
+
+    @property
+    def incidence_matrix(self):
+        if self.cache.get("incidence_matrix") is None:
+            if (
+                self.cache.get("edges_col") is None
+                or self.cache.get("indptr_list") is None
+            ):
+                edges_col = []
+                indptr_list = []
+                ptr = 0
+                for v in self.n_e_dict.values():
+                    edges_col.extend(v)
+                    indptr_list.append(ptr)
+                    ptr += len(v)
+                indptr_list.append(ptr)
+                self.cache["edges_col"] = np.array(edges_col)
+                self.cache["indptr_list"] = np.array(indptr_list)
+
+            H = csr_matrix(
+                (
+                    [1] * len(self.cache["edges_col"]),
+                    self.cache["edges_col"],
+                    self.cache["indptr_list"],
+                ),
+                shape=(self.num_v, self.num_e),
+                dtype=int,
+            )
+            self.cache["incidence_matrix"] = H
+        return self.cache["incidence_matrix"]
+
+    def get_star_expansion(self):
+        r"""
+        The star expansion algorithm creates a graph  G*(V*, E*) for every hypergraph G(V, E).
+        The graph G*(V*, E*) introduces a node e∈E for each hyperedge in G(V, E), where V* = V ∪ E.
+        Each node e is connected to all the nodes belonging to the hyperedge it originates from, i.e., E* = {(u, e): u∈e, e∈E}.
+        It is worth noting that each hyperedge in the set E corresponds to a star-shaped structure in the graph G*(V*, E*),
+        and G* is a bipartite graph. The star expansion redistributes the weights of hyperedges to their corresponding ordinary pairwise graph edges.
+
+        $ \omega ^{*}(u,e)=\frac{\omega(e)}{\delta(e)} $
+
+        References
+        ----------
+        Antelmi, Alessia, et al. "A survey on hypergraph representation learning." ACM Computing Surveys 56.1 (2023): 1-38.
+
+        """
+        star_expansion_graph = eg.Graph()
+        for node in self.v:
+            star_expansion_graph.add_node(node, type="node")
+        e_index = len(self.v)
+        hyperedge_edge_list = self.e[0]
+        hyperedge_weight_list = self.e[1]
+        hyperedge_property_list = self.e[2]
+        for hyperedge_index, e in enumerate(hyperedge_edge_list):
+            hyperedge_weight = hyperedge_weight_list[hyperedge_index]
+            star_expansion_graph.add_node(e_index, type="hyperedge")
+            for index, node in enumerate(e):
+                star_expansion_graph.add_edge(
+                    e_index,
+                    node,
+                    weight=hyperedge_weight / len(e),
+                    hyperedge_index=hyperedge_index,
+                    edge_property=hyperedge_property_list[index],
+                )
+            e_index = e_index + 1
+        return star_expansion_graph
+
+    def neighbor_of_node(self, node):
+        neighbor_lst = list()
+        node_adj = self.adjacency_matrix()
+        if (
+            self.cache.get("neighbor") is None
+            or self.cache["neighbor"].get(node) is None
+        ):
+            start = node_adj.indptr[node]
+            end = node_adj.indptr[node + 1]
+
+            for j in range(start, end):
+                neighbor_lst.append(node_adj.indices[j])
+
+            if self.cache.get("neighbor") is None:
+                self.cache["neighbor"] = {}
+                self.cache["neighbor"][node] = neighbor_lst
+            else:
+                self.cache["neighbor"][node] = neighbor_lst
+
+        return self.cache["neighbor"][node]
+
+    def adjacency_matrix(self, s=1, weight=False):
+        r"""
+        The :term:`s-adjacency matrix` for the dual hypergraph.
+
+        Parameters
+        ----------
+        s : int, optional, default 1
+
+        Returns
+        -------
+        adjacency_matrix : scipy.sparse.csr.csr_matrix
+
+        """
+        if self.cache.get("adjacency_matrix") == None:
+            tmp_H = self.incidence_matrix
+            A = tmp_H @ (tmp_H.T)
+            A[np.diag_indices_from(A)] = 0
+            if not weight:
+                A = (A >= s) * 1
+            self.cache["adjacency_matrix"] = csr_matrix(A)
+        return self.cache["adjacency_matrix"]
+
+    def edge_adjacency_matrix(self, s=1, weight=False):
+        r"""
+        The :term:`s-adjacency matrix` for the dual hypergraph.
+
+        Parameters
+        ----------
+        s : int, optional, default 1
+
+        Returns
+        -------
+        adjacency_matrix : scipy.sparse.csr.csr_matrix
+
+        """
+        tmp_H = self.incidence_matrix
+        A = (tmp_H.T) @ (tmp_H)
+        A[np.diag_indices_from(A)] = 0
+        if not weight:
+            A = (A >= s) * 1
+        return csr_array(A)
+        # return A
+
+    def H_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the hypergraph incidence matrix :math:`\mathbf{H}` of the specified hyperedge group with ``torch.Tensor`` format.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("H") is None:
+            self.group_cache[group_name]["H"] = self._fetch_H()
+        return self.group_cache[group_name]["H"]
+
+    def edge_distance(self, source, target, s=1):
+        """
+
+        Parameters
+        ----------
+        source
+        target
+        s
+
+        Returns
+        -------
+        s- walk distance : the shortest s-walk edge distance
+
+        Notes
+        -----
+            The s-distance is the shortest s-walk length between the edges.
+            An s-walk between edges is a sequence of edges such that
+            consecutive pairwise edges intersect in at least s nodes. The
+            length of the shortest s-walk is 1 less than the number of edges
+            in the path sequence.
+
+        """
+        l_graph = self.get_clique_expansion(s=s, edge=True)
+        if source not in l_graph.nodes:
+            raise EasyGraphError("Please make sure source exist!")
+        dist = eg.Dijkstra(l_graph, source)
+        if target in dist:
+            return dist[target]
+        raise EasyGraphError("Please make sure target exist!")
+
+    def distance(self, source, target=None, s=1):
+        """
+
+        Parameters
+        ----------
+        source : node in the hypergraph
+        target : node in the hypergraph
+        s : positive integer
+            the number of edges
+
+        Returns
+        -------
+        s-walk distance : int
+
+        Notes
+        -----
+        The s-distance is the shortest s-walk length between the nodes.
+        An s-walk between nodes is a sequence of nodes that pairwise share
+        at least s edges. The length of the shortest s-walk is 1 less than
+        the number of nodes in the path sequence.
+
+        Uses the EasyGraph's Dijkstra method on the graph
+        generated by the s-adjacency matrix.
+
+        """
+
+        l_graph = self.get_clique_expansion(s=s)
+        if source not in l_graph.nodes:
+            raise EasyGraphError("Please make sure source exist!")
+        if target is not None and target not in l_graph.nodes:
+            raise EasyGraphError("Please make sure target exist!")
+        dist = eg.single_source_dijkstra(G=l_graph, source=source, target=target)
+        return dist[target] if target != None else dist
+
+    def edge_diameter(self, s=1):
+        """
+        Returns the length of the longest shortest s-walk between edges in
+        hypergraph
+
+        Parameters
+        ----------
+        s : int, optional, default 1
+
+        Return
+        ------
+        edge_diameter : int
+
+        Raises
+        ------
+        EasyGraphXError
+            If hypergraph is not s-edge-connected
+
+        Notes
+        -----
+        Two edges are s-adjacent if they share s nodes.
+        Two nodes e_start and e_end are s-walk connected if there is a
+        sequence of edges e_start, e_1, e_2, ... e_n-1, e_end such that
+        consecutive edges are s-adjacent. If the graph is not connected, an
+        error will be raised.
+
+        """
+        l_graph = self.get_clique_expansion(s=s, edge=True)
+        if eg.is_connected(l_graph):
+            return eg.diameter(l_graph)
+        raise EasyGraphError(f"Hypergraph is not s-connected. s={s}")
+
+    def diameter(self, s=1):
+        """
+        Returns the length of the longest shortest s-walk between nodes in
+        hypergraph
+
+        Parameters
+        ----------
+        s : int, optional, default 1
+
+        Returns
+        -------
+        diameter : int
+        Raises
+        ------
+        EasyGraphError
+            If hypergraph is not s-edge-connected
+
+        Notes
+        -----
+        Two nodes are s-adjacent if they share s edges.
+        Two nodes v_start and v_end are s-walk connected if there is a
+        sequence of nodes v_start, v_1, v_2, ... v_n-1, v_end such that
+        consecutive nodes are s-adjacent. If the graph is not connected,
+        an error will be raised.
+        """
+        l_graph = self.get_clique_expansion(s=s)
+        if eg.is_connected(l_graph):
+            return eg.diameter(l_graph)
+        raise EasyGraphError(f"Hypergraph is not s-connected. s={s}")
+
+    @property
+    def H_T(self) -> torch.Tensor:
+        r"""Return the transpose of the hypergraph incidence matrix :math:`\mathbf{H}^\top` with ``torch.Tensor`` format.
+        """
+        if self.cache.get("H_T") is None:
+            self.cache["H_T"] = self.H.t()
+        return self.cache["H_T"]
+
+    def H_T_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the transpose of the hypergraph incidence matrix :math:`\mathbf{H}^\top` of the specified hyperedge group with ``torch.Tensor`` format.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("H_T") is None:
+            self.group_cache[group_name]["H_T"] = self.H_of_group(group_name).t()
+        return self.group_cache[group_name]["H_T"]
+
+    @property
+    def W_e(self) -> torch.Tensor:
+        r"""Return the weight matrix :math:`\mathbf{W}_e` of hyperedges with ``torch.Tensor`` format.
+        """
+        if self.cache.get("W_e") is None:
+            _tmp = torch.tensor(self.e[1])
+            _num_e = _tmp.size(0)
+            self.cache["W_e"] = torch.sparse_coo_tensor(
+                torch.arange(0, _num_e).view(1, -1).repeat(2, 1),
+                _tmp,
+                torch.Size([_num_e, _num_e]),
+                device=self.device,
+            ).coalesce()
+
+        return self.cache["W_e"]
+
+    def W_e_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the weight matrix :math:`\mathbf{W}_e` of hyperedges of the specified hyperedge group with ``torch.Tensor`` format.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("W_e") is None:
+            w_list = [1.0] * len(self._raw_groups["main"])
+            _tmp = torch.tensor(w_list, device=self.device).view((-1, 1)).view(-1)
+            _num_e = _tmp.size(0)
+            self.group_cache[group_name]["W_e"] = torch.sparse_coo_tensor(
+                torch.arange(0, _num_e).view(1, -1).repeat(2, 1),
+                _tmp,
+                torch.Size([_num_e, _num_e]),
+                device=self.device,
+            ).coalesce()
+        return self.group_cache[group_name]["W_e"]
+
+    @property
+    def degree_node(self):
+        return self.deg_v_dict
+
+    @property
+    def D_v(self) -> torch.Tensor:
+        r"""Return the vertex degree matrix :math:`\mathbf{D}_v` with ``torch.sparse_coo_tensor`` format.
+        """
+        if self.cache.get("D_v") is None:
+            if self.cache.get("D_v_value") is None:
+                self.cache["D_v_value"] = (
+                    torch.sparse.sum(self.H, dim=1).to_dense().view(-1)
+                )
+
+            self.cache["D_v"] = torch.sparse_coo_tensor(
+                torch.arange(0, self.num_v).view(1, -1).repeat(2, 1),
+                self.cache["D_v_value"],
+                torch.Size([self.num_v, self.num_v]),
+                device=self.device,
+            ).coalesce()
+        return self.cache["D_v"]
+
+    def D_v_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the vertex degree matrix :math:`\mathbf{D}_v` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("D_v") is None:
+            _tmp = (
+                torch.sparse.sum(self.H_of_group(group_name), dim=1)
+                .to_dense()
+                .clone()
+                .view(-1)
+            )
+            _num_v = _tmp.size(0)
+            self.group_cache[group_name]["D_v"] = torch.sparse_coo_tensor(
+                torch.arange(0, _num_v).view(1, -1).repeat(2, 1),
+                _tmp,
+                torch.Size([_num_v, _num_v]),
+                device=self.device,
+            ).coalesce()
+        return self.group_cache[group_name]["D_v"]
+
+    @property
+    def D_v_neg_1(self) -> torch.Tensor:
+        r"""Return the vertex degree matrix :math:`\mathbf{D}_v^{-1}` with ``torch.sparse_coo_tensor`` format.
+        """
+        if self.cache.get("D_v_neg_1") is None:
+            if self.cache.get("D_v_value") is None:
+                self.cache["D_v_value"] = (
+                    torch.sparse.sum(self.H, dim=1).to_dense().view(-1)
+                )
+            _tmp = self.cache["D_v_value"]
+            _num_v = _tmp.size(0)
+            _val = _tmp**-1
+            _val[torch.isinf(_val)] = 0
+            self.cache["D_v_neg_1"] = torch.sparse_csr_tensor(
+                torch.arange(0, _num_v + 1),
+                torch.arange(0, _num_v),
+                _val,
+                torch.Size([_num_v, _num_v]),
+                device=self.device,
+            )
+
+        return self.cache["D_v_neg_1"]
+
+    def D_v_neg_1_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the vertex degree matrix :math:`\mathbf{D}_v^{-1}` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("D_v_neg_1") is None:
+            _mat = self.D_v_of_group(group_name).clone()
+            _val = _mat._values() ** -1
+            _val[torch.isinf(_val)] = 0
+            self.group_cache[group_name]["D_v_neg_1"] = torch.sparse_coo_tensor(
+                _mat._indices(), _val, _mat.size(), device=self.device
+            ).coalesce()
+        return self.group_cache[group_name]["D_v_neg_1"]
+
+    @property
+    def D_v_neg_1_2(self) -> torch.Tensor:
+        r"""Return the vertex degree matrix :math:`\mathbf{D}_v^{-\frac{1}{2}}` with ``torch.sparse_coo_tensor`` format.
+        """
+        if self.cache.get("D_v_neg_1_2") is None:
+            if self.cache.get("D_v_value") is None:
+                self.cache["D_v_value"] = (
+                    torch.sparse.sum(self.H, dim=1).to_dense().view(-1)
+                )
+            _mat = self.cache["D_v_value"]
+            _mat = _mat**-0.5
+            _mat[torch.isinf(_mat)] = 0
+            self.cache["D_v_neg_1_2"] = torch.sparse_csr_tensor(
+                torch.arange(0, self.num_v + 1),
+                torch.arange(0, self.num_v),
+                _mat,
+                torch.Size([self.num_v, self.num_v]),
+                device=self.device,
+            )
+
+        return self.cache["D_v_neg_1_2"]
+
+    def D_v_neg_1_2_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the vertex degree matrix :math:`\mathbf{D}_v^{-\frac{1}{2}}` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("D_v_neg_1_2") is None:
+            _mat = self.D_v_of_group(group_name).clone()
+            _val = _mat._values() ** -0.5
+            _val[torch.isinf(_val)] = 0
+            self.group_cache[group_name]["D_v_neg_1_2"] = torch.sparse_coo_tensor(
+                _mat._indices(), _val, _mat.size(), device=self.device
+            ).coalesce()
+        return self.group_cache[group_name]["D_v_neg_1_2"]
+
+    @property
+    def D_e(self) -> torch.Tensor:
+        r"""Return the hyperedge degree matrix :math:`\mathbf{D}_e` with ``torch.sparse_coo_tensor`` format.
+        """
+        if self.cache.get("D_e") is None:
+            _tmp = torch.sparse.sum(self.H_T, dim=1).to_dense().view(-1)
+            _num_e = _tmp.size(0)
+            self.cache["D_e"] = torch.sparse_csr_tensor(
+                torch.arange(0, _num_e + 1),
+                torch.arange(0, _num_e),
+                _tmp,
+                torch.Size([_num_e, _num_e]),
+                device=self.device,
+            )
+
+        return self.cache["D_e"]
+
+    def D_e_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the hyperedge degree matrix :math:`\mathbf{D}_e` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("D_e") is None:
+            _tmp = (
+                torch.sparse.sum(self._fetch_H().t(), dim=1).to_dense().clone().view(-1)
+            )
+            _num_e = _tmp.size(0)
+            self.group_cache[group_name]["D_e"] = torch.sparse_coo_tensor(
+                torch.arange(0, _num_e).view(1, -1).repeat(2, 1),
+                _tmp,
+                torch.Size([_num_e, _num_e]),
+                device=self.device,
+            ).coalesce()
+        return self.group_cache[group_name]["D_e"]
+
+    @property
+    def D_e_neg_1(self) -> torch.Tensor:
+        r"""Return the hyperedge degree matrix :math:`\mathbf{D}_e^{-1}` with ``torch.sparse_coo_tensor`` format.
+        """
+        if self.cache.get("D_e_neg_1") is None:
+            _tmp = torch.sparse.sum(self.H_T, dim=1).to_dense().view(-1)
+            _num_e = _tmp.size(0)
+            _val = _tmp**-1
+            _val[torch.isinf(_val)] = 0
+
+            self.cache["D_e_neg_1"] = torch.sparse_csr_tensor(
+                torch.arange(0, _num_e + 1),
+                torch.arange(0, _num_e),
+                _val,
+                torch.Size([_num_e, _num_e]),
+                device=self.device,
+            )
+
+        return self.cache["D_e_neg_1"]
+
+    def D_e_neg_1_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the hyperedge degree matrix :math:`\mathbf{D}_e^{-1}` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("D_e_neg_1") is None:
+            _mat = self.D_e_of_group(group_name).clone()
+            _val = _mat._values() ** -1
+            _val[torch.isinf(_val)] = 0
+            self.group_cache[group_name]["D_e_neg_1"] = torch.sparse_coo_tensor(
+                _mat._indices(), _val, _mat.size(), device=self.device
+            ).coalesce()
+        return self.group_cache[group_name]["D_e_neg_1"]
+
+    def _fetch_H(self):
+        r"""Fetch the H matrix of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
+
+        Args:
+            ``direction`` (``str``): The direction of hyperedges can be either ``'v2e'`` or ``'e2v'``.
+            ``group_name`` (``str``): The name of the group.
+        """
+        # assert (
+        #     group_name in self.group_names
+        # ), f"The specified {group_name} is not in existing hyperedge groups."
+        # assert direction in ["v2e", "e2v"], "direction must be one of ['v2e', 'e2v']"
+        # if direction == "v2e":
+        #     select_idx = 0
+        # else:
+        #     select_idx = 1
+        if self.cache.get("main_H") is None:
+            num_e = len(self._raw_groups["main"])
+            self.cache["main_H"] = torch.sparse_coo_tensor(
+                ([self.cache["v_idx"], self.cache["e_idx"]]),
+                torch.ones(len(self.cache["v_idx"])),
+                torch.Size([self.num_v, num_e]),
+                device=self.device,
+            ).coalesce()
+
+        return self.cache["main_H"]
+
+    def N_e(self, v_idx: int) -> torch.Tensor:
+        r"""Return the neighbor hyperedges of the specified vertex with ``torch.Tensor`` format.
+
+        .. note::
+            The ``v_idx`` must be in the range of [0, :attr:`num_v`).
+
+        Parameters:
+            ``v_idx`` (``int``): The index of the vertex.
+        """
+        assert v_idx < self.num_v
+        _tmp, e_bias = [], 0
+        for name in self.group_names:
+            _tmp.append(self.N_e_of_group(v_idx, name) + e_bias)
+            e_bias += self.num_e_of_group(name)
+        return torch.cat(_tmp, dim=0)
+
+    def N_e_of_group(self, v_idx: int, group_name: str) -> torch.Tensor:
+        r"""Return the neighbor hyperedges of the specified vertex of the specified hyperedge group with ``torch.Tensor`` format.
+
+        .. note::
+            The ``v_idx`` must be in the range of [0, :attr:`num_v`).
+
+        Parameters:
+            ``v_idx`` (``int``): The index of the vertex.
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        assert v_idx < self.num_v
+        e_indices = self.H_of_group(group_name)[v_idx]._indices()[0]
+        return e_indices.clone()
+
+    def N_v(self, e_idx: int) -> torch.Tensor:
+        r"""Return the neighbor vertices of the specified hyperedge with ``torch.Tensor`` format.
+
+        .. note::
+            The ``e_idx`` must be in the range of [0, :attr:`num_e`).
+
+        Parameters:
+            ``e_idx`` (``int``): The index of the hyperedge.
+        """
+        assert e_idx < self.num_e
+        for name in self.group_names:
+            if e_idx < self.num_e_of_group(name):
+                return self.N_v_of_group(e_idx, name)
+            else:
+                e_idx -= self.num_e_of_group(name)
+
+    def N_v_of_group(self, e_idx: int, group_name: str) -> torch.Tensor:
+        r"""Return the neighbor vertices of the specified hyperedge of the specified hyperedge group with ``torch.Tensor`` format.
+
+        .. note::
+            The ``e_idx`` must be in the range of [0, :func:`num_e_of_group`).
+
+        Parameters:
+            ``e_idx`` (``int``): The index of the hyperedge.
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        assert e_idx < self.num_e_of_group(group_name)
+        v_indices = self.H_T_of_group(group_name)[e_idx]._indices()[0]
+        return v_indices.clone()
+
+    # =====================================================================================
+    # spectral-based convolution/smoothing
+    def smoothing(self, X: torch.Tensor, L: torch.Tensor, lamb: float) -> torch.Tensor:
+        return super().smoothing(X, L, lamb)
+
+    @property
+    def L_sym(self) -> torch.Tensor:
+        r"""Return the symmetric Laplacian matrix :math:`\mathcal{L}_{sym}` of the hypergraph with ``torch.sparse_coo_tensor`` format.
+
+        .. math::
+            \mathcal{L}_{sym} = \mathbf{I} - \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}}
+        """
+        if self.cache.get("L_sym") is None:
+            L_HGNN = self.L_HGNN.clone()
+            self.cache["L_sym"] = torch.sparse_coo_tensor(
+                torch.hstack(
+                    [
+                        torch.arange(0, self.num_v).view(1, -1).repeat(2, 1),
+                        L_HGNN.to_sparse_coo()._indices(),
+                    ]
+                ),
+                torch.hstack(
+                    [torch.ones(self.num_v), -L_HGNN.to_sparse_coo()._values()]
+                ),
+                torch.Size([self.num_v, self.num_v]),
+                device=self.device,
+            ).coalesce()
+        return self.cache["L_sym"]
+
+    def L_sym_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the symmetric Laplacian matrix :math:`\mathcal{L}_{sym}` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
+
+        .. math::
+            \mathcal{L}_{sym} = \mathbf{I} - \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}}
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("L_sym") is None:
+            L_HGNN = self.L_HGNN_of_group(group_name).clone()
+            self.group_cache[group_name]["L_sym"] = torch.sparse_coo_tensor(
+                torch.hstack(
+                    [
+                        torch.arange(0, self.num_v).view(1, -1).repeat(2, 1),
+                        L_HGNN._indices(),
+                    ]
+                ),
+                torch.hstack([torch.ones(self.num_v), -L_HGNN._values()]),
+                torch.Size([self.num_v, self.num_v]),
+                device=self.device,
+            ).coalesce()
+        return self.group_cache[group_name]["L_sym"]
+
+    @property
+    def L_rw(self) -> torch.Tensor:
+        r"""Return the random walk Laplacian matrix :math:`\mathcal{L}_{rw}` of the hypergraph with ``torch.sparse_coo_tensor`` format.
+
+        .. math::
+            \mathcal{L}_{rw} = \mathbf{I} - \mathbf{D}_v^{-1} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top
+        """
+        if self.cache.get("L_rw") is None:
+            _tmp = (
+                self.D_v_neg_1.mm(self.H).mm(self.W_e).mm(self.D_e_neg_1).mm(self.H_T)
+            )
+            self.cache["L_rw"] = (
+                torch.sparse_coo_tensor(
+                    torch.hstack(
+                        [
+                            torch.arange(0, self.num_v).view(1, -1).repeat(2, 1),
+                            _tmp._indices(),
+                        ]
+                    ),
+                    torch.hstack([torch.ones(self.num_v), -_tmp._values()]),
+                    torch.Size([self.num_v, self.num_v]),
+                    device=self.device,
+                )
+                .coalesce()
+                .clone()
+            )
+        return self.cache["L_rw"]
+
+    def L_rw_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the random walk Laplacian matrix :math:`\mathcal{L}_{rw}` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
+
+        .. math::
+            \mathcal{L}_{rw} = \mathbf{I} - \mathbf{D}_v^{-1} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("L_rw") is None:
+            _tmp = (
+                self.D_v_neg_1_of_group(group_name)
+                .mm(self.H_of_group(group_name))
+                .mm(
+                    self.W_e_of_group(group_name),
+                )
+                .mm(
+                    self.D_e_neg_1_of_group(group_name),
+                )
+                .mm(
+                    self.H_T_of_group(group_name),
+                )
+            )
+            self.group_cache[group_name]["L_rw"] = (
+                torch.sparse_coo_tensor(
+                    torch.hstack(
+                        [
+                            torch.arange(0, self.num_v).view(1, -1).repeat(2, 1),
+                            _tmp._indices(),
+                        ]
+                    ),
+                    torch.hstack([torch.ones(self.num_v), -_tmp._values()]),
+                    torch.Size([self.num_v, self.num_v]),
+                    device=self.device,
+                )
+                .coalesce()
+                .clone()
+            )
+        return self.group_cache[group_name]["L_rw"]
+
+    ## HGNN Laplacian smoothing
+    @property
+    def L_HGNN(self) -> torch.Tensor:
+        r"""Return the HGNN Laplacian matrix :math:`\mathcal{L}_{HGNN}` of the hypergraph with ``torch.sparse_coo_tensor`` format.
+
+        .. math::
+            \mathcal{L}_{HGNN} = \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}}
+        """
+        if self.cache.get("L_HGNN") is None:
+            _d_v_neg_1_2 = self.D_v_neg_1_2.to_sparse_coo()
+            _tmp = (
+                _d_v_neg_1_2
+                @ self.H
+                @ self.W_e
+                @ self.D_e_neg_1.to_sparse_coo()
+                @ self.H_T
+                @ _d_v_neg_1_2
+            )
+            self.cache["L_HGNN"] = _tmp.to_sparse_csr()
+        return self.cache["L_HGNN"]
+
+    def L_HGNN_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the HGNN Laplacian matrix :math:`\mathcal{L}_{HGNN}` of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
+
+        .. math::
+            \mathcal{L}_{HGNN} = \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}}
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("L_HGNN") is None:
+            _tmp = (
+                self.D_v_neg_1_2_of_group(group_name)
+                .mm(self.H_of_group(group_name))
+                .mm(self.W_e_of_group(group_name))
+                .mm(
+                    self.D_e_neg_1_of_group(group_name),
+                )
+                .mm(
+                    self.H_T_of_group(group_name),
+                )
+                .mm(
+                    self.D_v_neg_1_2_of_group(group_name),
+                )
+            )
+            self.group_cache[group_name]["L_HGNN"] = _tmp.coalesce()
+        return self.group_cache[group_name]["L_HGNN"]
+
+    def smoothing_with_HGNN(
+        self, X: torch.Tensor, drop_rate: float = 0.0
+    ) -> torch.Tensor:
+        r"""Return the smoothed feature matrix with the HGNN Laplacian matrix :math:`\mathcal{L}_{HGNN}`.
+
+            .. math::
+                \mathbf{X} = \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}} \mathbf{X}
+
+        Parameters:
+            ``X`` (``torch.Tensor``): The feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
+        """
+        if self.device != X.device:
+            X = X.to(self.device)
+
+        if drop_rate > 0.0:
+            L_HGNN = sparse_dropout(self.L_HGNN, drop_rate)
+        else:
+            L_HGNN = self.L_HGNN
+        return L_HGNN.mm(X)
+
+    def smoothing_with_HGNN_of_group(
+        self, group_name: str, X: torch.Tensor, drop_rate: float = 0.0
+    ) -> torch.Tensor:
+        r"""Return the smoothed feature matrix with the HGNN Laplacian matrix :math:`\mathcal{L}_{HGNN}`.
+
+            .. math::
+                \mathbf{X} = \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}} \mathbf{X}
+
+        Parameters:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+            ``X`` (``torch.Tensor``): The feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.device != X.device:
+            X = X.to(self.device)
+        if drop_rate > 0.0:
+            L_HGNN = sparse_dropout(self.L_HGNN_of_group(group_name), drop_rate)
+        else:
+            L_HGNN = self.L_HGNN_of_group(group_name)
+        return L_HGNN.mm(X)
+
+    # =====================================================================================
+    # spatial-based convolution/message-passing
+    # general message passing functions
+    def v2e_aggregation(
+        self,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        v2e_weight: Optional[torch.Tensor] = None,
+        drop_rate: float = 0.0,
+    ):
+        r"""Message aggregation step of ``vertices to hyperedges``.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyperedges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
+        """
+        if self.device != X.device:
+            self.to(X.device)
+        if v2e_weight is None:
+            if drop_rate > 0.0:
+                P = sparse_dropout(self.H_T, drop_rate)
+            else:
+                P = self.H_T
+
+            if aggr == "mean":
+                X = torch.sparse.mm(P, X)
+                X = torch.sparse.mm(self.D_e_neg_1, X)
+            elif aggr == "sum":
+                X = torch.sparse.mm(P, X)
+            elif aggr == "softmax_then_sum":
+                P = torch.sparse.softmax(P, dim=1)
+                X = torch.sparse.mm(P, X)
+            else:
+                raise ValueError(f"Unknown aggregation method {aggr}.")
+        else:
+            # init message path
+            assert (
+                v2e_weight.shape[0] == self.v2e_weight.shape[0]
+            ), "The size of v2e_weight must be equal to the size of self.v2e_weight."
+            P = torch.sparse_coo_tensor(
+                self.H_T._indices(), v2e_weight, self.H_T.shape, device=self.device
+            )
+
+            if drop_rate > 0.0:
+                P = sparse_dropout(P, drop_rate)
+            # message passing
+            if aggr == "mean":
+                X = torch.sparse.mm(P, X)
+                D_e_neg_1 = torch.sparse.sum(P, dim=1).to_dense().view(-1, 1)
+                D_e_neg_1[torch.isinf(D_e_neg_1)] = 0
+                X = torch.sparse.mm(D_e_neg_1, X)
+            elif aggr == "sum":
+                X = torch.sparse.mm(P, X)
+            elif aggr == "softmax_then_sum":
+                P = torch.sparse.softmax(P, dim=1)
+                X = torch.sparse.mm(P, X)
+            else:
+                raise ValueError(f"Unknown aggregation method {aggr}.")
+        return X
+
+    def v2e_aggregation_of_group(
+        self,
+        group_name: str,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        v2e_weight: Optional[torch.Tensor] = None,
+        drop_rate: float = 0.0,
+    ):
+        r"""Message aggregation step of ``vertices to hyperedges`` in specified hyperedge group.
+
+        Parameters:
+            ``group_name`` (``str``): The specified hyperedge group.
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyperedges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        assert aggr in ["mean", "sum", "softmax_then_sum"]
+        if self.device != X.device:
+            self.to(X.device)
+        if v2e_weight is None:
+            if drop_rate > 0.0:
+                P = sparse_dropout(self.H_T_of_group(group_name), drop_rate)
+            else:
+                P = self.H_T_of_group(group_name)
+            if aggr == "mean":
+                X = torch.sparse.mm(P, X)
+                X = torch.sparse.mm(self.D_e_neg_1_of_group(group_name), X)
+            elif aggr == "sum":
+                X = torch.sparse.mm(P, X)
+            elif aggr == "softmax_then_sum":
+                P = torch.sparse.softmax(P, dim=1)
+                X = torch.sparse.mm(P, X)
+            else:
+                raise ValueError(f"Unknown aggregation method {aggr}.")
+        else:
+            # init message path
+            assert (
+                v2e_weight.shape[0] == self.v2e_weight_of_group(group_name).shape[0]
+            ), (
+                "The size of v2e_weight must be equal to the size of"
+                f" self.v2e_weight_of_group('{group_name}')."
+            )
+            P = torch.sparse_coo_tensor(
+                self.H_T_of_group(group_name)._indices(),
+                v2e_weight,
+                self.H_T_of_group(group_name).shape,
+                device=self.device,
+            )
+            if drop_rate > 0.0:
+                P = sparse_dropout(P, drop_rate)
+            # message passing
+            if aggr == "mean":
+                X = torch.sparse.mm(P, X)
+                D_e_neg_1 = torch.sparse.sum(P, dim=1).to_dense().view(-1, 1)
+                D_e_neg_1[torch.isinf(D_e_neg_1)] = 0
+                X = D_e_neg_1 * X
+            elif aggr == "sum":
+                X = torch.sparse.mm(P, X)
+            elif aggr == "softmax_then_sum":
+                P = torch.sparse.softmax(P, dim=1)
+                X = torch.sparse.mm(P, X)
+            else:
+                raise ValueError(f"Unknown aggregation method {aggr}.")
+        return X
+
+    def v2e_update(self, X: torch.Tensor, e_weight: Optional[torch.Tensor] = None):
+        r"""Message update step of ``vertices to hyperedges``.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
+            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+        if self.device != X.device:
+            self.to(X.device)
+        if e_weight is None:
+            X = torch.sparse.mm(self.W_e, X)
+        else:
+            e_weight = e_weight.view(-1, 1)
+            assert (
+                e_weight.shape[0] == self.num_e
+            ), "The size of e_weight must be equal to the size of self.num_e."
+            X = e_weight * X
+        return X
+
+    def v2e_update_of_group(
+        self, group_name: str, X: torch.Tensor, e_weight: Optional[torch.Tensor] = None
+    ):
+        r"""Message update step of ``vertices to hyperedges`` in specified hyperedge group.
+
+        Parameters:
+            ``group_name`` (``str``): The specified hyperedge group.
+            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
+            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.device != X.device:
+            self.to(X.device)
+        if e_weight is None:
+            X = torch.sparse.mm(self.W_e_of_group(group_name), X)
+        else:
+            e_weight = e_weight.view(-1, 1)
+            assert e_weight.shape[0] == self.num_e_of_group(group_name), (
+                "The size of e_weight must be equal to the size of"
+                f" self.num_e_of_group('{group_name}')."
+            )
+            X = e_weight * X
+        return X
+
+    def v2e(
+        self,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        v2e_weight: Optional[torch.Tensor] = None,
+        e_weight: Optional[torch.Tensor] = None,
+        drop_rate: float = 0.0,
+    ):
+        r"""Message passing of ``vertices to hyperedges``. The combination of ``v2e_aggregation`` and ``v2e_update``.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyperedges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
+        """
+
+        X = self.v2e_aggregation(X, aggr, v2e_weight, drop_rate=drop_rate)
+        X = self.v2e_update(X, e_weight)
+        return X
+
+    def v2e_of_group(
+        self,
+        group_name: str,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        v2e_weight: Optional[torch.Tensor] = None,
+        e_weight: Optional[torch.Tensor] = None,
+        drop_rate: float = 0.0,
+    ):
+        r"""Message passing of ``vertices to hyperedges`` in specified hyperedge group. The combination of ``e2v_aggregation_of_group`` and ``e2v_update_of_group``.
+
+        Parameters:
+            ``group_name`` (``str``): The specified hyperedge group.
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyperedges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        X = self.v2e_aggregation_of_group(
+            group_name, X, aggr, v2e_weight, drop_rate=drop_rate
+        )
+        X = self.v2e_update_of_group(group_name, X, e_weight)
+        return X
+
+    def e2v_aggregation(
+        self,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        e2v_weight: Optional[torch.Tensor] = None,
+        drop_rate: float = 0.0,
+    ):
+        r"""Message aggregation step of ``hyperedges to vertices``.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
+        """
+        if self.device != X.device:
+            self.to(X.device)
+        if e2v_weight is None:
+            if drop_rate > 0.0:
+                P = sparse_dropout(self.H, drop_rate)
+            else:
+                P = self.H
+            if aggr == "mean":
+                X = torch.sparse.mm(P, X)
+                X = torch.sparse.mm(self.D_v_neg_1, X)
+            elif aggr == "sum":
+                X = torch.sparse.mm(P, X)
+            elif aggr == "softmax_then_sum":
+                P = torch.sparse.softmax(P, dim=1)
+                X = torch.sparse.mm(P, X)
+            else:
+                raise ValueError(f"Unknown aggregation method: {aggr}")
+        else:
+            # init message path
+            assert (
+                e2v_weight.shape[0] == self.e2v_weight.shape[0]
+            ), "The size of e2v_weight must be equal to the size of self.e2v_weight."
+            P = torch.sparse_coo_tensor(
+                self.H._indices(), e2v_weight, self.H.shape, device=self.device
+            ).coalesce()
+
+            if drop_rate > 0.0:
+                P = sparse_dropout(P, drop_rate)
+            # message passing
+            if aggr == "mean":
+                X = torch.sparse.mm(P, X)
+                D_v_neg_1 = torch.sparse.sum(P, dim=1).to_dense().view(-1, 1)
+                D_v_neg_1[torch.isinf(D_v_neg_1)] = 0
+                X = D_v_neg_1 * X
+            elif aggr == "sum":
+                X = torch.sparse.mm(P, X)
+            elif aggr == "softmax_then_sum":
+                P = torch.sparse.softmax(P, dim=1)
+                X = torch.sparse.mm(P, X)
+            else:
+                raise ValueError(f"Unknown aggregation method: {aggr}")
+        return X
+
+    def e2v_aggregation_of_group(
+        self,
+        group_name: str,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        e2v_weight: Optional[torch.Tensor] = None,
+        drop_rate: float = 0.0,
+    ):
+        r"""Message aggregation step of ``hyperedges to vertices`` in specified hyperedge group.
+
+        Parameters:
+            ``group_name`` (``str``): The specified hyperedge group.
+            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        assert aggr in ["mean", "sum", "softmax_then_sum"]
+        if self.device != X.device:
+            self.to(X.device)
+        if e2v_weight is None:
+            if drop_rate > 0.0:
+                P = sparse_dropout(self.H_of_group(group_name), drop_rate)
+            else:
+                P = self.H_of_group(group_name)
+            if aggr == "mean":
+                X = torch.sparse.mm(P, X)
+                X = torch.sparse.mm(self.D_v_neg_1_of_group[group_name], X)
+            elif aggr == "sum":
+                X = torch.sparse.mm(P, X)
+            elif aggr == "softmax_then_sum":
+                P = torch.sparse.softmax(P, dim=1)
+                X = torch.sparse.mm(P, X)
+            else:
+                raise ValueError(f"Unknown aggregation method: {aggr}")
+        else:
+            # init message path
+            assert (
+                e2v_weight.shape[0] == self.e2v_weight_of_group[group_name].shape[0]
+            ), (
+                "The size of e2v_weight must be equal to the size of"
+                f" self.e2v_weight_of_group('{group_name}')."
+            )
+            P = torch.sparse_coo_tensor(
+                self.H_of_group[group_name]._indices(),
+                e2v_weight,
+                self.H_of_group[group_name].shape,
+                device=self.device,
+            )
+            if drop_rate > 0.0:
+                P = sparse_dropout(P, drop_rate)
+            # message passing
+            if aggr == "mean":
+                X = torch.sparse.mm(P, X)
+                D_v_neg_1 = torch.sparse.sum(P, dim=1).to_dense().view(-1, 1)
+                D_v_neg_1[torch.isinf(D_v_neg_1)] = 0
+                X = D_v_neg_1 * X
+            elif aggr == "sum":
+                X = torch.sparse.mm(P, X)
+            elif aggr == "softmax_then_sum":
+                P = torch.sparse.softmax(P, dim=1)
+                X = torch.sparse.mm(P, X)
+            else:
+                raise ValueError(f"Unknown aggregation method: {aggr}")
+        return X
+
+    def e2v_update(self, X: torch.Tensor):
+        r"""Message update step of ``hyperedges to vertices``.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+        """
+        if self.device != X.device:
+            self.to(X.device)
+        return X
+
+    def e2v_update_of_group(self, group_name: str, X: torch.Tensor):
+        r"""Message update step of ``hyperedges to vertices`` in specified hyperedge group.
+
+        Parameters:
+            ``group_name`` (``str``): The specified hyperedge group.
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.device != X.device:
+            self.to(X.device)
+        return X
+
+    def e2v(
+        self,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        e2v_weight: Optional[torch.Tensor] = None,
+        drop_rate: float = 0.0,
+    ):
+        r"""Message passing of ``hyperedges to vertices``. The combination of ``e2v_aggregation`` and ``e2v_update``.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
+        """
+        X = self.e2v_aggregation(X, aggr, e2v_weight, drop_rate=drop_rate)
+        X = self.e2v_update(X)
+        return X
+
+    def e2v_of_group(
+        self,
+        group_name: str,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        e2v_weight: Optional[torch.Tensor] = None,
+        drop_rate: float = 0.0,
+    ):
+        r"""Message passing of ``hyperedges to vertices`` in specified hyperedge group. The combination of ``e2v_aggregation_of_group`` and ``e2v_update_of_group``.
+
+        Parameters:
+            ``group_name`` (``str``): The specified hyperedge group.
+            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        X = self.e2v_aggregation_of_group(
+            group_name, X, aggr, e2v_weight, drop_rate=drop_rate
+        )
+        X = self.e2v_update_of_group(group_name, X)
+        return X
+
+    def v2v(
+        self,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        drop_rate: float = 0.0,
+        v2e_aggr: Optional[str] = None,
+        v2e_weight: Optional[torch.Tensor] = None,
+        v2e_drop_rate: Optional[float] = None,
+        e_weight: Optional[torch.Tensor] = None,
+        e2v_aggr: Optional[str] = None,
+        e2v_weight: Optional[torch.Tensor] = None,
+        e2v_drop_rate: Optional[float] = None,
+    ):
+        r"""Message passing of ``vertices to vertices``. The combination of ``v2e`` and ``e2v``.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, this ``aggr`` will be used to both ``v2e`` and ``e2v``.
+            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
+            ``v2e_aggr`` (``str``, optional): The aggregation method for hyperedges to vertices. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``e2v``.
+            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyperedges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``v2e_drop_rate`` (``float``, optional): Dropout rate for hyperedges to vertices. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. If specified, it will override the ``drop_rate`` in ``e2v``. Default: ``None``.
+            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``e2v_aggr`` (``str``, optional): The aggregation method for vertices to hyperedges. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``v2e``.
+            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``e2v_drop_rate`` (``float``, optional): Dropout rate for vertices to hyperedges. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. If specified, it will override the ``drop_rate`` in ``v2e``. Default: ``None``.
+        """
+        if v2e_aggr is None:
+            v2e_aggr = aggr
+        if e2v_aggr is None:
+            e2v_aggr = aggr
+        if v2e_drop_rate is None:
+            v2e_drop_rate = drop_rate
+        if e2v_drop_rate is None:
+            e2v_drop_rate = drop_rate
+
+        X = self.v2e(X, v2e_aggr, v2e_weight, e_weight, drop_rate=v2e_drop_rate)
+        X = self.e2v(X, e2v_aggr, e2v_weight, drop_rate=e2v_drop_rate)
+
+        return X
+
+    def v2v_of_group(
+        self,
+        group_name: str,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        drop_rate: float = 0.0,
+        v2e_aggr: Optional[str] = None,
+        v2e_weight: Optional[torch.Tensor] = None,
+        v2e_drop_rate: Optional[float] = None,
+        e_weight: Optional[torch.Tensor] = None,
+        e2v_aggr: Optional[str] = None,
+        e2v_weight: Optional[torch.Tensor] = None,
+        e2v_drop_rate: Optional[float] = None,
+    ):
+        r"""Message passing of ``vertices to vertices`` in specified hyperedge group. The combination of ``v2e_of_group`` and ``e2v_of_group``.
+
+        Parameters:
+            ``group_name`` (``str``): The specified hyperedge group.
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, this ``aggr`` will be used to both ``v2e_of_group`` and ``e2v_of_group``.
+            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.
+            ``v2e_aggr`` (``str``, optional): The aggregation method for hyperedges to vertices. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``e2v_of_group``.
+            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyperedges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``v2e_drop_rate`` (``float``, optional): Dropout rate for hyperedges to vertices. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. If specified, it will override the ``drop_rate`` in ``e2v_of_group``. Default: ``None``.
+            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``e2v_aggr`` (``str``, optional): The aggregation method for vertices to hyperedges. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``v2e_of_group``.
+            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``e2v_drop_rate`` (``float``, optional): Dropout rate for vertices to hyperedges. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. If specified, it will override the ``drop_rate`` in ``v2e_of_group``. Default: ``None``.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if v2e_aggr is None:
+            v2e_aggr = aggr
+        if e2v_aggr is None:
+            e2v_aggr = aggr
+        if v2e_drop_rate is None:
+            v2e_drop_rate = drop_rate
+        if e2v_drop_rate is None:
+            e2v_drop_rate = drop_rate
+        X = self.v2e_of_group(
+            group_name, X, v2e_aggr, v2e_weight, e_weight, drop_rate=v2e_drop_rate
+        )
+        X = self.e2v_of_group(
+            group_name, X, e2v_aggr, e2v_weight, drop_rate=e2v_drop_rate
+        )
+        return X
+
+    def get_linegraph(self, s=1, weight=True):
+        """
+        Get the linegraph of the hypergraph based on the clique expansion.
+        The edges will be the vertices of the line
+        graph. Two vertices are connected by an s-line-graph edge if the
+        corresponding hypergraph edges intersect in at least s hypergraph nodes.
+
+
+        Parameters
+        ----------
+        s : Two vertices are connected if the nodes they correspond to share
+        at least s incident hyper edges.
+        edge : If edges=True (default)then the edges will be the vertices of the line
+        graph. Two vertices are connected by an s-line-graph edge if the
+        corresponding hypergraph edges intersect in at least s hypergraph nodes.
+        If edges=False, the hypergraph nodes will be the vertices of the line
+        graph.
+        weight :
+
+        Returns
+        -------
+            Graph: easygraph.Graph, the linegraph of the hypergraph.
+
+        """
+        edge_adjacency = self.edge_adjacency_matrix(s=s, weight=weight)
+        graph = eg.from_scipy_sparse_matrix(edge_adjacency)
+        return graph
+
+    def get_clique_expansion(self, s=1, weight=True):
+        """
+        Get the linegraph of the hypergraph based on the clique expansion.
+        The hypergraph nodes will be the vertices of the line
+        graph. Two vertices are connected if the nodes they correspond to share
+        at least s incident hyper edges.
+
+        Parameters
+        ----------
+        s : Two vertices are connected if the nodes they correspond to share
+        at least s incident hyper edges.
+        edge : If edges=True (default)then the edges will be the vertices of the line
+        graph. Two vertices are connected by an s-line-graph edge if the
+        corresponding hypergraph edges intersect in at least s hypergraph nodes.
+        If edges=False, the hypergraph nodes will be the vertices of the line
+        graph.
+        weight :
+
+        Returns
+        -------
+            Graph: easygraph.Graph, the clique expansion of the hypergraph.
+
+        """
+
+        if self.cache.get("clique_expansion") is None:
+            A = self.adjacency_matrix(s=s, weight=weight)
+            graph = eg.Graph()
+            A = np.array(np.nonzero(A))
+            e1 = np.array([idx for idx in A[0]])
+            e2 = np.array([idx for idx in A[1]])
+            A = np.array([e1, e2]).T
+            graph.add_edges_from(A)
+            graph.add_nodes(list(range(0, self.num_v)))
+            self.cache["clique_expansion"] = graph
+
+        return self.cache["clique_expansion"]
+
+    def s_connected_components(self, s=1, edges=True, return_singletons=False):
+        """
+        Returns a generator for the :term:`s-edge-connected components
+        <s-edge-connected component>`
+        or the :term:`s-node-connected components <s-connected component,
+        s-node-connected component>` of the hypergraph.
+
+        Parameters
+        ----------
+        s : int, optional, default 1
+
+        edges : boolean, optional, default = True
+            If True will return edge components, if False will return node
+            components
+        return_singletons : bool, optional, default = False
+
+        Notes
+        -----
+        If edges=True, this method returns the s-edge-connected components as
+        lists of lists of edge uids.
+        An s-edge-component has the property that for any two edges e1 and e2
+        there is a sequence of edges starting with e1 and ending with e2
+        such that pairwise adjacent edges in the sequence intersect in at least
+        s nodes. If s=1 these are the path components of the hypergraph.
+
+        If edges=False this method returns s-node-connected components.
+        A list of sets of uids of the nodes which are s-walk connected.
+        Two nodes v1 and v2 are s-walk-connected if there is a
+        sequence of nodes starting with v1 and ending with v2 such that
+        pairwise adjacent nodes in the sequence share s edges. If s=1 these
+        are the path components of the hypergraph.
+
+        Example
+        -------
+            >>> S = {'A':{1,2,3},'B':{2,3,4},'C':{5,6},'D':{6}}
+            >>> H = Hypergraph(S)
+
+            >>> list(H.s_components(edges=True))
+            [{'C', 'D'}, {'A', 'B'}]
+            >>> list(H.s_components(edges=False))
+            [{1, 2, 3, 4}, {5, 6}]
+
+        Yields
+        ------
+        s_connected_components : iterator
+            Iterator returns sets of uids of the edges (or nodes) in the
+            s-edge(node) components of hypergraph.
+
+        """
+        if not edges:
+            g = self.get_clique_expansion()
+        else:
+            g = self.get_linegraph(s)
+        for c in eg.connected_components(g):
+            if not return_singletons and len(c) == 1:
+                continue
+            yield c
```

## easygraph/classes/graphviews.py

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-from easygraph.utils import only_implemented_for_Directed_graph
-
-
-__all__ = ["reverse_view"]
-
-
-@only_implemented_for_Directed_graph
-def reverse_view(G):
-    newG = G.__class__()
-    newG._graph = G
-    newG.graph = G.graph
-    newG._node = G._node
-    newG._succ, newG._pred = G._pred, G._succ
-    newG._adj = newG._succ
-    return newG
+from easygraph.utils import only_implemented_for_Directed_graph
+
+
+__all__ = ["reverse_view"]
+
+
+@only_implemented_for_Directed_graph
+def reverse_view(G):
+    newG = G.__class__()
+    newG._graph = G
+    newG.graph = G.graph
+    newG._node = G._node
+    newG._succ, newG._pred = G._pred, G._succ
+    newG._adj = newG._succ
+    return newG
```

## easygraph/classes/base.py

 * *Ordering differences only*

```diff
@@ -1,997 +1,997 @@
-import abc
-
-from collections import defaultdict
-from pathlib import Path
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Tuple
-from typing import Union
-
-import numpy as np
-import torch
-
-from easygraph.utils.exception import EasyGraphError
-
-
-__all__ = ["load_structure", "BaseHypergraph"]
-
-
-def load_structure(file_path: Union[str, Path]):
-    r"""Load a EasyGraph's high-order network structure from a file. The supported structure ``Hypergraph``.
-
-    Args:
-        ``file_path`` (``Union[str, Path]``): The file path to load the EasyGraph's structure.
-    """
-    import pickle as pkl
-
-    import easygraph
-
-    file_path = Path(file_path)
-    assert file_path.exists(), f"{file_path} does not exist"
-    with open(file_path, "rb") as f:
-        data = pkl.load(f)
-    class_name, state_dict = data["class"], data["state_dict"]
-    structure_class = getattr(easygraph, class_name)
-    structure = structure_class.from_state_dict(state_dict)
-    return structure
-
-
-class BaseHypergraph:
-    r"""The ``BaseHypergraph`` class is the base class for all hypergraph structures.
-
-    Args:
-        ``num_v`` (``int``): The number of vertices.
-        ``e_list`` (``Union[List[int], List[List[int]]], optional``): Edge list. Defaults to ``None``.
-        ``e_weight`` (``Union[float, List[float]], optional``): A list of weights for edges. Defaults to ``None``.
-        ``extra_selfloop`` (``bool``, optional): Whether to add extra self-loop to the graph. Defaults to ``False``.
-        ``device`` (``torch.device``, optional): The device to store the graph. Defaults to ``torch.device('cpu')``.
-
-    """
-
-    def __init__(
-        self,
-        num_v: int,
-        v_property: Optional[Union[Dict, List[Dict]]] = None,
-        e_list: Optional[Union[List[int], List[List[int]]]] = None,
-        e_property: Optional[Union[Dict, List[Dict]]] = None,
-        e_weight: Optional[Union[float, List[float]]] = None,
-        extra_selfloop: bool = False,
-        device: torch.device = torch.device("cpu"),
-    ):
-        assert (
-            isinstance(num_v, int) and num_v > 0
-        ), "num_v should be a positive integer"
-        self.clear()
-        self._rows = []
-        self._cols = []
-        self._num_v = num_v
-        self.device = device
-        if v_property == None:
-            self._v_property = [{} for i in range(num_v)]
-        else:
-            v_property = self._format_v_property_list(num_v, v_property)
-            self._v_property = v_property
-
-        if e_property == None and e_list != None:
-            self._e_property = [{} for i in range(len(e_list))]
-        elif e_property != None and e_list != None:
-            e_property = self._format_e_property_list(len(e_list), e_property)
-            self._e_property = e_property
-
-        self._has_extra_selfloop = extra_selfloop
-
-    @abc.abstractmethod
-    def __repr__(self) -> str:
-        r"""Print the hypergraph information."""
-
-    @property
-    @abc.abstractmethod
-    def state_dict(self) -> Dict[str, Any]:
-        r"""Get the state dict of the hypergraph."""
-
-    @abc.abstractmethod
-    def save(self, file_path: Union[str, Path]):
-        r"""Save the EasyGraph's hypergraph structure to a file.
-
-        Args:
-            ``file_path`` (``str``): The file_path to store the EasyGraph's hypergraph structure.
-        """
-
-    @staticmethod
-    @abc.abstractmethod
-    def load(file_path: Union[str, Path]):
-        r"""Load the EasyGraph's hypergraph structure from a file.
-
-        Args:
-            ``file_path`` (``str``): The file path to load the DEasyGraph's hypergraph structure.
-        """
-
-    @staticmethod
-    @abc.abstractmethod
-    def from_state_dict(state_dict: dict):
-        r"""Load the EasyGraph's hypergraph structure from the state dict.
-
-        Args:
-            ``state_dict`` (``dict``): The state dict to load the EasyGraph's hypergraph.
-        """
-
-    @abc.abstractmethod
-    def draw(self, **kwargs):
-        r"""Draw the structure."""
-
-    def clear(self):
-        r"""Remove all hyperedges and caches from the hypergraph."""
-        self._clear_raw()
-        self._clear_cache()
-
-    def _clear_raw(self):
-        self._v_weight = None
-        self._raw_groups = {}
-
-    def _clear_cache(self, group_name: Optional[str] = None):
-        r"""Clear the cache."""
-        self.cache = {}
-        if group_name is None:
-            self.group_cache = defaultdict(dict)
-        else:
-            self.group_cache.pop(group_name, None)
-
-    @abc.abstractmethod
-    def clone(self) -> "BaseHypergraph":
-        r"""Return a copy of this type of hypergraph."""
-
-    def to(self, device: torch.device):
-        r"""Move the hypergraph to the specified device.
-
-        Args:
-            ``device`` (``torch.device``): The device to store the hypergraph.
-        """
-        self.device = device
-        for v in self.vars_for_DL:
-            if v in self.cache and self.cache[v] is not None:
-                self.cache[v] = self.cache[v].to(device)
-            for name in self.group_names:
-                if (
-                    v in self.group_cache[name]
-                    and self.group_cache[name][v] is not None
-                ):
-                    self.group_cache[name][v] = self.group_cache[name][v].to(device)
-        return self
-
-    # utils
-    def _hyperedge_code(self, src_v_set: List[int], dst_v_set: List[int]) -> Tuple:
-        r"""Generate the hyperedge code.
-
-        Args:
-            ``src_v_set`` (``List[int]``): The source vertex set.
-            ``dst_v_set`` (``List[int]``): The destination vertex set.
-        """
-        return tuple([src_v_set, dst_v_set])
-
-    def _merge_hyperedges(self, e1: dict, e2: dict, op: str = "mean"):
-        assert op in [
-            "mean",
-            "sum",
-            "max",
-        ], "Hyperedge merge operation must be one of ['mean', 'sum', 'max']"
-        _func = {
-            "mean": lambda x, y: (x + y) / 2,
-            "sum": lambda x, y: x + y,
-            "max": lambda x, y: max(x, y),
-        }
-        _e = {}
-        if "w_v2e" in e1 and "w_v2e" in e2:
-            for _idx in range(len(e1["w_v2e"])):
-                _e["w_v2e"] = _func[op](e1["w_v2e"][_idx], e2["w_v2e"][_idx])
-        if "w_e2v" in e1 and "w_e2v" in e2:
-            for _idx in range(len(e1["w_e2v"])):
-                _e["w_e2v"] = _func[op](e1["w_e2v"][_idx], e2["w_e2v"][_idx])
-        _e["w_e"] = _func[op](e1["w_e"], e2["w_e"])
-        return _e
-
-    @staticmethod
-    def _format_e_list(e_list: Union[List[int], List[List[int]]]) -> List[List[int]]:
-        r"""Format the hyperedge list.
-
-        Args:
-            ``e_list`` (``List[int]`` or ``List[List[int]]``): The hyperedge list.
-        """
-        if len(e_list) == 0:
-            pass
-        elif type(e_list[0]) in (int, float):
-            return [tuple(sorted(e_list))]
-        elif type(e_list) == tuple:
-            e_list = list(e_list)
-        elif type(e_list) == list:
-            pass
-        else:
-            raise TypeError("e_list must be List[int] or List[List[int]].")
-        for _idx in range(len(e_list)):
-            e_list[_idx] = tuple(sorted(e_list[_idx]))
-        return e_list
-
-    def _format_e_property_list(self, e_num, e_property_list: Union[Dict, List[Dict]]):
-        r"""Format the property list.
-
-        Args:
-            ``e_list`` (``Dict`` or ``List[Dict]``): The property list.
-        """
-        if type(e_property_list) == dict:
-            return [e_property_list]
-        elif type(e_property_list) == list and len(e_property_list) != e_num:
-            raise EasyGraphError(
-                "The length of property list must be equal to edge number"
-            )
-        elif type(e_property_list) == list:
-            pass
-        else:
-            raise TypeError("e_property_list must be Dict or List[Dict].")
-
-        return e_property_list
-
-    def _format_v_property_list(self, v_num, v_property_list: Union[Dict, List[Dict]]):
-        r"""Format the property list.
-
-        Args:
-            ``e_list`` (``Dict`` or ``List[Dict]``): The property list.
-        """
-        if type(v_property_list) == dict:
-            return [v_property_list]
-        elif type(v_property_list) == list and len(v_property_list) != v_num:
-            raise EasyGraphError(
-                "The length of property list must be equal to node number"
-            )
-        elif type(v_property_list) == list:
-            pass
-        else:
-            raise TypeError("v_property_list must be Dict or List[Dict].")
-
-        return v_property_list
-
-    @staticmethod
-    def _format_e_list_and_w_on_them(
-        e_list: Union[List[int], List[List[int]]],
-        w_list: Optional[Union[List[int], List[List[int]]]] = None,
-    ):
-        r"""Format ``e_list`` and ``w_list``.
-
-        Args:
-            ``e_list`` (Union[List[int], List[List[int]]]): Hyperedge list.
-            ``w_list`` (Optional[Union[List[int], List[List[int]]]]): Weights on connections. Defaults to ``None``.
-        """
-        bad_connection_msg = (
-            "The weight on connections between vertices and hyperedges must have the"
-            " same size as the hyperedges."
-        )
-        if isinstance(e_list, tuple):
-            e_list = list(e_list)
-        if w_list is not None and isinstance(w_list, tuple):
-            w_list = list(w_list)
-        if isinstance(e_list[0], int) and w_list is None:
-            w_list = [1] * len(e_list)
-            e_list, w_list = [e_list], [w_list]
-        elif isinstance(e_list[0], int) and w_list is not None:
-            assert len(e_list) == len(w_list), bad_connection_msg
-            e_list, w_list = [e_list], [w_list]
-        elif isinstance(e_list[0], list) and w_list is None:
-            w_list = [[1] * len(e) for e in e_list]
-        assert len(e_list) == len(w_list), bad_connection_msg
-        # TODO: this step can be speeded up
-        for idx in range(len(e_list)):
-            assert len(e_list[idx]) == len(w_list[idx]), bad_connection_msg
-            cur_e, cur_w = np.array(e_list[idx]), np.array(w_list[idx])
-            sorted_idx = np.argsort(cur_e)
-            e_list[idx] = tuple(cur_e[sorted_idx].tolist())
-            w_list[idx] = cur_w[sorted_idx].tolist()
-        return e_list, w_list
-
-    def _fetch_H(self, direction: str, group_name: str):
-        r"""Fetch the H matrix of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
-
-        Args:
-            ``direction`` (``str``): The direction of hyperedges can be either ``'v2e'`` or ``'e2v'``.
-            ``group_name`` (``str``): The name of the group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        assert direction in ["v2e", "e2v"], "direction must be one of ['v2e', 'e2v']"
-        if direction == "v2e":
-            select_idx = 0
-        else:
-            select_idx = 1
-        num_e = len(self._raw_groups[group_name])
-        e_idx, v_idx = [], []
-        for _e_idx, e in enumerate(self._raw_groups[group_name].keys()):
-            sub_e = e[select_idx]
-            v_idx.extend(sub_e)
-            e_idx.extend([_e_idx] * len(sub_e))
-
-        H = torch.sparse_coo_tensor(
-            torch.tensor([v_idx, e_idx], dtype=torch.long),
-            torch.ones(len(v_idx)),
-            torch.Size([self.num_v, num_e]),
-            device=self.device,
-        ).coalesce()
-        return H
-
-    def _fetch_H_of_group(self, direction: str, group_name: str):
-        r"""Fetch the H matrix of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
-
-        Args:
-            ``direction`` (``str``): The direction of hyperedges can be either ``'v2e'`` or ``'e2v'``.
-            ``group_name`` (``str``): The name of the group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        assert direction in ["v2e", "e2v"], "direction must be one of ['v2e', 'e2v']"
-        if direction == "v2e":
-            select_idx = 0
-        else:
-            select_idx = 1
-        num_e = len(self._raw_groups[group_name])
-        e_idx, v_idx = [], []
-        for _e_idx, e in enumerate(self._raw_groups[group_name].keys()):
-            sub_e = e[select_idx]
-            v_idx.extend(sub_e)
-            e_idx.extend([_e_idx] * len(sub_e))
-
-        H = torch.sparse_coo_tensor(
-            torch.tensor([v_idx, e_idx], dtype=torch.long),
-            torch.ones(len(v_idx)),
-            torch.Size([self.num_v, num_e]),
-            device=self.device,
-        ).coalesce()
-        return H
-
-    def _fetch_R_of_group(self, direction: str, group_name: str):
-        r"""Fetch the R matrix of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
-
-        Args:
-            ``direction`` (``str``): The direction of hyperedges can be either ``'v2e'`` or ``'e2v'``.
-            ``group_name`` (``str``): The name of the group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        assert direction in ["v2e", "e2v"], "direction must be one of ['v2e', 'e2v']"
-        if direction == "v2e":
-            select_idx = 0
-        else:
-            select_idx = 1
-        num_e = len(self._raw_groups[group_name])
-        e_idx, v_idx, w_list = [], [], []
-        for _e_idx, e in enumerate(self._raw_groups[group_name].keys()):
-            sub_e = e[select_idx]
-            v_idx.extend(sub_e)
-            e_idx.extend([_e_idx] * len(sub_e))
-            w_list.extend(self._raw_groups[group_name][e][f"w_{direction}"])
-        R = torch.sparse_coo_tensor(
-            torch.vstack([v_idx, e_idx]),
-            torch.tensor(w_list),
-            torch.Size([self.num_v, num_e]),
-            device=self.device,
-        ).coalesce()
-        return R
-
-    def _fetch_W_of_group(self, group_name: str):
-        r"""Fetch the W matrix of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
-
-        Args:
-            ``group_name`` (``str``): The name of the group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        w_list = [1.0] * len(self._raw_groups["main"])
-        W = torch.tensor(w_list, device=self.device).view((-1, 1))
-        return W
-
-    # some structure modification functions
-    def add_hyperedges(
-        self,
-        e_list_v2e: Union[List[int], List[List[int]]],
-        e_list_e2v: Union[List[int], List[List[int]]],
-        w_list_v2e: Optional[Union[List[float], List[List[float]]]] = None,
-        w_list_e2v: Optional[Union[List[float], List[List[float]]]] = None,
-        e_weight: Optional[Union[float, List[float]]] = None,
-        merge_op: str = "mean",
-        group_name: str = "main",
-    ):
-        r"""Add hyperedges to the hypergraph. If the ``group_name`` is not specified, the hyperedges will be added to the default ``main`` hyperedge group.
-
-        Args:
-            ``num_v`` (``int``): The number of vertices in the hypergraph.
-            ``e_list_v2e`` (``Union[List[int], List[List[int]]]``): A list of hyperedges describes how the vertices point to the hyperedges.
-            ``e_list_e2v`` (``Union[List[int], List[List[int]]]``): A list of hyperedges describes how the hyperedges point to the vertices.
-            ``w_list_v2e`` (``Union[List[float], List[List[float]]]``, optional): The weights are attached to the connections from vertices to hyperedges, which has the same shape
-                as ``e_list_v2e``. If set to ``None``, the value ``1`` is used for all connections. Defaults to ``None``.
-            ``w_list_e2v`` (``Union[List[float], List[List[float]]]``, optional): The weights are attached to the connections from the hyperedges to the vertices, which has the
-                same shape to ``e_list_e2v``. If set to ``None``, the value ``1`` is used for all connections. Defaults to ``None``.
-            ``e_weight`` (``Union[float, List[float]]``, optional): A list of weights for hyperedges. If set to ``None``, the value ``1`` is used for all hyperedges. Defaults to ``None``.
-            ``merge_op`` (``str``): The merge operation for the conflicting hyperedges. The possible values are ``mean``, ``sum``, ``max``, and ``min``. Defaults to ``mean``.
-            ``group_name`` (``str``, optional): The target hyperedge group to add these hyperedges. Defaults to the ``main`` hyperedge group.
-        """
-        e_list_v2e, w_list_v2e = self._format_e_list_and_w_on_them(
-            e_list_v2e, w_list_v2e
-        )
-        e_list_e2v, w_list_e2v = self._format_e_list_and_w_on_them(
-            e_list_e2v, w_list_e2v
-        )
-        if e_weight is None:
-            e_weight = [1.0] * len(e_list_v2e)
-        assert len(e_list_v2e) == len(
-            e_weight
-        ), "The number of hyperedges and the number of weights are not equal."
-        assert len(e_list_v2e) == len(
-            e_list_e2v
-        ), "Hyperedges of 'v2e' and 'e2v' must have the same size."
-        for _idx in range(len(e_list_v2e)):
-            self._add_hyperedge(
-                self._hyperedge_code(e_list_v2e[_idx], e_list_e2v[_idx]),
-                {
-                    "w_v2e": w_list_v2e[_idx],
-                    "w_e2v": w_list_e2v[_idx],
-                    "w_e": e_weight[_idx],
-                },
-                merge_op,
-                group_name,
-            )
-        self._clear_cache(group_name)
-
-    def _add_hyperedge(
-        self,
-        hyperedge_code: Tuple[List[int], List[int]],
-        content: Dict[str, Any],
-        merge_op: str,
-        group_name: str,
-    ):
-        r"""Add a hyperedge to the specified hyperedge group.
-
-        Args:
-            ``hyperedge_code`` (``Tuple[List[int], List[int]]``): The hyperedge code.
-            ``content`` (``Dict[str, Any]``): The content of the hyperedge.
-            ``merge_op`` (``str``): The merge operation for the conflicting hyperedges.
-            ``group_name`` (``str``): The target hyperedge group to add this hyperedge.
-        """
-        if group_name not in self._raw_groups:
-            self._raw_groups[group_name] = {}
-            self._raw_groups[group_name][hyperedge_code] = content
-        else:
-            if hyperedge_code not in self._raw_groups[group_name]:
-                self._raw_groups[group_name][hyperedge_code] = content
-            else:
-                self._raw_groups[group_name][hyperedge_code] = self._merge_hyperedges(
-                    self._raw_groups[group_name][hyperedge_code], content, merge_op
-                )
-
-    def remove_hyperedges(
-        self,
-        e_list_v2e: Union[List[int], List[List[int]]],
-        e_list_e2v: Union[List[int], List[List[int]]],
-        group_name: Optional[str] = None,
-    ):
-        r"""Remove the specified hyperedges from the hypergraph.
-
-        Args:
-            ``e_list_v2e`` (``Union[List[int], List[List[int]]]``): A list of hyperedges describes how the vertices point to the hyperedges.
-            ``e_list_e2v`` (``Union[List[int], List[List[int]]]``): A list of hyperedges describes how the hyperedges point to the vertices.
-            ``group_name`` (``str``, optional): Remove these hyperedges from the specified hyperedge group. If not specified, the function will
-                remove those hyperedges from all hyperedge groups. Defaults to the ``None``.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        assert len(e_list_v2e) == len(
-            e_list_e2v
-        ), "Hyperedges of 'v2e' and 'e2v' must have the same size."
-        e_list_v2e = self._format_e_list(e_list_v2e)
-        e_list_e2v = self._format_e_list(e_list_e2v)
-        if group_name is None:
-            for _idx in range(len(e_list_v2e)):
-                e_code = self._hyperedge_code(e_list_v2e[_idx], e_list_e2v[_idx])
-                for name in self.group_names:
-                    self._raw_groups[name].pop(e_code, None)
-        else:
-            for _idx in range(len(e_list_v2e)):
-                e_code = self._hyperedge_code(e_list_v2e[_idx], e_list_e2v[_idx])
-                self._raw_groups[group_name].pop(e_code, None)
-        self._clear_cache(group_name)
-
-    @abc.abstractmethod
-    def drop_hyperedges(self, drop_rate: float, ord="uniform"):
-        r"""Randomly drop hyperedges from the hypergraph. This function will return a new hypergraph with non-dropped hyperedges.
-
-        Args:
-            ``drop_rate`` (``float``): The drop rate of hyperedges.
-            ``ord`` (``str``): The order of dropping edges. Currently, only ``'uniform'`` is supported. Defaults to ``uniform``.
-        """
-
-    @abc.abstractmethod
-    def drop_hyperedges_of_group(
-        self, group_name: str, drop_rate: float, ord="uniform"
-    ):
-        r"""Randomly drop hyperedges from the specified hyperedge group. This function will return a new hypergraph with non-dropped hyperedges.
-
-        Args:
-            ``group_name`` (``str``): The name of the hyperedge group.
-            ``drop_rate`` (``float``): The drop rate of hyperedges.
-            ``ord`` (``str``): The order of dropping edges. Currently, only ``'uniform'`` is supported. Defaults to ``uniform``.
-        """
-
-    # properties for the hypergraph
-    @property
-    def v(self) -> List[int]:
-        r"""Return the list of vertices."""
-        if self.cache.get("v") is None:
-            self.cache["v"] = list(range(self.num_v))
-        return self.cache["v"]
-
-    @property
-    def v_weight(self) -> List[float]:
-        r"""Return the vertex weights of the hypergraph."""
-        if self._v_weight is None:
-            self._v_weight = [1.0] * self.num_v
-        return self._v_weight
-
-    @v_weight.setter
-    def v_weight(self, v_weight: List[float]):
-        r"""Set the vertex weights of the hypergraph."""
-        assert (
-            len(v_weight) == self.num_v
-        ), "The length of vertex weights must be equal to the number of vertices."
-        self._v_weight = v_weight
-        self._clear_cache()
-
-    @property
-    @abc.abstractmethod
-    def e(self) -> Tuple[List[List[int]], List[float]]:
-        r"""Return all hyperedges and weights in the hypergraph."""
-
-    @abc.abstractmethod
-    def e_of_group(self, group_name: str) -> Tuple[List[List[int]], List[float]]:
-        r"""Return all hyperedges and weights in the specified hyperedge group.
-
-        Args:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-
-    @property
-    def v_property(self):
-        return self._v_property
-
-    @property
-    def e_property(self):
-        group_e_property = {}
-        for group in self._raw_groups:
-            group_e_property[group] = list(self._raw_groups[group].values())
-        return group_e_property
-
-    @property
-    def num_v(self) -> int:
-        r"""Return the number of vertices in the hypergraph."""
-        return self._num_v
-
-    @property
-    def num_e(self) -> int:
-        r"""Return the number of hyperedges in the hypergraph."""
-        _num_e = 0
-        for name in self.group_names:
-            _num_e += len(self._raw_groups[name])
-        return _num_e
-
-    def num_e_of_group(self, group_name: str) -> int:
-        r"""Return the number of hyperedges in the specified hyperedge group.
-
-        Args:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        return len(self._raw_groups[group_name])
-
-    @property
-    def num_groups(self) -> int:
-        r"""Return the number of hyperedge groups in the hypergraph."""
-        return len(self._raw_groups)
-
-    @property
-    def group_names(self) -> List[str]:
-        r"""Return the names of hyperedge groups in the hypergraph."""
-        return list(self._raw_groups.keys())
-
-    # properties for deep learning
-    @property
-    @abc.abstractmethod
-    def vars_for_DL(self) -> List[str]:
-        r"""Return a name list of available variables for deep learning in this type of hypergraph.
-        """
-
-    @property
-    def W_v(self) -> torch.Tensor:
-        r"""Return the vertex weight matrix of the hypergraph."""
-        if self.cache["W_v"] is None:
-            self.cache["W_v"] = torch.tensor(
-                self.v_weight, dtype=torch.float, device=self.device
-            ).view(-1, 1)
-        return self.cache["W_v"]
-
-    @property
-    def W_e(self) -> torch.Tensor:
-        r"""Return the hyperedge weight matrix of the hypergraph."""
-        if self.cache["W_e"] is None:
-            _tmp = [self.W_e_of_group(name) for name in self.group_names]
-            self.cache["W_e"] = torch.cat(_tmp, dim=0)
-        return self.cache["W_e"]
-
-    def W_e_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the hyperedge weight matrix of the specified hyperedge group.
-
-        Args:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name]["W_e"] is None:
-            self.group_cache[group_name]["W_e"] = self._fetch_W_of_group(group_name)
-        return self.group_cache[group_name]["W_e"]
-
-    @property
-    @abc.abstractmethod
-    def H(self) -> torch.Tensor:
-        r"""Return the hypergraph incidence matrix."""
-
-    @property
-    @abc.abstractmethod
-    def H_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the hypergraph incidence matrix in the specified hyperedge group.
-
-        Args:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-
-    @property
-    def H_v2e(self) -> torch.Tensor:
-        r"""Return the hypergraph incidence matrix with ``sparse matrix`` format."""
-        if self.cache.get("H_v2e") is None:
-            _tmp = [self.H_v2e_of_group(name) for name in self.group_names]
-            self.cache["H_v2e"] = torch.cat(_tmp, dim=1)
-        return self.cache["H_v2e"]
-
-    def H_v2e_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the hypergraph incidence matrix with ``sparse matrix`` format in the specified hyperedge group.
-
-        Args:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("H_v2e") is None:
-            self.group_cache[group_name]["H_v2e"] = self._fetch_H_of_group(
-                "v2e", group_name
-            )
-        return self.group_cache[group_name]["H_v2e"]
-
-    @property
-    def H_e2v(self) -> torch.Tensor:
-        r"""Return the hypergraph incidence matrix with ``sparse matrix`` format."""
-        if self.cache.get("H_e2v") is None:
-            _tmp = [self.H_e2v_of_group(name) for name in self.group_names]
-            self.cache["H_e2v"] = torch.cat(_tmp, dim=1)
-        return self.cache["H_e2v"]
-
-    def H_e2v_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the hypergraph incidence matrix with ``sparse matrix`` format in the specified hyperedge group.
-
-        Args:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("H_e2v") is None:
-            self.group_cache[group_name]["H_e2v"] = self._fetch_H_of_group(
-                "e2v", group_name
-            )
-        return self.group_cache[group_name]["H_e2v"]
-
-    @property
-    def R_v2e(self) -> torch.Tensor:
-        r"""Return the weight matrix of connections (vertices point to hyperedges) with ``sparse matrix`` format.
-        """
-        if self.cache.get("R_v2e") is None:
-            _tmp = [self.R_v2e_of_group(name) for name in self.group_names]
-            self.cache["R_v2e"] = torch.cat(_tmp, dim=1)
-        return self.cache["R_v2e"]
-
-    def R_v2e_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the weight matrix of connections (vertices point to hyperedges) with ``sparse matrix`` format in the specified hyperedge group.
-
-        Args:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("R_v2e") is None:
-            self.group_cache[group_name]["R_v2e"] = self._fetch_R_of_group(
-                "v2e", group_name
-            )
-        return self.group_cache[group_name]["R_v2e"]
-
-    @property
-    def R_e2v(self) -> torch.Tensor:
-        r"""Return the weight matrix of connections (hyperedges point to vertices) with ``sparse matrix`` format.
-        """
-        if self.cache.get("R_e2v") is None:
-            _tmp = [self.R_e2v_of_group(name) for name in self.group_names]
-            self.cache["R_e2v"] = torch.cat(_tmp, dim=1)
-        return self.cache["R_e2v"]
-
-    def R_e2v_of_group(self, group_name: str) -> torch.Tensor:
-        r"""Return the weight matrix of connections (hyperedges point to vertices) with ``sparse matrix`` format in the specified hyperedge group.
-
-        Args:
-            ``group_name`` (``str``): The name of the specified hyperedge group.
-        """
-        assert (
-            group_name in self.group_names
-        ), f"The specified {group_name} is not in existing hyperedge groups."
-        if self.group_cache[group_name].get("R_e2v") is None:
-            self.group_cache[group_name]["R_e2v"] = self._fetch_R_of_group(
-                "e2v", group_name
-            )
-        return self.group_cache[group_name]["R_e2v"]
-
-    # spectral-based smoothing
-    def smoothing(self, X: torch.Tensor, L: torch.Tensor, lamb: float) -> torch.Tensor:
-        r"""Spectral-based smoothing.
-
-        .. math::
-            X_{smoothed} = X + \lambda \mathcal{L} X
-
-        Args:
-            ``X`` (``torch.Tensor``): The vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``L`` (``torch.Tensor``): The Laplacian matrix with ``torch.sparse_coo_tensor`` format. Size :math:`(|\mathcal{V}|, |\mathcal{V}|)`.
-            ``lamb`` (``float``): :math:`\lambda`, the strength of smoothing.
-        """
-        return X + lamb * torch.sparse.mm(L, X)
-
-    # message passing functions
-    @abc.abstractmethod
-    def v2e_aggregation(
-        self,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        v2e_weight: Optional[torch.Tensor] = None,
-        drop_rate: float = 0.0,
-    ):
-        r"""Message aggretation step of ``vertices to hyperedges``.
-
-        Args:
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyepredges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-
-    @abc.abstractmethod
-    def v2e_aggregation_of_group(
-        self,
-        group_name: str,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        v2e_weight: Optional[torch.Tensor] = None,
-        drop_rate: float = 0.0,
-    ):
-        r"""Message aggregation step of ``vertices to hyperedges`` in specified hyperedge group.
-
-        Args:
-            ``group_name`` (``str``): The specified hyperedge group.
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyepredges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-
-    @abc.abstractmethod
-    def v2e_update(self, X: torch.Tensor, e_weight: Optional[torch.Tensor] = None):
-        r"""Message update step of ``vertices to hyperedges``.
-
-        Args:
-            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
-            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-
-    @abc.abstractmethod
-    def v2e_update_of_group(
-        self, group_name: str, X: torch.Tensor, e_weight: Optional[torch.Tensor] = None
-    ):
-        r"""Message update step of ``vertices to hyperedges`` in specified hyperedge group.
-
-        Args:
-            ``group_name`` (``str``): The specified hyperedge group.
-            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
-            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-
-    @abc.abstractmethod
-    def v2e(
-        self,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        v2e_weight: Optional[torch.Tensor] = None,
-        e_weight: Optional[torch.Tensor] = None,
-    ):
-        r"""Message passing of ``vertices to hyperedges``. The combination of ``v2e_aggregation`` and ``v2e_update``.
-
-        Args:
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyepredges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-
-    @abc.abstractmethod
-    def v2e_of_group(
-        self,
-        group_name: str,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        v2e_weight: Optional[torch.Tensor] = None,
-        e_weight: Optional[torch.Tensor] = None,
-    ):
-        r"""Message passing of ``vertices to hyperedges`` in specified hyperedge group. The combination of ``e2v_aggregation_of_group`` and ``e2v_update_of_group``.
-
-        Args:
-            ``group_name`` (``str``): The specified hyperedge group.
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyepredges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-
-    @abc.abstractmethod
-    def e2v_aggregation(
-        self,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        e2v_weight: Optional[torch.Tensor] = None,
-    ):
-        r"""Message aggregation step of ``hyperedges to vertices``.
-
-        Args:
-            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-
-    @abc.abstractmethod
-    def e2v_aggregation_of_group(
-        self,
-        group_name: str,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        e2v_weight: Optional[torch.Tensor] = None,
-    ):
-        r"""Message aggregation step of ``hyperedges to vertices`` in specified hyperedge group.
-
-        Args:
-            ``group_name`` (``str``): The specified hyperedge group.
-            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-
-    @abc.abstractmethod
-    def e2v_update(self, X: torch.Tensor, v_weight: Optional[torch.Tensor] = None):
-        r"""Message update step of ``hyperedges to vertices``.
-
-        Args:
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``v_weight`` (``torch.Tensor``, optional): The vertex weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-
-    @abc.abstractmethod
-    def e2v_update_of_group(
-        self, group_name: str, X: torch.Tensor, v_weight: Optional[torch.Tensor] = None
-    ):
-        r"""Message update step of ``hyperedges to vertices`` in specified hyperedge group.
-
-        Args:
-            ``group_name`` (``str``): The specified hyperedge group.
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``v_weight`` (``torch.Tensor``, optional): The vertex weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-
-    @abc.abstractmethod
-    def e2v(
-        self,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        e2v_weight: Optional[torch.Tensor] = None,
-        v_weight: Optional[torch.Tensor] = None,
-    ):
-        r"""Message passing of ``hyperedges to vertices``. The combination of ``e2v_aggregation`` and ``e2v_update``.
-
-        Args:
-            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``v_weight`` (``torch.Tensor``, optional): The vertex weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-
-    @abc.abstractmethod
-    def e2v_of_group(
-        self,
-        group_name: str,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        e2v_weight: Optional[torch.Tensor] = None,
-        v_weight: Optional[torch.Tensor] = None,
-    ):
-        r"""Message passing of ``hyperedges to vertices`` in specified hyperedge group. The combination of ``e2v_aggregation_of_group`` and ``e2v_update_of_group``.
-
-        Args:
-            ``group_name`` (``str``): The specified hyperedge group.
-            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
-            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``v_weight`` (``torch.Tensor``, optional): The vertex weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-
-    @abc.abstractmethod
-    def v2v(
-        self,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        v2e_aggr: Optional[str] = None,
-        v2e_weight: Optional[torch.Tensor] = None,
-        e_weight: Optional[torch.Tensor] = None,
-        e2v_aggr: Optional[str] = None,
-        e2v_weight: Optional[torch.Tensor] = None,
-        v_weight: Optional[torch.Tensor] = None,
-    ):
-        r"""Message passing of ``vertices to vertices``. The combination of ``v2e`` and ``e2v``.
-
-        Args:
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, this ``aggr`` will be used to both ``v2e`` and ``e2v``.
-            ``v2e_aggr`` (``str``, optional): The aggregation method for hyperedges to vertices. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``e2v``.
-            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyepredges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``e2v_aggr`` (``str``, optional): The aggregation method for vertices to hyperedges. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``v2e``.
-            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``v_weight`` (``torch.Tensor``, optional): The vertex weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
-
-    @abc.abstractmethod
-    def v2v_of_group(
-        self,
-        group_name: str,
-        X: torch.Tensor,
-        aggr: str = "mean",
-        v2e_aggr: Optional[str] = None,
-        v2e_weight: Optional[torch.Tensor] = None,
-        e_weight: Optional[torch.Tensor] = None,
-        e2v_aggr: Optional[str] = None,
-        e2v_weight: Optional[torch.Tensor] = None,
-        v_weight: Optional[torch.Tensor] = None,
-    ):
-        r"""Message passing of ``vertices to vertices`` in specified hyperedge group. The combination of ``v2e_of_group`` and ``e2v_of_group``.
-
-        Args:
-            ``group_name`` (``str``): The specified hyperedge group.
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, this ``aggr`` will be used to both ``v2e_of_group`` and ``e2v_of_group``.
-            ``v2e_aggr`` (``str``, optional): The aggregation method for hyperedges to vertices. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``e2v_of_group``.
-            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyepredges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``e2v_aggr`` (``str``, optional): The aggregation method for vertices to hyperedges. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``v2e_of_group``.
-            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-            ``v_weight`` (``torch.Tensor``, optional): The vertex weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
-        """
+import abc
+
+from collections import defaultdict
+from pathlib import Path
+from typing import Any
+from typing import Dict
+from typing import List
+from typing import Optional
+from typing import Tuple
+from typing import Union
+
+import numpy as np
+import torch
+
+from easygraph.utils.exception import EasyGraphError
+
+
+__all__ = ["load_structure", "BaseHypergraph"]
+
+
+def load_structure(file_path: Union[str, Path]):
+    r"""Load a EasyGraph's high-order network structure from a file. The supported structure ``Hypergraph``.
+
+    Args:
+        ``file_path`` (``Union[str, Path]``): The file path to load the EasyGraph's structure.
+    """
+    import pickle as pkl
+
+    import easygraph
+
+    file_path = Path(file_path)
+    assert file_path.exists(), f"{file_path} does not exist"
+    with open(file_path, "rb") as f:
+        data = pkl.load(f)
+    class_name, state_dict = data["class"], data["state_dict"]
+    structure_class = getattr(easygraph, class_name)
+    structure = structure_class.from_state_dict(state_dict)
+    return structure
+
+
+class BaseHypergraph:
+    r"""The ``BaseHypergraph`` class is the base class for all hypergraph structures.
+
+    Args:
+        ``num_v`` (``int``): The number of vertices.
+        ``e_list`` (``Union[List[int], List[List[int]]], optional``): Edge list. Defaults to ``None``.
+        ``e_weight`` (``Union[float, List[float]], optional``): A list of weights for edges. Defaults to ``None``.
+        ``extra_selfloop`` (``bool``, optional): Whether to add extra self-loop to the graph. Defaults to ``False``.
+        ``device`` (``torch.device``, optional): The device to store the graph. Defaults to ``torch.device('cpu')``.
+
+    """
+
+    def __init__(
+        self,
+        num_v: int,
+        v_property: Optional[Union[Dict, List[Dict]]] = None,
+        e_list: Optional[Union[List[int], List[List[int]]]] = None,
+        e_property: Optional[Union[Dict, List[Dict]]] = None,
+        e_weight: Optional[Union[float, List[float]]] = None,
+        extra_selfloop: bool = False,
+        device: torch.device = torch.device("cpu"),
+    ):
+        assert (
+            isinstance(num_v, int) and num_v > 0
+        ), "num_v should be a positive integer"
+        self.clear()
+        self._rows = []
+        self._cols = []
+        self._num_v = num_v
+        self.device = device
+        if v_property == None:
+            self._v_property = [{} for i in range(num_v)]
+        else:
+            v_property = self._format_v_property_list(num_v, v_property)
+            self._v_property = v_property
+
+        if e_property == None and e_list != None:
+            self._e_property = [{} for i in range(len(e_list))]
+        elif e_property != None and e_list != None:
+            e_property = self._format_e_property_list(len(e_list), e_property)
+            self._e_property = e_property
+
+        self._has_extra_selfloop = extra_selfloop
+
+    @abc.abstractmethod
+    def __repr__(self) -> str:
+        r"""Print the hypergraph information."""
+
+    @property
+    @abc.abstractmethod
+    def state_dict(self) -> Dict[str, Any]:
+        r"""Get the state dict of the hypergraph."""
+
+    @abc.abstractmethod
+    def save(self, file_path: Union[str, Path]):
+        r"""Save the EasyGraph's hypergraph structure to a file.
+
+        Args:
+            ``file_path`` (``str``): The file_path to store the EasyGraph's hypergraph structure.
+        """
+
+    @staticmethod
+    @abc.abstractmethod
+    def load(file_path: Union[str, Path]):
+        r"""Load the EasyGraph's hypergraph structure from a file.
+
+        Args:
+            ``file_path`` (``str``): The file path to load the DEasyGraph's hypergraph structure.
+        """
+
+    @staticmethod
+    @abc.abstractmethod
+    def from_state_dict(state_dict: dict):
+        r"""Load the EasyGraph's hypergraph structure from the state dict.
+
+        Args:
+            ``state_dict`` (``dict``): The state dict to load the EasyGraph's hypergraph.
+        """
+
+    @abc.abstractmethod
+    def draw(self, **kwargs):
+        r"""Draw the structure."""
+
+    def clear(self):
+        r"""Remove all hyperedges and caches from the hypergraph."""
+        self._clear_raw()
+        self._clear_cache()
+
+    def _clear_raw(self):
+        self._v_weight = None
+        self._raw_groups = {}
+
+    def _clear_cache(self, group_name: Optional[str] = None):
+        r"""Clear the cache."""
+        self.cache = {}
+        if group_name is None:
+            self.group_cache = defaultdict(dict)
+        else:
+            self.group_cache.pop(group_name, None)
+
+    @abc.abstractmethod
+    def clone(self) -> "BaseHypergraph":
+        r"""Return a copy of this type of hypergraph."""
+
+    def to(self, device: torch.device):
+        r"""Move the hypergraph to the specified device.
+
+        Args:
+            ``device`` (``torch.device``): The device to store the hypergraph.
+        """
+        self.device = device
+        for v in self.vars_for_DL:
+            if v in self.cache and self.cache[v] is not None:
+                self.cache[v] = self.cache[v].to(device)
+            for name in self.group_names:
+                if (
+                    v in self.group_cache[name]
+                    and self.group_cache[name][v] is not None
+                ):
+                    self.group_cache[name][v] = self.group_cache[name][v].to(device)
+        return self
+
+    # utils
+    def _hyperedge_code(self, src_v_set: List[int], dst_v_set: List[int]) -> Tuple:
+        r"""Generate the hyperedge code.
+
+        Args:
+            ``src_v_set`` (``List[int]``): The source vertex set.
+            ``dst_v_set`` (``List[int]``): The destination vertex set.
+        """
+        return tuple([src_v_set, dst_v_set])
+
+    def _merge_hyperedges(self, e1: dict, e2: dict, op: str = "mean"):
+        assert op in [
+            "mean",
+            "sum",
+            "max",
+        ], "Hyperedge merge operation must be one of ['mean', 'sum', 'max']"
+        _func = {
+            "mean": lambda x, y: (x + y) / 2,
+            "sum": lambda x, y: x + y,
+            "max": lambda x, y: max(x, y),
+        }
+        _e = {}
+        if "w_v2e" in e1 and "w_v2e" in e2:
+            for _idx in range(len(e1["w_v2e"])):
+                _e["w_v2e"] = _func[op](e1["w_v2e"][_idx], e2["w_v2e"][_idx])
+        if "w_e2v" in e1 and "w_e2v" in e2:
+            for _idx in range(len(e1["w_e2v"])):
+                _e["w_e2v"] = _func[op](e1["w_e2v"][_idx], e2["w_e2v"][_idx])
+        _e["w_e"] = _func[op](e1["w_e"], e2["w_e"])
+        return _e
+
+    @staticmethod
+    def _format_e_list(e_list: Union[List[int], List[List[int]]]) -> List[List[int]]:
+        r"""Format the hyperedge list.
+
+        Args:
+            ``e_list`` (``List[int]`` or ``List[List[int]]``): The hyperedge list.
+        """
+        if len(e_list) == 0:
+            pass
+        elif type(e_list[0]) in (int, float):
+            return [tuple(sorted(e_list))]
+        elif type(e_list) == tuple:
+            e_list = list(e_list)
+        elif type(e_list) == list:
+            pass
+        else:
+            raise TypeError("e_list must be List[int] or List[List[int]].")
+        for _idx in range(len(e_list)):
+            e_list[_idx] = tuple(sorted(e_list[_idx]))
+        return e_list
+
+    def _format_e_property_list(self, e_num, e_property_list: Union[Dict, List[Dict]]):
+        r"""Format the property list.
+
+        Args:
+            ``e_list`` (``Dict`` or ``List[Dict]``): The property list.
+        """
+        if type(e_property_list) == dict:
+            return [e_property_list]
+        elif type(e_property_list) == list and len(e_property_list) != e_num:
+            raise EasyGraphError(
+                "The length of property list must be equal to edge number"
+            )
+        elif type(e_property_list) == list:
+            pass
+        else:
+            raise TypeError("e_property_list must be Dict or List[Dict].")
+
+        return e_property_list
+
+    def _format_v_property_list(self, v_num, v_property_list: Union[Dict, List[Dict]]):
+        r"""Format the property list.
+
+        Args:
+            ``e_list`` (``Dict`` or ``List[Dict]``): The property list.
+        """
+        if type(v_property_list) == dict:
+            return [v_property_list]
+        elif type(v_property_list) == list and len(v_property_list) != v_num:
+            raise EasyGraphError(
+                "The length of property list must be equal to node number"
+            )
+        elif type(v_property_list) == list:
+            pass
+        else:
+            raise TypeError("v_property_list must be Dict or List[Dict].")
+
+        return v_property_list
+
+    @staticmethod
+    def _format_e_list_and_w_on_them(
+        e_list: Union[List[int], List[List[int]]],
+        w_list: Optional[Union[List[int], List[List[int]]]] = None,
+    ):
+        r"""Format ``e_list`` and ``w_list``.
+
+        Args:
+            ``e_list`` (Union[List[int], List[List[int]]]): Hyperedge list.
+            ``w_list`` (Optional[Union[List[int], List[List[int]]]]): Weights on connections. Defaults to ``None``.
+        """
+        bad_connection_msg = (
+            "The weight on connections between vertices and hyperedges must have the"
+            " same size as the hyperedges."
+        )
+        if isinstance(e_list, tuple):
+            e_list = list(e_list)
+        if w_list is not None and isinstance(w_list, tuple):
+            w_list = list(w_list)
+        if isinstance(e_list[0], int) and w_list is None:
+            w_list = [1] * len(e_list)
+            e_list, w_list = [e_list], [w_list]
+        elif isinstance(e_list[0], int) and w_list is not None:
+            assert len(e_list) == len(w_list), bad_connection_msg
+            e_list, w_list = [e_list], [w_list]
+        elif isinstance(e_list[0], list) and w_list is None:
+            w_list = [[1] * len(e) for e in e_list]
+        assert len(e_list) == len(w_list), bad_connection_msg
+        # TODO: this step can be speeded up
+        for idx in range(len(e_list)):
+            assert len(e_list[idx]) == len(w_list[idx]), bad_connection_msg
+            cur_e, cur_w = np.array(e_list[idx]), np.array(w_list[idx])
+            sorted_idx = np.argsort(cur_e)
+            e_list[idx] = tuple(cur_e[sorted_idx].tolist())
+            w_list[idx] = cur_w[sorted_idx].tolist()
+        return e_list, w_list
+
+    def _fetch_H(self, direction: str, group_name: str):
+        r"""Fetch the H matrix of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
+
+        Args:
+            ``direction`` (``str``): The direction of hyperedges can be either ``'v2e'`` or ``'e2v'``.
+            ``group_name`` (``str``): The name of the group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        assert direction in ["v2e", "e2v"], "direction must be one of ['v2e', 'e2v']"
+        if direction == "v2e":
+            select_idx = 0
+        else:
+            select_idx = 1
+        num_e = len(self._raw_groups[group_name])
+        e_idx, v_idx = [], []
+        for _e_idx, e in enumerate(self._raw_groups[group_name].keys()):
+            sub_e = e[select_idx]
+            v_idx.extend(sub_e)
+            e_idx.extend([_e_idx] * len(sub_e))
+
+        H = torch.sparse_coo_tensor(
+            torch.tensor([v_idx, e_idx], dtype=torch.long),
+            torch.ones(len(v_idx)),
+            torch.Size([self.num_v, num_e]),
+            device=self.device,
+        ).coalesce()
+        return H
+
+    def _fetch_H_of_group(self, direction: str, group_name: str):
+        r"""Fetch the H matrix of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
+
+        Args:
+            ``direction`` (``str``): The direction of hyperedges can be either ``'v2e'`` or ``'e2v'``.
+            ``group_name`` (``str``): The name of the group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        assert direction in ["v2e", "e2v"], "direction must be one of ['v2e', 'e2v']"
+        if direction == "v2e":
+            select_idx = 0
+        else:
+            select_idx = 1
+        num_e = len(self._raw_groups[group_name])
+        e_idx, v_idx = [], []
+        for _e_idx, e in enumerate(self._raw_groups[group_name].keys()):
+            sub_e = e[select_idx]
+            v_idx.extend(sub_e)
+            e_idx.extend([_e_idx] * len(sub_e))
+
+        H = torch.sparse_coo_tensor(
+            torch.tensor([v_idx, e_idx], dtype=torch.long),
+            torch.ones(len(v_idx)),
+            torch.Size([self.num_v, num_e]),
+            device=self.device,
+        ).coalesce()
+        return H
+
+    def _fetch_R_of_group(self, direction: str, group_name: str):
+        r"""Fetch the R matrix of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
+
+        Args:
+            ``direction`` (``str``): The direction of hyperedges can be either ``'v2e'`` or ``'e2v'``.
+            ``group_name`` (``str``): The name of the group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        assert direction in ["v2e", "e2v"], "direction must be one of ['v2e', 'e2v']"
+        if direction == "v2e":
+            select_idx = 0
+        else:
+            select_idx = 1
+        num_e = len(self._raw_groups[group_name])
+        e_idx, v_idx, w_list = [], [], []
+        for _e_idx, e in enumerate(self._raw_groups[group_name].keys()):
+            sub_e = e[select_idx]
+            v_idx.extend(sub_e)
+            e_idx.extend([_e_idx] * len(sub_e))
+            w_list.extend(self._raw_groups[group_name][e][f"w_{direction}"])
+        R = torch.sparse_coo_tensor(
+            torch.vstack([v_idx, e_idx]),
+            torch.tensor(w_list),
+            torch.Size([self.num_v, num_e]),
+            device=self.device,
+        ).coalesce()
+        return R
+
+    def _fetch_W_of_group(self, group_name: str):
+        r"""Fetch the W matrix of the specified hyperedge group with ``torch.sparse_coo_tensor`` format.
+
+        Args:
+            ``group_name`` (``str``): The name of the group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        w_list = [1.0] * len(self._raw_groups["main"])
+        W = torch.tensor(w_list, device=self.device).view((-1, 1))
+        return W
+
+    # some structure modification functions
+    def add_hyperedges(
+        self,
+        e_list_v2e: Union[List[int], List[List[int]]],
+        e_list_e2v: Union[List[int], List[List[int]]],
+        w_list_v2e: Optional[Union[List[float], List[List[float]]]] = None,
+        w_list_e2v: Optional[Union[List[float], List[List[float]]]] = None,
+        e_weight: Optional[Union[float, List[float]]] = None,
+        merge_op: str = "mean",
+        group_name: str = "main",
+    ):
+        r"""Add hyperedges to the hypergraph. If the ``group_name`` is not specified, the hyperedges will be added to the default ``main`` hyperedge group.
+
+        Args:
+            ``num_v`` (``int``): The number of vertices in the hypergraph.
+            ``e_list_v2e`` (``Union[List[int], List[List[int]]]``): A list of hyperedges describes how the vertices point to the hyperedges.
+            ``e_list_e2v`` (``Union[List[int], List[List[int]]]``): A list of hyperedges describes how the hyperedges point to the vertices.
+            ``w_list_v2e`` (``Union[List[float], List[List[float]]]``, optional): The weights are attached to the connections from vertices to hyperedges, which has the same shape
+                as ``e_list_v2e``. If set to ``None``, the value ``1`` is used for all connections. Defaults to ``None``.
+            ``w_list_e2v`` (``Union[List[float], List[List[float]]]``, optional): The weights are attached to the connections from the hyperedges to the vertices, which has the
+                same shape to ``e_list_e2v``. If set to ``None``, the value ``1`` is used for all connections. Defaults to ``None``.
+            ``e_weight`` (``Union[float, List[float]]``, optional): A list of weights for hyperedges. If set to ``None``, the value ``1`` is used for all hyperedges. Defaults to ``None``.
+            ``merge_op`` (``str``): The merge operation for the conflicting hyperedges. The possible values are ``mean``, ``sum``, ``max``, and ``min``. Defaults to ``mean``.
+            ``group_name`` (``str``, optional): The target hyperedge group to add these hyperedges. Defaults to the ``main`` hyperedge group.
+        """
+        e_list_v2e, w_list_v2e = self._format_e_list_and_w_on_them(
+            e_list_v2e, w_list_v2e
+        )
+        e_list_e2v, w_list_e2v = self._format_e_list_and_w_on_them(
+            e_list_e2v, w_list_e2v
+        )
+        if e_weight is None:
+            e_weight = [1.0] * len(e_list_v2e)
+        assert len(e_list_v2e) == len(
+            e_weight
+        ), "The number of hyperedges and the number of weights are not equal."
+        assert len(e_list_v2e) == len(
+            e_list_e2v
+        ), "Hyperedges of 'v2e' and 'e2v' must have the same size."
+        for _idx in range(len(e_list_v2e)):
+            self._add_hyperedge(
+                self._hyperedge_code(e_list_v2e[_idx], e_list_e2v[_idx]),
+                {
+                    "w_v2e": w_list_v2e[_idx],
+                    "w_e2v": w_list_e2v[_idx],
+                    "w_e": e_weight[_idx],
+                },
+                merge_op,
+                group_name,
+            )
+        self._clear_cache(group_name)
+
+    def _add_hyperedge(
+        self,
+        hyperedge_code: Tuple[List[int], List[int]],
+        content: Dict[str, Any],
+        merge_op: str,
+        group_name: str,
+    ):
+        r"""Add a hyperedge to the specified hyperedge group.
+
+        Args:
+            ``hyperedge_code`` (``Tuple[List[int], List[int]]``): The hyperedge code.
+            ``content`` (``Dict[str, Any]``): The content of the hyperedge.
+            ``merge_op`` (``str``): The merge operation for the conflicting hyperedges.
+            ``group_name`` (``str``): The target hyperedge group to add this hyperedge.
+        """
+        if group_name not in self._raw_groups:
+            self._raw_groups[group_name] = {}
+            self._raw_groups[group_name][hyperedge_code] = content
+        else:
+            if hyperedge_code not in self._raw_groups[group_name]:
+                self._raw_groups[group_name][hyperedge_code] = content
+            else:
+                self._raw_groups[group_name][hyperedge_code] = self._merge_hyperedges(
+                    self._raw_groups[group_name][hyperedge_code], content, merge_op
+                )
+
+    def remove_hyperedges(
+        self,
+        e_list_v2e: Union[List[int], List[List[int]]],
+        e_list_e2v: Union[List[int], List[List[int]]],
+        group_name: Optional[str] = None,
+    ):
+        r"""Remove the specified hyperedges from the hypergraph.
+
+        Args:
+            ``e_list_v2e`` (``Union[List[int], List[List[int]]]``): A list of hyperedges describes how the vertices point to the hyperedges.
+            ``e_list_e2v`` (``Union[List[int], List[List[int]]]``): A list of hyperedges describes how the hyperedges point to the vertices.
+            ``group_name`` (``str``, optional): Remove these hyperedges from the specified hyperedge group. If not specified, the function will
+                remove those hyperedges from all hyperedge groups. Defaults to the ``None``.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        assert len(e_list_v2e) == len(
+            e_list_e2v
+        ), "Hyperedges of 'v2e' and 'e2v' must have the same size."
+        e_list_v2e = self._format_e_list(e_list_v2e)
+        e_list_e2v = self._format_e_list(e_list_e2v)
+        if group_name is None:
+            for _idx in range(len(e_list_v2e)):
+                e_code = self._hyperedge_code(e_list_v2e[_idx], e_list_e2v[_idx])
+                for name in self.group_names:
+                    self._raw_groups[name].pop(e_code, None)
+        else:
+            for _idx in range(len(e_list_v2e)):
+                e_code = self._hyperedge_code(e_list_v2e[_idx], e_list_e2v[_idx])
+                self._raw_groups[group_name].pop(e_code, None)
+        self._clear_cache(group_name)
+
+    @abc.abstractmethod
+    def drop_hyperedges(self, drop_rate: float, ord="uniform"):
+        r"""Randomly drop hyperedges from the hypergraph. This function will return a new hypergraph with non-dropped hyperedges.
+
+        Args:
+            ``drop_rate`` (``float``): The drop rate of hyperedges.
+            ``ord`` (``str``): The order of dropping edges. Currently, only ``'uniform'`` is supported. Defaults to ``uniform``.
+        """
+
+    @abc.abstractmethod
+    def drop_hyperedges_of_group(
+        self, group_name: str, drop_rate: float, ord="uniform"
+    ):
+        r"""Randomly drop hyperedges from the specified hyperedge group. This function will return a new hypergraph with non-dropped hyperedges.
+
+        Args:
+            ``group_name`` (``str``): The name of the hyperedge group.
+            ``drop_rate`` (``float``): The drop rate of hyperedges.
+            ``ord`` (``str``): The order of dropping edges. Currently, only ``'uniform'`` is supported. Defaults to ``uniform``.
+        """
+
+    # properties for the hypergraph
+    @property
+    def v(self) -> List[int]:
+        r"""Return the list of vertices."""
+        if self.cache.get("v") is None:
+            self.cache["v"] = list(range(self.num_v))
+        return self.cache["v"]
+
+    @property
+    def v_weight(self) -> List[float]:
+        r"""Return the vertex weights of the hypergraph."""
+        if self._v_weight is None:
+            self._v_weight = [1.0] * self.num_v
+        return self._v_weight
+
+    @v_weight.setter
+    def v_weight(self, v_weight: List[float]):
+        r"""Set the vertex weights of the hypergraph."""
+        assert (
+            len(v_weight) == self.num_v
+        ), "The length of vertex weights must be equal to the number of vertices."
+        self._v_weight = v_weight
+        self._clear_cache()
+
+    @property
+    @abc.abstractmethod
+    def e(self) -> Tuple[List[List[int]], List[float]]:
+        r"""Return all hyperedges and weights in the hypergraph."""
+
+    @abc.abstractmethod
+    def e_of_group(self, group_name: str) -> Tuple[List[List[int]], List[float]]:
+        r"""Return all hyperedges and weights in the specified hyperedge group.
+
+        Args:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+
+    @property
+    def v_property(self):
+        return self._v_property
+
+    @property
+    def e_property(self):
+        group_e_property = {}
+        for group in self._raw_groups:
+            group_e_property[group] = list(self._raw_groups[group].values())
+        return group_e_property
+
+    @property
+    def num_v(self) -> int:
+        r"""Return the number of vertices in the hypergraph."""
+        return self._num_v
+
+    @property
+    def num_e(self) -> int:
+        r"""Return the number of hyperedges in the hypergraph."""
+        _num_e = 0
+        for name in self.group_names:
+            _num_e += len(self._raw_groups[name])
+        return _num_e
+
+    def num_e_of_group(self, group_name: str) -> int:
+        r"""Return the number of hyperedges in the specified hyperedge group.
+
+        Args:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        return len(self._raw_groups[group_name])
+
+    @property
+    def num_groups(self) -> int:
+        r"""Return the number of hyperedge groups in the hypergraph."""
+        return len(self._raw_groups)
+
+    @property
+    def group_names(self) -> List[str]:
+        r"""Return the names of hyperedge groups in the hypergraph."""
+        return list(self._raw_groups.keys())
+
+    # properties for deep learning
+    @property
+    @abc.abstractmethod
+    def vars_for_DL(self) -> List[str]:
+        r"""Return a name list of available variables for deep learning in this type of hypergraph.
+        """
+
+    @property
+    def W_v(self) -> torch.Tensor:
+        r"""Return the vertex weight matrix of the hypergraph."""
+        if self.cache["W_v"] is None:
+            self.cache["W_v"] = torch.tensor(
+                self.v_weight, dtype=torch.float, device=self.device
+            ).view(-1, 1)
+        return self.cache["W_v"]
+
+    @property
+    def W_e(self) -> torch.Tensor:
+        r"""Return the hyperedge weight matrix of the hypergraph."""
+        if self.cache["W_e"] is None:
+            _tmp = [self.W_e_of_group(name) for name in self.group_names]
+            self.cache["W_e"] = torch.cat(_tmp, dim=0)
+        return self.cache["W_e"]
+
+    def W_e_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the hyperedge weight matrix of the specified hyperedge group.
+
+        Args:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name]["W_e"] is None:
+            self.group_cache[group_name]["W_e"] = self._fetch_W_of_group(group_name)
+        return self.group_cache[group_name]["W_e"]
+
+    @property
+    @abc.abstractmethod
+    def H(self) -> torch.Tensor:
+        r"""Return the hypergraph incidence matrix."""
+
+    @property
+    @abc.abstractmethod
+    def H_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the hypergraph incidence matrix in the specified hyperedge group.
+
+        Args:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+
+    @property
+    def H_v2e(self) -> torch.Tensor:
+        r"""Return the hypergraph incidence matrix with ``sparse matrix`` format."""
+        if self.cache.get("H_v2e") is None:
+            _tmp = [self.H_v2e_of_group(name) for name in self.group_names]
+            self.cache["H_v2e"] = torch.cat(_tmp, dim=1)
+        return self.cache["H_v2e"]
+
+    def H_v2e_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the hypergraph incidence matrix with ``sparse matrix`` format in the specified hyperedge group.
+
+        Args:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("H_v2e") is None:
+            self.group_cache[group_name]["H_v2e"] = self._fetch_H_of_group(
+                "v2e", group_name
+            )
+        return self.group_cache[group_name]["H_v2e"]
+
+    @property
+    def H_e2v(self) -> torch.Tensor:
+        r"""Return the hypergraph incidence matrix with ``sparse matrix`` format."""
+        if self.cache.get("H_e2v") is None:
+            _tmp = [self.H_e2v_of_group(name) for name in self.group_names]
+            self.cache["H_e2v"] = torch.cat(_tmp, dim=1)
+        return self.cache["H_e2v"]
+
+    def H_e2v_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the hypergraph incidence matrix with ``sparse matrix`` format in the specified hyperedge group.
+
+        Args:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("H_e2v") is None:
+            self.group_cache[group_name]["H_e2v"] = self._fetch_H_of_group(
+                "e2v", group_name
+            )
+        return self.group_cache[group_name]["H_e2v"]
+
+    @property
+    def R_v2e(self) -> torch.Tensor:
+        r"""Return the weight matrix of connections (vertices point to hyperedges) with ``sparse matrix`` format.
+        """
+        if self.cache.get("R_v2e") is None:
+            _tmp = [self.R_v2e_of_group(name) for name in self.group_names]
+            self.cache["R_v2e"] = torch.cat(_tmp, dim=1)
+        return self.cache["R_v2e"]
+
+    def R_v2e_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the weight matrix of connections (vertices point to hyperedges) with ``sparse matrix`` format in the specified hyperedge group.
+
+        Args:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("R_v2e") is None:
+            self.group_cache[group_name]["R_v2e"] = self._fetch_R_of_group(
+                "v2e", group_name
+            )
+        return self.group_cache[group_name]["R_v2e"]
+
+    @property
+    def R_e2v(self) -> torch.Tensor:
+        r"""Return the weight matrix of connections (hyperedges point to vertices) with ``sparse matrix`` format.
+        """
+        if self.cache.get("R_e2v") is None:
+            _tmp = [self.R_e2v_of_group(name) for name in self.group_names]
+            self.cache["R_e2v"] = torch.cat(_tmp, dim=1)
+        return self.cache["R_e2v"]
+
+    def R_e2v_of_group(self, group_name: str) -> torch.Tensor:
+        r"""Return the weight matrix of connections (hyperedges point to vertices) with ``sparse matrix`` format in the specified hyperedge group.
+
+        Args:
+            ``group_name`` (``str``): The name of the specified hyperedge group.
+        """
+        assert (
+            group_name in self.group_names
+        ), f"The specified {group_name} is not in existing hyperedge groups."
+        if self.group_cache[group_name].get("R_e2v") is None:
+            self.group_cache[group_name]["R_e2v"] = self._fetch_R_of_group(
+                "e2v", group_name
+            )
+        return self.group_cache[group_name]["R_e2v"]
+
+    # spectral-based smoothing
+    def smoothing(self, X: torch.Tensor, L: torch.Tensor, lamb: float) -> torch.Tensor:
+        r"""Spectral-based smoothing.
+
+        .. math::
+            X_{smoothed} = X + \lambda \mathcal{L} X
+
+        Args:
+            ``X`` (``torch.Tensor``): The vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``L`` (``torch.Tensor``): The Laplacian matrix with ``torch.sparse_coo_tensor`` format. Size :math:`(|\mathcal{V}|, |\mathcal{V}|)`.
+            ``lamb`` (``float``): :math:`\lambda`, the strength of smoothing.
+        """
+        return X + lamb * torch.sparse.mm(L, X)
+
+    # message passing functions
+    @abc.abstractmethod
+    def v2e_aggregation(
+        self,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        v2e_weight: Optional[torch.Tensor] = None,
+        drop_rate: float = 0.0,
+    ):
+        r"""Message aggretation step of ``vertices to hyperedges``.
+
+        Args:
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyepredges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+
+    @abc.abstractmethod
+    def v2e_aggregation_of_group(
+        self,
+        group_name: str,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        v2e_weight: Optional[torch.Tensor] = None,
+        drop_rate: float = 0.0,
+    ):
+        r"""Message aggregation step of ``vertices to hyperedges`` in specified hyperedge group.
+
+        Args:
+            ``group_name`` (``str``): The specified hyperedge group.
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyepredges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+
+    @abc.abstractmethod
+    def v2e_update(self, X: torch.Tensor, e_weight: Optional[torch.Tensor] = None):
+        r"""Message update step of ``vertices to hyperedges``.
+
+        Args:
+            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
+            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+
+    @abc.abstractmethod
+    def v2e_update_of_group(
+        self, group_name: str, X: torch.Tensor, e_weight: Optional[torch.Tensor] = None
+    ):
+        r"""Message update step of ``vertices to hyperedges`` in specified hyperedge group.
+
+        Args:
+            ``group_name`` (``str``): The specified hyperedge group.
+            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
+            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+
+    @abc.abstractmethod
+    def v2e(
+        self,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        v2e_weight: Optional[torch.Tensor] = None,
+        e_weight: Optional[torch.Tensor] = None,
+    ):
+        r"""Message passing of ``vertices to hyperedges``. The combination of ``v2e_aggregation`` and ``v2e_update``.
+
+        Args:
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyepredges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+
+    @abc.abstractmethod
+    def v2e_of_group(
+        self,
+        group_name: str,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        v2e_weight: Optional[torch.Tensor] = None,
+        e_weight: Optional[torch.Tensor] = None,
+    ):
+        r"""Message passing of ``vertices to hyperedges`` in specified hyperedge group. The combination of ``e2v_aggregation_of_group`` and ``e2v_update_of_group``.
+
+        Args:
+            ``group_name`` (``str``): The specified hyperedge group.
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyepredges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+
+    @abc.abstractmethod
+    def e2v_aggregation(
+        self,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        e2v_weight: Optional[torch.Tensor] = None,
+    ):
+        r"""Message aggregation step of ``hyperedges to vertices``.
+
+        Args:
+            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+
+    @abc.abstractmethod
+    def e2v_aggregation_of_group(
+        self,
+        group_name: str,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        e2v_weight: Optional[torch.Tensor] = None,
+    ):
+        r"""Message aggregation step of ``hyperedges to vertices`` in specified hyperedge group.
+
+        Args:
+            ``group_name`` (``str``): The specified hyperedge group.
+            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+
+    @abc.abstractmethod
+    def e2v_update(self, X: torch.Tensor, v_weight: Optional[torch.Tensor] = None):
+        r"""Message update step of ``hyperedges to vertices``.
+
+        Args:
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``v_weight`` (``torch.Tensor``, optional): The vertex weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+
+    @abc.abstractmethod
+    def e2v_update_of_group(
+        self, group_name: str, X: torch.Tensor, v_weight: Optional[torch.Tensor] = None
+    ):
+        r"""Message update step of ``hyperedges to vertices`` in specified hyperedge group.
+
+        Args:
+            ``group_name`` (``str``): The specified hyperedge group.
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``v_weight`` (``torch.Tensor``, optional): The vertex weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+
+    @abc.abstractmethod
+    def e2v(
+        self,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        e2v_weight: Optional[torch.Tensor] = None,
+        v_weight: Optional[torch.Tensor] = None,
+    ):
+        r"""Message passing of ``hyperedges to vertices``. The combination of ``e2v_aggregation`` and ``e2v_update``.
+
+        Args:
+            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``v_weight`` (``torch.Tensor``, optional): The vertex weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+
+    @abc.abstractmethod
+    def e2v_of_group(
+        self,
+        group_name: str,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        e2v_weight: Optional[torch.Tensor] = None,
+        v_weight: Optional[torch.Tensor] = None,
+    ):
+        r"""Message passing of ``hyperedges to vertices`` in specified hyperedge group. The combination of ``e2v_aggregation_of_group`` and ``e2v_update_of_group``.
+
+        Args:
+            ``group_name`` (``str``): The specified hyperedge group.
+            ``X`` (``torch.Tensor``): Hyperedge feature matrix. Size :math:`(|\mathcal{E}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``.
+            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``v_weight`` (``torch.Tensor``, optional): The vertex weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+
+    @abc.abstractmethod
+    def v2v(
+        self,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        v2e_aggr: Optional[str] = None,
+        v2e_weight: Optional[torch.Tensor] = None,
+        e_weight: Optional[torch.Tensor] = None,
+        e2v_aggr: Optional[str] = None,
+        e2v_weight: Optional[torch.Tensor] = None,
+        v_weight: Optional[torch.Tensor] = None,
+    ):
+        r"""Message passing of ``vertices to vertices``. The combination of ``v2e`` and ``e2v``.
+
+        Args:
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, this ``aggr`` will be used to both ``v2e`` and ``e2v``.
+            ``v2e_aggr`` (``str``, optional): The aggregation method for hyperedges to vertices. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``e2v``.
+            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyepredges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``e2v_aggr`` (``str``, optional): The aggregation method for vertices to hyperedges. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``v2e``.
+            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``v_weight`` (``torch.Tensor``, optional): The vertex weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
+
+    @abc.abstractmethod
+    def v2v_of_group(
+        self,
+        group_name: str,
+        X: torch.Tensor,
+        aggr: str = "mean",
+        v2e_aggr: Optional[str] = None,
+        v2e_weight: Optional[torch.Tensor] = None,
+        e_weight: Optional[torch.Tensor] = None,
+        e2v_aggr: Optional[str] = None,
+        e2v_weight: Optional[torch.Tensor] = None,
+        v_weight: Optional[torch.Tensor] = None,
+    ):
+        r"""Message passing of ``vertices to vertices`` in specified hyperedge group. The combination of ``v2e_of_group`` and ``e2v_of_group``.
+
+        Args:
+            ``group_name`` (``str``): The specified hyperedge group.
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``aggr`` (``str``): The aggregation method. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, this ``aggr`` will be used to both ``v2e_of_group`` and ``e2v_of_group``.
+            ``v2e_aggr`` (``str``, optional): The aggregation method for hyperedges to vertices. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``e2v_of_group``.
+            ``v2e_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (vertices point to hyepredges). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``e_weight`` (``torch.Tensor``, optional): The hyperedge weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``e2v_aggr`` (``str``, optional): The aggregation method for vertices to hyperedges. Can be ``'mean'``, ``'sum'`` and ``'softmax_then_sum'``. If specified, it will override the ``aggr`` in ``v2e_of_group``.
+            ``e2v_weight`` (``torch.Tensor``, optional): The weight vector attached to connections (hyperedges point to vertices). If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+            ``v_weight`` (``torch.Tensor``, optional): The vertex weight vector. If not specified, the function will use the weights specified in hypergraph construction. Defaults to ``None``.
+        """
```

## easygraph/classes/graph.py

```diff
@@ -1,1633 +1,1633 @@
-import copy
-import warnings
-
-from copy import deepcopy
-from typing import Dict
-from typing import List
-from typing import Tuple
-
-import easygraph as eg
-import easygraph.convert as convert
-import torch
-
-from easygraph.utils.exception import EasyGraphError
-from easygraph.utils.sparse import sparse_dropout
-
-
-class Graph:
-    """
-    Base class for undirected graphs.
-
-        Nodes are allowed for any hashable Python objects, including int, string, dict, etc.
-        Edges are stored as Python dict type, with optional key/value attributes.
-
-    Parameters
-    ----------
-    graph_attr : keywords arguments, optional (default : None)
-        Attributes to add to graph as key=value pairs.
-
-    See Also
-    --------
-    DiGraph
-
-    Examples
-    --------
-    Create an empty undirected graph with no nodes and edges.
-
-    >>> G = eg.Graph()
-
-    Create a deep copy graph *G2* from existing Graph *G1*.
-
-    >>> G2 = G1.copy()
-
-    Create an graph with attributes.
-
-    >>> G = eg.Graph(name='Karate Club', date='2020.08.21')
-
-    **Attributes:**
-
-    Returns the adjacency matrix of the graph.
-
-    >>> G.adj
-
-    Returns all the nodes with their attributes.
-
-    >>> G.nodes
-
-    Returns all the edges with their attributes.
-
-    >>> G.edges
-
-    """
-
-    gnn_data_dict_factory = dict
-    raw_selfloop_dict = dict
-    graph_attr_dict_factory = dict
-    node_dict_factory = dict
-    node_attr_dict_factory = dict
-    adjlist_outer_dict_factory = dict
-    adjlist_inner_dict_factory = dict
-    edge_attr_dict_factory = dict
-    node_index_dict = dict
-
-    def __init__(self, incoming_graph_data=None, extra_selfloop=False, **graph_attr):
-        self.graph = self.graph_attr_dict_factory()
-        self._node = self.node_dict_factory()
-        self._adj = self.adjlist_outer_dict_factory()
-        self._raw_selfloop_dict = self.raw_selfloop_dict()
-        self.extra_selfloop = extra_selfloop
-        self._ndata = self.gnn_data_dict_factory()
-        self.cache = {}
-        self._node_index = self.node_index_dict()
-        self.cflag = 0
-        self._id = 0
-        self.device = "cpu"
-        if incoming_graph_data is not None:
-            convert.to_easygraph_graph(incoming_graph_data, create_using=self)
-        self.graph.update(graph_attr)
-
-    def __iter__(self):
-        return iter(self._node)
-
-    def __len__(self):
-        return len(self._node)
-
-    def __contains__(self, node):
-        try:
-            return node in self._node
-        except TypeError:
-            return False
-
-    def __getitem__(self, node):
-        # return list(self._adj[node].keys())
-        return self._adj[node]
-
-    @property
-    def ndata(self):
-        return self._ndata
-
-    @property
-    def adj(self):
-        """
-        Return the adjacency matrix
-        """
-        return self._adj
-
-    @property
-    def nodes(self):
-        """
-        return [node for node in self._node]
-        """
-        return self._node
-
-    @property
-    def node_index(self):
-        return self._node_index
-
-    @property
-    def edges(self):
-        """
-        Return an edge list
-        """
-        if self.cache.get("edges") != None:
-            return self.cache["edges"]
-        edge_lst = list()
-        seen = set()
-        for u in self._adj:
-            for v in self._adj[u]:
-                if (u, v) not in seen:
-                    seen.add((u, v))
-                    seen.add((v, u))
-                    edge_lst.append((u, v, self._adj[u][v]))
-        del seen
-        self.cache["edge"] = edge_lst
-        return self.cache["edge"]
-
-    @property
-    def name(self):
-        """String identifier of the graph.
-
-        This graph attribute appears in the attribute dict G.graph
-        keyed by the string `"name"`. as well as an attribute (technically
-        a property) `G.name`. This is entirely user controlled.
-        """
-        return self.graph.get("name", "")
-
-    @property
-    def e_both_side(self, weight="weight") -> Tuple[List[List], List[float]]:
-        r"""Return the list of edges including both directions."""
-        if self.cache.get("e_both_side") != None:
-            return self.cache["e_both_side"]
-        edges = list()
-        weights = list()
-        seen = set()
-        for u in self._adj:
-            for v in self._adj[u]:
-                if (u, v) not in seen:
-                    seen.add((u, v))
-                    seen.add((v, u))
-                    edges.append([u, v])
-                    edges.append([v, u])
-                    if weight not in self._adj[u][v]:
-                        warnings.warn("There is no property %s,default to 1" % (weight))
-                        weights.append(1.0)
-                        weights.append(1.0)
-                    else:
-                        weights.append(self._adj[u][v][weight])
-                        weights.append(self._adj[v][u][weight])
-        self.cache["e_both_side"] = (edges, weights)
-        return self.cache["e_both_side"]
-
-    @staticmethod
-    def from_hypergraph_hypergcn(
-        hypergraph,
-        feature,
-        with_mediator=False,
-        remove_selfloop=True,
-    ):
-        r"""Construct a graph from a hypergraph with methods proposed in `HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs <https://arxiv.org/pdf/1809.02589.pdf>`_ paper .
-
-        Args:
-            ``hypergraph`` (``Hypergraph``): The source hypergraph.
-            ``feature`` (``torch.Tensor``): The feature of the vertices.
-            ``with_mediator`` (``str``): Whether to use mediator to transform the hyperedges to edges in the graph. Defaults to ``False``.
-            ``remove_selfloop`` (``bool``): Whether to remove self-loop. Defaults to ``True``.
-            ``device`` (``torch.device``): The device to store the graph. Defaults to ``torch.device("cpu")``.
-        """
-
-        num_v = hypergraph.num_v
-        assert (
-            num_v == feature.shape[0]
-        ), "The number of vertices in hypergraph and feature.shape[0] must be equal!"
-        e_list, new_e_list, new_e_weight = hypergraph.e[0], [], []
-        rv = torch.rand((feature.shape[1], 1), device=feature.device)
-        for e in e_list:
-            num_v_in_e = len(e)
-            assert (
-                num_v_in_e >= 2
-            ), "The number of vertices in an edge must be greater than or equal to 2!"
-            p = torch.mm(feature[e, :], rv).squeeze()
-            v_a_idx, v_b_idx = torch.argmax(p), torch.argmin(p)
-            if not with_mediator:
-                new_e_list.append((e[v_a_idx], e[v_b_idx]))
-                new_e_weight.append(1.0 / num_v_in_e)
-            else:
-                w = 1.0 / (2 * num_v_in_e - 3)
-                for mid_v_idx in range(num_v_in_e):
-                    if mid_v_idx != v_a_idx and mid_v_idx != v_b_idx:
-                        new_e_list.append([e[v_a_idx], e[mid_v_idx]])
-                        new_e_weight.append(w)
-                        new_e_list.append([e[v_b_idx], e[mid_v_idx]])
-                        new_e_weight.append(w)
-        # remove selfloop
-        if remove_selfloop:
-            new_e_list = torch.tensor(new_e_list, dtype=torch.long)
-            new_e_weight = torch.tensor(new_e_weight, dtype=torch.float)
-            e_mask = (new_e_list[:, 0] != new_e_list[:, 1]).bool()
-            new_e_list = new_e_list[e_mask].numpy().tolist()
-            new_e_weight = new_e_weight[e_mask].numpy().tolist()
-
-        _g = Graph()
-
-        _g.add_nodes(list(range(0, num_v)))
-        for (
-            e,
-            w,
-        ) in zip(new_e_list, new_e_weight):
-            if _g.has_edge(e[0], e[1]):
-                _g.add_edge(e[0], e[1], weight=(w + _g.adj[e[0]][e[1]]["weight"]))
-            else:
-                _g.add_edge(e[0], e[1], weight=w)
-        now_edges = []
-        now_weight = []
-        for e in _g.edges:
-            now_edges.append((e[0], e[1]))
-            now_weight.append(e[2]["weight"])
-        now_edges.extend([(i, i) for i in range(num_v)])
-        now_weight.extend([1.0] * num_v)
-        _g.cache["e_both_side"] = (now_edges, now_weight)
-
-        return _g
-
-    @property
-    def A(self):
-        r"""Return the adjacency matrix :math:`\mathbf{A}` of the sample graph with ``torch.sparse_coo_tensor`` format. Size :math:`(|\mathcal{V}|, |\mathcal{V}|)`.
-        """
-        #  import torch
-
-        if self.cache.get("A", None) is None:
-            if len(self.edges) == 0:
-                self.cache["A"] = torch.sparse_coo_tensor(
-                    size=(len(self.nodes), len(self.nodes)), device=self.device
-                )
-            else:
-                if self.cache.get("e_both_side") is not None:
-                    e_list, e_weight = self.cache["e_both_side"]
-
-                else:
-                    e_list, e_weight = self.e_both_side
-
-                node_size = len(self.nodes)
-                self.cache["A"] = torch.sparse_coo_tensor(
-                    indices=torch.tensor(e_list, dtype=torch.int).t(),
-                    values=torch.tensor(e_weight),
-                    size=(node_size, node_size),
-                    device=self.device,
-                ).coalesce()
-        return self.cache["A"]
-
-    @property
-    def D_v_neg_1_2(
-        self,
-    ):
-        r"""Return the normalized diagonal matrix of vertex degree :math:`\mathbf{D}_v^{-\frac{1}{2}}` with ``torch.sparse_coo_tensor`` format. Size :math:`(|\mathcal{V}|, |\mathcal{V}|)`.
-        """
-        # import torch
-
-        if self.cache.get("D_v_neg_1_2") is None:
-            if self.cache.get("D_v_value") is None:
-                self.cache["D_v_value"] = (
-                    torch.sparse.sum(self.A, dim=1).to_dense().view(-1)
-                )
-                # self.cache["D_v_value"] = torch.tensor(list(self.degree().values())).float()
-
-            _mat = self.cache["D_v_value"]
-            # _mat = _tmp
-            _val = _mat**-0.5
-            _val[torch.isinf(_val)] = 0
-            nodes_num = len(self.nodes)
-            self.cache["D_v_neg_1_2"] = torch.sparse_coo_tensor(
-                torch.arange(0, len(self.nodes)).view(1, -1).repeat(2, 1),
-                _val,
-                torch.Size([nodes_num, nodes_num]),
-                device=self.device,
-            ).coalesce()
-        return self.cache["D_v_neg_1_2"]
-
-    @property
-    def index2node(self):
-        """
-        Assign an integer index for each node (start from 0)
-        """
-        if self.cache.get("index2node", None) is None:
-            index2node_dict = {}
-            index = 0
-            # for index in range(0, len(self.nodes)):
-
-            for index, n in enumerate(self.nodes):
-                index2node_dict[index] = n
-                # index += 1
-            self.cache["index2node"] = index2node_dict
-        return self.cache["index2node"]
-
-    @property
-    def node2index(self):
-        """
-        Assign an integer index for each node (start from 0)
-        """
-        if self.cache.get("node2index", None) is None:
-            node2index_dict = {}
-            index = 0
-            for n in self.nodes:
-                node2index_dict[n] = index
-                index += 1
-            self.cache["node2index"] = node2index_dict
-        return self.cache["node2index"]
-
-    @property
-    def e(self) -> Tuple[List[List[int]], List[float]]:
-        r"""Return the edge list, weight list and property list in the graph."""
-
-        if self.cache.get("e", None) is None:
-            node2index = self.node2index
-            e_list = [
-                (node2index[src_idx], node2index[dst_idx])
-                for src_idx, dst_idx, d in self.edges
-            ]
-            w_list = []
-            e_property_list = []
-            v_property_list = []
-
-            node_size = len(self.nodes)
-            for i in range(0, node_size):
-                v_property_list.append(self.nodes[self.index2node[i]])
-
-            for d in self.edges:
-                if "weight" not in d[2]:
-                    w_list.append(1.0)
-                    e_property_list.append(d[2])
-                else:
-                    w_list.append(d[2]["weight"])
-                    tmp_dict = copy.deepcopy(d[2])
-                    del tmp_dict["weight"]
-                    e_property_list.append(tmp_dict)
-
-            self.cache["e"] = e_list, w_list, v_property_list, e_property_list
-        return self.cache["e"]
-
-    @property
-    def D_v(self):
-        r"""Return the diagonal matrix of vertex degree :math:`\mathbf{D}_v` with ``torch.sparse_coo_tensor`` format. Size :math:`(|\mathcal{V}|, |\mathcal{V}|)`.
-        """
-
-        if self.cache.get("D_v") is None:
-            # print("self.A:",self.A)
-            _tmp = torch.sparse.sum(self.A, dim=1).to_dense().clone().view(-1)
-
-            nodes_num = len(self.nodes)
-            self.cache["D_v"] = torch.sparse_csr_tensor(
-                torch.arange(0, nodes_num + 1),
-                torch.arange(0, nodes_num),
-                _tmp,
-                torch.Size([nodes_num, nodes_num]),
-                device=self.device,
-            )
-
-            # self.cache["D_v"] = torch.sparse_coo_tensor(
-            #     torch.arange(0, len(self.nodes)).view(1, -1).repeat(2, 1),
-            #     _tmp,
-            #     torch.Size([len(self.nodes), len(self.nodes)]),
-            #     device=self.device,
-            # ).coalesce()
-        return self.cache["D_v"]
-
-    def add_extra_selfloop(self):
-        r"""Add extra selfloops to the graph."""
-        self._has_extra_selfloop = True
-        self._clear_cache()
-
-    def remove_extra_selfloop(self):
-        r"""Remove extra selfloops from the graph."""
-        self._has_extra_selfloop = False
-        self._clear_cache()
-
-    def remove_selfloop(self):
-        r"""Remove all selfloops from the graph."""
-        self._raw_selfloop_dict.clear()
-        self.remove_extra_selfloop()
-        self._clear_cache()
-
-    def nbr_v(self, v_idx: int) -> Tuple[List[int], List[float]]:
-        r"""Return a vertex list of the neighbors of the vertex ``v_idx``.
-
-        Args:
-            ``v_idx`` (``int``): The index of the vertex.
-        """
-        return self.N_v(v_idx).cpu().numpy().tolist()
-
-    def N_v(self, v_idx: int) -> Tuple[List[int], List[float]]:
-        r"""Return the neighbors of the vertex ``v_idx`` with ``torch.Tensor`` format.
-
-        Args:
-            ``v_idx`` (``int``): The index of the vertex.
-        """
-        sub_v_set = self.A[v_idx]._indices()[0].clone()
-        return sub_v_set
-
-    def clone(self):
-        r"""Clone the graph."""
-        # _g = Graph(self.num_v, extra_selfloop=self._has_extra_selfloop, device=self.device)
-        # _g=self.__class__()
-        # _g.device="cpu"
-        # _g.extra_selfloop=False
-        # _g.edges = deepcopy(self.edges)
-        # _g.cache = deepcopy(self.cache)
-        return self.copy()
-
-    @name.setter
-    def name(self, s):
-        """
-        Set graph name
-
-        Parameters
-        ----------
-        s : name
-        """
-        self.graph["name"] = s
-
-    def degree(self, weight="weight"):
-        """Returns the weighted degree of of each node.
-
-        Parameters
-        ----------
-        weight : string, optional (default: 'weight')
-            Weight key of the original weighted graph.
-
-        Returns
-        -------
-        degree : dict
-            Each node's (key) weighted degree (value).
-
-        Notes
-        -----
-        If the graph is not weighted, all the weights will be regarded as 1.
-
-        Examples
-        --------
-        You can call with no attributes, if 'weight' is the weight key:
-
-        >>> G.degree()
-
-        if you have customized weight key 'weight_1'.
-
-        >>> G.degree(weight='weight_1')
-
-        """
-        if self.cache.get("degree") != None:
-            return self.cache["degree"]
-        degree = dict()
-        for u, v, d in self.edges:
-            if u in degree:
-                degree[u] += d.get(weight, 1)
-            else:
-                degree[u] = d.get(weight, 1)
-            if v in degree:
-                degree[v] += d.get(weight, 1)
-            else:
-                degree[v] = d.get(weight, 1)
-
-        # For isolated nodes
-        for node in self.nodes:
-            if node not in degree:
-                degree[node] = 0
-        self.cache["degree"] = degree
-        return degree
-
-    def order(self):
-        """Returns the number of nodes in the graph.
-
-        Returns
-        -------
-        nnodes : int
-            The number of nodes in the graph.
-
-        See Also
-        --------
-        number_of_nodes: identical method
-        __len__: identical method
-
-        Examples
-        --------
-        >>> G = eg.path_graph(3)  # or DiGraph, MultiGraph, MultiDiGraph, etc
-        >>> G.order()
-        3
-        """
-        return len(self._node)
-
-    def size(self, weight=None):
-        """Returns the number of edges or total of all edge weights.
-
-        Parameters
-        -----------
-        weight : String or None, optional
-            The weight key. If None, it will calculate the number of
-            edges, instead of total of all edge weights.
-
-        Returns
-        -------
-        size : int or float, optional (default: None)
-            The number of edges or total of all edge weights.
-
-        Examples
-        --------
-
-        Returns the number of edges in G:
-
-        >>> G.size()
-
-        Returns the total of all edge weights in G:
-
-        >>> G.size(weight='weight')
-
-        """
-        if self.cache.get("size") != None:
-            return self.cache["size"]
-        s = sum(d for v, d in self.degree(weight=weight).items())
-        self.cache["size"] = s // 2 if weight is None else s / 2
-        return self.cache["size"]
-
-    # GCN Laplacian smoothing
-    @property
-    def L_GCN(self):
-        r"""Return the GCN Laplacian matrix :math:`\mathcal{L}_{GCN}` of the graph with ``torch.sparse_coo_tensor`` format. Size :math:`(|\mathcal{V}|, |\mathcal{V}|)`.
-
-        .. math::
-            \mathcal{L}_{GCN} = \mathbf{\hat{D}}_v^{-\frac{1}{2}} \mathbf{\hat{A}} \mathbf{\hat{D}}_v^{-\frac{1}{2}}
-
-        """
-        if self.cache.get("L_GCN") is None:
-            # self.add_extra_selfloop()
-            self.cache["L_GCN"] = (
-                self.D_v_neg_1_2.mm(self.A).mm(self.D_v_neg_1_2).coalesce()
-            )
-        return self.cache["L_GCN"]
-
-    def smoothing_with_GCN(self, X, drop_rate=0.0):
-        r"""Return the smoothed feature matrix with GCN Laplacian matrix :math:`\mathcal{L}_{GCN}`.
-
-        Args:
-            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
-            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in adjacency matrix with probability ``drop_rate``. Default: ``0.0``.
-        """
-        if drop_rate > 0.0:
-            L_GCN = sparse_dropout(self.L_GCN, drop_rate)
-        else:
-            L_GCN = self.L_GCN
-
-        return torch.sparse.mm(L_GCN, X)
-
-    def number_of_edges(self, u=None, v=None):
-        """Returns the number of edges between two nodes.
-
-        Parameters
-        ----------
-        u, v : nodes, optional (default=all edges)
-            If u and v are specified, return the number of edges between
-            u and v. Otherwise return the total number of all edges.
-
-        Returns
-        -------
-        nedges : int
-            The number of edges in the graph.  If nodes `u` and `v` are
-            specified return the number of edges between those nodes. If
-            the graph is directed, this only returns the number of edges
-            from `u` to `v`.
-
-        See Also
-        --------
-        size
-
-        Examples
-        --------
-        For undirected graphs, this method counts the total number of
-        edges in the graph:
-
-        >>> G = eg.path_graph(4)
-        >>> G.number_of_edges()
-        3
-
-        If you specify two nodes, this counts the total number of edges
-        joining the two nodes:
-
-        >>> G.number_of_edges(0, 1)
-        1
-
-        For directed graphs, this method can count the total number of
-        directed edges from `u` to `v`:
-
-        >>> G = eg.DiGraph()
-        >>> G.add_edge(0, 1)
-        >>> G.add_edge(1, 0)
-        >>> G.number_of_edges(0, 1)
-        1
-
-        """
-        if u is None:
-            return int(self.size())
-        if v in self._adj[u]:
-            return 1
-        return 0
-
-    def nbunch_iter(self, nbunch=None):
-        """Returns an iterator over nodes contained in nbunch that are
-        also in the graph.
-
-        The nodes in nbunch are checked for membership in the graph
-        and if not are silently ignored.
-
-        Parameters
-        ----------
-        nbunch : single node, container, or all nodes (default= all nodes)
-            The view will only report edges incident to these nodes.
-
-        Returns
-        -------
-        niter : iterator
-            An iterator over nodes in nbunch that are also in the graph.
-            If nbunch is None, iterate over all nodes in the graph.
-
-        Raises
-        ------
-        EasyGraphError
-            If nbunch is not a node or sequence of nodes.
-            If a node in nbunch is not hashable.
-
-        See Also
-        --------
-        Graph.__iter__
-
-        Notes
-        -----
-        When nbunch is an iterator, the returned iterator yields values
-        directly from nbunch, becoming exhausted when nbunch is exhausted.
-
-        To test whether nbunch is a single node, one can use
-        "if nbunch in self:", even after processing with this routine.
-
-        If nbunch is not a node or a (possibly empty) sequence/iterator
-        or None, a :exc:`EasyGraphError` is raised.  Also, if any object in
-        nbunch is not hashable, a :exc:`EasyGraphError` is raised.
-        """
-        if nbunch is None:  # include all nodes via iterator
-            bunch = iter(self._adj)
-        elif nbunch in self:  # if nbunch is a single node
-            bunch = iter([nbunch])
-        else:  # if nbunch is a sequence of nodes
-
-            def bunch_iter(nlist, adj):
-                try:
-                    for n in nlist:
-                        if n in adj:
-                            yield n
-                except TypeError as err:
-                    exc, message = err, err.args[0]
-                    # capture error for non-sequence/iterator nbunch.
-                    if "iter" in message:
-                        exc = EasyGraphError(
-                            "nbunch is not a node or a sequence of nodes."
-                        )
-                    # capture error for unhashable node.
-                    if "hashable" in message:
-                        exc = EasyGraphError(
-                            f"Node {n} in sequence nbunch is not a valid node."
-                        )
-                    raise exc
-
-            bunch = bunch_iter(nbunch, self._adj)
-        return bunch
-
-    def neighbors(self, node):
-        """Returns an iterator of a node's neighbors.
-
-        Parameters
-        ----------
-        node : Hashable
-            The target node.
-
-        Returns
-        -------
-        neighbors : iterator
-            An iterator of a node's neighbors.
-
-        Examples
-        --------
-        >>> G = eg.Graph()
-        >>> G.add_edges([(1,2), (2,3), (2,4)])
-        >>> for neighbor in G.neighbors(node=2):
-        ...     print(neighbor)
-
-        """
-        try:
-            return iter(self._adj[node])
-        except KeyError:
-            print("No node {}".format(node))
-
-    all_neighbors = neighbors
-
-    def add_node(self, node_for_adding, **node_attr):
-        """Add one node
-
-        Add one node, type of which is any hashable Python object, such as int, string, dict, or even Graph itself.
-        You can add with node attributes using Python dict type.
-
-        Parameters
-        ----------
-        node_for_adding : any hashable Python object
-            Nodes for adding.
-
-        node_attr : keywords arguments, optional
-            The node attributes.
-            You can customize them with different key-value pairs.
-
-        See Also
-        --------
-        add_nodes
-
-        Examples
-        --------
-        >>> G.add_node('a')
-        >>> G.add_node('hello world')
-        >>> G.add_node('Jack', age=10)
-
-        >>> G.add_node('Jack', **{
-        ...     'age': 10,
-        ...     'gender': 'M'
-        ... })
-
-        """
-        self._add_one_node(node_for_adding, node_attr)
-        self._clear_cache()
-
-    def add_nodes(self, nodes_for_adding: list, nodes_attr: List[Dict] = []):
-        """Add nodes with a list of nodes.
-
-        Parameters
-        ----------
-        nodes_for_adding : list
-
-        nodes_attr : list of dict
-            The corresponding attribute for each of *nodes_for_adding*.
-
-        See Also
-        --------
-        add_node
-
-        Examples
-        --------
-        Add nodes with a list of nodes.
-        You can add with node attributes using a list of Python dict type,
-        each of which is the attribute of each node, respectively.
-
-        >>> G.add_nodes([1, 2, 'a', 'b'])
-        >>> G.add_nodes(range(1, 200))
-
-        >>> G.add_nodes(['Jack', 'Tom', 'Lily'], nodes_attr=[
-        ...     {
-        ...         'age': 10,
-        ...         'gender': 'M'
-        ...     },
-        ...     {
-        ...         'age': 11,
-        ...         'gender': 'M'
-        ...     },
-        ...     {
-        ...         'age': 10,
-        ...         'gender': 'F'
-        ...     }
-        ... ])
-
-        """
-        if not len(nodes_attr) == 0:  # Nodes attributes included in input
-            assert len(nodes_for_adding) == len(
-                nodes_attr
-            ), "Nodes and Attributes lists must have same length."
-        else:  # Set empty attribute for each node
-            nodes_attr = [dict() for i in range(len(nodes_for_adding))]
-
-        for i in range(len(nodes_for_adding)):
-            try:
-                self._add_one_node(nodes_for_adding[i], nodes_attr[i])
-            except Exception as err:
-                print(err)
-                pass
-        self._clear_cache()
-
-    def add_nodes_from(self, nodes_for_adding, **attr):
-        """Add multiple nodes.
-
-        Parameters
-        ----------
-        nodes_for_adding : iterable container
-            A container of nodes (list, dict, set, etc.).
-            OR
-            A container of (node, attribute dict) tuples.
-            Node attributes are updated using the attribute dict.
-        attr : keyword arguments, optional (default= no attributes)
-            Update attributes for all nodes in nodes.
-            Node attributes specified in nodes as a tuple take
-            precedence over attributes specified via keyword arguments.
-
-        See Also
-        --------
-        add_node
-
-        Examples
-        --------
-        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
-        >>> G.add_nodes_from("Hello")
-        >>> K3 = eg.Graph([(0, 1), (1, 2), (2, 0)])
-        >>> G.add_nodes_from(K3)
-        >>> sorted(G.nodes(), key=str)
-        [0, 1, 2, 'H', 'e', 'l', 'o']
-
-        Use keywords to update specific node attributes for every node.
-
-        >>> G.add_nodes_from([1, 2], size=10)
-        >>> G.add_nodes_from([3, 4], weight=0.4)
-
-        Use (node, attrdict) tuples to update attributes for specific nodes.
-
-        >>> G.add_nodes_from([(1, dict(size=11)), (2, {"color": "blue"})])
-        >>> G.nodes[1]["size"]
-        11
-        >>> H = eg.Graph()
-        >>> H.add_nodes_from(G.nodes(data=True))
-        >>> H.nodes[1]["size"]
-        11
-
-        """
-        for n in nodes_for_adding:
-            try:
-                newnode = n not in self._node
-                newdict = attr
-            except TypeError:
-                n, ndict = n
-                newnode = n not in self._node
-                newdict = attr.copy()
-                newdict.update(ndict)
-            if newnode:
-                if n is None:
-                    raise ValueError("None cannot be a node")
-                self._adj[n] = self.adjlist_inner_dict_factory()
-                self._node[n] = self.node_attr_dict_factory()
-            self._node[n].update(newdict)
-        self._clear_cache()
-
-    def _add_one_node(self, one_node_for_adding, node_attr: dict = {}):
-        node = one_node_for_adding
-        if node not in self._node:
-            self._node_index[node] = self._id
-            self._id += 1
-            self._adj[node] = self.adjlist_inner_dict_factory()
-            attr_dict = self._node[node] = self.node_attr_dict_factory()
-            attr_dict.update(node_attr)
-        else:  # If already exists, there is no complain and still updating the node attribute
-            self._node[node].update(node_attr)
-        self._clear_cache()
-
-    def add_edge(self, u_of_edge, v_of_edge, **edge_attr):
-        """Add one edge.
-
-        Parameters
-        ----------
-        u_of_edge : object
-            One end of this edge
-
-        v_of_edge : object
-            The other one end of this edge
-
-        edge_attr : keywords arguments, optional
-            The attribute of the edge.
-
-        Notes
-        -----
-        Nodes of this edge will be automatically added to the graph, if they do not exist.
-
-        See Also
-        --------
-        add_edges
-
-        Examples
-        --------
-
-        >>> G.add_edge(1,2)
-        >>> G.add_edge('Jack', 'Tom', weight=10)
-
-        Add edge with attributes, edge weight, for example,
-
-        >>> G.add_edge(1, 2, **{
-        ...     'weight': 20
-        ... })
-
-        """
-        self._add_one_edge(u_of_edge, v_of_edge, edge_attr)
-        self._clear_cache()
-
-    def add_weighted_edge(self, u_of_edge, v_of_edge, weight):
-        """Add a weighted edge
-
-        Parameters
-        ----------
-        u_of_edge : start node
-
-        v_of_edge : end node
-
-        weight : weight value
-
-        Examples
-        --------
-        Add a weighted edge
-
-        >>> G.add_weighted_edge( 1 , 3 , 1.0)
-
-        """
-        self._add_one_edge(u_of_edge, v_of_edge, edge_attr={"weight": weight})
-        self._clear_cache()
-
-    def add_edges(self, edges_for_adding, edges_attr: List[Dict] = []):
-        """Add a list of edges.
-
-        Parameters
-        ----------
-        edges_for_adding : list of 2-element tuple
-            The edges for adding. Each element is a (u, v) tuple, and u, v are
-            two ends of the edge.
-
-        edges_attr : list of dict, optional
-            The corresponding attributes for each edge in *edges_for_adding*.
-
-        Examples
-        --------
-        Add a list of edges into *G*
-
-        >>> G.add_edges([
-        ...     (1, 2),
-        ...     (3, 4),
-        ...     ('Jack', 'Tom')
-        ... ])
-
-        Add edge with attributes, for example, edge weight,
-
-        >>> G.add_edges([(1,2), (2, 3)], edges_attr=[
-        ...     {
-        ...         'weight': 20
-        ...     },
-        ...     {
-        ...         'weight': 15
-        ...     }
-        ... ])
-
-        """
-        if edges_attr is None:
-            edges_attr = []
-        if not len(edges_attr) == 0:  # Edges attributes included in input
-            assert len(edges_for_adding) == len(
-                edges_attr
-            ), "Edges and Attributes lists must have same length."
-        else:  # Set empty attribute for each edge
-            edges_attr = [dict() for i in range(len(edges_for_adding))]
-
-        for i in range(len(edges_for_adding)):
-            try:
-                edge = edges_for_adding[i]
-                attr = edges_attr[i]
-                assert len(edge) == 2, "Edge tuple {} must be 2-tuple.".format(edge)
-                self._add_one_edge(edge[0], edge[1], attr)
-            except Exception as err:
-                print(err)
-        self._clear_cache()
-
-    def add_edges_from(self, ebunch_to_add, **attr):
-        """Add all the edges in ebunch_to_add.
-
-        Parameters
-        ----------
-        ebunch_to_add : container of edges
-            Each edge given in the container will be added to the
-            graph. The edges must be given as 2-tuples (u, v) or
-            3-tuples (u, v, d) where d is a dictionary containing edge data.
-        attr : keyword arguments, optional
-            Edge data (or labels or objects) can be assigned using
-            keyword arguments.
-
-        See Also
-        --------
-        add_edge : add a single edge
-        add_weighted_edges_from : convenient way to add weighted edges
-
-        Notes
-        -----
-        Adding the same edge twice has no effect but any edge data
-        will be updated when each duplicate edge is added.
-
-        Edge attributes specified in an ebunch take precedence over
-        attributes specified via keyword arguments.
-
-        Examples
-        --------
-        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
-        >>> G.add_edges_from([(0, 1), (1, 2)])  # using a list of edge tuples
-        >>> e = zip(range(0, 3), range(1, 4))
-        >>> G.add_edges_from(e)  # Add the path graph 0-1-2-3
-
-        Associate data to edges
-
-        >>> G.add_edges_from([(1, 2), (2, 3)], weight=3)
-        >>> G.add_edges_from([(3, 4), (1, 4)], label="WN2898")
-        """
-        for e in ebunch_to_add:
-            ne = len(e)
-            if ne == 3:
-                u, v, dd = e
-            elif ne == 2:
-                u, v = e
-                dd = {}  # doesn't need edge_attr_dict_factory
-            else:
-                raise EasyGraphError(f"Edge tuple {e} must be a 2-tuple or 3-tuple.")
-            if u not in self._node:
-                if u is None:
-                    raise ValueError("None cannot be a node")
-                self._adj[u] = self.adjlist_inner_dict_factory()
-                self._node[u] = self.node_attr_dict_factory()
-            if v not in self._node:
-                if v is None:
-                    raise ValueError("None cannot be a node")
-                self._adj[v] = self.adjlist_inner_dict_factory()
-                self._node[v] = self.node_attr_dict_factory()
-            datadict = self._adj[u].get(v, self.edge_attr_dict_factory())
-            datadict.update(attr)
-            datadict.update(dd)
-            self._adj[u][v] = datadict
-            self._adj[v][u] = datadict
-        self._clear_cache()
-
-    def add_weighted_edges_from(self, ebunch_to_add, weight="weight", **attr):
-        """Add weighted edges in `ebunch_to_add` with specified weight attr
-
-        Parameters
-        ----------
-        ebunch_to_add : container of edges
-            Each edge given in the list or container will be added
-            to the graph. The edges must be given as 3-tuples (u, v, w)
-            where w is a number.
-        weight : string, optional (default= 'weight')
-            The attribute name for the edge weights to be added.
-        attr : keyword arguments, optional (default= no attributes)
-            Edge attributes to add/update for all edges.
-
-        See Also
-        --------
-        add_edge : add a single edge
-        add_edges_from : add multiple edges
-
-        Notes
-        -----
-        Adding the same edge twice for Graph/DiGraph simply updates
-        the edge data. For MultiGraph/MultiDiGraph, duplicate edges
-        are stored.
-
-        Examples
-        --------
-        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
-        >>> G.add_weighted_edges_from([(0, 1, 3.0), (1, 2, 7.5)])
-        """
-        self.add_edges_from(((u, v, {weight: d}) for u, v, d in ebunch_to_add), **attr)
-
-    def add_weighted_edges_from(self, ebunch_to_add, weight="weight", **attr):
-        """Add weighted edges in `ebunch_to_add` with specified weight attr
-
-        Parameters
-        ----------
-        ebunch_to_add : container of edges
-            Each edge given in the list or container will be added
-            to the graph. The edges must be given as 3-tuples (u, v, w)
-            where w is a number.
-        weight : string, optional (default= 'weight')
-            The attribute name for the edge weights to be added.
-        attr : keyword arguments, optional (default= no attributes)
-            Edge attributes to add/update for all edges.
-
-        See Also
-        --------
-        add_edge : add a single edge
-        add_edges_from : add multiple edges
-
-        Notes
-        -----
-        Adding the same edge twice for Graph/DiGraph simply updates
-        the edge data. For MultiGraph/MultiDiGraph, duplicate edges
-        are stored.
-
-        Examples
-        --------
-        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
-        >>> G.add_weighted_edges_from([(0, 1, 3.0), (1, 2, 7.5)])
-        """
-        self.add_edges_from(((u, v, {weight: d}) for u, v, d in ebunch_to_add), **attr)
-
-    def add_edges_from_file(self, file, weighted=False):
-        """Added edges from file
-        For example, txt files,
-
-        Each line is in form like:
-        a b 23.0
-        which denotes an edge (a, b) with weight 23.0.
-
-        Parameters
-        ----------
-        file : string
-            The file path.
-
-        weighted : boolean, optional (default : False)
-            If the file consists of weight information, set `True`.
-            The weight key will be set as 'weight'.
-
-        Examples
-        --------
-
-        If `./club_network.txt` is:
-
-        Jack Mary 23.0
-
-        Mary Tom 15.0
-
-        Tom Ben 20.0
-
-        Then add them to *G*
-
-        >>> G.add_edges_from_file(file='./club_network.txt', weighted=True)
-
-
-        """
-        import re
-
-        with open(file, "r") as fp:
-            edges = fp.readlines()
-        if weighted:
-            for edge in edges:
-                edge = re.sub(",", " ", edge)
-                edge = edge.split()
-                try:
-                    self.add_edge(edge[0], edge[1], weight=float(edge[2]))
-                except:
-                    pass
-        else:
-            for edge in edges:
-                edge = re.sub(",", " ", edge)
-                edge = edge.split()
-                try:
-                    self.add_edge(edge[0], edge[1])
-                except:
-                    pass
-
-    def remove_nodes_from(self, nodes):
-        """Remove multiple nodes.
-
-        Parameters
-        ----------
-        nodes : iterable container
-            A container of nodes (list, dict, set, etc.).  If a node
-            in the container is not in the graph it is silently
-            ignored.
-
-        See Also
-        --------
-        remove_node
-
-        Examples
-        --------
-        >>> G = eg.path_graph(3)  # or DiGraph, MultiGraph, MultiDiGraph, etc
-        >>> e = list(G.nodes)
-        >>> e
-        [0, 1, 2]
-        >>> G.remove_nodes_from(e)
-        >>> list(G.nodes)
-        []
-
-        """
-        adj = self._adj
-        for n in nodes:
-            try:
-                del self._node[n]
-                for u in list(adj[n]):  # list handles self-loops
-                    del adj[u][n]  # (allows mutation of dict in loop)
-                del adj[n]
-            except KeyError:
-                pass
-
-    def _add_one_edge(self, u_of_edge, v_of_edge, edge_attr: dict = {}):
-        u, v = u_of_edge, v_of_edge
-        # add nodes
-        if u not in self._node:
-            self._add_one_node(u)
-        if v not in self._node:
-            self._add_one_node(v)
-        # add the edge
-        datadict = self._adj[u].get(v, self.edge_attr_dict_factory())
-        datadict.update(edge_attr)
-        self._adj[u][v] = datadict
-        self._adj[v][u] = datadict
-        if u == v:
-            self.extra_selfloop = True
-            self._raw_selfloop_dict[u] = datadict
-            self._clear_cache()
-
-    def remove_node(self, node_to_remove):
-        """Remove one node from your graph.
-
-        Parameters
-        ----------
-        node_to_remove : object
-            The node you want to remove.
-
-        See Also
-        --------
-        remove_nodes
-
-        Examples
-        --------
-        Remove node *Jack* from *G*
-
-        >>> G.remove_node('Jack')
-
-        """
-        try:
-            neighbors = list(self._adj[node_to_remove])
-            del self._node[node_to_remove]
-        except KeyError:  # Node not exists in self
-            raise EasyGraphError("No node {} in graph.".format(node_to_remove))
-        for neighbor in neighbors:  # Remove edges with other nodes
-            del self._adj[neighbor][node_to_remove]
-        del self._adj[node_to_remove]  # Remove this node
-        self._clear_cache()
-
-    def remove_nodes(self, nodes_to_remove: list):
-        """Remove nodes from your graph.
-
-        Parameters
-        ----------
-        nodes_to_remove : list of object
-            The list of nodes you want to remove.
-
-        See Also
-        --------
-        remove_node
-
-        Examples
-        --------
-        Remove node *[1, 2, 'a', 'b']* from *G*
-
-        >>> G.remove_nodes([1, 2, 'a', 'b'])
-
-        """
-        for (
-            node
-        ) in (
-            nodes_to_remove
-        ):  # If not all nodes included in graph, give up removing other nodes
-            assert node in self._node, "Remove Error: No node {} in graph".format(node)
-        for node in nodes_to_remove:
-            self.remove_node(node)
-        self._clear_cache()
-
-    def remove_edge(self, u, v):
-        """Remove one edge from your graph.
-
-        Parameters
-        ----------
-        u : object
-            One end of the edge.
-
-        v : object
-            The other end of the edge.
-
-        See Also
-        --------
-        remove_edges
-
-        Examples
-        --------
-        Remove edge (1,2) from *G*
-
-        >>> G.remove_edge(1,2)
-
-        """
-        try:
-            del self._adj[u][v]
-            if u != v:  # self-loop needs only one entry removed
-                del self._adj[v][u]
-            self._clear_cache()
-        except KeyError:
-            raise KeyError("No edge {}-{} in graph.".format(u, v))
-
-    def remove_edges(self, edges_to_remove: [tuple]):
-        """Remove a list of edges from your graph.
-
-        Parameters
-        ----------
-        edges_to_remove : list of tuple
-            The list of edges you want to remove,
-            Each element is (u, v) tuple, which denote the two ends of the edge.
-
-        See Also
-        --------
-        remove_edge
-
-        Examples
-        --------
-        Remove the edges *('Jack', 'Mary')* and *('Mary', 'Tom')* from *G*
-
-        >>> G.remove_edge([
-        ...     ('Jack', 'Mary'),
-        ...     ('Mary', 'Tom')
-        ... ])
-
-        """
-        for edge in edges_to_remove:
-            u, v = edge[:2]
-            self.remove_edge(u, v)
-        self._clear_cache()
-
-    def has_node(self, node):
-        """Returns whether a node exists
-
-        Parameters
-        ----------
-        node
-
-        Returns
-        -------
-        Bool : True (exist) or False (not exists)
-
-        """
-        return node in self._node
-
-    def has_edge(self, u, v):
-        """Returns whether an edge exists
-
-        Parameters
-        ----------
-        u : start node
-
-        v: end node
-
-        Returns
-        -------
-        Bool : True (exist) or False (not exists)
-
-        """
-        try:
-            return v in self._adj[u]
-        except KeyError:
-            return False
-
-    def number_of_nodes(self):
-        """Returns the number of nodes.
-
-        Returns
-        -------
-        number_of_nodes : int
-            The number of nodes.
-        """
-        return len(self._node)
-
-    def is_directed(self):
-        """Returns True if graph is a directed_graph, False otherwise."""
-        return False
-
-    def is_multigraph(self):
-        """Returns True if graph is a multigraph, False otherwise."""
-        return False
-
-    def copy(self):
-        """Return a deep copy of the graph.
-
-        Returns
-        -------
-        copy : easygraph.Graph
-            A deep copy of the original graph.
-
-        Examples
-        --------
-        *G2* is a deep copy of *G1*
-
-        >>> G2 = G1.copy()
-
-        """
-        G = self.__class__()
-        G.graph.update(self.graph)
-        for node, node_attr in self._node.items():
-            G.add_node(node, **node_attr)
-        for u, nbrs in self._adj.items():
-            for v, edge_data in nbrs.items():
-                G.add_edge(u, v, **edge_data)
-
-        return G
-
-    def nodes_subgraph(self, from_nodes: list):
-        """Returns a subgraph of some nodes
-
-        Parameters
-        ----------
-        from_nodes : list of object
-            The nodes in subgraph.
-
-        Returns
-        -------
-        nodes_subgraph : easygraph.Graph
-            The subgraph consisting of *from_nodes*.
-
-        Examples
-        --------
-
-        >>> G = eg.Graph()
-        >>> G.add_edges([(1,2), (2,3), (2,4), (4,5)])
-        >>> G_sub = G.nodes_subgraph(from_nodes= [1,2,3])
-
-        """
-        G = self.__class__()
-        G.graph.update(self.graph)
-        from_nodes = set(from_nodes)
-        for node in from_nodes:
-            try:
-                G.add_node(node, **self._node[node])
-            except KeyError:
-                pass
-
-            for v, edge_data in self._adj[node].items():
-                if v in from_nodes:
-                    G.add_edge(node, v, **edge_data)
-        return G
-
-    def ego_subgraph(self, center):
-        """Returns an ego network graph of a node.
-
-        Parameters
-        ----------
-        center : object
-            The center node of the ego network graph
-
-        Returns
-        -------
-        ego_subgraph : easygraph.Graph
-            The ego network graph of *center*.
-
-
-        Examples
-        --------
-        >>> G = eg.Graph()
-        >>> G.add_edges([
-        ...     ('Jack', 'Maria'),
-        ...     ('Maria', 'Andy'),
-        ...     ('Jack', 'Tom')
-        ... ])
-        >>> G.ego_subgraph(center='Jack')
-        """
-        neighbors_of_center = list(self.all_neighbors(center))
-        neighbors_of_center.append(center)
-        return self.nodes_subgraph(from_nodes=neighbors_of_center)
-
-    def to_index_node_graph(self, begin_index=0):
-        """Returns a deep copy of graph, with each node switched to its index.
-
-        Considering that the nodes of your graph may be any possible hashable Python object,
-        you can get an isomorphic graph of the original one, with each node switched to its index.
-
-        Parameters
-        ----------
-        begin_index : int
-            The begin index of the index graph.
-
-        Returns
-        -------
-        G : easygraph.Graph
-            Deep copy of graph, with each node switched to its index.
-
-        index_of_node : dict
-            Index of node
-
-        node_of_index : dict
-            Node of index
-
-        Examples
-        --------
-        The following method returns this isomorphic graph and index-to-node dictionary
-        as well as node-to-index dictionary.
-
-        >>> G = eg.Graph()
-        >>> G.add_edges([
-        ...     ('Jack', 'Maria'),
-        ...     ('Maria', 'Andy'),
-        ...     ('Jack', 'Tom')
-        ... ])
-        >>> G_index_graph, index_of_node, node_of_index = G.to_index_node_graph()
-
-        """
-        G = self.__class__()
-        G.graph.update(self.graph)
-        index_of_node = dict()
-        node_of_index = dict()
-        for index, (node, node_attr) in enumerate(self._node.items()):
-            G.add_node(index + begin_index, **node_attr)
-            index_of_node[node] = index + begin_index
-            node_of_index[index + begin_index] = node
-        for u, nbrs in self._adj.items():
-            for v, edge_data in nbrs.items():
-                G.add_edge(index_of_node[u], index_of_node[v], **edge_data)
-
-        return G, index_of_node, node_of_index
-
-    def _clear_cache(self):
-        r"""Clear the cache."""
-        self.cache = {}
-
-    def to_directed_class(self):
-        """Returns the class to use for empty directed copies.
-
-        If you subclass the base classes, use this to designate
-        what directed class to use for `to_directed()` copies.
-        """
-        return eg.DiGraph
-
-    def to_directed(self):
-        """Returns a directed representation of the graph.
-
-        Returns
-        -------
-        G : DiGraph
-            A directed graph with the same name, same nodes, and with
-            each edge (u, v, data) replaced by two directed edges
-            (u, v, data) and (v, u, data).
-
-        Notes
-        -----
-        This returns a "deepcopy" of the edge, node, and
-        graph attributes which attempts to completely copy
-        all of the data and references.
-
-        This is in contrast to the similar D=DiGraph(G) which returns a
-        shallow copy of the data.
-
-        See the Python copy module for more information on shallow
-        and deep copies, https://docs.python.org/3/library/copy.html.
-
-        Warning: If you have subclassed Graph to use dict-like objects
-        in the data structure, those changes do not transfer to the
-        DiGraph created by this method.
-
-        Examples
-        --------
-        >>> G = eg.Graph()  # or MultiGraph, etc
-        >>> G.add_edge(0, 1)
-        >>> H = G.to_directed()
-        >>> list(H.edges)
-        [(0, 1), (1, 0)]
-
-        If already directed, return a (deep) copy
-
-        >>> G = eg.DiGraph()  # or MultiDiGraph, etc
-        >>> G.add_edge(0, 1)
-        >>> H = G.to_directed()
-        >>> list(H.edges)
-        [(0, 1)]
-        """
-        graph_class = self.to_directed_class()
-
-        G = graph_class()
-        G.graph.update(deepcopy(self.graph))
-        G.add_nodes_from((n, deepcopy(d)) for n, d in self._node.items())
-        G.add_edges_from(
-            (u, v, deepcopy(data))
-            for u, nbrs in self._adj.items()
-            for v, data in nbrs.items()
-        )
-        return G
-
-    def cpp(self):
-        G = GraphC()
-        G.graph.update(self.graph)
-        for u, attr in self.nodes.items():
-            G.add_node(u, **attr)
-        for u, v, attr in self.edges:
-            G.add_edge(u, v, **attr)
-        G.generate_linkgraph()
-        return G
-
-
-try:
-    import cpp_easygraph
-
-    class GraphC(cpp_easygraph.Graph):
-        cflag = 1
-
-except ImportError:
-
-    class GraphC:
-        def __init__(self, **graph_attr):
-            print(
-                "Object cannot be instantiated because C extension has not been"
-                " successfully compiled and installed. Please refer to"
-                " https://github.com/easy-graph/Easy-Graph/blob/master/README.rst and"
-                " reinstall easygraph."
-            )
-            raise RuntimeError
+import copy
+import warnings
+
+from copy import deepcopy
+from typing import Dict
+from typing import List
+from typing import Tuple
+
+import easygraph as eg
+import easygraph.convert as convert
+import torch
+
+from easygraph.utils.exception import EasyGraphError
+from easygraph.utils.sparse import sparse_dropout
+
+
+class Graph:
+    """
+    Base class for undirected graphs.
+
+        Nodes are allowed for any hashable Python objects, including int, string, dict, etc.
+        Edges are stored as Python dict type, with optional key/value attributes.
+
+    Parameters
+    ----------
+    graph_attr : keywords arguments, optional (default : None)
+        Attributes to add to graph as key=value pairs.
+
+    See Also
+    --------
+    DiGraph
+
+    Examples
+    --------
+    Create an empty undirected graph with no nodes and edges.
+
+    >>> G = eg.Graph()
+
+    Create a deep copy graph *G2* from existing Graph *G1*.
+
+    >>> G2 = G1.copy()
+
+    Create an graph with attributes.
+
+    >>> G = eg.Graph(name='Karate Club', date='2020.08.21')
+
+    **Attributes:**
+
+    Returns the adjacency matrix of the graph.
+
+    >>> G.adj
+
+    Returns all the nodes with their attributes.
+
+    >>> G.nodes
+
+    Returns all the edges with their attributes.
+
+    >>> G.edges
+
+    """
+
+    gnn_data_dict_factory = dict
+    raw_selfloop_dict = dict
+    graph_attr_dict_factory = dict
+    node_dict_factory = dict
+    node_attr_dict_factory = dict
+    adjlist_outer_dict_factory = dict
+    adjlist_inner_dict_factory = dict
+    edge_attr_dict_factory = dict
+    node_index_dict = dict
+
+    def __init__(self, incoming_graph_data=None, extra_selfloop=False, **graph_attr):
+        self.graph = self.graph_attr_dict_factory()
+        self._node = self.node_dict_factory()
+        self._adj = self.adjlist_outer_dict_factory()
+        self._raw_selfloop_dict = self.raw_selfloop_dict()
+        self.extra_selfloop = extra_selfloop
+        self._ndata = self.gnn_data_dict_factory()
+        self.cache = {}
+        self._node_index = self.node_index_dict()
+        self.cflag = 0
+        self._id = 0
+        self.device = "cpu"
+        if incoming_graph_data is not None:
+            convert.to_easygraph_graph(incoming_graph_data, create_using=self)
+        self.graph.update(graph_attr)
+
+    def __iter__(self):
+        return iter(self._node)
+
+    def __len__(self):
+        return len(self._node)
+
+    def __contains__(self, node):
+        try:
+            return node in self._node
+        except TypeError:
+            return False
+
+    def __getitem__(self, node):
+        # return list(self._adj[node].keys())
+        return self._adj[node]
+
+    @property
+    def ndata(self):
+        return self._ndata
+
+    @property
+    def adj(self):
+        """
+        Return the adjacency matrix
+        """
+        return self._adj
+
+    @property
+    def nodes(self):
+        """
+        return [node for node in self._node]
+        """
+        return self._node
+
+    @property
+    def node_index(self):
+        return self._node_index
+
+    @property
+    def edges(self):
+        """
+        Return an edge list
+        """
+        if self.cache.get("edges") != None:
+            return self.cache["edges"]
+        edge_lst = list()
+        seen = set()
+        for u in self._adj:
+            for v in self._adj[u]:
+                if (u, v) not in seen:
+                    seen.add((u, v))
+                    seen.add((v, u))
+                    edge_lst.append((u, v, self._adj[u][v]))
+        del seen
+        self.cache["edge"] = edge_lst
+        return self.cache["edge"]
+
+    @property
+    def name(self):
+        """String identifier of the graph.
+
+        This graph attribute appears in the attribute dict G.graph
+        keyed by the string `"name"`. as well as an attribute (technically
+        a property) `G.name`. This is entirely user controlled.
+        """
+        return self.graph.get("name", "")
+
+    @property
+    def e_both_side(self, weight="weight") -> Tuple[List[List], List[float]]:
+        r"""Return the list of edges including both directions."""
+        if self.cache.get("e_both_side") != None:
+            return self.cache["e_both_side"]
+        edges = list()
+        weights = list()
+        seen = set()
+        for u in self._adj:
+            for v in self._adj[u]:
+                if (u, v) not in seen:
+                    seen.add((u, v))
+                    seen.add((v, u))
+                    edges.append([u, v])
+                    edges.append([v, u])
+                    if weight not in self._adj[u][v]:
+                        warnings.warn("There is no property %s,default to 1" % (weight))
+                        weights.append(1.0)
+                        weights.append(1.0)
+                    else:
+                        weights.append(self._adj[u][v][weight])
+                        weights.append(self._adj[v][u][weight])
+        self.cache["e_both_side"] = (edges, weights)
+        return self.cache["e_both_side"]
+
+    @staticmethod
+    def from_hypergraph_hypergcn(
+        hypergraph,
+        feature,
+        with_mediator=False,
+        remove_selfloop=True,
+    ):
+        r"""Construct a graph from a hypergraph with methods proposed in `HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs <https://arxiv.org/pdf/1809.02589.pdf>`_ paper .
+
+        Args:
+            ``hypergraph`` (``Hypergraph``): The source hypergraph.
+            ``feature`` (``torch.Tensor``): The feature of the vertices.
+            ``with_mediator`` (``str``): Whether to use mediator to transform the hyperedges to edges in the graph. Defaults to ``False``.
+            ``remove_selfloop`` (``bool``): Whether to remove self-loop. Defaults to ``True``.
+            ``device`` (``torch.device``): The device to store the graph. Defaults to ``torch.device("cpu")``.
+        """
+
+        num_v = hypergraph.num_v
+        assert (
+            num_v == feature.shape[0]
+        ), "The number of vertices in hypergraph and feature.shape[0] must be equal!"
+        e_list, new_e_list, new_e_weight = hypergraph.e[0], [], []
+        rv = torch.rand((feature.shape[1], 1), device=feature.device)
+        for e in e_list:
+            num_v_in_e = len(e)
+            assert (
+                num_v_in_e >= 2
+            ), "The number of vertices in an edge must be greater than or equal to 2!"
+            p = torch.mm(feature[e, :], rv).squeeze()
+            v_a_idx, v_b_idx = torch.argmax(p), torch.argmin(p)
+            if not with_mediator:
+                new_e_list.append((e[v_a_idx], e[v_b_idx]))
+                new_e_weight.append(1.0 / num_v_in_e)
+            else:
+                w = 1.0 / (2 * num_v_in_e - 3)
+                for mid_v_idx in range(num_v_in_e):
+                    if mid_v_idx != v_a_idx and mid_v_idx != v_b_idx:
+                        new_e_list.append([e[v_a_idx], e[mid_v_idx]])
+                        new_e_weight.append(w)
+                        new_e_list.append([e[v_b_idx], e[mid_v_idx]])
+                        new_e_weight.append(w)
+        # remove selfloop
+        if remove_selfloop:
+            new_e_list = torch.tensor(new_e_list, dtype=torch.long)
+            new_e_weight = torch.tensor(new_e_weight, dtype=torch.float)
+            e_mask = (new_e_list[:, 0] != new_e_list[:, 1]).bool()
+            new_e_list = new_e_list[e_mask].numpy().tolist()
+            new_e_weight = new_e_weight[e_mask].numpy().tolist()
+
+        _g = Graph()
+
+        _g.add_nodes(list(range(0, num_v)))
+        for (
+            e,
+            w,
+        ) in zip(new_e_list, new_e_weight):
+            if _g.has_edge(e[0], e[1]):
+                _g.add_edge(e[0], e[1], weight=(w + _g.adj[e[0]][e[1]]["weight"]))
+            else:
+                _g.add_edge(e[0], e[1], weight=w)
+        now_edges = []
+        now_weight = []
+        for e in _g.edges:
+            now_edges.append((e[0], e[1]))
+            now_weight.append(e[2]["weight"])
+        now_edges.extend([(i, i) for i in range(num_v)])
+        now_weight.extend([1.0] * num_v)
+        _g.cache["e_both_side"] = (now_edges, now_weight)
+
+        return _g
+
+    @property
+    def A(self):
+        r"""Return the adjacency matrix :math:`\mathbf{A}` of the sample graph with ``torch.sparse_coo_tensor`` format. Size :math:`(|\mathcal{V}|, |\mathcal{V}|)`.
+        """
+        #  import torch
+
+        if self.cache.get("A", None) is None:
+            if len(self.edges) == 0:
+                self.cache["A"] = torch.sparse_coo_tensor(
+                    size=(len(self.nodes), len(self.nodes)), device=self.device
+                )
+            else:
+                if self.cache.get("e_both_side") is not None:
+                    e_list, e_weight = self.cache["e_both_side"]
+
+                else:
+                    e_list, e_weight = self.e_both_side
+
+                node_size = len(self.nodes)
+                self.cache["A"] = torch.sparse_coo_tensor(
+                    indices=torch.tensor(e_list, dtype=torch.int).t(),
+                    values=torch.tensor(e_weight),
+                    size=(node_size, node_size),
+                    device=self.device,
+                ).coalesce()
+        return self.cache["A"]
+
+    @property
+    def D_v_neg_1_2(
+        self,
+    ):
+        r"""Return the normalized diagonal matrix of vertex degree :math:`\mathbf{D}_v^{-\frac{1}{2}}` with ``torch.sparse_coo_tensor`` format. Size :math:`(|\mathcal{V}|, |\mathcal{V}|)`.
+        """
+        # import torch
+
+        if self.cache.get("D_v_neg_1_2") is None:
+            if self.cache.get("D_v_value") is None:
+                self.cache["D_v_value"] = (
+                    torch.sparse.sum(self.A, dim=1).to_dense().view(-1)
+                )
+                # self.cache["D_v_value"] = torch.tensor(list(self.degree().values())).float()
+
+            _mat = self.cache["D_v_value"]
+            # _mat = _tmp
+            _val = _mat**-0.5
+            _val[torch.isinf(_val)] = 0
+            nodes_num = len(self.nodes)
+            self.cache["D_v_neg_1_2"] = torch.sparse_coo_tensor(
+                torch.arange(0, len(self.nodes)).view(1, -1).repeat(2, 1),
+                _val,
+                torch.Size([nodes_num, nodes_num]),
+                device=self.device,
+            ).coalesce()
+        return self.cache["D_v_neg_1_2"]
+
+    @property
+    def index2node(self):
+        """
+        Assign an integer index for each node (start from 0)
+        """
+        if self.cache.get("index2node", None) is None:
+            index2node_dict = {}
+            index = 0
+            # for index in range(0, len(self.nodes)):
+
+            for index, n in enumerate(self.nodes):
+                index2node_dict[index] = n
+                # index += 1
+            self.cache["index2node"] = index2node_dict
+        return self.cache["index2node"]
+
+    @property
+    def node_index(self):
+        """
+        Assign an integer index for each node (start from 0)
+        """
+        if self.cache.get("node_index", None) is None:
+            node2index_dict = {}
+            index = 0
+            for n in self.nodes:
+                node2index_dict[n] = index
+                index += 1
+            self.cache["node_index"] = node2index_dict
+        return self.cache["node_index"]
+
+    @property
+    def e(self) -> Tuple[List[List[int]], List[float]]:
+        r"""Return the edge list, weight list and property list in the graph."""
+
+        if self.cache.get("e", None) is None:
+            node_index = self.node_index
+            e_list = [
+                (node_index[src_idx], node_index[dst_idx])
+                for src_idx, dst_idx, d in self.edges
+            ]
+            w_list = []
+            e_property_list = []
+            v_property_list = []
+
+            node_size = len(self.nodes)
+            for i in range(0, node_size):
+                v_property_list.append(self.nodes[self.index2node[i]])
+
+            for d in self.edges:
+                if "weight" not in d[2]:
+                    w_list.append(1.0)
+                    e_property_list.append(d[2])
+                else:
+                    w_list.append(d[2]["weight"])
+                    tmp_dict = copy.deepcopy(d[2])
+                    del tmp_dict["weight"]
+                    e_property_list.append(tmp_dict)
+
+            self.cache["e"] = e_list, w_list, v_property_list, e_property_list
+        return self.cache["e"]
+
+    @property
+    def D_v(self):
+        r"""Return the diagonal matrix of vertex degree :math:`\mathbf{D}_v` with ``torch.sparse_coo_tensor`` format. Size :math:`(|\mathcal{V}|, |\mathcal{V}|)`.
+        """
+
+        if self.cache.get("D_v") is None:
+            # print("self.A:",self.A)
+            _tmp = torch.sparse.sum(self.A, dim=1).to_dense().clone().view(-1)
+
+            nodes_num = len(self.nodes)
+            self.cache["D_v"] = torch.sparse_csr_tensor(
+                torch.arange(0, nodes_num + 1),
+                torch.arange(0, nodes_num),
+                _tmp,
+                torch.Size([nodes_num, nodes_num]),
+                device=self.device,
+            )
+
+            # self.cache["D_v"] = torch.sparse_coo_tensor(
+            #     torch.arange(0, len(self.nodes)).view(1, -1).repeat(2, 1),
+            #     _tmp,
+            #     torch.Size([len(self.nodes), len(self.nodes)]),
+            #     device=self.device,
+            # ).coalesce()
+        return self.cache["D_v"]
+
+    def add_extra_selfloop(self):
+        r"""Add extra selfloops to the graph."""
+        self._has_extra_selfloop = True
+        self._clear_cache()
+
+    def remove_extra_selfloop(self):
+        r"""Remove extra selfloops from the graph."""
+        self._has_extra_selfloop = False
+        self._clear_cache()
+
+    def remove_selfloop(self):
+        r"""Remove all selfloops from the graph."""
+        self._raw_selfloop_dict.clear()
+        self.remove_extra_selfloop()
+        self._clear_cache()
+
+    def nbr_v(self, v_idx: int) -> Tuple[List[int], List[float]]:
+        r"""Return a vertex list of the neighbors of the vertex ``v_idx``.
+
+        Args:
+            ``v_idx`` (``int``): The index of the vertex.
+        """
+        return self.N_v(v_idx).cpu().numpy().tolist()
+
+    def N_v(self, v_idx: int) -> Tuple[List[int], List[float]]:
+        r"""Return the neighbors of the vertex ``v_idx`` with ``torch.Tensor`` format.
+
+        Args:
+            ``v_idx`` (``int``): The index of the vertex.
+        """
+        sub_v_set = self.A[v_idx]._indices()[0].clone()
+        return sub_v_set
+
+    def clone(self):
+        r"""Clone the graph."""
+        # _g = Graph(self.num_v, extra_selfloop=self._has_extra_selfloop, device=self.device)
+        # _g=self.__class__()
+        # _g.device="cpu"
+        # _g.extra_selfloop=False
+        # _g.edges = deepcopy(self.edges)
+        # _g.cache = deepcopy(self.cache)
+        return self.copy()
+
+    @name.setter
+    def name(self, s):
+        """
+        Set graph name
+
+        Parameters
+        ----------
+        s : name
+        """
+        self.graph["name"] = s
+
+    def degree(self, weight="weight"):
+        """Returns the weighted degree of of each node.
+
+        Parameters
+        ----------
+        weight : string, optional (default: 'weight')
+            Weight key of the original weighted graph.
+
+        Returns
+        -------
+        degree : dict
+            Each node's (key) weighted degree (value).
+
+        Notes
+        -----
+        If the graph is not weighted, all the weights will be regarded as 1.
+
+        Examples
+        --------
+        You can call with no attributes, if 'weight' is the weight key:
+
+        >>> G.degree()
+
+        if you have customized weight key 'weight_1'.
+
+        >>> G.degree(weight='weight_1')
+
+        """
+        if self.cache.get("degree") != None:
+            return self.cache["degree"]
+        degree = dict()
+        for u, v, d in self.edges:
+            if u in degree:
+                degree[u] += d.get(weight, 1)
+            else:
+                degree[u] = d.get(weight, 1)
+            if v in degree:
+                degree[v] += d.get(weight, 1)
+            else:
+                degree[v] = d.get(weight, 1)
+
+        # For isolated nodes
+        for node in self.nodes:
+            if node not in degree:
+                degree[node] = 0
+        self.cache["degree"] = degree
+        return degree
+
+    def order(self):
+        """Returns the number of nodes in the graph.
+
+        Returns
+        -------
+        nnodes : int
+            The number of nodes in the graph.
+
+        See Also
+        --------
+        number_of_nodes: identical method
+        __len__: identical method
+
+        Examples
+        --------
+        >>> G = eg.path_graph(3)  # or DiGraph, MultiGraph, MultiDiGraph, etc
+        >>> G.order()
+        3
+        """
+        return len(self._node)
+
+    def size(self, weight=None):
+        """Returns the number of edges or total of all edge weights.
+
+        Parameters
+        -----------
+        weight : String or None, optional
+            The weight key. If None, it will calculate the number of
+            edges, instead of total of all edge weights.
+
+        Returns
+        -------
+        size : int or float, optional (default: None)
+            The number of edges or total of all edge weights.
+
+        Examples
+        --------
+
+        Returns the number of edges in G:
+
+        >>> G.size()
+
+        Returns the total of all edge weights in G:
+
+        >>> G.size(weight='weight')
+
+        """
+        if self.cache.get("size") != None:
+            return self.cache["size"]
+        s = sum(d for v, d in self.degree(weight=weight).items())
+        self.cache["size"] = s // 2 if weight is None else s / 2
+        return self.cache["size"]
+
+    # GCN Laplacian smoothing
+    @property
+    def L_GCN(self):
+        r"""Return the GCN Laplacian matrix :math:`\mathcal{L}_{GCN}` of the graph with ``torch.sparse_coo_tensor`` format. Size :math:`(|\mathcal{V}|, |\mathcal{V}|)`.
+
+        .. math::
+            \mathcal{L}_{GCN} = \mathbf{\hat{D}}_v^{-\frac{1}{2}} \mathbf{\hat{A}} \mathbf{\hat{D}}_v^{-\frac{1}{2}}
+
+        """
+        if self.cache.get("L_GCN") is None:
+            # self.add_extra_selfloop()
+            self.cache["L_GCN"] = (
+                self.D_v_neg_1_2.mm(self.A).mm(self.D_v_neg_1_2).coalesce()
+            )
+        return self.cache["L_GCN"]
+
+    def smoothing_with_GCN(self, X, drop_rate=0.0):
+        r"""Return the smoothed feature matrix with GCN Laplacian matrix :math:`\mathcal{L}_{GCN}`.
+
+        Args:
+            ``X`` (``torch.Tensor``): Vertex feature matrix. Size :math:`(|\mathcal{V}|, C)`.
+            ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in adjacency matrix with probability ``drop_rate``. Default: ``0.0``.
+        """
+        if drop_rate > 0.0:
+            L_GCN = sparse_dropout(self.L_GCN, drop_rate)
+        else:
+            L_GCN = self.L_GCN
+
+        return torch.sparse.mm(L_GCN, X)
+
+    def number_of_edges(self, u=None, v=None):
+        """Returns the number of edges between two nodes.
+
+        Parameters
+        ----------
+        u, v : nodes, optional (default=all edges)
+            If u and v are specified, return the number of edges between
+            u and v. Otherwise return the total number of all edges.
+
+        Returns
+        -------
+        nedges : int
+            The number of edges in the graph.  If nodes `u` and `v` are
+            specified return the number of edges between those nodes. If
+            the graph is directed, this only returns the number of edges
+            from `u` to `v`.
+
+        See Also
+        --------
+        size
+
+        Examples
+        --------
+        For undirected graphs, this method counts the total number of
+        edges in the graph:
+
+        >>> G = eg.path_graph(4)
+        >>> G.number_of_edges()
+        3
+
+        If you specify two nodes, this counts the total number of edges
+        joining the two nodes:
+
+        >>> G.number_of_edges(0, 1)
+        1
+
+        For directed graphs, this method can count the total number of
+        directed edges from `u` to `v`:
+
+        >>> G = eg.DiGraph()
+        >>> G.add_edge(0, 1)
+        >>> G.add_edge(1, 0)
+        >>> G.number_of_edges(0, 1)
+        1
+
+        """
+        if u is None:
+            return int(self.size())
+        if v in self._adj[u]:
+            return 1
+        return 0
+
+    def nbunch_iter(self, nbunch=None):
+        """Returns an iterator over nodes contained in nbunch that are
+        also in the graph.
+
+        The nodes in nbunch are checked for membership in the graph
+        and if not are silently ignored.
+
+        Parameters
+        ----------
+        nbunch : single node, container, or all nodes (default= all nodes)
+            The view will only report edges incident to these nodes.
+
+        Returns
+        -------
+        niter : iterator
+            An iterator over nodes in nbunch that are also in the graph.
+            If nbunch is None, iterate over all nodes in the graph.
+
+        Raises
+        ------
+        EasyGraphError
+            If nbunch is not a node or sequence of nodes.
+            If a node in nbunch is not hashable.
+
+        See Also
+        --------
+        Graph.__iter__
+
+        Notes
+        -----
+        When nbunch is an iterator, the returned iterator yields values
+        directly from nbunch, becoming exhausted when nbunch is exhausted.
+
+        To test whether nbunch is a single node, one can use
+        "if nbunch in self:", even after processing with this routine.
+
+        If nbunch is not a node or a (possibly empty) sequence/iterator
+        or None, a :exc:`EasyGraphError` is raised.  Also, if any object in
+        nbunch is not hashable, a :exc:`EasyGraphError` is raised.
+        """
+        if nbunch is None:  # include all nodes via iterator
+            bunch = iter(self._adj)
+        elif nbunch in self:  # if nbunch is a single node
+            bunch = iter([nbunch])
+        else:  # if nbunch is a sequence of nodes
+
+            def bunch_iter(nlist, adj):
+                try:
+                    for n in nlist:
+                        if n in adj:
+                            yield n
+                except TypeError as err:
+                    exc, message = err, err.args[0]
+                    # capture error for non-sequence/iterator nbunch.
+                    if "iter" in message:
+                        exc = EasyGraphError(
+                            "nbunch is not a node or a sequence of nodes."
+                        )
+                    # capture error for unhashable node.
+                    if "hashable" in message:
+                        exc = EasyGraphError(
+                            f"Node {n} in sequence nbunch is not a valid node."
+                        )
+                    raise exc
+
+            bunch = bunch_iter(nbunch, self._adj)
+        return bunch
+
+    def neighbors(self, node):
+        """Returns an iterator of a node's neighbors.
+
+        Parameters
+        ----------
+        node : Hashable
+            The target node.
+
+        Returns
+        -------
+        neighbors : iterator
+            An iterator of a node's neighbors.
+
+        Examples
+        --------
+        >>> G = eg.Graph()
+        >>> G.add_edges([(1,2), (2,3), (2,4)])
+        >>> for neighbor in G.neighbors(node=2):
+        ...     print(neighbor)
+
+        """
+        try:
+            return iter(self._adj[node])
+        except KeyError:
+            print("No node {}".format(node))
+
+    all_neighbors = neighbors
+
+    def add_node(self, node_for_adding, **node_attr):
+        """Add one node
+
+        Add one node, type of which is any hashable Python object, such as int, string, dict, or even Graph itself.
+        You can add with node attributes using Python dict type.
+
+        Parameters
+        ----------
+        node_for_adding : any hashable Python object
+            Nodes for adding.
+
+        node_attr : keywords arguments, optional
+            The node attributes.
+            You can customize them with different key-value pairs.
+
+        See Also
+        --------
+        add_nodes
+
+        Examples
+        --------
+        >>> G.add_node('a')
+        >>> G.add_node('hello world')
+        >>> G.add_node('Jack', age=10)
+
+        >>> G.add_node('Jack', **{
+        ...     'age': 10,
+        ...     'gender': 'M'
+        ... })
+
+        """
+        self._add_one_node(node_for_adding, node_attr)
+        self._clear_cache()
+
+    def add_nodes(self, nodes_for_adding: list, nodes_attr: List[Dict] = []):
+        """Add nodes with a list of nodes.
+
+        Parameters
+        ----------
+        nodes_for_adding : list
+
+        nodes_attr : list of dict
+            The corresponding attribute for each of *nodes_for_adding*.
+
+        See Also
+        --------
+        add_node
+
+        Examples
+        --------
+        Add nodes with a list of nodes.
+        You can add with node attributes using a list of Python dict type,
+        each of which is the attribute of each node, respectively.
+
+        >>> G.add_nodes([1, 2, 'a', 'b'])
+        >>> G.add_nodes(range(1, 200))
+
+        >>> G.add_nodes(['Jack', 'Tom', 'Lily'], nodes_attr=[
+        ...     {
+        ...         'age': 10,
+        ...         'gender': 'M'
+        ...     },
+        ...     {
+        ...         'age': 11,
+        ...         'gender': 'M'
+        ...     },
+        ...     {
+        ...         'age': 10,
+        ...         'gender': 'F'
+        ...     }
+        ... ])
+
+        """
+        if not len(nodes_attr) == 0:  # Nodes attributes included in input
+            assert len(nodes_for_adding) == len(
+                nodes_attr
+            ), "Nodes and Attributes lists must have same length."
+        else:  # Set empty attribute for each node
+            nodes_attr = [dict() for i in range(len(nodes_for_adding))]
+
+        for i in range(len(nodes_for_adding)):
+            try:
+                self._add_one_node(nodes_for_adding[i], nodes_attr[i])
+            except Exception as err:
+                print(err)
+                pass
+        self._clear_cache()
+
+    def add_nodes_from(self, nodes_for_adding, **attr):
+        """Add multiple nodes.
+
+        Parameters
+        ----------
+        nodes_for_adding : iterable container
+            A container of nodes (list, dict, set, etc.).
+            OR
+            A container of (node, attribute dict) tuples.
+            Node attributes are updated using the attribute dict.
+        attr : keyword arguments, optional (default= no attributes)
+            Update attributes for all nodes in nodes.
+            Node attributes specified in nodes as a tuple take
+            precedence over attributes specified via keyword arguments.
+
+        See Also
+        --------
+        add_node
+
+        Examples
+        --------
+        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
+        >>> G.add_nodes_from("Hello")
+        >>> K3 = eg.Graph([(0, 1), (1, 2), (2, 0)])
+        >>> G.add_nodes_from(K3)
+        >>> sorted(G.nodes(), key=str)
+        [0, 1, 2, 'H', 'e', 'l', 'o']
+
+        Use keywords to update specific node attributes for every node.
+
+        >>> G.add_nodes_from([1, 2], size=10)
+        >>> G.add_nodes_from([3, 4], weight=0.4)
+
+        Use (node, attrdict) tuples to update attributes for specific nodes.
+
+        >>> G.add_nodes_from([(1, dict(size=11)), (2, {"color": "blue"})])
+        >>> G.nodes[1]["size"]
+        11
+        >>> H = eg.Graph()
+        >>> H.add_nodes_from(G.nodes(data=True))
+        >>> H.nodes[1]["size"]
+        11
+
+        """
+        for n in nodes_for_adding:
+            try:
+                newnode = n not in self._node
+                newdict = attr
+            except TypeError:
+                n, ndict = n
+                newnode = n not in self._node
+                newdict = attr.copy()
+                newdict.update(ndict)
+            if newnode:
+                if n is None:
+                    raise ValueError("None cannot be a node")
+                self._adj[n] = self.adjlist_inner_dict_factory()
+                self._node[n] = self.node_attr_dict_factory()
+            self._node[n].update(newdict)
+        self._clear_cache()
+
+    def _add_one_node(self, one_node_for_adding, node_attr: dict = {}):
+        node = one_node_for_adding
+        if node not in self._node:
+            self._node_index[node] = self._id
+            self._id += 1
+            self._adj[node] = self.adjlist_inner_dict_factory()
+            attr_dict = self._node[node] = self.node_attr_dict_factory()
+            attr_dict.update(node_attr)
+        else:  # If already exists, there is no complain and still updating the node attribute
+            self._node[node].update(node_attr)
+        self._clear_cache()
+
+    def add_edge(self, u_of_edge, v_of_edge, **edge_attr):
+        """Add one edge.
+
+        Parameters
+        ----------
+        u_of_edge : object
+            One end of this edge
+
+        v_of_edge : object
+            The other one end of this edge
+
+        edge_attr : keywords arguments, optional
+            The attribute of the edge.
+
+        Notes
+        -----
+        Nodes of this edge will be automatically added to the graph, if they do not exist.
+
+        See Also
+        --------
+        add_edges
+
+        Examples
+        --------
+
+        >>> G.add_edge(1,2)
+        >>> G.add_edge('Jack', 'Tom', weight=10)
+
+        Add edge with attributes, edge weight, for example,
+
+        >>> G.add_edge(1, 2, **{
+        ...     'weight': 20
+        ... })
+
+        """
+        self._add_one_edge(u_of_edge, v_of_edge, edge_attr)
+        self._clear_cache()
+
+    def add_weighted_edge(self, u_of_edge, v_of_edge, weight):
+        """Add a weighted edge
+
+        Parameters
+        ----------
+        u_of_edge : start node
+
+        v_of_edge : end node
+
+        weight : weight value
+
+        Examples
+        --------
+        Add a weighted edge
+
+        >>> G.add_weighted_edge( 1 , 3 , 1.0)
+
+        """
+        self._add_one_edge(u_of_edge, v_of_edge, edge_attr={"weight": weight})
+        self._clear_cache()
+
+    def add_edges(self, edges_for_adding, edges_attr: List[Dict] = []):
+        """Add a list of edges.
+
+        Parameters
+        ----------
+        edges_for_adding : list of 2-element tuple
+            The edges for adding. Each element is a (u, v) tuple, and u, v are
+            two ends of the edge.
+
+        edges_attr : list of dict, optional
+            The corresponding attributes for each edge in *edges_for_adding*.
+
+        Examples
+        --------
+        Add a list of edges into *G*
+
+        >>> G.add_edges([
+        ...     (1, 2),
+        ...     (3, 4),
+        ...     ('Jack', 'Tom')
+        ... ])
+
+        Add edge with attributes, for example, edge weight,
+
+        >>> G.add_edges([(1,2), (2, 3)], edges_attr=[
+        ...     {
+        ...         'weight': 20
+        ...     },
+        ...     {
+        ...         'weight': 15
+        ...     }
+        ... ])
+
+        """
+        if edges_attr is None:
+            edges_attr = []
+        if not len(edges_attr) == 0:  # Edges attributes included in input
+            assert len(edges_for_adding) == len(
+                edges_attr
+            ), "Edges and Attributes lists must have same length."
+        else:  # Set empty attribute for each edge
+            edges_attr = [dict() for i in range(len(edges_for_adding))]
+
+        for i in range(len(edges_for_adding)):
+            try:
+                edge = edges_for_adding[i]
+                attr = edges_attr[i]
+                assert len(edge) == 2, "Edge tuple {} must be 2-tuple.".format(edge)
+                self._add_one_edge(edge[0], edge[1], attr)
+            except Exception as err:
+                print(err)
+        self._clear_cache()
+
+    def add_edges_from(self, ebunch_to_add, **attr):
+        """Add all the edges in ebunch_to_add.
+
+        Parameters
+        ----------
+        ebunch_to_add : container of edges
+            Each edge given in the container will be added to the
+            graph. The edges must be given as 2-tuples (u, v) or
+            3-tuples (u, v, d) where d is a dictionary containing edge data.
+        attr : keyword arguments, optional
+            Edge data (or labels or objects) can be assigned using
+            keyword arguments.
+
+        See Also
+        --------
+        add_edge : add a single edge
+        add_weighted_edges_from : convenient way to add weighted edges
+
+        Notes
+        -----
+        Adding the same edge twice has no effect but any edge data
+        will be updated when each duplicate edge is added.
+
+        Edge attributes specified in an ebunch take precedence over
+        attributes specified via keyword arguments.
+
+        Examples
+        --------
+        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
+        >>> G.add_edges_from([(0, 1), (1, 2)])  # using a list of edge tuples
+        >>> e = zip(range(0, 3), range(1, 4))
+        >>> G.add_edges_from(e)  # Add the path graph 0-1-2-3
+
+        Associate data to edges
+
+        >>> G.add_edges_from([(1, 2), (2, 3)], weight=3)
+        >>> G.add_edges_from([(3, 4), (1, 4)], label="WN2898")
+        """
+        for e in ebunch_to_add:
+            ne = len(e)
+            if ne == 3:
+                u, v, dd = e
+            elif ne == 2:
+                u, v = e
+                dd = {}  # doesn't need edge_attr_dict_factory
+            else:
+                raise EasyGraphError(f"Edge tuple {e} must be a 2-tuple or 3-tuple.")
+            if u not in self._node:
+                if u is None:
+                    raise ValueError("None cannot be a node")
+                self._adj[u] = self.adjlist_inner_dict_factory()
+                self._node[u] = self.node_attr_dict_factory()
+            if v not in self._node:
+                if v is None:
+                    raise ValueError("None cannot be a node")
+                self._adj[v] = self.adjlist_inner_dict_factory()
+                self._node[v] = self.node_attr_dict_factory()
+            datadict = self._adj[u].get(v, self.edge_attr_dict_factory())
+            datadict.update(attr)
+            datadict.update(dd)
+            self._adj[u][v] = datadict
+            self._adj[v][u] = datadict
+        self._clear_cache()
+
+    def add_weighted_edges_from(self, ebunch_to_add, weight="weight", **attr):
+        """Add weighted edges in `ebunch_to_add` with specified weight attr
+
+        Parameters
+        ----------
+        ebunch_to_add : container of edges
+            Each edge given in the list or container will be added
+            to the graph. The edges must be given as 3-tuples (u, v, w)
+            where w is a number.
+        weight : string, optional (default= 'weight')
+            The attribute name for the edge weights to be added.
+        attr : keyword arguments, optional (default= no attributes)
+            Edge attributes to add/update for all edges.
+
+        See Also
+        --------
+        add_edge : add a single edge
+        add_edges_from : add multiple edges
+
+        Notes
+        -----
+        Adding the same edge twice for Graph/DiGraph simply updates
+        the edge data. For MultiGraph/MultiDiGraph, duplicate edges
+        are stored.
+
+        Examples
+        --------
+        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
+        >>> G.add_weighted_edges_from([(0, 1, 3.0), (1, 2, 7.5)])
+        """
+        self.add_edges_from(((u, v, {weight: d}) for u, v, d in ebunch_to_add), **attr)
+
+    def add_weighted_edges_from(self, ebunch_to_add, weight="weight", **attr):
+        """Add weighted edges in `ebunch_to_add` with specified weight attr
+
+        Parameters
+        ----------
+        ebunch_to_add : container of edges
+            Each edge given in the list or container will be added
+            to the graph. The edges must be given as 3-tuples (u, v, w)
+            where w is a number.
+        weight : string, optional (default= 'weight')
+            The attribute name for the edge weights to be added.
+        attr : keyword arguments, optional (default= no attributes)
+            Edge attributes to add/update for all edges.
+
+        See Also
+        --------
+        add_edge : add a single edge
+        add_edges_from : add multiple edges
+
+        Notes
+        -----
+        Adding the same edge twice for Graph/DiGraph simply updates
+        the edge data. For MultiGraph/MultiDiGraph, duplicate edges
+        are stored.
+
+        Examples
+        --------
+        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
+        >>> G.add_weighted_edges_from([(0, 1, 3.0), (1, 2, 7.5)])
+        """
+        self.add_edges_from(((u, v, {weight: d}) for u, v, d in ebunch_to_add), **attr)
+
+    def add_edges_from_file(self, file, weighted=False):
+        """Added edges from file
+        For example, txt files,
+
+        Each line is in form like:
+        a b 23.0
+        which denotes an edge (a, b) with weight 23.0.
+
+        Parameters
+        ----------
+        file : string
+            The file path.
+
+        weighted : boolean, optional (default : False)
+            If the file consists of weight information, set `True`.
+            The weight key will be set as 'weight'.
+
+        Examples
+        --------
+
+        If `./club_network.txt` is:
+
+        Jack Mary 23.0
+
+        Mary Tom 15.0
+
+        Tom Ben 20.0
+
+        Then add them to *G*
+
+        >>> G.add_edges_from_file(file='./club_network.txt', weighted=True)
+
+
+        """
+        import re
+
+        with open(file, "r") as fp:
+            edges = fp.readlines()
+        if weighted:
+            for edge in edges:
+                edge = re.sub(",", " ", edge)
+                edge = edge.split()
+                try:
+                    self.add_edge(edge[0], edge[1], weight=float(edge[2]))
+                except:
+                    pass
+        else:
+            for edge in edges:
+                edge = re.sub(",", " ", edge)
+                edge = edge.split()
+                try:
+                    self.add_edge(edge[0], edge[1])
+                except:
+                    pass
+
+    def remove_nodes_from(self, nodes):
+        """Remove multiple nodes.
+
+        Parameters
+        ----------
+        nodes : iterable container
+            A container of nodes (list, dict, set, etc.).  If a node
+            in the container is not in the graph it is silently
+            ignored.
+
+        See Also
+        --------
+        remove_node
+
+        Examples
+        --------
+        >>> G = eg.path_graph(3)  # or DiGraph, MultiGraph, MultiDiGraph, etc
+        >>> e = list(G.nodes)
+        >>> e
+        [0, 1, 2]
+        >>> G.remove_nodes_from(e)
+        >>> list(G.nodes)
+        []
+
+        """
+        adj = self._adj
+        for n in nodes:
+            try:
+                del self._node[n]
+                for u in list(adj[n]):  # list handles self-loops
+                    del adj[u][n]  # (allows mutation of dict in loop)
+                del adj[n]
+            except KeyError:
+                pass
+
+    def _add_one_edge(self, u_of_edge, v_of_edge, edge_attr: dict = {}):
+        u, v = u_of_edge, v_of_edge
+        # add nodes
+        if u not in self._node:
+            self._add_one_node(u)
+        if v not in self._node:
+            self._add_one_node(v)
+        # add the edge
+        datadict = self._adj[u].get(v, self.edge_attr_dict_factory())
+        datadict.update(edge_attr)
+        self._adj[u][v] = datadict
+        self._adj[v][u] = datadict
+        if u == v:
+            self.extra_selfloop = True
+            self._raw_selfloop_dict[u] = datadict
+            self._clear_cache()
+
+    def remove_node(self, node_to_remove):
+        """Remove one node from your graph.
+
+        Parameters
+        ----------
+        node_to_remove : object
+            The node you want to remove.
+
+        See Also
+        --------
+        remove_nodes
+
+        Examples
+        --------
+        Remove node *Jack* from *G*
+
+        >>> G.remove_node('Jack')
+
+        """
+        try:
+            neighbors = list(self._adj[node_to_remove])
+            del self._node[node_to_remove]
+        except KeyError:  # Node not exists in self
+            raise EasyGraphError("No node {} in graph.".format(node_to_remove))
+        for neighbor in neighbors:  # Remove edges with other nodes
+            del self._adj[neighbor][node_to_remove]
+        del self._adj[node_to_remove]  # Remove this node
+        self._clear_cache()
+
+    def remove_nodes(self, nodes_to_remove: list):
+        """Remove nodes from your graph.
+
+        Parameters
+        ----------
+        nodes_to_remove : list of object
+            The list of nodes you want to remove.
+
+        See Also
+        --------
+        remove_node
+
+        Examples
+        --------
+        Remove node *[1, 2, 'a', 'b']* from *G*
+
+        >>> G.remove_nodes([1, 2, 'a', 'b'])
+
+        """
+        for (
+            node
+        ) in (
+            nodes_to_remove
+        ):  # If not all nodes included in graph, give up removing other nodes
+            assert node in self._node, "Remove Error: No node {} in graph".format(node)
+        for node in nodes_to_remove:
+            self.remove_node(node)
+        self._clear_cache()
+
+    def remove_edge(self, u, v):
+        """Remove one edge from your graph.
+
+        Parameters
+        ----------
+        u : object
+            One end of the edge.
+
+        v : object
+            The other end of the edge.
+
+        See Also
+        --------
+        remove_edges
+
+        Examples
+        --------
+        Remove edge (1,2) from *G*
+
+        >>> G.remove_edge(1,2)
+
+        """
+        try:
+            del self._adj[u][v]
+            if u != v:  # self-loop needs only one entry removed
+                del self._adj[v][u]
+            self._clear_cache()
+        except KeyError:
+            raise KeyError("No edge {}-{} in graph.".format(u, v))
+
+    def remove_edges(self, edges_to_remove: [tuple]):
+        """Remove a list of edges from your graph.
+
+        Parameters
+        ----------
+        edges_to_remove : list of tuple
+            The list of edges you want to remove,
+            Each element is (u, v) tuple, which denote the two ends of the edge.
+
+        See Also
+        --------
+        remove_edge
+
+        Examples
+        --------
+        Remove the edges *('Jack', 'Mary')* and *('Mary', 'Tom')* from *G*
+
+        >>> G.remove_edge([
+        ...     ('Jack', 'Mary'),
+        ...     ('Mary', 'Tom')
+        ... ])
+
+        """
+        for edge in edges_to_remove:
+            u, v = edge[:2]
+            self.remove_edge(u, v)
+        self._clear_cache()
+
+    def has_node(self, node):
+        """Returns whether a node exists
+
+        Parameters
+        ----------
+        node
+
+        Returns
+        -------
+        Bool : True (exist) or False (not exists)
+
+        """
+        return node in self._node
+
+    def has_edge(self, u, v):
+        """Returns whether an edge exists
+
+        Parameters
+        ----------
+        u : start node
+
+        v: end node
+
+        Returns
+        -------
+        Bool : True (exist) or False (not exists)
+
+        """
+        try:
+            return v in self._adj[u]
+        except KeyError:
+            return False
+
+    def number_of_nodes(self):
+        """Returns the number of nodes.
+
+        Returns
+        -------
+        number_of_nodes : int
+            The number of nodes.
+        """
+        return len(self._node)
+
+    def is_directed(self):
+        """Returns True if graph is a directed_graph, False otherwise."""
+        return False
+
+    def is_multigraph(self):
+        """Returns True if graph is a multigraph, False otherwise."""
+        return False
+
+    def copy(self):
+        """Return a deep copy of the graph.
+
+        Returns
+        -------
+        copy : easygraph.Graph
+            A deep copy of the original graph.
+
+        Examples
+        --------
+        *G2* is a deep copy of *G1*
+
+        >>> G2 = G1.copy()
+
+        """
+        G = self.__class__()
+        G.graph.update(self.graph)
+        for node, node_attr in self._node.items():
+            G.add_node(node, **node_attr)
+        for u, nbrs in self._adj.items():
+            for v, edge_data in nbrs.items():
+                G.add_edge(u, v, **edge_data)
+
+        return G
+
+    def nodes_subgraph(self, from_nodes: list):
+        """Returns a subgraph of some nodes
+
+        Parameters
+        ----------
+        from_nodes : list of object
+            The nodes in subgraph.
+
+        Returns
+        -------
+        nodes_subgraph : easygraph.Graph
+            The subgraph consisting of *from_nodes*.
+
+        Examples
+        --------
+
+        >>> G = eg.Graph()
+        >>> G.add_edges([(1,2), (2,3), (2,4), (4,5)])
+        >>> G_sub = G.nodes_subgraph(from_nodes= [1,2,3])
+
+        """
+        G = self.__class__()
+        G.graph.update(self.graph)
+        from_nodes = set(from_nodes)
+        for node in from_nodes:
+            try:
+                G.add_node(node, **self._node[node])
+            except KeyError:
+                pass
+
+            for v, edge_data in self._adj[node].items():
+                if v in from_nodes:
+                    G.add_edge(node, v, **edge_data)
+        return G
+
+    def ego_subgraph(self, center):
+        """Returns an ego network graph of a node.
+
+        Parameters
+        ----------
+        center : object
+            The center node of the ego network graph
+
+        Returns
+        -------
+        ego_subgraph : easygraph.Graph
+            The ego network graph of *center*.
+
+
+        Examples
+        --------
+        >>> G = eg.Graph()
+        >>> G.add_edges([
+        ...     ('Jack', 'Maria'),
+        ...     ('Maria', 'Andy'),
+        ...     ('Jack', 'Tom')
+        ... ])
+        >>> G.ego_subgraph(center='Jack')
+        """
+        neighbors_of_center = list(self.all_neighbors(center))
+        neighbors_of_center.append(center)
+        return self.nodes_subgraph(from_nodes=neighbors_of_center)
+
+    def to_index_node_graph(self, begin_index=0):
+        """Returns a deep copy of graph, with each node switched to its index.
+
+        Considering that the nodes of your graph may be any possible hashable Python object,
+        you can get an isomorphic graph of the original one, with each node switched to its index.
+
+        Parameters
+        ----------
+        begin_index : int
+            The begin index of the index graph.
+
+        Returns
+        -------
+        G : easygraph.Graph
+            Deep copy of graph, with each node switched to its index.
+
+        index_of_node : dict
+            Index of node
+
+        node_of_index : dict
+            Node of index
+
+        Examples
+        --------
+        The following method returns this isomorphic graph and index-to-node dictionary
+        as well as node-to-index dictionary.
+
+        >>> G = eg.Graph()
+        >>> G.add_edges([
+        ...     ('Jack', 'Maria'),
+        ...     ('Maria', 'Andy'),
+        ...     ('Jack', 'Tom')
+        ... ])
+        >>> G_index_graph, index_of_node, node_of_index = G.to_index_node_graph()
+
+        """
+        G = self.__class__()
+        G.graph.update(self.graph)
+        index_of_node = dict()
+        node_of_index = dict()
+        for index, (node, node_attr) in enumerate(self._node.items()):
+            G.add_node(index + begin_index, **node_attr)
+            index_of_node[node] = index + begin_index
+            node_of_index[index + begin_index] = node
+        for u, nbrs in self._adj.items():
+            for v, edge_data in nbrs.items():
+                G.add_edge(index_of_node[u], index_of_node[v], **edge_data)
+
+        return G, index_of_node, node_of_index
+
+    def _clear_cache(self):
+        r"""Clear the cache."""
+        self.cache = {}
+
+    def to_directed_class(self):
+        """Returns the class to use for empty directed copies.
+
+        If you subclass the base classes, use this to designate
+        what directed class to use for `to_directed()` copies.
+        """
+        return eg.DiGraph
+
+    def to_directed(self):
+        """Returns a directed representation of the graph.
+
+        Returns
+        -------
+        G : DiGraph
+            A directed graph with the same name, same nodes, and with
+            each edge (u, v, data) replaced by two directed edges
+            (u, v, data) and (v, u, data).
+
+        Notes
+        -----
+        This returns a "deepcopy" of the edge, node, and
+        graph attributes which attempts to completely copy
+        all of the data and references.
+
+        This is in contrast to the similar D=DiGraph(G) which returns a
+        shallow copy of the data.
+
+        See the Python copy module for more information on shallow
+        and deep copies, https://docs.python.org/3/library/copy.html.
+
+        Warning: If you have subclassed Graph to use dict-like objects
+        in the data structure, those changes do not transfer to the
+        DiGraph created by this method.
+
+        Examples
+        --------
+        >>> G = eg.Graph()  # or MultiGraph, etc
+        >>> G.add_edge(0, 1)
+        >>> H = G.to_directed()
+        >>> list(H.edges)
+        [(0, 1), (1, 0)]
+
+        If already directed, return a (deep) copy
+
+        >>> G = eg.DiGraph()  # or MultiDiGraph, etc
+        >>> G.add_edge(0, 1)
+        >>> H = G.to_directed()
+        >>> list(H.edges)
+        [(0, 1)]
+        """
+        graph_class = self.to_directed_class()
+
+        G = graph_class()
+        G.graph.update(deepcopy(self.graph))
+        G.add_nodes_from((n, deepcopy(d)) for n, d in self._node.items())
+        G.add_edges_from(
+            (u, v, deepcopy(data))
+            for u, nbrs in self._adj.items()
+            for v, data in nbrs.items()
+        )
+        return G
+
+    def cpp(self):
+        G = GraphC()
+        G.graph.update(self.graph)
+        for u, attr in self.nodes.items():
+            G.add_node(u, **attr)
+        for u, v, attr in self.edges:
+            G.add_edge(u, v, **attr)
+        G.generate_linkgraph()
+        return G
+
+
+try:
+    import cpp_easygraph
+
+    class GraphC(cpp_easygraph.Graph):
+        cflag = 1
+
+except ImportError:
+
+    class GraphC:
+        def __init__(self, **graph_attr):
+            print(
+                "Object cannot be instantiated because C extension has not been"
+                " successfully compiled and installed. Please refer to"
+                " https://github.com/easy-graph/Easy-Graph/blob/master/README.rst and"
+                " reinstall easygraph."
+            )
+            raise RuntimeError
```

## easygraph/classes/multigraph.py

 * *Ordering differences only*

```diff
@@ -1,729 +1,729 @@
-"""Base class for MultiGraph."""
-from copy import deepcopy
-from typing import Dict
-from typing import List
-
-import easygraph as eg
-import easygraph.convert as convert
-
-from easygraph.classes.graph import Graph
-from easygraph.utils.exception import EasyGraphError
-
-
-__all__ = ["MultiGraph"]
-
-
-class MultiGraph(Graph):
-    edge_key_dict_factory = dict
-
-    def __init__(self, incoming_graph_data=None, multigraph_input=None, **attr):
-        """Initialize a graph with edges, name, or graph attributes.
-
-        Parameters
-        ----------
-        incoming_graph_data : input graph
-            Data to initialize graph.  If incoming_graph_data=None (default)
-            an empty graph is created.  The data can be an edge list, or any
-            EasyGraph graph object.  If the corresponding optional Python
-            packages are installed the data can also be a NumPy matrix
-            or 2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.
-
-        multigraph_input : bool or None (default None)
-            Note: Only used when `incoming_graph_data` is a dict.
-            If True, `incoming_graph_data` is assumed to be a
-            dict-of-dict-of-dict-of-dict structure keyed by
-            node to neighbor to edge keys to edge data for multi-edges.
-            A EasyGraphError is raised if this is not the case.
-            If False, :func:`to_easygraph_graph` is used to try to determine
-            the dict's graph data structure as either a dict-of-dict-of-dict
-            keyed by node to neighbor to edge data, or a dict-of-iterable
-            keyed by node to neighbors.
-            If None, the treatment for True is tried, but if it fails,
-            the treatment for False is tried.
-
-        attr : keyword arguments, optional (default= no attributes)
-            Attributes to add to graph as key=value pairs.
-
-        See Also
-        --------
-        convert
-
-        Examples
-        --------
-        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
-        >>> G = eg.Graph(name="my graph")
-        >>> e = [(1, 2), (2, 3), (3, 4)]  # list of edges
-        >>> G = eg.Graph(e)
-
-        Arbitrary graph attribute pairs (key=value) may be assigned
-
-        >>> G = eg.Graph(e, day="Friday")
-        >>> G.graph
-        {'day': 'Friday'}
-
-        """
-        self.edge_key_dict_factory = self.edge_key_dict_factory
-        if isinstance(incoming_graph_data, dict) and multigraph_input is not False:
-            Graph.__init__(self)
-            try:
-                convert.from_dict_of_dicts(
-                    incoming_graph_data, create_using=self, multigraph_input=True
-                )
-                self.graph.update(attr)
-            except Exception as err:
-                if multigraph_input is True:
-                    raise eg.EasyGraphError(
-                        f"converting multigraph_input raised:\n{type(err)}: {err}"
-                    )
-                Graph.__init__(self, incoming_graph_data, **attr)
-        else:
-            Graph.__init__(self, incoming_graph_data, **attr)
-
-    def new_edge_key(self, u, v):
-        """Returns an unused key for edges between nodes `u` and `v`.
-
-        The nodes `u` and `v` do not need to be already in the graph.
-
-        Notes
-        -----
-        In the standard MultiGraph class the new key is the number of existing
-        edges between `u` and `v` (increased if necessary to ensure unused).
-        The first edge will have key 0, then 1, etc. If an edge is removed
-        further new_edge_keys may not be in this order.
-
-        Parameters
-        ----------
-        u, v : nodes
-
-        Returns
-        -------
-        key : int
-        """
-        try:
-            keydict = self._adj[u][v]
-        except KeyError:
-            return 0
-        key = len(keydict)
-        while key in keydict:
-            key += 1
-        return key
-
-    def add_edge(self, u_for_edge, v_for_edge, key=None, **attr):
-        """Add an edge between u and v.
-
-        The nodes u and v will be automatically added if they are
-        not already in the graph.
-
-        Edge attributes can be specified with keywords or by directly
-        accessing the edge's attribute dictionary. See examples below.
-
-        Parameters
-        ----------
-        u_for_edge, v_for_edge : nodes
-            Nodes can be, for example, strings or numbers.
-            Nodes must be hashable (and not None) Python objects.
-        key : hashable identifier, optional (default=lowest unused integer)
-            Used to distinguish multiedges between a pair of nodes.
-        attr : keyword arguments, optional
-            Edge data (or labels or objects) can be assigned using
-            keyword arguments.
-
-        Returns
-        -------
-        The edge key assigned to the edge.
-
-        See Also
-        --------
-        add_edges_from : add a collection of edges
-
-        Notes
-        -----
-        To replace/update edge data, use the optional key argument
-        to identify a unique edge.  Otherwise a new edge will be created.
-
-        EasyGraph algorithms designed for weighted graphs cannot use
-        multigraphs directly because it is not clear how to handle
-        multiedge weights.  Convert to Graph using edge attribute
-        'weight' to enable weighted graph algorithms.
-
-        Default keys are generated using the method `new_edge_key()`.
-        This method can be overridden by subclassing the base class and
-        providing a custom `new_edge_key()` method.
-
-        Examples
-        --------
-        The following all add the edge e=(1, 2) to graph G:
-
-        >>> G = eg.MultiGraph()
-        >>> e = (1, 2)
-        >>> ekey = G.add_edge(1, 2)  # explicit two-node form
-        >>> G.add_edge(*e)  # single edge as tuple of two nodes
-        1
-        >>> G.add_edges_from([(1, 2)])  # add edges from iterable container
-        [2]
-
-        Associate data to edges using keywords:
-
-        >>> ekey = G.add_edge(1, 2, weight=3)
-        >>> ekey = G.add_edge(1, 2, key=0, weight=4)  # update data for key=0
-        >>> ekey = G.add_edge(1, 3, weight=7, capacity=15, length=342.7)
-
-        For non-string attribute keys, use subscript notation.
-
-        >>> ekey = G.add_edge(1, 2)
-        >>> G[1][2][0].update({0: 5})
-        >>> G.edges[1, 2, 0].update({0: 5})
-        """
-        u, v = u_for_edge, v_for_edge
-        # add nodes
-        if u not in self._adj:
-            if u is None:
-                raise ValueError("None cannot be a node")
-            self._adj[u] = self.adjlist_inner_dict_factory()
-            self._node[u] = self.node_attr_dict_factory()
-        if v not in self._adj:
-            if v is None:
-                raise ValueError("None cannot be a node")
-            self._adj[v] = self.adjlist_inner_dict_factory()
-            self._node[v] = self.node_attr_dict_factory()
-        if key is None:
-            key = self.new_edge_key(u, v)
-        if v in self._adj[u]:
-            keydict = self._adj[u][v]
-            datadict = keydict.get(key, self.edge_attr_dict_factory())
-            datadict.update(attr)
-            keydict[key] = datadict
-        else:
-            # selfloops work this way without special treatment
-            datadict = self.edge_attr_dict_factory()
-            datadict.update(attr)
-            keydict = self.edge_key_dict_factory()
-            keydict[key] = datadict
-            self._adj[u][v] = keydict
-            self._adj[v][u] = keydict
-        return key
-
-    def add_edges_from(self, ebunch_to_add, **attr):
-        """Add all the edges in ebunch_to_add.
-
-        Parameters
-        ----------
-        ebunch_to_add : container of edges
-            Each edge given in the container will be added to the
-            graph. The edges can be:
-
-                - 2-tuples (u, v) or
-                - 3-tuples (u, v, d) for an edge data dict d, or
-                - 3-tuples (u, v, k) for not iterable key k, or
-                - 4-tuples (u, v, k, d) for an edge with data and key k
-
-        attr : keyword arguments, optional
-            Edge data (or labels or objects) can be assigned using
-            keyword arguments.
-
-        Returns
-        -------
-        A list of edge keys assigned to the edges in `ebunch`.
-
-        See Also
-        --------
-        add_edge : add a single edge
-        add_weighted_edges_from : convenient way to add weighted edges
-
-        Notes
-        -----
-        Adding the same edge twice has no effect but any edge data
-        will be updated when each duplicate edge is added.
-
-        Edge attributes specified in an ebunch take precedence over
-        attributes specified via keyword arguments.
-
-        Default keys are generated using the method ``new_edge_key()``.
-        This method can be overridden by subclassing the base class and
-        providing a custom ``new_edge_key()`` method.
-
-        Examples
-        --------
-        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
-        >>> G.add_edges_from([(0, 1), (1, 2)])  # using a list of edge tuples
-        >>> e = zip(range(0, 3), range(1, 4))
-        >>> G.add_edges_from(e)  # Add the path graph 0-1-2-3
-
-        Associate data to edges
-
-        >>> G.add_edges_from([(1, 2), (2, 3)], weight=3)
-        >>> G.add_edges_from([(3, 4), (1, 4)], label="WN2898")
-        """
-        keylist = []
-        for e in ebunch_to_add:
-            ne = len(e)
-            if ne == 4:
-                u, v, key, dd = e
-            elif ne == 3:
-                u, v, dd = e
-                key = None
-            elif ne == 2:
-                u, v = e
-                dd = {}
-                key = None
-            else:
-                msg = f"Edge tuple {e} must be a 2-tuple, 3-tuple or 4-tuple."
-                raise EasyGraphError(msg)
-            ddd = {}
-            ddd.update(attr)
-            try:
-                ddd.update(dd)
-            except (TypeError, ValueError):
-                if ne != 3:
-                    raise
-                key = dd  # ne == 3 with 3rd value not dict, must be a key
-            key = self.add_edge(u, v, key)
-            self[u][v][key].update(ddd)
-            keylist.append(key)
-        return keylist
-
-    def remove_edge(self, u, v, key=None):
-        """Remove an edge between u and v.
-
-        Parameters
-        ----------
-        u, v : nodes
-            Remove an edge between nodes u and v.
-        key : hashable identifier, optional (default=None)
-            Used to distinguish multiple edges between a pair of nodes.
-            If None remove a single (arbitrary) edge between u and v.
-
-        Raises
-        ------
-        EasyGraphError
-            If there is not an edge between u and v, or
-            if there is no edge with the specified key.
-
-        See Also
-        --------
-        remove_edges_from : remove a collection of edges
-
-        Examples
-        --------
-        For multiple edges
-
-        >>> G = eg.MultiGraph()  # or MultiDiGraph, etc
-        >>> G.add_edges_from([(1, 2), (1, 2), (1, 2)])  # key_list returned
-        [0, 1, 2]
-        >>> G.remove_edge(1, 2)  # remove a single (arbitrary) edge
-
-        For edges with keys
-
-        >>> G = eg.MultiGraph()  # or MultiDiGraph, etc
-        >>> G.add_edge(1, 2, key="first")
-        'first'
-        >>> G.add_edge(1, 2, key="second")
-        'second'
-        >>> G.remove_edge(1, 2, key="second")
-
-        """
-        try:
-            d = self._adj[u][v]
-        except KeyError as err:
-            raise EasyGraphError(f"The edge {u}-{v} is not in the graph.") from err
-        # remove the edge with specified data
-        if key is None:
-            d.popitem()
-        else:
-            try:
-                del d[key]
-            except KeyError as err:
-                msg = f"The edge {u}-{v} with key {key} is not in the graph."
-                raise EasyGraphError(msg) from err
-        if len(d) == 0:
-            # remove the key entries if last edge
-            del self._adj[u][v]
-            if u != v:  # check for selfloop
-                del self._adj[v][u]
-
-    def remove_edges_from(self, ebunch):
-        """Remove all edges specified in ebunch.
-
-        Parameters
-        ----------
-        ebunch: list or container of edge tuples
-            Each edge given in the list or container will be removed
-            from the graph. The edges can be:
-
-                - 2-tuples (u, v) All edges between u and v are removed.
-                - 3-tuples (u, v, key) The edge identified by key is removed.
-                - 4-tuples (u, v, key, data) where data is ignored.
-
-        See Also
-        --------
-        remove_edge : remove a single edge
-
-        Notes
-        -----
-        Will fail silently if an edge in ebunch is not in the graph.
-
-        Examples
-        --------
-        Removing multiple copies of edges
-
-        >>> G = eg.MultiGraph()
-        >>> keys = G.add_edges_from([(1, 2), (1, 2), (1, 2)])
-        >>> G.remove_edges_from([(1, 2), (1, 2)])
-        >>> list(G.edges())
-        [(1, 2)]
-        >>> G.remove_edges_from([(1, 2), (1, 2)])  # silently ignore extra copy
-        >>> list(G.edges)  # now empty graph
-        []
-        """
-        for e in ebunch:
-            try:
-                self.remove_edge(*e[:3])
-            except EasyGraphError:
-                pass
-
-    def has_edge(self, u, v, key=None):
-        """Returns True if the graph has an edge between nodes u and v.
-
-        This is the same as `v in G[u] or key in G[u][v]`
-        without KeyError exceptions.
-
-        Parameters
-        ----------
-        u, v : nodes
-            Nodes can be, for example, strings or numbers.
-
-        key : hashable identifier, optional (default=None)
-            If specified return True only if the edge with
-            key is found.
-
-        Returns
-        -------
-        edge_ind : bool
-            True if edge is in the graph, False otherwise.
-
-        Examples
-        --------
-        Can be called either using two nodes u, v, an edge tuple (u, v),
-        or an edge tuple (u, v, key).
-
-        >>> G = eg.MultiGraph()  # or MultiDiGraph
-        >>> G = eg.complete_graph(4, create_using=eg.MultiDiGraph)
-        >>> G.has_edge(0, 1)  # using two nodes
-        True
-        >>> e = (0, 1)
-        >>> G.has_edge(*e)  #  e is a 2-tuple (u, v)
-        True
-        >>> G.add_edge(0, 1, key="a")
-        'a'
-        >>> G.has_edge(0, 1, key="a")  # specify key
-        True
-        >>> e = (0, 1, "a")
-        >>> G.has_edge(*e)  # e is a 3-tuple (u, v, 'a')
-        True
-
-        The following syntax are equivalent:
-
-        >>> G.has_edge(0, 1)
-        True
-        >>> 1 in G[0]  # though this gives :exc:`KeyError` if 0 not in G
-        True
-
-        """
-        try:
-            if key is None:
-                return v in self._adj[u]
-            else:
-                return key in self._adj[u][v]
-        except KeyError:
-            return False
-
-    @property
-    def edges(self):
-        edges = list()
-        seen = {}
-        for n, nbrs in self._adj.items():
-            for nbr, kd in nbrs.items():
-                if nbr not in seen:
-                    for k, dd in kd.items():
-                        edges.append((n, nbr, k, dd))
-            seen[n] = 1
-        del seen
-        return edges
-
-    def get_edge_data(self, u, v, key=None, default=None):
-        """Returns the attribute dictionary associated with edge (u, v).
-
-        This is identical to `G[u][v][key]` except the default is returned
-        instead of an exception is the edge doesn't exist.
-
-        Parameters
-        ----------
-        u, v : nodes
-
-        default :  any Python object (default=None)
-            Value to return if the edge (u, v) is not found.
-
-        key : hashable identifier, optional (default=None)
-            Return data only for the edge with specified key.
-
-        Returns
-        -------
-        edge_dict : dictionary
-            The edge attribute dictionary.
-
-        Examples
-        --------
-        >>> G = eg.MultiGraph()  # or MultiDiGraph
-        >>> key = G.add_edge(0, 1, key="a", weight=7)
-        >>> G[0][1]["a"]  # key='a'
-        {'weight': 7}
-        >>> G.edges[0, 1, "a"]  # key='a'
-        {'weight': 7}
-
-        Warning: we protect the graph data structure by making
-        `G.edges` and `G[1][2]` read-only dict-like structures.
-        However, you can assign values to attributes in e.g.
-        `G.edges[1, 2, 'a']` or `G[1][2]['a']` using an additional
-        bracket as shown next. You need to specify all edge info
-        to assign to the edge data associated with an edge.
-
-        >>> G[0][1]["a"]["weight"] = 10
-        >>> G.edges[0, 1, "a"]["weight"] = 10
-        >>> G[0][1]["a"]["weight"]
-        10
-        >>> G.edges[1, 0, "a"]["weight"]
-        10
-
-        >>> G = eg.MultiGraph()  # or MultiDiGraph
-        >>> G = eg.complete_graph(4, create_using=eg.MultiDiGraph)
-        >>> G.get_edge_data(0, 1)
-        {0: {}}
-        >>> e = (0, 1)
-        >>> G.get_edge_data(*e)  # tuple form
-        {0: {}}
-        >>> G.get_edge_data("a", "b", default=0)  # edge not in graph, return 0
-        0
-        """
-        try:
-            if key is None:
-                return self._adj[u][v]
-            else:
-                return self._adj[u][v][key]
-        except KeyError:
-            return default
-
-    @property
-    def degree(self, weight="weight"):
-        degree = dict()
-        if weight is None:
-            for n in self._nodes:
-                nbrs = self._succ[n]
-                deg = sum(len(keys) for keys in nbrs.values()) + (
-                    n in nbrs and len(nbrs[n])
-                )
-                degree[n] = deg
-        else:
-            for n in self._nodes:
-                nbrs = self._succ[n]
-                deg = sum(
-                    d.get(weight, 1)
-                    for key_dict in nbrs.values()
-                    for d in key_dict.values()
-                )
-                if n in nbrs:
-                    deg += sum(d.get(weight, 1) for d in nbrs[n].values())
-                degree[n] = deg
-
-    def is_multigraph(self):
-        """Returns True if graph is a multigraph, False otherwise."""
-        return True
-
-    def is_directed(self):
-        """Returns True if graph is directed, False otherwise."""
-        return False
-
-    def copy(self):
-        """Returns a copy of the graph.
-
-        The copy method by default returns an independent shallow copy
-        of the graph and attributes. That is, if an attribute is a
-        container, that container is shared by the original an the copy.
-        Use Python's `copy.deepcopy` for new containers.
-
-        Notes
-        -----
-        All copies reproduce the graph structure, but data attributes
-        may be handled in different ways. There are four types of copies
-        of a graph that people might want.
-
-        Deepcopy -- A "deepcopy" copies the graph structure as well as
-        all data attributes and any objects they might contain.
-        The entire graph object is new so that changes in the copy
-        do not affect the original object. (see Python's copy.deepcopy)
-
-        Data Reference (Shallow) -- For a shallow copy the graph structure
-        is copied but the edge, node and graph attribute dicts are
-        references to those in the original graph. This saves
-        time and memory but could cause confusion if you change an attribute
-        in one graph and it changes the attribute in the other.
-        EasyGraph does not provide this level of shallow copy.
-
-        Independent Shallow -- This copy creates new independent attribute
-        dicts and then does a shallow copy of the attributes. That is, any
-        attributes that are containers are shared between the new graph
-        and the original. This is exactly what `dict.copy()` provides.
-        You can obtain this style copy using:
-
-            >>> G = eg.path_graph(5)
-            >>> H = G.copy()
-            >>> H = eg.Graph(G)
-            >>> H = G.__class__(G)
-
-        Fresh Data -- For fresh data, the graph structure is copied while
-        new empty data attribute dicts are created. The resulting graph
-        is independent of the original and it has no edge, node or graph
-        attributes. Fresh copies are not enabled. Instead use:
-
-            >>> H = G.__class__()
-            >>> H.add_nodes_from(G)
-            >>> H.add_edges_from(G.edges)
-
-        See the Python copy module for more information on shallow
-        and deep copies, https://docs.python.org/3/library/copy.html.
-
-        Returns
-        -------
-        G : Graph
-            A copy of the graph.
-
-        See Also
-        --------
-        to_directed: return a directed copy of the graph.
-
-        Examples
-        --------
-        >>> G = eg.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc
-        >>> H = G.copy()
-
-        """
-        G = self.__class__()
-        G.graph.update(self.graph)
-        G.add_nodes_from((n, d.copy()) for n, d in self._node.items())
-        G.add_edges_from(
-            (u, v, key, datadict.copy())
-            for u, nbrs in self._adj.items()
-            for v, keydict in nbrs.items()
-            for key, datadict in keydict.items()
-        )
-        return G
-
-    def to_directed(self):
-        """Returns a directed representation of the graph.
-
-        Returns
-        -------
-        G : MultiDiGraph
-            A directed graph with the same name, same nodes, and with
-            each edge (u, v, data) replaced by two directed edges
-            (u, v, data) and (v, u, data).
-
-        Notes
-        -----
-        This returns a "deepcopy" of the edge, node, and
-        graph attributes which attempts to completely copy
-        all of the data and references.
-
-        This is in contrast to the similar D=DiGraph(G) which returns a
-        shallow copy of the data.
-
-        See the Python copy module for more information on shallow
-        and deep copies, https://docs.python.org/3/library/copy.html.
-
-        Warning: If you have subclassed MultiGraph to use dict-like objects
-        in the data structure, those changes do not transfer to the
-        MultiDiGraph created by this method.
-
-        Examples
-        --------
-        >>> G = eg.Graph()  # or MultiGraph, etc
-        >>> G.add_edge(0, 1)
-        >>> H = G.to_directed()
-        >>> list(H.edges)
-        [(0, 1), (1, 0)]
-
-        If already directed, return a (deep) copy
-
-        >>> G = eg.DiGraph()  # or MultiDiGraph, etc
-        >>> G.add_edge(0, 1)
-        >>> H = G.to_directed()
-        >>> list(H.edges)
-        [(0, 1)]
-        """
-        G = eg.MultiDiGraph()
-        G.graph.update(deepcopy(self.graph))
-        G.add_nodes_from((n, deepcopy(d)) for n, d in self._node.items())
-        G.add_edges_from(
-            (u, v, key, deepcopy(datadict))
-            for u, nbrs in self.adj.items()
-            for v, keydict in nbrs.items()
-            for key, datadict in keydict.items()
-        )
-        return G
-
-    def number_of_edges(self, u=None, v=None):
-        """Returns the number of edges between two nodes.
-
-        Parameters
-        ----------
-        u, v : nodes, optional (Gefault=all edges)
-            If u and v are specified, return the number of edges between
-            u and v. Otherwise return the total number of all edges.
-
-        Returns
-        -------
-        nedges : int
-            The number of edges in the graph.  If nodes `u` and `v` are
-            specified return the number of edges between those nodes. If
-            the graph is directed, this only returns the number of edges
-            from `u` to `v`.
-
-        See Also
-        --------
-        size
-
-        Examples
-        --------
-        For undirected multigraphs, this method counts the total number
-        of edges in the graph::
-
-            >>> G = eg.MultiGraph()
-            >>> G.add_edges_from([(0, 1), (0, 1), (1, 2)])
-            [0, 1, 0]
-            >>> G.number_of_edges()
-            3
-
-        If you specify two nodes, this counts the total number of edges
-        joining the two nodes::
-
-            >>> G.number_of_edges(0, 1)
-            2
-
-        For directed multigraphs, this method can count the total number
-        of directed edges from `u` to `v`::
-
-            >>> G = eg.MultiDiGraph()
-            >>> G.add_edges_from([(0, 1), (0, 1), (1, 0)])
-            [0, 1, 0]
-            >>> G.number_of_edges(0, 1)
-            2
-            >>> G.number_of_edges(1, 0)
-            1
-
-        """
-        if u is None:
-            return self.size()
-        try:
-            edgedata = self._adj[u][v]
-        except KeyError:
-            return 0  # no such edge
-        return len(edgedata)
+"""Base class for MultiGraph."""
+from copy import deepcopy
+from typing import Dict
+from typing import List
+
+import easygraph as eg
+import easygraph.convert as convert
+
+from easygraph.classes.graph import Graph
+from easygraph.utils.exception import EasyGraphError
+
+
+__all__ = ["MultiGraph"]
+
+
+class MultiGraph(Graph):
+    edge_key_dict_factory = dict
+
+    def __init__(self, incoming_graph_data=None, multigraph_input=None, **attr):
+        """Initialize a graph with edges, name, or graph attributes.
+
+        Parameters
+        ----------
+        incoming_graph_data : input graph
+            Data to initialize graph.  If incoming_graph_data=None (default)
+            an empty graph is created.  The data can be an edge list, or any
+            EasyGraph graph object.  If the corresponding optional Python
+            packages are installed the data can also be a NumPy matrix
+            or 2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.
+
+        multigraph_input : bool or None (default None)
+            Note: Only used when `incoming_graph_data` is a dict.
+            If True, `incoming_graph_data` is assumed to be a
+            dict-of-dict-of-dict-of-dict structure keyed by
+            node to neighbor to edge keys to edge data for multi-edges.
+            A EasyGraphError is raised if this is not the case.
+            If False, :func:`to_easygraph_graph` is used to try to determine
+            the dict's graph data structure as either a dict-of-dict-of-dict
+            keyed by node to neighbor to edge data, or a dict-of-iterable
+            keyed by node to neighbors.
+            If None, the treatment for True is tried, but if it fails,
+            the treatment for False is tried.
+
+        attr : keyword arguments, optional (default= no attributes)
+            Attributes to add to graph as key=value pairs.
+
+        See Also
+        --------
+        convert
+
+        Examples
+        --------
+        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
+        >>> G = eg.Graph(name="my graph")
+        >>> e = [(1, 2), (2, 3), (3, 4)]  # list of edges
+        >>> G = eg.Graph(e)
+
+        Arbitrary graph attribute pairs (key=value) may be assigned
+
+        >>> G = eg.Graph(e, day="Friday")
+        >>> G.graph
+        {'day': 'Friday'}
+
+        """
+        self.edge_key_dict_factory = self.edge_key_dict_factory
+        if isinstance(incoming_graph_data, dict) and multigraph_input is not False:
+            Graph.__init__(self)
+            try:
+                convert.from_dict_of_dicts(
+                    incoming_graph_data, create_using=self, multigraph_input=True
+                )
+                self.graph.update(attr)
+            except Exception as err:
+                if multigraph_input is True:
+                    raise eg.EasyGraphError(
+                        f"converting multigraph_input raised:\n{type(err)}: {err}"
+                    )
+                Graph.__init__(self, incoming_graph_data, **attr)
+        else:
+            Graph.__init__(self, incoming_graph_data, **attr)
+
+    def new_edge_key(self, u, v):
+        """Returns an unused key for edges between nodes `u` and `v`.
+
+        The nodes `u` and `v` do not need to be already in the graph.
+
+        Notes
+        -----
+        In the standard MultiGraph class the new key is the number of existing
+        edges between `u` and `v` (increased if necessary to ensure unused).
+        The first edge will have key 0, then 1, etc. If an edge is removed
+        further new_edge_keys may not be in this order.
+
+        Parameters
+        ----------
+        u, v : nodes
+
+        Returns
+        -------
+        key : int
+        """
+        try:
+            keydict = self._adj[u][v]
+        except KeyError:
+            return 0
+        key = len(keydict)
+        while key in keydict:
+            key += 1
+        return key
+
+    def add_edge(self, u_for_edge, v_for_edge, key=None, **attr):
+        """Add an edge between u and v.
+
+        The nodes u and v will be automatically added if they are
+        not already in the graph.
+
+        Edge attributes can be specified with keywords or by directly
+        accessing the edge's attribute dictionary. See examples below.
+
+        Parameters
+        ----------
+        u_for_edge, v_for_edge : nodes
+            Nodes can be, for example, strings or numbers.
+            Nodes must be hashable (and not None) Python objects.
+        key : hashable identifier, optional (default=lowest unused integer)
+            Used to distinguish multiedges between a pair of nodes.
+        attr : keyword arguments, optional
+            Edge data (or labels or objects) can be assigned using
+            keyword arguments.
+
+        Returns
+        -------
+        The edge key assigned to the edge.
+
+        See Also
+        --------
+        add_edges_from : add a collection of edges
+
+        Notes
+        -----
+        To replace/update edge data, use the optional key argument
+        to identify a unique edge.  Otherwise a new edge will be created.
+
+        EasyGraph algorithms designed for weighted graphs cannot use
+        multigraphs directly because it is not clear how to handle
+        multiedge weights.  Convert to Graph using edge attribute
+        'weight' to enable weighted graph algorithms.
+
+        Default keys are generated using the method `new_edge_key()`.
+        This method can be overridden by subclassing the base class and
+        providing a custom `new_edge_key()` method.
+
+        Examples
+        --------
+        The following all add the edge e=(1, 2) to graph G:
+
+        >>> G = eg.MultiGraph()
+        >>> e = (1, 2)
+        >>> ekey = G.add_edge(1, 2)  # explicit two-node form
+        >>> G.add_edge(*e)  # single edge as tuple of two nodes
+        1
+        >>> G.add_edges_from([(1, 2)])  # add edges from iterable container
+        [2]
+
+        Associate data to edges using keywords:
+
+        >>> ekey = G.add_edge(1, 2, weight=3)
+        >>> ekey = G.add_edge(1, 2, key=0, weight=4)  # update data for key=0
+        >>> ekey = G.add_edge(1, 3, weight=7, capacity=15, length=342.7)
+
+        For non-string attribute keys, use subscript notation.
+
+        >>> ekey = G.add_edge(1, 2)
+        >>> G[1][2][0].update({0: 5})
+        >>> G.edges[1, 2, 0].update({0: 5})
+        """
+        u, v = u_for_edge, v_for_edge
+        # add nodes
+        if u not in self._adj:
+            if u is None:
+                raise ValueError("None cannot be a node")
+            self._adj[u] = self.adjlist_inner_dict_factory()
+            self._node[u] = self.node_attr_dict_factory()
+        if v not in self._adj:
+            if v is None:
+                raise ValueError("None cannot be a node")
+            self._adj[v] = self.adjlist_inner_dict_factory()
+            self._node[v] = self.node_attr_dict_factory()
+        if key is None:
+            key = self.new_edge_key(u, v)
+        if v in self._adj[u]:
+            keydict = self._adj[u][v]
+            datadict = keydict.get(key, self.edge_attr_dict_factory())
+            datadict.update(attr)
+            keydict[key] = datadict
+        else:
+            # selfloops work this way without special treatment
+            datadict = self.edge_attr_dict_factory()
+            datadict.update(attr)
+            keydict = self.edge_key_dict_factory()
+            keydict[key] = datadict
+            self._adj[u][v] = keydict
+            self._adj[v][u] = keydict
+        return key
+
+    def add_edges_from(self, ebunch_to_add, **attr):
+        """Add all the edges in ebunch_to_add.
+
+        Parameters
+        ----------
+        ebunch_to_add : container of edges
+            Each edge given in the container will be added to the
+            graph. The edges can be:
+
+                - 2-tuples (u, v) or
+                - 3-tuples (u, v, d) for an edge data dict d, or
+                - 3-tuples (u, v, k) for not iterable key k, or
+                - 4-tuples (u, v, k, d) for an edge with data and key k
+
+        attr : keyword arguments, optional
+            Edge data (or labels or objects) can be assigned using
+            keyword arguments.
+
+        Returns
+        -------
+        A list of edge keys assigned to the edges in `ebunch`.
+
+        See Also
+        --------
+        add_edge : add a single edge
+        add_weighted_edges_from : convenient way to add weighted edges
+
+        Notes
+        -----
+        Adding the same edge twice has no effect but any edge data
+        will be updated when each duplicate edge is added.
+
+        Edge attributes specified in an ebunch take precedence over
+        attributes specified via keyword arguments.
+
+        Default keys are generated using the method ``new_edge_key()``.
+        This method can be overridden by subclassing the base class and
+        providing a custom ``new_edge_key()`` method.
+
+        Examples
+        --------
+        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
+        >>> G.add_edges_from([(0, 1), (1, 2)])  # using a list of edge tuples
+        >>> e = zip(range(0, 3), range(1, 4))
+        >>> G.add_edges_from(e)  # Add the path graph 0-1-2-3
+
+        Associate data to edges
+
+        >>> G.add_edges_from([(1, 2), (2, 3)], weight=3)
+        >>> G.add_edges_from([(3, 4), (1, 4)], label="WN2898")
+        """
+        keylist = []
+        for e in ebunch_to_add:
+            ne = len(e)
+            if ne == 4:
+                u, v, key, dd = e
+            elif ne == 3:
+                u, v, dd = e
+                key = None
+            elif ne == 2:
+                u, v = e
+                dd = {}
+                key = None
+            else:
+                msg = f"Edge tuple {e} must be a 2-tuple, 3-tuple or 4-tuple."
+                raise EasyGraphError(msg)
+            ddd = {}
+            ddd.update(attr)
+            try:
+                ddd.update(dd)
+            except (TypeError, ValueError):
+                if ne != 3:
+                    raise
+                key = dd  # ne == 3 with 3rd value not dict, must be a key
+            key = self.add_edge(u, v, key)
+            self[u][v][key].update(ddd)
+            keylist.append(key)
+        return keylist
+
+    def remove_edge(self, u, v, key=None):
+        """Remove an edge between u and v.
+
+        Parameters
+        ----------
+        u, v : nodes
+            Remove an edge between nodes u and v.
+        key : hashable identifier, optional (default=None)
+            Used to distinguish multiple edges between a pair of nodes.
+            If None remove a single (arbitrary) edge between u and v.
+
+        Raises
+        ------
+        EasyGraphError
+            If there is not an edge between u and v, or
+            if there is no edge with the specified key.
+
+        See Also
+        --------
+        remove_edges_from : remove a collection of edges
+
+        Examples
+        --------
+        For multiple edges
+
+        >>> G = eg.MultiGraph()  # or MultiDiGraph, etc
+        >>> G.add_edges_from([(1, 2), (1, 2), (1, 2)])  # key_list returned
+        [0, 1, 2]
+        >>> G.remove_edge(1, 2)  # remove a single (arbitrary) edge
+
+        For edges with keys
+
+        >>> G = eg.MultiGraph()  # or MultiDiGraph, etc
+        >>> G.add_edge(1, 2, key="first")
+        'first'
+        >>> G.add_edge(1, 2, key="second")
+        'second'
+        >>> G.remove_edge(1, 2, key="second")
+
+        """
+        try:
+            d = self._adj[u][v]
+        except KeyError as err:
+            raise EasyGraphError(f"The edge {u}-{v} is not in the graph.") from err
+        # remove the edge with specified data
+        if key is None:
+            d.popitem()
+        else:
+            try:
+                del d[key]
+            except KeyError as err:
+                msg = f"The edge {u}-{v} with key {key} is not in the graph."
+                raise EasyGraphError(msg) from err
+        if len(d) == 0:
+            # remove the key entries if last edge
+            del self._adj[u][v]
+            if u != v:  # check for selfloop
+                del self._adj[v][u]
+
+    def remove_edges_from(self, ebunch):
+        """Remove all edges specified in ebunch.
+
+        Parameters
+        ----------
+        ebunch: list or container of edge tuples
+            Each edge given in the list or container will be removed
+            from the graph. The edges can be:
+
+                - 2-tuples (u, v) All edges between u and v are removed.
+                - 3-tuples (u, v, key) The edge identified by key is removed.
+                - 4-tuples (u, v, key, data) where data is ignored.
+
+        See Also
+        --------
+        remove_edge : remove a single edge
+
+        Notes
+        -----
+        Will fail silently if an edge in ebunch is not in the graph.
+
+        Examples
+        --------
+        Removing multiple copies of edges
+
+        >>> G = eg.MultiGraph()
+        >>> keys = G.add_edges_from([(1, 2), (1, 2), (1, 2)])
+        >>> G.remove_edges_from([(1, 2), (1, 2)])
+        >>> list(G.edges())
+        [(1, 2)]
+        >>> G.remove_edges_from([(1, 2), (1, 2)])  # silently ignore extra copy
+        >>> list(G.edges)  # now empty graph
+        []
+        """
+        for e in ebunch:
+            try:
+                self.remove_edge(*e[:3])
+            except EasyGraphError:
+                pass
+
+    def has_edge(self, u, v, key=None):
+        """Returns True if the graph has an edge between nodes u and v.
+
+        This is the same as `v in G[u] or key in G[u][v]`
+        without KeyError exceptions.
+
+        Parameters
+        ----------
+        u, v : nodes
+            Nodes can be, for example, strings or numbers.
+
+        key : hashable identifier, optional (default=None)
+            If specified return True only if the edge with
+            key is found.
+
+        Returns
+        -------
+        edge_ind : bool
+            True if edge is in the graph, False otherwise.
+
+        Examples
+        --------
+        Can be called either using two nodes u, v, an edge tuple (u, v),
+        or an edge tuple (u, v, key).
+
+        >>> G = eg.MultiGraph()  # or MultiDiGraph
+        >>> G = eg.complete_graph(4, create_using=eg.MultiDiGraph)
+        >>> G.has_edge(0, 1)  # using two nodes
+        True
+        >>> e = (0, 1)
+        >>> G.has_edge(*e)  #  e is a 2-tuple (u, v)
+        True
+        >>> G.add_edge(0, 1, key="a")
+        'a'
+        >>> G.has_edge(0, 1, key="a")  # specify key
+        True
+        >>> e = (0, 1, "a")
+        >>> G.has_edge(*e)  # e is a 3-tuple (u, v, 'a')
+        True
+
+        The following syntax are equivalent:
+
+        >>> G.has_edge(0, 1)
+        True
+        >>> 1 in G[0]  # though this gives :exc:`KeyError` if 0 not in G
+        True
+
+        """
+        try:
+            if key is None:
+                return v in self._adj[u]
+            else:
+                return key in self._adj[u][v]
+        except KeyError:
+            return False
+
+    @property
+    def edges(self):
+        edges = list()
+        seen = {}
+        for n, nbrs in self._adj.items():
+            for nbr, kd in nbrs.items():
+                if nbr not in seen:
+                    for k, dd in kd.items():
+                        edges.append((n, nbr, k, dd))
+            seen[n] = 1
+        del seen
+        return edges
+
+    def get_edge_data(self, u, v, key=None, default=None):
+        """Returns the attribute dictionary associated with edge (u, v).
+
+        This is identical to `G[u][v][key]` except the default is returned
+        instead of an exception is the edge doesn't exist.
+
+        Parameters
+        ----------
+        u, v : nodes
+
+        default :  any Python object (default=None)
+            Value to return if the edge (u, v) is not found.
+
+        key : hashable identifier, optional (default=None)
+            Return data only for the edge with specified key.
+
+        Returns
+        -------
+        edge_dict : dictionary
+            The edge attribute dictionary.
+
+        Examples
+        --------
+        >>> G = eg.MultiGraph()  # or MultiDiGraph
+        >>> key = G.add_edge(0, 1, key="a", weight=7)
+        >>> G[0][1]["a"]  # key='a'
+        {'weight': 7}
+        >>> G.edges[0, 1, "a"]  # key='a'
+        {'weight': 7}
+
+        Warning: we protect the graph data structure by making
+        `G.edges` and `G[1][2]` read-only dict-like structures.
+        However, you can assign values to attributes in e.g.
+        `G.edges[1, 2, 'a']` or `G[1][2]['a']` using an additional
+        bracket as shown next. You need to specify all edge info
+        to assign to the edge data associated with an edge.
+
+        >>> G[0][1]["a"]["weight"] = 10
+        >>> G.edges[0, 1, "a"]["weight"] = 10
+        >>> G[0][1]["a"]["weight"]
+        10
+        >>> G.edges[1, 0, "a"]["weight"]
+        10
+
+        >>> G = eg.MultiGraph()  # or MultiDiGraph
+        >>> G = eg.complete_graph(4, create_using=eg.MultiDiGraph)
+        >>> G.get_edge_data(0, 1)
+        {0: {}}
+        >>> e = (0, 1)
+        >>> G.get_edge_data(*e)  # tuple form
+        {0: {}}
+        >>> G.get_edge_data("a", "b", default=0)  # edge not in graph, return 0
+        0
+        """
+        try:
+            if key is None:
+                return self._adj[u][v]
+            else:
+                return self._adj[u][v][key]
+        except KeyError:
+            return default
+
+    @property
+    def degree(self, weight="weight"):
+        degree = dict()
+        if weight is None:
+            for n in self._nodes:
+                nbrs = self._succ[n]
+                deg = sum(len(keys) for keys in nbrs.values()) + (
+                    n in nbrs and len(nbrs[n])
+                )
+                degree[n] = deg
+        else:
+            for n in self._nodes:
+                nbrs = self._succ[n]
+                deg = sum(
+                    d.get(weight, 1)
+                    for key_dict in nbrs.values()
+                    for d in key_dict.values()
+                )
+                if n in nbrs:
+                    deg += sum(d.get(weight, 1) for d in nbrs[n].values())
+                degree[n] = deg
+
+    def is_multigraph(self):
+        """Returns True if graph is a multigraph, False otherwise."""
+        return True
+
+    def is_directed(self):
+        """Returns True if graph is directed, False otherwise."""
+        return False
+
+    def copy(self):
+        """Returns a copy of the graph.
+
+        The copy method by default returns an independent shallow copy
+        of the graph and attributes. That is, if an attribute is a
+        container, that container is shared by the original an the copy.
+        Use Python's `copy.deepcopy` for new containers.
+
+        Notes
+        -----
+        All copies reproduce the graph structure, but data attributes
+        may be handled in different ways. There are four types of copies
+        of a graph that people might want.
+
+        Deepcopy -- A "deepcopy" copies the graph structure as well as
+        all data attributes and any objects they might contain.
+        The entire graph object is new so that changes in the copy
+        do not affect the original object. (see Python's copy.deepcopy)
+
+        Data Reference (Shallow) -- For a shallow copy the graph structure
+        is copied but the edge, node and graph attribute dicts are
+        references to those in the original graph. This saves
+        time and memory but could cause confusion if you change an attribute
+        in one graph and it changes the attribute in the other.
+        EasyGraph does not provide this level of shallow copy.
+
+        Independent Shallow -- This copy creates new independent attribute
+        dicts and then does a shallow copy of the attributes. That is, any
+        attributes that are containers are shared between the new graph
+        and the original. This is exactly what `dict.copy()` provides.
+        You can obtain this style copy using:
+
+            >>> G = eg.path_graph(5)
+            >>> H = G.copy()
+            >>> H = eg.Graph(G)
+            >>> H = G.__class__(G)
+
+        Fresh Data -- For fresh data, the graph structure is copied while
+        new empty data attribute dicts are created. The resulting graph
+        is independent of the original and it has no edge, node or graph
+        attributes. Fresh copies are not enabled. Instead use:
+
+            >>> H = G.__class__()
+            >>> H.add_nodes_from(G)
+            >>> H.add_edges_from(G.edges)
+
+        See the Python copy module for more information on shallow
+        and deep copies, https://docs.python.org/3/library/copy.html.
+
+        Returns
+        -------
+        G : Graph
+            A copy of the graph.
+
+        See Also
+        --------
+        to_directed: return a directed copy of the graph.
+
+        Examples
+        --------
+        >>> G = eg.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc
+        >>> H = G.copy()
+
+        """
+        G = self.__class__()
+        G.graph.update(self.graph)
+        G.add_nodes_from((n, d.copy()) for n, d in self._node.items())
+        G.add_edges_from(
+            (u, v, key, datadict.copy())
+            for u, nbrs in self._adj.items()
+            for v, keydict in nbrs.items()
+            for key, datadict in keydict.items()
+        )
+        return G
+
+    def to_directed(self):
+        """Returns a directed representation of the graph.
+
+        Returns
+        -------
+        G : MultiDiGraph
+            A directed graph with the same name, same nodes, and with
+            each edge (u, v, data) replaced by two directed edges
+            (u, v, data) and (v, u, data).
+
+        Notes
+        -----
+        This returns a "deepcopy" of the edge, node, and
+        graph attributes which attempts to completely copy
+        all of the data and references.
+
+        This is in contrast to the similar D=DiGraph(G) which returns a
+        shallow copy of the data.
+
+        See the Python copy module for more information on shallow
+        and deep copies, https://docs.python.org/3/library/copy.html.
+
+        Warning: If you have subclassed MultiGraph to use dict-like objects
+        in the data structure, those changes do not transfer to the
+        MultiDiGraph created by this method.
+
+        Examples
+        --------
+        >>> G = eg.Graph()  # or MultiGraph, etc
+        >>> G.add_edge(0, 1)
+        >>> H = G.to_directed()
+        >>> list(H.edges)
+        [(0, 1), (1, 0)]
+
+        If already directed, return a (deep) copy
+
+        >>> G = eg.DiGraph()  # or MultiDiGraph, etc
+        >>> G.add_edge(0, 1)
+        >>> H = G.to_directed()
+        >>> list(H.edges)
+        [(0, 1)]
+        """
+        G = eg.MultiDiGraph()
+        G.graph.update(deepcopy(self.graph))
+        G.add_nodes_from((n, deepcopy(d)) for n, d in self._node.items())
+        G.add_edges_from(
+            (u, v, key, deepcopy(datadict))
+            for u, nbrs in self.adj.items()
+            for v, keydict in nbrs.items()
+            for key, datadict in keydict.items()
+        )
+        return G
+
+    def number_of_edges(self, u=None, v=None):
+        """Returns the number of edges between two nodes.
+
+        Parameters
+        ----------
+        u, v : nodes, optional (Gefault=all edges)
+            If u and v are specified, return the number of edges between
+            u and v. Otherwise return the total number of all edges.
+
+        Returns
+        -------
+        nedges : int
+            The number of edges in the graph.  If nodes `u` and `v` are
+            specified return the number of edges between those nodes. If
+            the graph is directed, this only returns the number of edges
+            from `u` to `v`.
+
+        See Also
+        --------
+        size
+
+        Examples
+        --------
+        For undirected multigraphs, this method counts the total number
+        of edges in the graph::
+
+            >>> G = eg.MultiGraph()
+            >>> G.add_edges_from([(0, 1), (0, 1), (1, 2)])
+            [0, 1, 0]
+            >>> G.number_of_edges()
+            3
+
+        If you specify two nodes, this counts the total number of edges
+        joining the two nodes::
+
+            >>> G.number_of_edges(0, 1)
+            2
+
+        For directed multigraphs, this method can count the total number
+        of directed edges from `u` to `v`::
+
+            >>> G = eg.MultiDiGraph()
+            >>> G.add_edges_from([(0, 1), (0, 1), (1, 0)])
+            [0, 1, 0]
+            >>> G.number_of_edges(0, 1)
+            2
+            >>> G.number_of_edges(1, 0)
+            1
+
+        """
+        if u is None:
+            return self.size()
+        try:
+            edgedata = self._adj[u][v]
+        except KeyError:
+            return 0  # no such edge
+        return len(edgedata)
```

## easygraph/classes/directed_multigraph.py

 * *Ordering differences only*

```diff
@@ -1,417 +1,417 @@
-from copy import deepcopy
-from typing import Dict
-from typing import List
-
-import easygraph as eg
-import easygraph.convert as convert
-
-from easygraph.classes.directed_graph import DiGraph
-from easygraph.classes.multigraph import MultiGraph
-from easygraph.utils.exception import EasyGraphError
-
-
-__all__ = ["MultiDiGraph"]
-
-
-class MultiDiGraph(MultiGraph, DiGraph):
-    edge_key_dict_factory = dict
-
-    def __init__(self, incoming_graph_data=None, multigraph_input=None, **attr):
-        """Initialize a graph with edges, name, or graph attributes.
-
-        Parameters
-        ----------
-        incoming_graph_data : input graph
-            Data to initialize graph.  If incoming_graph_data=None (default)
-            an empty graph is created.  The data can be an edge list, or any
-            EasyGraph graph object.  If the corresponding optional Python
-            packages are installed the data can also be a NumPy matrix
-            or 2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.
-
-        multigraph_input : bool or None (default None)
-            Note: Only used when `incoming_graph_data` is a dict.
-            If True, `incoming_graph_data` is assumed to be a
-            dict-of-dict-of-dict-of-dict structure keyed by
-            node to neighbor to edge keys to edge data for multi-edges.
-            A EasyGraphError is raised if this is not the case.
-            If False, :func:`to_easygraph_graph` is used to try to determine
-            the dict's graph data structure as either a dict-of-dict-of-dict
-            keyed by node to neighbor to edge data, or a dict-of-iterable
-            keyed by node to neighbors.
-            If None, the treatment for True is tried, but if it fails,
-            the treatment for False is tried.
-
-        attr : keyword arguments, optional (default= no attributes)
-            Attributes to add to graph as key=value pairs.
-
-        See Also
-        --------
-        convert
-
-        Examples
-        --------
-        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
-        >>> G = eg.Graph(name="my graph")
-        >>> e = [(1, 2), (2, 3), (3, 4)]  # list of edges
-        >>> G = eg.Graph(e)
-
-        Arbitrary graph attribute pairs (key=value) may be assigned
-
-        >>> G = eg.Graph(e, day="Friday")
-        >>> G.graph
-        {'day': 'Friday'}
-
-        """
-        self.edge_key_dict_factory = self.edge_key_dict_factory
-        # multigraph_input can be None/True/False. So check "is not False"
-        if isinstance(incoming_graph_data, dict) and multigraph_input is not False:
-            DiGraph.__init__(self)
-            try:
-                convert.from_dict_of_dicts(
-                    incoming_graph_data, create_using=self, multigraph_input=True
-                )
-                self.graph.update(attr)
-            except Exception as err:
-                if multigraph_input is True:
-                    raise EasyGraphError(
-                        f"converting multigraph_input raised:\n{type(err)}: {err}"
-                    )
-                DiGraph.__init__(self, incoming_graph_data, **attr)
-        else:
-            DiGraph.__init__(self, incoming_graph_data, **attr)
-
-    def add_edge(self, u_for_edge, v_for_edge, key=None, **attr):
-        """Add an edge between u and v.
-
-        The nodes u and v will be automatically added if they are
-        not already in the graph.
-
-        Edge attributes can be specified with keywords or by directly
-        accessing the edge's attribute dictionary. See examples below.
-
-        Parameters
-        ----------
-        u_for_edge, v_for_edge : nodes
-            Nodes can be, for example, strings or numbers.
-            Nodes must be hashable (and not None) Python objects.
-        key : hashable identifier, optional (default=lowest unused integer)
-            Used to distinguish multiedges between a pair of nodes.
-        attr : keyword arguments, optional
-            Edge data (or labels or objects) can be assigned using
-            keyword arguments.
-
-        Returns
-        -------
-        The edge key assigned to the edge.
-
-        See Also
-        --------
-        add_edges_from : add a collection of edges
-
-        Notes
-        -----
-        To replace/update edge data, use the optional key argument
-        to identify a unique edge.  Otherwise a new edge will be created.
-
-        EasyGraph algorithms designed for weighted graphs cannot use
-        multigraphs directly because it is not clear how to handle
-        multiedge weights.  Convert to Graph using edge attribute
-        'weight' to enable weighted graph algorithms.
-
-        Default keys are generated using the method `new_edge_key()`.
-        This method can be overridden by subclassing the base class and
-        providing a custom `new_edge_key()` method.
-
-        Examples
-        --------
-        The following all add the edge e=(1, 2) to graph G:
-
-        >>> G = eg.MultiDiGraph()
-        >>> e = (1, 2)
-        >>> key = G.add_edge(1, 2)  # explicit two-node form
-        >>> G.add_edge(*e)  # single edge as tuple of two nodes
-        1
-        >>> G.add_edges_from([(1, 2)])  # add edges from iterable container
-        [2]
-
-        Associate data to edges using keywords:
-
-        >>> key = G.add_edge(1, 2, weight=3)
-        >>> key = G.add_edge(1, 2, key=0, weight=4)  # update data for key=0
-        >>> key = G.add_edge(1, 3, weight=7, capacity=15, length=342.7)
-
-        For non-string attribute keys, use subscript notation.
-
-        >>> ekey = G.add_edge(1, 2)
-        >>> G[1][2][0].update({0: 5})
-        >>> G.edges[1, 2, 0].update({0: 5})
-        """
-        u, v = u_for_edge, v_for_edge
-        # add nodes
-        if u not in self._adj:
-            if u is None:
-                raise ValueError("None cannot be a node")
-            self._adj[u] = self.adjlist_inner_dict_factory()
-            self._pred[u] = self.adjlist_inner_dict_factory()
-            self._node[u] = self.node_attr_dict_factory()
-        if v not in self._adj:
-            if v is None:
-                raise ValueError("None cannot be a node")
-            self._adj[v] = self.adjlist_inner_dict_factory()
-            self._pred[v] = self.adjlist_inner_dict_factory()
-            self._node[v] = self.node_attr_dict_factory()
-        if key is None:
-            key = self.new_edge_key(u, v)
-        if v in self._adj[u]:
-            keydict = self._adj[u][v]
-            datadict = keydict.get(key, self.edge_key_dict_factory())
-            datadict.update(attr)
-            keydict[key] = datadict
-        else:
-            # selfloops work this way without special treatment
-            datadict = self.edge_attr_dict_factory()
-            datadict.update(attr)
-            keydict = self.edge_key_dict_factory()
-            keydict[key] = datadict
-            self._adj[u][v] = keydict
-            self._pred[v][u] = keydict
-        return key
-
-    def remove_edge(self, u, v, key=None):
-        """Remove an edge between u and v.
-
-        Parameters
-        ----------
-        u, v : nodes
-            Remove an edge between nodes u and v.
-        key : hashable identifier, optional (default=None)
-            Used to distinguish multiple edges between a pair of nodes.
-            If None remove a single (arbitrary) edge between u and v.
-
-        Raises
-        ------
-        EasyGraphError
-            If there is not an edge between u and v, or
-            if there is no edge with the specified key.
-
-        See Also
-        --------
-        remove_edges_from : remove a collection of edges
-
-        Examples
-        --------
-        >>> G = eg.MultiDiGraph()
-        >>> G.add_edges_from([(1, 2), (1, 2), (1, 2)])  # key_list returned
-        [0, 1, 2]
-        >>> G.remove_edge(1, 2)  # remove a single (arbitrary) edge
-
-        For edges with keys
-
-        >>> G = eg.MultiDiGraph()
-        >>> G.add_edge(1, 2, key="first")
-        'first'
-        >>> G.add_edge(1, 2, key="second")
-        'second'
-        >>> G.remove_edge(1, 2, key="second")
-
-        """
-        try:
-            d = self._adj[u][v]
-        except KeyError as err:
-            raise EasyGraphError(f"The edge {u}-{v} is not in the graph.") from err
-        # remove the edge with specified data
-        if key is None:
-            d.popitem()
-        else:
-            try:
-                del d[key]
-            except KeyError as err:
-                msg = f"The edge {u}-{v} with key {key} is not in the graph."
-                raise EasyGraphError(msg) from err
-        if len(d) == 0:
-            # remove the key entries if last edge
-            del self._adj[u][v]
-            del self._pred[v][u]
-
-    @property
-    def edges(self):
-        edges = list()
-        for n, nbrs in self._adj.items():
-            for nbr, kd in nbrs.items():
-                for k, dd in kd.items():
-                    edges.append((n, nbr, k, dd))
-        return edges
-
-    out_edges = edges
-
-    @property
-    def in_edges(self):
-        edges = list()
-        for n, nbrs in self._adj.items():
-            for nbr, kd in nbrs.items():
-                for k, dd in kd.items():
-                    edges.append((nbr, n, k))
-        return edges
-
-    @property
-    def degree(self, weight="weight"):
-        degree = dict()
-        if weight is None:
-            for n in self._nodes:
-                succs = self._adj[n]
-                preds = self._pred[n]
-                deg = sum(len(keys) for keys in succs.values()) + sum(
-                    len(keys) for keys in preds.values()
-                )
-                degree[n] = deg
-        else:
-            for n in self._nodes:
-                succs = self._adj[n]
-                preds = self._pred[n]
-                deg = sum(
-                    d.get(weight, 1)
-                    for key_dict in succs.values()
-                    for d in key_dict.values()
-                ) + sum(
-                    d.get(weight, 1)
-                    for key_dict in preds.values()
-                    for d in key_dict.values()
-                )
-                degree[n] = deg
-
-    @property
-    def in_degree(self, weight="weight"):
-        degree = dict()
-        if weight is None:
-            for n in self._nodes:
-                preds = self._pred[n]
-                deg = sum(len(keys) for keys in preds.values())
-                degree[n] = deg
-        else:
-            for n in self._nodes:
-                preds = self._pred[n]
-                deg = sum(
-                    d.get(weight, 1)
-                    for key_dict in preds.values()
-                    for d in key_dict.values()
-                )
-                degree[n] = deg
-
-    @property
-    def out_degree(self, weight="weight"):
-        degree = dict()
-        if weight is None:
-            for n in self._nodes:
-                succs = self._adj[n]
-                deg = sum(len(keys) for keys in succs.values())
-                degree[n] = deg
-        else:
-            for n in self._nodes:
-                succs = self._adj[n]
-                deg = sum(
-                    d.get(weight, 1)
-                    for key_dict in succs.values()
-                    for d in key_dict.values()
-                )
-                degree[n] = deg
-
-    def is_multigraph(self):
-        """Returns True if graph is a multigraph, False otherwise."""
-        return True
-
-    def is_directed(self):
-        """Returns True if graph is directed, False otherwise."""
-        return True
-
-    def to_undirected(self, reciprocal=False):
-        """Returns an undirected representation of the multidigraph.
-
-        Parameters
-        ----------
-        reciprocal : bool (optional)
-          If True only keep edges that appear in both directions
-          in the original digraph.
-
-        Returns
-        -------
-        G : MultiGraph
-            An undirected graph with the same name and nodes and
-            with edge (u, v, data) if either (u, v, data) or (v, u, data)
-            is in the digraph.  If both edges exist in digraph and
-            their edge data is different, only one edge is created
-            with an arbitrary choice of which edge data to use.
-            You must check and correct for this manually if desired.
-
-        See Also
-        --------
-        MultiGraph, add_edge, add_edges_from
-
-        Notes
-        -----
-        This returns a "deepcopy" of the edge, node, and
-        graph attributes which attempts to completely copy
-        all of the data and references.
-
-        This is in contrast to the similar D=MultiDiGraph(G) which
-        returns a shallow copy of the data.
-
-        See the Python copy module for more information on shallow
-        and deep copies, https://docs.python.org/3/library/copy.html.
-
-        Warning: If you have subclassed MultiDiGraph to use dict-like
-        objects in the data structure, those changes do not transfer
-        to the MultiGraph created by this method.
-
-        Examples
-        --------
-        >>> G = eg.path_graph(2)  # or MultiGraph, etc
-        >>> H = G.to_directed()
-        >>> list(H.edges)
-        [(0, 1), (1, 0)]
-        >>> G2 = H.to_undirected()
-        >>> list(G2.edges)
-        [(0, 1)]
-        """
-        G = eg.MultiGraph()
-        G.graph.update(deepcopy(self.graph))
-        G.add_nodes_from((n, deepcopy(d)) for n, d in self._node.items())
-        if reciprocal is True:
-            G.add_edges_from(
-                (u, v, key, deepcopy(data))
-                for u, nbrs in self._adj.items()
-                for v, keydict in nbrs.items()
-                for key, data in keydict.items()
-                if v in self._pred[u] and key in self._pred[u][v]
-            )
-        else:
-            G.add_edges_from(
-                (u, v, key, deepcopy(data))
-                for u, nbrs in self._adj.items()
-                for v, keydict in nbrs.items()
-                for key, data in keydict.items()
-            )
-        return G
-
-    def reverse(self, copy=True):
-        """Returns the reverse of the graph.
-
-        The reverse is a graph with the same nodes and edges
-        but with the directions of the edges reversed.
-
-        Parameters
-        ----------
-        copy : bool optional (default=True)
-            If True, return a new DiGraph holding the reversed edges.
-            If False, the reverse graph is created using a view of
-            the original graph.
-        """
-        if copy:
-            H = self.__class__()
-            H.graph.update(deepcopy(self.graph))
-            H.add_nodes_from((n, deepcopy(d)) for n, d in self._node.items())
-            H.add_edges_from(
-                (v, u, k, deepcopy(d))
-                for u, v, k, d in self.edges(keys=True, data=True)
-            )
-            return H
-        return eg.graphviews.reverse_view(self)
+from copy import deepcopy
+from typing import Dict
+from typing import List
+
+import easygraph as eg
+import easygraph.convert as convert
+
+from easygraph.classes.directed_graph import DiGraph
+from easygraph.classes.multigraph import MultiGraph
+from easygraph.utils.exception import EasyGraphError
+
+
+__all__ = ["MultiDiGraph"]
+
+
+class MultiDiGraph(MultiGraph, DiGraph):
+    edge_key_dict_factory = dict
+
+    def __init__(self, incoming_graph_data=None, multigraph_input=None, **attr):
+        """Initialize a graph with edges, name, or graph attributes.
+
+        Parameters
+        ----------
+        incoming_graph_data : input graph
+            Data to initialize graph.  If incoming_graph_data=None (default)
+            an empty graph is created.  The data can be an edge list, or any
+            EasyGraph graph object.  If the corresponding optional Python
+            packages are installed the data can also be a NumPy matrix
+            or 2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.
+
+        multigraph_input : bool or None (default None)
+            Note: Only used when `incoming_graph_data` is a dict.
+            If True, `incoming_graph_data` is assumed to be a
+            dict-of-dict-of-dict-of-dict structure keyed by
+            node to neighbor to edge keys to edge data for multi-edges.
+            A EasyGraphError is raised if this is not the case.
+            If False, :func:`to_easygraph_graph` is used to try to determine
+            the dict's graph data structure as either a dict-of-dict-of-dict
+            keyed by node to neighbor to edge data, or a dict-of-iterable
+            keyed by node to neighbors.
+            If None, the treatment for True is tried, but if it fails,
+            the treatment for False is tried.
+
+        attr : keyword arguments, optional (default= no attributes)
+            Attributes to add to graph as key=value pairs.
+
+        See Also
+        --------
+        convert
+
+        Examples
+        --------
+        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
+        >>> G = eg.Graph(name="my graph")
+        >>> e = [(1, 2), (2, 3), (3, 4)]  # list of edges
+        >>> G = eg.Graph(e)
+
+        Arbitrary graph attribute pairs (key=value) may be assigned
+
+        >>> G = eg.Graph(e, day="Friday")
+        >>> G.graph
+        {'day': 'Friday'}
+
+        """
+        self.edge_key_dict_factory = self.edge_key_dict_factory
+        # multigraph_input can be None/True/False. So check "is not False"
+        if isinstance(incoming_graph_data, dict) and multigraph_input is not False:
+            DiGraph.__init__(self)
+            try:
+                convert.from_dict_of_dicts(
+                    incoming_graph_data, create_using=self, multigraph_input=True
+                )
+                self.graph.update(attr)
+            except Exception as err:
+                if multigraph_input is True:
+                    raise EasyGraphError(
+                        f"converting multigraph_input raised:\n{type(err)}: {err}"
+                    )
+                DiGraph.__init__(self, incoming_graph_data, **attr)
+        else:
+            DiGraph.__init__(self, incoming_graph_data, **attr)
+
+    def add_edge(self, u_for_edge, v_for_edge, key=None, **attr):
+        """Add an edge between u and v.
+
+        The nodes u and v will be automatically added if they are
+        not already in the graph.
+
+        Edge attributes can be specified with keywords or by directly
+        accessing the edge's attribute dictionary. See examples below.
+
+        Parameters
+        ----------
+        u_for_edge, v_for_edge : nodes
+            Nodes can be, for example, strings or numbers.
+            Nodes must be hashable (and not None) Python objects.
+        key : hashable identifier, optional (default=lowest unused integer)
+            Used to distinguish multiedges between a pair of nodes.
+        attr : keyword arguments, optional
+            Edge data (or labels or objects) can be assigned using
+            keyword arguments.
+
+        Returns
+        -------
+        The edge key assigned to the edge.
+
+        See Also
+        --------
+        add_edges_from : add a collection of edges
+
+        Notes
+        -----
+        To replace/update edge data, use the optional key argument
+        to identify a unique edge.  Otherwise a new edge will be created.
+
+        EasyGraph algorithms designed for weighted graphs cannot use
+        multigraphs directly because it is not clear how to handle
+        multiedge weights.  Convert to Graph using edge attribute
+        'weight' to enable weighted graph algorithms.
+
+        Default keys are generated using the method `new_edge_key()`.
+        This method can be overridden by subclassing the base class and
+        providing a custom `new_edge_key()` method.
+
+        Examples
+        --------
+        The following all add the edge e=(1, 2) to graph G:
+
+        >>> G = eg.MultiDiGraph()
+        >>> e = (1, 2)
+        >>> key = G.add_edge(1, 2)  # explicit two-node form
+        >>> G.add_edge(*e)  # single edge as tuple of two nodes
+        1
+        >>> G.add_edges_from([(1, 2)])  # add edges from iterable container
+        [2]
+
+        Associate data to edges using keywords:
+
+        >>> key = G.add_edge(1, 2, weight=3)
+        >>> key = G.add_edge(1, 2, key=0, weight=4)  # update data for key=0
+        >>> key = G.add_edge(1, 3, weight=7, capacity=15, length=342.7)
+
+        For non-string attribute keys, use subscript notation.
+
+        >>> ekey = G.add_edge(1, 2)
+        >>> G[1][2][0].update({0: 5})
+        >>> G.edges[1, 2, 0].update({0: 5})
+        """
+        u, v = u_for_edge, v_for_edge
+        # add nodes
+        if u not in self._adj:
+            if u is None:
+                raise ValueError("None cannot be a node")
+            self._adj[u] = self.adjlist_inner_dict_factory()
+            self._pred[u] = self.adjlist_inner_dict_factory()
+            self._node[u] = self.node_attr_dict_factory()
+        if v not in self._adj:
+            if v is None:
+                raise ValueError("None cannot be a node")
+            self._adj[v] = self.adjlist_inner_dict_factory()
+            self._pred[v] = self.adjlist_inner_dict_factory()
+            self._node[v] = self.node_attr_dict_factory()
+        if key is None:
+            key = self.new_edge_key(u, v)
+        if v in self._adj[u]:
+            keydict = self._adj[u][v]
+            datadict = keydict.get(key, self.edge_key_dict_factory())
+            datadict.update(attr)
+            keydict[key] = datadict
+        else:
+            # selfloops work this way without special treatment
+            datadict = self.edge_attr_dict_factory()
+            datadict.update(attr)
+            keydict = self.edge_key_dict_factory()
+            keydict[key] = datadict
+            self._adj[u][v] = keydict
+            self._pred[v][u] = keydict
+        return key
+
+    def remove_edge(self, u, v, key=None):
+        """Remove an edge between u and v.
+
+        Parameters
+        ----------
+        u, v : nodes
+            Remove an edge between nodes u and v.
+        key : hashable identifier, optional (default=None)
+            Used to distinguish multiple edges between a pair of nodes.
+            If None remove a single (arbitrary) edge between u and v.
+
+        Raises
+        ------
+        EasyGraphError
+            If there is not an edge between u and v, or
+            if there is no edge with the specified key.
+
+        See Also
+        --------
+        remove_edges_from : remove a collection of edges
+
+        Examples
+        --------
+        >>> G = eg.MultiDiGraph()
+        >>> G.add_edges_from([(1, 2), (1, 2), (1, 2)])  # key_list returned
+        [0, 1, 2]
+        >>> G.remove_edge(1, 2)  # remove a single (arbitrary) edge
+
+        For edges with keys
+
+        >>> G = eg.MultiDiGraph()
+        >>> G.add_edge(1, 2, key="first")
+        'first'
+        >>> G.add_edge(1, 2, key="second")
+        'second'
+        >>> G.remove_edge(1, 2, key="second")
+
+        """
+        try:
+            d = self._adj[u][v]
+        except KeyError as err:
+            raise EasyGraphError(f"The edge {u}-{v} is not in the graph.") from err
+        # remove the edge with specified data
+        if key is None:
+            d.popitem()
+        else:
+            try:
+                del d[key]
+            except KeyError as err:
+                msg = f"The edge {u}-{v} with key {key} is not in the graph."
+                raise EasyGraphError(msg) from err
+        if len(d) == 0:
+            # remove the key entries if last edge
+            del self._adj[u][v]
+            del self._pred[v][u]
+
+    @property
+    def edges(self):
+        edges = list()
+        for n, nbrs in self._adj.items():
+            for nbr, kd in nbrs.items():
+                for k, dd in kd.items():
+                    edges.append((n, nbr, k, dd))
+        return edges
+
+    out_edges = edges
+
+    @property
+    def in_edges(self):
+        edges = list()
+        for n, nbrs in self._adj.items():
+            for nbr, kd in nbrs.items():
+                for k, dd in kd.items():
+                    edges.append((nbr, n, k))
+        return edges
+
+    @property
+    def degree(self, weight="weight"):
+        degree = dict()
+        if weight is None:
+            for n in self._nodes:
+                succs = self._adj[n]
+                preds = self._pred[n]
+                deg = sum(len(keys) for keys in succs.values()) + sum(
+                    len(keys) for keys in preds.values()
+                )
+                degree[n] = deg
+        else:
+            for n in self._nodes:
+                succs = self._adj[n]
+                preds = self._pred[n]
+                deg = sum(
+                    d.get(weight, 1)
+                    for key_dict in succs.values()
+                    for d in key_dict.values()
+                ) + sum(
+                    d.get(weight, 1)
+                    for key_dict in preds.values()
+                    for d in key_dict.values()
+                )
+                degree[n] = deg
+
+    @property
+    def in_degree(self, weight="weight"):
+        degree = dict()
+        if weight is None:
+            for n in self._nodes:
+                preds = self._pred[n]
+                deg = sum(len(keys) for keys in preds.values())
+                degree[n] = deg
+        else:
+            for n in self._nodes:
+                preds = self._pred[n]
+                deg = sum(
+                    d.get(weight, 1)
+                    for key_dict in preds.values()
+                    for d in key_dict.values()
+                )
+                degree[n] = deg
+
+    @property
+    def out_degree(self, weight="weight"):
+        degree = dict()
+        if weight is None:
+            for n in self._nodes:
+                succs = self._adj[n]
+                deg = sum(len(keys) for keys in succs.values())
+                degree[n] = deg
+        else:
+            for n in self._nodes:
+                succs = self._adj[n]
+                deg = sum(
+                    d.get(weight, 1)
+                    for key_dict in succs.values()
+                    for d in key_dict.values()
+                )
+                degree[n] = deg
+
+    def is_multigraph(self):
+        """Returns True if graph is a multigraph, False otherwise."""
+        return True
+
+    def is_directed(self):
+        """Returns True if graph is directed, False otherwise."""
+        return True
+
+    def to_undirected(self, reciprocal=False):
+        """Returns an undirected representation of the multidigraph.
+
+        Parameters
+        ----------
+        reciprocal : bool (optional)
+          If True only keep edges that appear in both directions
+          in the original digraph.
+
+        Returns
+        -------
+        G : MultiGraph
+            An undirected graph with the same name and nodes and
+            with edge (u, v, data) if either (u, v, data) or (v, u, data)
+            is in the digraph.  If both edges exist in digraph and
+            their edge data is different, only one edge is created
+            with an arbitrary choice of which edge data to use.
+            You must check and correct for this manually if desired.
+
+        See Also
+        --------
+        MultiGraph, add_edge, add_edges_from
+
+        Notes
+        -----
+        This returns a "deepcopy" of the edge, node, and
+        graph attributes which attempts to completely copy
+        all of the data and references.
+
+        This is in contrast to the similar D=MultiDiGraph(G) which
+        returns a shallow copy of the data.
+
+        See the Python copy module for more information on shallow
+        and deep copies, https://docs.python.org/3/library/copy.html.
+
+        Warning: If you have subclassed MultiDiGraph to use dict-like
+        objects in the data structure, those changes do not transfer
+        to the MultiGraph created by this method.
+
+        Examples
+        --------
+        >>> G = eg.path_graph(2)  # or MultiGraph, etc
+        >>> H = G.to_directed()
+        >>> list(H.edges)
+        [(0, 1), (1, 0)]
+        >>> G2 = H.to_undirected()
+        >>> list(G2.edges)
+        [(0, 1)]
+        """
+        G = eg.MultiGraph()
+        G.graph.update(deepcopy(self.graph))
+        G.add_nodes_from((n, deepcopy(d)) for n, d in self._node.items())
+        if reciprocal is True:
+            G.add_edges_from(
+                (u, v, key, deepcopy(data))
+                for u, nbrs in self._adj.items()
+                for v, keydict in nbrs.items()
+                for key, data in keydict.items()
+                if v in self._pred[u] and key in self._pred[u][v]
+            )
+        else:
+            G.add_edges_from(
+                (u, v, key, deepcopy(data))
+                for u, nbrs in self._adj.items()
+                for v, keydict in nbrs.items()
+                for key, data in keydict.items()
+            )
+        return G
+
+    def reverse(self, copy=True):
+        """Returns the reverse of the graph.
+
+        The reverse is a graph with the same nodes and edges
+        but with the directions of the edges reversed.
+
+        Parameters
+        ----------
+        copy : bool optional (default=True)
+            If True, return a new DiGraph holding the reversed edges.
+            If False, the reverse graph is created using a view of
+            the original graph.
+        """
+        if copy:
+            H = self.__class__()
+            H.graph.update(deepcopy(self.graph))
+            H.add_nodes_from((n, deepcopy(d)) for n, d in self._node.items())
+            H.add_edges_from(
+                (v, u, k, deepcopy(d))
+                for u, v, k, d in self.edges(keys=True, data=True)
+            )
+            return H
+        return eg.graphviews.reverse_view(self)
```

## easygraph/classes/directed_graph.py

```diff
@@ -1,1286 +1,1315 @@
-from typing import Dict
-from typing import List
-
-import easygraph
-import easygraph.convert as convert
-
-from easygraph.classes.graph import Graph
-from easygraph.utils.exception import EasyGraphError
-
-
-class DiGraph(Graph):
-    """
-    Base class for directed graphs.
-
-        Nodes are allowed for any hashable Python objects, including int, string, dict, etc.
-        Edges are stored as Python dict type, with optional key/value attributes.
-
-    Parameters
-    ----------
-    graph_attr : keywords arguments, optional (default : None)
-        Attributes to add to graph as key=value pairs.
-
-    See Also
-    --------
-    Graph
-
-    Examples
-    --------
-    Create an empty directed graph with no nodes and edges.
-
-    >>> G = eg.DiGraph()
-
-    Create a deep copy graph *G2* from existing Graph *G1*.
-
-    >>> G2 = G1.copy()
-
-    Create an graph with attributes.
-
-    >>> G = eg.DiGraph(name='Karate Club', date='2020.08.21')
-
-    **Attributes:**
-
-    Returns the adjacency matrix of the graph.
-
-    >>> G.adj
-
-    Returns all the nodes with their attributes.
-
-    >>> G.nodes
-
-    Returns all the edges with their attributes.
-
-    >>> G.edges
-
-    """
-
-    gnn_data_dict_factory = dict
-    graph_attr_dict_factory = dict
-    node_dict_factory = dict
-    node_attr_dict_factory = dict
-    adjlist_outer_dict_factory = dict
-    adjlist_inner_dict_factory = dict
-    edge_attr_dict_factory = dict
-    node_index_dict = dict
-
-    def __init__(self, incoming_graph_data=None, **graph_attr):
-        self.graph = self.graph_attr_dict_factory()
-        self._ndata = self.gnn_data_dict_factory()
-        self._node = self.node_dict_factory()
-        self._adj = self.adjlist_outer_dict_factory()
-        self._pred = self.adjlist_outer_dict_factory()
-        self._node_index = self.node_index_dict()
-        self._id = 0
-        self.cflag = 0
-        if incoming_graph_data is not None:
-            convert.to_easygraph_graph(incoming_graph_data, create_using=self)
-        self.graph.update(graph_attr)
-
-    def __iter__(self):
-        return iter(self._node)
-
-    def __len__(self):
-        return len(self._node)
-
-    def __contains__(self, node):
-        try:
-            return node in self._node
-        except TypeError:
-            return False
-
-    def __getitem__(self, node):
-        # return list(self._adj[node].keys())
-        return self._adj[node]
-
-    @property
-    def node_index(self):
-        return self._node_index
-
-    @property
-    def ndata(self):
-        return self._ndata
-
-    @property
-    def pred(self):
-        """
-        Return the pred of each node
-        """
-        return self._pred
-
-    @property
-    def adj(self):
-        """
-        Return the adjacency matrix
-        """
-        return self._adj
-
-    @property
-    def nodes(self):
-        """
-        return [node for node in self._node]
-        """
-        return self._node
-
-    @property
-    def edges(self):
-        """
-        Return an edge list
-        """
-        edges = list()
-        for u in self._adj:
-            for v in self._adj[u]:
-                edges.append((u, v, self._adj[u][v]))
-        return edges
-
-    @property
-    def name(self):
-        """String identifier of the graph.
-
-        This graph attribute appears in the attribute dict G.graph
-        keyed by the string `"name"`. as well as an attribute (technically
-        a property) `G.name`. This is entirely user controlled.
-        """
-        return self.graph.get("name", "")
-
-    @name.setter
-    def name(self, s):
-        """
-        Set graph name
-
-        Parameters
-        ----------
-        s : name
-        """
-        self.graph["name"] = s
-
-    def out_degree(self, weight="weight"):
-        """Returns the weighted out degree of each node.
-
-        Parameters
-        ----------
-        weight : string, optional (default : 'weight')
-            Weight key of the original weighted graph.
-
-        Returns
-        -------
-        out_degree : dict
-            Each node's (key) weighted out degree (value).
-
-        Notes
-        -----
-        If the graph is not weighted, all the weights will be regarded as 1.
-
-        See Also
-        --------
-        in_degree
-        degree
-
-        Examples
-        --------
-
-        >>> G.out_degree(weight='weight')
-
-        """
-        degree = dict()
-        for u, v, d in self.edges:
-            if u in degree:
-                degree[u] += d.get(weight, 1)
-            else:
-                degree[u] = d.get(weight, 1)
-
-        # For isolated nodes
-        for node in self.nodes:
-            if node not in degree:
-                degree[node] = 0
-
-        return degree
-
-    def in_degree(self, weight="weight"):
-        """Returns the weighted in degree of each node.
-
-        Parameters
-        ----------
-        weight : string, optional (default : 'weight')
-            Weight key of the original weighted graph.
-
-        Returns
-        -------
-        in_degree : dict
-            Each node's (key) weighted in degree (value).
-
-        Notes
-        -----
-        If the graph is not weighted, all the weights will be regarded as 1.
-
-        See Also
-        --------
-        out_degree
-        degree
-
-        Examples
-        --------
-
-        >>> G.in_degree(weight='weight')
-
-        """
-        degree = dict()
-        for u, v, d in self.edges:
-            if v in degree:
-                degree[v] += d.get(weight, 1)
-            else:
-                degree[v] = d.get(weight, 1)
-
-        # For isolated nodes
-        for node in self.nodes:
-            if node not in degree:
-                degree[node] = 0
-
-        return degree
-
-    def degree(self, weight="weight"):
-        """Returns the weighted degree of each node, i.e. sum of out/in degree.
-
-        Parameters
-        ----------
-        weight : string, optional (default : 'weight')
-            Weight key of the original weighted graph.
-
-        Returns
-        -------
-        degree : dict
-            Each node's (key) weighted in degree (value).
-            For directed graph, it returns the sum of out degree and in degree.
-
-        Notes
-        -----
-        If the graph is not weighted, all the weights will be regarded as 1.
-
-        See also
-        --------
-        out_degree
-        in_degree
-
-        Examples
-        --------
-
-        >>> G.degree()
-        >>> G.degree(weight='weight')
-
-        or you can customize the weight key
-
-        >>> G.degree(weight='weight_1')
-
-        """
-        degree = dict()
-        outdegree = self.out_degree(weight=weight)
-        indegree = self.in_degree(weight=weight)
-        for u in outdegree:
-            degree[u] = outdegree[u] + indegree[u]
-        return degree
-
-    def size(self, weight=None):
-        """Returns the number of edges or total of all edge weights.
-
-        Parameters
-        -----------
-        weight : String or None, optional
-            The weight key. If None, it will calculate the number of
-            edges, instead of total of all edge weights.
-
-        Returns
-        -------
-        size : int or float, optional (default: None)
-            The number of edges or total of all edge weights.
-
-        Examples
-        --------
-
-        Returns the number of edges in G:
-
-        >>> G.size()
-
-        Returns the total of all edge weights in G:
-
-        >>> G.size(weight='weight')
-
-        """
-        s = sum(d for v, d in self.out_degree(weight=weight).items())
-        return int(s) if weight is None else s
-
-    def number_of_edges(self, u=None, v=None):
-        """Returns the number of edges between two nodes.
-
-        Parameters
-        ----------
-        u, v : nodes, optional (default=all edges)
-            If u and v are specified, return the number of edges between
-            u and v. Otherwise return the total number of all edges.
-
-        Returns
-        -------
-        nedges : int
-            The number of edges in the graph.  If nodes `u` and `v` are
-            specified return the number of edges between those nodes. If
-            the graph is directed, this only returns the number of edges
-            from `u` to `v`.
-
-        See Also
-        --------
-        size
-
-        Examples
-        --------
-        For undirected graphs, this method counts the total number of
-        edges in the graph:
-
-        >>> G = eg.path_graph(4)
-        >>> G.number_of_edges()
-        3
-
-        If you specify two nodes, this counts the total number of edges
-        joining the two nodes:
-
-        >>> G.number_of_edges(0, 1)
-        1
-
-        For directed graphs, this method can count the total number of
-        directed edges from `u` to `v`:
-
-        >>> G = eg.DiGraph()
-        >>> G.add_edge(0, 1)
-        >>> G.add_edge(1, 0)
-        >>> G.number_of_edges(0, 1)
-        1
-
-        """
-        if u is None:
-            return int(self.size())
-        if v in self._adj[u]:
-            return 1
-        return 0
-
-    def nbunch_iter(self, nbunch=None):
-        """Returns an iterator over nodes contained in nbunch that are
-        also in the graph.
-
-        The nodes in nbunch are checked for membership in the graph
-        and if not are silently ignored.
-
-        Parameters
-        ----------
-        nbunch : single node, container, or all nodes (default= all nodes)
-            The view will only report edges incident to these nodes.
-
-        Returns
-        -------
-        niter : iterator
-            An iterator over nodes in nbunch that are also in the graph.
-            If nbunch is None, iterate over all nodes in the graph.
-
-        Raises
-        ------
-        EasyGraphError
-            If nbunch is not a node or sequence of nodes.
-            If a node in nbunch is not hashable.
-
-        See Also
-        --------
-        Graph.__iter__
-
-        Notes
-        -----
-        When nbunch is an iterator, the returned iterator yields values
-        directly from nbunch, becoming exhausted when nbunch is exhausted.
-
-        To test whether nbunch is a single node, one can use
-        "if nbunch in self:", even after processing with this routine.
-
-        If nbunch is not a node or a (possibly empty) sequence/iterator
-        or None, a :exc:`EasyGraphError` is raised.  Also, if any object in
-        nbunch is not hashable, a :exc:`EasyGraphError` is raised.
-        """
-        if nbunch is None:  # include all nodes via iterator
-            bunch = iter(self._adj)
-        elif nbunch in self:  # if nbunch is a single node
-            bunch = iter([nbunch])
-        else:  # if nbunch is a sequence of nodes
-
-            def bunch_iter(nlist, adj):
-                try:
-                    for n in nlist:
-                        if n in adj:
-                            yield n
-                except TypeError as err:
-                    exc, message = err, err.args[0]
-                    # capture error for non-sequence/iterator nbunch.
-                    if "iter" in message:
-                        exc = EasyGraphError(
-                            "nbunch is not a node or a sequence of nodes."
-                        )
-                    # capture error for unhashable node.
-                    if "hashable" in message:
-                        exc = EasyGraphError(
-                            f"Node {n} in sequence nbunch is not a valid node."
-                        )
-                    raise exc
-
-            bunch = bunch_iter(nbunch, self._adj)
-        return bunch
-
-    def neighbors(self, node):
-        """Returns an iterator of a node's neighbors (successors).
-
-        Parameters
-        ----------
-        node : Hashable
-            The target node.
-
-        Returns
-        -------
-        neighbors : iterator
-            An iterator of a node's neighbors (successors).
-
-        Examples
-        --------
-        >>> G = eg.Graph()
-        >>> G.add_edges([(1,2), (2,3), (2,4)])
-        >>> for neighbor in G.neighbors(node=2):
-        ...     print(neighbor)
-
-        """
-        # successors
-        try:
-            return iter(self._adj[node])
-        except KeyError:
-            print("No node {}".format(node))
-
-    successors = neighbors
-
-    def predecessors(self, node):
-        """Returns an iterator of a node's neighbors (predecessors).
-
-        Parameters
-        ----------
-        node : Hashable
-            The target node.
-
-        Returns
-        -------
-        neighbors : iterator
-            An iterator of a node's neighbors (predecessors).
-
-        Examples
-        --------
-        >>> G = eg.Graph()
-        >>> G.add_edges([(1,2), (2,3), (2,4)])
-        >>> for predecessor in G.predecessors(node=2):
-        ...     print(predecessor)
-
-        """
-        # predecessors
-        try:
-            return iter(self._pred[node])
-        except KeyError:
-            print("No node {}".format(node))
-
-    def all_neighbors(self, node):
-        """Returns an iterator of a node's neighbors, including both successors and predecessors.
-
-        Parameters
-        ----------
-        node : Hashable
-            The target node.
-
-        Returns
-        -------
-        neighbors : iterator
-            An iterator of a node's neighbors, including both successors and predecessors.
-
-        Examples
-        --------
-        >>> G = eg.Graph()
-        >>> G.add_edges([(1,2), (2,3), (2,4)])
-        >>> for neighbor in G.all_neighbors(node=2):
-        ...     print(neighbor)
-
-        """
-        # union of successors and predecessors
-        try:
-            neighbors = list(self._adj[node])
-            neighbors.extend(self._pred[node])
-            return iter(neighbors)
-        except KeyError:
-            print("No node {}".format(node))
-
-    def add_node(self, node_for_adding, **node_attr):
-        """Add one node
-
-        Add one node, type of which is any hashable Python object, such as int, string, dict, or even Graph itself.
-        You can add with node attributes using Python dict type.
-
-        Parameters
-        ----------
-        node_for_adding : any hashable Python object
-            Nodes for adding.
-
-        node_attr : keywords arguments, optional
-            The node attributes.
-            You can customize them with different key-value pairs.
-
-        See Also
-        --------
-        add_nodes
-
-        Examples
-        --------
-        >>> G.add_node('a')
-        >>> G.add_node('hello world')
-        >>> G.add_node('Jack', age=10)
-
-        >>> G.add_node('Jack', **{
-        ...     'age': 10,
-        ...     'gender': 'M'
-        ... })
-
-        """
-        self._add_one_node(node_for_adding, node_attr)
-
-    def add_nodes(self, nodes_for_adding: list, nodes_attr: List[Dict] = []):
-        """Add nodes with a list of nodes.
-
-        Parameters
-        ----------
-        nodes_for_adding : list
-
-        nodes_attr : list of dict
-            The corresponding attribute for each of *nodes_for_adding*.
-
-        See Also
-        --------
-        add_node
-
-        Examples
-        --------
-        Add nodes with a list of nodes.
-        You can add with node attributes using a list of Python dict type,
-        each of which is the attribute of each node, respectively.
-
-        >>> G.add_nodes([1, 2, 'a', 'b'])
-        >>> G.add_nodes(range(1, 200))
-
-        >>> G.add_nodes(['Jack', 'Tom', 'Lily'], nodes_attr=[
-        ...     {
-        ...         'age': 10,
-        ...         'gender': 'M'
-        ...     },
-        ...     {
-        ...         'age': 11,
-        ...         'gender': 'M'
-        ...     },
-        ...     {
-        ...         'age': 10,
-        ...         'gender': 'F'
-        ...     }
-        ... ])
-
-        """
-        if nodes_attr is None:
-            nodes_attr = []
-        if not len(nodes_attr) == 0:  # Nodes attributes included in input
-            assert len(nodes_for_adding) == len(
-                nodes_attr
-            ), "Nodes and Attributes lists must have same length."
-        else:  # Set empty attribute for each node
-            nodes_attr = [dict() for i in range(len(nodes_for_adding))]
-
-        for i in range(len(nodes_for_adding)):
-            try:
-                self._add_one_node(nodes_for_adding[i], nodes_attr[i])
-            except Exception as err:
-                print(err)
-                pass
-
-    def add_nodes_from(self, nodes_for_adding, **attr):
-        """Add multiple nodes.
-
-        Parameters
-        ----------
-        nodes_for_adding : iterable container
-            A container of nodes (list, dict, set, etc.).
-            OR
-            A container of (node, attribute dict) tuples.
-            Node attributes are updated using the attribute dict.
-        attr : keyword arguments, optional (default= no attributes)
-            Update attributes for all nodes in nodes.
-            Node attributes specified in nodes as a tuple take
-            precedence over attributes specified via keyword arguments.
-
-        See Also
-        --------
-        add_node
-
-        Examples
-        --------
-        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
-        >>> G.add_nodes_from("Hello")
-        >>> K3 = eg.Graph([(0, 1), (1, 2), (2, 0)])
-        >>> G.add_nodes_from(K3)
-        >>> sorted(G.nodes(), key=str)
-        [0, 1, 2, 'H', 'e', 'l', 'o']
-
-        Use keywords to update specific node attributes for every node.
-
-        >>> G.add_nodes_from([1, 2], size=10)
-        >>> G.add_nodes_from([3, 4], weight=0.4)
-
-        Use (node, attrdict) tuples to update attributes for specific nodes.
-
-        >>> G.add_nodes_from([(1, dict(size=11)), (2, {"color": "blue"})])
-        >>> G.nodes[1]["size"]
-        11
-        >>> H = eg.Graph()
-        >>> H.add_nodes_from(G.nodes(data=True))
-        >>> H.nodes[1]["size"]
-        11
-
-        """
-        for n in nodes_for_adding:
-            try:
-                newnode = n not in self._node
-                newdict = attr
-            except TypeError:
-                n, ndict = n
-                newnode = n not in self._node
-                newdict = attr.copy()
-                newdict.update(ndict)
-            if newnode:
-                if n is None:
-                    raise ValueError("None cannot be a node")
-                self._adj[n] = self.adjlist_inner_dict_factory()
-                self._pred[n] = self.adjlist_inner_dict_factory()
-                self._node[n] = self.node_attr_dict_factory()
-            self._node[n].update(newdict)
-
-    def _add_one_node(self, one_node_for_adding, node_attr: dict = {}):
-        node = one_node_for_adding
-        if node not in self._node:
-            self._node_index[node] = self._id
-            self._id += 1
-            self._adj[node] = self.adjlist_inner_dict_factory()
-            self._pred[node] = self.adjlist_inner_dict_factory()
-            attr_dict = self._node[node] = self.node_attr_dict_factory()
-            attr_dict.update(node_attr)
-        else:  # If already exists, there is no complain and still updating the node attribute
-            self._node[node].update(node_attr)
-
-    def add_edge(self, u_of_edge, v_of_edge, **edge_attr):
-        """Add a directed edge.
-
-        Parameters
-        ----------
-        u_of_edge : object
-            The start end of this edge
-
-        v_of_edge : object
-            The destination end of this edge
-
-        edge_attr : keywords arguments, optional
-            The attribute of the edge.
-
-        Notes
-        -----
-        Nodes of this edge will be automatically added to the graph, if they do not exist.
-
-        See Also
-        --------
-        add_edges
-
-        Examples
-        --------
-
-        >>> G.add_edge(1,2)
-        >>> G.add_edge('Jack', 'Tom', weight=10)
-
-        Add edge with attributes, edge weight, for example,
-
-        >>> G.add_edge(1, 2, **{
-        ...     'weight': 20
-        ... })
-
-        """
-        self._add_one_edge(u_of_edge, v_of_edge, edge_attr)
-
-    def add_weighted_edge(self, u_of_edge, v_of_edge, weight):
-        """Add a weighted edge
-
-        Parameters
-        ----------
-        u_of_edge : start node
-
-        v_of_edge : end node
-
-        weight : weight value
-
-        Examples
-        --------
-        Add a weighted edge
-
-        >>> G.add_weighted_edge( 1 , 3 , 1.0)
-
-        """
-        self._add_one_edge(u_of_edge, v_of_edge, edge_attr={"weight": weight})
-
-    def add_edges(self, edges_for_adding, edges_attr: List[Dict] = []):
-        """Add a list of edges.
-
-        Parameters
-        ----------
-        edges_for_adding : list of 2-element tuple
-            The edges for adding. Each element is a (u, v) tuple, and u, v are
-            start end and destination end, respectively.
-
-        edges_attr : list of dict, optional
-            The corresponding attributes for each edge in *edges_for_adding*.
-
-        Examples
-        --------
-        Add a list of edges into *G*
-
-        >>> G.add_edges([
-        ...     (1, 2),
-        ...     (3, 4),
-        ...     ('Jack', 'Tom')
-        ... ])
-
-        Add edge with attributes, for example, edge weight,
-
-        >>> G.add_edges([(1,2), (2, 3)], edges_attr=[
-        ...     {
-        ...         'weight': 20
-        ...     },
-        ...     {
-        ...         'weight': 15
-        ...     }
-        ... ])
-
-        """
-        if edges_attr is None:
-            edges_attr = []
-        if not len(edges_attr) == 0:  # Edges attributes included in input
-            assert len(edges_for_adding) == len(
-                edges_attr
-            ), "Edges and Attributes lists must have same length."
-        else:  # Set empty attribute for each edge
-            edges_attr = [dict() for i in range(len(edges_for_adding))]
-
-        for i in range(len(edges_for_adding)):
-            try:
-                edge = edges_for_adding[i]
-                attr = edges_attr[i]
-                assert len(edge) == 2, "Edge tuple {} must be 2-tuple.".format(edge)
-                self._add_one_edge(edge[0], edge[1], attr)
-            except Exception as err:
-                print(err)
-
-    def add_edges_from(self, ebunch_to_add, **attr):
-        """Add all the edges in ebunch_to_add.
-
-        Parameters
-        ----------
-        ebunch_to_add : container of edges
-            Each edge given in the container will be added to the
-            graph. The edges must be given as 2-tuples (u, v) or
-            3-tuples (u, v, d) where d is a dictionary containing edge data.
-        attr : keyword arguments, optional
-            Edge data (or labels or objects) can be assigned using
-            keyword arguments.
-
-        See Also
-        --------
-        add_edge : add a single edge
-        add_weighted_edges_from : convenient way to add weighted edges
-
-        Notes
-        -----
-        Adding the same edge twice has no effect but any edge data
-        will be updated when each duplicate edge is added.
-
-        Edge attributes specified in an ebunch take precedence over
-        attributes specified via keyword arguments.
-
-        Examples
-        --------
-        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
-        >>> G.add_edges_from([(0, 1), (1, 2)])  # using a list of edge tuples
-        >>> e = zip(range(0, 3), range(1, 4))
-        >>> G.add_edges_from(e)  # Add the path graph 0-1-2-3
-
-        Associate data to edges
-
-        >>> G.add_edges_from([(1, 2), (2, 3)], weight=3)
-        >>> G.add_edges_from([(3, 4), (1, 4)], label="WN2898")
-        """
-        for e in ebunch_to_add:
-            ne = len(e)
-            if ne == 3:
-                u, v, dd = e
-            elif ne == 2:
-                u, v = e
-                dd = {}
-            else:
-                raise EasyGraphError(f"Edge tuple {e} must be a 2-tuple or 3-tuple.")
-            if u not in self._adj:
-                if u is None:
-                    raise ValueError("None cannot be a node")
-                self._adj[u] = self.adjlist_inner_dict_factory()
-                self._pred[u] = self.adjlist_inner_dict_factory()
-                self._node[u] = self.node_attr_dict_factory()
-            if v not in self._adj:
-                if v is None:
-                    raise ValueError("None cannot be a node")
-                self._adj[v] = self.adjlist_inner_dict_factory()
-                self._pred[v] = self.adjlist_inner_dict_factory()
-                self._node[v] = self.node_attr_dict_factory()
-            datadict = self._adj[u].get(v, self.edge_attr_dict_factory())
-            datadict.update(attr)
-            datadict.update(dd)
-            self._adj[u][v] = datadict
-            self._pred[v][u] = datadict
-
-    def add_edges_from_file(self, file, weighted=False):
-        """Added edges from file
-        For example, txt files,
-
-        Each line is in form like:
-        a b 23.0
-        which denotes an edge `a → b` with weight 23.0.
-
-        Parameters
-        ----------
-        file : string
-            The file path.
-
-        weighted : boolean, optional (default : False)
-            If the file consists of weight information, set `True`.
-            The weight key will be set as 'weight'.
-
-        Examples
-        --------
-
-        If `./club_network.txt` is:
-
-        Jack Mary 23.0
-
-        Mary Tom 15.0
-
-        Tom Ben 20.0
-
-        Then add them to *G*
-
-        >>> G.add_edges_from_file(file='./club_network.txt', weighted=True)
-
-
-        """
-        import re
-
-        with open(file, "r") as fp:
-            edges = fp.readlines()
-        if weighted:
-            for edge in edges:
-                edge = re.sub(",", " ", edge)
-                edge = edge.split()
-                try:
-                    self.add_edge(edge[0], edge[1], weight=float(edge[2]))
-                except:
-                    pass
-        else:
-            for edge in edges:
-                edge = re.sub(",", " ", edge)
-                edge = edge.split()
-                try:
-                    self.add_edge(edge[0], edge[1])
-                except:
-                    pass
-
-    def _add_one_edge(self, u_of_edge, v_of_edge, edge_attr: dict = {}):
-        u, v = u_of_edge, v_of_edge
-        # add nodes
-        if u not in self._node:
-            self._add_one_node(u)
-        if v not in self._node:
-            self._add_one_node(v)
-        # add the edge
-        datadict = self._adj[u].get(v, self.edge_attr_dict_factory())
-        datadict.update(edge_attr)
-        self._adj[u][v] = datadict
-        self._pred[v][u] = datadict
-
-    def remove_node(self, node_to_remove):
-        """Remove one node from your graph.
-
-        Parameters
-        ----------
-        node_to_remove : object
-            The node you want to remove.
-
-        See Also
-        --------
-        remove_nodes
-
-        Examples
-        --------
-        Remove node *Jack* from *G*
-
-        >>> G.remove_node('Jack')
-
-        """
-        try:
-            succs = list(self._adj[node_to_remove])
-            preds = list(self._pred[node_to_remove])
-            del self._node[node_to_remove]
-        except KeyError:  # Node not exists in self
-            raise KeyError("No node {} in graph.".format(node_to_remove))
-        for succ in succs:  # Remove edges start with node_to_remove
-            del self._pred[succ][node_to_remove]
-        for pred in preds:  # Remove edges end with node_to_remove
-            del self._adj[pred][node_to_remove]
-
-        # Remove this node
-        del self._adj[node_to_remove]
-        del self._pred[node_to_remove]
-
-    def remove_nodes(self, nodes_to_remove: list):
-        """Remove nodes from your graph.
-
-        Parameters
-        ----------
-        nodes_to_remove : list of object
-            The list of nodes you want to remove.
-
-        See Also
-        --------
-        remove_node
-
-        Examples
-        --------
-        Remove node *[1, 2, 'a', 'b']* from *G*
-
-        >>> G.remove_nodes([1, 2, 'a', 'b'])
-
-        """
-        for (
-            node
-        ) in (
-            nodes_to_remove
-        ):  # If not all nodes included in graph, give up removing other nodes
-            assert node in self._node, "Remove Error: No node {} in graph".format(node)
-        for node in nodes_to_remove:
-            self.remove_node(node)
-
-    def remove_edge(self, u, v):
-        """Remove one edge from your graph.
-
-        Parameters
-        ----------
-        u : object
-            The start end of the edge.
-
-        v : object
-            The destination end of the edge.
-
-        See Also
-        --------
-        remove_edges
-
-        Examples
-        --------
-        Remove edge (1,2) from *G*
-
-        >>> G.remove_edge(1,2)
-
-        """
-        try:
-            del self._adj[u][v]
-            del self._pred[v][u]
-        except KeyError:
-            raise KeyError("No edge {}-{} in graph.".format(u, v))
-
-    def remove_edges(self, edges_to_remove: [tuple]):
-        """Remove a list of edges from your graph.
-
-        Parameters
-        ----------
-        edges_to_remove : list of tuple
-            The list of edges you want to remove,
-            Each element is (u, v) tuple, which denote the start and destination
-            end of the edge, respectively.
-
-        See Also
-        --------
-        remove_edge
-
-        Examples
-        --------
-        Remove the edges *('Jack', 'Mary')* amd *('Mary', 'Tom')* from *G*
-
-        >>> G.remove_edge([
-        ...     ('Jack', 'Mary'),
-        ...     ('Mary', 'Tom')
-        ... ])
-
-        """
-        for edge in edges_to_remove:
-            u, v = edge[:2]
-            self.remove_edge(u, v)
-
-    def remove_edges_from(self, ebunch):
-        """Remove all edges specified in ebunch.
-
-        Parameters
-        ----------
-        ebunch: list or container of edge tuples
-            Each edge given in the list or container will be removed
-            from the graph. The edges can be:
-
-                - 2-tuples (u, v) edge between u and v.
-                - 3-tuples (u, v, k) where k is ignored.
-
-        See Also
-        --------
-        remove_edge : remove a single edge
-
-        Notes
-        -----
-        Will fail silently if an edge in ebunch is not in the graph.
-
-        Examples
-        --------
-        >>> G = eg.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc
-        >>> ebunch = [(1, 2), (2, 3)]
-        >>> G.remove_edges_from(ebunch)
-        """
-        for e in ebunch:
-            u, v = e[:2]  # ignore edge data
-            if u in self._adj and v in self._adj[u]:
-                del self._adj[u][v]
-                del self._pred[v][u]
-
-    def has_node(self, node):
-        """Returns whether a node exists
-
-        Parameters
-        ----------
-        node
-
-        Returns
-        -------
-        Bool : True (exist) or False (not exists)
-
-        """
-        return node in self._node
-
-    def has_edge(self, u, v):
-        """Returns whether an edge exists
-
-        Parameters
-        ----------
-        u : start node
-
-        v: end node
-
-        Returns
-        -------
-        Bool : True (exist) or False (not exists)
-
-        """
-        try:
-            return v in self._adj[u]
-        except KeyError:
-            return False
-
-    def number_of_nodes(self):
-        """Returns the number of nodes.
-
-        Returns
-        -------
-        number_of_nodes : int
-            The number of nodes.
-        """
-        return len(self._node)
-
-    def is_directed(self):
-        """Returns True if graph is a directed_graph, False otherwise."""
-        return True
-
-    def is_multigraph(self):
-        """Returns True if graph is a multigraph, False otherwise."""
-        return False
-
-    def copy(self):
-        """Return a deep copy of the graph.
-
-        Returns
-        -------
-        copy : easygraph.DiGraph
-            A deep copy of the original graph.
-
-        Examples
-        --------
-        *G2* is a deep copy of *G1*
-
-        >>> G2 = G1.copy()
-
-        """
-        G = self.__class__()
-        G.graph.update(self.graph)
-        for node, node_attr in self._node.items():
-            G.add_node(node, **node_attr)
-        for u, nbrs in self._adj.items():
-            for v, edge_data in nbrs.items():
-                G.add_edge(u, v, **edge_data)
-
-        return G
-
-    def nodes_subgraph(self, from_nodes: list):
-        """Returns a subgraph of some nodes
-
-        Parameters
-        ----------
-        from_nodes : list of object
-            The nodes in subgraph.
-
-        Returns
-        -------
-        nodes_subgraph : easygraph.Graph
-            The subgraph consisting of *from_nodes*.
-
-        Examples
-        --------
-
-        >>> G = eg.Graph()
-        >>> G.add_edges([(1,2), (2,3), (2,4), (4,5)])
-        >>> G_sub = G.nodes_subgraph(from_nodes= [1,2,3])
-
-        """
-        # Edge
-        from_nodes = set(from_nodes)
-        G = self.__class__()
-        G.graph.update(self.graph)
-        from_nodes = set(from_nodes)
-        for node in from_nodes:
-            try:
-                G.add_node(node, **self._node[node])
-            except KeyError:
-                pass
-
-            for v, edge_data in self._adj[node].items():
-                if v in from_nodes:
-                    G.add_edge(node, v, **edge_data)
-        return G
-
-    def ego_subgraph(self, center):
-        """Returns an ego network graph of a node.
-
-        Parameters
-        ----------
-        center : object
-            The center node of the ego network graph
-
-        Returns
-        -------
-        ego_subgraph : easygraph.Graph
-            The ego network graph of *center*.
-
-
-        Examples
-        --------
-        >>> G = eg.Graph()
-        >>> G.add_edges([
-        ...     ('Jack', 'Maria'),
-        ...     ('Maria', 'Andy'),
-        ...     ('Jack', 'Tom')
-        ... ])
-        >>> G.ego_subgraph(center='Jack')
-        """
-        neighbors_of_center = list(self.all_neighbors(center))
-        neighbors_of_center.append(center)
-        return self.nodes_subgraph(from_nodes=neighbors_of_center)
-
-    def to_index_node_graph(self, begin_index=0):
-        """Returns a deep copy of graph, with each node switched to its index.
-
-        Considering that the nodes of your graph may be any possible hashable Python object,
-        you can get an isomorphic graph of the original one, with each node switched to its index.
-
-        Parameters
-        ----------
-        begin_index : int
-            The begin index of the index graph.
-
-        Returns
-        -------
-        G : easygraph.Graph
-            Deep copy of graph, with each node switched to its index.
-
-        index_of_node : dict
-            Index of node
-
-        node_of_index : dict
-            Node of index
-
-        Examples
-        --------
-        The following method returns this isomorphic graph and index-to-node dictionary
-        as well as node-to-index dictionary.
-
-        >>> G = eg.Graph()
-        >>> G.add_edges([
-        ...     ('Jack', 'Maria'),
-        ...     ('Maria', 'Andy'),
-        ...     ('Jack', 'Tom')
-        ... ])
-        >>> G_index_graph, index_of_node, node_of_index = G.to_index_node_graph()
-
-        """
-        G = self.__class__()
-        G.graph.update(self.graph)
-        index_of_node = dict()
-        node_of_index = dict()
-        for index, (node, node_attr) in enumerate(self._node.items()):
-            G.add_node(index + begin_index, **node_attr)
-            index_of_node[node] = index + begin_index
-            node_of_index[index + begin_index] = node
-        for u, nbrs in self._adj.items():
-            for v, edge_data in nbrs.items():
-                G.add_edge(index_of_node[u], index_of_node[v], **edge_data)
-
-        return G, index_of_node, node_of_index
-
-    def cpp(self):
-        G = DiGraphC()
-        G.graph.update(self.graph)
-        for u, attr in self.nodes.items():
-            G.add_node(u, **attr)
-        for u, v, attr in self.edges:
-            G.add_edge(u, v, **attr)
-        G.generate_linkgraph()
-        return G
-
-
-try:
-    import cpp_easygraph
-
-    class DiGraphC(cpp_easygraph.DiGraph):
-        cflag = 1
-
-except ImportError:
-
-    class DiGraphC:
-        def __init__(self, **graph_attr):
-            print(
-                "Object cannot be instantiated because C extension has not been"
-                " successfully compiled and installed. Please refer to"
-                " https://github.com/easy-graph/Easy-Graph/blob/master/README.rst and"
-                " reinstall easygraph."
-            )
-            raise RuntimeError
+from typing import Dict
+from typing import List
+
+import easygraph
+import easygraph.convert as convert
+
+from easygraph.classes.graph import Graph
+from easygraph.utils.exception import EasyGraphError
+
+
+class DiGraph(Graph):
+    """
+    Base class for directed graphs.
+
+        Nodes are allowed for any hashable Python objects, including int, string, dict, etc.
+        Edges are stored as Python dict type, with optional key/value attributes.
+
+    Parameters
+    ----------
+    graph_attr : keywords arguments, optional (default : None)
+        Attributes to add to graph as key=value pairs.
+
+    See Also
+    --------
+    Graph
+
+    Examples
+    --------
+    Create an empty directed graph with no nodes and edges.
+
+    >>> G = eg.DiGraph()
+
+    Create a deep copy graph *G2* from existing Graph *G1*.
+
+    >>> G2 = G1.copy()
+
+    Create an graph with attributes.
+
+    >>> G = eg.DiGraph(name='Karate Club', date='2020.08.21')
+
+    **Attributes:**
+
+    Returns the adjacency matrix of the graph.
+
+    >>> G.adj
+
+    Returns all the nodes with their attributes.
+
+    >>> G.nodes
+
+    Returns all the edges with their attributes.
+
+    >>> G.edges
+
+    """
+
+    gnn_data_dict_factory = dict
+    graph_attr_dict_factory = dict
+    node_dict_factory = dict
+    node_attr_dict_factory = dict
+    adjlist_outer_dict_factory = dict
+    adjlist_inner_dict_factory = dict
+    edge_attr_dict_factory = dict
+    node_index_dict = dict
+
+    def __init__(self, incoming_graph_data=None, **graph_attr):
+        self.graph = self.graph_attr_dict_factory()
+        self._ndata = self.gnn_data_dict_factory()
+        self._node = self.node_dict_factory()
+        self._adj = self.adjlist_outer_dict_factory()
+        self._pred = self.adjlist_outer_dict_factory()
+        self._node_index = self.node_index_dict()
+        self._id = 0
+        self.cflag = 0
+        self.cache = {}
+        self._node_index = self.node_index_dict()
+        if incoming_graph_data is not None:
+            convert.to_easygraph_graph(incoming_graph_data, create_using=self)
+        self.graph.update(graph_attr)
+
+    def __iter__(self):
+        return iter(self._node)
+
+    def __len__(self):
+        return len(self._node)
+
+    def __contains__(self, node):
+        try:
+            return node in self._node
+        except TypeError:
+            return False
+
+    def __getitem__(self, node):
+        # return list(self._adj[node].keys())
+        return self._adj[node]
+
+    @property
+    def node_index(self):
+        return self._node_index
+
+    @property
+    def ndata(self):
+        return self._ndata
+
+    @property
+    def pred(self):
+        """
+        Return the pred of each node
+        """
+        return self._pred
+
+    @property
+    def adj(self):
+        """
+        Return the adjacency matrix
+        """
+        return self._adj
+
+    @property
+    def nodes(self):
+        """
+        return [node for node in self._node]
+        """
+        return self._node
+
+    @property
+    def edges(self):
+        """
+        Return an edge list
+        """
+        edges = list()
+        for u in self._adj:
+            for v in self._adj[u]:
+                edges.append((u, v, self._adj[u][v]))
+        return edges
+
+    @property
+    def name(self):
+        """String identifier of the graph.
+
+        This graph attribute appears in the attribute dict G.graph
+        keyed by the string `"name"`. as well as an attribute (technically
+        a property) `G.name`. This is entirely user controlled.
+        """
+        return self.graph.get("name", "")
+
+    @name.setter
+    def name(self, s):
+        """
+        Set graph name
+
+        Parameters
+        ----------
+        s : name
+        """
+        self.graph["name"] = s
+
+    @property
+    def node_index(self):
+        """
+        Assign an integer index for each node (start from 0)
+        """
+        if self.cache.get("node_index", None) is None:
+            node2index_dict = {}
+            index = 0
+            for n in self.nodes:
+                node2index_dict[n] = index
+                index += 1
+            self.cache["node_index"] = node2index_dict
+        return self.cache["node_index"]
+
+    @property
+    def index2node(self):
+        """
+        Assign an integer index for each node (start from 0)
+        """
+        if self.cache.get("index2node", None) is None:
+            index2node_dict = {}
+            for index, n in enumerate(self.nodes):
+                index2node_dict[index] = n
+            self.cache["index2node"] = index2node_dict
+        return self.cache["index2node"]
+
+    def out_degree(self, weight="weight"):
+        """Returns the weighted out degree of each node.
+
+        Parameters
+        ----------
+        weight : string, optional (default : 'weight')
+            Weight key of the original weighted graph.
+
+        Returns
+        -------
+        out_degree : dict
+            Each node's (key) weighted out degree (value).
+
+        Notes
+        -----
+        If the graph is not weighted, all the weights will be regarded as 1.
+
+        See Also
+        --------
+        in_degree
+        degree
+
+        Examples
+        --------
+
+        >>> G.out_degree(weight='weight')
+
+        """
+        degree = dict()
+        for u, v, d in self.edges:
+            if u in degree:
+                degree[u] += d.get(weight, 1)
+            else:
+                degree[u] = d.get(weight, 1)
+
+        # For isolated nodes
+        for node in self.nodes:
+            if node not in degree:
+                degree[node] = 0
+
+        return degree
+
+    def in_degree(self, weight="weight"):
+        """Returns the weighted in degree of each node.
+
+        Parameters
+        ----------
+        weight : string, optional (default : 'weight')
+            Weight key of the original weighted graph.
+
+        Returns
+        -------
+        in_degree : dict
+            Each node's (key) weighted in degree (value).
+
+        Notes
+        -----
+        If the graph is not weighted, all the weights will be regarded as 1.
+
+        See Also
+        --------
+        out_degree
+        degree
+
+        Examples
+        --------
+
+        >>> G.in_degree(weight='weight')
+
+        """
+        degree = dict()
+        for u, v, d in self.edges:
+            if v in degree:
+                degree[v] += d.get(weight, 1)
+            else:
+                degree[v] = d.get(weight, 1)
+
+        # For isolated nodes
+        for node in self.nodes:
+            if node not in degree:
+                degree[node] = 0
+
+        return degree
+
+    def degree(self, weight="weight"):
+        """Returns the weighted degree of each node, i.e. sum of out/in degree.
+
+        Parameters
+        ----------
+        weight : string, optional (default : 'weight')
+            Weight key of the original weighted graph.
+
+        Returns
+        -------
+        degree : dict
+            Each node's (key) weighted in degree (value).
+            For directed graph, it returns the sum of out degree and in degree.
+
+        Notes
+        -----
+        If the graph is not weighted, all the weights will be regarded as 1.
+
+        See also
+        --------
+        out_degree
+        in_degree
+
+        Examples
+        --------
+
+        >>> G.degree()
+        >>> G.degree(weight='weight')
+
+        or you can customize the weight key
+
+        >>> G.degree(weight='weight_1')
+
+        """
+        degree = dict()
+        outdegree = self.out_degree(weight=weight)
+        indegree = self.in_degree(weight=weight)
+        all_nodes = set(outdegree.keys()) | set(indegree.keys())
+        for u in all_nodes:
+            degree[u] = outdegree[u] + indegree[u]
+        return degree
+
+    def size(self, weight=None):
+        """Returns the number of edges or total of all edge weights.
+
+        Parameters
+        -----------
+        weight : String or None, optional
+            The weight key. If None, it will calculate the number of
+            edges, instead of total of all edge weights.
+
+        Returns
+        -------
+        size : int or float, optional (default: None)
+            The number of edges or total of all edge weights.
+
+        Examples
+        --------
+
+        Returns the number of edges in G:
+
+        >>> G.size()
+
+        Returns the total of all edge weights in G:
+
+        >>> G.size(weight='weight')
+
+        """
+        s = sum(d for v, d in self.out_degree(weight=weight).items())
+        return int(s) if weight is None else s
+
+    def number_of_edges(self, u=None, v=None):
+        """Returns the number of edges between two nodes.
+
+        Parameters
+        ----------
+        u, v : nodes, optional (default=all edges)
+            If u and v are specified, return the number of edges between
+            u and v. Otherwise return the total number of all edges.
+
+        Returns
+        -------
+        nedges : int
+            The number of edges in the graph.  If nodes `u` and `v` are
+            specified return the number of edges between those nodes. If
+            the graph is directed, this only returns the number of edges
+            from `u` to `v`.
+
+        See Also
+        --------
+        size
+
+        Examples
+        --------
+        For undirected graphs, this method counts the total number of
+        edges in the graph:
+
+        >>> G = eg.path_graph(4)
+        >>> G.number_of_edges()
+        3
+
+        If you specify two nodes, this counts the total number of edges
+        joining the two nodes:
+
+        >>> G.number_of_edges(0, 1)
+        1
+
+        For directed graphs, this method can count the total number of
+        directed edges from `u` to `v`:
+
+        >>> G = eg.DiGraph()
+        >>> G.add_edge(0, 1)
+        >>> G.add_edge(1, 0)
+        >>> G.number_of_edges(0, 1)
+        1
+
+        """
+        if u is None:
+            return int(self.size())
+        if v in self._adj[u]:
+            return 1
+        return 0
+
+    def nbunch_iter(self, nbunch=None):
+        """Returns an iterator over nodes contained in nbunch that are
+        also in the graph.
+
+        The nodes in nbunch are checked for membership in the graph
+        and if not are silently ignored.
+
+        Parameters
+        ----------
+        nbunch : single node, container, or all nodes (default= all nodes)
+            The view will only report edges incident to these nodes.
+
+        Returns
+        -------
+        niter : iterator
+            An iterator over nodes in nbunch that are also in the graph.
+            If nbunch is None, iterate over all nodes in the graph.
+
+        Raises
+        ------
+        EasyGraphError
+            If nbunch is not a node or sequence of nodes.
+            If a node in nbunch is not hashable.
+
+        See Also
+        --------
+        Graph.__iter__
+
+        Notes
+        -----
+        When nbunch is an iterator, the returned iterator yields values
+        directly from nbunch, becoming exhausted when nbunch is exhausted.
+
+        To test whether nbunch is a single node, one can use
+        "if nbunch in self:", even after processing with this routine.
+
+        If nbunch is not a node or a (possibly empty) sequence/iterator
+        or None, a :exc:`EasyGraphError` is raised.  Also, if any object in
+        nbunch is not hashable, a :exc:`EasyGraphError` is raised.
+        """
+        if nbunch is None:  # include all nodes via iterator
+            bunch = iter(self._adj)
+        elif nbunch in self:  # if nbunch is a single node
+            bunch = iter([nbunch])
+        else:  # if nbunch is a sequence of nodes
+
+            def bunch_iter(nlist, adj):
+                try:
+                    for n in nlist:
+                        if n in adj:
+                            yield n
+                except TypeError as err:
+                    exc, message = err, err.args[0]
+                    # capture error for non-sequence/iterator nbunch.
+                    if "iter" in message:
+                        exc = EasyGraphError(
+                            "nbunch is not a node or a sequence of nodes."
+                        )
+                    # capture error for unhashable node.
+                    if "hashable" in message:
+                        exc = EasyGraphError(
+                            f"Node {n} in sequence nbunch is not a valid node."
+                        )
+                    raise exc
+
+            bunch = bunch_iter(nbunch, self._adj)
+        return bunch
+
+    def neighbors(self, node):
+        """Returns an iterator of a node's neighbors (successors).
+
+        Parameters
+        ----------
+        node : Hashable
+            The target node.
+
+        Returns
+        -------
+        neighbors : iterator
+            An iterator of a node's neighbors (successors).
+
+        Examples
+        --------
+        >>> G = eg.Graph()
+        >>> G.add_edges([(1,2), (2,3), (2,4)])
+        >>> for neighbor in G.neighbors(node=2):
+        ...     print(neighbor)
+
+        """
+        # successors
+        try:
+            return iter(self._adj[node])
+        except KeyError:
+            print("No node {}".format(node))
+
+    successors = neighbors
+
+    def predecessors(self, node):
+        """Returns an iterator of a node's neighbors (predecessors).
+
+        Parameters
+        ----------
+        node : Hashable
+            The target node.
+
+        Returns
+        -------
+        neighbors : iterator
+            An iterator of a node's neighbors (predecessors).
+
+        Examples
+        --------
+        >>> G = eg.Graph()
+        >>> G.add_edges([(1,2), (2,3), (2,4)])
+        >>> for predecessor in G.predecessors(node=2):
+        ...     print(predecessor)
+
+        """
+        # predecessors
+        try:
+            return iter(self._pred[node])
+        except KeyError:
+            print("No node {}".format(node))
+
+    def all_neighbors(self, node):
+        """Returns an iterator of a node's neighbors, including both successors and predecessors.
+
+        Parameters
+        ----------
+        node : Hashable
+            The target node.
+
+        Returns
+        -------
+        neighbors : iterator
+            An iterator of a node's neighbors, including both successors and predecessors.
+
+        Examples
+        --------
+        >>> G = eg.Graph()
+        >>> G.add_edges([(1,2), (2,3), (2,4)])
+        >>> for neighbor in G.all_neighbors(node=2):
+        ...     print(neighbor)
+
+        """
+        # union of successors and predecessors
+        try:
+            neighbors = list(self._adj[node])
+            neighbors.extend(self._pred[node])
+            return iter(neighbors)
+        except KeyError:
+            print("No node {}".format(node))
+
+    def add_node(self, node_for_adding, **node_attr):
+        """Add one node
+
+        Add one node, type of which is any hashable Python object, such as int, string, dict, or even Graph itself.
+        You can add with node attributes using Python dict type.
+
+        Parameters
+        ----------
+        node_for_adding : any hashable Python object
+            Nodes for adding.
+
+        node_attr : keywords arguments, optional
+            The node attributes.
+            You can customize them with different key-value pairs.
+
+        See Also
+        --------
+        add_nodes
+
+        Examples
+        --------
+        >>> G.add_node('a')
+        >>> G.add_node('hello world')
+        >>> G.add_node('Jack', age=10)
+
+        >>> G.add_node('Jack', **{
+        ...     'age': 10,
+        ...     'gender': 'M'
+        ... })
+
+        """
+        self._add_one_node(node_for_adding, node_attr)
+
+    def add_nodes(self, nodes_for_adding: list, nodes_attr: List[Dict] = []):
+        """Add nodes with a list of nodes.
+
+        Parameters
+        ----------
+        nodes_for_adding : list
+
+        nodes_attr : list of dict
+            The corresponding attribute for each of *nodes_for_adding*.
+
+        See Also
+        --------
+        add_node
+
+        Examples
+        --------
+        Add nodes with a list of nodes.
+        You can add with node attributes using a list of Python dict type,
+        each of which is the attribute of each node, respectively.
+
+        >>> G.add_nodes([1, 2, 'a', 'b'])
+        >>> G.add_nodes(range(1, 200))
+
+        >>> G.add_nodes(['Jack', 'Tom', 'Lily'], nodes_attr=[
+        ...     {
+        ...         'age': 10,
+        ...         'gender': 'M'
+        ...     },
+        ...     {
+        ...         'age': 11,
+        ...         'gender': 'M'
+        ...     },
+        ...     {
+        ...         'age': 10,
+        ...         'gender': 'F'
+        ...     }
+        ... ])
+
+        """
+        if nodes_attr is None:
+            nodes_attr = []
+        if not len(nodes_attr) == 0:  # Nodes attributes included in input
+            assert len(nodes_for_adding) == len(
+                nodes_attr
+            ), "Nodes and Attributes lists must have same length."
+        else:  # Set empty attribute for each node
+            nodes_attr = [dict() for i in range(len(nodes_for_adding))]
+
+        for i in range(len(nodes_for_adding)):
+            try:
+                self._add_one_node(nodes_for_adding[i], nodes_attr[i])
+            except Exception as err:
+                print(err)
+                pass
+
+    def add_nodes_from(self, nodes_for_adding, **attr):
+        """Add multiple nodes.
+
+        Parameters
+        ----------
+        nodes_for_adding : iterable container
+            A container of nodes (list, dict, set, etc.).
+            OR
+            A container of (node, attribute dict) tuples.
+            Node attributes are updated using the attribute dict.
+        attr : keyword arguments, optional (default= no attributes)
+            Update attributes for all nodes in nodes.
+            Node attributes specified in nodes as a tuple take
+            precedence over attributes specified via keyword arguments.
+
+        See Also
+        --------
+        add_node
+
+        Examples
+        --------
+        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
+        >>> G.add_nodes_from("Hello")
+        >>> K3 = eg.Graph([(0, 1), (1, 2), (2, 0)])
+        >>> G.add_nodes_from(K3)
+        >>> sorted(G.nodes(), key=str)
+        [0, 1, 2, 'H', 'e', 'l', 'o']
+
+        Use keywords to update specific node attributes for every node.
+
+        >>> G.add_nodes_from([1, 2], size=10)
+        >>> G.add_nodes_from([3, 4], weight=0.4)
+
+        Use (node, attrdict) tuples to update attributes for specific nodes.
+
+        >>> G.add_nodes_from([(1, dict(size=11)), (2, {"color": "blue"})])
+        >>> G.nodes[1]["size"]
+        11
+        >>> H = eg.Graph()
+        >>> H.add_nodes_from(G.nodes(data=True))
+        >>> H.nodes[1]["size"]
+        11
+
+        """
+        for n in nodes_for_adding:
+            try:
+                newnode = n not in self._node
+                newdict = attr
+            except TypeError:
+                n, ndict = n
+                newnode = n not in self._node
+                newdict = attr.copy()
+                newdict.update(ndict)
+            if newnode:
+                if n is None:
+                    raise ValueError("None cannot be a node")
+                self._adj[n] = self.adjlist_inner_dict_factory()
+                self._pred[n] = self.adjlist_inner_dict_factory()
+                self._node[n] = self.node_attr_dict_factory()
+            self._node[n].update(newdict)
+
+    def _add_one_node(self, one_node_for_adding, node_attr: dict = {}):
+        node = one_node_for_adding
+        if node not in self._node:
+            self._node_index[node] = self._id
+            self._id += 1
+            self._adj[node] = self.adjlist_inner_dict_factory()
+            self._pred[node] = self.adjlist_inner_dict_factory()
+            attr_dict = self._node[node] = self.node_attr_dict_factory()
+            attr_dict.update(node_attr)
+        else:  # If already exists, there is no complain and still updating the node attribute
+            self._node[node].update(node_attr)
+
+    def add_edge(self, u_of_edge, v_of_edge, **edge_attr):
+        """Add a directed edge.
+
+        Parameters
+        ----------
+        u_of_edge : object
+            The start end of this edge
+
+        v_of_edge : object
+            The destination end of this edge
+
+        edge_attr : keywords arguments, optional
+            The attribute of the edge.
+
+        Notes
+        -----
+        Nodes of this edge will be automatically added to the graph, if they do not exist.
+
+        See Also
+        --------
+        add_edges
+
+        Examples
+        --------
+
+        >>> G.add_edge(1,2)
+        >>> G.add_edge('Jack', 'Tom', weight=10)
+
+        Add edge with attributes, edge weight, for example,
+
+        >>> G.add_edge(1, 2, **{
+        ...     'weight': 20
+        ... })
+
+        """
+        self._add_one_edge(u_of_edge, v_of_edge, edge_attr)
+
+    def add_weighted_edge(self, u_of_edge, v_of_edge, weight):
+        """Add a weighted edge
+
+        Parameters
+        ----------
+        u_of_edge : start node
+
+        v_of_edge : end node
+
+        weight : weight value
+
+        Examples
+        --------
+        Add a weighted edge
+
+        >>> G.add_weighted_edge( 1 , 3 , 1.0)
+
+        """
+        self._add_one_edge(u_of_edge, v_of_edge, edge_attr={"weight": weight})
+
+    def add_edges(self, edges_for_adding, edges_attr: List[Dict] = []):
+        """Add a list of edges.
+
+        Parameters
+        ----------
+        edges_for_adding : list of 2-element tuple
+            The edges for adding. Each element is a (u, v) tuple, and u, v are
+            start end and destination end, respectively.
+
+        edges_attr : list of dict, optional
+            The corresponding attributes for each edge in *edges_for_adding*.
+
+        Examples
+        --------
+        Add a list of edges into *G*
+
+        >>> G.add_edges([
+        ...     (1, 2),
+        ...     (3, 4),
+        ...     ('Jack', 'Tom')
+        ... ])
+
+        Add edge with attributes, for example, edge weight,
+
+        >>> G.add_edges([(1,2), (2, 3)], edges_attr=[
+        ...     {
+        ...         'weight': 20
+        ...     },
+        ...     {
+        ...         'weight': 15
+        ...     }
+        ... ])
+
+        """
+        if edges_attr is None:
+            edges_attr = []
+        if not len(edges_attr) == 0:  # Edges attributes included in input
+            assert len(edges_for_adding) == len(
+                edges_attr
+            ), "Edges and Attributes lists must have same length."
+        else:  # Set empty attribute for each edge
+            edges_attr = [dict() for i in range(len(edges_for_adding))]
+
+        for i in range(len(edges_for_adding)):
+            try:
+                edge = edges_for_adding[i]
+                attr = edges_attr[i]
+                assert len(edge) == 2, "Edge tuple {} must be 2-tuple.".format(edge)
+                self._add_one_edge(edge[0], edge[1], attr)
+            except Exception as err:
+                print(err)
+
+    def add_edges_from(self, ebunch_to_add, **attr):
+        """Add all the edges in ebunch_to_add.
+
+        Parameters
+        ----------
+        ebunch_to_add : container of edges
+            Each edge given in the container will be added to the
+            graph. The edges must be given as 2-tuples (u, v) or
+            3-tuples (u, v, d) where d is a dictionary containing edge data.
+        attr : keyword arguments, optional
+            Edge data (or labels or objects) can be assigned using
+            keyword arguments.
+
+        See Also
+        --------
+        add_edge : add a single edge
+        add_weighted_edges_from : convenient way to add weighted edges
+
+        Notes
+        -----
+        Adding the same edge twice has no effect but any edge data
+        will be updated when each duplicate edge is added.
+
+        Edge attributes specified in an ebunch take precedence over
+        attributes specified via keyword arguments.
+
+        Examples
+        --------
+        >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
+        >>> G.add_edges_from([(0, 1), (1, 2)])  # using a list of edge tuples
+        >>> e = zip(range(0, 3), range(1, 4))
+        >>> G.add_edges_from(e)  # Add the path graph 0-1-2-3
+
+        Associate data to edges
+
+        >>> G.add_edges_from([(1, 2), (2, 3)], weight=3)
+        >>> G.add_edges_from([(3, 4), (1, 4)], label="WN2898")
+        """
+        for e in ebunch_to_add:
+            ne = len(e)
+            if ne == 3:
+                u, v, dd = e
+            elif ne == 2:
+                u, v = e
+                dd = {}
+            else:
+                raise EasyGraphError(f"Edge tuple {e} must be a 2-tuple or 3-tuple.")
+            if u not in self._adj:
+                if u is None:
+                    raise ValueError("None cannot be a node")
+                self._adj[u] = self.adjlist_inner_dict_factory()
+                self._pred[u] = self.adjlist_inner_dict_factory()
+                self._node[u] = self.node_attr_dict_factory()
+            if v not in self._adj:
+                if v is None:
+                    raise ValueError("None cannot be a node")
+                self._adj[v] = self.adjlist_inner_dict_factory()
+                self._pred[v] = self.adjlist_inner_dict_factory()
+                self._node[v] = self.node_attr_dict_factory()
+            datadict = self._adj[u].get(v, self.edge_attr_dict_factory())
+            datadict.update(attr)
+            datadict.update(dd)
+            self._adj[u][v] = datadict
+            self._pred[v][u] = datadict
+
+    def add_edges_from_file(self, file, weighted=False):
+        """Added edges from file
+        For example, txt files,
+
+        Each line is in form like:
+        a b 23.0
+        which denotes an edge `a → b` with weight 23.0.
+
+        Parameters
+        ----------
+        file : string
+            The file path.
+
+        weighted : boolean, optional (default : False)
+            If the file consists of weight information, set `True`.
+            The weight key will be set as 'weight'.
+
+        Examples
+        --------
+
+        If `./club_network.txt` is:
+
+        Jack Mary 23.0
+
+        Mary Tom 15.0
+
+        Tom Ben 20.0
+
+        Then add them to *G*
+
+        >>> G.add_edges_from_file(file='./club_network.txt', weighted=True)
+
+
+        """
+        import re
+
+        with open(file, "r") as fp:
+            edges = fp.readlines()
+        if weighted:
+            for edge in edges:
+                edge = re.sub(",", " ", edge)
+                edge = edge.split()
+                try:
+                    self.add_edge(edge[0], edge[1], weight=float(edge[2]))
+                except:
+                    pass
+        else:
+            for edge in edges:
+                edge = re.sub(",", " ", edge)
+                edge = edge.split()
+                try:
+                    self.add_edge(edge[0], edge[1])
+                except:
+                    pass
+
+    def _add_one_edge(self, u_of_edge, v_of_edge, edge_attr: dict = {}):
+        u, v = u_of_edge, v_of_edge
+        # add nodes
+        if u not in self._node:
+            self._add_one_node(u)
+        if v not in self._node:
+            self._add_one_node(v)
+        # add the edge
+        datadict = self._adj[u].get(v, self.edge_attr_dict_factory())
+        datadict.update(edge_attr)
+        self._adj[u][v] = datadict
+        self._pred[v][u] = datadict
+
+    def remove_node(self, node_to_remove):
+        """Remove one node from your graph.
+
+        Parameters
+        ----------
+        node_to_remove : object
+            The node you want to remove.
+
+        See Also
+        --------
+        remove_nodes
+
+        Examples
+        --------
+        Remove node *Jack* from *G*
+
+        >>> G.remove_node('Jack')
+
+        """
+        try:
+            succs = list(self._adj[node_to_remove])
+            preds = list(self._pred[node_to_remove])
+            del self._node[node_to_remove]
+        except KeyError:  # Node not exists in self
+            raise KeyError("No node {} in graph.".format(node_to_remove))
+        for succ in succs:  # Remove edges start with node_to_remove
+            del self._pred[succ][node_to_remove]
+        for pred in preds:  # Remove edges end with node_to_remove
+            del self._adj[pred][node_to_remove]
+
+        # Remove this node
+        del self._adj[node_to_remove]
+        del self._pred[node_to_remove]
+
+    def remove_nodes(self, nodes_to_remove: list):
+        """Remove nodes from your graph.
+
+        Parameters
+        ----------
+        nodes_to_remove : list of object
+            The list of nodes you want to remove.
+
+        See Also
+        --------
+        remove_node
+
+        Examples
+        --------
+        Remove node *[1, 2, 'a', 'b']* from *G*
+
+        >>> G.remove_nodes([1, 2, 'a', 'b'])
+
+        """
+        for (
+            node
+        ) in (
+            nodes_to_remove
+        ):  # If not all nodes included in graph, give up removing other nodes
+            assert node in self._node, "Remove Error: No node {} in graph".format(node)
+        for node in nodes_to_remove:
+            self.remove_node(node)
+
+    def remove_edge(self, u, v):
+        """Remove one edge from your graph.
+
+        Parameters
+        ----------
+        u : object
+            The start end of the edge.
+
+        v : object
+            The destination end of the edge.
+
+        See Also
+        --------
+        remove_edges
+
+        Examples
+        --------
+        Remove edge (1,2) from *G*
+
+        >>> G.remove_edge(1,2)
+
+        """
+        try:
+            del self._adj[u][v]
+            del self._pred[v][u]
+        except KeyError:
+            raise KeyError("No edge {}-{} in graph.".format(u, v))
+
+    def remove_edges(self, edges_to_remove: [tuple]):
+        """Remove a list of edges from your graph.
+
+        Parameters
+        ----------
+        edges_to_remove : list of tuple
+            The list of edges you want to remove,
+            Each element is (u, v) tuple, which denote the start and destination
+            end of the edge, respectively.
+
+        See Also
+        --------
+        remove_edge
+
+        Examples
+        --------
+        Remove the edges *('Jack', 'Mary')* amd *('Mary', 'Tom')* from *G*
+
+        >>> G.remove_edge([
+        ...     ('Jack', 'Mary'),
+        ...     ('Mary', 'Tom')
+        ... ])
+
+        """
+        for edge in edges_to_remove:
+            u, v = edge[:2]
+            self.remove_edge(u, v)
+
+    def remove_edges_from(self, ebunch):
+        """Remove all edges specified in ebunch.
+
+        Parameters
+        ----------
+        ebunch: list or container of edge tuples
+            Each edge given in the list or container will be removed
+            from the graph. The edges can be:
+
+                - 2-tuples (u, v) edge between u and v.
+                - 3-tuples (u, v, k) where k is ignored.
+
+        See Also
+        --------
+        remove_edge : remove a single edge
+
+        Notes
+        -----
+        Will fail silently if an edge in ebunch is not in the graph.
+
+        Examples
+        --------
+        >>> G = eg.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc
+        >>> ebunch = [(1, 2), (2, 3)]
+        >>> G.remove_edges_from(ebunch)
+        """
+        for e in ebunch:
+            u, v = e[:2]  # ignore edge data
+            if u in self._adj and v in self._adj[u]:
+                del self._adj[u][v]
+                del self._pred[v][u]
+
+    def has_node(self, node):
+        """Returns whether a node exists
+
+        Parameters
+        ----------
+        node
+
+        Returns
+        -------
+        Bool : True (exist) or False (not exists)
+
+        """
+        return node in self._node
+
+    def has_edge(self, u, v):
+        """Returns whether an edge exists
+
+        Parameters
+        ----------
+        u : start node
+
+        v: end node
+
+        Returns
+        -------
+        Bool : True (exist) or False (not exists)
+
+        """
+        try:
+            return v in self._adj[u]
+        except KeyError:
+            return False
+
+    def number_of_nodes(self):
+        """Returns the number of nodes.
+
+        Returns
+        -------
+        number_of_nodes : int
+            The number of nodes.
+        """
+        return len(self._node)
+
+    def is_directed(self):
+        """Returns True if graph is a directed_graph, False otherwise."""
+        return True
+
+    def is_multigraph(self):
+        """Returns True if graph is a multigraph, False otherwise."""
+        return False
+
+    def copy(self):
+        """Return a deep copy of the graph.
+
+        Returns
+        -------
+        copy : easygraph.DiGraph
+            A deep copy of the original graph.
+
+        Examples
+        --------
+        *G2* is a deep copy of *G1*
+
+        >>> G2 = G1.copy()
+
+        """
+        G = self.__class__()
+        G.graph.update(self.graph)
+        for node, node_attr in self._node.items():
+            G.add_node(node, **node_attr)
+        for u, nbrs in self._adj.items():
+            for v, edge_data in nbrs.items():
+                G.add_edge(u, v, **edge_data)
+
+        return G
+
+    def nodes_subgraph(self, from_nodes: list):
+        """Returns a subgraph of some nodes
+
+        Parameters
+        ----------
+        from_nodes : list of object
+            The nodes in subgraph.
+
+        Returns
+        -------
+        nodes_subgraph : easygraph.Graph
+            The subgraph consisting of *from_nodes*.
+
+        Examples
+        --------
+
+        >>> G = eg.Graph()
+        >>> G.add_edges([(1,2), (2,3), (2,4), (4,5)])
+        >>> G_sub = G.nodes_subgraph(from_nodes= [1,2,3])
+
+        """
+        # Edge
+        from_nodes = set(from_nodes)
+        G = self.__class__()
+        G.graph.update(self.graph)
+        from_nodes = set(from_nodes)
+        for node in from_nodes:
+            try:
+                G.add_node(node, **self._node[node])
+            except KeyError:
+                pass
+
+            for v, edge_data in self._adj[node].items():
+                if v in from_nodes:
+                    G.add_edge(node, v, **edge_data)
+        return G
+
+    def ego_subgraph(self, center):
+        """Returns an ego network graph of a node.
+
+        Parameters
+        ----------
+        center : object
+            The center node of the ego network graph
+
+        Returns
+        -------
+        ego_subgraph : easygraph.Graph
+            The ego network graph of *center*.
+
+
+        Examples
+        --------
+        >>> G = eg.Graph()
+        >>> G.add_edges([
+        ...     ('Jack', 'Maria'),
+        ...     ('Maria', 'Andy'),
+        ...     ('Jack', 'Tom')
+        ... ])
+        >>> G.ego_subgraph(center='Jack')
+        """
+        neighbors_of_center = list(self.all_neighbors(center))
+        neighbors_of_center.append(center)
+        return self.nodes_subgraph(from_nodes=neighbors_of_center)
+
+    def to_index_node_graph(self, begin_index=0):
+        """Returns a deep copy of graph, with each node switched to its index.
+
+        Considering that the nodes of your graph may be any possible hashable Python object,
+        you can get an isomorphic graph of the original one, with each node switched to its index.
+
+        Parameters
+        ----------
+        begin_index : int
+            The begin index of the index graph.
+
+        Returns
+        -------
+        G : easygraph.Graph
+            Deep copy of graph, with each node switched to its index.
+
+        index_of_node : dict
+            Index of node
+
+        node_of_index : dict
+            Node of index
+
+        Examples
+        --------
+        The following method returns this isomorphic graph and index-to-node dictionary
+        as well as node-to-index dictionary.
+
+        >>> G = eg.Graph()
+        >>> G.add_edges([
+        ...     ('Jack', 'Maria'),
+        ...     ('Maria', 'Andy'),
+        ...     ('Jack', 'Tom')
+        ... ])
+        >>> G_index_graph, index_of_node, node_of_index = G.to_index_node_graph()
+
+        """
+        G = self.__class__()
+        G.graph.update(self.graph)
+        index_of_node = dict()
+        node_of_index = dict()
+        for index, (node, node_attr) in enumerate(self._node.items()):
+            G.add_node(index + begin_index, **node_attr)
+            index_of_node[node] = index + begin_index
+            node_of_index[index + begin_index] = node
+        for u, nbrs in self._adj.items():
+            for v, edge_data in nbrs.items():
+                G.add_edge(index_of_node[u], index_of_node[v], **edge_data)
+
+        return G, index_of_node, node_of_index
+
+    def cpp(self):
+        G = DiGraphC()
+        G.graph.update(self.graph)
+        for u, attr in self.nodes.items():
+            G.add_node(u, **attr)
+        for u, v, attr in self.edges:
+            G.add_edge(u, v, **attr)
+        G.generate_linkgraph()
+        return G
+
+
+try:
+    import cpp_easygraph
+
+    class DiGraphC(cpp_easygraph.DiGraph):
+        cflag = 1
+
+except ImportError:
+
+    class DiGraphC:
+        def __init__(self, **graph_attr):
+            print(
+                "Object cannot be instantiated because C extension has not been"
+                " successfully compiled and installed. Please refer to"
+                " https://github.com/easy-graph/Easy-Graph/blob/master/README.rst and"
+                " reinstall easygraph."
+            )
+            raise RuntimeError
```

## easygraph/classes/operation.py

 * *Ordering differences only*

```diff
@@ -1,447 +1,447 @@
-from itertools import chain
-
-import easygraph as eg
-
-from easygraph.utils import *
-
-
-__all__ = [
-    "set_edge_attributes",
-    "add_path",
-    "set_node_attributes",
-    "selfloop_edges",
-    "topological_sort",
-    "number_of_selfloops",
-    "density",
-]
-
-
-def set_edge_attributes(G, values, name=None):
-    """Sets edge attributes from a given value or dictionary of values.
-
-    .. Warning:: The call order of arguments `values` and `name`
-        switched between v1.x & v2.x.
-
-    Parameters
-    ----------
-    G : EasyGraph Graph
-
-    values : scalar value, dict-like
-        What the edge attribute should be set to.  If `values` is
-        not a dictionary, then it is treated as a single attribute value
-        that is then applied to every edge in `G`.  This means that if
-        you provide a mutable object, like a list, updates to that object
-        will be reflected in the edge attribute for each edge.  The attribute
-        name will be `name`.
-
-        If `values` is a dict or a dict of dict, it should be keyed
-        by edge tuple to either an attribute value or a dict of attribute
-        key/value pairs used to update the edge's attributes.
-        For multigraphs, the edge tuples must be of the form ``(u, v, key)``,
-        where `u` and `v` are nodes and `key` is the edge key.
-        For non-multigraphs, the keys must be tuples of the form ``(u, v)``.
-
-    name : string (optional, default=None)
-        Name of the edge attribute to set if values is a scalar.
-
-    Examples
-    --------
-    After computing some property of the edges of a graph, you may want
-    to assign a edge attribute to store the value of that property for
-    each edge::
-
-        >>> G = eg.path_graph(3)
-        >>> bb = eg.edge_betweenness_centrality(G, normalized=False)
-        >>> eg.set_edge_attributes(G, bb, "betweenness")
-        >>> G.edges[1, 2]["betweenness"]
-        2.0
-
-    If you provide a list as the second argument, updates to the list
-    will be reflected in the edge attribute for each edge::
-
-        >>> labels = []
-        >>> eg.set_edge_attributes(G, labels, "labels")
-        >>> labels.append("foo")
-        >>> G.edges[0, 1]["labels"]
-        ['foo']
-        >>> G.edges[1, 2]["labels"]
-        ['foo']
-
-    If you provide a dictionary of dictionaries as the second argument,
-    the entire dictionary will be used to update edge attributes::
-
-        >>> G = eg.path_graph(3)
-        >>> attrs = {(0, 1): {"attr1": 20, "attr2": "nothing"}, (1, 2): {"attr2": 3}}
-        >>> eg.set_edge_attributes(G, attrs)
-        >>> G[0][1]["attr1"]
-        20
-        >>> G[0][1]["attr2"]
-        'nothing'
-        >>> G[1][2]["attr2"]
-        3
-
-    Note that if the dict contains edges that are not in `G`, they are
-    silently ignored::
-
-        >>> G = eg.Graph([(0, 1)])
-        >>> eg.set_edge_attributes(G, {(1, 2): {"weight": 2.0}})
-        >>> (1, 2) in G.edges()
-        False
-
-    """
-    if name is not None:
-        # `values` does not contain attribute names
-        try:
-            # if `values` is a dict using `.items()` => {edge: value}
-            if G.is_multigraph():
-                for (u, v, key), value in values.items():
-                    try:
-                        G[u][v][key][name] = value
-                    except KeyError:
-                        pass
-            else:
-                for (u, v), value in values.items():
-                    try:
-                        G[u][v][name] = value
-                    except KeyError:
-                        pass
-        except AttributeError:
-            # treat `values` as a constant
-            for u, v, data in G.edges:
-                data[name] = values
-    else:
-        # `values` consists of doct-of-dict {edge: {attr: value}} shape
-        if G.is_multigraph():
-            for (u, v, key), d in values.items():
-                try:
-                    G[u][v][key].update(d)
-                except KeyError:
-                    pass
-        else:
-            for (u, v), d in values.items():
-                try:
-                    G[u][v].update(d)
-                except KeyError:
-                    pass
-
-
-def add_path(G_to_add_to, nodes_for_path, **attr):
-    """Add a path to the Graph G_to_add_to.
-
-    Parameters
-    ----------
-    G_to_add_to : graph
-        A EasyGraph graph
-    nodes_for_path : iterable container
-        A container of nodes.  A path will be constructed from
-        the nodes (in order) and added to the graph.
-    attr : keyword arguments, optional (default= no attributes)
-        Attributes to add to every edge in path.
-
-    See Also
-    --------
-    add_star, add_cycle
-
-    Examples
-    --------
-    >>> G = eg.Graph()
-    >>> eg.add_path(G, [0, 1, 2, 3])
-    >>> eg.add_path(G, [10, 11, 12], weight=7)
-    """
-    nlist = iter(nodes_for_path)
-    try:
-        first_node = next(nlist)
-    except StopIteration:
-        return
-    G_to_add_to.add_node(first_node)
-    G_to_add_to.add_edges_from(pairwise(chain((first_node,), nlist)), **attr)
-
-
-def set_node_attributes(G, values, name=None):
-    """Sets node attributes from a given value or dictionary of values.
-
-    .. Warning:: The call order of arguments `values` and `name`
-        switched between v1.x & v2.x.
-
-    Parameters
-    ----------
-    G : EasyGraph Graph
-
-    values : scalar value, dict-like
-        What the node attribute should be set to.  If `values` is
-        not a dictionary, then it is treated as a single attribute value
-        that is then applied to every node in `G`.  This means that if
-        you provide a mutable object, like a list, updates to that object
-        will be reflected in the node attribute for every node.
-        The attribute name will be `name`.
-
-        If `values` is a dict or a dict of dict, it should be keyed
-        by node to either an attribute value or a dict of attribute key/value
-        pairs used to update the node's attributes.
-
-    name : string (optional, default=None)
-        Name of the node attribute to set if values is a scalar.
-
-    Examples
-    --------
-    After computing some property of the nodes of a graph, you may want
-    to assign a node attribute to store the value of that property for
-    each node::
-
-        >>> G = eg.path_graph(3)
-        >>> bb = eg.betweenness_centrality(G)
-        >>> isinstance(bb, dict)
-        True
-        >>> eg.set_node_attributes(G, bb, "betweenness")
-        >>> G.nodes[1]["betweenness"]
-        1.0
-
-    If you provide a list as the second argument, updates to the list
-    will be reflected in the node attribute for each node::
-
-        >>> G = eg.path_graph(3)
-        >>> labels = []
-        >>> eg.set_node_attributes(G, labels, "labels")
-        >>> labels.append("foo")
-        >>> G.nodes[0]["labels"]
-        ['foo']
-        >>> G.nodes[1]["labels"]
-        ['foo']
-        >>> G.nodes[2]["labels"]
-        ['foo']
-
-    If you provide a dictionary of dictionaries as the second argument,
-    the outer dictionary is assumed to be keyed by node to an inner
-    dictionary of node attributes for that node::
-
-        >>> G = eg.path_graph(3)
-        >>> attrs = {0: {"attr1": 20, "attr2": "nothing"}, 1: {"attr2": 3}}
-        >>> eg.set_node_attributes(G, attrs)
-        >>> G.nodes[0]["attr1"]
-        20
-        >>> G.nodes[0]["attr2"]
-        'nothing'
-        >>> G.nodes[1]["attr2"]
-        3
-        >>> G.nodes[2]
-        {}
-
-    Note that if the dictionary contains nodes that are not in `G`, the
-    values are silently ignored::
-
-        >>> G = eg.Graph()
-        >>> G.add_node(0)
-        >>> eg.set_node_attributes(G, {0: "red", 1: "blue"}, name="color")
-        >>> G.nodes[0]["color"]
-        'red'
-        >>> 1 in G.nodes
-        False
-
-    """
-    # Set node attributes based on type of `values`
-    if name is not None:  # `values` must not be a dict of dict
-        try:  # `values` is a dict
-            for n, v in values.items():
-                try:
-                    G.nodes[n][name] = values[n]
-                except KeyError:
-                    pass
-        except AttributeError:  # `values` is a constant
-            for n in G:
-                G.nodes[n][name] = values
-    else:  # `values` must be dict of dict
-        for n, d in values.items():
-            try:
-                G.nodes[n].update(d)
-            except KeyError:
-                pass
-
-
-def topological_generations(G):
-    if not G.is_directed():
-        raise AssertionError("Topological sort not defined on undirected graphs.")
-    indegree_map = {v: d for v, d in G.in_degree() if d > 0}
-    zero_indegree = [v for v, d in G.in_degree() if d == 0]
-    while zero_indegree:
-        this_generation = zero_indegree
-        zero_indegree = []
-        for node in this_generation:
-            if node not in G:
-                raise RuntimeError("Graph changed during iteration")
-            for child in G.neighbors(node):
-                try:
-                    indegree_map[child] -= 1
-                except KeyError as err:
-                    raise RuntimeError("Graph changed during iteration") from err
-                if indegree_map[child] == 0:
-                    zero_indegree.append(child)
-                    del indegree_map[child]
-        yield this_generation
-
-    if indegree_map:
-        raise AssertionError("Graph contains a cycle or graph changed during iteration")
-
-
-def topological_sort(G):
-    for generation in eg.topological_generations(G):
-        yield from generation
-
-
-def number_of_selfloops(G):
-    """Returns the number of selfloop edges.
-
-    A selfloop edge has the same node at both ends.
-
-    Returns
-    -------
-    nloops : int
-        The number of selfloops.
-
-    See Also
-    --------
-    nodes_with_selfloops, selfloop_edges
-
-    Examples
-    --------
-    >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
-    >>> G.add_edge(1, 1)
-    >>> G.add_edge(1, 2)
-    >>> eg.number_of_selfloops(G)
-    1
-    """
-    return sum(1 for _ in eg.selfloop_edges(G))
-
-
-def selfloop_edges(G, data=False, keys=False, default=None):
-    """Returns an iterator over selfloop edges.
-
-    A selfloop edge has the same node at both ends.
-
-    Parameters
-    ----------
-    G : graph
-        A EasyGraph graph.
-    data : string or bool, optional (default=False)
-        Return selfloop edges as two tuples (u, v) (data=False)
-        or three-tuples (u, v, datadict) (data=True)
-        or three-tuples (u, v, datavalue) (data='attrname')
-    keys : bool, optional (default=False)
-        If True, return edge keys with each edge.
-    default : value, optional (default=None)
-        Value used for edges that don't have the requested attribute.
-        Only relevant if data is not True or False.
-
-    Returns
-    -------
-    edgeiter : iterator over edge tuples
-        An iterator over all selfloop edges.
-
-    See Also
-    --------
-    nodes_with_selfloops, number_of_selfloops
-
-    Examples
-    --------
-    >>> G = eg.MultiGraph()  # or Graph, DiGraph, MultiDiGraph, etc
-    >>> ekey = G.add_edge(1, 1)
-    >>> ekey = G.add_edge(1, 2)
-    >>> list(eg.selfloop_edges(G))
-    [(1, 1)]
-    >>> list(eg.selfloop_edges(G, data=True))
-    [(1, 1, {})]
-    >>> list(eg.selfloop_edges(G, keys=True))
-    [(1, 1, 0)]
-    >>> list(eg.selfloop_edges(G, keys=True, data=True))
-    [(1, 1, 0, {})]
-    """
-    if data is True:
-        if G.is_multigraph():
-            if keys is True:
-                return (
-                    (n, n, k, d)
-                    for n, nbrs in G.adj.items()
-                    if n in nbrs
-                    for k, d in nbrs[n].items()
-                )
-            else:
-                return (
-                    (n, n, d)
-                    for n, nbrs in G.adj.items()
-                    if n in nbrs
-                    for d in nbrs[n].values()
-                )
-        else:
-            return ((n, n, nbrs[n]) for n, nbrs in G.adj.items() if n in nbrs)
-    elif data is not False:
-        if G.is_multigraph():
-            if keys is True:
-                return (
-                    (n, n, k, d.get(data, default))
-                    for n, nbrs in G.adj.items()
-                    if n in nbrs
-                    for k, d in nbrs[n].items()
-                )
-            else:
-                return (
-                    (n, n, d.get(data, default))
-                    for n, nbrs in G.adj.items()
-                    if n in nbrs
-                    for d in nbrs[n].values()
-                )
-        else:
-            return (
-                (n, n, nbrs[n].get(data, default))
-                for n, nbrs in G.adj.items()
-                if n in nbrs
-            )
-    else:
-        if G.is_multigraph():
-            if keys is True:
-                return (
-                    (n, n, k) for n, nbrs in G.adj.items() if n in nbrs for k in nbrs[n]
-                )
-            else:
-                return (
-                    (n, n)
-                    for n, nbrs in G.adj.items()
-                    if n in nbrs
-                    for i in range(len(nbrs[n]))  # for easy edge removal (#4068)
-                )
-        else:
-            return ((n, n) for n, nbrs in G.adj.items() if n in nbrs)
-
-
-@hybrid("cpp_density")
-def density(G):
-    r"""Returns the density of a graph.
-
-    The density for undirected graphs is
-
-    .. math::
-
-       d = \frac{2m}{n(n-1)},
-
-    and for directed graphs is
-
-    .. math::
-
-       d = \frac{m}{n(n-1)},
-
-    where `n` is the number of nodes and `m`  is the number of edges in `G`.
-
-    Notes
-    -----
-    The density is 0 for a graph without edges and 1 for a complete graph.
-    The density of multigraphs can be higher than 1.
-
-    Self loops are counted in the total number of edges so graphs with self
-    loops can have density higher than 1.
-    """
-    n = G.number_of_nodes()
-    m = G.number_of_edges()
-    if m == 0 or n <= 1:
-        return 0
-    d = m / (n * (n - 1))
-    if not G.is_directed():
-        d *= 2
-    return d
+from itertools import chain
+
+import easygraph as eg
+
+from easygraph.utils import *
+
+
+__all__ = [
+    "set_edge_attributes",
+    "add_path",
+    "set_node_attributes",
+    "selfloop_edges",
+    "topological_sort",
+    "number_of_selfloops",
+    "density",
+]
+
+
+def set_edge_attributes(G, values, name=None):
+    """Sets edge attributes from a given value or dictionary of values.
+
+    .. Warning:: The call order of arguments `values` and `name`
+        switched between v1.x & v2.x.
+
+    Parameters
+    ----------
+    G : EasyGraph Graph
+
+    values : scalar value, dict-like
+        What the edge attribute should be set to.  If `values` is
+        not a dictionary, then it is treated as a single attribute value
+        that is then applied to every edge in `G`.  This means that if
+        you provide a mutable object, like a list, updates to that object
+        will be reflected in the edge attribute for each edge.  The attribute
+        name will be `name`.
+
+        If `values` is a dict or a dict of dict, it should be keyed
+        by edge tuple to either an attribute value or a dict of attribute
+        key/value pairs used to update the edge's attributes.
+        For multigraphs, the edge tuples must be of the form ``(u, v, key)``,
+        where `u` and `v` are nodes and `key` is the edge key.
+        For non-multigraphs, the keys must be tuples of the form ``(u, v)``.
+
+    name : string (optional, default=None)
+        Name of the edge attribute to set if values is a scalar.
+
+    Examples
+    --------
+    After computing some property of the edges of a graph, you may want
+    to assign a edge attribute to store the value of that property for
+    each edge::
+
+        >>> G = eg.path_graph(3)
+        >>> bb = eg.edge_betweenness_centrality(G, normalized=False)
+        >>> eg.set_edge_attributes(G, bb, "betweenness")
+        >>> G.edges[1, 2]["betweenness"]
+        2.0
+
+    If you provide a list as the second argument, updates to the list
+    will be reflected in the edge attribute for each edge::
+
+        >>> labels = []
+        >>> eg.set_edge_attributes(G, labels, "labels")
+        >>> labels.append("foo")
+        >>> G.edges[0, 1]["labels"]
+        ['foo']
+        >>> G.edges[1, 2]["labels"]
+        ['foo']
+
+    If you provide a dictionary of dictionaries as the second argument,
+    the entire dictionary will be used to update edge attributes::
+
+        >>> G = eg.path_graph(3)
+        >>> attrs = {(0, 1): {"attr1": 20, "attr2": "nothing"}, (1, 2): {"attr2": 3}}
+        >>> eg.set_edge_attributes(G, attrs)
+        >>> G[0][1]["attr1"]
+        20
+        >>> G[0][1]["attr2"]
+        'nothing'
+        >>> G[1][2]["attr2"]
+        3
+
+    Note that if the dict contains edges that are not in `G`, they are
+    silently ignored::
+
+        >>> G = eg.Graph([(0, 1)])
+        >>> eg.set_edge_attributes(G, {(1, 2): {"weight": 2.0}})
+        >>> (1, 2) in G.edges()
+        False
+
+    """
+    if name is not None:
+        # `values` does not contain attribute names
+        try:
+            # if `values` is a dict using `.items()` => {edge: value}
+            if G.is_multigraph():
+                for (u, v, key), value in values.items():
+                    try:
+                        G[u][v][key][name] = value
+                    except KeyError:
+                        pass
+            else:
+                for (u, v), value in values.items():
+                    try:
+                        G[u][v][name] = value
+                    except KeyError:
+                        pass
+        except AttributeError:
+            # treat `values` as a constant
+            for u, v, data in G.edges:
+                data[name] = values
+    else:
+        # `values` consists of doct-of-dict {edge: {attr: value}} shape
+        if G.is_multigraph():
+            for (u, v, key), d in values.items():
+                try:
+                    G[u][v][key].update(d)
+                except KeyError:
+                    pass
+        else:
+            for (u, v), d in values.items():
+                try:
+                    G[u][v].update(d)
+                except KeyError:
+                    pass
+
+
+def add_path(G_to_add_to, nodes_for_path, **attr):
+    """Add a path to the Graph G_to_add_to.
+
+    Parameters
+    ----------
+    G_to_add_to : graph
+        A EasyGraph graph
+    nodes_for_path : iterable container
+        A container of nodes.  A path will be constructed from
+        the nodes (in order) and added to the graph.
+    attr : keyword arguments, optional (default= no attributes)
+        Attributes to add to every edge in path.
+
+    See Also
+    --------
+    add_star, add_cycle
+
+    Examples
+    --------
+    >>> G = eg.Graph()
+    >>> eg.add_path(G, [0, 1, 2, 3])
+    >>> eg.add_path(G, [10, 11, 12], weight=7)
+    """
+    nlist = iter(nodes_for_path)
+    try:
+        first_node = next(nlist)
+    except StopIteration:
+        return
+    G_to_add_to.add_node(first_node)
+    G_to_add_to.add_edges_from(pairwise(chain((first_node,), nlist)), **attr)
+
+
+def set_node_attributes(G, values, name=None):
+    """Sets node attributes from a given value or dictionary of values.
+
+    .. Warning:: The call order of arguments `values` and `name`
+        switched between v1.x & v2.x.
+
+    Parameters
+    ----------
+    G : EasyGraph Graph
+
+    values : scalar value, dict-like
+        What the node attribute should be set to.  If `values` is
+        not a dictionary, then it is treated as a single attribute value
+        that is then applied to every node in `G`.  This means that if
+        you provide a mutable object, like a list, updates to that object
+        will be reflected in the node attribute for every node.
+        The attribute name will be `name`.
+
+        If `values` is a dict or a dict of dict, it should be keyed
+        by node to either an attribute value or a dict of attribute key/value
+        pairs used to update the node's attributes.
+
+    name : string (optional, default=None)
+        Name of the node attribute to set if values is a scalar.
+
+    Examples
+    --------
+    After computing some property of the nodes of a graph, you may want
+    to assign a node attribute to store the value of that property for
+    each node::
+
+        >>> G = eg.path_graph(3)
+        >>> bb = eg.betweenness_centrality(G)
+        >>> isinstance(bb, dict)
+        True
+        >>> eg.set_node_attributes(G, bb, "betweenness")
+        >>> G.nodes[1]["betweenness"]
+        1.0
+
+    If you provide a list as the second argument, updates to the list
+    will be reflected in the node attribute for each node::
+
+        >>> G = eg.path_graph(3)
+        >>> labels = []
+        >>> eg.set_node_attributes(G, labels, "labels")
+        >>> labels.append("foo")
+        >>> G.nodes[0]["labels"]
+        ['foo']
+        >>> G.nodes[1]["labels"]
+        ['foo']
+        >>> G.nodes[2]["labels"]
+        ['foo']
+
+    If you provide a dictionary of dictionaries as the second argument,
+    the outer dictionary is assumed to be keyed by node to an inner
+    dictionary of node attributes for that node::
+
+        >>> G = eg.path_graph(3)
+        >>> attrs = {0: {"attr1": 20, "attr2": "nothing"}, 1: {"attr2": 3}}
+        >>> eg.set_node_attributes(G, attrs)
+        >>> G.nodes[0]["attr1"]
+        20
+        >>> G.nodes[0]["attr2"]
+        'nothing'
+        >>> G.nodes[1]["attr2"]
+        3
+        >>> G.nodes[2]
+        {}
+
+    Note that if the dictionary contains nodes that are not in `G`, the
+    values are silently ignored::
+
+        >>> G = eg.Graph()
+        >>> G.add_node(0)
+        >>> eg.set_node_attributes(G, {0: "red", 1: "blue"}, name="color")
+        >>> G.nodes[0]["color"]
+        'red'
+        >>> 1 in G.nodes
+        False
+
+    """
+    # Set node attributes based on type of `values`
+    if name is not None:  # `values` must not be a dict of dict
+        try:  # `values` is a dict
+            for n, v in values.items():
+                try:
+                    G.nodes[n][name] = values[n]
+                except KeyError:
+                    pass
+        except AttributeError:  # `values` is a constant
+            for n in G:
+                G.nodes[n][name] = values
+    else:  # `values` must be dict of dict
+        for n, d in values.items():
+            try:
+                G.nodes[n].update(d)
+            except KeyError:
+                pass
+
+
+def topological_generations(G):
+    if not G.is_directed():
+        raise AssertionError("Topological sort not defined on undirected graphs.")
+    indegree_map = {v: d for v, d in G.in_degree() if d > 0}
+    zero_indegree = [v for v, d in G.in_degree() if d == 0]
+    while zero_indegree:
+        this_generation = zero_indegree
+        zero_indegree = []
+        for node in this_generation:
+            if node not in G:
+                raise RuntimeError("Graph changed during iteration")
+            for child in G.neighbors(node):
+                try:
+                    indegree_map[child] -= 1
+                except KeyError as err:
+                    raise RuntimeError("Graph changed during iteration") from err
+                if indegree_map[child] == 0:
+                    zero_indegree.append(child)
+                    del indegree_map[child]
+        yield this_generation
+
+    if indegree_map:
+        raise AssertionError("Graph contains a cycle or graph changed during iteration")
+
+
+def topological_sort(G):
+    for generation in eg.topological_generations(G):
+        yield from generation
+
+
+def number_of_selfloops(G):
+    """Returns the number of selfloop edges.
+
+    A selfloop edge has the same node at both ends.
+
+    Returns
+    -------
+    nloops : int
+        The number of selfloops.
+
+    See Also
+    --------
+    nodes_with_selfloops, selfloop_edges
+
+    Examples
+    --------
+    >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
+    >>> G.add_edge(1, 1)
+    >>> G.add_edge(1, 2)
+    >>> eg.number_of_selfloops(G)
+    1
+    """
+    return sum(1 for _ in eg.selfloop_edges(G))
+
+
+def selfloop_edges(G, data=False, keys=False, default=None):
+    """Returns an iterator over selfloop edges.
+
+    A selfloop edge has the same node at both ends.
+
+    Parameters
+    ----------
+    G : graph
+        A EasyGraph graph.
+    data : string or bool, optional (default=False)
+        Return selfloop edges as two tuples (u, v) (data=False)
+        or three-tuples (u, v, datadict) (data=True)
+        or three-tuples (u, v, datavalue) (data='attrname')
+    keys : bool, optional (default=False)
+        If True, return edge keys with each edge.
+    default : value, optional (default=None)
+        Value used for edges that don't have the requested attribute.
+        Only relevant if data is not True or False.
+
+    Returns
+    -------
+    edgeiter : iterator over edge tuples
+        An iterator over all selfloop edges.
+
+    See Also
+    --------
+    nodes_with_selfloops, number_of_selfloops
+
+    Examples
+    --------
+    >>> G = eg.MultiGraph()  # or Graph, DiGraph, MultiDiGraph, etc
+    >>> ekey = G.add_edge(1, 1)
+    >>> ekey = G.add_edge(1, 2)
+    >>> list(eg.selfloop_edges(G))
+    [(1, 1)]
+    >>> list(eg.selfloop_edges(G, data=True))
+    [(1, 1, {})]
+    >>> list(eg.selfloop_edges(G, keys=True))
+    [(1, 1, 0)]
+    >>> list(eg.selfloop_edges(G, keys=True, data=True))
+    [(1, 1, 0, {})]
+    """
+    if data is True:
+        if G.is_multigraph():
+            if keys is True:
+                return (
+                    (n, n, k, d)
+                    for n, nbrs in G.adj.items()
+                    if n in nbrs
+                    for k, d in nbrs[n].items()
+                )
+            else:
+                return (
+                    (n, n, d)
+                    for n, nbrs in G.adj.items()
+                    if n in nbrs
+                    for d in nbrs[n].values()
+                )
+        else:
+            return ((n, n, nbrs[n]) for n, nbrs in G.adj.items() if n in nbrs)
+    elif data is not False:
+        if G.is_multigraph():
+            if keys is True:
+                return (
+                    (n, n, k, d.get(data, default))
+                    for n, nbrs in G.adj.items()
+                    if n in nbrs
+                    for k, d in nbrs[n].items()
+                )
+            else:
+                return (
+                    (n, n, d.get(data, default))
+                    for n, nbrs in G.adj.items()
+                    if n in nbrs
+                    for d in nbrs[n].values()
+                )
+        else:
+            return (
+                (n, n, nbrs[n].get(data, default))
+                for n, nbrs in G.adj.items()
+                if n in nbrs
+            )
+    else:
+        if G.is_multigraph():
+            if keys is True:
+                return (
+                    (n, n, k) for n, nbrs in G.adj.items() if n in nbrs for k in nbrs[n]
+                )
+            else:
+                return (
+                    (n, n)
+                    for n, nbrs in G.adj.items()
+                    if n in nbrs
+                    for i in range(len(nbrs[n]))  # for easy edge removal (#4068)
+                )
+        else:
+            return ((n, n) for n, nbrs in G.adj.items() if n in nbrs)
+
+
+@hybrid("cpp_density")
+def density(G):
+    r"""Returns the density of a graph.
+
+    The density for undirected graphs is
+
+    .. math::
+
+       d = \frac{2m}{n(n-1)},
+
+    and for directed graphs is
+
+    .. math::
+
+       d = \frac{m}{n(n-1)},
+
+    where `n` is the number of nodes and `m`  is the number of edges in `G`.
+
+    Notes
+    -----
+    The density is 0 for a graph without edges and 1 for a complete graph.
+    The density of multigraphs can be higher than 1.
+
+    Self loops are counted in the total number of edges so graphs with self
+    loops can have density higher than 1.
+    """
+    n = G.number_of_nodes()
+    m = G.number_of_edges()
+    if m == 0 or n <= 1:
+        return 0
+    d = m / (n * (n - 1))
+    if not G.is_directed():
+        d *= 2
+    return d
```

## easygraph/classes/__init__.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-# try:
-from .base import BaseHypergraph
-from .base import load_structure
-from .directed_graph import DiGraph
-from .directed_graph import DiGraphC
-from .directed_multigraph import MultiDiGraph
-from .graph import Graph
-from .graph import GraphC
-from .graphviews import *
-from .hypergraph import Hypergraph
-from .multigraph import MultiGraph
-from .operation import *
-
-
-# except:
-#     print(
-#         "Warning raise in module:classes. Please install Pytorch before you use"
-#         " functions related to Hypergraph"
-#     )
+# try:
+from .base import BaseHypergraph
+from .base import load_structure
+from .directed_graph import DiGraph
+from .directed_graph import DiGraphC
+from .directed_multigraph import MultiDiGraph
+from .graph import Graph
+from .graph import GraphC
+from .graphviews import *
+from .hypergraph import Hypergraph
+from .multigraph import MultiGraph
+from .operation import *
+
+
+# except:
+#     print(
+#         "Warning raise in module:classes. Please install Pytorch before you use"
+#         " functions related to Hypergraph"
+#     )
```

## easygraph/classes/tests/test_multigraph.py

 * *Ordering differences only*

```diff
@@ -1,61 +1,61 @@
-import easygraph as eg
-import pytest
-
-
-class TestMultiGraph:
-    def setup_method(self):
-        self.Graph = eg.MultiGraph
-        # build K3
-        ed1, ed2, ed3 = ({0: {}}, {0: {}}, {0: {}})
-        self.k3adj = {0: {1: ed1, 2: ed2}, 1: {0: ed1, 2: ed3}, 2: {0: ed2, 1: ed3}}
-        self.k3edges = [(0, 1), (0, 2), (1, 2)]
-        self.k3nodes = [0, 1, 2]
-        self.K3 = self.Graph()
-        self.K3._adj = self.k3adj
-        self.K3._node = {}
-        self.K3._node[0] = {}
-        self.K3._node[1] = {}
-        self.K3._node[2] = {}
-
-    def test_data_input(self):
-        G = self.Graph({1: [2], 2: [1]}, name="test")
-        assert G.name == "test"
-        expected = [(1, {2: {0: {}}}), (2, {1: {0: {}}})]
-        assert sorted(G.adj.items()) == expected
-
-    def test_has_edge(self):
-        G = self.K3
-        assert G.has_edge(0, 1)
-        assert not G.has_edge(0, -1)
-        assert G.has_edge(0, 1, 0)
-        assert not G.has_edge(0, 1, 1)
-
-    def test_get_edge_data(self):
-        G = self.K3
-        assert G.get_edge_data(0, 1) == {0: {}}
-        assert G[0][1] == {0: {}}
-        assert G[0][1][0] == {}
-        assert G.get_edge_data(10, 20) is None
-        assert G.get_edge_data(0, 1, 0) == {}
-
-    def test_data_multigraph_input(self):
-        # standard case with edge keys and edge data
-        edata0 = dict(w=200, s="foo")
-        edata1 = dict(w=201, s="bar")
-        keydict = {0: edata0, 1: edata1}
-        dododod = {"a": {"b": keydict}}
-
-        multiple_edge = [("a", "b", 0, edata0), ("a", "b", 1, edata1)]
-        single_edge = [("a", "b", 0, keydict)]
-
-        G = self.Graph(dododod, multigraph_input=None)
-        assert list(G.edges) == multiple_edge
-        G = self.Graph(dododod, multigraph_input=False)
-        assert list(G.edges) == single_edge
-
-    def test_remove_node(self):
-        G = self.K3
-        G.remove_node(0)
-        assert G.adj == {1: {2: {0: {}}}, 2: {1: {0: {}}}}
-        with pytest.raises(eg.EasyGraphError):
-            G.remove_node(-1)
+import easygraph as eg
+import pytest
+
+
+class TestMultiGraph:
+    def setup_method(self):
+        self.Graph = eg.MultiGraph
+        # build K3
+        ed1, ed2, ed3 = ({0: {}}, {0: {}}, {0: {}})
+        self.k3adj = {0: {1: ed1, 2: ed2}, 1: {0: ed1, 2: ed3}, 2: {0: ed2, 1: ed3}}
+        self.k3edges = [(0, 1), (0, 2), (1, 2)]
+        self.k3nodes = [0, 1, 2]
+        self.K3 = self.Graph()
+        self.K3._adj = self.k3adj
+        self.K3._node = {}
+        self.K3._node[0] = {}
+        self.K3._node[1] = {}
+        self.K3._node[2] = {}
+
+    def test_data_input(self):
+        G = self.Graph({1: [2], 2: [1]}, name="test")
+        assert G.name == "test"
+        expected = [(1, {2: {0: {}}}), (2, {1: {0: {}}})]
+        assert sorted(G.adj.items()) == expected
+
+    def test_has_edge(self):
+        G = self.K3
+        assert G.has_edge(0, 1)
+        assert not G.has_edge(0, -1)
+        assert G.has_edge(0, 1, 0)
+        assert not G.has_edge(0, 1, 1)
+
+    def test_get_edge_data(self):
+        G = self.K3
+        assert G.get_edge_data(0, 1) == {0: {}}
+        assert G[0][1] == {0: {}}
+        assert G[0][1][0] == {}
+        assert G.get_edge_data(10, 20) is None
+        assert G.get_edge_data(0, 1, 0) == {}
+
+    def test_data_multigraph_input(self):
+        # standard case with edge keys and edge data
+        edata0 = dict(w=200, s="foo")
+        edata1 = dict(w=201, s="bar")
+        keydict = {0: edata0, 1: edata1}
+        dododod = {"a": {"b": keydict}}
+
+        multiple_edge = [("a", "b", 0, edata0), ("a", "b", 1, edata1)]
+        single_edge = [("a", "b", 0, keydict)]
+
+        G = self.Graph(dododod, multigraph_input=None)
+        assert list(G.edges) == multiple_edge
+        G = self.Graph(dododod, multigraph_input=False)
+        assert list(G.edges) == single_edge
+
+    def test_remove_node(self):
+        G = self.K3
+        G.remove_node(0)
+        assert G.adj == {1: {2: {0: {}}}, 2: {1: {0: {}}}}
+        with pytest.raises(eg.EasyGraphError):
+            G.remove_node(-1)
```

## easygraph/classes/tests/test_hypergraph.py

 * *Ordering differences only*

```diff
@@ -1,1001 +1,1001 @@
-import sys
-
-from copy import deepcopy
-
-import easygraph as eg
-import pytest
-
-
-@pytest.fixture()
-def g1():
-    e_list = [(0, 1, 2, 5), (0, 1), (2, 3, 4), (3, 2, 4)]
-    g = eg.Hypergraph(6, e_list=e_list)
-    return g
-
-
-@pytest.fixture()
-def g2():
-    e_list = [(1, 2, 3), (0, 1, 3), (0, 1), (2, 4, 3), (2, 3)]
-    e_weight = [0.5, 1, 0.5, 1, 0.5]
-    g = eg.Hypergraph(5, e_list=e_list, e_weight=e_weight)
-    return g
-
-
-@pytest.fixture()
-def g3():
-    e_list = [[0, 1], [0, 1, 2], [2, 3, 4]]
-    e_weight = [1, 1, 1]
-    g = eg.Hypergraph(5, e_list=e_list, e_weight=e_weight)
-    return g
-
-
-def test_expansion(g3):
-    star_expansion_graph = g3.get_star_expansion()
-    node_clique_expansion_graph = g3.get_clique_expansion()
-    edge_clique_expansion_graph = g3.get_clique_expansion()
-    print(star_expansion_graph.edges)
-    print(node_clique_expansion_graph.edges)
-    print(edge_clique_expansion_graph.edges)
-
-
-def test_property(g1, g2):
-    assert g2.distance(1, 2) == 1
-    assert g2.diameter() == 2
-    assert g1.adjacency_matrix != None
-    assert g1.edge_adjacency_matrix != None
-    assert g2.adjacency_matrix != None
-    assert g2.edge_adjacency_matrix != None
-
-
-def test_save(g1, tmp_path):
-    from easygraph import load_structure
-
-    print("g1:", g1, g1.e)
-    g1.save(tmp_path / "g1")
-    g2 = load_structure(tmp_path / "g1")
-
-    for e1, e2 in zip(g1.e[0], g2.e[0]):
-        assert e1 == e2
-    for w1, w2 in zip(g1.e[1], g2.e[1]):
-        assert w1 == w2
-
-
-# test construction
-def test_from_feature_kNN():
-    import numpy as np
-    import scipy.spatial
-    import torch
-
-    ft = np.random.rand(32, 8)
-    cdist = scipy.spatial.distance.cdist(ft, ft)
-    tk_mat = np.argsort(cdist, axis=1)[:, :3]
-    hg = eg.Hypergraph.from_feature_kNN(torch.tensor(ft), k=3)
-    assert tuple(sorted(tk_mat[0].tolist())) in hg.e[0]
-    assert tuple(sorted(tk_mat[8].tolist())) in hg.e[0]
-    assert tuple(sorted(tk_mat[13].tolist())) in hg.e[0]
-    assert tuple(sorted(tk_mat[26].tolist())) in hg.e[0]
-
-
-def test_from_graph():
-    g = eg.Graph()
-    g.add_nodes(list(range(0, 5)))
-    g.add_edges(
-        [(0, 1), (0, 3), (1, 4), (2, 3), (3, 4)],
-        [
-            {"weight": 1.0},
-            {"weight": 1.0},
-            {"weight": 1.0},
-            {"weight": 1.0},
-            {"weight": 1.0},
-        ],
-    )
-    hg = eg.Hypergraph.from_graph(g)
-    assert hg.num_e == 5
-    assert (0, 1) in hg.e[0]
-    assert (1, 4) in hg.e[0]
-
-
-def test_from_graph_kHop():
-    g = eg.Graph()
-    g.add_nodes(range(0, 5))
-    g.add_edges(
-        [(0, 1), (0, 3), (1, 4), (2, 3)],
-        [{"weight": 1.0}, {"weight": 1.0}, {"weight": 1.0}, {"weight": 1.0}],
-    )
-    hg = eg.Hypergraph.from_graph_kHop(g, k=1)
-    assert hg.num_e == 5
-    assert (0, 1, 3) in hg.e[0]
-    assert (0, 1, 4) in hg.e[0]
-    assert (1, 4) in hg.e[0]
-    assert (2, 3) in hg.e[0]
-    assert (0, 2, 3) in hg.e[0]
-    hg = eg.Hypergraph.from_graph_kHop(g, k=2)
-    assert hg.num_e == 5
-    assert (0, 1, 3, 4) in hg.e[0]
-    hg = eg.Hypergraph.from_graph_kHop(g, k=2, only_kHop=True)
-    assert hg.num_e == 4
-
-
-# test representation
-def test_empty():
-    g = eg.Hypergraph(10)
-    assert g.num_v == 10
-    assert g.e == ([], [], [])
-
-
-def test_init(g1, g2):
-    assert g1.num_v == 6
-    assert g1.num_e == 3
-    assert g1.e[0] == [(0, 1, 2, 5), (0, 1), (2, 3, 4)]
-    assert g1.e[1] == [1, 1, 1]
-    assert g2.num_v == 5
-    assert g2.num_e == 5
-    assert g2.e[0] == [(1, 2, 3), (0, 1, 3), (0, 1), (2, 3, 4), (2, 3)]
-    assert g2.e[1] == [0.5, 1, 0.5, 1, 0.5]
-
-
-def test_clear(g1):
-    assert g1.num_e == 3
-    g1.clear()
-    assert g1.num_e == 0
-    assert g1.e == ([], [], [])
-
-
-def test_add_and_merge_hyperedges(g1):
-    assert g1.e[1] == [1, 1, 1]
-    print("g1:", g1, g1.e)
-    g1.add_hyperedges(e_list=[0, 1], e_weight=3, merge_op="mean")
-    assert g1.e[1] == [1, 2, 1]
-    assert g1.e[0] == [(0, 1, 2, 5), (0, 1), (2, 3, 4)]
-    g1.add_hyperedges([(2, 4, 3), (1, 0), (3, 4)], [1, 1, 1], merge_op="sum")
-    assert g1.e[0] == [(0, 1, 2, 5), (0, 1), (2, 3, 4), (3, 4)]
-    assert g1.e[1] == [1, 3, 2, 1]
-
-
-def test_add_hyperedges_from_feature_kNN(g1):
-    import numpy as np
-    import scipy.spatial
-    import torch
-
-    origin_e = deepcopy(g1.e[0])
-    ft = np.random.rand(6, 8)
-    cdist = scipy.spatial.distance.cdist(ft, ft)
-    tk_mat = np.argsort(cdist, axis=1)[:, :3]
-
-    g1.add_hyperedges_from_feature_kNN(torch.tensor(ft), k=3, group_name="knn")
-    assert tuple(sorted(tk_mat[0].tolist())) in g1.e_of_group("knn")[0]
-    assert tuple(sorted(tk_mat[3].tolist())) in g1.e_of_group("knn")[0]
-    assert tuple(sorted(tk_mat[4].tolist())) in g1.e_of_group("knn")[0]
-    assert tuple(sorted(tk_mat[5].tolist())) in g1.e_of_group("knn")[0]
-
-    for e in origin_e:
-        assert e in g1.e_of_group("main")[0]
-
-    for e in g1.e_of_group("main")[0]:
-        assert e in origin_e
-
-
-# def test_add_hyperedges_from_graph(g1):
-#     g = eg.graph_Gnm(6, 3)
-#     origin_e = deepcopy(g1.e[0])
-#
-#     g1.add_hyperedges_from_graph(g, group_name="graph")
-#     g_e = g.e[0]
-#     g1_e = g1.e_of_group("graph")[0]
-#
-#     for e in g_e:
-#         assert e in g1_e
-#
-#     for e in origin_e:
-#         assert e in g1.e_of_group("main")[0]
-#
-#     for e in g1.e[0]:
-#         assert e in origin_e or e in g_e
-
-
-def test_add_hyperedges_from_graph_kHop(g1):
-    g = eg.graph_Gnm(6, 5)
-
-    origin_e = deepcopy(g1.e[0])
-    for k in range(1, 3):
-        gg1 = deepcopy(g1)
-        gg1.add_hyperedges_from_graph_kHop(g, k=k, group_name="kHop")
-
-        khop = [[] for _ in range(6)]
-        for kk in range(k):
-            for v in range(6):
-                if kk == 0:
-                    khop[v] = g.nbr_v(v)
-                else:
-                    kk_hop_v = []
-                    for nbr in khop[v]:
-                        kk_hop_v += g.nbr_v(nbr)
-                    khop[v] += kk_hop_v
-                khop[v] = list(set(khop[v]))
-
-        for v in range(6):
-            edge = [v] + khop[v]
-            edge = tuple(set(sorted(edge)))
-            assert edge in gg1.e_of_group("kHop")[0]
-
-        gg2 = deepcopy(g1)
-        gg2.add_hyperedges_from_graph_kHop(g, k=k, group_name="kHop", only_kHop=True)
-
-        khop = [[] for _ in range(6)]
-        for kk in range(k):
-            for v in range(6):
-                if len(khop[v]) == 0:
-                    khop[v] = g.nbr_v(v)
-                else:
-                    kk_hop_v = []
-                    for nbr in khop[v]:
-                        kk_hop_v += g.nbr_v(nbr)
-                    khop[v] = kk_hop_v
-                khop[v] = list(set(khop[v]))
-
-        for v in range(6):
-            edge = [v] + khop[v]
-            edge = tuple(set(sorted(edge)))
-            assert edge in gg2.e_of_group("kHop")[0]
-
-        for e in origin_e:
-            assert e in gg1.e_of_group("main")[0]
-            assert e in gg2.e_of_group("main")[0]
-
-        for e in gg1.e_of_group("main")[0]:
-            assert e in origin_e
-        for e in gg2.e_of_group("main")[0]:
-            assert e in origin_e
-
-
-def test_remove_hyperedges(g1):
-    assert g1.e[0] == [(0, 1, 2, 5), (0, 1), (2, 3, 4)]
-    assert g1.e[1] == [1, 1, 1]
-    g1.remove_hyperedges([0, 1])
-    assert (0, 1) not in g1.e[0]
-    assert (0, 1, 5) not in g1.e[0]
-    g1.add_hyperedges([[0, 1, 5], [2, 3, 4]])
-    assert (0, 1, 5) in g1.e[0]
-    g1.remove_hyperedges([[0, 1, 5], (0, 1, 2, 5)])
-    assert (0, 1, 5) not in g1.e[0]
-    assert (0, 1, 2, 5) not in g1.e[0]
-    g1.clear()
-    assert g1.num_e == 0
-    assert g1.e == ([], [], [])
-
-
-# def test_remove_group(g1):
-#     origin_e = deepcopy(g1.e[0])
-#
-#     g1.add_hyperedges(([0, 1, 2, 5], [0, 1]), group_name="test")
-#     for e in origin_e:
-#         assert e in g1.e_of_group("main")[0]
-#     for e in g1.e_of_group("main")[0]:
-#         assert e in origin_e
-#
-#     # g1.remove_group("none")
-#
-#     g1.remove_group("test")
-#     assert "test" not in g1.group_names
-#
-#     for e in origin_e:
-#         assert e in g1.e_of_group("main")[0]
-#     for e in g1.e_of_group("main")[0]:
-#         assert e in origin_e
-#
-#     g1.remove_group("main")
-#
-#     assert len(g1.e[0]) == 0
-#     assert len(g1.e[1]) == 0
-#
-#
-# def test_add_and_remove_group(g1):
-#     assert g1.group_names == ["main"]
-#     g1.add_hyperedges([0, 2, 3], group_name="knn")
-#     assert len(g1.group_names) == 2
-#     assert "main" in g1.group_names
-#     assert "knn" in g1.group_names
-#     assert (0, 2, 3) in g1.e[0]
-#     assert (0, 2, 3) in g1.e_of_group("knn")[0]
-#     assert (0, 2, 3) not in g1.e_of_group("main")[0]
-#     g1.remove_hyperedges([0, 2, 3], group_name="knn")
-#     assert (0, 2, 3) not in g1.e[0]
-#     assert (0, 2, 3) not in g1.e_of_group("knn")[0]
-
-
-def test_deg(g1, g2):
-    assert g1.deg_v == [2, 2, 2, 1, 1, 1]
-    assert g1.deg_e == [4, 2, 3]
-    assert g2.deg_v == [2, 3, 3, 4, 1]
-    assert g2.deg_e == [3, 3, 2, 3, 2]
-
-
-# def test_deg_group(g1):
-#     assert g1.deg_v == [2, 2, 2, 1, 1, 1]
-#     assert g1.deg_e == [4, 2, 3]
-#     g1.add_hyperedges([0, 2], 1, group_name="knn")
-#     assert g1.deg_v == [3, 2, 3, 1, 1, 1]
-#     assert g1.deg_e == [4, 2, 3, 2]
-#     assert g1.deg_v_of_group("main") == [2, 2, 2, 1, 1, 1]
-#     assert g1.deg_e_of_group("main") == [4, 2, 3]
-#     assert g1.deg_v_of_group("knn") == [1, 0, 1, 0, 0, 0]
-#     assert g1.deg_e_of_group("knn") == [2]
-
-
-def test_nbr(g1, g2):
-    assert g1.nbr_v(0) == [0, 1, 2, 5]
-    assert g1.nbr_e(1) == [0, 1]
-    assert g2.nbr_v(2) == [0, 1]
-    assert g2.nbr_e(4) == [3]
-
-
-# def test_nbr_group(g1):
-#     print("g1:", g1.e, g1.v)
-#     assert g1.nbr_v(1) == [0, 1]
-#     assert g1.nbr_e(0) == [0, 1]
-#     g1.add_hyperedges([[0, 1]], group_name="knn")
-#     assert g1.nbr_v(1) == [0, 1]
-#     assert g1.nbr_e(1) == [0, 1, 3]
-#     assert g1.nbr_v_of_group(1, "main") == [0, 1]
-#     assert g1.nbr_e_of_group(2, "main") == [0, 2]
-#     assert g1.nbr_v_of_group(0, "knn") == [0, 1]
-#     assert g1.nbr_e_of_group(1, "knn") == [0]
-
-
-def test_clone(g1):
-    assert g1.num_v == 6
-    assert g1.num_e == 3
-    g1_clone = g1.clone()
-    g1_clone.add_hyperedges([0, 2], 1, group_name="knn")
-    assert g1.num_e == 3
-    assert g1_clone.num_e == 4
-
-
-# test deep learning
-def test_v2e_index(g1):
-    import torch
-
-    v2e_src = g1.v2e_src.view(-1, 1)
-    v2e_dst = g1.v2e_dst.view(-1, 1)
-
-    index = torch.cat((v2e_src, v2e_dst), dim=1)
-    index = index.numpy().tolist()
-    index = list(map(lambda x: tuple(x), index))
-
-    assert (0, 0) in index
-    assert (1, 0) in index
-    assert (2, 0) in index
-    assert (5, 0) in index
-    assert (0, 1) in index
-    assert (1, 1) in index
-    assert (2, 2) in index
-    assert (3, 2) in index
-    assert (4, 2) in index
-
-
-def test_v2e_index_group(g1):
-    import torch
-
-    v2e_src = g1.v2e_src_of_group("main").view(-1, 1)
-    v2e_dst = g1.v2e_dst_of_group("main").view(-1, 1)
-
-    index = torch.cat((v2e_src, v2e_dst), dim=1)
-    index = index.numpy().tolist()
-    index = list(map(lambda x: tuple(x), index))
-
-    assert (0, 0) in index
-    assert (1, 0) in index
-    assert (2, 0) in index
-    assert (5, 0) in index
-    assert (0, 1) in index
-    assert (1, 1) in index
-    assert (2, 2) in index
-    assert (3, 2) in index
-    assert (4, 2) in index
-
-
-def test_e2v_index(g1):
-    import torch
-
-    e2v_src = g1.e2v_src.view(-1, 1)
-    e2v_dst = g1.e2v_dst.view(-1, 1)
-
-    index = torch.cat((e2v_src, e2v_dst), dim=1)
-    index = index.numpy().tolist()
-    index = list(map(lambda x: tuple(x), index))
-
-    assert (0, 0) in index
-    assert (0, 1) in index
-    assert (0, 2) in index
-    assert (0, 5) in index
-    assert (1, 0) in index
-    assert (1, 1) in index
-    assert (2, 2) in index
-    assert (2, 3) in index
-    assert (2, 4) in index
-
-
-def test_e2v_index_group(g1):
-    import torch
-
-    e2v_src = g1.e2v_src_of_group("main").view(-1, 1)
-    e2v_dst = g1.e2v_dst_of_group("main").view(-1, 1)
-
-    index = torch.cat((e2v_src, e2v_dst), dim=1)
-    index = index.numpy().tolist()
-    index = list(map(lambda x: tuple(x), index))
-
-    assert (0, 0) in index
-    assert (0, 1) in index
-    assert (0, 2) in index
-    assert (0, 5) in index
-    assert (1, 0) in index
-    assert (1, 1) in index
-    assert (2, 2) in index
-    assert (2, 3) in index
-    assert (2, 4) in index
-
-
-def test_H(g1):
-    import torch
-
-    print("g1", g1.H.to_dense())
-    assert (
-        g1.H.to_dense().cpu()
-        == torch.tensor(
-            [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]]
-        )
-    ).all()
-
-
-# def test_H_group(g1):
-#     import torch
-#
-#     g1.add_hyperedges([0, 4, 5], group_name="knn")
-#     assert (
-#         g1.H.to_dense().cpu()
-#         == torch.tensor(
-#             [
-#                 [1, 1, 0, 1],
-#                 [1, 1, 0, 0],
-#                 [1, 0, 1, 0],
-#                 [0, 0, 1, 0],
-#                 [0, 0, 1, 1],
-#                 [1, 0, 0, 1],
-#             ]
-#         )
-#     ).all()
-#     assert (
-#         g1.H_of_group("main").to_dense().cpu()
-#         == torch.tensor(
-#             [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]]
-#         )
-#     ).all()
-#     assert (
-#         g1.H_of_group("knn").to_dense().cpu()
-#         == torch.tensor([[1], [0], [0], [0], [1], [1]])
-#     ).all()
-
-
-def test_H_T(g1):
-    import torch
-
-    assert (
-        g1.H_T.to_dense().cpu()
-        == torch.tensor(
-            [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]]
-        ).t()
-    ).all()
-
-
-# def test_H_T_group(g1):
-#     import torch
-#
-#     g1.add_hyperedges([0, 4, 5], group_name="knn")
-#     assert (
-#         g1.H_T.to_dense().cpu()
-#         == torch.tensor(
-#             [
-#                 [1, 1, 0, 1],
-#                 [1, 1, 0, 0],
-#                 [1, 0, 1, 0],
-#                 [0, 0, 1, 0],
-#                 [0, 0, 1, 1],
-#                 [1, 0, 0, 1],
-#             ]
-#         ).t()
-#     ).all()
-#     assert (
-#         g1.H_T_of_group("main").to_dense().cpu()
-#         == torch.tensor(
-#             [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]]
-#         ).t()
-#     ).all()
-#     assert (
-#         g1.H_T_of_group("knn").to_dense().cpu() == torch.tensor([[1, 0, 0, 0, 1, 1]])
-#     ).all()
-
-
-def test_W_e(g2):
-    import torch
-
-    assert (
-        g2.W_e.to_sparse_coo().cpu()._values() == torch.tensor([0.5, 1, 0.5, 1, 0.5])
-    ).all()
-
-
-# def test_W_e_group(g2):
-#     import torch
-#
-#     g2.add_hyperedges([0, 4, 5], group_name="knn")
-#     assert (g2.W_e.cpu()._values() == torch.tensor([0.5, 1, 0.5, 1, 0.5, 1])).all()
-#     assert (
-#         g2.W_e_of_group("main").cpu()._values() == torch.tensor([0.5, 1, 0.5, 1, 0.5])
-#     ).all()
-#     assert (g2.W_e_of_group("knn").cpu()._values() == torch.tensor([1])).all()
-
-
-def test_D(g1, g2):
-    import torch
-
-    assert (g1.D_v.cpu()._values() == torch.tensor([2, 2, 2, 1, 1, 1])).all()
-    assert (g1.D_e.to_sparse_coo().cpu()._values() == torch.tensor([4, 2, 3])).all()
-    assert (g2.D_v.cpu()._values() == torch.tensor([2, 3, 3, 4, 1])).all()
-    assert (
-        g2.D_e.to_sparse_coo().cpu()._values() == torch.tensor([3, 3, 2, 3, 2])
-    ).all()
-
-
-# def test_D_group(g1):
-#     import torch
-#
-#     assert (g1.D_v.cpu()._values() == torch.tensor([2, 2, 2, 1, 1, 1])).all()
-#     assert (g1.D_e.cpu()._values() == torch.tensor([4, 2, 3])).all()
-#     g1.add_hyperedges([[0, 2], [1, 2, 3]], group_name="knn")
-#     assert (g1.D_v.cpu()._values() == torch.tensor([3, 3, 4, 2, 1, 1])).all()
-#     assert (g1.D_e.cpu()._values() == torch.tensor([4, 2, 3, 2, 3])).all()
-#     assert (
-#         g1.D_v_of_group("main").cpu()._values() == torch.tensor([2, 2, 2, 1, 1, 1])
-#     ).all()
-#     assert (g1.D_e_of_group("main").cpu()._values() == torch.tensor([4, 2, 3])).all()
-#     assert (
-#         g1.D_v_of_group("knn").cpu()._values() == torch.tensor([1, 1, 2, 1, 0, 0])
-#     ).all()
-#     assert (g1.D_e_of_group("knn").cpu()._values() == torch.tensor([2, 3])).all()
-
-
-def test_D_neg(g1, g2):
-    import torch
-
-    # -1
-    assert (
-        g1.D_v_neg_1.to_sparse_coo().cpu()._values()
-        == torch.tensor([2, 2, 2, 1, 1, 1]) ** (-1.0)
-    ).all()
-    assert (
-        g1.D_e_neg_1.to_sparse_coo().cpu()._values()
-        == torch.tensor([4, 2, 3]) ** (-1.0)
-    ).all()
-    assert (
-        g2.D_v_neg_1.to_sparse_coo().cpu()._values()
-        == torch.tensor([2, 3, 3, 4, 1]) ** (-1.0)
-    ).all()
-    assert (
-        g2.D_e_neg_1.to_sparse_coo().cpu()._values()
-        == torch.tensor([3, 3, 2, 3, 2]) ** (-1.0)
-    ).all()
-    # -1/2
-    assert (
-        g1.D_v_neg_1_2.to_sparse_coo().cpu()._values()
-        == torch.tensor([2, 2, 2, 1, 1, 1]) ** (-0.5)
-    ).all()
-    assert (
-        g2.D_v_neg_1_2.to_sparse_coo().cpu()._values()
-        == torch.tensor([2, 3, 3, 4, 1]) ** (-0.5)
-    ).all()
-    # isolated vertex
-    g3 = eg.Hypergraph(num_v=3, e_list=[0, 1])
-    assert (
-        g3.D_v_neg_1.to_sparse_coo().cpu()._values() == torch.tensor([1, 1, 0])
-    ).all()
-
-
-# def test_D_neg_group(g1):
-#     import torch
-#
-#     # -1
-#     assert (
-#         g1.D_v_neg_1.cpu()._values() == torch.tensor([2, 2, 2, 1, 1, 1]) ** (-1.0)
-#     ).all()
-#     assert (g1.D_e_neg_1.cpu()._values() == torch.tensor([4, 2, 3]) ** (-1.0)).all()
-#     g1.add_hyperedges([[0, 2], [1, 2, 3]], group_name="knn")
-#     assert (
-#         g1.D_v_neg_1.cpu()._values() == torch.tensor([3, 3, 4, 2, 1, 1]) ** (-1.0)
-#     ).all()
-#     assert (
-#         g1.D_e_neg_1.cpu()._values() == torch.tensor([4, 2, 3, 2, 3]) ** (-1.0)
-#     ).all()
-#     assert (
-#         g1.D_v_neg_1_of_group("main").cpu()._values()
-#         == torch.tensor([2, 2, 2, 1, 1, 1]) ** (-1.0)
-#     ).all()
-#     assert (
-#         g1.D_e_neg_1_of_group("main").cpu()._values()
-#         == torch.tensor([4, 2, 3]) ** (-1.0)
-#     ).all()
-#     assert (
-#         g1.D_v_neg_1_of_group("knn").cpu()._values()
-#         == torch.tensor([1 / 1, 1 / 1, 1 / 2, 1 / 1, 0, 0])
-#     ).all()
-#     assert (
-#         g1.D_e_neg_1_of_group("knn").cpu()._values() == torch.tensor([2, 3]) ** (-1.0)
-#     ).all()
-#     # -1/2
-#     assert (
-#         g1.D_v_neg_1_2.cpu()._values() == torch.tensor([3, 3, 4, 2, 1, 1]) ** (-0.5)
-#     ).all()
-#     assert (
-#         g1.D_v_neg_1_2_of_group("main").cpu()._values()
-#         == torch.tensor([2, 2, 2, 1, 1, 1]) ** (-0.5)
-#     ).all()
-#     assert (
-#         g1.D_v_neg_1_2_of_group("knn").cpu()._values()
-#         == torch.tensor([1 ** (-0.5), 1 ** (-0.5), 2 ** (-0.5), 1 ** (-0.5), 0, 0])
-#     ).all()
-
-
-def test_N(g1, g2):
-    import torch
-
-    assert (g1.N_v(0).cpu() == torch.tensor([0, 1, 2, 5])).all()
-    assert (g1.N_e(2).cpu() == torch.tensor([0, 2])).all()
-    assert (g2.N_v(1).cpu() == torch.tensor([0, 1, 3])).all()
-    assert (g2.N_e(3).cpu() == torch.tensor([0, 1, 3, 4])).all()
-
-
-# def test_N_group(g1):
-#     import torch
-#
-#     assert (g1.N_v(1).cpu() == torch.tensor([0, 1])).all()
-#     assert (g1.N_e(1).cpu() == torch.tensor([0, 1])).all()
-#     g1.add_hyperedges([[0, 1], [1, 2]], group_name="knn")
-#     assert (g1.N_v(1).cpu() == torch.tensor([0, 1])).all()
-#     assert (g1.N_e(1).cpu() == torch.tensor([0, 1, 3, 4])).all()
-#     assert (g1.N_v_of_group(1, "main").cpu() == torch.tensor([0, 1])).all()
-#     assert (g1.N_e_of_group(2, "main").cpu() == torch.tensor([0, 2])).all()
-#     assert (g1.N_v_of_group(1, "knn").cpu() == torch.tensor([1, 2])).all()
-#     assert (g1.N_e_of_group(1, "knn").cpu() == torch.tensor([0, 1])).all()
-#
-
-
-@pytest.mark.skipif(
-    sys.version_info.major <= 3 and sys.version_info.minor < 7,
-    reason="python requires >= 3.7",
-)
-def test_L_HGNN(g1):
-    import torch
-
-    print("g1:", g1, g1.e)
-    H = g1.H.to_dense().cpu()
-    D_v_neg_1_2 = torch.diag(H.sum(dim=1).view(-1) ** (-0.5))
-    D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
-    W_e = g1.W_e.to_dense()
-    L_HGNN = D_v_neg_1_2 @ H @ W_e @ D_e_neg_1 @ H.t() @ D_v_neg_1_2
-    assert (L_HGNN == g1.L_HGNN.to_dense().cpu()).all()
-
-
-@pytest.mark.skipif(
-    sys.version_info.major <= 3 and sys.version_info.minor < 7,
-    reason="python requires >= 3.7",
-)
-# def test_L_HGNN_group(g1):
-#     import torch
-#
-#     g1.add_hyperedges([[0, 1]], group_name="knn")
-#     # all
-#     H = g1.H.to_dense().cpu()
-#     D_v_neg_1_2 = torch.diag(H.sum(dim=1).view(-1) ** (-0.5))
-#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
-#     W_e = g1.W_e.to_dense()
-#     L_HGNN = D_v_neg_1_2 @ H @ W_e @ D_e_neg_1 @ H.t() @ D_v_neg_1_2
-#     assert (L_HGNN == g1.L_HGNN.to_dense().cpu()).all()
-#     # main group
-#     H = g1.H_of_group("main").to_dense().cpu()
-#     D_v_neg_1_2 = torch.diag(H.sum(dim=1).view(-1) ** (-0.5))
-#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
-#     W_e = g1.W_e_of_group("main").to_dense()
-#     L_HGNN = D_v_neg_1_2 @ H @ W_e @ D_e_neg_1 @ H.t() @ D_v_neg_1_2
-#     assert (L_HGNN == g1.L_HGNN_of_group("main").to_dense().cpu()).all()
-#     # knn group
-#     H = g1.H_of_group("knn").to_dense().cpu()
-#     D_v_neg_1_2 = H.sum(dim=1).view(-1) ** (-0.5)
-#     D_v_neg_1_2[torch.isinf(D_v_neg_1_2)] = 0
-#     D_v_neg_1_2 = torch.diag(D_v_neg_1_2)
-#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
-#     W_e = g1.W_e_of_group("knn").to_dense()
-#     L_HGNN = D_v_neg_1_2 @ H @ W_e @ D_e_neg_1 @ H.t() @ D_v_neg_1_2
-#     assert (L_HGNN == g1.L_HGNN_of_group("knn").to_dense().cpu()).all()
-
-
-@pytest.mark.skipif(
-    sys.version_info.major <= 3 and sys.version_info.minor < 7,
-    reason="python requires >= 3.7",
-)
-def test_smoothing():
-    import torch
-
-    x = torch.rand(10, 5)
-    L = torch.rand(10, 10)
-    g = eg.Hypergraph(10)
-    lbd = 0.1
-    assert pytest.approx(g.smoothing(x, L, lbd)) == x + lbd * L @ x
-
-
-@pytest.mark.skipif(
-    sys.version_info.major <= 3 and sys.version_info.minor < 7,
-    reason="python requires >= 3.7",
-)
-def test_L_sym(g1):
-    import torch
-
-    H = g1.H.to_sparse_coo().to_dense().cpu()
-    D_v_neg_1_2 = torch.diag(H.sum(dim=1).view(-1) ** (-0.5))
-    D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
-    W_e = g1.W_e.to_dense()
-    L_sym = (
-        torch.eye(H.shape[0])
-        - D_v_neg_1_2.to_sparse_coo()
-        @ H.to_sparse_coo()
-        @ W_e
-        @ D_e_neg_1.to_sparse_coo()
-        @ H.t().to_sparse_coo()
-        @ D_v_neg_1_2.to_sparse_coo()
-    )
-    assert (L_sym == g1.L_sym.to_dense().cpu()).all()
-
-
-@pytest.mark.skipif(
-    sys.version_info.major <= 3 and sys.version_info.minor < 7,
-    reason="python requires >= 3.7",
-)
-# def test_L_sym_group(g1):
-#     import torch
-#
-#     g1.add_hyperedges([[0, 1]], group_name="knn")
-#     # all
-#     H = g1.H.to_dense().cpu()
-#     D_v_neg_1_2 = torch.diag(H.sum(dim=1).view(-1) ** (-0.5))
-#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
-#     W_e = g1.W_e.to_dense()
-#     L_sym = (
-#         torch.eye(H.shape[0]) - D_v_neg_1_2 @ H @ W_e @ D_e_neg_1 @ H.t() @ D_v_neg_1_2
-#     )
-#     assert (L_sym == g1.L_sym.to_dense().cpu()).all()
-#     # main group
-#     H = g1.H_of_group("main").to_dense().cpu()
-#     D_v_neg_1_2 = torch.diag(H.sum(dim=1).view(-1) ** (-0.5))
-#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
-#     W_e = g1.W_e_of_group("main").to_dense()
-#     L_sym = (
-#         torch.eye(H.shape[0]) - D_v_neg_1_2 @ H @ W_e @ D_e_neg_1 @ H.t() @ D_v_neg_1_2
-#     )
-#     assert (L_sym == g1.L_sym_of_group("main").to_dense().cpu()).all()
-#     # knn group
-#     H = g1.H_of_group("knn").to_dense().cpu()
-#     D_v_neg_1_2 = H.sum(dim=1).view(-1) ** (-0.5)
-#     D_v_neg_1_2[torch.isinf(D_v_neg_1_2)] = 0
-#     D_v_neg_1_2 = torch.diag(D_v_neg_1_2)
-#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
-#     W_e = g1.W_e_of_group("knn").to_dense()
-#     L_sym = (
-#         torch.eye(H.shape[0]) - D_v_neg_1_2 @ H @ W_e @ D_e_neg_1 @ H.t() @ D_v_neg_1_2
-#     )
-#     assert (L_sym == g1.L_sym_of_group("knn").to_dense().cpu()).all()
-
-
-@pytest.mark.skipif(
-    sys.version_info.major <= 3 and sys.version_info.minor < 7,
-    reason="python requires >= 3.7",
-)
-# def test_L_rw(g1):
-#     import torch
-#
-#     H = g1.H.to_dense().cpu()
-#     D_v_neg_1 = torch.diag(H.sum(dim=1).view(-1) ** (-1))
-#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
-#     W_e = g1.W_e.to_dense()
-#     L_rw = torch.eye(H.shape[0]) - D_v_neg_1 @ H @ W_e @ D_e_neg_1 @ H.t()
-#     assert (L_rw == g1.L_rw.to_dense().cpu()).all()
-
-
-@pytest.mark.skipif(
-    sys.version_info.major <= 3 and sys.version_info.minor < 7,
-    reason="python requires >= 3.7",
-)
-# def test_L_rw_group(g1):
-#     import torch
-#
-#     g1.add_hyperedges([[0, 1]], group_name="knn")
-#     # all
-#     H = g1.H.to_dense().cpu()
-#     D_v_neg_1 = torch.diag(H.sum(dim=1).view(-1) ** (-1))
-#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
-#     W_e = g1.W_e.to_dense()
-#     L_rw = torch.eye(H.shape[0]) - D_v_neg_1 @ H @ W_e @ D_e_neg_1 @ H.t()
-#     assert (L_rw == g1.L_rw.to_dense().cpu()).all()
-#     # main group
-#     H = g1.H_of_group("main").to_dense().cpu()
-#     D_v_neg_1 = torch.diag(H.sum(dim=1).view(-1) ** (-1))
-#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
-#     W_e = g1.W_e_of_group("main").to_dense()
-#     L_rw = torch.eye(H.shape[0]) - D_v_neg_1 @ H @ W_e @ D_e_neg_1 @ H.t()
-#     assert (L_rw == g1.L_rw_of_group("main").to_dense().cpu()).all()
-#     # knn group
-#     H = g1.H_of_group("knn").to_dense().cpu()
-#     D_v_neg_1 = H.sum(dim=1).view(-1) ** (-1)
-#     D_v_neg_1[torch.isinf(D_v_neg_1)] = 0
-#     D_v_neg_1 = torch.diag(D_v_neg_1)
-#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
-#     W_e = g1.W_e_of_group("knn").to_dense()
-#     L_rw = torch.eye(H.shape[0]) - D_v_neg_1 @ H @ W_e @ D_e_neg_1 @ H.t()
-#     assert (L_rw == g1.L_rw_of_group("knn").to_dense().cpu()).all()
-
-
-@pytest.mark.skipif(
-    sys.version_info.major <= 3 and sys.version_info.minor < 7,
-    reason="python requires >= 3.7",
-)
-def test_smoothing_with_HGNN(g1):
-    import torch
-
-    H = torch.tensor(
-        [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]],
-        dtype=torch.float32,
-    )
-    D_v_inv_1_2 = H.sum(1).view(-1) ** (-0.5)
-    D_v_inv_1_2[torch.isinf(D_v_inv_1_2)] = 0
-    D_v_inv_1_2 = torch.diag(D_v_inv_1_2)
-
-    D_e_inv = H.sum(0).view(-1) ** (-1)
-    D_e_inv[torch.isinf(D_e_inv)] = 0
-    D_e_inv = torch.diag(D_e_inv)
-
-    x = torch.rand(H.shape[0], 8)
-
-    gt = D_v_inv_1_2 @ H @ D_e_inv @ H.t() @ D_v_inv_1_2 @ x
-
-    res = g1.smoothing_with_HGNN(x)
-
-    assert pytest.approx(gt, rel=1e-6) == res.cpu()
-
-
-@pytest.mark.skipif(
-    sys.version_info.major <= 3 and sys.version_info.minor < 7,
-    reason="python requires >= 3.7",
-)
-def test_smoothing_with_HGNN_group(g1):
-    import torch
-
-    H = torch.tensor(
-        [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]],
-        dtype=torch.float32,
-    )
-    D_v_inv_1_2 = H.sum(1).view(-1) ** (-0.5)
-    D_v_inv_1_2[torch.isinf(D_v_inv_1_2)] = 0
-    D_v_inv_1_2 = torch.diag(D_v_inv_1_2)
-
-    D_e_inv = H.sum(0).view(-1) ** (-1)
-    D_e_inv[torch.isinf(D_e_inv)] = 0
-    D_e_inv = torch.diag(D_e_inv)
-
-    x = torch.rand(H.shape[0], 8)
-
-    gt = D_v_inv_1_2 @ H @ D_e_inv @ H.t() @ D_v_inv_1_2 @ x
-
-    res = g1.smoothing_with_HGNN_of_group("main", x)
-
-    assert pytest.approx(gt, rel=1e-6) == res.cpu()
-
-
-def test_v2e_message_passing(g1):
-    import torch
-
-    H = torch.tensor(
-        [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]],
-        dtype=torch.float32,
-    )
-
-    x = torch.rand(H.shape[0], 8)
-
-    gt_sum = H.t() @ x
-    res_sum = g1.v2e(x, aggr="sum")
-    assert pytest.approx(gt_sum, rel=1e-6) == res_sum.cpu()
-
-    D_e_inv = H.sum(0).view(-1) ** (-1)
-    D_e_inv[torch.isinf(D_e_inv)] = 0
-    D_e_inv = torch.diag(D_e_inv)
-
-    gt_mean = D_e_inv @ gt_sum
-    res_mean = g1.v2e(x, aggr="mean")
-    assert pytest.approx(gt_mean, rel=1e-6) == res_mean.cpu()
-
-
-def test_e2v_message_passing(g1):
-    import torch
-
-    H = torch.tensor(
-        [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]],
-        dtype=torch.float32,
-    )
-
-    x = torch.rand(3, 8)
-
-    gt_sum = H @ x
-    res_sum = g1.e2v(x, aggr="sum")
-    assert pytest.approx(gt_sum, rel=1e-6) == res_sum.cpu()
-
-    D_v_inv = H.sum(1).view(-1) ** (-1)
-    D_v_inv[torch.isinf(D_v_inv)] = 0
-    D_v_inv = torch.diag(D_v_inv)
-
-    gt_mean = D_v_inv @ gt_sum
-    res_mean = g1.e2v(x, aggr="mean")
-    assert pytest.approx(gt_mean, rel=1e-6) == res_mean.cpu()
-
-
-def test_v2v_message_passing(g1):
-    import torch
-
-    H = torch.tensor(
-        [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]],
-        dtype=torch.float32,
-    )
-
-    x = torch.rand(6, 8)
-
-    gt_sum = H @ H.t() @ x
-    res_sum = g1.v2v(x, aggr="sum")
-    assert pytest.approx(gt_sum, rel=1e-6) == res_sum.cpu()
-
-    D_v_inv = H.sum(1).view(-1) ** (-1)
-    D_v_inv[torch.isinf(D_v_inv)] = 0
-    D_v_inv = torch.diag(D_v_inv)
-
-    D_e_inv = H.sum(0).view(-1) ** (-1)
-    D_e_inv[torch.isinf(D_e_inv)] = 0
-    D_e_inv = torch.diag(D_e_inv)
-
-    gt_mean = D_v_inv @ H @ D_e_inv @ H.t() @ x
-    res_mean = g1.v2v(x, aggr="mean")
-    assert pytest.approx(gt_mean, rel=1e-6) == res_mean.cpu()
-
-
-@pytest.mark.skipif(
-    sys.version_info.major <= 3 and sys.version_info.minor < 7,
-    reason="python requires >= 3.7",
-)
-# def test_graph_and_hypergraph():
-#     import torch
-#
-#     g = eg.Graph()
-#     g.add_nodes([0, 1, 2, 3])
-#     g.add_edges(
-#         [(0, 1), (0, 2), (1, 3)], [{"weight": 1.0}, {"weight": 1.0}, {"weight": 1.0}]
-#     )
-#     hg = eg.Hypergraph.from_graph(g)
-#     _mm = torch.sparse.mm
-#     est_A = _mm(_mm(g.D_v_neg_1_2, g.A), g.D_v_neg_1_2) + torch.eye(4).to_sparse()
-#     assert pytest.approx(est_A.to_dense() / 2) == hg.L_HGNN.to_dense()
-
-
-@pytest.mark.skip(reason="skip")
-def test_get_linegraph():
-    num_v = 5
-    e_list = [[0, 1], [1, 2, 3], [0, 3, 4]]
-    e_weight = [1.0, 0.5, 2.0]
-    v_weight = [0.2, 0.3, 0.4, 0.5, 0.6]
-
-    hg = eg.Hypergraph(num_v=num_v, e_list=e_list, e_weight=e_weight)
-    lg = hg.get_clique_expansion()
-    assert lg.edges == [[0, 1], [0, 2], [1, 2]]
-    assert lg.nodes == [0, 1, 2]
+import sys
+
+from copy import deepcopy
+
+import easygraph as eg
+import pytest
+
+
+@pytest.fixture()
+def g1():
+    e_list = [(0, 1, 2, 5), (0, 1), (2, 3, 4), (3, 2, 4)]
+    g = eg.Hypergraph(6, e_list=e_list)
+    return g
+
+
+@pytest.fixture()
+def g2():
+    e_list = [(1, 2, 3), (0, 1, 3), (0, 1), (2, 4, 3), (2, 3)]
+    e_weight = [0.5, 1, 0.5, 1, 0.5]
+    g = eg.Hypergraph(5, e_list=e_list, e_weight=e_weight)
+    return g
+
+
+@pytest.fixture()
+def g3():
+    e_list = [[0, 1], [0, 1, 2], [2, 3, 4]]
+    e_weight = [1, 1, 1]
+    g = eg.Hypergraph(5, e_list=e_list, e_weight=e_weight)
+    return g
+
+
+def test_expansion(g3):
+    star_expansion_graph = g3.get_star_expansion()
+    node_clique_expansion_graph = g3.get_clique_expansion()
+    edge_clique_expansion_graph = g3.get_clique_expansion()
+    print(star_expansion_graph.edges)
+    print(node_clique_expansion_graph.edges)
+    print(edge_clique_expansion_graph.edges)
+
+
+def test_property(g1, g2):
+    assert g2.distance(1, 2) == 1
+    assert g2.diameter() == 2
+    assert g1.adjacency_matrix != None
+    assert g1.edge_adjacency_matrix != None
+    assert g2.adjacency_matrix != None
+    assert g2.edge_adjacency_matrix != None
+
+
+def test_save(g1, tmp_path):
+    from easygraph import load_structure
+
+    print("g1:", g1, g1.e)
+    g1.save(tmp_path / "g1")
+    g2 = load_structure(tmp_path / "g1")
+
+    for e1, e2 in zip(g1.e[0], g2.e[0]):
+        assert e1 == e2
+    for w1, w2 in zip(g1.e[1], g2.e[1]):
+        assert w1 == w2
+
+
+# test construction
+def test_from_feature_kNN():
+    import numpy as np
+    import scipy.spatial
+    import torch
+
+    ft = np.random.rand(32, 8)
+    cdist = scipy.spatial.distance.cdist(ft, ft)
+    tk_mat = np.argsort(cdist, axis=1)[:, :3]
+    hg = eg.Hypergraph.from_feature_kNN(torch.tensor(ft), k=3)
+    assert tuple(sorted(tk_mat[0].tolist())) in hg.e[0]
+    assert tuple(sorted(tk_mat[8].tolist())) in hg.e[0]
+    assert tuple(sorted(tk_mat[13].tolist())) in hg.e[0]
+    assert tuple(sorted(tk_mat[26].tolist())) in hg.e[0]
+
+
+def test_from_graph():
+    g = eg.Graph()
+    g.add_nodes(list(range(0, 5)))
+    g.add_edges(
+        [(0, 1), (0, 3), (1, 4), (2, 3), (3, 4)],
+        [
+            {"weight": 1.0},
+            {"weight": 1.0},
+            {"weight": 1.0},
+            {"weight": 1.0},
+            {"weight": 1.0},
+        ],
+    )
+    hg = eg.Hypergraph.from_graph(g)
+    assert hg.num_e == 5
+    assert (0, 1) in hg.e[0]
+    assert (1, 4) in hg.e[0]
+
+
+def test_from_graph_kHop():
+    g = eg.Graph()
+    g.add_nodes(range(0, 5))
+    g.add_edges(
+        [(0, 1), (0, 3), (1, 4), (2, 3)],
+        [{"weight": 1.0}, {"weight": 1.0}, {"weight": 1.0}, {"weight": 1.0}],
+    )
+    hg = eg.Hypergraph.from_graph_kHop(g, k=1)
+    assert hg.num_e == 5
+    assert (0, 1, 3) in hg.e[0]
+    assert (0, 1, 4) in hg.e[0]
+    assert (1, 4) in hg.e[0]
+    assert (2, 3) in hg.e[0]
+    assert (0, 2, 3) in hg.e[0]
+    hg = eg.Hypergraph.from_graph_kHop(g, k=2)
+    assert hg.num_e == 5
+    assert (0, 1, 3, 4) in hg.e[0]
+    hg = eg.Hypergraph.from_graph_kHop(g, k=2, only_kHop=True)
+    assert hg.num_e == 4
+
+
+# test representation
+def test_empty():
+    g = eg.Hypergraph(10)
+    assert g.num_v == 10
+    assert g.e == ([], [], [])
+
+
+def test_init(g1, g2):
+    assert g1.num_v == 6
+    assert g1.num_e == 3
+    assert g1.e[0] == [(0, 1, 2, 5), (0, 1), (2, 3, 4)]
+    assert g1.e[1] == [1, 1, 1]
+    assert g2.num_v == 5
+    assert g2.num_e == 5
+    assert g2.e[0] == [(1, 2, 3), (0, 1, 3), (0, 1), (2, 3, 4), (2, 3)]
+    assert g2.e[1] == [0.5, 1, 0.5, 1, 0.5]
+
+
+def test_clear(g1):
+    assert g1.num_e == 3
+    g1.clear()
+    assert g1.num_e == 0
+    assert g1.e == ([], [], [])
+
+
+def test_add_and_merge_hyperedges(g1):
+    assert g1.e[1] == [1, 1, 1]
+    print("g1:", g1, g1.e)
+    g1.add_hyperedges(e_list=[0, 1], e_weight=3, merge_op="mean")
+    assert g1.e[1] == [1, 2, 1]
+    assert g1.e[0] == [(0, 1, 2, 5), (0, 1), (2, 3, 4)]
+    g1.add_hyperedges([(2, 4, 3), (1, 0), (3, 4)], [1, 1, 1], merge_op="sum")
+    assert g1.e[0] == [(0, 1, 2, 5), (0, 1), (2, 3, 4), (3, 4)]
+    assert g1.e[1] == [1, 3, 2, 1]
+
+
+def test_add_hyperedges_from_feature_kNN(g1):
+    import numpy as np
+    import scipy.spatial
+    import torch
+
+    origin_e = deepcopy(g1.e[0])
+    ft = np.random.rand(6, 8)
+    cdist = scipy.spatial.distance.cdist(ft, ft)
+    tk_mat = np.argsort(cdist, axis=1)[:, :3]
+
+    g1.add_hyperedges_from_feature_kNN(torch.tensor(ft), k=3, group_name="knn")
+    assert tuple(sorted(tk_mat[0].tolist())) in g1.e_of_group("knn")[0]
+    assert tuple(sorted(tk_mat[3].tolist())) in g1.e_of_group("knn")[0]
+    assert tuple(sorted(tk_mat[4].tolist())) in g1.e_of_group("knn")[0]
+    assert tuple(sorted(tk_mat[5].tolist())) in g1.e_of_group("knn")[0]
+
+    for e in origin_e:
+        assert e in g1.e_of_group("main")[0]
+
+    for e in g1.e_of_group("main")[0]:
+        assert e in origin_e
+
+
+# def test_add_hyperedges_from_graph(g1):
+#     g = eg.graph_Gnm(6, 3)
+#     origin_e = deepcopy(g1.e[0])
+#
+#     g1.add_hyperedges_from_graph(g, group_name="graph")
+#     g_e = g.e[0]
+#     g1_e = g1.e_of_group("graph")[0]
+#
+#     for e in g_e:
+#         assert e in g1_e
+#
+#     for e in origin_e:
+#         assert e in g1.e_of_group("main")[0]
+#
+#     for e in g1.e[0]:
+#         assert e in origin_e or e in g_e
+
+
+def test_add_hyperedges_from_graph_kHop(g1):
+    g = eg.graph_Gnm(6, 5)
+
+    origin_e = deepcopy(g1.e[0])
+    for k in range(1, 3):
+        gg1 = deepcopy(g1)
+        gg1.add_hyperedges_from_graph_kHop(g, k=k, group_name="kHop")
+
+        khop = [[] for _ in range(6)]
+        for kk in range(k):
+            for v in range(6):
+                if kk == 0:
+                    khop[v] = g.nbr_v(v)
+                else:
+                    kk_hop_v = []
+                    for nbr in khop[v]:
+                        kk_hop_v += g.nbr_v(nbr)
+                    khop[v] += kk_hop_v
+                khop[v] = list(set(khop[v]))
+
+        for v in range(6):
+            edge = [v] + khop[v]
+            edge = tuple(set(sorted(edge)))
+            assert edge in gg1.e_of_group("kHop")[0]
+
+        gg2 = deepcopy(g1)
+        gg2.add_hyperedges_from_graph_kHop(g, k=k, group_name="kHop", only_kHop=True)
+
+        khop = [[] for _ in range(6)]
+        for kk in range(k):
+            for v in range(6):
+                if len(khop[v]) == 0:
+                    khop[v] = g.nbr_v(v)
+                else:
+                    kk_hop_v = []
+                    for nbr in khop[v]:
+                        kk_hop_v += g.nbr_v(nbr)
+                    khop[v] = kk_hop_v
+                khop[v] = list(set(khop[v]))
+
+        for v in range(6):
+            edge = [v] + khop[v]
+            edge = tuple(set(sorted(edge)))
+            assert edge in gg2.e_of_group("kHop")[0]
+
+        for e in origin_e:
+            assert e in gg1.e_of_group("main")[0]
+            assert e in gg2.e_of_group("main")[0]
+
+        for e in gg1.e_of_group("main")[0]:
+            assert e in origin_e
+        for e in gg2.e_of_group("main")[0]:
+            assert e in origin_e
+
+
+def test_remove_hyperedges(g1):
+    assert g1.e[0] == [(0, 1, 2, 5), (0, 1), (2, 3, 4)]
+    assert g1.e[1] == [1, 1, 1]
+    g1.remove_hyperedges([0, 1])
+    assert (0, 1) not in g1.e[0]
+    assert (0, 1, 5) not in g1.e[0]
+    g1.add_hyperedges([[0, 1, 5], [2, 3, 4]])
+    assert (0, 1, 5) in g1.e[0]
+    g1.remove_hyperedges([[0, 1, 5], (0, 1, 2, 5)])
+    assert (0, 1, 5) not in g1.e[0]
+    assert (0, 1, 2, 5) not in g1.e[0]
+    g1.clear()
+    assert g1.num_e == 0
+    assert g1.e == ([], [], [])
+
+
+# def test_remove_group(g1):
+#     origin_e = deepcopy(g1.e[0])
+#
+#     g1.add_hyperedges(([0, 1, 2, 5], [0, 1]), group_name="test")
+#     for e in origin_e:
+#         assert e in g1.e_of_group("main")[0]
+#     for e in g1.e_of_group("main")[0]:
+#         assert e in origin_e
+#
+#     # g1.remove_group("none")
+#
+#     g1.remove_group("test")
+#     assert "test" not in g1.group_names
+#
+#     for e in origin_e:
+#         assert e in g1.e_of_group("main")[0]
+#     for e in g1.e_of_group("main")[0]:
+#         assert e in origin_e
+#
+#     g1.remove_group("main")
+#
+#     assert len(g1.e[0]) == 0
+#     assert len(g1.e[1]) == 0
+#
+#
+# def test_add_and_remove_group(g1):
+#     assert g1.group_names == ["main"]
+#     g1.add_hyperedges([0, 2, 3], group_name="knn")
+#     assert len(g1.group_names) == 2
+#     assert "main" in g1.group_names
+#     assert "knn" in g1.group_names
+#     assert (0, 2, 3) in g1.e[0]
+#     assert (0, 2, 3) in g1.e_of_group("knn")[0]
+#     assert (0, 2, 3) not in g1.e_of_group("main")[0]
+#     g1.remove_hyperedges([0, 2, 3], group_name="knn")
+#     assert (0, 2, 3) not in g1.e[0]
+#     assert (0, 2, 3) not in g1.e_of_group("knn")[0]
+
+
+def test_deg(g1, g2):
+    assert g1.deg_v == [2, 2, 2, 1, 1, 1]
+    assert g1.deg_e == [4, 2, 3]
+    assert g2.deg_v == [2, 3, 3, 4, 1]
+    assert g2.deg_e == [3, 3, 2, 3, 2]
+
+
+# def test_deg_group(g1):
+#     assert g1.deg_v == [2, 2, 2, 1, 1, 1]
+#     assert g1.deg_e == [4, 2, 3]
+#     g1.add_hyperedges([0, 2], 1, group_name="knn")
+#     assert g1.deg_v == [3, 2, 3, 1, 1, 1]
+#     assert g1.deg_e == [4, 2, 3, 2]
+#     assert g1.deg_v_of_group("main") == [2, 2, 2, 1, 1, 1]
+#     assert g1.deg_e_of_group("main") == [4, 2, 3]
+#     assert g1.deg_v_of_group("knn") == [1, 0, 1, 0, 0, 0]
+#     assert g1.deg_e_of_group("knn") == [2]
+
+
+def test_nbr(g1, g2):
+    assert g1.nbr_v(0) == [0, 1, 2, 5]
+    assert g1.nbr_e(1) == [0, 1]
+    assert g2.nbr_v(2) == [0, 1]
+    assert g2.nbr_e(4) == [3]
+
+
+# def test_nbr_group(g1):
+#     print("g1:", g1.e, g1.v)
+#     assert g1.nbr_v(1) == [0, 1]
+#     assert g1.nbr_e(0) == [0, 1]
+#     g1.add_hyperedges([[0, 1]], group_name="knn")
+#     assert g1.nbr_v(1) == [0, 1]
+#     assert g1.nbr_e(1) == [0, 1, 3]
+#     assert g1.nbr_v_of_group(1, "main") == [0, 1]
+#     assert g1.nbr_e_of_group(2, "main") == [0, 2]
+#     assert g1.nbr_v_of_group(0, "knn") == [0, 1]
+#     assert g1.nbr_e_of_group(1, "knn") == [0]
+
+
+def test_clone(g1):
+    assert g1.num_v == 6
+    assert g1.num_e == 3
+    g1_clone = g1.clone()
+    g1_clone.add_hyperedges([0, 2], 1, group_name="knn")
+    assert g1.num_e == 3
+    assert g1_clone.num_e == 4
+
+
+# test deep learning
+def test_v2e_index(g1):
+    import torch
+
+    v2e_src = g1.v2e_src.view(-1, 1)
+    v2e_dst = g1.v2e_dst.view(-1, 1)
+
+    index = torch.cat((v2e_src, v2e_dst), dim=1)
+    index = index.numpy().tolist()
+    index = list(map(lambda x: tuple(x), index))
+
+    assert (0, 0) in index
+    assert (1, 0) in index
+    assert (2, 0) in index
+    assert (5, 0) in index
+    assert (0, 1) in index
+    assert (1, 1) in index
+    assert (2, 2) in index
+    assert (3, 2) in index
+    assert (4, 2) in index
+
+
+def test_v2e_index_group(g1):
+    import torch
+
+    v2e_src = g1.v2e_src_of_group("main").view(-1, 1)
+    v2e_dst = g1.v2e_dst_of_group("main").view(-1, 1)
+
+    index = torch.cat((v2e_src, v2e_dst), dim=1)
+    index = index.numpy().tolist()
+    index = list(map(lambda x: tuple(x), index))
+
+    assert (0, 0) in index
+    assert (1, 0) in index
+    assert (2, 0) in index
+    assert (5, 0) in index
+    assert (0, 1) in index
+    assert (1, 1) in index
+    assert (2, 2) in index
+    assert (3, 2) in index
+    assert (4, 2) in index
+
+
+def test_e2v_index(g1):
+    import torch
+
+    e2v_src = g1.e2v_src.view(-1, 1)
+    e2v_dst = g1.e2v_dst.view(-1, 1)
+
+    index = torch.cat((e2v_src, e2v_dst), dim=1)
+    index = index.numpy().tolist()
+    index = list(map(lambda x: tuple(x), index))
+
+    assert (0, 0) in index
+    assert (0, 1) in index
+    assert (0, 2) in index
+    assert (0, 5) in index
+    assert (1, 0) in index
+    assert (1, 1) in index
+    assert (2, 2) in index
+    assert (2, 3) in index
+    assert (2, 4) in index
+
+
+def test_e2v_index_group(g1):
+    import torch
+
+    e2v_src = g1.e2v_src_of_group("main").view(-1, 1)
+    e2v_dst = g1.e2v_dst_of_group("main").view(-1, 1)
+
+    index = torch.cat((e2v_src, e2v_dst), dim=1)
+    index = index.numpy().tolist()
+    index = list(map(lambda x: tuple(x), index))
+
+    assert (0, 0) in index
+    assert (0, 1) in index
+    assert (0, 2) in index
+    assert (0, 5) in index
+    assert (1, 0) in index
+    assert (1, 1) in index
+    assert (2, 2) in index
+    assert (2, 3) in index
+    assert (2, 4) in index
+
+
+def test_H(g1):
+    import torch
+
+    print("g1", g1.H.to_dense())
+    assert (
+        g1.H.to_dense().cpu()
+        == torch.tensor(
+            [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]]
+        )
+    ).all()
+
+
+# def test_H_group(g1):
+#     import torch
+#
+#     g1.add_hyperedges([0, 4, 5], group_name="knn")
+#     assert (
+#         g1.H.to_dense().cpu()
+#         == torch.tensor(
+#             [
+#                 [1, 1, 0, 1],
+#                 [1, 1, 0, 0],
+#                 [1, 0, 1, 0],
+#                 [0, 0, 1, 0],
+#                 [0, 0, 1, 1],
+#                 [1, 0, 0, 1],
+#             ]
+#         )
+#     ).all()
+#     assert (
+#         g1.H_of_group("main").to_dense().cpu()
+#         == torch.tensor(
+#             [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]]
+#         )
+#     ).all()
+#     assert (
+#         g1.H_of_group("knn").to_dense().cpu()
+#         == torch.tensor([[1], [0], [0], [0], [1], [1]])
+#     ).all()
+
+
+def test_H_T(g1):
+    import torch
+
+    assert (
+        g1.H_T.to_dense().cpu()
+        == torch.tensor(
+            [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]]
+        ).t()
+    ).all()
+
+
+# def test_H_T_group(g1):
+#     import torch
+#
+#     g1.add_hyperedges([0, 4, 5], group_name="knn")
+#     assert (
+#         g1.H_T.to_dense().cpu()
+#         == torch.tensor(
+#             [
+#                 [1, 1, 0, 1],
+#                 [1, 1, 0, 0],
+#                 [1, 0, 1, 0],
+#                 [0, 0, 1, 0],
+#                 [0, 0, 1, 1],
+#                 [1, 0, 0, 1],
+#             ]
+#         ).t()
+#     ).all()
+#     assert (
+#         g1.H_T_of_group("main").to_dense().cpu()
+#         == torch.tensor(
+#             [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]]
+#         ).t()
+#     ).all()
+#     assert (
+#         g1.H_T_of_group("knn").to_dense().cpu() == torch.tensor([[1, 0, 0, 0, 1, 1]])
+#     ).all()
+
+
+def test_W_e(g2):
+    import torch
+
+    assert (
+        g2.W_e.to_sparse_coo().cpu()._values() == torch.tensor([0.5, 1, 0.5, 1, 0.5])
+    ).all()
+
+
+# def test_W_e_group(g2):
+#     import torch
+#
+#     g2.add_hyperedges([0, 4, 5], group_name="knn")
+#     assert (g2.W_e.cpu()._values() == torch.tensor([0.5, 1, 0.5, 1, 0.5, 1])).all()
+#     assert (
+#         g2.W_e_of_group("main").cpu()._values() == torch.tensor([0.5, 1, 0.5, 1, 0.5])
+#     ).all()
+#     assert (g2.W_e_of_group("knn").cpu()._values() == torch.tensor([1])).all()
+
+
+def test_D(g1, g2):
+    import torch
+
+    assert (g1.D_v.cpu()._values() == torch.tensor([2, 2, 2, 1, 1, 1])).all()
+    assert (g1.D_e.to_sparse_coo().cpu()._values() == torch.tensor([4, 2, 3])).all()
+    assert (g2.D_v.cpu()._values() == torch.tensor([2, 3, 3, 4, 1])).all()
+    assert (
+        g2.D_e.to_sparse_coo().cpu()._values() == torch.tensor([3, 3, 2, 3, 2])
+    ).all()
+
+
+# def test_D_group(g1):
+#     import torch
+#
+#     assert (g1.D_v.cpu()._values() == torch.tensor([2, 2, 2, 1, 1, 1])).all()
+#     assert (g1.D_e.cpu()._values() == torch.tensor([4, 2, 3])).all()
+#     g1.add_hyperedges([[0, 2], [1, 2, 3]], group_name="knn")
+#     assert (g1.D_v.cpu()._values() == torch.tensor([3, 3, 4, 2, 1, 1])).all()
+#     assert (g1.D_e.cpu()._values() == torch.tensor([4, 2, 3, 2, 3])).all()
+#     assert (
+#         g1.D_v_of_group("main").cpu()._values() == torch.tensor([2, 2, 2, 1, 1, 1])
+#     ).all()
+#     assert (g1.D_e_of_group("main").cpu()._values() == torch.tensor([4, 2, 3])).all()
+#     assert (
+#         g1.D_v_of_group("knn").cpu()._values() == torch.tensor([1, 1, 2, 1, 0, 0])
+#     ).all()
+#     assert (g1.D_e_of_group("knn").cpu()._values() == torch.tensor([2, 3])).all()
+
+
+def test_D_neg(g1, g2):
+    import torch
+
+    # -1
+    assert (
+        g1.D_v_neg_1.to_sparse_coo().cpu()._values()
+        == torch.tensor([2, 2, 2, 1, 1, 1]) ** (-1.0)
+    ).all()
+    assert (
+        g1.D_e_neg_1.to_sparse_coo().cpu()._values()
+        == torch.tensor([4, 2, 3]) ** (-1.0)
+    ).all()
+    assert (
+        g2.D_v_neg_1.to_sparse_coo().cpu()._values()
+        == torch.tensor([2, 3, 3, 4, 1]) ** (-1.0)
+    ).all()
+    assert (
+        g2.D_e_neg_1.to_sparse_coo().cpu()._values()
+        == torch.tensor([3, 3, 2, 3, 2]) ** (-1.0)
+    ).all()
+    # -1/2
+    assert (
+        g1.D_v_neg_1_2.to_sparse_coo().cpu()._values()
+        == torch.tensor([2, 2, 2, 1, 1, 1]) ** (-0.5)
+    ).all()
+    assert (
+        g2.D_v_neg_1_2.to_sparse_coo().cpu()._values()
+        == torch.tensor([2, 3, 3, 4, 1]) ** (-0.5)
+    ).all()
+    # isolated vertex
+    g3 = eg.Hypergraph(num_v=3, e_list=[0, 1])
+    assert (
+        g3.D_v_neg_1.to_sparse_coo().cpu()._values() == torch.tensor([1, 1, 0])
+    ).all()
+
+
+# def test_D_neg_group(g1):
+#     import torch
+#
+#     # -1
+#     assert (
+#         g1.D_v_neg_1.cpu()._values() == torch.tensor([2, 2, 2, 1, 1, 1]) ** (-1.0)
+#     ).all()
+#     assert (g1.D_e_neg_1.cpu()._values() == torch.tensor([4, 2, 3]) ** (-1.0)).all()
+#     g1.add_hyperedges([[0, 2], [1, 2, 3]], group_name="knn")
+#     assert (
+#         g1.D_v_neg_1.cpu()._values() == torch.tensor([3, 3, 4, 2, 1, 1]) ** (-1.0)
+#     ).all()
+#     assert (
+#         g1.D_e_neg_1.cpu()._values() == torch.tensor([4, 2, 3, 2, 3]) ** (-1.0)
+#     ).all()
+#     assert (
+#         g1.D_v_neg_1_of_group("main").cpu()._values()
+#         == torch.tensor([2, 2, 2, 1, 1, 1]) ** (-1.0)
+#     ).all()
+#     assert (
+#         g1.D_e_neg_1_of_group("main").cpu()._values()
+#         == torch.tensor([4, 2, 3]) ** (-1.0)
+#     ).all()
+#     assert (
+#         g1.D_v_neg_1_of_group("knn").cpu()._values()
+#         == torch.tensor([1 / 1, 1 / 1, 1 / 2, 1 / 1, 0, 0])
+#     ).all()
+#     assert (
+#         g1.D_e_neg_1_of_group("knn").cpu()._values() == torch.tensor([2, 3]) ** (-1.0)
+#     ).all()
+#     # -1/2
+#     assert (
+#         g1.D_v_neg_1_2.cpu()._values() == torch.tensor([3, 3, 4, 2, 1, 1]) ** (-0.5)
+#     ).all()
+#     assert (
+#         g1.D_v_neg_1_2_of_group("main").cpu()._values()
+#         == torch.tensor([2, 2, 2, 1, 1, 1]) ** (-0.5)
+#     ).all()
+#     assert (
+#         g1.D_v_neg_1_2_of_group("knn").cpu()._values()
+#         == torch.tensor([1 ** (-0.5), 1 ** (-0.5), 2 ** (-0.5), 1 ** (-0.5), 0, 0])
+#     ).all()
+
+
+def test_N(g1, g2):
+    import torch
+
+    assert (g1.N_v(0).cpu() == torch.tensor([0, 1, 2, 5])).all()
+    assert (g1.N_e(2).cpu() == torch.tensor([0, 2])).all()
+    assert (g2.N_v(1).cpu() == torch.tensor([0, 1, 3])).all()
+    assert (g2.N_e(3).cpu() == torch.tensor([0, 1, 3, 4])).all()
+
+
+# def test_N_group(g1):
+#     import torch
+#
+#     assert (g1.N_v(1).cpu() == torch.tensor([0, 1])).all()
+#     assert (g1.N_e(1).cpu() == torch.tensor([0, 1])).all()
+#     g1.add_hyperedges([[0, 1], [1, 2]], group_name="knn")
+#     assert (g1.N_v(1).cpu() == torch.tensor([0, 1])).all()
+#     assert (g1.N_e(1).cpu() == torch.tensor([0, 1, 3, 4])).all()
+#     assert (g1.N_v_of_group(1, "main").cpu() == torch.tensor([0, 1])).all()
+#     assert (g1.N_e_of_group(2, "main").cpu() == torch.tensor([0, 2])).all()
+#     assert (g1.N_v_of_group(1, "knn").cpu() == torch.tensor([1, 2])).all()
+#     assert (g1.N_e_of_group(1, "knn").cpu() == torch.tensor([0, 1])).all()
+#
+
+
+@pytest.mark.skipif(
+    sys.version_info.major <= 3 and sys.version_info.minor < 7,
+    reason="python requires >= 3.7",
+)
+def test_L_HGNN(g1):
+    import torch
+
+    print("g1:", g1, g1.e)
+    H = g1.H.to_dense().cpu()
+    D_v_neg_1_2 = torch.diag(H.sum(dim=1).view(-1) ** (-0.5))
+    D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
+    W_e = g1.W_e.to_dense()
+    L_HGNN = D_v_neg_1_2 @ H @ W_e @ D_e_neg_1 @ H.t() @ D_v_neg_1_2
+    assert (L_HGNN == g1.L_HGNN.to_dense().cpu()).all()
+
+
+@pytest.mark.skipif(
+    sys.version_info.major <= 3 and sys.version_info.minor < 7,
+    reason="python requires >= 3.7",
+)
+# def test_L_HGNN_group(g1):
+#     import torch
+#
+#     g1.add_hyperedges([[0, 1]], group_name="knn")
+#     # all
+#     H = g1.H.to_dense().cpu()
+#     D_v_neg_1_2 = torch.diag(H.sum(dim=1).view(-1) ** (-0.5))
+#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
+#     W_e = g1.W_e.to_dense()
+#     L_HGNN = D_v_neg_1_2 @ H @ W_e @ D_e_neg_1 @ H.t() @ D_v_neg_1_2
+#     assert (L_HGNN == g1.L_HGNN.to_dense().cpu()).all()
+#     # main group
+#     H = g1.H_of_group("main").to_dense().cpu()
+#     D_v_neg_1_2 = torch.diag(H.sum(dim=1).view(-1) ** (-0.5))
+#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
+#     W_e = g1.W_e_of_group("main").to_dense()
+#     L_HGNN = D_v_neg_1_2 @ H @ W_e @ D_e_neg_1 @ H.t() @ D_v_neg_1_2
+#     assert (L_HGNN == g1.L_HGNN_of_group("main").to_dense().cpu()).all()
+#     # knn group
+#     H = g1.H_of_group("knn").to_dense().cpu()
+#     D_v_neg_1_2 = H.sum(dim=1).view(-1) ** (-0.5)
+#     D_v_neg_1_2[torch.isinf(D_v_neg_1_2)] = 0
+#     D_v_neg_1_2 = torch.diag(D_v_neg_1_2)
+#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
+#     W_e = g1.W_e_of_group("knn").to_dense()
+#     L_HGNN = D_v_neg_1_2 @ H @ W_e @ D_e_neg_1 @ H.t() @ D_v_neg_1_2
+#     assert (L_HGNN == g1.L_HGNN_of_group("knn").to_dense().cpu()).all()
+
+
+@pytest.mark.skipif(
+    sys.version_info.major <= 3 and sys.version_info.minor < 7,
+    reason="python requires >= 3.7",
+)
+def test_smoothing():
+    import torch
+
+    x = torch.rand(10, 5)
+    L = torch.rand(10, 10)
+    g = eg.Hypergraph(10)
+    lbd = 0.1
+    assert pytest.approx(g.smoothing(x, L, lbd)) == x + lbd * L @ x
+
+
+@pytest.mark.skipif(
+    sys.version_info.major <= 3 and sys.version_info.minor < 7,
+    reason="python requires >= 3.7",
+)
+def test_L_sym(g1):
+    import torch
+
+    H = g1.H.to_sparse_coo().to_dense().cpu()
+    D_v_neg_1_2 = torch.diag(H.sum(dim=1).view(-1) ** (-0.5))
+    D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
+    W_e = g1.W_e.to_dense()
+    L_sym = (
+        torch.eye(H.shape[0])
+        - D_v_neg_1_2.to_sparse_coo()
+        @ H.to_sparse_coo()
+        @ W_e
+        @ D_e_neg_1.to_sparse_coo()
+        @ H.t().to_sparse_coo()
+        @ D_v_neg_1_2.to_sparse_coo()
+    )
+    assert (L_sym == g1.L_sym.to_dense().cpu()).all()
+
+
+@pytest.mark.skipif(
+    sys.version_info.major <= 3 and sys.version_info.minor < 7,
+    reason="python requires >= 3.7",
+)
+# def test_L_sym_group(g1):
+#     import torch
+#
+#     g1.add_hyperedges([[0, 1]], group_name="knn")
+#     # all
+#     H = g1.H.to_dense().cpu()
+#     D_v_neg_1_2 = torch.diag(H.sum(dim=1).view(-1) ** (-0.5))
+#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
+#     W_e = g1.W_e.to_dense()
+#     L_sym = (
+#         torch.eye(H.shape[0]) - D_v_neg_1_2 @ H @ W_e @ D_e_neg_1 @ H.t() @ D_v_neg_1_2
+#     )
+#     assert (L_sym == g1.L_sym.to_dense().cpu()).all()
+#     # main group
+#     H = g1.H_of_group("main").to_dense().cpu()
+#     D_v_neg_1_2 = torch.diag(H.sum(dim=1).view(-1) ** (-0.5))
+#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
+#     W_e = g1.W_e_of_group("main").to_dense()
+#     L_sym = (
+#         torch.eye(H.shape[0]) - D_v_neg_1_2 @ H @ W_e @ D_e_neg_1 @ H.t() @ D_v_neg_1_2
+#     )
+#     assert (L_sym == g1.L_sym_of_group("main").to_dense().cpu()).all()
+#     # knn group
+#     H = g1.H_of_group("knn").to_dense().cpu()
+#     D_v_neg_1_2 = H.sum(dim=1).view(-1) ** (-0.5)
+#     D_v_neg_1_2[torch.isinf(D_v_neg_1_2)] = 0
+#     D_v_neg_1_2 = torch.diag(D_v_neg_1_2)
+#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
+#     W_e = g1.W_e_of_group("knn").to_dense()
+#     L_sym = (
+#         torch.eye(H.shape[0]) - D_v_neg_1_2 @ H @ W_e @ D_e_neg_1 @ H.t() @ D_v_neg_1_2
+#     )
+#     assert (L_sym == g1.L_sym_of_group("knn").to_dense().cpu()).all()
+
+
+@pytest.mark.skipif(
+    sys.version_info.major <= 3 and sys.version_info.minor < 7,
+    reason="python requires >= 3.7",
+)
+# def test_L_rw(g1):
+#     import torch
+#
+#     H = g1.H.to_dense().cpu()
+#     D_v_neg_1 = torch.diag(H.sum(dim=1).view(-1) ** (-1))
+#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
+#     W_e = g1.W_e.to_dense()
+#     L_rw = torch.eye(H.shape[0]) - D_v_neg_1 @ H @ W_e @ D_e_neg_1 @ H.t()
+#     assert (L_rw == g1.L_rw.to_dense().cpu()).all()
+
+
+@pytest.mark.skipif(
+    sys.version_info.major <= 3 and sys.version_info.minor < 7,
+    reason="python requires >= 3.7",
+)
+# def test_L_rw_group(g1):
+#     import torch
+#
+#     g1.add_hyperedges([[0, 1]], group_name="knn")
+#     # all
+#     H = g1.H.to_dense().cpu()
+#     D_v_neg_1 = torch.diag(H.sum(dim=1).view(-1) ** (-1))
+#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
+#     W_e = g1.W_e.to_dense()
+#     L_rw = torch.eye(H.shape[0]) - D_v_neg_1 @ H @ W_e @ D_e_neg_1 @ H.t()
+#     assert (L_rw == g1.L_rw.to_dense().cpu()).all()
+#     # main group
+#     H = g1.H_of_group("main").to_dense().cpu()
+#     D_v_neg_1 = torch.diag(H.sum(dim=1).view(-1) ** (-1))
+#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
+#     W_e = g1.W_e_of_group("main").to_dense()
+#     L_rw = torch.eye(H.shape[0]) - D_v_neg_1 @ H @ W_e @ D_e_neg_1 @ H.t()
+#     assert (L_rw == g1.L_rw_of_group("main").to_dense().cpu()).all()
+#     # knn group
+#     H = g1.H_of_group("knn").to_dense().cpu()
+#     D_v_neg_1 = H.sum(dim=1).view(-1) ** (-1)
+#     D_v_neg_1[torch.isinf(D_v_neg_1)] = 0
+#     D_v_neg_1 = torch.diag(D_v_neg_1)
+#     D_e_neg_1 = torch.diag(H.sum(dim=0).view(-1) ** (-1))
+#     W_e = g1.W_e_of_group("knn").to_dense()
+#     L_rw = torch.eye(H.shape[0]) - D_v_neg_1 @ H @ W_e @ D_e_neg_1 @ H.t()
+#     assert (L_rw == g1.L_rw_of_group("knn").to_dense().cpu()).all()
+
+
+@pytest.mark.skipif(
+    sys.version_info.major <= 3 and sys.version_info.minor < 7,
+    reason="python requires >= 3.7",
+)
+def test_smoothing_with_HGNN(g1):
+    import torch
+
+    H = torch.tensor(
+        [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]],
+        dtype=torch.float32,
+    )
+    D_v_inv_1_2 = H.sum(1).view(-1) ** (-0.5)
+    D_v_inv_1_2[torch.isinf(D_v_inv_1_2)] = 0
+    D_v_inv_1_2 = torch.diag(D_v_inv_1_2)
+
+    D_e_inv = H.sum(0).view(-1) ** (-1)
+    D_e_inv[torch.isinf(D_e_inv)] = 0
+    D_e_inv = torch.diag(D_e_inv)
+
+    x = torch.rand(H.shape[0], 8)
+
+    gt = D_v_inv_1_2 @ H @ D_e_inv @ H.t() @ D_v_inv_1_2 @ x
+
+    res = g1.smoothing_with_HGNN(x)
+
+    assert pytest.approx(gt, rel=1e-6) == res.cpu()
+
+
+@pytest.mark.skipif(
+    sys.version_info.major <= 3 and sys.version_info.minor < 7,
+    reason="python requires >= 3.7",
+)
+def test_smoothing_with_HGNN_group(g1):
+    import torch
+
+    H = torch.tensor(
+        [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]],
+        dtype=torch.float32,
+    )
+    D_v_inv_1_2 = H.sum(1).view(-1) ** (-0.5)
+    D_v_inv_1_2[torch.isinf(D_v_inv_1_2)] = 0
+    D_v_inv_1_2 = torch.diag(D_v_inv_1_2)
+
+    D_e_inv = H.sum(0).view(-1) ** (-1)
+    D_e_inv[torch.isinf(D_e_inv)] = 0
+    D_e_inv = torch.diag(D_e_inv)
+
+    x = torch.rand(H.shape[0], 8)
+
+    gt = D_v_inv_1_2 @ H @ D_e_inv @ H.t() @ D_v_inv_1_2 @ x
+
+    res = g1.smoothing_with_HGNN_of_group("main", x)
+
+    assert pytest.approx(gt, rel=1e-6) == res.cpu()
+
+
+def test_v2e_message_passing(g1):
+    import torch
+
+    H = torch.tensor(
+        [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]],
+        dtype=torch.float32,
+    )
+
+    x = torch.rand(H.shape[0], 8)
+
+    gt_sum = H.t() @ x
+    res_sum = g1.v2e(x, aggr="sum")
+    assert pytest.approx(gt_sum, rel=1e-6) == res_sum.cpu()
+
+    D_e_inv = H.sum(0).view(-1) ** (-1)
+    D_e_inv[torch.isinf(D_e_inv)] = 0
+    D_e_inv = torch.diag(D_e_inv)
+
+    gt_mean = D_e_inv @ gt_sum
+    res_mean = g1.v2e(x, aggr="mean")
+    assert pytest.approx(gt_mean, rel=1e-6) == res_mean.cpu()
+
+
+def test_e2v_message_passing(g1):
+    import torch
+
+    H = torch.tensor(
+        [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]],
+        dtype=torch.float32,
+    )
+
+    x = torch.rand(3, 8)
+
+    gt_sum = H @ x
+    res_sum = g1.e2v(x, aggr="sum")
+    assert pytest.approx(gt_sum, rel=1e-6) == res_sum.cpu()
+
+    D_v_inv = H.sum(1).view(-1) ** (-1)
+    D_v_inv[torch.isinf(D_v_inv)] = 0
+    D_v_inv = torch.diag(D_v_inv)
+
+    gt_mean = D_v_inv @ gt_sum
+    res_mean = g1.e2v(x, aggr="mean")
+    assert pytest.approx(gt_mean, rel=1e-6) == res_mean.cpu()
+
+
+def test_v2v_message_passing(g1):
+    import torch
+
+    H = torch.tensor(
+        [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]],
+        dtype=torch.float32,
+    )
+
+    x = torch.rand(6, 8)
+
+    gt_sum = H @ H.t() @ x
+    res_sum = g1.v2v(x, aggr="sum")
+    assert pytest.approx(gt_sum, rel=1e-6) == res_sum.cpu()
+
+    D_v_inv = H.sum(1).view(-1) ** (-1)
+    D_v_inv[torch.isinf(D_v_inv)] = 0
+    D_v_inv = torch.diag(D_v_inv)
+
+    D_e_inv = H.sum(0).view(-1) ** (-1)
+    D_e_inv[torch.isinf(D_e_inv)] = 0
+    D_e_inv = torch.diag(D_e_inv)
+
+    gt_mean = D_v_inv @ H @ D_e_inv @ H.t() @ x
+    res_mean = g1.v2v(x, aggr="mean")
+    assert pytest.approx(gt_mean, rel=1e-6) == res_mean.cpu()
+
+
+@pytest.mark.skipif(
+    sys.version_info.major <= 3 and sys.version_info.minor < 7,
+    reason="python requires >= 3.7",
+)
+# def test_graph_and_hypergraph():
+#     import torch
+#
+#     g = eg.Graph()
+#     g.add_nodes([0, 1, 2, 3])
+#     g.add_edges(
+#         [(0, 1), (0, 2), (1, 3)], [{"weight": 1.0}, {"weight": 1.0}, {"weight": 1.0}]
+#     )
+#     hg = eg.Hypergraph.from_graph(g)
+#     _mm = torch.sparse.mm
+#     est_A = _mm(_mm(g.D_v_neg_1_2, g.A), g.D_v_neg_1_2) + torch.eye(4).to_sparse()
+#     assert pytest.approx(est_A.to_dense() / 2) == hg.L_HGNN.to_dense()
+
+
+@pytest.mark.skip(reason="skip")
+def test_get_linegraph():
+    num_v = 5
+    e_list = [[0, 1], [1, 2, 3], [0, 3, 4]]
+    e_weight = [1.0, 0.5, 2.0]
+    v_weight = [0.2, 0.3, 0.4, 0.5, 0.6]
+
+    hg = eg.Hypergraph(num_v=num_v, e_list=e_list, e_weight=e_weight)
+    lg = hg.get_clique_expansion()
+    assert lg.edges == [[0, 1], [0, 2], [1, 2]]
+    assert lg.nodes == [0, 1, 2]
```

## easygraph/classes/tests/test_operation.py

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-import easygraph as eg
-import pytest
-
-from easygraph.utils import edges_equal
-
-
-@pytest.mark.parametrize(
-    "graph_type", [eg.Graph, eg.DiGraph, eg.MultiGraph, eg.MultiDiGraph]
-)
-def test_selfloops(graph_type):
-    G = eg.complete_graph(3, create_using=graph_type)
-    G.add_edge(0, 0)
-    assert edges_equal(eg.selfloop_edges(G), [(0, 0)])
-    assert edges_equal(eg.selfloop_edges(G, data=True), [(0, 0, {})])
-    assert eg.number_of_selfloops(G) == 1
+import easygraph as eg
+import pytest
+
+from easygraph.utils import edges_equal
+
+
+@pytest.mark.parametrize(
+    "graph_type", [eg.Graph, eg.DiGraph, eg.MultiGraph, eg.MultiDiGraph]
+)
+def test_selfloops(graph_type):
+    G = eg.complete_graph(3, create_using=graph_type)
+    G.add_edge(0, 0)
+    assert edges_equal(eg.selfloop_edges(G), [(0, 0)])
+    assert edges_equal(eg.selfloop_edges(G, data=True), [(0, 0, {})])
+    assert eg.number_of_selfloops(G) == 1
```

## easygraph/datapipe/common.py

 * *Ordering differences only*

```diff
@@ -1,106 +1,106 @@
-from typing import Any
-from typing import Callable
-from typing import List
-from typing import Union
-
-import numpy as np
-import scipy.sparse
-import torch
-
-
-def to_tensor(
-    X: Union[list, np.ndarray, torch.Tensor, scipy.sparse.csr_matrix]
-) -> torch.Tensor:
-    r"""Convert ``List``, ``numpy.ndarray``, ``scipy.sparse.csr_matrix`` to ``torch.Tensor``.
-
-    Args:
-        ``X`` (``Union[List, np.ndarray, torch.Tensor, scipy.sparse.csr_matrix]``): Input.
-
-    Examples:
-        >>> import easygraph.datapipe as dd
-        >>> X = [[0.1, 0.2, 0.5],
-                 [0.5, 0.2, 0.3],
-                 [0.3, 0.2, 0]]
-        >>> dd.to_tensor(X)
-        tensor([[0.1000, 0.2000, 0.5000],
-                [0.5000, 0.2000, 0.3000],
-                [0.3000, 0.2000, 0.0000]])
-    """
-    if isinstance(X, list):
-        X = torch.tensor(X)
-    elif isinstance(X, scipy.sparse.csr_matrix):
-        X = X.todense()
-        X = torch.tensor(X)
-    elif isinstance(X, scipy.sparse.coo_matrix):
-        X = X.todense()
-        X = torch.tensor(X)
-    elif isinstance(X, np.ndarray):
-        X = torch.tensor(X)
-    else:
-        X = torch.tensor(X)
-    return X.float()
-
-
-def to_bool_tensor(X: Union[List, np.ndarray, torch.Tensor]) -> torch.BoolTensor:
-    r"""Convert ``List``, ``numpy.ndarray``, ``torch.Tensor`` to ``torch.BoolTensor``.
-
-    Args:
-        ``X`` (``Union[List, np.ndarray, torch.Tensor]``): Input.
-
-    Examples:
-        >>> import easygraph.datapipe as dd
-        >>> X = [[0.1, 0.2, 0.5],
-                 [0.5, 0.2, 0.3],
-                 [0.3, 0.2, 0]]
-        >>> dd.to_bool_tensor(X)
-        tensor([[ True,  True,  True],
-                [ True,  True,  True],
-                [ True,  True, False]])
-    """
-    if isinstance(X, list):
-        X = torch.tensor(X)
-    elif isinstance(X, np.ndarray):
-        X = torch.tensor(X)
-    else:
-        X = torch.tensor(X)
-    return X.bool()
-
-
-def to_long_tensor(X: Union[List, np.ndarray, torch.Tensor]) -> torch.LongTensor:
-    r"""Convert ``List``, ``numpy.ndarray``, ``torch.Tensor`` to ``torch.LongTensor``.
-
-    Args:
-        ``X`` (``Union[List, np.ndarray, torch.Tensor]``): Input.
-
-    Examples:
-        >>> import easygraph.datapipe as dd
-        >>> X = [[1, 2, 5],
-                 [5, 2, 3],
-                 [3, 2, 0]]
-        >>> dd.to_long_tensor(X)
-        tensor([[1, 2, 5],
-                [5, 2, 3],
-                [3, 2, 0]])
-    """
-    if isinstance(X, list):
-        X = torch.tensor(X)
-    elif isinstance(X, np.ndarray):
-        X = torch.tensor(X)
-    else:
-        X = torch.tensor(X)
-    return X.long()
-
-
-def compose_pipes(*pipes: Callable) -> Callable:
-    r"""Compose datapipe functions.
-
-    Args:
-        ``pipes`` (``Callable``): Datapipe functions to compose.
-    """
-
-    def composed_pipes(X: Any) -> torch.Tensor:
-        for pipe in pipes:
-            X = pipe(X)
-        return X
-
-    return composed_pipes
+from typing import Any
+from typing import Callable
+from typing import List
+from typing import Union
+
+import numpy as np
+import scipy.sparse
+import torch
+
+
+def to_tensor(
+    X: Union[list, np.ndarray, torch.Tensor, scipy.sparse.csr_matrix]
+) -> torch.Tensor:
+    r"""Convert ``List``, ``numpy.ndarray``, ``scipy.sparse.csr_matrix`` to ``torch.Tensor``.
+
+    Args:
+        ``X`` (``Union[List, np.ndarray, torch.Tensor, scipy.sparse.csr_matrix]``): Input.
+
+    Examples:
+        >>> import easygraph.datapipe as dd
+        >>> X = [[0.1, 0.2, 0.5],
+                 [0.5, 0.2, 0.3],
+                 [0.3, 0.2, 0]]
+        >>> dd.to_tensor(X)
+        tensor([[0.1000, 0.2000, 0.5000],
+                [0.5000, 0.2000, 0.3000],
+                [0.3000, 0.2000, 0.0000]])
+    """
+    if isinstance(X, list):
+        X = torch.tensor(X)
+    elif isinstance(X, scipy.sparse.csr_matrix):
+        X = X.todense()
+        X = torch.tensor(X)
+    elif isinstance(X, scipy.sparse.coo_matrix):
+        X = X.todense()
+        X = torch.tensor(X)
+    elif isinstance(X, np.ndarray):
+        X = torch.tensor(X)
+    else:
+        X = torch.tensor(X)
+    return X.float()
+
+
+def to_bool_tensor(X: Union[List, np.ndarray, torch.Tensor]) -> torch.BoolTensor:
+    r"""Convert ``List``, ``numpy.ndarray``, ``torch.Tensor`` to ``torch.BoolTensor``.
+
+    Args:
+        ``X`` (``Union[List, np.ndarray, torch.Tensor]``): Input.
+
+    Examples:
+        >>> import easygraph.datapipe as dd
+        >>> X = [[0.1, 0.2, 0.5],
+                 [0.5, 0.2, 0.3],
+                 [0.3, 0.2, 0]]
+        >>> dd.to_bool_tensor(X)
+        tensor([[ True,  True,  True],
+                [ True,  True,  True],
+                [ True,  True, False]])
+    """
+    if isinstance(X, list):
+        X = torch.tensor(X)
+    elif isinstance(X, np.ndarray):
+        X = torch.tensor(X)
+    else:
+        X = torch.tensor(X)
+    return X.bool()
+
+
+def to_long_tensor(X: Union[List, np.ndarray, torch.Tensor]) -> torch.LongTensor:
+    r"""Convert ``List``, ``numpy.ndarray``, ``torch.Tensor`` to ``torch.LongTensor``.
+
+    Args:
+        ``X`` (``Union[List, np.ndarray, torch.Tensor]``): Input.
+
+    Examples:
+        >>> import easygraph.datapipe as dd
+        >>> X = [[1, 2, 5],
+                 [5, 2, 3],
+                 [3, 2, 0]]
+        >>> dd.to_long_tensor(X)
+        tensor([[1, 2, 5],
+                [5, 2, 3],
+                [3, 2, 0]])
+    """
+    if isinstance(X, list):
+        X = torch.tensor(X)
+    elif isinstance(X, np.ndarray):
+        X = torch.tensor(X)
+    else:
+        X = torch.tensor(X)
+    return X.long()
+
+
+def compose_pipes(*pipes: Callable) -> Callable:
+    r"""Compose datapipe functions.
+
+    Args:
+        ``pipes`` (``Callable``): Datapipe functions to compose.
+    """
+
+    def composed_pipes(X: Any) -> torch.Tensor:
+        for pipe in pipes:
+            X = pipe(X)
+        return X
+
+    return composed_pipes
```

## easygraph/datapipe/normalize.py

 * *Ordering differences only*

```diff
@@ -1,74 +1,74 @@
-from typing import Optional
-from typing import Union
-
-import torch
-
-
-def norm_ft(X: torch.Tensor, ord: Optional[Union[int, float]] = None) -> torch.Tensor:
-    r"""Normalize the input feature matrix with specified ``ord`` refer to pytorch's `torch.linalg.norm <https://pytorch.org/docs/stable/generated/torch.linalg.norm.html#torch.linalg.norm>`_ function.
-
-    .. note::
-        The input feature matrix is expected to be a 1D vector or a 2D tensor with shape (num_samples, num_features).
-
-    Args:
-        ``X`` (``torch.Tensor``): The input feature.
-        ``ord`` (``Union[int, float]``, optional): The order of the norm can be either an ``int``, ``float``. If ``ord`` is ``None``, the norm is computed with the 2-norm. Defaults to ``None``.
-
-    Examples:
-        >>> import easygraph.datapipe as dd
-        >>> import torch
-        >>> X = torch.tensor([
-                    [0.1, 0.2, 0.5],
-                    [0.5, 0.2, 0.3],
-                    [0.3, 0.2, 0]
-                ])
-        >>> dd.norm_ft(X)
-        tensor([[0.1826, 0.3651, 0.9129],
-                [0.8111, 0.3244, 0.4867],
-                [0.8321, 0.5547, 0.0000]])
-    """
-    if X.dim() == 1:
-        X_norm = 1 / torch.linalg.norm(X, ord=ord)
-        X_norm[torch.isinf(X_norm)] = 0
-        return X * X_norm
-    elif X.dim() == 2:
-        X_norm = 1 / torch.linalg.norm(X, ord=ord, dim=1, keepdim=True)
-        X_norm[torch.isinf(X_norm)] = 0
-        return X * X_norm
-    else:
-        raise ValueError(
-            "The input feature matrix is expected to be a 1D verter or a 2D tensor with"
-            " shape (num_samples, num_features)."
-        )
-
-
-def min_max_scaler(X: torch.Tensor, ft_min: float, ft_max: float) -> torch.Tensor:
-    r"""Normalize the input feature matrix with min-max scaling.
-
-    Args:
-        ``X`` (``torch.Tensor``): The input feature.
-        ``ft_min`` (``float``): The minimum value of the output feature.
-        ``ft_max`` (``float``): The maximum value of the output feature.
-
-    Examples:
-        >>> import easygraph.datapipe as dd
-        >>> import torch
-        >>> X = torch.tensor([
-                    [0.1, 0.2, 0.5],
-                    [0.5, 0.2, 0.3],
-                    [0.3, 0.2, 0.0]
-                ])
-        >>> dd.min_max_scaler(X, -1, 1)
-        tensor([[-0.6000, -0.2000,  1.0000],
-                [ 1.0000, -0.2000,  0.2000],
-                [ 0.2000, -0.2000, -1.0000]])
-    """
-    assert (
-        ft_min < ft_max
-    ), "The minimum value of the feature should be less than the maximum value."
-    X_min, X_max = X.min().item(), X.max().item()
-    X_range = X_max - X_min
-    scale_ = (ft_max - ft_min) / X_range
-    min_ = ft_min - X_min * scale_
-    X = X * scale_ + min_
-    return X
+from typing import Optional
+from typing import Union
+
+import torch
+
+
+def norm_ft(X: torch.Tensor, ord: Optional[Union[int, float]] = None) -> torch.Tensor:
+    r"""Normalize the input feature matrix with specified ``ord`` refer to pytorch's `torch.linalg.norm <https://pytorch.org/docs/stable/generated/torch.linalg.norm.html#torch.linalg.norm>`_ function.
+
+    .. note::
+        The input feature matrix is expected to be a 1D vector or a 2D tensor with shape (num_samples, num_features).
+
+    Args:
+        ``X`` (``torch.Tensor``): The input feature.
+        ``ord`` (``Union[int, float]``, optional): The order of the norm can be either an ``int``, ``float``. If ``ord`` is ``None``, the norm is computed with the 2-norm. Defaults to ``None``.
+
+    Examples:
+        >>> import easygraph.datapipe as dd
+        >>> import torch
+        >>> X = torch.tensor([
+                    [0.1, 0.2, 0.5],
+                    [0.5, 0.2, 0.3],
+                    [0.3, 0.2, 0]
+                ])
+        >>> dd.norm_ft(X)
+        tensor([[0.1826, 0.3651, 0.9129],
+                [0.8111, 0.3244, 0.4867],
+                [0.8321, 0.5547, 0.0000]])
+    """
+    if X.dim() == 1:
+        X_norm = 1 / torch.linalg.norm(X, ord=ord)
+        X_norm[torch.isinf(X_norm)] = 0
+        return X * X_norm
+    elif X.dim() == 2:
+        X_norm = 1 / torch.linalg.norm(X, ord=ord, dim=1, keepdim=True)
+        X_norm[torch.isinf(X_norm)] = 0
+        return X * X_norm
+    else:
+        raise ValueError(
+            "The input feature matrix is expected to be a 1D verter or a 2D tensor with"
+            " shape (num_samples, num_features)."
+        )
+
+
+def min_max_scaler(X: torch.Tensor, ft_min: float, ft_max: float) -> torch.Tensor:
+    r"""Normalize the input feature matrix with min-max scaling.
+
+    Args:
+        ``X`` (``torch.Tensor``): The input feature.
+        ``ft_min`` (``float``): The minimum value of the output feature.
+        ``ft_max`` (``float``): The maximum value of the output feature.
+
+    Examples:
+        >>> import easygraph.datapipe as dd
+        >>> import torch
+        >>> X = torch.tensor([
+                    [0.1, 0.2, 0.5],
+                    [0.5, 0.2, 0.3],
+                    [0.3, 0.2, 0.0]
+                ])
+        >>> dd.min_max_scaler(X, -1, 1)
+        tensor([[-0.6000, -0.2000,  1.0000],
+                [ 1.0000, -0.2000,  0.2000],
+                [ 0.2000, -0.2000, -1.0000]])
+    """
+    assert (
+        ft_min < ft_max
+    ), "The minimum value of the feature should be less than the maximum value."
+    X_min, X_max = X.min().item(), X.max().item()
+    X_range = X_max - X_min
+    scale_ = (ft_max - ft_min) / X_range
+    min_ = ft_min - X_min * scale_
+    X = X * scale_ + min_
+    return X
```

## easygraph/datapipe/loader.py

 * *Ordering differences only*

```diff
@@ -1,90 +1,90 @@
-import json
-import pickle as pkl
-import re
-
-from pathlib import Path
-from typing import Callable
-from typing import List
-from typing import Optional
-from typing import Union
-
-
-def load_from_pickle(
-    file_path: Path, keys: Optional[Union[str, List[str]]] = None, **kwargs
-):
-    r"""Load data from a pickle file.
-
-    Args:
-        ``file_path`` (``Path``): The local path of the file.
-        ``keys`` (``Union[str, List[str]]``, optional): The keys of the data. Defaults to ``None``.
-    """
-    if isinstance(file_path, list):
-        raise ValueError("This function only support loading data from a single file.")
-    with open(file_path, "rb") as f:
-        data = pkl.load(f, **kwargs)
-    if keys is None:
-        return data
-    elif isinstance(keys, str):
-        return data[keys]
-    else:
-        return {key: data[key] for key in keys}
-
-
-def load_from_json(file_path: Path, **kwargs):
-    r"""Load data from a json file.
-
-    Args:
-        ``file_path`` (``Path``): The local path of the file.
-    """
-    with open(file_path, "r") as f:
-        data = json.load(f, **kwargs)
-    return data
-
-
-def load_from_txt(
-    file_path: Path,
-    dtype: Union[str, Callable],
-    sep: str = ",| |\t",
-    ignore_header: int = 0,
-):
-    r"""Load data from a txt file.
-
-    .. note::
-        The separator is a regular expression of ``re`` module. Multiple separators can be separated by ``|``. More details can refer to `re.split <https://docs.python.org/3/library/re.html#re.split>`_.
-
-    Args:
-        ``file_path`` (``Path``): The local path of the file.
-        ``dtype`` (``Union[str, Callable]``): The data type of the data can be either a string or a callable function.
-        ``sep`` (``str``, optional): The separator of each line in the file. Defaults to ``",| |\t"``.
-        ``ignore_header`` (``int``, optional): The number of lines to ignore in the header of the file. Defaults to ``0``.
-    """
-    cast_fun = ret_cast_fun(dtype)
-    file_path = Path(file_path)
-    assert file_path.exists(), f"{file_path} does not exist."
-    data = []
-    with open(file_path, "r") as f:
-        for _ in range(ignore_header):
-            f.readline()
-        data = [
-            list(map(cast_fun, re.split(sep, line.strip()))) for line in f.readlines()
-        ]
-    return data
-
-
-def ret_cast_fun(dtype: Union[str, Callable]):
-    r"""Return the cast function of the data type. The supported data types are: ``int``, ``float``, ``str``.
-
-    Args:
-        ``dtype`` (``Union[str, Callable]``): The data type of the data can be either a string or a callable function.
-    """
-    if isinstance(dtype, str):
-        if dtype == "int":
-            return int
-        elif dtype == "float":
-            return float
-        elif dtype == "str":
-            return str
-        else:
-            raise ValueError("dtype must be one of 'int', 'float', 'str'.")
-    else:
-        return dtype
+import json
+import pickle as pkl
+import re
+
+from pathlib import Path
+from typing import Callable
+from typing import List
+from typing import Optional
+from typing import Union
+
+
+def load_from_pickle(
+    file_path: Path, keys: Optional[Union[str, List[str]]] = None, **kwargs
+):
+    r"""Load data from a pickle file.
+
+    Args:
+        ``file_path`` (``Path``): The local path of the file.
+        ``keys`` (``Union[str, List[str]]``, optional): The keys of the data. Defaults to ``None``.
+    """
+    if isinstance(file_path, list):
+        raise ValueError("This function only support loading data from a single file.")
+    with open(file_path, "rb") as f:
+        data = pkl.load(f, **kwargs)
+    if keys is None:
+        return data
+    elif isinstance(keys, str):
+        return data[keys]
+    else:
+        return {key: data[key] for key in keys}
+
+
+def load_from_json(file_path: Path, **kwargs):
+    r"""Load data from a json file.
+
+    Args:
+        ``file_path`` (``Path``): The local path of the file.
+    """
+    with open(file_path, "r") as f:
+        data = json.load(f, **kwargs)
+    return data
+
+
+def load_from_txt(
+    file_path: Path,
+    dtype: Union[str, Callable],
+    sep: str = ",| |\t",
+    ignore_header: int = 0,
+):
+    r"""Load data from a txt file.
+
+    .. note::
+        The separator is a regular expression of ``re`` module. Multiple separators can be separated by ``|``. More details can refer to `re.split <https://docs.python.org/3/library/re.html#re.split>`_.
+
+    Args:
+        ``file_path`` (``Path``): The local path of the file.
+        ``dtype`` (``Union[str, Callable]``): The data type of the data can be either a string or a callable function.
+        ``sep`` (``str``, optional): The separator of each line in the file. Defaults to ``",| |\t"``.
+        ``ignore_header`` (``int``, optional): The number of lines to ignore in the header of the file. Defaults to ``0``.
+    """
+    cast_fun = ret_cast_fun(dtype)
+    file_path = Path(file_path)
+    assert file_path.exists(), f"{file_path} does not exist."
+    data = []
+    with open(file_path, "r") as f:
+        for _ in range(ignore_header):
+            f.readline()
+        data = [
+            list(map(cast_fun, re.split(sep, line.strip()))) for line in f.readlines()
+        ]
+    return data
+
+
+def ret_cast_fun(dtype: Union[str, Callable]):
+    r"""Return the cast function of the data type. The supported data types are: ``int``, ``float``, ``str``.
+
+    Args:
+        ``dtype`` (``Union[str, Callable]``): The data type of the data can be either a string or a callable function.
+    """
+    if isinstance(dtype, str):
+        if dtype == "int":
+            return int
+        elif dtype == "float":
+            return float
+        elif dtype == "str":
+            return str
+        else:
+            raise ValueError("dtype must be one of 'int', 'float', 'str'.")
+    else:
+        return dtype
```

## easygraph/datapipe/__init__.py

 * *Ordering differences only*

```diff
@@ -1,29 +1,29 @@
-try:
-    from .common import compose_pipes
-    from .common import to_bool_tensor
-    from .common import to_long_tensor
-    from .common import to_tensor
-    from .normalize import min_max_scaler
-    from .normalize import norm_ft
-except:
-    print(
-        "Warning raise in module:datapipe. Please install Pytorch before you use"
-        " functions related to nueral network"
-    )
-
-from .loader import load_from_json
-from .loader import load_from_pickle
-from .loader import load_from_txt
-
-
-# __all__ = [
-#     "compose_pipes",
-#     "norm_ft",
-#     "min_max_scaler",
-#     "to_tensor",
-#     "to_bool_tensor",
-#     "to_long_tensor",
-#     "load_from_pickle",
-#     "load_from_json",
-#     "load_from_txt",
-# ]
+try:
+    from .common import compose_pipes
+    from .common import to_bool_tensor
+    from .common import to_long_tensor
+    from .common import to_tensor
+    from .normalize import min_max_scaler
+    from .normalize import norm_ft
+except:
+    print(
+        "Warning raise in module:datapipe. Please install Pytorch before you use"
+        " functions related to nueral network"
+    )
+
+from .loader import load_from_json
+from .loader import load_from_pickle
+from .loader import load_from_txt
+
+
+# __all__ = [
+#     "compose_pipes",
+#     "norm_ft",
+#     "min_max_scaler",
+#     "to_tensor",
+#     "to_bool_tensor",
+#     "to_long_tensor",
+#     "load_from_pickle",
+#     "load_from_json",
+#     "load_from_txt",
+# ]
```

## easygraph/datasets/ppi.py

 * *Ordering differences only*

```diff
@@ -1,216 +1,216 @@
-""" PPIDataset for inductive learning. """
-import json
-import os
-
-import numpy as np
-
-from easygraph.classes.directed_graph import DiGraph
-
-from ..readwrite import json_graph
-from .graph_dataset_base import EasyGraphBuiltinDataset
-from .utils import _get_dgl_url
-from .utils import data_type_dict
-from .utils import tensor
-
-
-class PPIDataset(EasyGraphBuiltinDataset):
-    r"""Protein-Protein Interaction dataset for inductive node classification
-
-    A toy Protein-Protein Interaction network dataset. The dataset contains
-    24 graphs. The average number of nodes per graph is 2372. Each node has
-    50 features and 121 labels. 20 graphs for training, 2 for validation
-    and 2 for testing.
-
-    Reference: `<http://snap.stanford.edu/graphsage/>`_
-
-    Statistics:
-
-    - Train examples: 20
-    - Valid examples: 2
-    - Test examples: 2
-
-    Parameters
-    ----------
-    mode : str
-        Must be one of ('train', 'valid', 'test').
-        Default: 'train'
-    raw_dir : str
-        Raw file directory to download/contains the input data directory.
-        Default: ~/.eg/
-    force_reload : bool
-        Whether to reload the dataset.
-        Default: False
-    verbose : bool
-        Whether to print out progress information.
-        Default: True.
-    transform : callable, optional
-        A transform that takes in a :class:`~eg.DGLGraph` object and returns
-        a transformed version. The :class:`~eg.DGLGraph` object will be
-        transformed before every access.
-
-    Attributes
-    ----------
-    num_labels : int
-        Number of labels for each node
-    labels : Tensor
-        Node labels
-    features : Tensor
-        Node features
-
-    Examples
-    --------
-    >>> dataset = PPIDataset(mode='valid')
-    >>> num_labels = dataset.num_labels
-    >>> for g in dataset:
-    ....    feat = g.ndata['feat']
-    ....    label = g.ndata['label']
-    ....    # your code here
-    >>>
-    """
-
-    def __init__(
-        self,
-        mode="train",
-        raw_dir=None,
-        force_reload=False,
-        verbose=False,
-        transform=None,
-    ):
-        assert mode in ["train", "valid", "test"]
-        self.mode = mode
-        _url = _get_dgl_url("dataset/ppi.zip")
-        super(PPIDataset, self).__init__(
-            name="ppi",
-            url=_url,
-            raw_dir=raw_dir,
-            force_reload=force_reload,
-            verbose=verbose,
-            transform=transform,
-        )
-
-    def process(self):
-        graph_file = os.path.join(
-            self.save_path, "ppi", "{}_graph.json".format(self.mode)
-        )
-        label_file = os.path.join(
-            self.save_path, "ppi", "{}_labels.npy".format(self.mode)
-        )
-        feat_file = os.path.join(
-            self.save_path, "ppi", "{}_feats.npy".format(self.mode)
-        )
-        graph_id_file = os.path.join(
-            self.save_path, "ppi", "{}_graph_id.npy".format(self.mode)
-        )
-
-        g_data = json.load(open(graph_file))
-        self._labels = np.load(label_file)
-        self._feats = np.load(feat_file)
-        self.graph = DiGraph(json_graph.node_link_graph(g_data))
-        graph_id = np.load(graph_id_file)
-
-        # lo, hi means the range of graph ids for different portion of the dataset,
-        # 20 graphs for training, 2 for validation and 2 for testing.
-        lo, hi = 1, 21
-        if self.mode == "valid":
-            lo, hi = 21, 23
-        elif self.mode == "test":
-            lo, hi = 23, 25
-
-        graph_masks = []
-        self.graphs = []
-        for g_id in range(lo, hi):
-            g_mask = np.where(graph_id == g_id)[0]
-            graph_masks.append(g_mask)
-            g = self.graph.nodes_subgraph(g_mask)
-            g.ndata["feat"] = tensor(
-                self._feats[g_mask], dtype=data_type_dict()["float32"]
-            )
-            g.ndata["label"] = tensor(
-                self._labels[g_mask], dtype=data_type_dict()["float32"]
-            )
-            self.graphs.append(g)
-
-    def has_cache(self):
-        graph_list_path = os.path.join(
-            self.save_path, "{}_eg_graph_list.bin".format(self.mode)
-        )
-        g_path = os.path.join(self.save_path, "{}_eg_graph.bin".format(self.mode))
-        info_path = os.path.join(self.save_path, "{}_info.pkl".format(self.mode))
-        return (
-            os.path.exists(graph_list_path)
-            and os.path.exists(g_path)
-            and os.path.exists(info_path)
-        )
-
-    def save(self):
-        graph_list_path = os.path.join(
-            self.save_path, "{}_eg_graph_list.bin".format(self.mode)
-        )
-        g_path = os.path.join(self.save_path, "{}_eg_graph.bin".format(self.mode))
-        info_path = os.path.join(self.save_path, "{}_info.pkl".format(self.mode))
-        # save_graphs(graph_list_path, self.graphs)
-        # save_graphs(g_path, self.graph)
-        # save_info(info_path, {'labels': self._labels, 'feats': self._feats})
-
-    # def load(self):
-    #     graph_list_path = os.path.join(self.save_path, '{}_eg_graph_list.bin'.format(self.mode))
-    #     g_path = os.path.join(self.save_path, '{}_eg_graph.bin'.format(self.mode))
-    #     info_path = os.path.join(self.save_path, '{}_info.pkl'.format(self.mode))
-    #     self.graphs = load_graphs(graph_list_path)[0]
-    #     g, _ = load_graphs(g_path)
-    #     self.graph = g[0]
-    #     info = load_info(info_path)
-    #     self._labels = info['labels']
-    #     self._feats = info['feats']
-
-    @property
-    def num_labels(self):
-        return 121
-
-    def __len__(self):
-        """Return number of samples in this dataset."""
-        return len(self.graphs)
-
-    def __getitem__(self, item):
-        """Get the item^th sample.
-
-        Parameters
-        ---------
-        item : int
-            The sample index.
-
-        Returns
-        -------
-        :class:`eg.Graph`
-            graph structure, node features and node labels.
-
-            - ``ndata['feat']``: node features
-            - ``ndata['label']``: node labels
-        """
-        if self._transform is None:
-            return self.graphs[item]
-        else:
-            return self._transform(self.graphs[item])
-
-
-class LegacyPPIDataset(PPIDataset):
-    """Legacy version of PPI Dataset"""
-
-    def __getitem__(self, item):
-        """Get the item^th sample.
-
-        Parameters
-        ---------
-        idx : int
-            The sample index.
-
-        Returns
-        -------
-        (eg.DGLGraph, Tensor, Tensor)
-            The graph, features and its label.
-        """
-        if self._transform is None:
-            g = self.graphs[item]
-        else:
-            g = self._transform(self.graphs[item])
-        return g, g.ndata["feat"], g.ndata["label"]
+""" PPIDataset for inductive learning. """
+import json
+import os
+
+import numpy as np
+
+from easygraph.classes.directed_graph import DiGraph
+
+from ..readwrite import json_graph
+from .graph_dataset_base import EasyGraphBuiltinDataset
+from .utils import _get_dgl_url
+from .utils import data_type_dict
+from .utils import tensor
+
+
+class PPIDataset(EasyGraphBuiltinDataset):
+    r"""Protein-Protein Interaction dataset for inductive node classification
+
+    A toy Protein-Protein Interaction network dataset. The dataset contains
+    24 graphs. The average number of nodes per graph is 2372. Each node has
+    50 features and 121 labels. 20 graphs for training, 2 for validation
+    and 2 for testing.
+
+    Reference: `<http://snap.stanford.edu/graphsage/>`_
+
+    Statistics:
+
+    - Train examples: 20
+    - Valid examples: 2
+    - Test examples: 2
+
+    Parameters
+    ----------
+    mode : str
+        Must be one of ('train', 'valid', 'test').
+        Default: 'train'
+    raw_dir : str
+        Raw file directory to download/contains the input data directory.
+        Default: ~/.eg/
+    force_reload : bool
+        Whether to reload the dataset.
+        Default: False
+    verbose : bool
+        Whether to print out progress information.
+        Default: True.
+    transform : callable, optional
+        A transform that takes in a :class:`~eg.DGLGraph` object and returns
+        a transformed version. The :class:`~eg.DGLGraph` object will be
+        transformed before every access.
+
+    Attributes
+    ----------
+    num_labels : int
+        Number of labels for each node
+    labels : Tensor
+        Node labels
+    features : Tensor
+        Node features
+
+    Examples
+    --------
+    >>> dataset = PPIDataset(mode='valid')
+    >>> num_labels = dataset.num_labels
+    >>> for g in dataset:
+    ....    feat = g.ndata['feat']
+    ....    label = g.ndata['label']
+    ....    # your code here
+    >>>
+    """
+
+    def __init__(
+        self,
+        mode="train",
+        raw_dir=None,
+        force_reload=False,
+        verbose=False,
+        transform=None,
+    ):
+        assert mode in ["train", "valid", "test"]
+        self.mode = mode
+        _url = _get_dgl_url("dataset/ppi.zip")
+        super(PPIDataset, self).__init__(
+            name="ppi",
+            url=_url,
+            raw_dir=raw_dir,
+            force_reload=force_reload,
+            verbose=verbose,
+            transform=transform,
+        )
+
+    def process(self):
+        graph_file = os.path.join(
+            self.save_path, "ppi", "{}_graph.json".format(self.mode)
+        )
+        label_file = os.path.join(
+            self.save_path, "ppi", "{}_labels.npy".format(self.mode)
+        )
+        feat_file = os.path.join(
+            self.save_path, "ppi", "{}_feats.npy".format(self.mode)
+        )
+        graph_id_file = os.path.join(
+            self.save_path, "ppi", "{}_graph_id.npy".format(self.mode)
+        )
+
+        g_data = json.load(open(graph_file))
+        self._labels = np.load(label_file)
+        self._feats = np.load(feat_file)
+        self.graph = DiGraph(json_graph.node_link_graph(g_data))
+        graph_id = np.load(graph_id_file)
+
+        # lo, hi means the range of graph ids for different portion of the dataset,
+        # 20 graphs for training, 2 for validation and 2 for testing.
+        lo, hi = 1, 21
+        if self.mode == "valid":
+            lo, hi = 21, 23
+        elif self.mode == "test":
+            lo, hi = 23, 25
+
+        graph_masks = []
+        self.graphs = []
+        for g_id in range(lo, hi):
+            g_mask = np.where(graph_id == g_id)[0]
+            graph_masks.append(g_mask)
+            g = self.graph.nodes_subgraph(g_mask)
+            g.ndata["feat"] = tensor(
+                self._feats[g_mask], dtype=data_type_dict()["float32"]
+            )
+            g.ndata["label"] = tensor(
+                self._labels[g_mask], dtype=data_type_dict()["float32"]
+            )
+            self.graphs.append(g)
+
+    def has_cache(self):
+        graph_list_path = os.path.join(
+            self.save_path, "{}_eg_graph_list.bin".format(self.mode)
+        )
+        g_path = os.path.join(self.save_path, "{}_eg_graph.bin".format(self.mode))
+        info_path = os.path.join(self.save_path, "{}_info.pkl".format(self.mode))
+        return (
+            os.path.exists(graph_list_path)
+            and os.path.exists(g_path)
+            and os.path.exists(info_path)
+        )
+
+    def save(self):
+        graph_list_path = os.path.join(
+            self.save_path, "{}_eg_graph_list.bin".format(self.mode)
+        )
+        g_path = os.path.join(self.save_path, "{}_eg_graph.bin".format(self.mode))
+        info_path = os.path.join(self.save_path, "{}_info.pkl".format(self.mode))
+        # save_graphs(graph_list_path, self.graphs)
+        # save_graphs(g_path, self.graph)
+        # save_info(info_path, {'labels': self._labels, 'feats': self._feats})
+
+    # def load(self):
+    #     graph_list_path = os.path.join(self.save_path, '{}_eg_graph_list.bin'.format(self.mode))
+    #     g_path = os.path.join(self.save_path, '{}_eg_graph.bin'.format(self.mode))
+    #     info_path = os.path.join(self.save_path, '{}_info.pkl'.format(self.mode))
+    #     self.graphs = load_graphs(graph_list_path)[0]
+    #     g, _ = load_graphs(g_path)
+    #     self.graph = g[0]
+    #     info = load_info(info_path)
+    #     self._labels = info['labels']
+    #     self._feats = info['feats']
+
+    @property
+    def num_labels(self):
+        return 121
+
+    def __len__(self):
+        """Return number of samples in this dataset."""
+        return len(self.graphs)
+
+    def __getitem__(self, item):
+        """Get the item^th sample.
+
+        Parameters
+        ---------
+        item : int
+            The sample index.
+
+        Returns
+        -------
+        :class:`eg.Graph`
+            graph structure, node features and node labels.
+
+            - ``ndata['feat']``: node features
+            - ``ndata['label']``: node labels
+        """
+        if self._transform is None:
+            return self.graphs[item]
+        else:
+            return self._transform(self.graphs[item])
+
+
+class LegacyPPIDataset(PPIDataset):
+    """Legacy version of PPI Dataset"""
+
+    def __getitem__(self, item):
+        """Get the item^th sample.
+
+        Parameters
+        ---------
+        idx : int
+            The sample index.
+
+        Returns
+        -------
+        (eg.DGLGraph, Tensor, Tensor)
+            The graph, features and its label.
+        """
+        if self._transform is None:
+            g = self.graphs[item]
+        else:
+            g = self._transform(self.graphs[item])
+        return g, g.ndata["feat"], g.ndata["label"]
```

## easygraph/datasets/karate.py

 * *Ordering differences only*

```diff
@@ -1,93 +1,93 @@
-import easygraph as eg
-
-from .graph_dataset_base import EasyGraphDataset
-from .utils import _set_labels
-from .utils import tensor
-
-
-""" KarateClubDataset for inductive learning. """
-
-
-class KarateClubDataset(EasyGraphDataset):
-    """Karate Club dataset for Node Classification
-
-    Zachary's karate club is a social network of a university
-    karate club, described in the paper "An Information Flow
-    Model for Conflict and Fission in Small Groups" by Wayne W. Zachary.
-    The network became a popular example of community structure in
-    networks after its use by Michelle Girvan and Mark Newman in 2002.
-    Official website: `<http://konect.cc/networks/ucidata-zachary/>`_
-
-    Karate Club dataset statistics:
-
-    - Nodes: 34
-    - Edges: 156
-    - Number of Classes: 2
-
-    Parameters
-    ----------
-    transform : callable, optional
-        A transform that takes in a :class:`~eg.Graph` object and returns
-        a transformed version. The :class:`~eg.Graph` object will be
-        transformed before every access.
-
-    Attributes
-    ----------
-    num_classes : int
-        Number of node classes
-
-    Examples
-    --------
-    >>> dataset = KarateClubDataset()
-    >>> num_classes = dataset.num_classes
-    >>> g = dataset[0]
-    >>> labels = g.ndata['label']
-    """
-
-    def __init__(self, transform=None):
-        super(KarateClubDataset, self).__init__(name="karate_club", transform=transform)
-
-    def process(self):
-        import numpy as np
-
-        kc_graph = eg.get_graph_karateclub()
-        label = np.asarray(
-            [kc_graph.nodes[i]["club"] != "Mr. Hi" for i in kc_graph.nodes]
-        ).astype(np.int64)
-        label = tensor(label)
-
-        kc_graph = _set_labels(kc_graph, label)
-        kc_graph.ndata["label"] = label
-        self._graph = kc_graph
-        self._data = [kc_graph]
-
-    @property
-    def num_classes(self):
-        """Number of classes."""
-        return 2
-
-    def __getitem__(self, idx):
-        r"""Get graph object
-
-        Parameters
-        ----------
-        idx : int
-            Item index, KarateClubDataset has only one graph object
-
-        Returns
-        -------
-        :class:`eg.Graph`
-
-            graph structure and labels.
-
-            - ``ndata['label']``: ground truth labels
-        """
-        assert idx == 0, "This dataset has only one graph"
-        if self._transform is None:
-            return self._graph
-        else:
-            return self._transform(self._graph)
-
-    def __len__(self):
-        r"""The number of graphs in the dataset."""
-        return 1
+import easygraph as eg
+
+from .graph_dataset_base import EasyGraphDataset
+from .utils import _set_labels
+from .utils import tensor
+
+
+""" KarateClubDataset for inductive learning. """
+
+
+class KarateClubDataset(EasyGraphDataset):
+    """Karate Club dataset for Node Classification
+
+    Zachary's karate club is a social network of a university
+    karate club, described in the paper "An Information Flow
+    Model for Conflict and Fission in Small Groups" by Wayne W. Zachary.
+    The network became a popular example of community structure in
+    networks after its use by Michelle Girvan and Mark Newman in 2002.
+    Official website: `<http://konect.cc/networks/ucidata-zachary/>`_
+
+    Karate Club dataset statistics:
+
+    - Nodes: 34
+    - Edges: 156
+    - Number of Classes: 2
+
+    Parameters
+    ----------
+    transform : callable, optional
+        A transform that takes in a :class:`~eg.Graph` object and returns
+        a transformed version. The :class:`~eg.Graph` object will be
+        transformed before every access.
+
+    Attributes
+    ----------
+    num_classes : int
+        Number of node classes
+
+    Examples
+    --------
+    >>> dataset = KarateClubDataset()
+    >>> num_classes = dataset.num_classes
+    >>> g = dataset[0]
+    >>> labels = g.ndata['label']
+    """
+
+    def __init__(self, transform=None):
+        super(KarateClubDataset, self).__init__(name="karate_club", transform=transform)
+
+    def process(self):
+        import numpy as np
+
+        kc_graph = eg.get_graph_karateclub()
+        label = np.asarray(
+            [kc_graph.nodes[i]["club"] != "Mr. Hi" for i in kc_graph.nodes]
+        ).astype(np.int64)
+        label = tensor(label)
+
+        kc_graph = _set_labels(kc_graph, label)
+        kc_graph.ndata["label"] = label
+        self._graph = kc_graph
+        self._data = [kc_graph]
+
+    @property
+    def num_classes(self):
+        """Number of classes."""
+        return 2
+
+    def __getitem__(self, idx):
+        r"""Get graph object
+
+        Parameters
+        ----------
+        idx : int
+            Item index, KarateClubDataset has only one graph object
+
+        Returns
+        -------
+        :class:`eg.Graph`
+
+            graph structure and labels.
+
+            - ``ndata['label']``: ground truth labels
+        """
+        assert idx == 0, "This dataset has only one graph"
+        if self._transform is None:
+            return self._graph
+        else:
+            return self._transform(self._graph)
+
+    def __len__(self):
+        r"""The number of graphs in the dataset."""
+        return 1
```

## easygraph/datasets/utils.py

 * *Ordering differences only*

```diff
@@ -1,358 +1,358 @@
-import errno
-import hashlib
-import numbers
-import os
-
-from pathlib import Path
-
-import numpy as np
-import requests
-import torch as th
-
-
-__all__ = [
-    "download",
-    "extract_archive",
-    "get_download_dir",
-    "makedirs",
-    "generate_mask_tensor",
-]
-
-import warnings
-
-from easygraph.utils.download import _retry
-
-
-def _get_eg_url(file_url):
-    """Get EasyGraph online url for download."""
-    eg_repo_url = "https://gitlab.com/easy-graph/"
-    repo_url = eg_repo_url
-    if repo_url[-1] != "/":
-        repo_url = repo_url + "/"
-    return repo_url + file_url
-
-
-def _get_dgl_url(file_url):
-    """Get DGL online url for download."""
-    dgl_repo_url = "https://data.dgl.ai/"
-    repo_url = os.environ.get("DGL_REPO", dgl_repo_url)
-    if repo_url[-1] != "/":
-        repo_url = repo_url + "/"
-    return repo_url + file_url
-
-
-def _set_labels(G, labels):
-    index = 0
-    for node in G.nodes:
-        G.add_node(node, label=labels[index])
-        index += 1
-    return G
-
-
-def _set_features(G, features):
-    index = 0
-    for node in G.nodes:
-        G.add_node(node, feat=features[index])
-        index += 1
-    return G
-
-
-def nonzero_1d(input):
-    x = th.nonzero(input, as_tuple=False).squeeze()
-    return x if x.dim() == 1 else x.view(-1)
-
-
-def tensor(data, dtype=None):
-    if isinstance(data, numbers.Number):
-        data = [data]
-    if isinstance(data, list) and len(data) > 0 and isinstance(data[0], th.Tensor):
-        # prevent GPU->CPU->GPU copies
-        if data[0].ndim == 0:
-            # zero dimension scalar tensors
-            return th.stack(data)
-    if isinstance(data, th.Tensor):
-        return th.as_tensor(data, dtype=dtype, device=data.device)
-    else:
-        return th.as_tensor(data, dtype=dtype)
-
-
-def data_type_dict():
-    return {
-        "float16": th.float16,
-        "float32": th.float32,
-        "float64": th.float64,
-        "uint8": th.uint8,
-        "int8": th.int8,
-        "int16": th.int16,
-        "int32": th.int32,
-        "int64": th.int64,
-        "bool": th.bool,
-    }
-
-
-def download(
-    url,
-    path=None,
-    overwrite=True,
-    sha1_hash=None,
-    retries=5,
-    verify_ssl=True,
-    log=True,
-):
-    """Download a given URL.
-
-    Codes borrowed from mxnet/gluon/utils.py
-
-    Parameters
-    ----------
-    url : str
-        URL to download.
-    path : str, optional
-        Destination path to store downloaded file. By default stores to the
-        current directory with the same name as in url.
-    overwrite : bool, optional
-        Whether to overwrite the destination file if it already exists.
-        By default always overwrites the downloaded file.
-    sha1_hash : str, optional
-        Expected sha1 hash in hexadecimal digits. Will ignore existing file when hash is specified
-        but doesn't match.
-    retries : integer, default 5
-        The number of times to attempt downloading in case of failure or non 200 return codes.
-    verify_ssl : bool, default True
-        Verify SSL certificates.
-    log : bool, default True
-        Whether to print the progress for download
-
-    Returns
-    -------
-    str
-        The file path of the downloaded file.
-    """
-    if path is None:
-        fname = url.split("/")[-1]
-        # Empty filenames are invalid
-        assert fname, (
-            "Can't construct file-name from this URL. "
-            "Please set the `path` option manually."
-        )
-    else:
-        path = os.path.expanduser(path)
-        if os.path.isdir(path):
-            fname = os.path.join(path, url.split("/")[-1])
-        else:
-            fname = path
-    assert retries >= 0, "Number of retries should be at least 0"
-
-    if not verify_ssl:
-        warnings.warn(
-            "Unverified HTTPS request is being made (verify_ssl=False). "
-            "Adding certificate verification is strongly advised."
-        )
-
-    if (
-        overwrite
-        or not os.path.exists(fname)
-        or (sha1_hash and not check_sha1(fname, sha1_hash))
-    ):
-        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))
-        if not os.path.exists(dirname):
-            os.makedirs(dirname)
-        while retries + 1 > 0:
-            # Disable pyling too broad Exception
-            # pylint: disable=W0703
-            try:
-                if log:
-                    print("Downloading %s from %s..." % (fname, url))
-                r = requests.get(url, stream=True, verify=verify_ssl)
-                if r.status_code != 200:
-                    raise RuntimeError("Failed downloading url %s" % url)
-                with open(fname, "wb") as f:
-                    for chunk in r.iter_content(chunk_size=1024):
-                        if chunk:  # filter out keep-alive new chunks
-                            f.write(chunk)
-                if sha1_hash and not check_sha1(fname, sha1_hash):
-                    raise UserWarning(
-                        "File {} is downloaded but the content hash does not match."
-                        " The repo may be outdated or download may be incomplete. "
-                        'If the "repo_url" is overridden, consider switching to '
-                        "the default repo.".format(fname)
-                    )
-                break
-            except Exception as e:
-                retries -= 1
-                if retries <= 0:
-                    raise e
-                else:
-                    if log:
-                        print(
-                            "download failed, retrying, {} attempt{} left".format(
-                                retries, "s" if retries > 1 else ""
-                            )
-                        )
-
-    return fname
-
-
-def extract_archive(file, target_dir, overwrite=False):
-    """Extract archive file.
-
-    Parameters
-    ----------
-    file : str
-        Absolute path of the archive file.
-    target_dir : str
-        Target directory of the archive to be uncompressed.
-    overwrite : bool, default True
-        Whether to overwrite the contents inside the directory.
-        By default always overwrites.
-    """
-    if os.path.exists(target_dir) and not overwrite:
-        return
-    print("Extracting file to {}".format(target_dir))
-    if file.endswith(".tar.gz") or file.endswith(".tar") or file.endswith(".tgz"):
-        import tarfile
-
-        with tarfile.open(file, "r") as archive:
-            archive.extractall(path=target_dir)
-    elif file.endswith(".gz"):
-        import gzip
-        import shutil
-
-        with gzip.open(file, "rb") as f_in:
-            target_file = os.path.join(target_dir, os.path.basename(file)[:-3])
-            with open(target_file, "wb") as f_out:
-                shutil.copyfileobj(f_in, f_out)
-    elif file.endswith(".zip"):
-        import zipfile
-
-        with zipfile.ZipFile(file, "r") as archive:
-            archive.extractall(path=target_dir)
-    else:
-        raise Exception("Unrecognized file type: " + file)
-
-
-def get_download_dir():
-    """Get the absolute path to the download directory.
-
-    Returns
-    -------
-    dirname : str
-        Path to the download directory
-    """
-    default_dir = os.path.join(os.path.expanduser("~"), ".EasyGraphData")
-    dirname = os.environ.get("EG_DOWNLOAD_DIR", default_dir)
-    if not os.path.exists(dirname):
-        os.makedirs(dirname)
-    return dirname
-
-
-def makedirs(path):
-    try:
-        os.makedirs(os.path.expanduser(os.path.normpath(path)))
-    except OSError as e:
-        if e.errno != errno.EEXIST and os.path.isdir(path):
-            raise e
-
-
-def check_sha1(filename, sha1_hash):
-    """Check whether the sha1 hash of the file content matches the expected hash.
-
-    Codes borrowed from mxnet/gluon/utils.py
-
-    Parameters
-    ----------
-    filename : str
-        Path to the file.
-    sha1_hash : str
-        Expected sha1 hash in hexadecimal digits.
-
-    Returns
-    -------
-    bool
-        Whether the file content matches the expected hash.
-    """
-    sha1 = hashlib.sha1()
-    with open(filename, "rb") as f:
-        while True:
-            data = f.read(1048576)
-            if not data:
-                break
-            sha1.update(data)
-
-    return sha1.hexdigest() == sha1_hash
-
-
-def generate_mask_tensor(mask):
-    """Generate mask tensor according to different backend
-    For torch, it will create a bool tensor
-    Parameters
-    ----------
-    mask: numpy ndarray
-        input mask tensor
-    """
-    assert isinstance(
-        mask, np.ndarray
-    ), "input for generate_mask_tensor should be an numpy ndarray"
-    return tensor(mask, dtype=data_type_dict()["bool"])
-
-
-def deprecate_property(old, new):
-    warnings.warn(
-        "Property {} will be deprecated, please use {} instead.".format(old, new)
-    )
-
-
-def check_file(file_path: Path, md5: str):
-    r"""Check if a file is valid.
-
-    Args:
-        ``file_path`` (``Path``): The local path of the file.
-        ``md5`` (``str``): The md5 of the file.
-
-    Raises:
-        FileNotFoundError: Not found the file.
-    """
-    if not file_path.exists():
-        raise FileNotFoundError(f"{file_path} does not exist.")
-    else:
-        with open(file_path, "rb") as f:
-            data = f.read()
-        cur_md5 = hashlib.md5(data).hexdigest()
-        return cur_md5 == md5
-
-
-def download_file(url: str, file_path: Path):
-    r"""Download a file from a url.
-
-    Args:
-        ``url`` (``str``): the url of the file
-        ``file_path`` (``str``): the path to the file
-    """
-    file_path.parent.mkdir(parents=True, exist_ok=True)
-    r = requests.get(url, stream=True, verify=True)
-    if r.status_code != 200:
-        raise requests.HTTPError(f"{url} is not accessible.")
-    with open(file_path, "wb") as f:
-        for chunk in r.iter_content(chunk_size=1024):
-            if chunk:
-                f.write(chunk)
-
-
-@_retry(3)
-def download_and_check(url: str, file_path: Path, md5: str):
-    r"""Download a file from a url and check its integrity.
-
-    Args:
-        ``url`` (``str``): The url of the file.
-        ``file_path`` (``Path``): The path to the file.
-        ``md5`` (``str``): The md5 of the file.
-    """
-    if not file_path.exists():
-        download_file(url, file_path)
-    if not check_file(file_path, md5):
-        file_path.unlink()
-        raise ValueError(
-            f"{file_path} is corrupted. We will delete it, and try to download it"
-            " again."
-        )
-    return True
+import errno
+import hashlib
+import numbers
+import os
+
+from pathlib import Path
+
+import numpy as np
+import requests
+import torch as th
+
+
+__all__ = [
+    "download",
+    "extract_archive",
+    "get_download_dir",
+    "makedirs",
+    "generate_mask_tensor",
+]
+
+import warnings
+
+from easygraph.utils.download import _retry
+
+
+def _get_eg_url(file_url):
+    """Get EasyGraph online url for download."""
+    eg_repo_url = "https://gitlab.com/easy-graph/"
+    repo_url = eg_repo_url
+    if repo_url[-1] != "/":
+        repo_url = repo_url + "/"
+    return repo_url + file_url
+
+
+def _get_dgl_url(file_url):
+    """Get DGL online url for download."""
+    dgl_repo_url = "https://data.dgl.ai/"
+    repo_url = os.environ.get("DGL_REPO", dgl_repo_url)
+    if repo_url[-1] != "/":
+        repo_url = repo_url + "/"
+    return repo_url + file_url
+
+
+def _set_labels(G, labels):
+    index = 0
+    for node in G.nodes:
+        G.add_node(node, label=labels[index])
+        index += 1
+    return G
+
+
+def _set_features(G, features):
+    index = 0
+    for node in G.nodes:
+        G.add_node(node, feat=features[index])
+        index += 1
+    return G
+
+
+def nonzero_1d(input):
+    x = th.nonzero(input, as_tuple=False).squeeze()
+    return x if x.dim() == 1 else x.view(-1)
+
+
+def tensor(data, dtype=None):
+    if isinstance(data, numbers.Number):
+        data = [data]
+    if isinstance(data, list) and len(data) > 0 and isinstance(data[0], th.Tensor):
+        # prevent GPU->CPU->GPU copies
+        if data[0].ndim == 0:
+            # zero dimension scalar tensors
+            return th.stack(data)
+    if isinstance(data, th.Tensor):
+        return th.as_tensor(data, dtype=dtype, device=data.device)
+    else:
+        return th.as_tensor(data, dtype=dtype)
+
+
+def data_type_dict():
+    return {
+        "float16": th.float16,
+        "float32": th.float32,
+        "float64": th.float64,
+        "uint8": th.uint8,
+        "int8": th.int8,
+        "int16": th.int16,
+        "int32": th.int32,
+        "int64": th.int64,
+        "bool": th.bool,
+    }
+
+
+def download(
+    url,
+    path=None,
+    overwrite=True,
+    sha1_hash=None,
+    retries=5,
+    verify_ssl=True,
+    log=True,
+):
+    """Download a given URL.
+
+    Codes borrowed from mxnet/gluon/utils.py
+
+    Parameters
+    ----------
+    url : str
+        URL to download.
+    path : str, optional
+        Destination path to store downloaded file. By default stores to the
+        current directory with the same name as in url.
+    overwrite : bool, optional
+        Whether to overwrite the destination file if it already exists.
+        By default always overwrites the downloaded file.
+    sha1_hash : str, optional
+        Expected sha1 hash in hexadecimal digits. Will ignore existing file when hash is specified
+        but doesn't match.
+    retries : integer, default 5
+        The number of times to attempt downloading in case of failure or non 200 return codes.
+    verify_ssl : bool, default True
+        Verify SSL certificates.
+    log : bool, default True
+        Whether to print the progress for download
+
+    Returns
+    -------
+    str
+        The file path of the downloaded file.
+    """
+    if path is None:
+        fname = url.split("/")[-1]
+        # Empty filenames are invalid
+        assert fname, (
+            "Can't construct file-name from this URL. "
+            "Please set the `path` option manually."
+        )
+    else:
+        path = os.path.expanduser(path)
+        if os.path.isdir(path):
+            fname = os.path.join(path, url.split("/")[-1])
+        else:
+            fname = path
+    assert retries >= 0, "Number of retries should be at least 0"
+
+    if not verify_ssl:
+        warnings.warn(
+            "Unverified HTTPS request is being made (verify_ssl=False). "
+            "Adding certificate verification is strongly advised."
+        )
+
+    if (
+        overwrite
+        or not os.path.exists(fname)
+        or (sha1_hash and not check_sha1(fname, sha1_hash))
+    ):
+        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))
+        if not os.path.exists(dirname):
+            os.makedirs(dirname)
+        while retries + 1 > 0:
+            # Disable pyling too broad Exception
+            # pylint: disable=W0703
+            try:
+                if log:
+                    print("Downloading %s from %s..." % (fname, url))
+                r = requests.get(url, stream=True, verify=verify_ssl)
+                if r.status_code != 200:
+                    raise RuntimeError("Failed downloading url %s" % url)
+                with open(fname, "wb") as f:
+                    for chunk in r.iter_content(chunk_size=1024):
+                        if chunk:  # filter out keep-alive new chunks
+                            f.write(chunk)
+                if sha1_hash and not check_sha1(fname, sha1_hash):
+                    raise UserWarning(
+                        "File {} is downloaded but the content hash does not match."
+                        " The repo may be outdated or download may be incomplete. "
+                        'If the "repo_url" is overridden, consider switching to '
+                        "the default repo.".format(fname)
+                    )
+                break
+            except Exception as e:
+                retries -= 1
+                if retries <= 0:
+                    raise e
+                else:
+                    if log:
+                        print(
+                            "download failed, retrying, {} attempt{} left".format(
+                                retries, "s" if retries > 1 else ""
+                            )
+                        )
+
+    return fname
+
+
+def extract_archive(file, target_dir, overwrite=False):
+    """Extract archive file.
+
+    Parameters
+    ----------
+    file : str
+        Absolute path of the archive file.
+    target_dir : str
+        Target directory of the archive to be uncompressed.
+    overwrite : bool, default True
+        Whether to overwrite the contents inside the directory.
+        By default always overwrites.
+    """
+    if os.path.exists(target_dir) and not overwrite:
+        return
+    print("Extracting file to {}".format(target_dir))
+    if file.endswith(".tar.gz") or file.endswith(".tar") or file.endswith(".tgz"):
+        import tarfile
+
+        with tarfile.open(file, "r") as archive:
+            archive.extractall(path=target_dir)
+    elif file.endswith(".gz"):
+        import gzip
+        import shutil
+
+        with gzip.open(file, "rb") as f_in:
+            target_file = os.path.join(target_dir, os.path.basename(file)[:-3])
+            with open(target_file, "wb") as f_out:
+                shutil.copyfileobj(f_in, f_out)
+    elif file.endswith(".zip"):
+        import zipfile
+
+        with zipfile.ZipFile(file, "r") as archive:
+            archive.extractall(path=target_dir)
+    else:
+        raise Exception("Unrecognized file type: " + file)
+
+
+def get_download_dir():
+    """Get the absolute path to the download directory.
+
+    Returns
+    -------
+    dirname : str
+        Path to the download directory
+    """
+    default_dir = os.path.join(os.path.expanduser("~"), ".EasyGraphData")
+    dirname = os.environ.get("EG_DOWNLOAD_DIR", default_dir)
+    if not os.path.exists(dirname):
+        os.makedirs(dirname)
+    return dirname
+
+
+def makedirs(path):
+    try:
+        os.makedirs(os.path.expanduser(os.path.normpath(path)))
+    except OSError as e:
+        if e.errno != errno.EEXIST and os.path.isdir(path):
+            raise e
+
+
+def check_sha1(filename, sha1_hash):
+    """Check whether the sha1 hash of the file content matches the expected hash.
+
+    Codes borrowed from mxnet/gluon/utils.py
+
+    Parameters
+    ----------
+    filename : str
+        Path to the file.
+    sha1_hash : str
+        Expected sha1 hash in hexadecimal digits.
+
+    Returns
+    -------
+    bool
+        Whether the file content matches the expected hash.
+    """
+    sha1 = hashlib.sha1()
+    with open(filename, "rb") as f:
+        while True:
+            data = f.read(1048576)
+            if not data:
+                break
+            sha1.update(data)
+
+    return sha1.hexdigest() == sha1_hash
+
+
+def generate_mask_tensor(mask):
+    """Generate mask tensor according to different backend
+    For torch, it will create a bool tensor
+    Parameters
+    ----------
+    mask: numpy ndarray
+        input mask tensor
+    """
+    assert isinstance(
+        mask, np.ndarray
+    ), "input for generate_mask_tensor should be an numpy ndarray"
+    return tensor(mask, dtype=data_type_dict()["bool"])
+
+
+def deprecate_property(old, new):
+    warnings.warn(
+        "Property {} will be deprecated, please use {} instead.".format(old, new)
+    )
+
+
+def check_file(file_path: Path, md5: str):
+    r"""Check if a file is valid.
+
+    Args:
+        ``file_path`` (``Path``): The local path of the file.
+        ``md5`` (``str``): The md5 of the file.
+
+    Raises:
+        FileNotFoundError: Not found the file.
+    """
+    if not file_path.exists():
+        raise FileNotFoundError(f"{file_path} does not exist.")
+    else:
+        with open(file_path, "rb") as f:
+            data = f.read()
+        cur_md5 = hashlib.md5(data).hexdigest()
+        return cur_md5 == md5
+
+
+def download_file(url: str, file_path: Path):
+    r"""Download a file from a url.
+
+    Args:
+        ``url`` (``str``): the url of the file
+        ``file_path`` (``str``): the path to the file
+    """
+    file_path.parent.mkdir(parents=True, exist_ok=True)
+    r = requests.get(url, stream=True, verify=True)
+    if r.status_code != 200:
+        raise requests.HTTPError(f"{url} is not accessible.")
+    with open(file_path, "wb") as f:
+        for chunk in r.iter_content(chunk_size=1024):
+            if chunk:
+                f.write(chunk)
+
+
+@_retry(3)
+def download_and_check(url: str, file_path: Path, md5: str):
+    r"""Download a file from a url and check its integrity.
+
+    Args:
+        ``url`` (``str``): The url of the file.
+        ``file_path`` (``Path``): The path to the file.
+        ``md5`` (``str``): The md5 of the file.
+    """
+    if not file_path.exists():
+        download_file(url, file_path)
+    if not check_file(file_path, md5):
+        file_path.unlink()
+        raise ValueError(
+            f"{file_path} is corrupted. We will delete it, and try to download it"
+            " again."
+        )
+    return True
```

## easygraph/datasets/citation_graph.py

 * *Ordering differences only*

```diff
@@ -1,875 +1,875 @@
-"""Cora, citeseer, pubmed dataset.
-
-"""
-from __future__ import absolute_import
-
-import os
-import pickle as pkl
-import sys
-
-import easygraph as eg
-import numpy as np
-import scipy.sparse as sp
-
-from easygraph.classes.graph import Graph
-
-from .graph_dataset_base import EasyGraphBuiltinDataset
-from .utils import _get_dgl_url
-from .utils import data_type_dict
-from .utils import deprecate_property
-from .utils import generate_mask_tensor
-from .utils import nonzero_1d
-from .utils import tensor
-
-
-def _pickle_load(pkl_file):
-    if sys.version_info > (3, 0):
-        return pkl.load(pkl_file, encoding="latin1")
-    else:
-        return pkl.load(pkl_file)
-
-
-class CitationGraphDataset(EasyGraphBuiltinDataset):
-    r"""The citation graph dataset, including Cora, CiteSeer and PubMed.
-    Nodes mean authors and edges mean citation relationships.
-
-    Parameters
-    -----------
-    name: str
-      name can be 'Cora', 'CiteSeer' or 'PubMed'.
-    raw_dir : str
-        Raw file directory to download/contains the input data directory.
-        Default: ~/.dgl/
-    force_reload : bool
-        Whether to reload the dataset. Default: False
-    verbose : bool
-        Whether to print out progress information. Default: True.
-    reverse_edge : bool
-        Whether to add reverse edges in graph. Default: True.
-    transform : callable, optional
-        A transform that takes in a :class:`~eg.Graph` object and returns
-        a transformed version. The :class:`~eg.Graph` object will be
-        transformed before every access.
-    reorder : bool
-        Whether to reorder the graph using :func:`~eg.reorder_graph`. Default: False.
-    """
-    _urls = {
-        "cora_v2": "dataset/cora_v2.zip",
-        "citeseer": "dataset/citeSeer.zip",
-        "pubmed": "dataset/pubmed.zip",
-    }
-
-    def __init__(
-        self,
-        name,
-        raw_dir=None,
-        force_reload=False,
-        verbose=True,
-        reverse_edge=True,
-        transform=None,
-        reorder=False,
-    ):
-        assert name.lower() in ["cora", "citeseer", "pubmed"]
-
-        # Previously we use the pre-processing in pygcn (https://github.com/tkipf/pygcn)
-        # for Cora, which is slightly different from the one used in the GCN paper
-        if name.lower() == "cora":
-            name = "cora_v2"
-
-        url = _get_dgl_url(self._urls[name])
-        self._reverse_edge = reverse_edge
-        self._reorder = reorder
-
-        super(CitationGraphDataset, self).__init__(
-            name,
-            url=url,
-            raw_dir=raw_dir,
-            force_reload=force_reload,
-            verbose=verbose,
-            transform=transform,
-        )
-
-    def process(self):
-        """Loads input data from data directory and reorder graph for better locality
-
-        ind.name.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;
-        ind.name.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;
-        ind.name.allx => the feature vectors of both labeled and unlabeled training instances
-            (a superset of ind.name.x) as scipy.sparse.csr.csr_matrix object;
-        ind.name.y => the one-hot labels of the labeled training instances as numpy.ndarray object;
-        ind.name.ty => the one-hot labels of the test instances as numpy.ndarray object;
-        ind.name.ally => the labels for instances in ind.name.allx as numpy.ndarray object;
-        ind.name.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict
-            object;
-        ind.name.test.index => the indices of test instances in graph, for the inductive setting as list object.
-        """
-        root = self.raw_path
-        objnames = ["x", "y", "tx", "ty", "allx", "ally", "graph"]
-        objects = []
-        for i in range(len(objnames)):
-            with open("{}/ind.{}.{}".format(root, self.name, objnames[i]), "rb") as f:
-                objects.append(_pickle_load(f))
-
-        x, y, tx, ty, allx, ally, graph = tuple(objects)
-        test_idx_reorder = _parse_index_file(
-            "{}/ind.{}.test.index".format(root, self.name)
-        )
-        test_idx_range = np.sort(test_idx_reorder)
-
-        if self.name == "citeseer":
-            # Fix CiteSeer dataset (there are some isolated nodes in the graph)
-            # Find isolated nodes, add them as zero-vecs into the right position
-            test_idx_range_full = range(
-                min(test_idx_reorder), max(test_idx_reorder) + 1
-            )
-            tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))
-            tx_extended[test_idx_range - min(test_idx_range), :] = tx
-            tx = tx_extended
-            ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))
-            ty_extended[test_idx_range - min(test_idx_range), :] = ty
-            ty = ty_extended
-
-        features = sp.vstack((allx, tx)).tolil()
-        features[test_idx_reorder, :] = features[test_idx_range, :]
-
-        if self.reverse_edge:
-            g = eg.DiGraph(eg.from_dict_of_lists(graph))
-            # g = from_networkx(graph)
-        else:
-            graph = eg.Graph(eg.from_dict_of_lists(graph))
-            # edges = list(graph.edges())
-            # u, v = map(list, zip(*edges))
-            # g = dgl_graph((u, v))
-
-        onehot_labels = np.vstack((ally, ty))
-        onehot_labels[test_idx_reorder, :] = onehot_labels[test_idx_range, :]
-        labels = np.argmax(onehot_labels, 1)
-
-        idx_test = test_idx_range.tolist()
-        idx_train = range(len(y))
-        idx_val = range(len(y), len(y) + 500)
-
-        train_mask = generate_mask_tensor(_sample_mask(idx_train, labels.shape[0]))
-        val_mask = generate_mask_tensor(_sample_mask(idx_val, labels.shape[0]))
-        test_mask = generate_mask_tensor(_sample_mask(idx_test, labels.shape[0]))
-
-        g.ndata["train_mask"] = train_mask
-        g.ndata["val_mask"] = val_mask
-        g.ndata["test_mask"] = test_mask
-        g.ndata["label"] = tensor(labels)
-        g.ndata["feat"] = tensor(
-            _preprocess_features(features), dtype=data_type_dict()["float32"]
-        )
-        self._num_classes = onehot_labels.shape[1]
-        self._labels = labels
-        # if self._reorder:
-        #     self._g = reorder_graph(
-        #         g, node_permute_algo='rcmk', edge_permute_algo='dst', store_ids=False)
-        # else:
-        self._g = g
-
-        if self.verbose:
-            print("Finished data loading and preprocessing.")
-            print("  NumNodes: {}".format(self._g.number_of_nodes()))
-            print("  NumEdges: {}".format(self._g.number_of_edges()))
-            print("  NumFeats: {}".format(self._g.ndata["feat"].shape[1]))
-            print("  NumClasses: {}".format(self.num_classes))
-            print(
-                "  NumTrainingSamples: {}".format(
-                    nonzero_1d(self._g.ndata["train_mask"]).shape[0]
-                )
-            )
-            print(
-                "  NumValidationSamples: {}".format(
-                    nonzero_1d(self._g.ndata["val_mask"]).shape[0]
-                )
-            )
-            print(
-                "  NumTestSamples: {}".format(
-                    nonzero_1d(self._g.ndata["test_mask"]).shape[0]
-                )
-            )
-
-    def has_cache(self):
-        graph_path = os.path.join(self.save_path, self.save_name + ".bin")
-        info_path = os.path.join(self.save_path, self.save_name + ".pkl")
-        if os.path.exists(graph_path) and os.path.exists(info_path):
-            return True
-
-        return False
-
-    # def save(self):
-    #     """save the graph list and the labels"""
-    #     graph_path = os.path.join(self.save_path,
-    #                               self.save_name + '.bin')
-    #     info_path = os.path.join(self.save_path,
-    #                              self.save_name + '.pkl')
-    #     save_graphs(str(graph_path), self._g)
-    #     save_info(str(info_path), {'num_classes': self.num_classes})
-    #
-    # def load(self):
-    #     graph_path = os.path.join(self.save_path,
-    #                               self.save_name + '.bin')
-    #     info_path = os.path.join(self.save_path,
-    #                              self.save_name + '.pkl')
-    #     graphs, _ = load_graphs(str(graph_path))
-    #
-    #     info = load_info(str(info_path))
-    #     graph = graphs[0]
-    #     self._g = graph
-    #     # for compatibility
-    #     graph = graph.clone()
-    #     graph.ndata.pop('train_mask')
-    #     graph.ndata.pop('val_mask')
-    #     graph.ndata.pop('test_mask')
-    #     graph.ndata.pop('feat')
-    #     graph.ndata.pop('label')
-    #     graph = to_networkx(graph)
-    #
-    #     self._num_classes = info['num_classes']
-    #     self._g.ndata['train_mask'] = generate_mask_tensor(F.asnumpy(self._g.ndata['train_mask']))
-    #     self._g.ndata['val_mask'] = generate_mask_tensor(F.asnumpy(self._g.ndata['val_mask']))
-    #     self._g.ndata['test_mask'] = generate_mask_tensor(F.asnumpy(self._g.ndata['test_mask']))
-    #     # hack for mxnet compatibility
-    #
-    #     if self.verbose:
-    #         print('  NumNodes: {}'.format(self._g.number_of_nodes()))
-    #         print('  NumEdges: {}'.format(self._g.number_of_edges()))
-    #         print('  NumFeats: {}'.format(self._g.ndata['feat'].shape[1]))
-    #         print('  NumClasses: {}'.format(self.num_classes))
-    #         print('  NumTrainingSamples: {}'.format(
-    #             F.nonzero_1d(self._g.ndata['train_mask']).shape[0]))
-    #         print('  NumValidationSamples: {}'.format(
-    #             F.nonzero_1d(self._g.ndata['val_mask']).shape[0]))
-    #         print('  NumTestSamples: {}'.format(
-    #             F.nonzero_1d(self._g.ndata['test_mask']).shape[0]))
-
-    def __getitem__(self, idx):
-        assert idx == 0, "This dataset has only one graph"
-        if self._transform is None:
-            return self._g
-        else:
-            return self._transform(self._g)
-
-    def __len__(self):
-        return 1
-
-    @property
-    def save_name(self):
-        return self.name + "_dgl_graph"
-
-    @property
-    def num_labels(self):
-        deprecate_property("dataset.num_labels", "dataset.num_classes")
-        return self.num_classes
-
-    @property
-    def num_classes(self):
-        return self._num_classes
-
-    """ Citation graph is used in many examples
-        We preserve these properties for compatibility.
-    """
-
-    @property
-    def reverse_edge(self):
-        return self._reverse_edge
-
-
-def _preprocess_features(features):
-    """Row-normalize feature matrix and convert to tuple representation"""
-    rowsum = np.asarray(features.sum(1))
-    r_inv = np.power(rowsum, -1).flatten()
-    r_inv[np.isinf(r_inv)] = 0.0
-    r_mat_inv = sp.diags(r_inv)
-    features = r_mat_inv.dot(features)
-    return np.asarray(features.todense())
-
-
-def _parse_index_file(filename):
-    """Parse index file."""
-    index = []
-    for line in open(filename):
-        index.append(int(line.strip()))
-    return index
-
-
-def _sample_mask(idx, l):
-    """Create mask."""
-    mask = np.zeros(l)
-    mask[idx] = 1
-    return mask
-
-
-class CoraGraphDataset(CitationGraphDataset):
-    r"""Cora citation network dataset.
-
-    Nodes mean paper and edges mean citation
-    relationships. Each node has a predefined
-    feature with 1433 dimensions. The dataset is
-    designed for the node classification task.
-    The task is to predict the category of
-    certain paper.
-
-    Statistics:
-
-    - Nodes: 2708
-    - Edges: 10556
-    - Number of Classes: 7
-    - Label split:
-
-        - Train: 140
-        - Valid: 500
-        - Test: 1000
-
-    Parameters
-    ----------
-    raw_dir : str
-        Raw file directory to download/contains the input data directory.
-        Default: ~/.dgl/
-    force_reload : bool
-        Whether to reload the dataset. Default: False
-    verbose : bool
-        Whether to print out progress information. Default: True.
-    reverse_edge : bool
-        Whether to add reverse edges in graph. Default: True.
-    transform : callable, optional
-        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
-        a transformed version. The :class:`~dgl.DGLGraph` object will be
-        transformed before every access.
-    reorder : bool
-        Whether to reorder the graph using :func:`~dgl.reorder_graph`. Default: False.
-
-    Attributes
-    ----------
-    num_classes: int
-        Number of label classes
-
-    Notes
-    -----
-    The node feature is row-normalized.
-
-    Examples
-    --------
-    >>> dataset = CoraGraphDataset()
-    >>> g = dataset[0]
-    >>> num_class = dataset.num_classes
-    >>>
-    >>> # get node feature
-    >>> feat = g.ndata['feat']
-    >>>
-    >>> # get data split
-    >>> train_mask = g.ndata['train_mask']
-    >>> val_mask = g.ndata['val_mask']
-    >>> test_mask = g.ndata['test_mask']
-    >>>
-    >>> # get labels
-    >>> label = g.ndata['label']
-
-    """
-
-    def __init__(
-        self,
-        raw_dir=None,
-        force_reload=False,
-        verbose=True,
-        reverse_edge=True,
-        transform=None,
-        reorder=False,
-    ):
-        name = "cora"
-
-        super(CoraGraphDataset, self).__init__(
-            name, raw_dir, force_reload, verbose, reverse_edge, transform, reorder
-        )
-
-    def __getitem__(self, idx):
-        r"""Gets the graph object
-
-        Parameters
-        -----------
-        idx: int
-            Item index, CoraGraphDataset has only one graph object
-
-        Return
-        ------
-        :class:`dgl.DGLGraph`
-
-            graph structure, node features and labels.
-
-            - ``ndata['train_mask']``: mask for training node set
-            - ``ndata['val_mask']``: mask for validation node set
-            - ``ndata['test_mask']``: mask for test node set
-            - ``ndata['feat']``: node feature
-            - ``ndata['label']``: ground truth labels
-        """
-        return super(CoraGraphDataset, self).__getitem__(idx)
-
-    def __len__(self):
-        r"""The number of graphs in the dataset."""
-        return super(CoraGraphDataset, self).__len__()
-
-
-class CiteseerGraphDataset(CitationGraphDataset):
-    r"""Citeseer citation network dataset.
-
-    Nodes mean scientific publications and edges
-    mean citation relationships. Each node has a
-    predefined feature with 3703 dimensions. The
-    dataset is designed for the node classification
-    task. The task is to predict the category of
-    certain publication.
-
-    Statistics:
-
-    - Nodes: 3327
-    - Edges: 9228
-    - Number of Classes: 6
-    - Label Split:
-
-        - Train: 120
-        - Valid: 500
-        - Test: 1000
-
-    Parameters
-    -----------
-    raw_dir : str
-        Raw file directory to download/contains the input data directory.
-        Default: ~/.dgl/
-    force_reload : bool
-        Whether to reload the dataset. Default: False
-    verbose : bool
-        Whether to print out progress information. Default: True.
-    reverse_edge : bool
-        Whether to add reverse edges in graph. Default: True.
-    transform : callable, optional
-        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
-        a transformed version. The :class:`~dgl.DGLGraph` object will be
-        transformed before every access.
-    reorder : bool
-        Whether to reorder the graph using :func:`~dgl.reorder_graph`. Default: False.
-
-    Attributes
-    ----------
-    num_classes: int
-        Number of label classes
-
-    Notes
-    -----
-    The node feature is row-normalized.
-
-    In citeseer dataset, there are some isolated nodes in the graph.
-    These isolated nodes are added as zero-vecs into the right position.
-
-    Examples
-    --------
-    >>> dataset = CiteseerGraphDataset()
-    >>> g = dataset[0]
-    >>> num_class = dataset.num_classes
-    >>>
-    >>> # get node feature
-    >>> feat = g.ndata['feat']
-    >>>
-    >>> # get data split
-    >>> train_mask = g.ndata['train_mask']
-    >>> val_mask = g.ndata['val_mask']
-    >>> test_mask = g.ndata['test_mask']
-    >>>
-    >>> # get labels
-    >>> label = g.ndata['label']
-
-    """
-
-    def __init__(
-        self,
-        raw_dir=None,
-        force_reload=False,
-        verbose=True,
-        reverse_edge=True,
-        transform=None,
-        reorder=False,
-    ):
-        name = "citeseer"
-
-        super(CiteseerGraphDataset, self).__init__(
-            name, raw_dir, force_reload, verbose, reverse_edge, transform, reorder
-        )
-
-    def __getitem__(self, idx):
-        r"""Gets the graph object
-
-        Parameters
-        -----------
-        idx: int
-            Item index, CiteseerGraphDataset has only one graph object
-
-        Return
-        ------
-        :class:`dgl.DGLGraph`
-
-            graph structure, node features and labels.
-
-            - ``ndata['train_mask']``: mask for training node set
-            - ``ndata['val_mask']``: mask for validation node set
-            - ``ndata['test_mask']``: mask for test node set
-            - ``ndata['feat']``: node feature
-            - ``ndata['label']``: ground truth labels
-        """
-        return super(CiteseerGraphDataset, self).__getitem__(idx)
-
-    def __len__(self):
-        r"""The number of graphs in the dataset."""
-        return super(CiteseerGraphDataset, self).__len__()
-
-
-class PubmedGraphDataset(CitationGraphDataset):
-    r"""Pubmed citation network dataset.
-
-    Nodes mean scientific publications and edges
-    mean citation relationships. Each node has a
-    predefined feature with 500 dimensions. The
-    dataset is designed for the node classification
-    task. The task is to predict the category of
-    certain publication.
-
-    Statistics:
-
-    - Nodes: 19717
-    - Edges: 88651
-    - Number of Classes: 3
-    - Label Split:
-
-        - Train: 60
-        - Valid: 500
-        - Test: 1000
-
-    Parameters
-    -----------
-    raw_dir : str
-        Raw file directory to download/contains the input data directory.
-        Default: ~/.dgl/
-    force_reload : bool
-        Whether to reload the dataset. Default: False
-    verbose : bool
-        Whether to print out progress information. Default: True.
-    reverse_edge : bool
-        Whether to add reverse edges in graph. Default: True.
-    transform : callable, optional
-        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
-        a transformed version. The :class:`~dgl.DGLGraph` object will be
-        transformed before every access.
-    reorder : bool
-        Whether to reorder the graph using :func:`~dgl.reorder_graph`. Default: False.
-
-    Attributes
-    ----------
-    num_classes: int
-        Number of label classes
-
-    Notes
-    -----
-    The node feature is row-normalized.
-
-    Examples
-    --------
-    >>> dataset = PubmedGraphDataset()
-    >>> g = dataset[0]
-    >>> num_class = dataset.num_of_class
-    >>>
-    >>> # get node feature
-    >>> feat = g.ndata['feat']
-    >>>
-    >>> # get data split
-    >>> train_mask = g.ndata['train_mask']
-    >>> val_mask = g.ndata['val_mask']
-    >>> test_mask = g.ndata['test_mask']
-    >>>
-    >>> # get labels
-    >>> label = g.ndata['label']
-
-    """
-
-    def __init__(
-        self,
-        raw_dir=None,
-        force_reload=False,
-        verbose=True,
-        reverse_edge=True,
-        transform=None,
-        reorder=False,
-    ):
-        name = "pubmed"
-
-        super(PubmedGraphDataset, self).__init__(
-            name, raw_dir, force_reload, verbose, reverse_edge, transform, reorder
-        )
-
-    def __getitem__(self, idx):
-        r"""Gets the graph object
-
-        Parameters
-        -----------
-        idx: int
-            Item index, PubmedGraphDataset has only one graph object
-
-        Return
-        ------
-        :class:`dgl.DGLGraph`
-
-            graph structure, node features and labels.
-
-            - ``ndata['train_mask']``: mask for training node set
-            - ``ndata['val_mask']``: mask for validation node set
-            - ``ndata['test_mask']``: mask for test node set
-            - ``ndata['feat']``: node feature
-            - ``ndata['label']``: ground truth labels
-        """
-        return super(PubmedGraphDataset, self).__getitem__(idx)
-
-    def __len__(self):
-        r"""The number of graphs in the dataset."""
-        return super(PubmedGraphDataset, self).__len__()
-
-
-def load_cora(
-    raw_dir=None, force_reload=False, verbose=True, reverse_edge=True, transform=None
-):
-    """Get CoraGraphDataset
-
-    Parameters
-    -----------
-    raw_dir : str
-        Raw file directory to download/contains the input data directory.
-        Default: ~/.dgl/
-    force_reload : bool
-        Whether to reload the dataset. Default: False
-    verbose : bool
-        Whether to print out progress information. Default: True.
-    reverse_edge : bool
-        Whether to add reverse edges in graph. Default: True.
-    transform : callable, optional
-        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
-        a transformed version. The :class:`~dgl.DGLGraph` object will be
-        transformed before every access.
-
-    Return
-    -------
-    CoraGraphDataset
-    """
-    data = CoraGraphDataset(raw_dir, force_reload, verbose, reverse_edge, transform)
-    return data
-
-
-def load_citeseer(
-    raw_dir=None, force_reload=False, verbose=True, reverse_edge=True, transform=None
-):
-    """Get CiteseerGraphDataset
-
-    Parameters
-    -----------
-    raw_dir : str
-        Raw file directory to download/contains the input data directory.
-        Default: ~/.dgl/
-    force_reload : bool
-        Whether to reload the dataset. Default: False
-    verbose : bool
-        Whether to print out progress information. Default: True.
-    reverse_edge : bool
-        Whether to add reverse edges in graph. Default: True.
-    transform : callable, optional
-        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
-        a transformed version. The :class:`~dgl.DGLGraph` object will be
-        transformed before every access.
-
-    Return
-    -------
-    CiteseerGraphDataset
-    """
-    data = CiteseerGraphDataset(raw_dir, force_reload, verbose, reverse_edge, transform)
-    return data
-
-
-def load_pubmed(
-    raw_dir=None, force_reload=False, verbose=True, reverse_edge=True, transform=None
-):
-    """Get PubmedGraphDataset
-
-    Parameters
-    -----------
-    raw_dir : str
-        Raw file directory to download/contains the input data directory.
-        Default: ~/.dgl/
-    force_reload : bool
-        Whether to reload the dataset. Default: False
-    verbose : bool
-        Whether to print out progress information. Default: True.
-    reverse_edge : bool
-        Whether to add reverse edges in graph. Default: True.
-    transform : callable, optional
-        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
-        a transformed version. The :class:`~dgl.DGLGraph` object will be
-        transformed before every access.
-
-    Return
-    -------
-    PubmedGraphDataset
-    """
-    data = PubmedGraphDataset(raw_dir, force_reload, verbose, reverse_edge, transform)
-    return data
-
-
-class CoraBinary(EasyGraphBuiltinDataset):
-    """A mini-dataset for binary classification task using Cora.
-
-    After loaded, it has following members:
-
-    graphs : list of :class:`~dgl.DGLGraph`
-    pmpds : list of :class:`scipy.sparse.coo_matrix`
-    labels : list of :class:`numpy.ndarray`
-
-    Parameters
-    -----------
-    raw_dir : str
-        Raw file directory to download/contains the input data directory.
-        Default: ~/.dgl/
-    force_reload : bool
-        Whether to reload the dataset. Default: False
-    verbose: bool
-        Whether to print out progress information. Default: True.
-    transform : callable, optional
-        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
-        a transformed version. The :class:`~dgl.DGLGraph` object will be
-        transformed before every access.
-    """
-
-    def __init__(self, raw_dir=None, force_reload=False, verbose=True, transform=None):
-        name = "cora_binary"
-        url = _get_dgl_url("dataset/cora_binary.zip")
-        super(CoraBinary, self).__init__(
-            name,
-            url=url,
-            raw_dir=raw_dir,
-            force_reload=force_reload,
-            verbose=verbose,
-            transform=transform,
-        )
-
-    def process(self):
-        root = self.raw_path
-        # load graphs
-        self.graphs = []
-        with open("{}/graphs.txt".format(root), "r") as f:
-            elist = []
-            for line in f.readlines():
-                if line.startswith("graph"):
-                    if len(elist) != 0:
-                        self.graphs.append(Graph(elist))
-                    elist = []
-                else:
-                    u, v = line.strip().split(" ")
-                    elist.append((int(u), int(v)))
-            if len(elist) != 0:
-                self.graphs.append(Graph(tuple(zip(*elist))))
-        with open("{}/pmpds.pkl".format(root), "rb") as f:
-            self.pmpds = _pickle_load(f)
-        self.labels = []
-        with open("{}/labels.txt".format(root), "r") as f:
-            cur = []
-            for line in f.readlines():
-                if line.startswith("graph"):
-                    if len(cur) != 0:
-                        self.labels.append(np.asarray(cur))
-                    cur = []
-                else:
-                    cur.append(int(line.strip()))
-            if len(cur) != 0:
-                self.labels.append(np.asarray(cur))
-        # sanity check
-        assert len(self.graphs) == len(self.pmpds)
-        assert len(self.graphs) == len(self.labels)
-
-    def has_cache(self):
-        graph_path = os.path.join(self.save_path, self.save_name + ".bin")
-        if os.path.exists(graph_path):
-            return True
-
-        return False
-
-    # def save(self):
-    #     """save the graph list and the labels"""
-    #     graph_path = os.path.join(self.save_path,
-    #                               self.save_name + '.bin')
-    #     labels = {}
-    #     for i, label in enumerate(self.labels):
-    #         labels['{}'.format(i)] = F.tensor(label)
-    #     save_graphs(str(graph_path), self.graphs, labels)
-    #     if self.verbose:
-    #         print('Done saving data into cached files.')
-    #
-    # def load(self):
-    #     graph_path = os.path.join(self.save_path,
-    #                               self.save_name + '.bin')
-    #     self.graphs, labels = load_graphs(str(graph_path))
-    #
-    #     self.labels = []
-    #     for i in range(len(labels)):
-    #         self.labels.append(F.asnumpy(labels['{}'.format(i)]))
-    #     # load pmpds under self.raw_path
-    #     with open("{}/pmpds.pkl".format(self.raw_path), 'rb') as f:
-    #         self.pmpds = _pickle_load(f)
-    #     if self.verbose:
-    #         print('Done loading data into cached files.')
-    #     # sanity check
-    #     assert len(self.graphs) == len(self.pmpds)
-    #     assert len(self.graphs) == len(self.labels)
-
-    def __len__(self):
-        return len(self.graphs)
-
-    def __getitem__(self, i):
-        r"""Gets the idx-th sample.
-
-        Parameters
-        -----------
-        idx : int
-            The sample index.
-
-        Returns
-        -------
-        (dgl.DGLGraph, scipy.sparse.coo_matrix, int)
-            The graph, scipy sparse coo_matrix and its label.
-        """
-        if self._transform is None:
-            g = self.graphs[i]
-        else:
-            g = self._transform(self.graphs[i])
-        return (g, self.pmpds[i], self.labels[i])
-
-    @property
-    def save_name(self):
-        return self.name + "_dgl_graph"
-
-    # @staticmethod
-    # def collate_fn(cur):
-    #     graphs, pmpds, labels = zip(*cur)
-    #     batched_graphs = batch.batch(graphs)
-    #     batched_pmpds = sp.block_diag(pmpds)
-    #     batched_labels = np.concatenate(labels, axis=0)
-    #     return batched_graphs, batched_pmpds, batched_labels
-
-
-def _normalize(mx):
-    """Row-normalize sparse matrix"""
-    rowsum = np.asarray(mx.sum(1))
-    r_inv = np.power(rowsum, -1).flatten()
-    r_inv[np.isinf(r_inv)] = 0.0
-    r_mat_inv = sp.diags(r_inv)
-    mx = r_mat_inv.dot(mx)
-    return mx
-
-
-def _encode_onehot(labels):
-    classes = list(sorted(set(labels)))
-    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}
-    labels_onehot = np.asarray(list(map(classes_dict.get, labels)), dtype=np.int32)
-    return labels_onehot
+"""Cora, citeseer, pubmed dataset.
+
+"""
+from __future__ import absolute_import
+
+import os
+import pickle as pkl
+import sys
+
+import easygraph as eg
+import numpy as np
+import scipy.sparse as sp
+
+from easygraph.classes.graph import Graph
+
+from .graph_dataset_base import EasyGraphBuiltinDataset
+from .utils import _get_dgl_url
+from .utils import data_type_dict
+from .utils import deprecate_property
+from .utils import generate_mask_tensor
+from .utils import nonzero_1d
+from .utils import tensor
+
+
+def _pickle_load(pkl_file):
+    if sys.version_info > (3, 0):
+        return pkl.load(pkl_file, encoding="latin1")
+    else:
+        return pkl.load(pkl_file)
+
+
+class CitationGraphDataset(EasyGraphBuiltinDataset):
+    r"""The citation graph dataset, including Cora, CiteSeer and PubMed.
+    Nodes mean authors and edges mean citation relationships.
+
+    Parameters
+    -----------
+    name: str
+      name can be 'Cora', 'CiteSeer' or 'PubMed'.
+    raw_dir : str
+        Raw file directory to download/contains the input data directory.
+        Default: ~/.dgl/
+    force_reload : bool
+        Whether to reload the dataset. Default: False
+    verbose : bool
+        Whether to print out progress information. Default: True.
+    reverse_edge : bool
+        Whether to add reverse edges in graph. Default: True.
+    transform : callable, optional
+        A transform that takes in a :class:`~eg.Graph` object and returns
+        a transformed version. The :class:`~eg.Graph` object will be
+        transformed before every access.
+    reorder : bool
+        Whether to reorder the graph using :func:`~eg.reorder_graph`. Default: False.
+    """
+    _urls = {
+        "cora_v2": "dataset/cora_v2.zip",
+        "citeseer": "dataset/citeSeer.zip",
+        "pubmed": "dataset/pubmed.zip",
+    }
+
+    def __init__(
+        self,
+        name,
+        raw_dir=None,
+        force_reload=False,
+        verbose=True,
+        reverse_edge=True,
+        transform=None,
+        reorder=False,
+    ):
+        assert name.lower() in ["cora", "citeseer", "pubmed"]
+
+        # Previously we use the pre-processing in pygcn (https://github.com/tkipf/pygcn)
+        # for Cora, which is slightly different from the one used in the GCN paper
+        if name.lower() == "cora":
+            name = "cora_v2"
+
+        url = _get_dgl_url(self._urls[name])
+        self._reverse_edge = reverse_edge
+        self._reorder = reorder
+
+        super(CitationGraphDataset, self).__init__(
+            name,
+            url=url,
+            raw_dir=raw_dir,
+            force_reload=force_reload,
+            verbose=verbose,
+            transform=transform,
+        )
+
+    def process(self):
+        """Loads input data from data directory and reorder graph for better locality
+
+        ind.name.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;
+        ind.name.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;
+        ind.name.allx => the feature vectors of both labeled and unlabeled training instances
+            (a superset of ind.name.x) as scipy.sparse.csr.csr_matrix object;
+        ind.name.y => the one-hot labels of the labeled training instances as numpy.ndarray object;
+        ind.name.ty => the one-hot labels of the test instances as numpy.ndarray object;
+        ind.name.ally => the labels for instances in ind.name.allx as numpy.ndarray object;
+        ind.name.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict
+            object;
+        ind.name.test.index => the indices of test instances in graph, for the inductive setting as list object.
+        """
+        root = self.raw_path
+        objnames = ["x", "y", "tx", "ty", "allx", "ally", "graph"]
+        objects = []
+        for i in range(len(objnames)):
+            with open("{}/ind.{}.{}".format(root, self.name, objnames[i]), "rb") as f:
+                objects.append(_pickle_load(f))
+
+        x, y, tx, ty, allx, ally, graph = tuple(objects)
+        test_idx_reorder = _parse_index_file(
+            "{}/ind.{}.test.index".format(root, self.name)
+        )
+        test_idx_range = np.sort(test_idx_reorder)
+
+        if self.name == "citeseer":
+            # Fix CiteSeer dataset (there are some isolated nodes in the graph)
+            # Find isolated nodes, add them as zero-vecs into the right position
+            test_idx_range_full = range(
+                min(test_idx_reorder), max(test_idx_reorder) + 1
+            )
+            tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))
+            tx_extended[test_idx_range - min(test_idx_range), :] = tx
+            tx = tx_extended
+            ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))
+            ty_extended[test_idx_range - min(test_idx_range), :] = ty
+            ty = ty_extended
+
+        features = sp.vstack((allx, tx)).tolil()
+        features[test_idx_reorder, :] = features[test_idx_range, :]
+
+        if self.reverse_edge:
+            g = eg.DiGraph(eg.from_dict_of_lists(graph))
+            # g = from_networkx(graph)
+        else:
+            graph = eg.Graph(eg.from_dict_of_lists(graph))
+            # edges = list(graph.edges())
+            # u, v = map(list, zip(*edges))
+            # g = dgl_graph((u, v))
+
+        onehot_labels = np.vstack((ally, ty))
+        onehot_labels[test_idx_reorder, :] = onehot_labels[test_idx_range, :]
+        labels = np.argmax(onehot_labels, 1)
+
+        idx_test = test_idx_range.tolist()
+        idx_train = range(len(y))
+        idx_val = range(len(y), len(y) + 500)
+
+        train_mask = generate_mask_tensor(_sample_mask(idx_train, labels.shape[0]))
+        val_mask = generate_mask_tensor(_sample_mask(idx_val, labels.shape[0]))
+        test_mask = generate_mask_tensor(_sample_mask(idx_test, labels.shape[0]))
+
+        g.ndata["train_mask"] = train_mask
+        g.ndata["val_mask"] = val_mask
+        g.ndata["test_mask"] = test_mask
+        g.ndata["label"] = tensor(labels)
+        g.ndata["feat"] = tensor(
+            _preprocess_features(features), dtype=data_type_dict()["float32"]
+        )
+        self._num_classes = onehot_labels.shape[1]
+        self._labels = labels
+        # if self._reorder:
+        #     self._g = reorder_graph(
+        #         g, node_permute_algo='rcmk', edge_permute_algo='dst', store_ids=False)
+        # else:
+        self._g = g
+
+        if self.verbose:
+            print("Finished data loading and preprocessing.")
+            print("  NumNodes: {}".format(self._g.number_of_nodes()))
+            print("  NumEdges: {}".format(self._g.number_of_edges()))
+            print("  NumFeats: {}".format(self._g.ndata["feat"].shape[1]))
+            print("  NumClasses: {}".format(self.num_classes))
+            print(
+                "  NumTrainingSamples: {}".format(
+                    nonzero_1d(self._g.ndata["train_mask"]).shape[0]
+                )
+            )
+            print(
+                "  NumValidationSamples: {}".format(
+                    nonzero_1d(self._g.ndata["val_mask"]).shape[0]
+                )
+            )
+            print(
+                "  NumTestSamples: {}".format(
+                    nonzero_1d(self._g.ndata["test_mask"]).shape[0]
+                )
+            )
+
+    def has_cache(self):
+        graph_path = os.path.join(self.save_path, self.save_name + ".bin")
+        info_path = os.path.join(self.save_path, self.save_name + ".pkl")
+        if os.path.exists(graph_path) and os.path.exists(info_path):
+            return True
+
+        return False
+
+    # def save(self):
+    #     """save the graph list and the labels"""
+    #     graph_path = os.path.join(self.save_path,
+    #                               self.save_name + '.bin')
+    #     info_path = os.path.join(self.save_path,
+    #                              self.save_name + '.pkl')
+    #     save_graphs(str(graph_path), self._g)
+    #     save_info(str(info_path), {'num_classes': self.num_classes})
+    #
+    # def load(self):
+    #     graph_path = os.path.join(self.save_path,
+    #                               self.save_name + '.bin')
+    #     info_path = os.path.join(self.save_path,
+    #                              self.save_name + '.pkl')
+    #     graphs, _ = load_graphs(str(graph_path))
+    #
+    #     info = load_info(str(info_path))
+    #     graph = graphs[0]
+    #     self._g = graph
+    #     # for compatibility
+    #     graph = graph.clone()
+    #     graph.ndata.pop('train_mask')
+    #     graph.ndata.pop('val_mask')
+    #     graph.ndata.pop('test_mask')
+    #     graph.ndata.pop('feat')
+    #     graph.ndata.pop('label')
+    #     graph = to_networkx(graph)
+    #
+    #     self._num_classes = info['num_classes']
+    #     self._g.ndata['train_mask'] = generate_mask_tensor(F.asnumpy(self._g.ndata['train_mask']))
+    #     self._g.ndata['val_mask'] = generate_mask_tensor(F.asnumpy(self._g.ndata['val_mask']))
+    #     self._g.ndata['test_mask'] = generate_mask_tensor(F.asnumpy(self._g.ndata['test_mask']))
+    #     # hack for mxnet compatibility
+    #
+    #     if self.verbose:
+    #         print('  NumNodes: {}'.format(self._g.number_of_nodes()))
+    #         print('  NumEdges: {}'.format(self._g.number_of_edges()))
+    #         print('  NumFeats: {}'.format(self._g.ndata['feat'].shape[1]))
+    #         print('  NumClasses: {}'.format(self.num_classes))
+    #         print('  NumTrainingSamples: {}'.format(
+    #             F.nonzero_1d(self._g.ndata['train_mask']).shape[0]))
+    #         print('  NumValidationSamples: {}'.format(
+    #             F.nonzero_1d(self._g.ndata['val_mask']).shape[0]))
+    #         print('  NumTestSamples: {}'.format(
+    #             F.nonzero_1d(self._g.ndata['test_mask']).shape[0]))
+
+    def __getitem__(self, idx):
+        assert idx == 0, "This dataset has only one graph"
+        if self._transform is None:
+            return self._g
+        else:
+            return self._transform(self._g)
+
+    def __len__(self):
+        return 1
+
+    @property
+    def save_name(self):
+        return self.name + "_dgl_graph"
+
+    @property
+    def num_labels(self):
+        deprecate_property("dataset.num_labels", "dataset.num_classes")
+        return self.num_classes
+
+    @property
+    def num_classes(self):
+        return self._num_classes
+
+    """ Citation graph is used in many examples
+        We preserve these properties for compatibility.
+    """
+
+    @property
+    def reverse_edge(self):
+        return self._reverse_edge
+
+
+def _preprocess_features(features):
+    """Row-normalize feature matrix and convert to tuple representation"""
+    rowsum = np.asarray(features.sum(1))
+    r_inv = np.power(rowsum, -1).flatten()
+    r_inv[np.isinf(r_inv)] = 0.0
+    r_mat_inv = sp.diags(r_inv)
+    features = r_mat_inv.dot(features)
+    return np.asarray(features.todense())
+
+
+def _parse_index_file(filename):
+    """Parse index file."""
+    index = []
+    for line in open(filename):
+        index.append(int(line.strip()))
+    return index
+
+
+def _sample_mask(idx, l):
+    """Create mask."""
+    mask = np.zeros(l)
+    mask[idx] = 1
+    return mask
+
+
+class CoraGraphDataset(CitationGraphDataset):
+    r"""Cora citation network dataset.
+
+    Nodes mean paper and edges mean citation
+    relationships. Each node has a predefined
+    feature with 1433 dimensions. The dataset is
+    designed for the node classification task.
+    The task is to predict the category of
+    certain paper.
+
+    Statistics:
+
+    - Nodes: 2708
+    - Edges: 10556
+    - Number of Classes: 7
+    - Label split:
+
+        - Train: 140
+        - Valid: 500
+        - Test: 1000
+
+    Parameters
+    ----------
+    raw_dir : str
+        Raw file directory to download/contains the input data directory.
+        Default: ~/.dgl/
+    force_reload : bool
+        Whether to reload the dataset. Default: False
+    verbose : bool
+        Whether to print out progress information. Default: True.
+    reverse_edge : bool
+        Whether to add reverse edges in graph. Default: True.
+    transform : callable, optional
+        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
+        a transformed version. The :class:`~dgl.DGLGraph` object will be
+        transformed before every access.
+    reorder : bool
+        Whether to reorder the graph using :func:`~dgl.reorder_graph`. Default: False.
+
+    Attributes
+    ----------
+    num_classes: int
+        Number of label classes
+
+    Notes
+    -----
+    The node feature is row-normalized.
+
+    Examples
+    --------
+    >>> dataset = CoraGraphDataset()
+    >>> g = dataset[0]
+    >>> num_class = dataset.num_classes
+    >>>
+    >>> # get node feature
+    >>> feat = g.ndata['feat']
+    >>>
+    >>> # get data split
+    >>> train_mask = g.ndata['train_mask']
+    >>> val_mask = g.ndata['val_mask']
+    >>> test_mask = g.ndata['test_mask']
+    >>>
+    >>> # get labels
+    >>> label = g.ndata['label']
+
+    """
+
+    def __init__(
+        self,
+        raw_dir=None,
+        force_reload=False,
+        verbose=True,
+        reverse_edge=True,
+        transform=None,
+        reorder=False,
+    ):
+        name = "cora"
+
+        super(CoraGraphDataset, self).__init__(
+            name, raw_dir, force_reload, verbose, reverse_edge, transform, reorder
+        )
+
+    def __getitem__(self, idx):
+        r"""Gets the graph object
+
+        Parameters
+        -----------
+        idx: int
+            Item index, CoraGraphDataset has only one graph object
+
+        Return
+        ------
+        :class:`dgl.DGLGraph`
+
+            graph structure, node features and labels.
+
+            - ``ndata['train_mask']``: mask for training node set
+            - ``ndata['val_mask']``: mask for validation node set
+            - ``ndata['test_mask']``: mask for test node set
+            - ``ndata['feat']``: node feature
+            - ``ndata['label']``: ground truth labels
+        """
+        return super(CoraGraphDataset, self).__getitem__(idx)
+
+    def __len__(self):
+        r"""The number of graphs in the dataset."""
+        return super(CoraGraphDataset, self).__len__()
+
+
+class CiteseerGraphDataset(CitationGraphDataset):
+    r"""Citeseer citation network dataset.
+
+    Nodes mean scientific publications and edges
+    mean citation relationships. Each node has a
+    predefined feature with 3703 dimensions. The
+    dataset is designed for the node classification
+    task. The task is to predict the category of
+    certain publication.
+
+    Statistics:
+
+    - Nodes: 3327
+    - Edges: 9228
+    - Number of Classes: 6
+    - Label Split:
+
+        - Train: 120
+        - Valid: 500
+        - Test: 1000
+
+    Parameters
+    -----------
+    raw_dir : str
+        Raw file directory to download/contains the input data directory.
+        Default: ~/.dgl/
+    force_reload : bool
+        Whether to reload the dataset. Default: False
+    verbose : bool
+        Whether to print out progress information. Default: True.
+    reverse_edge : bool
+        Whether to add reverse edges in graph. Default: True.
+    transform : callable, optional
+        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
+        a transformed version. The :class:`~dgl.DGLGraph` object will be
+        transformed before every access.
+    reorder : bool
+        Whether to reorder the graph using :func:`~dgl.reorder_graph`. Default: False.
+
+    Attributes
+    ----------
+    num_classes: int
+        Number of label classes
+
+    Notes
+    -----
+    The node feature is row-normalized.
+
+    In citeseer dataset, there are some isolated nodes in the graph.
+    These isolated nodes are added as zero-vecs into the right position.
+
+    Examples
+    --------
+    >>> dataset = CiteseerGraphDataset()
+    >>> g = dataset[0]
+    >>> num_class = dataset.num_classes
+    >>>
+    >>> # get node feature
+    >>> feat = g.ndata['feat']
+    >>>
+    >>> # get data split
+    >>> train_mask = g.ndata['train_mask']
+    >>> val_mask = g.ndata['val_mask']
+    >>> test_mask = g.ndata['test_mask']
+    >>>
+    >>> # get labels
+    >>> label = g.ndata['label']
+
+    """
+
+    def __init__(
+        self,
+        raw_dir=None,
+        force_reload=False,
+        verbose=True,
+        reverse_edge=True,
+        transform=None,
+        reorder=False,
+    ):
+        name = "citeseer"
+
+        super(CiteseerGraphDataset, self).__init__(
+            name, raw_dir, force_reload, verbose, reverse_edge, transform, reorder
+        )
+
+    def __getitem__(self, idx):
+        r"""Gets the graph object
+
+        Parameters
+        -----------
+        idx: int
+            Item index, CiteseerGraphDataset has only one graph object
+
+        Return
+        ------
+        :class:`dgl.DGLGraph`
+
+            graph structure, node features and labels.
+
+            - ``ndata['train_mask']``: mask for training node set
+            - ``ndata['val_mask']``: mask for validation node set
+            - ``ndata['test_mask']``: mask for test node set
+            - ``ndata['feat']``: node feature
+            - ``ndata['label']``: ground truth labels
+        """
+        return super(CiteseerGraphDataset, self).__getitem__(idx)
+
+    def __len__(self):
+        r"""The number of graphs in the dataset."""
+        return super(CiteseerGraphDataset, self).__len__()
+
+
+class PubmedGraphDataset(CitationGraphDataset):
+    r"""Pubmed citation network dataset.
+
+    Nodes mean scientific publications and edges
+    mean citation relationships. Each node has a
+    predefined feature with 500 dimensions. The
+    dataset is designed for the node classification
+    task. The task is to predict the category of
+    certain publication.
+
+    Statistics:
+
+    - Nodes: 19717
+    - Edges: 88651
+    - Number of Classes: 3
+    - Label Split:
+
+        - Train: 60
+        - Valid: 500
+        - Test: 1000
+
+    Parameters
+    -----------
+    raw_dir : str
+        Raw file directory to download/contains the input data directory.
+        Default: ~/.dgl/
+    force_reload : bool
+        Whether to reload the dataset. Default: False
+    verbose : bool
+        Whether to print out progress information. Default: True.
+    reverse_edge : bool
+        Whether to add reverse edges in graph. Default: True.
+    transform : callable, optional
+        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
+        a transformed version. The :class:`~dgl.DGLGraph` object will be
+        transformed before every access.
+    reorder : bool
+        Whether to reorder the graph using :func:`~dgl.reorder_graph`. Default: False.
+
+    Attributes
+    ----------
+    num_classes: int
+        Number of label classes
+
+    Notes
+    -----
+    The node feature is row-normalized.
+
+    Examples
+    --------
+    >>> dataset = PubmedGraphDataset()
+    >>> g = dataset[0]
+    >>> num_class = dataset.num_of_class
+    >>>
+    >>> # get node feature
+    >>> feat = g.ndata['feat']
+    >>>
+    >>> # get data split
+    >>> train_mask = g.ndata['train_mask']
+    >>> val_mask = g.ndata['val_mask']
+    >>> test_mask = g.ndata['test_mask']
+    >>>
+    >>> # get labels
+    >>> label = g.ndata['label']
+
+    """
+
+    def __init__(
+        self,
+        raw_dir=None,
+        force_reload=False,
+        verbose=True,
+        reverse_edge=True,
+        transform=None,
+        reorder=False,
+    ):
+        name = "pubmed"
+
+        super(PubmedGraphDataset, self).__init__(
+            name, raw_dir, force_reload, verbose, reverse_edge, transform, reorder
+        )
+
+    def __getitem__(self, idx):
+        r"""Gets the graph object
+
+        Parameters
+        -----------
+        idx: int
+            Item index, PubmedGraphDataset has only one graph object
+
+        Return
+        ------
+        :class:`dgl.DGLGraph`
+
+            graph structure, node features and labels.
+
+            - ``ndata['train_mask']``: mask for training node set
+            - ``ndata['val_mask']``: mask for validation node set
+            - ``ndata['test_mask']``: mask for test node set
+            - ``ndata['feat']``: node feature
+            - ``ndata['label']``: ground truth labels
+        """
+        return super(PubmedGraphDataset, self).__getitem__(idx)
+
+    def __len__(self):
+        r"""The number of graphs in the dataset."""
+        return super(PubmedGraphDataset, self).__len__()
+
+
+def load_cora(
+    raw_dir=None, force_reload=False, verbose=True, reverse_edge=True, transform=None
+):
+    """Get CoraGraphDataset
+
+    Parameters
+    -----------
+    raw_dir : str
+        Raw file directory to download/contains the input data directory.
+        Default: ~/.dgl/
+    force_reload : bool
+        Whether to reload the dataset. Default: False
+    verbose : bool
+        Whether to print out progress information. Default: True.
+    reverse_edge : bool
+        Whether to add reverse edges in graph. Default: True.
+    transform : callable, optional
+        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
+        a transformed version. The :class:`~dgl.DGLGraph` object will be
+        transformed before every access.
+
+    Return
+    -------
+    CoraGraphDataset
+    """
+    data = CoraGraphDataset(raw_dir, force_reload, verbose, reverse_edge, transform)
+    return data
+
+
+def load_citeseer(
+    raw_dir=None, force_reload=False, verbose=True, reverse_edge=True, transform=None
+):
+    """Get CiteseerGraphDataset
+
+    Parameters
+    -----------
+    raw_dir : str
+        Raw file directory to download/contains the input data directory.
+        Default: ~/.dgl/
+    force_reload : bool
+        Whether to reload the dataset. Default: False
+    verbose : bool
+        Whether to print out progress information. Default: True.
+    reverse_edge : bool
+        Whether to add reverse edges in graph. Default: True.
+    transform : callable, optional
+        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
+        a transformed version. The :class:`~dgl.DGLGraph` object will be
+        transformed before every access.
+
+    Return
+    -------
+    CiteseerGraphDataset
+    """
+    data = CiteseerGraphDataset(raw_dir, force_reload, verbose, reverse_edge, transform)
+    return data
+
+
+def load_pubmed(
+    raw_dir=None, force_reload=False, verbose=True, reverse_edge=True, transform=None
+):
+    """Get PubmedGraphDataset
+
+    Parameters
+    -----------
+    raw_dir : str
+        Raw file directory to download/contains the input data directory.
+        Default: ~/.dgl/
+    force_reload : bool
+        Whether to reload the dataset. Default: False
+    verbose : bool
+        Whether to print out progress information. Default: True.
+    reverse_edge : bool
+        Whether to add reverse edges in graph. Default: True.
+    transform : callable, optional
+        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
+        a transformed version. The :class:`~dgl.DGLGraph` object will be
+        transformed before every access.
+
+    Return
+    -------
+    PubmedGraphDataset
+    """
+    data = PubmedGraphDataset(raw_dir, force_reload, verbose, reverse_edge, transform)
+    return data
+
+
+class CoraBinary(EasyGraphBuiltinDataset):
+    """A mini-dataset for binary classification task using Cora.
+
+    After loaded, it has following members:
+
+    graphs : list of :class:`~dgl.DGLGraph`
+    pmpds : list of :class:`scipy.sparse.coo_matrix`
+    labels : list of :class:`numpy.ndarray`
+
+    Parameters
+    -----------
+    raw_dir : str
+        Raw file directory to download/contains the input data directory.
+        Default: ~/.dgl/
+    force_reload : bool
+        Whether to reload the dataset. Default: False
+    verbose: bool
+        Whether to print out progress information. Default: True.
+    transform : callable, optional
+        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
+        a transformed version. The :class:`~dgl.DGLGraph` object will be
+        transformed before every access.
+    """
+
+    def __init__(self, raw_dir=None, force_reload=False, verbose=True, transform=None):
+        name = "cora_binary"
+        url = _get_dgl_url("dataset/cora_binary.zip")
+        super(CoraBinary, self).__init__(
+            name,
+            url=url,
+            raw_dir=raw_dir,
+            force_reload=force_reload,
+            verbose=verbose,
+            transform=transform,
+        )
+
+    def process(self):
+        root = self.raw_path
+        # load graphs
+        self.graphs = []
+        with open("{}/graphs.txt".format(root), "r") as f:
+            elist = []
+            for line in f.readlines():
+                if line.startswith("graph"):
+                    if len(elist) != 0:
+                        self.graphs.append(Graph(elist))
+                    elist = []
+                else:
+                    u, v = line.strip().split(" ")
+                    elist.append((int(u), int(v)))
+            if len(elist) != 0:
+                self.graphs.append(Graph(tuple(zip(*elist))))
+        with open("{}/pmpds.pkl".format(root), "rb") as f:
+            self.pmpds = _pickle_load(f)
+        self.labels = []
+        with open("{}/labels.txt".format(root), "r") as f:
+            cur = []
+            for line in f.readlines():
+                if line.startswith("graph"):
+                    if len(cur) != 0:
+                        self.labels.append(np.asarray(cur))
+                    cur = []
+                else:
+                    cur.append(int(line.strip()))
+            if len(cur) != 0:
+                self.labels.append(np.asarray(cur))
+        # sanity check
+        assert len(self.graphs) == len(self.pmpds)
+        assert len(self.graphs) == len(self.labels)
+
+    def has_cache(self):
+        graph_path = os.path.join(self.save_path, self.save_name + ".bin")
+        if os.path.exists(graph_path):
+            return True
+
+        return False
+
+    # def save(self):
+    #     """save the graph list and the labels"""
+    #     graph_path = os.path.join(self.save_path,
+    #                               self.save_name + '.bin')
+    #     labels = {}
+    #     for i, label in enumerate(self.labels):
+    #         labels['{}'.format(i)] = F.tensor(label)
+    #     save_graphs(str(graph_path), self.graphs, labels)
+    #     if self.verbose:
+    #         print('Done saving data into cached files.')
+    #
+    # def load(self):
+    #     graph_path = os.path.join(self.save_path,
+    #                               self.save_name + '.bin')
+    #     self.graphs, labels = load_graphs(str(graph_path))
+    #
+    #     self.labels = []
+    #     for i in range(len(labels)):
+    #         self.labels.append(F.asnumpy(labels['{}'.format(i)]))
+    #     # load pmpds under self.raw_path
+    #     with open("{}/pmpds.pkl".format(self.raw_path), 'rb') as f:
+    #         self.pmpds = _pickle_load(f)
+    #     if self.verbose:
+    #         print('Done loading data into cached files.')
+    #     # sanity check
+    #     assert len(self.graphs) == len(self.pmpds)
+    #     assert len(self.graphs) == len(self.labels)
+
+    def __len__(self):
+        return len(self.graphs)
+
+    def __getitem__(self, i):
+        r"""Gets the idx-th sample.
+
+        Parameters
+        -----------
+        idx : int
+            The sample index.
+
+        Returns
+        -------
+        (dgl.DGLGraph, scipy.sparse.coo_matrix, int)
+            The graph, scipy sparse coo_matrix and its label.
+        """
+        if self._transform is None:
+            g = self.graphs[i]
+        else:
+            g = self._transform(self.graphs[i])
+        return (g, self.pmpds[i], self.labels[i])
+
+    @property
+    def save_name(self):
+        return self.name + "_dgl_graph"
+
+    # @staticmethod
+    # def collate_fn(cur):
+    #     graphs, pmpds, labels = zip(*cur)
+    #     batched_graphs = batch.batch(graphs)
+    #     batched_pmpds = sp.block_diag(pmpds)
+    #     batched_labels = np.concatenate(labels, axis=0)
+    #     return batched_graphs, batched_pmpds, batched_labels
+
+
+def _normalize(mx):
+    """Row-normalize sparse matrix"""
+    rowsum = np.asarray(mx.sum(1))
+    r_inv = np.power(rowsum, -1).flatten()
+    r_inv[np.isinf(r_inv)] = 0.0
+    r_mat_inv = sp.diags(r_inv)
+    mx = r_mat_inv.dot(mx)
+    return mx
+
+
+def _encode_onehot(labels):
+    classes = list(sorted(set(labels)))
+    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}
+    labels_onehot = np.asarray(list(map(classes_dict.get, labels)), dtype=np.int32)
+    return labels_onehot
```

## easygraph/datasets/gnn_benchmark.py

 * *Ordering differences only*

```diff
@@ -1,216 +1,216 @@
-import os
-
-import numpy as np
-import scipy.sparse as sp
-
-from easygraph.classes.graph import Graph
-
-from .graph_dataset_base import EasyGraphBuiltinDataset
-from .utils import _get_dgl_url
-from .utils import _set_labels
-from .utils import data_type_dict
-from .utils import tensor
-
-
-__all__ = [
-    "AmazonCoBuyComputerDataset",
-]
-
-
-class GNNBenchmarkDataset(EasyGraphBuiltinDataset):
-    r"""Base Class for GNN Benchmark dataset
-
-    Reference: https://github.com/shchur/gnn-benchmark#datasets
-    """
-
-    def __init__(
-        self, name, raw_dir=None, force_reload=False, verbose=True, transform=None
-    ):
-        _url = _get_dgl_url("dataset/" + name + ".zip")
-        super(GNNBenchmarkDataset, self).__init__(
-            name=name,
-            url=_url,
-            raw_dir=raw_dir,
-            force_reload=force_reload,
-            verbose=verbose,
-            transform=transform,
-        )
-
-    def process(self):
-        npz_path = os.path.join(self.raw_path, self.name + ".npz")
-        g = self._load_npz(npz_path)
-        # g = transforms.reorder_graph(
-        #     g, node_permute_algo='rcmk', edge_permute_algo='dst', store_ids=False)
-        self._graph = g
-        self._data = [g]
-        self._print_info()
-
-    def has_cache(self):
-        graph_path = os.path.join(self.save_path, "dgl_graph_v1.bin")
-        if os.path.exists(graph_path):
-            return True
-        return False
-
-    # def save(self):
-    #     graph_path = os.path.join(self.save_path, 'dgl_graph_v1.bin')
-    #     save_graphs(graph_path, self._graph)
-    #
-    # def load(self):
-    #     graph_path = os.path.join(self.save_path, 'dgl_graph_v1.bin')
-    #     graphs, _ = load_graphs(graph_path)
-    #     self._graph = graphs[0]
-    #     self._data = [graphs[0]]
-    #     self._print_info()
-
-    def _print_info(self):
-        if self.verbose:
-            print("  NumNodes: {}".format(self._graph.number_of_nodes()))
-            print("  NumEdges: {}".format(2 * self._graph.number_of_edges()))
-            print("  NumFeats: {}".format(self._graph.ndata["feat"].shape[-1]))
-            print("  NumbClasses: {}".format(self.num_classes))
-
-    def _load_npz(self, file_name):
-        with np.load(file_name, allow_pickle=True) as loader:
-            loader = dict(loader)
-            num_nodes = loader["adj_shape"][0]
-            adj_matrix = sp.csr_matrix(
-                (loader["adj_data"], loader["adj_indices"], loader["adj_indptr"]),
-                shape=loader["adj_shape"],
-            ).tocoo()
-
-            if "attr_data" in loader:
-                # Attributes are stored as a sparse CSR matrix
-                attr_matrix = sp.csr_matrix(
-                    (
-                        loader["attr_data"],
-                        loader["attr_indices"],
-                        loader["attr_indptr"],
-                    ),
-                    shape=loader["attr_shape"],
-                ).todense()
-            elif "attr_matrix" in loader:
-                # Attributes are stored as a (dense) np.ndarray
-                attr_matrix = loader["attr_matrix"]
-            else:
-                attr_matrix = None
-
-            if "labels_data" in loader:
-                # Labels are stored as a CSR matrix
-                labels = sp.csr_matrix(
-                    (
-                        loader["labels_data"],
-                        loader["labels_indices"],
-                        loader["labels_indptr"],
-                    ),
-                    shape=loader["labels_shape"],
-                ).todense()
-            elif "labels" in loader:
-                # Labels are stored as a numpy array
-                labels = loader["labels"]
-            else:
-                labels = None
-        if hasattr(adj_matrix, "format"):
-            print("can be generate eg!")
-        g = Graph(incoming_graph_data=adj_matrix)
-        # g = transforms.to_bidirected(g)
-        g = _set_labels(g, labels)
-        g.ndata["feat"] = tensor(attr_matrix, data_type_dict()["float32"])
-        g.ndata["label"] = tensor(labels, data_type_dict()["int64"])
-        return g
-
-    @property
-    def num_classes(self):
-        """Number of classes."""
-        raise NotImplementedError
-
-    def __getitem__(self, idx):
-        r"""Get graph by index
-
-        Parameters
-        ----------
-        idx : int
-            Item index
-
-        Returns
-        -------
-        :class:`dgl.DGLGraph`
-
-            The graph contains:
-
-            - ``ndata['feat']``: node features
-            - ``ndata['label']``: node labels
-        """
-        assert idx == 0, "This dataset has only one graph"
-        if self._transform is None:
-            return self._graph
-        else:
-            return self._transform(self._graph)
-
-    def __len__(self):
-        r"""Number of graphs in the dataset"""
-        return 1
-
-
-class AmazonCoBuyComputerDataset(GNNBenchmarkDataset):
-    r"""'Computer' part of the AmazonCoBuy dataset for node classification task.
-
-    Amazon Computers and Amazon Photo are segments of the Amazon co-purchase graph [McAuley et al., 2015],
-    where nodes represent goods, edges indicate that two goods are frequently bought together, node
-    features are bag-of-words encoded product reviews, and class labels are given by the product category.
-
-    Reference: `<https://github.com/shchur/gnn-benchmark#datasets>`_
-
-    Statistics:
-
-    - Nodes: 13,752
-    - Edges: 491,722 (note that the original dataset has 245,778 edges but DGL adds
-      the reverse edges and remove the duplicates, hence with a different number)
-    - Number of classes: 10
-    - Node feature size: 767
-
-    Parameters
-    ----------
-    raw_dir : str
-        Raw file directory to download/contains the input data directory.
-        Default: ~/.dgl/
-    force_reload : bool
-        Whether to reload the dataset. Default: False
-    verbose : bool
-        Whether to print out progress information. Default: True.
-    transform : callable, optional
-        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
-        a transformed version. The :class:`~dgl.DGLGraph` object will be
-        transformed before every access.
-
-    Attributes
-    ----------
-    num_classes : int
-        Number of classes for each node.
-
-    Examples
-    --------
-    >>> data = AmazonCoBuyComputerDataset()
-    >>> g = data[0]
-    >>> num_class = data.num_classes
-    >>> feat = g.ndata['feat']  # get node feature
-    >>> label = g.ndata['label']  # get node labels
-    """
-
-    def __init__(self, raw_dir=None, force_reload=False, verbose=True, transform=None):
-        super(AmazonCoBuyComputerDataset, self).__init__(
-            name="amazon_co_buy_computer",
-            raw_dir=raw_dir,
-            force_reload=force_reload,
-            verbose=verbose,
-            transform=transform,
-        )
-
-    @property
-    def num_classes(self):
-        """Number of classes.
-
-        Return
-        -------
-        int
-        """
-        return 10
+import os
+
+import numpy as np
+import scipy.sparse as sp
+
+from easygraph.classes.graph import Graph
+
+from .graph_dataset_base import EasyGraphBuiltinDataset
+from .utils import _get_dgl_url
+from .utils import _set_labels
+from .utils import data_type_dict
+from .utils import tensor
+
+
+__all__ = [
+    "AmazonCoBuyComputerDataset",
+]
+
+
+class GNNBenchmarkDataset(EasyGraphBuiltinDataset):
+    r"""Base Class for GNN Benchmark dataset
+
+    Reference: https://github.com/shchur/gnn-benchmark#datasets
+    """
+
+    def __init__(
+        self, name, raw_dir=None, force_reload=False, verbose=True, transform=None
+    ):
+        _url = _get_dgl_url("dataset/" + name + ".zip")
+        super(GNNBenchmarkDataset, self).__init__(
+            name=name,
+            url=_url,
+            raw_dir=raw_dir,
+            force_reload=force_reload,
+            verbose=verbose,
+            transform=transform,
+        )
+
+    def process(self):
+        npz_path = os.path.join(self.raw_path, self.name + ".npz")
+        g = self._load_npz(npz_path)
+        # g = transforms.reorder_graph(
+        #     g, node_permute_algo='rcmk', edge_permute_algo='dst', store_ids=False)
+        self._graph = g
+        self._data = [g]
+        self._print_info()
+
+    def has_cache(self):
+        graph_path = os.path.join(self.save_path, "dgl_graph_v1.bin")
+        if os.path.exists(graph_path):
+            return True
+        return False
+
+    # def save(self):
+    #     graph_path = os.path.join(self.save_path, 'dgl_graph_v1.bin')
+    #     save_graphs(graph_path, self._graph)
+    #
+    # def load(self):
+    #     graph_path = os.path.join(self.save_path, 'dgl_graph_v1.bin')
+    #     graphs, _ = load_graphs(graph_path)
+    #     self._graph = graphs[0]
+    #     self._data = [graphs[0]]
+    #     self._print_info()
+
+    def _print_info(self):
+        if self.verbose:
+            print("  NumNodes: {}".format(self._graph.number_of_nodes()))
+            print("  NumEdges: {}".format(2 * self._graph.number_of_edges()))
+            print("  NumFeats: {}".format(self._graph.ndata["feat"].shape[-1]))
+            print("  NumbClasses: {}".format(self.num_classes))
+
+    def _load_npz(self, file_name):
+        with np.load(file_name, allow_pickle=True) as loader:
+            loader = dict(loader)
+            num_nodes = loader["adj_shape"][0]
+            adj_matrix = sp.csr_matrix(
+                (loader["adj_data"], loader["adj_indices"], loader["adj_indptr"]),
+                shape=loader["adj_shape"],
+            ).tocoo()
+
+            if "attr_data" in loader:
+                # Attributes are stored as a sparse CSR matrix
+                attr_matrix = sp.csr_matrix(
+                    (
+                        loader["attr_data"],
+                        loader["attr_indices"],
+                        loader["attr_indptr"],
+                    ),
+                    shape=loader["attr_shape"],
+                ).todense()
+            elif "attr_matrix" in loader:
+                # Attributes are stored as a (dense) np.ndarray
+                attr_matrix = loader["attr_matrix"]
+            else:
+                attr_matrix = None
+
+            if "labels_data" in loader:
+                # Labels are stored as a CSR matrix
+                labels = sp.csr_matrix(
+                    (
+                        loader["labels_data"],
+                        loader["labels_indices"],
+                        loader["labels_indptr"],
+                    ),
+                    shape=loader["labels_shape"],
+                ).todense()
+            elif "labels" in loader:
+                # Labels are stored as a numpy array
+                labels = loader["labels"]
+            else:
+                labels = None
+        if hasattr(adj_matrix, "format"):
+            print("can be generate eg!")
+        g = Graph(incoming_graph_data=adj_matrix)
+        # g = transforms.to_bidirected(g)
+        g = _set_labels(g, labels)
+        g.ndata["feat"] = tensor(attr_matrix, data_type_dict()["float32"])
+        g.ndata["label"] = tensor(labels, data_type_dict()["int64"])
+        return g
+
+    @property
+    def num_classes(self):
+        """Number of classes."""
+        raise NotImplementedError
+
+    def __getitem__(self, idx):
+        r"""Get graph by index
+
+        Parameters
+        ----------
+        idx : int
+            Item index
+
+        Returns
+        -------
+        :class:`dgl.DGLGraph`
+
+            The graph contains:
+
+            - ``ndata['feat']``: node features
+            - ``ndata['label']``: node labels
+        """
+        assert idx == 0, "This dataset has only one graph"
+        if self._transform is None:
+            return self._graph
+        else:
+            return self._transform(self._graph)
+
+    def __len__(self):
+        r"""Number of graphs in the dataset"""
+        return 1
+
+
+class AmazonCoBuyComputerDataset(GNNBenchmarkDataset):
+    r"""'Computer' part of the AmazonCoBuy dataset for node classification task.
+
+    Amazon Computers and Amazon Photo are segments of the Amazon co-purchase graph [McAuley et al., 2015],
+    where nodes represent goods, edges indicate that two goods are frequently bought together, node
+    features are bag-of-words encoded product reviews, and class labels are given by the product category.
+
+    Reference: `<https://github.com/shchur/gnn-benchmark#datasets>`_
+
+    Statistics:
+
+    - Nodes: 13,752
+    - Edges: 491,722 (note that the original dataset has 245,778 edges but DGL adds
+      the reverse edges and remove the duplicates, hence with a different number)
+    - Number of classes: 10
+    - Node feature size: 767
+
+    Parameters
+    ----------
+    raw_dir : str
+        Raw file directory to download/contains the input data directory.
+        Default: ~/.dgl/
+    force_reload : bool
+        Whether to reload the dataset. Default: False
+    verbose : bool
+        Whether to print out progress information. Default: True.
+    transform : callable, optional
+        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
+        a transformed version. The :class:`~dgl.DGLGraph` object will be
+        transformed before every access.
+
+    Attributes
+    ----------
+    num_classes : int
+        Number of classes for each node.
+
+    Examples
+    --------
+    >>> data = AmazonCoBuyComputerDataset()
+    >>> g = data[0]
+    >>> num_class = data.num_classes
+    >>> feat = g.ndata['feat']  # get node feature
+    >>> label = g.ndata['label']  # get node labels
+    """
+
+    def __init__(self, raw_dir=None, force_reload=False, verbose=True, transform=None):
+        super(AmazonCoBuyComputerDataset, self).__init__(
+            name="amazon_co_buy_computer",
+            raw_dir=raw_dir,
+            force_reload=force_reload,
+            verbose=verbose,
+            transform=transform,
+        )
+
+    @property
+    def num_classes(self):
+        """Number of classes.
+
+        Return
+        -------
+        int
+        """
+        return 10
```

## easygraph/datasets/get_sample_graph.py

 * *Ordering differences only*

```diff
@@ -1,210 +1,210 @@
-import easygraph as eg
-
-
-# import progressbar
-
-
-__all__ = [
-    "get_graph_karateclub",
-    "get_graph_blogcatalog",
-    "get_graph_youtube",
-    "get_graph_flickr",
-]
-
-
-def get_graph_karateclub():
-    """Returns the undirected graph of Karate Club.
-
-    Returns
-    -------
-    get_graph_karateclub : easygraph.Graph
-        The undirected graph instance of karate club from dataset:
-        http://vlado.fmf.uni-lj.si/pub/networks/data/Ucinet/UciData.htm
-
-    References
-    ----------
-    .. [1] http://vlado.fmf.uni-lj.si/pub/networks/data/Ucinet/UciData.htm
-
-    """
-    all_members = set(range(34))
-    club1 = {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 16, 17, 19, 21}
-    # club2 = all_members - club1
-
-    G = eg.Graph(name="Zachary's Karate Club")
-    for node in all_members:
-        G.add_node(node + 1)
-
-    zacharydat = """\
-0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0
-1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0
-1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0
-1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
-1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
-1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
-1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
-1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
-1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1
-0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
-1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
-1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
-1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
-1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
-0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1
-0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1
-0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
-1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
-0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1
-1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
-0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1
-1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
-0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1
-0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1
-0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0
-0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0
-0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1
-0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1
-0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1
-0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1
-0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1
-1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1
-0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1
-0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0"""
-
-    for row, line in enumerate(zacharydat.split("\n")):
-        thisrow = [int(b) for b in line.split()]
-        for col, entry in enumerate(thisrow):
-            if entry == 1:
-                G.add_edge(row + 1, col + 1)
-
-    # Add the name of each member's club as a node attribute.
-    for v in G:
-        G.nodes[v]["club"] = "Mr. Hi" if v in club1 else "Officer"
-    return G
-
-
-def get_graph_blogcatalog():
-    """Returns the undirected graph of blogcatalog.
-
-    Returns
-    -------
-    get_graph_blogcatalog : easygraph.Graph
-        The undirected graph instance of blogcatalog from dataset:
-        https://github.com/phanein/deepwalk/blob/master/example_graphs/blogcatalog.mat
-
-    References
-    ----------
-    .. [1] https://github.com/phanein/deepwalk/blob/master/example_graphs/blogcatalog.mat
-
-    """
-    from scipy.io import loadmat
-
-    def sparse2graph(x):
-        from collections import defaultdict
-
-        G = defaultdict(lambda: set())
-        cx = x.tocoo()
-        for i, j, v in zip(cx.row, cx.col, cx.data):
-            G[i].add(j)
-        return {str(k): [str(x) for x in v] for k, v in G.items()}
-
-    mat = loadmat("./samples/blogcatalog.mat")
-    A = mat["network"]
-    data = sparse2graph(A)
-
-    G = eg.Graph()
-    for u in data:
-        for v in data[u]:
-            G.add_edge(u, v)
-
-    return G
-
-
-def get_graph_youtube():
-    """Returns the undirected graph of Youtube dataset.
-
-    Returns
-    -------
-    get_graph_youtube : easygraph.Graph
-        The undirected graph instance of Youtube from dataset:
-        http://socialnetworks.mpi-sws.mpg.de/data/youtube-links.txt.gz
-
-    References
-    ----------
-    .. [1] http://socialnetworks.mpi-sws.mpg.de/data/youtube-links.txt.gz
-
-    """
-    import gzip
-
-    from urllib import request
-
-    url = "http://socialnetworks.mpi-sws.mpg.de/data/youtube-links.txt.gz"
-    zipped_data_path = "./samples/youtube-links.txt.gz"
-    unzipped_data_path = "./samples/youtube-links.txt"
-
-    # Download .gz file
-    print("Downloading Youtube dataset...")
-    request.urlretrieve(url, zipped_data_path, _show_progress)
-
-    # Unzip
-    unzipped_data = gzip.GzipFile(zipped_data_path)
-    open(unzipped_data_path, "wb+").write(unzipped_data.read())
-    unzipped_data.close()
-
-    # Returns graph
-    G = eg.Graph()
-    G.add_edges_from_file(file=unzipped_data_path)
-    return G
-
-
-def get_graph_flickr():
-    """Returns the undirected graph of Flickr dataset.
-
-    Returns
-    -------
-    get_graph_flickr : easygraph.Graph
-        The undirected graph instance of Flickr from dataset:
-        http://socialnetworks.mpi-sws.mpg.de/data/flickr-links.txt.gz
-
-    References
-    ----------
-    .. [1] http://socialnetworks.mpi-sws.mpg.de/data/flickr-links.txt.gz
-
-    """
-    import gzip
-
-    from urllib import request
-
-    url = "http://socialnetworks.mpi-sws.mpg.de/data/flickr-links.txt.gz"
-    zipped_data_path = "./samples/flickr-links.txt.gz"
-    unzipped_data_path = "./samples/flickr-links.txt"
-
-    # Download .gz file
-    print("Downloading Flickr dataset...")
-    request.urlretrieve(url, zipped_data_path, _show_progress)
-
-    # Unzip
-    unzipped_data = gzip.GzipFile(zipped_data_path)
-    open(unzipped_data_path, "wb+").write(unzipped_data.read())
-    unzipped_data.close()
-
-    # Returns graph
-    G = eg.Graph()
-    G.add_edges_from_file(file=unzipped_data_path)
-    return G
-
-
-_pbar = None
-
-
-def _show_progress(block_num, block_size, total_size):
-    global _pbar
-    if _pbar is None:
-        _pbar = progressbar.ProgressBar(maxval=total_size)
-        _pbar.start()
-
-    downloaded = block_num * block_size
-    if downloaded < total_size:
-        _pbar.update(downloaded)
-    else:
-        _pbar.finish()
-        _pbar = None
+import easygraph as eg
+
+
+# import progressbar
+
+
+__all__ = [
+    "get_graph_karateclub",
+    "get_graph_blogcatalog",
+    "get_graph_youtube",
+    "get_graph_flickr",
+]
+
+
+def get_graph_karateclub():
+    """Returns the undirected graph of Karate Club.
+
+    Returns
+    -------
+    get_graph_karateclub : easygraph.Graph
+        The undirected graph instance of karate club from dataset:
+        http://vlado.fmf.uni-lj.si/pub/networks/data/Ucinet/UciData.htm
+
+    References
+    ----------
+    .. [1] http://vlado.fmf.uni-lj.si/pub/networks/data/Ucinet/UciData.htm
+
+    """
+    all_members = set(range(34))
+    club1 = {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 16, 17, 19, 21}
+    # club2 = all_members - club1
+
+    G = eg.Graph(name="Zachary's Karate Club")
+    for node in all_members:
+        G.add_node(node + 1)
+
+    zacharydat = """\
+0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0
+1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0
+1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0
+1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
+1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
+1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
+1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
+1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
+1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1
+0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
+1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
+1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
+1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
+1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
+0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1
+0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1
+0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
+1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
+0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1
+1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
+0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1
+1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
+0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1
+0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1
+0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0
+0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0
+0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1
+0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1
+0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1
+0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1
+0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1
+1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1
+0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1
+0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0"""
+
+    for row, line in enumerate(zacharydat.split("\n")):
+        thisrow = [int(b) for b in line.split()]
+        for col, entry in enumerate(thisrow):
+            if entry == 1:
+                G.add_edge(row + 1, col + 1)
+
+    # Add the name of each member's club as a node attribute.
+    for v in G:
+        G.nodes[v]["club"] = "Mr. Hi" if v in club1 else "Officer"
+    return G
+
+
+def get_graph_blogcatalog():
+    """Returns the undirected graph of blogcatalog.
+
+    Returns
+    -------
+    get_graph_blogcatalog : easygraph.Graph
+        The undirected graph instance of blogcatalog from dataset:
+        https://github.com/phanein/deepwalk/blob/master/example_graphs/blogcatalog.mat
+
+    References
+    ----------
+    .. [1] https://github.com/phanein/deepwalk/blob/master/example_graphs/blogcatalog.mat
+
+    """
+    from scipy.io import loadmat
+
+    def sparse2graph(x):
+        from collections import defaultdict
+
+        G = defaultdict(lambda: set())
+        cx = x.tocoo()
+        for i, j, v in zip(cx.row, cx.col, cx.data):
+            G[i].add(j)
+        return {str(k): [str(x) for x in v] for k, v in G.items()}
+
+    mat = loadmat("./samples/blogcatalog.mat")
+    A = mat["network"]
+    data = sparse2graph(A)
+
+    G = eg.Graph()
+    for u in data:
+        for v in data[u]:
+            G.add_edge(u, v)
+
+    return G
+
+
+def get_graph_youtube():
+    """Returns the undirected graph of Youtube dataset.
+
+    Returns
+    -------
+    get_graph_youtube : easygraph.Graph
+        The undirected graph instance of Youtube from dataset:
+        http://socialnetworks.mpi-sws.mpg.de/data/youtube-links.txt.gz
+
+    References
+    ----------
+    .. [1] http://socialnetworks.mpi-sws.mpg.de/data/youtube-links.txt.gz
+
+    """
+    import gzip
+
+    from urllib import request
+
+    url = "http://socialnetworks.mpi-sws.mpg.de/data/youtube-links.txt.gz"
+    zipped_data_path = "./samples/youtube-links.txt.gz"
+    unzipped_data_path = "./samples/youtube-links.txt"
+
+    # Download .gz file
+    print("Downloading Youtube dataset...")
+    request.urlretrieve(url, zipped_data_path, _show_progress)
+
+    # Unzip
+    unzipped_data = gzip.GzipFile(zipped_data_path)
+    open(unzipped_data_path, "wb+").write(unzipped_data.read())
+    unzipped_data.close()
+
+    # Returns graph
+    G = eg.Graph()
+    G.add_edges_from_file(file=unzipped_data_path)
+    return G
+
+
+def get_graph_flickr():
+    """Returns the undirected graph of Flickr dataset.
+
+    Returns
+    -------
+    get_graph_flickr : easygraph.Graph
+        The undirected graph instance of Flickr from dataset:
+        http://socialnetworks.mpi-sws.mpg.de/data/flickr-links.txt.gz
+
+    References
+    ----------
+    .. [1] http://socialnetworks.mpi-sws.mpg.de/data/flickr-links.txt.gz
+
+    """
+    import gzip
+
+    from urllib import request
+
+    url = "http://socialnetworks.mpi-sws.mpg.de/data/flickr-links.txt.gz"
+    zipped_data_path = "./samples/flickr-links.txt.gz"
+    unzipped_data_path = "./samples/flickr-links.txt"
+
+    # Download .gz file
+    print("Downloading Flickr dataset...")
+    request.urlretrieve(url, zipped_data_path, _show_progress)
+
+    # Unzip
+    unzipped_data = gzip.GzipFile(zipped_data_path)
+    open(unzipped_data_path, "wb+").write(unzipped_data.read())
+    unzipped_data.close()
+
+    # Returns graph
+    G = eg.Graph()
+    G.add_edges_from_file(file=unzipped_data_path)
+    return G
+
+
+_pbar = None
+
+
+def _show_progress(block_num, block_size, total_size):
+    global _pbar
+    if _pbar is None:
+        _pbar = progressbar.ProgressBar(maxval=total_size)
+        _pbar.start()
+
+    downloaded = block_num * block_size
+    if downloaded < total_size:
+        _pbar.update(downloaded)
+    else:
+        _pbar.finish()
+        _pbar = None
```

## easygraph/datasets/graph_dataset_base.py

 * *Ordering differences only*

```diff
@@ -1,319 +1,319 @@
-"""Basic EasyGraph Dataset
-"""
-
-from __future__ import absolute_import
-
-import abc
-import hashlib
-import os
-import sys
-import traceback
-
-from ..utils import retry_method_with_fix
-from .utils import download
-from .utils import extract_archive
-from .utils import get_download_dir
-from .utils import makedirs
-
-
-class EasyGraphDataset(object):
-    r"""The basic EasyGraph dataset for creating graph datasets.
-    This class defines a basic template class for EasyGraph Dataset.
-    The following steps will be executed automatically:
-
-      1. Check whether there is a dataset cache on disk
-         (already processed and stored on the disk) by
-         invoking ``has_cache()``. If true, goto 5.
-      2. Call ``download()`` to download the data if ``url`` is not None.
-      3. Call ``process()`` to process the data.
-      4. Call ``save()`` to save the processed dataset on disk and goto 6.
-      5. Call ``load()`` to load the processed dataset from disk.
-      6. Done.
-
-    Users can overwrite these functions with their
-    own data processing logic.
-
-    Parameters
-    ----------
-    name : str
-        Name of the dataset
-    url : str
-        Url to download the raw dataset. Default: None
-    raw_dir : str
-        Specifying the directory that will store the
-        downloaded data or the directory that
-        already stores the input data.
-        Default: ~/.EasyGraphData/
-    save_dir : str
-        Directory to save the processed dataset.
-        Default: same as raw_dir
-    hash_key : tuple
-        A tuple of values as the input for the hash function.
-        Users can distinguish instances (and their caches on the disk)
-        from the same dataset class by comparing the hash values.
-        Default: (), the corresponding hash value is ``'f9065fa7'``.
-    force_reload : bool
-        Whether to reload the dataset. Default: False
-    verbose : bool
-        Whether to print out progress information
-    transform : callable, optional
-        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
-        a transformed version. The :class:`~dgl.DGLGraph` object will be
-        transformed before every access.
-
-    """
-
-    def __init__(
-        self,
-        name,
-        url=None,
-        raw_dir=None,
-        save_dir=None,
-        hash_key=(),
-        force_reload=False,
-        verbose=False,
-        transform=None,
-    ):
-        self._name = name
-        self._url = url
-        self._force_reload = force_reload
-        self._verbose = verbose
-        self._hash_key = hash_key
-        self._hash = self._get_hash()
-        self._transform = transform
-
-        # if no dir is provided, the default EasyGraph download dir is used.
-        if raw_dir is None:
-            self._raw_dir = get_download_dir()
-        else:
-            self._raw_dir = raw_dir
-
-        if save_dir is None:
-            self._save_dir = self._raw_dir
-        else:
-            self._save_dir = save_dir
-        self._load()
-
-    def download(self):
-        r"""Overwrite to realize your own logic of downloading data.
-
-        It is recommended to download the to the :obj:`self.raw_dir`
-        folder. Can be ignored if the dataset is
-        already in :obj:`self.raw_dir`.
-        """
-        pass
-
-    def save(self):
-        r"""Overwrite to realize your own logic of
-        saving the processed dataset into files.
-
-        It is recommended to use ``dgl.data.utils.save_graphs``
-        to save dgl graph into files and use
-        ``dgl.data.utils.save_info`` to save extra
-        information into files.
-        """
-        pass
-
-    def load(self):
-        r"""Overwrite to realize your own logic of
-        loading the saved dataset from files.
-
-        It is recommended to use ``dgl.data.utils.load_graphs``
-        to load dgl graph from files and use
-        ``dgl.data.utils.load_info`` to load extra information
-        into python dict object.
-        """
-        pass
-
-    @abc.abstractmethod
-    def process(self):
-        r"""Overwrite to realize your own logic of processing the input data."""
-        pass
-
-    def has_cache(self):
-        r"""Overwrite to realize your own logic of
-        deciding whether there exists a cached dataset.
-
-        By default False.
-        """
-        return False
-
-    @retry_method_with_fix(download)
-    def _download(self):
-        """Download dataset by calling ``self.download()``
-        if the dataset does not exists under ``self.raw_path``.
-
-        By default ``self.raw_path = os.path.join(self.raw_dir, self.name)``
-        One can overwrite ``raw_path()`` function to change the path.
-        """
-
-        if os.path.exists(self.raw_path):  # pragma: no cover
-            return
-
-        makedirs(self.raw_dir)
-        self.download()
-
-    def _load(self):
-        """Entry point from __init__ to load the dataset.
-
-        If cache exists:
-
-          - Load the dataset from saved dgl graph and information files.
-          - If loading process fails, re-download and process the dataset.
-
-        else:
-
-          - Download the dataset if needed.
-          - Process the dataset and build the dgl graph.
-          - Save the processed dataset into files.
-        """
-
-        load_flag = not self._force_reload and self.has_cache()
-        if load_flag:
-            try:
-                self.load()
-                self.process()
-                if self.verbose:
-                    print("Done loading data from cached files.")
-            except KeyboardInterrupt:
-                raise
-            except:
-                load_flag = False
-                if self.verbose:
-                    print(traceback.format_exc())
-                    print("Loading from cache failed, re-processing.")
-
-        if not load_flag:
-            self._download()
-            self.process()
-            self.save()
-            if self.verbose:
-                print("Done saving data into cached files.")
-
-    def _get_hash(self):
-        """Compute the hash of the input tuple
-
-        Example
-        -------
-        Assume `self._hash_key = (10, False, True)`
-
-        >>> hash_value = self._get_hash()
-        >>> hash_value
-        'a770b222'
-        """
-        hash_func = hashlib.sha1()
-        hash_func.update(str(self._hash_key).encode("utf-8"))
-        return hash_func.hexdigest()[:8]
-
-    @property
-    def url(self):
-        r"""Get url to download the raw dataset."""
-        return self._url
-
-    @property
-    def name(self):
-        r"""Name of the dataset."""
-        return self._name
-
-    @property
-    def raw_dir(self):
-        r"""Raw file directory contains the input data folder."""
-        return self._raw_dir
-
-    @property
-    def raw_path(self):
-        r"""Directory contains the input data files.
-        By default raw_path = os.path.join(self.raw_dir, self.name)
-        """
-        return os.path.join(self.raw_dir, self.name)
-
-    @property
-    def save_dir(self):
-        r"""Directory to save the processed dataset."""
-        return self._save_dir
-
-    @property
-    def save_path(self):
-        r"""Path to save the processed dataset."""
-        return os.path.join(self._save_dir)
-
-    @property
-    def verbose(self):
-        r"""Whether to print information."""
-        return self._verbose
-
-    @property
-    def hash(self):
-        r"""Hash value for the dataset and the setting."""
-        return self._hash
-
-    @abc.abstractmethod
-    def __getitem__(self, idx):
-        r"""Gets the data object at index."""
-        pass
-
-    @abc.abstractmethod
-    def __len__(self):
-        r"""The number of examples in the dataset."""
-        pass
-
-    def __repr__(self):
-        return f'Dataset("{self.name}"' + f" save_path={self.save_path})"
-
-
-class EasyGraphBuiltinDataset(EasyGraphDataset):
-    r"""The Basic EasyGraph Builtin Dataset.
-
-    Parameters
-    ----------
-    name : str
-        Name of the dataset.
-    url : str
-        Url to download the raw dataset.
-    raw_dir : str
-        Specifying the directory that will store the
-        downloaded data or the directory that
-        already stores the input data.
-        Default: ~/.dgl/
-    hash_key : tuple
-        A tuple of values as the input for the hash function.
-        Users can distinguish instances (and their caches on the disk)
-        from the same dataset class by comparing the hash values.
-    force_reload : bool
-        Whether to reload the dataset. Default: False
-    verbose : bool
-        Whether to print out progress information. Default: False
-    transform : callable, optional
-        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
-        a transformed version. The :class:`~dgl.DGLGraph` object will be
-        transformed before every access.
-    """
-
-    def __init__(
-        self,
-        name,
-        url,
-        raw_dir=None,
-        hash_key=(),
-        force_reload=False,
-        verbose=True,
-        transform=None,
-        save_dir=None,
-    ):
-        super(EasyGraphBuiltinDataset, self).__init__(
-            name,
-            url=url,
-            raw_dir=raw_dir,
-            save_dir=save_dir,
-            hash_key=hash_key,
-            force_reload=force_reload,
-            verbose=verbose,
-            transform=transform,
-        )
-
-    def download(self):
-        r"""Automatically download data and extract it."""
-        if self.url is not None:
-            zip_file_path = os.path.join(self.raw_dir, self.name + ".zip")
-            download(self.url, path=zip_file_path)
-            extract_archive(zip_file_path, self.raw_path)
+"""Basic EasyGraph Dataset
+"""
+
+from __future__ import absolute_import
+
+import abc
+import hashlib
+import os
+import sys
+import traceback
+
+from ..utils import retry_method_with_fix
+from .utils import download
+from .utils import extract_archive
+from .utils import get_download_dir
+from .utils import makedirs
+
+
+class EasyGraphDataset(object):
+    r"""The basic EasyGraph dataset for creating graph datasets.
+    This class defines a basic template class for EasyGraph Dataset.
+    The following steps will be executed automatically:
+
+      1. Check whether there is a dataset cache on disk
+         (already processed and stored on the disk) by
+         invoking ``has_cache()``. If true, goto 5.
+      2. Call ``download()`` to download the data if ``url`` is not None.
+      3. Call ``process()`` to process the data.
+      4. Call ``save()`` to save the processed dataset on disk and goto 6.
+      5. Call ``load()`` to load the processed dataset from disk.
+      6. Done.
+
+    Users can overwrite these functions with their
+    own data processing logic.
+
+    Parameters
+    ----------
+    name : str
+        Name of the dataset
+    url : str
+        Url to download the raw dataset. Default: None
+    raw_dir : str
+        Specifying the directory that will store the
+        downloaded data or the directory that
+        already stores the input data.
+        Default: ~/.EasyGraphData/
+    save_dir : str
+        Directory to save the processed dataset.
+        Default: same as raw_dir
+    hash_key : tuple
+        A tuple of values as the input for the hash function.
+        Users can distinguish instances (and their caches on the disk)
+        from the same dataset class by comparing the hash values.
+        Default: (), the corresponding hash value is ``'f9065fa7'``.
+    force_reload : bool
+        Whether to reload the dataset. Default: False
+    verbose : bool
+        Whether to print out progress information
+    transform : callable, optional
+        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
+        a transformed version. The :class:`~dgl.DGLGraph` object will be
+        transformed before every access.
+
+    """
+
+    def __init__(
+        self,
+        name,
+        url=None,
+        raw_dir=None,
+        save_dir=None,
+        hash_key=(),
+        force_reload=False,
+        verbose=False,
+        transform=None,
+    ):
+        self._name = name
+        self._url = url
+        self._force_reload = force_reload
+        self._verbose = verbose
+        self._hash_key = hash_key
+        self._hash = self._get_hash()
+        self._transform = transform
+
+        # if no dir is provided, the default EasyGraph download dir is used.
+        if raw_dir is None:
+            self._raw_dir = get_download_dir()
+        else:
+            self._raw_dir = raw_dir
+
+        if save_dir is None:
+            self._save_dir = self._raw_dir
+        else:
+            self._save_dir = save_dir
+        self._load()
+
+    def download(self):
+        r"""Overwrite to realize your own logic of downloading data.
+
+        It is recommended to download the to the :obj:`self.raw_dir`
+        folder. Can be ignored if the dataset is
+        already in :obj:`self.raw_dir`.
+        """
+        pass
+
+    def save(self):
+        r"""Overwrite to realize your own logic of
+        saving the processed dataset into files.
+
+        It is recommended to use ``dgl.data.utils.save_graphs``
+        to save dgl graph into files and use
+        ``dgl.data.utils.save_info`` to save extra
+        information into files.
+        """
+        pass
+
+    def load(self):
+        r"""Overwrite to realize your own logic of
+        loading the saved dataset from files.
+
+        It is recommended to use ``dgl.data.utils.load_graphs``
+        to load dgl graph from files and use
+        ``dgl.data.utils.load_info`` to load extra information
+        into python dict object.
+        """
+        pass
+
+    @abc.abstractmethod
+    def process(self):
+        r"""Overwrite to realize your own logic of processing the input data."""
+        pass
+
+    def has_cache(self):
+        r"""Overwrite to realize your own logic of
+        deciding whether there exists a cached dataset.
+
+        By default False.
+        """
+        return False
+
+    @retry_method_with_fix(download)
+    def _download(self):
+        """Download dataset by calling ``self.download()``
+        if the dataset does not exists under ``self.raw_path``.
+
+        By default ``self.raw_path = os.path.join(self.raw_dir, self.name)``
+        One can overwrite ``raw_path()`` function to change the path.
+        """
+
+        if os.path.exists(self.raw_path):  # pragma: no cover
+            return
+
+        makedirs(self.raw_dir)
+        self.download()
+
+    def _load(self):
+        """Entry point from __init__ to load the dataset.
+
+        If cache exists:
+
+          - Load the dataset from saved dgl graph and information files.
+          - If loading process fails, re-download and process the dataset.
+
+        else:
+
+          - Download the dataset if needed.
+          - Process the dataset and build the dgl graph.
+          - Save the processed dataset into files.
+        """
+
+        load_flag = not self._force_reload and self.has_cache()
+        if load_flag:
+            try:
+                self.load()
+                self.process()
+                if self.verbose:
+                    print("Done loading data from cached files.")
+            except KeyboardInterrupt:
+                raise
+            except:
+                load_flag = False
+                if self.verbose:
+                    print(traceback.format_exc())
+                    print("Loading from cache failed, re-processing.")
+
+        if not load_flag:
+            self._download()
+            self.process()
+            self.save()
+            if self.verbose:
+                print("Done saving data into cached files.")
+
+    def _get_hash(self):
+        """Compute the hash of the input tuple
+
+        Example
+        -------
+        Assume `self._hash_key = (10, False, True)`
+
+        >>> hash_value = self._get_hash()
+        >>> hash_value
+        'a770b222'
+        """
+        hash_func = hashlib.sha1()
+        hash_func.update(str(self._hash_key).encode("utf-8"))
+        return hash_func.hexdigest()[:8]
+
+    @property
+    def url(self):
+        r"""Get url to download the raw dataset."""
+        return self._url
+
+    @property
+    def name(self):
+        r"""Name of the dataset."""
+        return self._name
+
+    @property
+    def raw_dir(self):
+        r"""Raw file directory contains the input data folder."""
+        return self._raw_dir
+
+    @property
+    def raw_path(self):
+        r"""Directory contains the input data files.
+        By default raw_path = os.path.join(self.raw_dir, self.name)
+        """
+        return os.path.join(self.raw_dir, self.name)
+
+    @property
+    def save_dir(self):
+        r"""Directory to save the processed dataset."""
+        return self._save_dir
+
+    @property
+    def save_path(self):
+        r"""Path to save the processed dataset."""
+        return os.path.join(self._save_dir)
+
+    @property
+    def verbose(self):
+        r"""Whether to print information."""
+        return self._verbose
+
+    @property
+    def hash(self):
+        r"""Hash value for the dataset and the setting."""
+        return self._hash
+
+    @abc.abstractmethod
+    def __getitem__(self, idx):
+        r"""Gets the data object at index."""
+        pass
+
+    @abc.abstractmethod
+    def __len__(self):
+        r"""The number of examples in the dataset."""
+        pass
+
+    def __repr__(self):
+        return f'Dataset("{self.name}"' + f" save_path={self.save_path})"
+
+
+class EasyGraphBuiltinDataset(EasyGraphDataset):
+    r"""The Basic EasyGraph Builtin Dataset.
+
+    Parameters
+    ----------
+    name : str
+        Name of the dataset.
+    url : str
+        Url to download the raw dataset.
+    raw_dir : str
+        Specifying the directory that will store the
+        downloaded data or the directory that
+        already stores the input data.
+        Default: ~/.dgl/
+    hash_key : tuple
+        A tuple of values as the input for the hash function.
+        Users can distinguish instances (and their caches on the disk)
+        from the same dataset class by comparing the hash values.
+    force_reload : bool
+        Whether to reload the dataset. Default: False
+    verbose : bool
+        Whether to print out progress information. Default: False
+    transform : callable, optional
+        A transform that takes in a :class:`~dgl.DGLGraph` object and returns
+        a transformed version. The :class:`~dgl.DGLGraph` object will be
+        transformed before every access.
+    """
+
+    def __init__(
+        self,
+        name,
+        url,
+        raw_dir=None,
+        hash_key=(),
+        force_reload=False,
+        verbose=True,
+        transform=None,
+        save_dir=None,
+    ):
+        super(EasyGraphBuiltinDataset, self).__init__(
+            name,
+            url=url,
+            raw_dir=raw_dir,
+            save_dir=save_dir,
+            hash_key=hash_key,
+            force_reload=force_reload,
+            verbose=verbose,
+            transform=transform,
+        )
+
+    def download(self):
+        r"""Automatically download data and extract it."""
+        if self.url is not None:
+            zip_file_path = os.path.join(self.raw_dir, self.name + ".zip")
+            download(self.url, path=zip_file_path)
+            extract_archive(zip_file_path, self.raw_path)
```

## easygraph/datasets/hypergraph/House_Committees.py

 * *Ordering differences only*

```diff
@@ -1,112 +1,112 @@
-import requests
-
-from easygraph.utils.exception import EasyGraphError
-
-
-def request_text_from_url(url):
-    try:
-        r = requests.get(url)
-    except requests.ConnectionError:
-        raise EasyGraphError("Connection Error!")
-
-    if r.ok:
-        return r.text
-    else:
-        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
-
-
-class House_Committees:
-    def __init__(self, data_root=None):
-        self.data_root = "https://" if data_root is not None else data_root
-        self.hyperedges_path = "https://gitlab.com/easy-graph/easygraph-data-house-committees/-/raw/main/hyperedges-house-committees.txt?inline=false"
-        self.node_labels_path = "https://gitlab.com/easy-graph/easygraph-data-house-committees/-/raw/main/node-labels-house-committees.txt?ref_type=heads&inline=false"
-        self.node_names_path = "https://gitlab.com/easy-graph/easygraph-data-house-committees/-/raw/main/node-names-house-committees.txt?ref_type=heads&inline=false"
-        self.label_names_path = "https://gitlab.com/easy-graph/easygraph-data-house-committees/-/raw/main/label-names-house-committees.txt?ref_type=heads&inline=false"
-        self._hyperedges = []
-        self._node_labels = []
-        self._label_names = []
-        self._node_names = []
-        self.generate_hypergraph(
-            hyperedges_path=self.hyperedges_path,
-            node_labels_path=self.node_labels_path,
-            node_names_path=self.node_names_path,
-            label_names_path=self.label_names_path,
-        )
-
-        self._content = {
-            "num_classes": len(self._label_names),
-            "num_vertices": len(self._node_labels),
-            "num_edges": len(self._hyperedges),
-            "edge_list": self._hyperedges,
-            "labels": self._node_labels,
-        }
-
-    def process_label_txt(self, data_str, delimiter="\n", transform_fun=str):
-        data_str = data_str.strip()
-        data_lst = data_str.split(delimiter)
-        final_lst = []
-        for data in data_lst:
-            data = data.strip()
-            data = transform_fun(data)
-            final_lst.append(data)
-        return final_lst
-
-    @property
-    def node_labels(self):
-        return self._node_labels
-
-    @property
-    def node_names(self):
-        return self._node_names
-
-    @property
-    def label_names(self):
-        return self._label_names
-
-    @property
-    def hyperedges(self):
-        return self._hyperedges
-
-    def generate_hypergraph(
-        self,
-        hyperedges_path=None,
-        node_labels_path=None,
-        node_names_path=None,
-        label_names_path=None,
-    ):
-        def fun(data):
-            data = int(data) - 1
-            return data
-
-        hyperedges_info = request_text_from_url(hyperedges_path)
-        hyperedges_info = hyperedges_info.strip()
-        hyperedges_lst = hyperedges_info.split("\n")
-        for hyperedge in hyperedges_lst:
-            hyperedge = hyperedge.strip()
-            hyperedge = [int(i) - 1 for i in hyperedge.split(",")]
-            self._hyperedges.append(tuple(hyperedge))
-        # print(self.hyperedges)
-
-        node_labels_info = request_text_from_url(node_labels_path)
-
-        process_node_labels_info = self.process_label_txt(
-            node_labels_info, transform_fun=fun
-        )
-        self._node_labels = process_node_labels_info
-        # print("process_node_labels_info:", process_node_labels_info)
-        node_names_info = request_text_from_url(node_names_path)
-        process_node_names_info = self.process_label_txt(node_names_info)
-        self._node_names = process_node_names_info
-        # print("process_node_names_info:", process_node_names_info)
-        label_names_info = request_text_from_url(label_names_path)
-        process_label_names_info = self.process_label_txt(label_names_info)
-        self._label_names = process_label_names_info
-        # print("process_label_names_info:", process_label_names_info)
-
-
-#
-# if __name__ == "__main__":
-#     a = House_Committees()
-#     print(a.node_labels)
-#     print(a.label_names)
-#     print(a.node_names)
+import requests
+
+from easygraph.utils.exception import EasyGraphError
+
+
+def request_text_from_url(url):
+    try:
+        r = requests.get(url)
+    except requests.ConnectionError:
+        raise EasyGraphError("Connection Error!")
+
+    if r.ok:
+        return r.text
+    else:
+        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
+
+
+class House_Committees:
+    def __init__(self, data_root=None):
+        self.data_root = "https://" if data_root is not None else data_root
+        self.hyperedges_path = "https://gitlab.com/easy-graph/easygraph-data-house-committees/-/raw/main/hyperedges-house-committees.txt?inline=false"
+        self.node_labels_path = "https://gitlab.com/easy-graph/easygraph-data-house-committees/-/raw/main/node-labels-house-committees.txt?ref_type=heads&inline=false"
+        self.node_names_path = "https://gitlab.com/easy-graph/easygraph-data-house-committees/-/raw/main/node-names-house-committees.txt?ref_type=heads&inline=false"
+        self.label_names_path = "https://gitlab.com/easy-graph/easygraph-data-house-committees/-/raw/main/label-names-house-committees.txt?ref_type=heads&inline=false"
+        self._hyperedges = []
+        self._node_labels = []
+        self._label_names = []
+        self._node_names = []
+        self.generate_hypergraph(
+            hyperedges_path=self.hyperedges_path,
+            node_labels_path=self.node_labels_path,
+            node_names_path=self.node_names_path,
+            label_names_path=self.label_names_path,
+        )
+
+        self._content = {
+            "num_classes": len(self._label_names),
+            "num_vertices": len(self._node_labels),
+            "num_edges": len(self._hyperedges),
+            "edge_list": self._hyperedges,
+            "labels": self._node_labels,
+        }
+
+    def process_label_txt(self, data_str, delimiter="\n", transform_fun=str):
+        data_str = data_str.strip()
+        data_lst = data_str.split(delimiter)
+        final_lst = []
+        for data in data_lst:
+            data = data.strip()
+            data = transform_fun(data)
+            final_lst.append(data)
+        return final_lst
+
+    @property
+    def node_labels(self):
+        return self._node_labels
+
+    @property
+    def node_names(self):
+        return self._node_names
+
+    @property
+    def label_names(self):
+        return self._label_names
+
+    @property
+    def hyperedges(self):
+        return self._hyperedges
+
+    def generate_hypergraph(
+        self,
+        hyperedges_path=None,
+        node_labels_path=None,
+        node_names_path=None,
+        label_names_path=None,
+    ):
+        def fun(data):
+            data = int(data) - 1
+            return data
+
+        hyperedges_info = request_text_from_url(hyperedges_path)
+        hyperedges_info = hyperedges_info.strip()
+        hyperedges_lst = hyperedges_info.split("\n")
+        for hyperedge in hyperedges_lst:
+            hyperedge = hyperedge.strip()
+            hyperedge = [int(i) - 1 for i in hyperedge.split(",")]
+            self._hyperedges.append(tuple(hyperedge))
+        # print(self.hyperedges)
+
+        node_labels_info = request_text_from_url(node_labels_path)
+
+        process_node_labels_info = self.process_label_txt(
+            node_labels_info, transform_fun=fun
+        )
+        self._node_labels = process_node_labels_info
+        # print("process_node_labels_info:", process_node_labels_info)
+        node_names_info = request_text_from_url(node_names_path)
+        process_node_names_info = self.process_label_txt(node_names_info)
+        self._node_names = process_node_names_info
+        # print("process_node_names_info:", process_node_names_info)
+        label_names_info = request_text_from_url(label_names_path)
+        process_label_names_info = self.process_label_txt(label_names_info)
+        self._label_names = process_label_names_info
+        # print("process_label_names_info:", process_label_names_info)
+
+
+#
+# if __name__ == "__main__":
+#     a = House_Committees()
+#     print(a.node_labels)
+#     print(a.label_names)
+#     print(a.node_names)
```

## easygraph/datasets/hypergraph/_global.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-from pathlib import Path
-
-
-def get_eg_cache_root():
-    root = Path.home() / Path(".easygraph/")
-    root.mkdir(parents=True, exist_ok=True)
-    return root
-
-
-CACHE_ROOT = get_eg_cache_root()
-DATASETS_ROOT = CACHE_ROOT / "datasets"
-
-REMOTE_ROOT = "https://download.moon-lab.tech:28501/"
-REMOTE_DATASETS_ROOT = REMOTE_ROOT + "datasets/"
+from pathlib import Path
+
+
+def get_eg_cache_root():
+    root = Path.home() / Path(".easygraph/")
+    root.mkdir(parents=True, exist_ok=True)
+    return root
+
+
+CACHE_ROOT = get_eg_cache_root()
+DATASETS_ROOT = CACHE_ROOT / "datasets"
+
+REMOTE_ROOT = "https://download.moon-lab.tech:28501/"
+REMOTE_DATASETS_ROOT = REMOTE_ROOT + "datasets/"
```

## easygraph/datasets/hypergraph/senate_committees.py

 * *Ordering differences only*

```diff
@@ -1,106 +1,106 @@
-import requests
-
-from easygraph.utils.exception import EasyGraphError
-
-
-def request_text_from_url(url):
-    try:
-        r = requests.get(url)
-    except requests.ConnectionError:
-        raise EasyGraphError("Connection Error!")
-
-    if r.ok:
-        return r.text
-    else:
-        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
-
-
-class senate_committees:
-    def __init__(self, data_root=None):
-        self.data_root = "https://" if data_root is not None else data_root
-        self.hyperedges_path = "https://gitlab.com/easy-graph/easygraph-data-senate-committees/-/raw/main/hyperedges-senate-committees.txt?inline=false"
-        self.node_labels_path = "https://gitlab.com/easy-graph/easygraph-data-senate-committees/-/raw/main/node-labels-senate-committees.txt?ref_type=heads&inline=false"
-        self.node_names_path = "https://gitlab.com/easy-graph/easygraph-data-senate-committees/-/raw/main/node-names-senate-committees.txt?ref_type=heads&inline=false"
-        self.label_names_path = "https://gitlab.com/easy-graph/easygraph-data-senate-committees/-/raw/main/label-names-senate-committees.txt?ref_type=heads&inline=false"
-        self._hyperedges = []
-        self._node_labels = []
-        self._label_names = []
-        self._node_names = []
-        self.generate_hypergraph(
-            hyperedges_path=self.hyperedges_path,
-            node_labels_path=self.node_labels_path,
-            node_names_path=self.node_names_path,
-            label_names_path=self.label_names_path,
-        )
-        self._content = {
-            "num_classes": len(self._label_names),
-            "num_vertices": len(self._node_labels),
-            "num_edges": len(self._hyperedges),
-            "edge_list": self._hyperedges,
-            "labels": self._node_labels,
-        }
-
-    def __getitem__(self, key: str):
-        return self._content[key]
-
-    def process_label_txt(self, data_str, delimiter="\n", transform_fun=str):
-        data_str = data_str.strip()
-        data_lst = data_str.split(delimiter)
-        final_lst = []
-        for data in data_lst:
-            data = data.strip()
-            data = transform_fun(data)
-            final_lst.append(data)
-        return final_lst
-
-    @property
-    def node_labels(self):
-        return self._node_labels
-
-    @property
-    def node_names(self):
-        return self._node_names
-
-    @property
-    def label_names(self):
-        return self._label_names
-
-    @property
-    def hyperedges(self):
-        return self._hyperedges
-
-    def generate_hypergraph(
-        self,
-        hyperedges_path=None,
-        node_labels_path=None,
-        node_names_path=None,
-        label_names_path=None,
-    ):
-        def fun(data):
-            data = int(data) - 1
-            return data
-
-        hyperedges_info = request_text_from_url(hyperedges_path)
-        hyperedges_info = hyperedges_info.strip()
-        hyperedges_lst = hyperedges_info.split("\n")
-        for hyperedge in hyperedges_lst:
-            hyperedge = hyperedge.strip()
-            hyperedge = [int(i) - 1 for i in hyperedge.split(",")]
-            self._hyperedges.append(tuple(hyperedge))
-        # print(self.hyperedges)
-
-        node_labels_info = request_text_from_url(node_labels_path)
-
-        process_node_labels_info = self.process_label_txt(
-            node_labels_info, transform_fun=fun
-        )
-        self._node_labels = process_node_labels_info
-        # print("process_node_labels_info:", process_node_labels_info)
-        node_names_info = request_text_from_url(node_names_path)
-        process_node_names_info = self.process_label_txt(node_names_info)
-        self._node_names = process_node_names_info
-        # print("process_node_names_info:", process_node_names_info)
-        label_names_info = request_text_from_url(label_names_path)
-        process_label_names_info = self.process_label_txt(label_names_info)
-        self._label_names = process_label_names_info
-        # print("process_label_names_info:", process_label_names_info)
+import requests
+
+from easygraph.utils.exception import EasyGraphError
+
+
+def request_text_from_url(url):
+    try:
+        r = requests.get(url)
+    except requests.ConnectionError:
+        raise EasyGraphError("Connection Error!")
+
+    if r.ok:
+        return r.text
+    else:
+        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
+
+
+class senate_committees:
+    def __init__(self, data_root=None):
+        self.data_root = "https://" if data_root is not None else data_root
+        self.hyperedges_path = "https://gitlab.com/easy-graph/easygraph-data-senate-committees/-/raw/main/hyperedges-senate-committees.txt?inline=false"
+        self.node_labels_path = "https://gitlab.com/easy-graph/easygraph-data-senate-committees/-/raw/main/node-labels-senate-committees.txt?ref_type=heads&inline=false"
+        self.node_names_path = "https://gitlab.com/easy-graph/easygraph-data-senate-committees/-/raw/main/node-names-senate-committees.txt?ref_type=heads&inline=false"
+        self.label_names_path = "https://gitlab.com/easy-graph/easygraph-data-senate-committees/-/raw/main/label-names-senate-committees.txt?ref_type=heads&inline=false"
+        self._hyperedges = []
+        self._node_labels = []
+        self._label_names = []
+        self._node_names = []
+        self.generate_hypergraph(
+            hyperedges_path=self.hyperedges_path,
+            node_labels_path=self.node_labels_path,
+            node_names_path=self.node_names_path,
+            label_names_path=self.label_names_path,
+        )
+        self._content = {
+            "num_classes": len(self._label_names),
+            "num_vertices": len(self._node_labels),
+            "num_edges": len(self._hyperedges),
+            "edge_list": self._hyperedges,
+            "labels": self._node_labels,
+        }
+
+    def __getitem__(self, key: str):
+        return self._content[key]
+
+    def process_label_txt(self, data_str, delimiter="\n", transform_fun=str):
+        data_str = data_str.strip()
+        data_lst = data_str.split(delimiter)
+        final_lst = []
+        for data in data_lst:
+            data = data.strip()
+            data = transform_fun(data)
+            final_lst.append(data)
+        return final_lst
+
+    @property
+    def node_labels(self):
+        return self._node_labels
+
+    @property
+    def node_names(self):
+        return self._node_names
+
+    @property
+    def label_names(self):
+        return self._label_names
+
+    @property
+    def hyperedges(self):
+        return self._hyperedges
+
+    def generate_hypergraph(
+        self,
+        hyperedges_path=None,
+        node_labels_path=None,
+        node_names_path=None,
+        label_names_path=None,
+    ):
+        def fun(data):
+            data = int(data) - 1
+            return data
+
+        hyperedges_info = request_text_from_url(hyperedges_path)
+        hyperedges_info = hyperedges_info.strip()
+        hyperedges_lst = hyperedges_info.split("\n")
+        for hyperedge in hyperedges_lst:
+            hyperedge = hyperedge.strip()
+            hyperedge = [int(i) - 1 for i in hyperedge.split(",")]
+            self._hyperedges.append(tuple(hyperedge))
+        # print(self.hyperedges)
+
+        node_labels_info = request_text_from_url(node_labels_path)
+
+        process_node_labels_info = self.process_label_txt(
+            node_labels_info, transform_fun=fun
+        )
+        self._node_labels = process_node_labels_info
+        # print("process_node_labels_info:", process_node_labels_info)
+        node_names_info = request_text_from_url(node_names_path)
+        process_node_names_info = self.process_label_txt(node_names_info)
+        self._node_names = process_node_names_info
+        # print("process_node_names_info:", process_node_names_info)
+        label_names_info = request_text_from_url(label_names_path)
+        process_label_names_info = self.process_label_txt(label_names_info)
+        self._label_names = process_label_names_info
+        # print("process_label_names_info:", process_label_names_info)
```

## easygraph/datasets/hypergraph/cocitation.py

 * *Ordering differences only*

```diff
@@ -1,279 +1,279 @@
-from functools import partial
-from typing import Optional
-
-from easygraph.datapipe import load_from_pickle
-from easygraph.datapipe import norm_ft
-from easygraph.datapipe import to_bool_tensor
-from easygraph.datapipe import to_long_tensor
-from easygraph.datapipe import to_tensor
-from easygraph.datasets.hypergraph.hypergraph_dataset_base import BaseData
-
-
-class CocitationCora(BaseData):
-    r"""The Co-citation Cora dataset is a citation network dataset for vertex classification task.
-    More details see the `HyperGCN <https://papers.nips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf>`_ paper.
-
-    The content of the Co-citation Cora dataset includes the following:
-
-    - ``num_classes``: The number of classes: :math:`7`.
-    - ``num_vertices``: The number of vertices: :math:`2,708`.
-    - ``num_edges``: The number of edges: :math:`1,579`.
-    - ``dim_features``: The dimension of features: :math:`1,433`.
-    - ``features``: The vertex feature matrix. ``torch.Tensor`` with size :math:`(2,708 \times 1,433)`.
-    - ``edge_list``: The edge list. ``List`` with length :math:`1,579`.
-    - ``labels``: The label list. ``torch.LongTensor`` with size :math:`(2,708, )`.
-    - ``train_mask``: The train mask. ``torch.BoolTensor`` with size :math:`(2,708, )`.
-    - ``val_mask``: The validation mask. ``torch.BoolTensor`` with size :math:`(2,708, )`.
-    - ``test_mask``: The test mask. ``torch.BoolTensor`` with size :math:`(2,708, )`.
-
-    Args:
-        ``data_root`` (``str``, optional): The ``data_root`` has stored the data. If set to ``None``, this function will auto-download from server and save into the default direction ``~/.dhg/datasets/``. Defaults to ``None``.
-    """
-
-    def __init__(self, data_root: Optional[str] = None) -> None:
-        super().__init__("cocitation_cora", data_root)
-        self._content = {
-            "num_classes": 7,
-            "num_vertices": 2708,
-            "num_edges": 1579,
-            "dim_features": 1433,
-            "features": {
-                "upon": [
-                    {
-                        "filename": "features.pkl",
-                        "md5": "14257c0e24b4eb741b469a351e524785",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_tensor, partial(norm_ft, ord=1)],
-            },
-            "edge_list": {
-                "upon": [
-                    {
-                        "filename": "edge_list.pkl",
-                        "md5": "e43d1321880c8ecb2260d8fb7effd9ea",
-                    }
-                ],
-                "loader": load_from_pickle,
-            },
-            "labels": {
-                "upon": [
-                    {
-                        "filename": "labels.pkl",
-                        "md5": "c8d11c452e0be69f79a47dd839279117",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_long_tensor],
-            },
-            "train_mask": {
-                "upon": [
-                    {
-                        "filename": "train_mask.pkl",
-                        "md5": "111db6c6f986be2908378df7bdca7a9b",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-            "val_mask": {
-                "upon": [
-                    {
-                        "filename": "val_mask.pkl",
-                        "md5": "ffab1055193ffb2fe74822bb575d332a",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-            "test_mask": {
-                "upon": [
-                    {
-                        "filename": "test_mask.pkl",
-                        "md5": "ffab1055193ffb2fe74822bb575d332a",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-        }
-
-
-class CocitationCiteseer(BaseData):
-    r"""The Co-citation Citeseer dataset is a citation network dataset for vertex classification task.
-    More details see the `HyperGCN <https://papers.nips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf>`_ paper.
-
-    The content of the Co-citation Citaseer dataset includes the following:
-
-    - ``num_classes``: The number of classes: :math:`6`.
-    - ``num_vertices``: The number of vertices: :math:`3,327`.
-    - ``num_edges``: The number of edges: :math:`1,079`.
-    - ``dim_features``: The dimension of features: :math:`3,703`.
-    - ``features``: The vertex feature matrix. ``torch.Tensor`` with size :math:`(3,327 \times 3,703)`.
-    - ``edge_list``: The edge list. ``List`` with length :math:`1,079`.
-    - ``labels``: The label list. ``torch.LongTensor`` with size :math:`(3,327, )`.
-    - ``train_mask``: The train mask. ``torch.BoolTensor`` with size :math:`(3,327, )`.
-    - ``val_mask``: The validation mask. ``torch.BoolTensor`` with size :math:`(3,327, )`.
-    - ``test_mask``: The test mask. ``torch.BoolTensor`` with size :math:`(3,327, )`.
-
-    Args:
-        ``data_root`` (``str``, optional): The ``data_root`` has stored the data. If set to ``None``, this function will auto-download from server and save into the default direction ``~/.dhg/datasets/``. Defaults to ``None``.
-    """
-
-    def __init__(self, data_root: Optional[str] = None) -> None:
-        super().__init__("cocitation_citeseer", data_root)
-        self._content = {
-            "num_classes": 6,
-            "num_vertices": 3312,
-            "num_edges": 1079,
-            "dim_features": 3703,
-            "features": {
-                "upon": [
-                    {
-                        "filename": "features.pkl",
-                        "md5": "1ee0dc89e0d5f5ac9187b55a407683e8",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_tensor, partial(norm_ft, ord=1)],
-            },
-            "edge_list": {
-                "upon": [
-                    {
-                        "filename": "edge_list.pkl",
-                        "md5": "6687b2e96159c534a424253f536b49ae",
-                    }
-                ],
-                "loader": load_from_pickle,
-            },
-            "labels": {
-                "upon": [
-                    {
-                        "filename": "labels.pkl",
-                        "md5": "71069f78e83fa85dd6a4b9b6570447c2",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_long_tensor],
-            },
-            "train_mask": {
-                "upon": [
-                    {
-                        "filename": "train_mask.pkl",
-                        "md5": "3b831318fc3d3e588bead5ba469fe38f",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-            "val_mask": {
-                "upon": [
-                    {
-                        "filename": "val_mask.pkl",
-                        "md5": "c22eb5b7493908042c7e039c8bb5a82e",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-            "test_mask": {
-                "upon": [
-                    {
-                        "filename": "test_mask.pkl",
-                        "md5": "c22eb5b7493908042c7e039c8bb5a82e",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-        }
-
-
-class CocitationPubmed(BaseData):
-    r"""The Co-citation PubMed dataset is a citation network dataset for vertex classification task.
-    More details see the `HyperGCN <https://papers.nips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf>`_ paper.
-
-    The content of the Co-citation PubMed dataset includes the following:
-
-    - ``num_classes``: The number of classes: :math:`3`.
-    - ``num_vertices``: The number of vertices: :math:`19,717`.
-    - ``num_edges``: The number of edges: :math:`7,963`.
-    - ``dim_features``: The dimension of features: :math:`500`.
-    - ``features``: The vertex feature matrix. ``torch.Tensor`` with size :math:`(19,717 \times 500)`.
-    - ``edge_list``: The edge list. ``List`` with length :math:`7,963`.
-    - ``labels``: The label list. ``torch.LongTensor`` with size :math:`(19,717, )`.
-    - ``train_mask``: The train mask. ``torch.BoolTensor`` with size :math:`(19,717, )`.
-    - ``val_mask``: The validation mask. ``torch.BoolTensor`` with size :math:`(19,717, )`.
-    - ``test_mask``: The test mask. ``torch.BoolTensor`` with size :math:`(19,717, )`.
-
-    Args:
-        ``data_root`` (``str``, optional): The ``data_root`` has stored the data. If set to ``None``, this function will auto-download from server and save into the default direction ``~/.dhg/datasets/``. Defaults to ``None``.
-    """
-
-    def __init__(self, data_root: Optional[str] = None) -> None:
-        super().__init__("cocitation_pubmed", data_root)
-        self._content = {
-            "num_classes": 3,
-            "num_vertices": 19717,
-            "num_edges": 7963,
-            "dim_features": 500,
-            "features": {
-                "upon": [
-                    {
-                        "filename": "features.pkl",
-                        "md5": "f89502c432ca451156a8235c5efc034e",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_tensor, partial(norm_ft, ord=1)],
-            },
-            "edge_list": {
-                "upon": [
-                    {
-                        "filename": "edge_list.pkl",
-                        "md5": "c5fbedf63e5be527f200e8c4e0391b00",
-                    }
-                ],
-                "loader": load_from_pickle,
-            },
-            "labels": {
-                "upon": [
-                    {
-                        "filename": "labels.pkl",
-                        "md5": "c039f778409a15f9b2ceefacad9c2202",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_long_tensor],
-            },
-            "train_mask": {
-                "upon": [
-                    {
-                        "filename": "train_mask.pkl",
-                        "md5": "81b422937f3adccd89a334d7093b67d7",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-            "val_mask": {
-                "upon": [
-                    {
-                        "filename": "val_mask.pkl",
-                        "md5": "10717940ddbfa3e4f6c0b148bb394f79",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-            "test_mask": {
-                "upon": [
-                    {
-                        "filename": "test_mask.pkl",
-                        "md5": "10717940ddbfa3e4f6c0b148bb394f79",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-        }
+from functools import partial
+from typing import Optional
+
+from easygraph.datapipe import load_from_pickle
+from easygraph.datapipe import norm_ft
+from easygraph.datapipe import to_bool_tensor
+from easygraph.datapipe import to_long_tensor
+from easygraph.datapipe import to_tensor
+from easygraph.datasets.hypergraph.hypergraph_dataset_base import BaseData
+
+
+class CocitationCora(BaseData):
+    r"""The Co-citation Cora dataset is a citation network dataset for vertex classification task.
+    More details see the `HyperGCN <https://papers.nips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf>`_ paper.
+
+    The content of the Co-citation Cora dataset includes the following:
+
+    - ``num_classes``: The number of classes: :math:`7`.
+    - ``num_vertices``: The number of vertices: :math:`2,708`.
+    - ``num_edges``: The number of edges: :math:`1,579`.
+    - ``dim_features``: The dimension of features: :math:`1,433`.
+    - ``features``: The vertex feature matrix. ``torch.Tensor`` with size :math:`(2,708 \times 1,433)`.
+    - ``edge_list``: The edge list. ``List`` with length :math:`1,579`.
+    - ``labels``: The label list. ``torch.LongTensor`` with size :math:`(2,708, )`.
+    - ``train_mask``: The train mask. ``torch.BoolTensor`` with size :math:`(2,708, )`.
+    - ``val_mask``: The validation mask. ``torch.BoolTensor`` with size :math:`(2,708, )`.
+    - ``test_mask``: The test mask. ``torch.BoolTensor`` with size :math:`(2,708, )`.
+
+    Args:
+        ``data_root`` (``str``, optional): The ``data_root`` has stored the data. If set to ``None``, this function will auto-download from server and save into the default direction ``~/.dhg/datasets/``. Defaults to ``None``.
+    """
+
+    def __init__(self, data_root: Optional[str] = None) -> None:
+        super().__init__("cocitation_cora", data_root)
+        self._content = {
+            "num_classes": 7,
+            "num_vertices": 2708,
+            "num_edges": 1579,
+            "dim_features": 1433,
+            "features": {
+                "upon": [
+                    {
+                        "filename": "features.pkl",
+                        "md5": "14257c0e24b4eb741b469a351e524785",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_tensor, partial(norm_ft, ord=1)],
+            },
+            "edge_list": {
+                "upon": [
+                    {
+                        "filename": "edge_list.pkl",
+                        "md5": "e43d1321880c8ecb2260d8fb7effd9ea",
+                    }
+                ],
+                "loader": load_from_pickle,
+            },
+            "labels": {
+                "upon": [
+                    {
+                        "filename": "labels.pkl",
+                        "md5": "c8d11c452e0be69f79a47dd839279117",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_long_tensor],
+            },
+            "train_mask": {
+                "upon": [
+                    {
+                        "filename": "train_mask.pkl",
+                        "md5": "111db6c6f986be2908378df7bdca7a9b",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+            "val_mask": {
+                "upon": [
+                    {
+                        "filename": "val_mask.pkl",
+                        "md5": "ffab1055193ffb2fe74822bb575d332a",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+            "test_mask": {
+                "upon": [
+                    {
+                        "filename": "test_mask.pkl",
+                        "md5": "ffab1055193ffb2fe74822bb575d332a",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+        }
+
+
+class CocitationCiteseer(BaseData):
+    r"""The Co-citation Citeseer dataset is a citation network dataset for vertex classification task.
+    More details see the `HyperGCN <https://papers.nips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf>`_ paper.
+
+    The content of the Co-citation Citaseer dataset includes the following:
+
+    - ``num_classes``: The number of classes: :math:`6`.
+    - ``num_vertices``: The number of vertices: :math:`3,327`.
+    - ``num_edges``: The number of edges: :math:`1,079`.
+    - ``dim_features``: The dimension of features: :math:`3,703`.
+    - ``features``: The vertex feature matrix. ``torch.Tensor`` with size :math:`(3,327 \times 3,703)`.
+    - ``edge_list``: The edge list. ``List`` with length :math:`1,079`.
+    - ``labels``: The label list. ``torch.LongTensor`` with size :math:`(3,327, )`.
+    - ``train_mask``: The train mask. ``torch.BoolTensor`` with size :math:`(3,327, )`.
+    - ``val_mask``: The validation mask. ``torch.BoolTensor`` with size :math:`(3,327, )`.
+    - ``test_mask``: The test mask. ``torch.BoolTensor`` with size :math:`(3,327, )`.
+
+    Args:
+        ``data_root`` (``str``, optional): The ``data_root`` has stored the data. If set to ``None``, this function will auto-download from server and save into the default direction ``~/.dhg/datasets/``. Defaults to ``None``.
+    """
+
+    def __init__(self, data_root: Optional[str] = None) -> None:
+        super().__init__("cocitation_citeseer", data_root)
+        self._content = {
+            "num_classes": 6,
+            "num_vertices": 3312,
+            "num_edges": 1079,
+            "dim_features": 3703,
+            "features": {
+                "upon": [
+                    {
+                        "filename": "features.pkl",
+                        "md5": "1ee0dc89e0d5f5ac9187b55a407683e8",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_tensor, partial(norm_ft, ord=1)],
+            },
+            "edge_list": {
+                "upon": [
+                    {
+                        "filename": "edge_list.pkl",
+                        "md5": "6687b2e96159c534a424253f536b49ae",
+                    }
+                ],
+                "loader": load_from_pickle,
+            },
+            "labels": {
+                "upon": [
+                    {
+                        "filename": "labels.pkl",
+                        "md5": "71069f78e83fa85dd6a4b9b6570447c2",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_long_tensor],
+            },
+            "train_mask": {
+                "upon": [
+                    {
+                        "filename": "train_mask.pkl",
+                        "md5": "3b831318fc3d3e588bead5ba469fe38f",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+            "val_mask": {
+                "upon": [
+                    {
+                        "filename": "val_mask.pkl",
+                        "md5": "c22eb5b7493908042c7e039c8bb5a82e",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+            "test_mask": {
+                "upon": [
+                    {
+                        "filename": "test_mask.pkl",
+                        "md5": "c22eb5b7493908042c7e039c8bb5a82e",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+        }
+
+
+class CocitationPubmed(BaseData):
+    r"""The Co-citation PubMed dataset is a citation network dataset for vertex classification task.
+    More details see the `HyperGCN <https://papers.nips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf>`_ paper.
+
+    The content of the Co-citation PubMed dataset includes the following:
+
+    - ``num_classes``: The number of classes: :math:`3`.
+    - ``num_vertices``: The number of vertices: :math:`19,717`.
+    - ``num_edges``: The number of edges: :math:`7,963`.
+    - ``dim_features``: The dimension of features: :math:`500`.
+    - ``features``: The vertex feature matrix. ``torch.Tensor`` with size :math:`(19,717 \times 500)`.
+    - ``edge_list``: The edge list. ``List`` with length :math:`7,963`.
+    - ``labels``: The label list. ``torch.LongTensor`` with size :math:`(19,717, )`.
+    - ``train_mask``: The train mask. ``torch.BoolTensor`` with size :math:`(19,717, )`.
+    - ``val_mask``: The validation mask. ``torch.BoolTensor`` with size :math:`(19,717, )`.
+    - ``test_mask``: The test mask. ``torch.BoolTensor`` with size :math:`(19,717, )`.
+
+    Args:
+        ``data_root`` (``str``, optional): The ``data_root`` has stored the data. If set to ``None``, this function will auto-download from server and save into the default direction ``~/.dhg/datasets/``. Defaults to ``None``.
+    """
+
+    def __init__(self, data_root: Optional[str] = None) -> None:
+        super().__init__("cocitation_pubmed", data_root)
+        self._content = {
+            "num_classes": 3,
+            "num_vertices": 19717,
+            "num_edges": 7963,
+            "dim_features": 500,
+            "features": {
+                "upon": [
+                    {
+                        "filename": "features.pkl",
+                        "md5": "f89502c432ca451156a8235c5efc034e",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_tensor, partial(norm_ft, ord=1)],
+            },
+            "edge_list": {
+                "upon": [
+                    {
+                        "filename": "edge_list.pkl",
+                        "md5": "c5fbedf63e5be527f200e8c4e0391b00",
+                    }
+                ],
+                "loader": load_from_pickle,
+            },
+            "labels": {
+                "upon": [
+                    {
+                        "filename": "labels.pkl",
+                        "md5": "c039f778409a15f9b2ceefacad9c2202",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_long_tensor],
+            },
+            "train_mask": {
+                "upon": [
+                    {
+                        "filename": "train_mask.pkl",
+                        "md5": "81b422937f3adccd89a334d7093b67d7",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+            "val_mask": {
+                "upon": [
+                    {
+                        "filename": "val_mask.pkl",
+                        "md5": "10717940ddbfa3e4f6c0b148bb394f79",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+            "test_mask": {
+                "upon": [
+                    {
+                        "filename": "test_mask.pkl",
+                        "md5": "10717940ddbfa3e4f6c0b148bb394f79",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+        }
```

## easygraph/datasets/hypergraph/mathoverflow_answers.py

 * *Ordering differences only*

```diff
@@ -1,113 +1,113 @@
-import requests
-
-from easygraph.utils.exception import EasyGraphError
-
-
-def request_text_from_url(url):
-    try:
-        r = requests.get(url)
-    except requests.ConnectionError:
-        raise EasyGraphError("Connection Error!")
-
-    if r.ok:
-        return r.text
-    else:
-        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
-
-
-class mathoverflow_answers:
-    def __init__(self, data_root=None):
-        self.data_root = "https://" if data_root is not None else data_root
-        self.hyperedges_path = "https://gitlab.com/easy-graph/easygraph-data-mathoverflow-answers/-/raw/main/hyperedges-mathoverflow-answers.txt?inline=false"
-        self.node_labels_path = "https://gitlab.com/easy-graph/easygraph-data-mathoverflow-answers/-/raw/main/node-labels-mathoverflow-answers.txt?ref_type=heads&inline=false"
-        # self.node_names_path = "https://gitlab.com/easy-graph/easygraph-data-house-committees/-/raw/main/node-names-house-committees.txt?ref_type=heads&inline=false"
-        self.label_names_path = "https://gitlab.com/easy-graph/easygraph-data-mathoverflow-answers/-/raw/main/label-names-mathoverflow-answers.txt?ref_type=heads&inline=false"
-        self._hyperedges = []
-        self._node_labels = []
-        self._label_names = []
-        self._node_names = []
-        self.generate_hypergraph(
-            hyperedges_path=self.hyperedges_path,
-            node_labels_path=self.node_labels_path,
-            # node_names_path=self.node_names_path,
-            label_names_path=self.label_names_path,
-        )
-        self._content = {
-            "num_classes": len(self._label_names),
-            "num_vertices": len(self._node_labels),
-            "num_edges": len(self._hyperedges),
-            "edge_list": self._hyperedges,
-            "labels": self._node_labels,
-        }
-
-    def __getitem__(self, key: str):
-        return self._content[key]
-
-    def process_label_txt(self, data_str, delimiter="\n", transform_fun=str):
-        data_str = data_str.strip()
-        data_lst = data_str.split(delimiter)
-        final_lst = []
-        for data in data_lst:
-            data = data.strip()
-            data = transform_fun(data)
-            final_lst.append(data)
-        return final_lst
-
-    @property
-    def node_labels(self):
-        return self._node_labels
-
-    """
-    @property
-    def node_names(self):
-        return self._node_names
-    """
-
-    @property
-    def label_names(self):
-        return self._label_names
-
-    @property
-    def hyperedges(self):
-        return self._hyperedges
-
-    def generate_hypergraph(
-        self,
-        hyperedges_path=None,
-        node_labels_path=None,
-        # node_names_path=None,
-        label_names_path=None,
-    ):
-        def fun(data):
-            data = int(data) - 1
-            return data
-
-        hyperedges_info = request_text_from_url(hyperedges_path)
-        hyperedges_info = hyperedges_info.strip()
-        hyperedges_lst = hyperedges_info.split("\n")
-        for hyperedge in hyperedges_lst:
-            hyperedge = hyperedge.strip()
-            hyperedge = [int(i) - 1 for i in hyperedge.split(",")]
-            self._hyperedges.append(tuple(hyperedge))
-        # print(self.hyperedges)
-        """
-        node_labels_info = request_text_from_url(node_labels_path)
-
-        process_node_labels_info = self.process_label_txt(
-            node_labels_info, transform_fun=fun
-        )
-        self._node_labels = process_node_labels_info
-        """
-        node_labels_info = request_text_from_url(node_labels_path)
-        node_labels_info = node_labels_info.strip()
-        node_labels_lst = node_labels_info.split("\n")
-        for node_label in node_labels_lst:
-            node_label = node_label.strip()
-            node_label = [int(i) - 1 for i in node_label.split(",")]
-            self._node_labels.append(tuple(node_label))
-        # print("process_node_labels_info:", process_node_labels_info)
-        # print("process_node_names_info:", process_node_names_info)
-        label_names_info = request_text_from_url(label_names_path)
-        process_label_names_info = self.process_label_txt(label_names_info)
-        self._label_names = process_label_names_info
-        # print("process_label_names_info:", process_label_names_info)
+import requests
+
+from easygraph.utils.exception import EasyGraphError
+
+
+def request_text_from_url(url):
+    try:
+        r = requests.get(url)
+    except requests.ConnectionError:
+        raise EasyGraphError("Connection Error!")
+
+    if r.ok:
+        return r.text
+    else:
+        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
+
+
+class mathoverflow_answers:
+    def __init__(self, data_root=None):
+        self.data_root = "https://" if data_root is not None else data_root
+        self.hyperedges_path = "https://gitlab.com/easy-graph/easygraph-data-mathoverflow-answers/-/raw/main/hyperedges-mathoverflow-answers.txt?inline=false"
+        self.node_labels_path = "https://gitlab.com/easy-graph/easygraph-data-mathoverflow-answers/-/raw/main/node-labels-mathoverflow-answers.txt?ref_type=heads&inline=false"
+        # self.node_names_path = "https://gitlab.com/easy-graph/easygraph-data-house-committees/-/raw/main/node-names-house-committees.txt?ref_type=heads&inline=false"
+        self.label_names_path = "https://gitlab.com/easy-graph/easygraph-data-mathoverflow-answers/-/raw/main/label-names-mathoverflow-answers.txt?ref_type=heads&inline=false"
+        self._hyperedges = []
+        self._node_labels = []
+        self._label_names = []
+        self._node_names = []
+        self.generate_hypergraph(
+            hyperedges_path=self.hyperedges_path,
+            node_labels_path=self.node_labels_path,
+            # node_names_path=self.node_names_path,
+            label_names_path=self.label_names_path,
+        )
+        self._content = {
+            "num_classes": len(self._label_names),
+            "num_vertices": len(self._node_labels),
+            "num_edges": len(self._hyperedges),
+            "edge_list": self._hyperedges,
+            "labels": self._node_labels,
+        }
+
+    def __getitem__(self, key: str):
+        return self._content[key]
+
+    def process_label_txt(self, data_str, delimiter="\n", transform_fun=str):
+        data_str = data_str.strip()
+        data_lst = data_str.split(delimiter)
+        final_lst = []
+        for data in data_lst:
+            data = data.strip()
+            data = transform_fun(data)
+            final_lst.append(data)
+        return final_lst
+
+    @property
+    def node_labels(self):
+        return self._node_labels
+
+    """
+    @property
+    def node_names(self):
+        return self._node_names
+    """
+
+    @property
+    def label_names(self):
+        return self._label_names
+
+    @property
+    def hyperedges(self):
+        return self._hyperedges
+
+    def generate_hypergraph(
+        self,
+        hyperedges_path=None,
+        node_labels_path=None,
+        # node_names_path=None,
+        label_names_path=None,
+    ):
+        def fun(data):
+            data = int(data) - 1
+            return data
+
+        hyperedges_info = request_text_from_url(hyperedges_path)
+        hyperedges_info = hyperedges_info.strip()
+        hyperedges_lst = hyperedges_info.split("\n")
+        for hyperedge in hyperedges_lst:
+            hyperedge = hyperedge.strip()
+            hyperedge = [int(i) - 1 for i in hyperedge.split(",")]
+            self._hyperedges.append(tuple(hyperedge))
+        # print(self.hyperedges)
+        """
+        node_labels_info = request_text_from_url(node_labels_path)
+
+        process_node_labels_info = self.process_label_txt(
+            node_labels_info, transform_fun=fun
+        )
+        self._node_labels = process_node_labels_info
+        """
+        node_labels_info = request_text_from_url(node_labels_path)
+        node_labels_info = node_labels_info.strip()
+        node_labels_lst = node_labels_info.split("\n")
+        for node_label in node_labels_lst:
+            node_label = node_label.strip()
+            node_label = [int(i) - 1 for i in node_label.split(",")]
+            self._node_labels.append(tuple(node_label))
+        # print("process_node_labels_info:", process_node_labels_info)
+        # print("process_node_names_info:", process_node_names_info)
+        label_names_info = request_text_from_url(label_names_path)
+        process_label_names_info = self.process_label_txt(label_names_info)
+        self._label_names = process_label_names_info
+        # print("process_label_names_info:", process_label_names_info)
```

## easygraph/datasets/hypergraph/cat_edge_Cooking.py

 * *Ordering differences only*

```diff
@@ -1,115 +1,115 @@
-import requests
-
-from easygraph.utils.exception import EasyGraphError
-
-
-def request_text_from_url(url):
-    try:
-        r = requests.get(url)
-    except requests.ConnectionError:
-        raise EasyGraphError("Connection Error!")
-
-    if r.ok:
-        return r.text
-    else:
-        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
-
-
-class cat_edge_Cooking:
-    def __init__(self, data_root=None):
-        self.data_root = "https://" if data_root is not None else data_root
-        self.hyperedges_path = "https://gitlab.com/easy-graph/easygraph-data-cat-edge-cooking/-/raw/main/hyperedges.txt?inline=false"
-        self.edge_labels_path = "https://gitlab.com/easy-graph/easygraph-data-cat-edge-cooking/-/raw/main/hyperedge-labels.txt?ref_type=heads&inline=false"
-        self.node_names_path = "https://gitlab.com/easy-graph/easygraph-data-cat-edge-cooking/-/raw/main/main/node-labels.txt?ref_type=heads&inline=false"
-        self.label_names_path = "https://gitlab.com/easy-graph/easygraph-data-cat-edge-cooking/-/raw/main/hyperedge-label-identities.txt?ref_type=heads&inline=false"
-        self.hyperedges_path = []
-        self.edge_labels_path = []
-        self.node_names_path = []
-        self.label_names_path = []
-        self.generate_hypergraph(
-            hyperedges_path=self.hyperedges_path,
-            edge_labels_path=self.edge_labels_path,
-            node_names_path=self.node_names_path,
-            label_names_path=self.label_names_path,
-        )
-        self._content = {
-            "num_classes": len(self._label_names),
-            "num_vertices": len(self._node_labels),
-            "num_edges": len(self._hyperedges),
-            "edge_list": self._hyperedges,
-            "labels": self._node_labels,
-        }
-
-    def __getitem__(self, key: str):
-        return self._content[key]
-
-    def process_label_txt(self, data_str, delimiter="\n", transform_fun=str):
-        data_str = data_str.strip()
-        data_lst = data_str.split(delimiter)
-        final_lst = []
-        for data in data_lst:
-            data = data.strip()
-            data = transform_fun(data)
-            final_lst.append(data)
-        return final_lst
-
-    @property
-    def edge_labels(self):
-        return self._edge_labels
-
-    @property
-    def node_names(self):
-        return self._node_names
-
-    @property
-    def label_names(self):
-        return self._label_names
-
-    @property
-    def hyperedges(self):
-        return self._hyperedges
-
-    def generate_hypergraph(
-        self,
-        hyperedges_path=None,
-        node_labels_path=None,
-        node_names_path=None,
-        label_names_path=None,
-    ):
-        def fun(data):
-            data = int(data) - 1
-            return data
-
-        hyperedges_info = request_text_from_url(hyperedges_path)
-        hyperedges_info = hyperedges_info.strip()
-        hyperedges_lst = hyperedges_info.split("\n")
-        for hyperedge in hyperedges_lst:
-            hyperedge = hyperedge.strip()
-            hyperedge = [int(i) - 1 for i in hyperedge.split(" ")]
-            self._hyperedges.append(tuple(hyperedge))
-        # print(self.hyperedges)
-
-        edge_labels_info = request_text_from_url(edge_labels_path)
-        process_node_labels_info = self.process_label_txt(
-            node_labels_info, transform_fun=fun
-        )
-        self._edge_labels = process_edge_labels_info
-        # print("process_node_labels_info:", process_node_labels_info)
-
-        node_names_info = request_text_from_url(node_names_path)
-        process_node_names_info = self.process_label_txt(node_names_info)
-        self._node_names = process_node_names_info
-
-        # print("process_node_names_info:", process_node_names_info)
-        label_names_info = request_text_from_url(label_names_path)
-        process_label_names_info = self.process_label_txt(label_names_info)
-        self._label_names = process_label_names_info
-        # print("process_label_names_info:", process_label_names_info)
-
-
-#
-# if __name__ == "__main__":
-#     a = House_Committees()
-#     print(a.node_labels)
-#     print(a.label_names)
-#     print(a.node_names)
+import requests
+
+from easygraph.utils.exception import EasyGraphError
+
+
+def request_text_from_url(url):
+    try:
+        r = requests.get(url)
+    except requests.ConnectionError:
+        raise EasyGraphError("Connection Error!")
+
+    if r.ok:
+        return r.text
+    else:
+        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
+
+
+class cat_edge_Cooking:
+    def __init__(self, data_root=None):
+        self.data_root = "https://" if data_root is not None else data_root
+        self.hyperedges_path = "https://gitlab.com/easy-graph/easygraph-data-cat-edge-cooking/-/raw/main/hyperedges.txt?inline=false"
+        self.edge_labels_path = "https://gitlab.com/easy-graph/easygraph-data-cat-edge-cooking/-/raw/main/hyperedge-labels.txt?ref_type=heads&inline=false"
+        self.node_names_path = "https://gitlab.com/easy-graph/easygraph-data-cat-edge-cooking/-/raw/main/main/node-labels.txt?ref_type=heads&inline=false"
+        self.label_names_path = "https://gitlab.com/easy-graph/easygraph-data-cat-edge-cooking/-/raw/main/hyperedge-label-identities.txt?ref_type=heads&inline=false"
+        self.hyperedges_path = []
+        self.edge_labels_path = []
+        self.node_names_path = []
+        self.label_names_path = []
+        self.generate_hypergraph(
+            hyperedges_path=self.hyperedges_path,
+            edge_labels_path=self.edge_labels_path,
+            node_names_path=self.node_names_path,
+            label_names_path=self.label_names_path,
+        )
+        self._content = {
+            "num_classes": len(self._label_names),
+            "num_vertices": len(self._node_labels),
+            "num_edges": len(self._hyperedges),
+            "edge_list": self._hyperedges,
+            "labels": self._node_labels,
+        }
+
+    def __getitem__(self, key: str):
+        return self._content[key]
+
+    def process_label_txt(self, data_str, delimiter="\n", transform_fun=str):
+        data_str = data_str.strip()
+        data_lst = data_str.split(delimiter)
+        final_lst = []
+        for data in data_lst:
+            data = data.strip()
+            data = transform_fun(data)
+            final_lst.append(data)
+        return final_lst
+
+    @property
+    def edge_labels(self):
+        return self._edge_labels
+
+    @property
+    def node_names(self):
+        return self._node_names
+
+    @property
+    def label_names(self):
+        return self._label_names
+
+    @property
+    def hyperedges(self):
+        return self._hyperedges
+
+    def generate_hypergraph(
+        self,
+        hyperedges_path=None,
+        node_labels_path=None,
+        node_names_path=None,
+        label_names_path=None,
+    ):
+        def fun(data):
+            data = int(data) - 1
+            return data
+
+        hyperedges_info = request_text_from_url(hyperedges_path)
+        hyperedges_info = hyperedges_info.strip()
+        hyperedges_lst = hyperedges_info.split("\n")
+        for hyperedge in hyperedges_lst:
+            hyperedge = hyperedge.strip()
+            hyperedge = [int(i) - 1 for i in hyperedge.split(" ")]
+            self._hyperedges.append(tuple(hyperedge))
+        # print(self.hyperedges)
+
+        edge_labels_info = request_text_from_url(edge_labels_path)
+        process_node_labels_info = self.process_label_txt(
+            node_labels_info, transform_fun=fun
+        )
+        self._edge_labels = process_edge_labels_info
+        # print("process_node_labels_info:", process_node_labels_info)
+
+        node_names_info = request_text_from_url(node_names_path)
+        process_node_names_info = self.process_label_txt(node_names_info)
+        self._node_names = process_node_names_info
+
+        # print("process_node_names_info:", process_node_names_info)
+        label_names_info = request_text_from_url(label_names_path)
+        process_label_names_info = self.process_label_txt(label_names_info)
+        self._label_names = process_label_names_info
+        # print("process_label_names_info:", process_label_names_info)
+
+
+#
+# if __name__ == "__main__":
+#     a = House_Committees()
+#     print(a.node_labels)
+#     print(a.label_names)
+#     print(a.node_names)
```

## easygraph/datasets/hypergraph/Yelp.py

 * *Ordering differences only*

```diff
@@ -1,82 +1,82 @@
-from typing import Optional
-
-from easygraph.datapipe import load_from_pickle
-from easygraph.datapipe import to_long_tensor
-from easygraph.datapipe import to_tensor
-from easygraph.datasets.hypergraph.hypergraph_dataset_base import BaseData
-
-
-class YelpRestaurant(BaseData):
-    r"""The Yelp-Restaurant dataset is a restaurant-review network dataset for node classification task.
-
-    More details see the DHG or `YOU ARE ALLSET: A MULTISET LEARNING FRAMEWORK FOR HYPERGRAPH NEURAL NETWORKS <https://openreview.net/pdf?id=hpBTIv2uy_E>`_ paper.
-
-    The content of the Yelp-Restaurant dataset includes the following:
-
-    - ``num_classes``: The number of classes: :math:`11`.
-    - ``num_vertices``: The number of vertices: :math:`50,758`.
-    - ``num_edges``: The number of edges: :math:`679,302`.
-    - ``dim_features``: The dimension of features: :math:`1,862`.
-    - ``features``: The vertex feature matrix. ``torch.Tensor`` with size :math:`(50,758 \times 1,862)`.
-    - ``edge_list``: The edge list. ``List`` with length :math:`679,302`.
-    - ``labels``: The label list. ``torch.LongTensor`` with size :math:`(50,758, )`.
-    - ``state``: The state list. ``torch.LongTensor`` with size :math:`(50,758, )`.
-    - ``city``: The city list. ``torch.LongTensor`` with size :math:`(50,758, )`.
-
-    Args:
-        ``data_root`` (``str``, optional): The ``data_root`` has stored the data. If set to ``None``, this function will auto-download from server and save into the default direction ``~/.dhg/datasets/``. Defaults to ``None``.
-    """
-
-    def __init__(self, data_root: Optional[str] = None) -> None:
-        super().__init__("yelp_restaurant", data_root)
-        self._content = {
-            "num_classes": 11,
-            "num_vertices": 50758,
-            "num_edges": 679302,
-            "dim_features": 1862,
-            "features": {
-                "upon": [
-                    {
-                        "filename": "features.pkl",
-                        "md5": "cedc4443884477c2e626025411c44cd7",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [
-                    to_tensor,
-                ],
-            },
-            "edge_list": {
-                "upon": [
-                    {
-                        "filename": "edge_list.pkl",
-                        "md5": "4b26eecaa22305dd10edcd6372eb49da",
-                    }
-                ],
-                "loader": load_from_pickle,
-            },
-            "labels": {
-                "upon": [
-                    {
-                        "filename": "labels.pkl",
-                        "md5": "1cdc1ed9fb1f57b2accaa42db214d4ef",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_long_tensor],
-            },
-            "state": {
-                "upon": [
-                    {"filename": "state.pkl", "md5": "eef3b835fad37409f29ad36539296b57"}
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_long_tensor],
-            },
-            "city": {
-                "upon": [
-                    {"filename": "city.pkl", "md5": "8302b167262b23067698e865cacd0b17"}
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_long_tensor],
-            },
-        }
+from typing import Optional
+
+from easygraph.datapipe import load_from_pickle
+from easygraph.datapipe import to_long_tensor
+from easygraph.datapipe import to_tensor
+from easygraph.datasets.hypergraph.hypergraph_dataset_base import BaseData
+
+
+class YelpRestaurant(BaseData):
+    r"""The Yelp-Restaurant dataset is a restaurant-review network dataset for node classification task.
+
+    More details see the DHG or `YOU ARE ALLSET: A MULTISET LEARNING FRAMEWORK FOR HYPERGRAPH NEURAL NETWORKS <https://openreview.net/pdf?id=hpBTIv2uy_E>`_ paper.
+
+    The content of the Yelp-Restaurant dataset includes the following:
+
+    - ``num_classes``: The number of classes: :math:`11`.
+    - ``num_vertices``: The number of vertices: :math:`50,758`.
+    - ``num_edges``: The number of edges: :math:`679,302`.
+    - ``dim_features``: The dimension of features: :math:`1,862`.
+    - ``features``: The vertex feature matrix. ``torch.Tensor`` with size :math:`(50,758 \times 1,862)`.
+    - ``edge_list``: The edge list. ``List`` with length :math:`679,302`.
+    - ``labels``: The label list. ``torch.LongTensor`` with size :math:`(50,758, )`.
+    - ``state``: The state list. ``torch.LongTensor`` with size :math:`(50,758, )`.
+    - ``city``: The city list. ``torch.LongTensor`` with size :math:`(50,758, )`.
+
+    Args:
+        ``data_root`` (``str``, optional): The ``data_root`` has stored the data. If set to ``None``, this function will auto-download from server and save into the default direction ``~/.dhg/datasets/``. Defaults to ``None``.
+    """
+
+    def __init__(self, data_root: Optional[str] = None) -> None:
+        super().__init__("yelp_restaurant", data_root)
+        self._content = {
+            "num_classes": 11,
+            "num_vertices": 50758,
+            "num_edges": 679302,
+            "dim_features": 1862,
+            "features": {
+                "upon": [
+                    {
+                        "filename": "features.pkl",
+                        "md5": "cedc4443884477c2e626025411c44cd7",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [
+                    to_tensor,
+                ],
+            },
+            "edge_list": {
+                "upon": [
+                    {
+                        "filename": "edge_list.pkl",
+                        "md5": "4b26eecaa22305dd10edcd6372eb49da",
+                    }
+                ],
+                "loader": load_from_pickle,
+            },
+            "labels": {
+                "upon": [
+                    {
+                        "filename": "labels.pkl",
+                        "md5": "1cdc1ed9fb1f57b2accaa42db214d4ef",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_long_tensor],
+            },
+            "state": {
+                "upon": [
+                    {"filename": "state.pkl", "md5": "eef3b835fad37409f29ad36539296b57"}
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_long_tensor],
+            },
+            "city": {
+                "upon": [
+                    {"filename": "city.pkl", "md5": "8302b167262b23067698e865cacd0b17"}
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_long_tensor],
+            },
+        }
```

## easygraph/datasets/hypergraph/walmart_trips.py

 * *Ordering differences only*

```diff
@@ -1,109 +1,109 @@
-import requests
-
-from easygraph.utils.exception import EasyGraphError
-
-
-def request_text_from_url(url):
-    try:
-        r = requests.get(url)
-    except requests.ConnectionError:
-        raise EasyGraphError("Connection Error!")
-
-    if r.ok:
-        return r.text
-    else:
-        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
-
-
-class walmart_trips:
-    def __init__(self, data_root=None, local_path=None):
-        # if local_path is not None:
-
-        self.data_root = "https://" if data_root is not None else data_root
-        self.hyperedges_path = "https://gitlab.com/easy-graph/easygraph-data-walmart-trips/-/raw/main/hyperedges-walmart-trips.txt?inline=false"
-        self.node_labels_path = "https://gitlab.com/easy-graph/easygraph-data-walmart-trips/-/raw/main/node-labels-walmart-trips.txt?ref_type=heads&inline=false"
-        # self.node_names_path = "https://gitlab.com/easy-graph/easygraph-data-walmart-trips/-/raw/main/node-names-house-committees.txt?ref_type=heads&inline=false"
-        self.label_names_path = "https://gitlab.com/easy-graph/easygraph-data-walmart-trips/-/raw/main/label-names-walmart-trips.txt?ref_type=heads&inline=false"
-        self._hyperedges = []
-        self._node_labels = []
-        self._label_names = []
-        self._node_names = []
-
-        self.generate_hypergraph(
-            hyperedges_path=self.hyperedges_path,
-            node_labels_path=self.node_labels_path,
-            # node_names_path=self.node_names_path,
-            label_names_path=self.label_names_path,
-        )
-
-        self._content = {
-            "num_classes": len(self._label_names),
-            "num_vertices": len(self._node_labels),
-            "num_edges": len(self._hyperedges),
-            "edge_list": self._hyperedges,
-            "labels": self._node_labels,
-        }
-
-    def __getitem__(self, key: str):
-        return self._content[key]
-
-    def process_label_txt(self, data_str, delimiter="\n", transform_fun=str):
-        data_str = data_str.strip()
-        data_lst = data_str.split(delimiter)
-        final_lst = []
-        for data in data_lst:
-            data = data.strip()
-            data = transform_fun(data)
-            final_lst.append(data)
-        return final_lst
-
-    @property
-    def node_labels(self):
-        return self._node_labels
-
-    """
-    @property
-    def node_names(self):
-        return self._node_names
-    """
-
-    @property
-    def label_names(self):
-        return self._label_names
-
-    @property
-    def hyperedges(self):
-        return self._hyperedges
-
-    def generate_hypergraph(
-        self,
-        hyperedges_path=None,
-        node_labels_path=None,
-        # node_names_path=None,
-        label_names_path=None,
-    ):
-        def fun(data):
-            data = int(data) - 1
-            return data
-
-        hyperedges_info = request_text_from_url(hyperedges_path)
-        hyperedges_info = hyperedges_info.strip()
-        hyperedges_lst = hyperedges_info.split("\n")
-        for hyperedge in hyperedges_lst:
-            hyperedge = hyperedge.strip()
-            hyperedge = [int(i) - 1 for i in hyperedge.split(",")]
-            self._hyperedges.append(tuple(hyperedge))
-        # print(self.hyperedges)
-
-        node_labels_info = request_text_from_url(node_labels_path)
-
-        process_node_labels_info = self.process_label_txt(
-            node_labels_info, transform_fun=fun
-        )
-        self._node_labels = process_node_labels_info
-        # print("process_node_labels_info:", process_node_labels_info)
-        # print("process_node_names_info:", process_node_names_info)
-        label_names_info = request_text_from_url(label_names_path)
-        process_label_names_info = self.process_label_txt(label_names_info)
-        self._label_names = process_label_names_info
-        # print("process_label_names_info:", process_label_names_info)
+import requests
+
+from easygraph.utils.exception import EasyGraphError
+
+
+def request_text_from_url(url):
+    try:
+        r = requests.get(url)
+    except requests.ConnectionError:
+        raise EasyGraphError("Connection Error!")
+
+    if r.ok:
+        return r.text
+    else:
+        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
+
+
+class walmart_trips:
+    def __init__(self, data_root=None, local_path=None):
+        # if local_path is not None:
+
+        self.data_root = "https://" if data_root is not None else data_root
+        self.hyperedges_path = "https://gitlab.com/easy-graph/easygraph-data-walmart-trips/-/raw/main/hyperedges-walmart-trips.txt?inline=false"
+        self.node_labels_path = "https://gitlab.com/easy-graph/easygraph-data-walmart-trips/-/raw/main/node-labels-walmart-trips.txt?ref_type=heads&inline=false"
+        # self.node_names_path = "https://gitlab.com/easy-graph/easygraph-data-walmart-trips/-/raw/main/node-names-house-committees.txt?ref_type=heads&inline=false"
+        self.label_names_path = "https://gitlab.com/easy-graph/easygraph-data-walmart-trips/-/raw/main/label-names-walmart-trips.txt?ref_type=heads&inline=false"
+        self._hyperedges = []
+        self._node_labels = []
+        self._label_names = []
+        self._node_names = []
+
+        self.generate_hypergraph(
+            hyperedges_path=self.hyperedges_path,
+            node_labels_path=self.node_labels_path,
+            # node_names_path=self.node_names_path,
+            label_names_path=self.label_names_path,
+        )
+
+        self._content = {
+            "num_classes": len(self._label_names),
+            "num_vertices": len(self._node_labels),
+            "num_edges": len(self._hyperedges),
+            "edge_list": self._hyperedges,
+            "labels": self._node_labels,
+        }
+
+    def __getitem__(self, key: str):
+        return self._content[key]
+
+    def process_label_txt(self, data_str, delimiter="\n", transform_fun=str):
+        data_str = data_str.strip()
+        data_lst = data_str.split(delimiter)
+        final_lst = []
+        for data in data_lst:
+            data = data.strip()
+            data = transform_fun(data)
+            final_lst.append(data)
+        return final_lst
+
+    @property
+    def node_labels(self):
+        return self._node_labels
+
+    """
+    @property
+    def node_names(self):
+        return self._node_names
+    """
+
+    @property
+    def label_names(self):
+        return self._label_names
+
+    @property
+    def hyperedges(self):
+        return self._hyperedges
+
+    def generate_hypergraph(
+        self,
+        hyperedges_path=None,
+        node_labels_path=None,
+        # node_names_path=None,
+        label_names_path=None,
+    ):
+        def fun(data):
+            data = int(data) - 1
+            return data
+
+        hyperedges_info = request_text_from_url(hyperedges_path)
+        hyperedges_info = hyperedges_info.strip()
+        hyperedges_lst = hyperedges_info.split("\n")
+        for hyperedge in hyperedges_lst:
+            hyperedge = hyperedge.strip()
+            hyperedge = [int(i) - 1 for i in hyperedge.split(",")]
+            self._hyperedges.append(tuple(hyperedge))
+        # print(self.hyperedges)
+
+        node_labels_info = request_text_from_url(node_labels_path)
+
+        process_node_labels_info = self.process_label_txt(
+            node_labels_info, transform_fun=fun
+        )
+        self._node_labels = process_node_labels_info
+        # print("process_node_labels_info:", process_node_labels_info)
+        # print("process_node_names_info:", process_node_names_info)
+        label_names_info = request_text_from_url(label_names_path)
+        process_label_names_info = self.process_label_txt(label_names_info)
+        self._label_names = process_label_names_info
+        # print("process_label_names_info:", process_label_names_info)
```

## easygraph/datasets/hypergraph/contact_primary_school.py

 * *Ordering differences only*

```diff
@@ -1,112 +1,112 @@
-import requests
-
-from easygraph.utils.exception import EasyGraphError
-
-
-def request_text_from_url(url):
-    try:
-        r = requests.get(url)
-    except requests.ConnectionError:
-        raise EasyGraphError("Connection Error!")
-
-    if r.ok:
-        return r.text
-    else:
-        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
-
-
-class contact_primary_school:
-    def __init__(self, data_root=None):
-        self.data_root = "https://" if data_root is not None else data_root
-        self.hyperedges_path = "https://gitlab.com/easy-graph/easygraph-data-contact-primary-school/-/raw/main/hyperedges-contact-primary-school.txt?inline=false"
-        self.node_labels_path = "https://gitlab.com/easy-graph/easygraph-data-contact-primary-school/-/raw/main/node-labels-contact-primary-school.txt?ref_type=heads&inline=false"
-        # self.node_names_path = "https://gitlab.com/easy-graph/easygraph-data-house-committees/-/raw/main/node-names-house-committees.txt?ref_type=heads&inline=false"
-        self.label_names_path = "https://gitlab.com/easy-graph/easygraph-data-contact-primary-school/-/raw/main/label-names-contact-primary-school.txt?ref_type=heads&inline=false"
-        self._hyperedges = []
-        self._node_labels = []
-        self._label_names = []
-        self._node_names = []
-        self.generate_hypergraph(
-            hyperedges_path=self.hyperedges_path,
-            node_labels_path=self.node_labels_path,
-            # node_names_path=self.node_names_path,
-            label_names_path=self.label_names_path,
-        )
-        self._content = {
-            "num_classes": len(self._label_names),
-            "num_vertices": len(self._node_labels),
-            "num_edges": len(self._hyperedges),
-            "edge_list": self._hyperedges,
-            "labels": self._node_labels,
-        }
-
-    def __getitem__(self, key: str):
-        return self._content[key]
-
-    def process_label_txt(self, data_str, delimiter="\n", transform_fun=str):
-        data_str = data_str.strip()
-        data_lst = data_str.split(delimiter)
-        final_lst = []
-        for data in data_lst:
-            data = data.strip()
-            data = transform_fun(data)
-            final_lst.append(data)
-        return final_lst
-
-    @property
-    def node_labels(self):
-        return self._node_labels
-
-    """
-    @property
-    def node_names(self):
-        return self._node_names
-    """
-
-    @property
-    def label_names(self):
-        return self._label_names
-
-    @property
-    def hyperedges(self):
-        return self._hyperedges
-
-    def generate_hypergraph(
-        self,
-        hyperedges_path=None,
-        node_labels_path=None,
-        # node_names_path=None,
-        label_names_path=None,
-    ):
-        def fun(data):
-            data = int(data) - 1
-            return data
-
-        hyperedges_info = request_text_from_url(hyperedges_path)
-        hyperedges_info = hyperedges_info.strip()
-        hyperedges_lst = hyperedges_info.split("\n")
-        for hyperedge in hyperedges_lst:
-            hyperedge = hyperedge.strip()
-            hyperedge = [int(i) - 1 for i in hyperedge.split(",")]
-            self._hyperedges.append(tuple(hyperedge))
-        # print(self.hyperedges)
-
-        node_labels_info = request_text_from_url(node_labels_path)
-
-        process_node_labels_info = self.process_label_txt(
-            node_labels_info, transform_fun=fun
-        )
-        self._node_labels = process_node_labels_info
-        # print("process_node_labels_info:", process_node_labels_info)
-        # print("process_node_names_info:", process_node_names_info)
-        label_names_info = request_text_from_url(label_names_path)
-        process_label_names_info = self.process_label_txt(label_names_info)
-        self._label_names = process_label_names_info
-        # print("process_label_names_info:", process_label_names_info)
-
-
-# if __name__ == "__main__":
-#    a = contact_primary_school()
-#    print(a.node_labels)
-#    print(a.label_names)
-#    print(a.node_names)
+import requests
+
+from easygraph.utils.exception import EasyGraphError
+
+
+def request_text_from_url(url):
+    try:
+        r = requests.get(url)
+    except requests.ConnectionError:
+        raise EasyGraphError("Connection Error!")
+
+    if r.ok:
+        return r.text
+    else:
+        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
+
+
+class contact_primary_school:
+    def __init__(self, data_root=None):
+        self.data_root = "https://" if data_root is not None else data_root
+        self.hyperedges_path = "https://gitlab.com/easy-graph/easygraph-data-contact-primary-school/-/raw/main/hyperedges-contact-primary-school.txt?inline=false"
+        self.node_labels_path = "https://gitlab.com/easy-graph/easygraph-data-contact-primary-school/-/raw/main/node-labels-contact-primary-school.txt?ref_type=heads&inline=false"
+        # self.node_names_path = "https://gitlab.com/easy-graph/easygraph-data-house-committees/-/raw/main/node-names-house-committees.txt?ref_type=heads&inline=false"
+        self.label_names_path = "https://gitlab.com/easy-graph/easygraph-data-contact-primary-school/-/raw/main/label-names-contact-primary-school.txt?ref_type=heads&inline=false"
+        self._hyperedges = []
+        self._node_labels = []
+        self._label_names = []
+        self._node_names = []
+        self.generate_hypergraph(
+            hyperedges_path=self.hyperedges_path,
+            node_labels_path=self.node_labels_path,
+            # node_names_path=self.node_names_path,
+            label_names_path=self.label_names_path,
+        )
+        self._content = {
+            "num_classes": len(self._label_names),
+            "num_vertices": len(self._node_labels),
+            "num_edges": len(self._hyperedges),
+            "edge_list": self._hyperedges,
+            "labels": self._node_labels,
+        }
+
+    def __getitem__(self, key: str):
+        return self._content[key]
+
+    def process_label_txt(self, data_str, delimiter="\n", transform_fun=str):
+        data_str = data_str.strip()
+        data_lst = data_str.split(delimiter)
+        final_lst = []
+        for data in data_lst:
+            data = data.strip()
+            data = transform_fun(data)
+            final_lst.append(data)
+        return final_lst
+
+    @property
+    def node_labels(self):
+        return self._node_labels
+
+    """
+    @property
+    def node_names(self):
+        return self._node_names
+    """
+
+    @property
+    def label_names(self):
+        return self._label_names
+
+    @property
+    def hyperedges(self):
+        return self._hyperedges
+
+    def generate_hypergraph(
+        self,
+        hyperedges_path=None,
+        node_labels_path=None,
+        # node_names_path=None,
+        label_names_path=None,
+    ):
+        def fun(data):
+            data = int(data) - 1
+            return data
+
+        hyperedges_info = request_text_from_url(hyperedges_path)
+        hyperedges_info = hyperedges_info.strip()
+        hyperedges_lst = hyperedges_info.split("\n")
+        for hyperedge in hyperedges_lst:
+            hyperedge = hyperedge.strip()
+            hyperedge = [int(i) - 1 for i in hyperedge.split(",")]
+            self._hyperedges.append(tuple(hyperedge))
+        # print(self.hyperedges)
+
+        node_labels_info = request_text_from_url(node_labels_path)
+
+        process_node_labels_info = self.process_label_txt(
+            node_labels_info, transform_fun=fun
+        )
+        self._node_labels = process_node_labels_info
+        # print("process_node_labels_info:", process_node_labels_info)
+        # print("process_node_names_info:", process_node_names_info)
+        label_names_info = request_text_from_url(label_names_path)
+        process_label_names_info = self.process_label_txt(label_names_info)
+        self._label_names = process_label_names_info
+        # print("process_label_names_info:", process_label_names_info)
+
+
+# if __name__ == "__main__":
+#    a = contact_primary_school()
+#    print(a.node_labels)
+#    print(a.label_names)
+#    print(a.node_names)
```

## easygraph/datasets/hypergraph/hypergraph_dataset_base.py

 * *Ordering differences only*

```diff
@@ -1,119 +1,119 @@
-from pathlib import Path
-from typing import Any
-from typing import Dict
-from typing import List
-
-from easygraph.datapipe import compose_pipes
-from easygraph.datasets.hypergraph._global import DATASETS_ROOT
-from easygraph.datasets.hypergraph._global import REMOTE_DATASETS_ROOT
-from easygraph.datasets.utils import download_and_check
-
-
-class BaseData:
-    r"""The Base Class of all datasets.
-
-    ::
-
-        self._content = {
-            'item': {
-                'upon': [
-                    {'filename': 'part1.pkl', 'md5': 'xxxxx',},
-                    {'filename': 'part2.pkl', 'md5': 'xxxxx',},
-                ],
-                'loader': loader_function,
-                'preprocess': [datapipe1, datapipe2],
-            },
-            ...
-        }
-
-    """
-
-    def __init__(self, name: str, data_root=None):
-        # configure the data local/remote root
-        self.name = name
-        if data_root is None:
-            self.data_root = DATASETS_ROOT / name
-        else:
-            self.data_root = Path(data_root) / name
-        self.remote_root = REMOTE_DATASETS_ROOT + name + "/"
-        # init
-        self._content = {}
-        self._raw = {}
-
-    def __repr__(self) -> str:
-        return (
-            f"This is {self.name} dataset:\n"
-            + "\n".join(f"  ->  {k}" for k in self.content)
-            + "\nPlease try `data['name']` to get the specified data."
-        )
-
-    @property
-    def content(self):
-        r"""Return the content of the dataset."""
-        return list(self._content.keys())
-
-    def needs_to_load(self, item_name: str) -> bool:
-        r"""Return whether the ``item_name`` of the dataset needs to be loaded.
-
-        Args:
-            ``item_name`` (``str``): The name of the item in the dataset.
-        """
-        assert item_name in self.content, f"{item_name} is not provided in the Data"
-        return (
-            isinstance(self._content[item_name], dict)
-            and "upon" in self._content[item_name]
-            and "loader" in self._content[item_name]
-        )
-
-    def __getitem__(self, key: str) -> Any:
-        if self.needs_to_load(key):
-            cur_cfg = self._content[key]
-            if cur_cfg.get("cache", None) is None:
-                # get raw data
-                item = self.raw(key)
-                # preprocess and cache
-                pipes = cur_cfg.get("preprocess", None)
-                if pipes is not None:
-                    cur_cfg["cache"] = compose_pipes(*pipes)(item)
-                else:
-                    cur_cfg["cache"] = item
-            return cur_cfg["cache"]
-        else:
-            return self._content[key]
-
-    def raw(self, key: str) -> Any:
-        r"""Return the ``key`` of the dataset with un-preprocessed format."""
-        if self.needs_to_load(key):
-            cur_cfg = self._content[key]
-            if self._raw.get(key, None) is None:
-                upon = cur_cfg["upon"]
-                if len(upon) == 0:
-                    return None
-                self.fetch_files(cur_cfg["upon"])
-                file_path_list = [
-                    self.data_root / u["filename"] for u in cur_cfg["upon"]
-                ]
-                if len(file_path_list) == 1:
-                    self._raw[key] = cur_cfg["loader"](file_path_list[0])
-                else:
-                    # here, you should implement a multi-file loader
-                    self._raw[key] = cur_cfg["loader"](file_path_list)
-            return self._raw[key]
-        else:
-            return self._content[key]
-
-    def fetch_files(self, files: List[Dict[str, str]]):
-        r"""Download and check the files if they are not exist.
-
-        Args:
-            ``files`` (``List[Dict[str, str]]``): The files to download, each element
-                in the list is a dict with at lease two keys: ``filename`` and ``md5``.
-                If extra key ``bk_url`` is provided, it will be used to download the
-                file from the backup url.
-        """
-        for file in files:
-            cur_filename = file["filename"]
-            cur_url = file.get("bk_url", None)
-            if cur_url is None:
-                cur_url = self.remote_root + cur_filename
-            download_and_check(cur_url, self.data_root / cur_filename, file["md5"])
+from pathlib import Path
+from typing import Any
+from typing import Dict
+from typing import List
+
+from easygraph.datapipe import compose_pipes
+from easygraph.datasets.hypergraph._global import DATASETS_ROOT
+from easygraph.datasets.hypergraph._global import REMOTE_DATASETS_ROOT
+from easygraph.datasets.utils import download_and_check
+
+
+class BaseData:
+    r"""The Base Class of all datasets.
+
+    ::
+
+        self._content = {
+            'item': {
+                'upon': [
+                    {'filename': 'part1.pkl', 'md5': 'xxxxx',},
+                    {'filename': 'part2.pkl', 'md5': 'xxxxx',},
+                ],
+                'loader': loader_function,
+                'preprocess': [datapipe1, datapipe2],
+            },
+            ...
+        }
+
+    """
+
+    def __init__(self, name: str, data_root=None):
+        # configure the data local/remote root
+        self.name = name
+        if data_root is None:
+            self.data_root = DATASETS_ROOT / name
+        else:
+            self.data_root = Path(data_root) / name
+        self.remote_root = REMOTE_DATASETS_ROOT + name + "/"
+        # init
+        self._content = {}
+        self._raw = {}
+
+    def __repr__(self) -> str:
+        return (
+            f"This is {self.name} dataset:\n"
+            + "\n".join(f"  ->  {k}" for k in self.content)
+            + "\nPlease try `data['name']` to get the specified data."
+        )
+
+    @property
+    def content(self):
+        r"""Return the content of the dataset."""
+        return list(self._content.keys())
+
+    def needs_to_load(self, item_name: str) -> bool:
+        r"""Return whether the ``item_name`` of the dataset needs to be loaded.
+
+        Args:
+            ``item_name`` (``str``): The name of the item in the dataset.
+        """
+        assert item_name in self.content, f"{item_name} is not provided in the Data"
+        return (
+            isinstance(self._content[item_name], dict)
+            and "upon" in self._content[item_name]
+            and "loader" in self._content[item_name]
+        )
+
+    def __getitem__(self, key: str) -> Any:
+        if self.needs_to_load(key):
+            cur_cfg = self._content[key]
+            if cur_cfg.get("cache", None) is None:
+                # get raw data
+                item = self.raw(key)
+                # preprocess and cache
+                pipes = cur_cfg.get("preprocess", None)
+                if pipes is not None:
+                    cur_cfg["cache"] = compose_pipes(*pipes)(item)
+                else:
+                    cur_cfg["cache"] = item
+            return cur_cfg["cache"]
+        else:
+            return self._content[key]
+
+    def raw(self, key: str) -> Any:
+        r"""Return the ``key`` of the dataset with un-preprocessed format."""
+        if self.needs_to_load(key):
+            cur_cfg = self._content[key]
+            if self._raw.get(key, None) is None:
+                upon = cur_cfg["upon"]
+                if len(upon) == 0:
+                    return None
+                self.fetch_files(cur_cfg["upon"])
+                file_path_list = [
+                    self.data_root / u["filename"] for u in cur_cfg["upon"]
+                ]
+                if len(file_path_list) == 1:
+                    self._raw[key] = cur_cfg["loader"](file_path_list[0])
+                else:
+                    # here, you should implement a multi-file loader
+                    self._raw[key] = cur_cfg["loader"](file_path_list)
+            return self._raw[key]
+        else:
+            return self._content[key]
+
+    def fetch_files(self, files: List[Dict[str, str]]):
+        r"""Download and check the files if they are not exist.
+
+        Args:
+            ``files`` (``List[Dict[str, str]]``): The files to download, each element
+                in the list is a dict with at lease two keys: ``filename`` and ``md5``.
+                If extra key ``bk_url`` is provided, it will be used to download the
+                file from the backup url.
+        """
+        for file in files:
+            cur_filename = file["filename"]
+            cur_url = file.get("bk_url", None)
+            if cur_url is None:
+                cur_url = self.remote_root + cur_filename
+            download_and_check(cur_url, self.data_root / cur_filename, file["md5"])
```

## easygraph/datasets/hypergraph/cooking_200.py

 * *Ordering differences only*

```diff
@@ -1,85 +1,85 @@
-from typing import Optional
-
-from easygraph.datapipe import load_from_pickle
-from easygraph.datapipe import to_bool_tensor
-from easygraph.datapipe import to_long_tensor
-from easygraph.datasets.hypergraph.hypergraph_dataset_base import BaseData
-
-
-class Cooking200(BaseData):
-    r"""The Cooking 200 dataset is collected from `Yummly.com <https://www.yummly.com/>`_ for vertex classification task.
-    It is a hypergraph dataset, in which vertex denotes the dish and hyperedge denotes
-    the ingredient. Each dish is also associated with category information, which indicates the dish's cuisine like
-    Chinese, Japanese, French, and Russian.
-
-    The content of the Cooking200 dataset includes the following:
-
-    - ``num_classes``: The number of classes: :math:`20`.
-    - ``num_vertices``: The number of vertices: :math:`7,403`.
-    - ``num_edges``: The number of edges: :math:`2,755`.
-    - ``edge_list``: The edge list. ``List`` with length :math:`(2,755)`.
-    - ``labels``: The label list. ``torch.LongTensor`` with size :math:`(7,403)`.
-    - ``train_mask``: The train mask. ``torch.BoolTensor`` with size :math:`(7,403)`.
-    - ``val_mask``: The validation mask. ``torch.BoolTensor`` with size :math:`(7,403)`.
-    - ``test_mask``: The test mask. ``torch.BoolTensor`` with size :math:`(7,403)`.
-
-    Args:
-        ``data_root`` (``str``, optional): The ``data_root`` has stored the data. If set to ``None``, this function will auto-download from server and save into the default direction ``~/.dhg/datasets/``. Defaults to ``None``.
-    """
-
-    def __init__(self, data_root: Optional[str] = None) -> None:
-        super().__init__("cooking_200", data_root)
-        self._content = {
-            "num_classes": 20,
-            "num_vertices": 7403,
-            "num_edges": 2755,
-            "edge_list": {
-                "upon": [
-                    {
-                        "filename": "edge_list.pkl",
-                        "md5": "2cd32e13dd4e33576c43936542975220",
-                    }
-                ],
-                "loader": load_from_pickle,
-            },
-            "labels": {
-                "upon": [
-                    {
-                        "filename": "labels.pkl",
-                        "md5": "f1f3c0399c9c28547088f44e0bfd5c81",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_long_tensor],
-            },
-            "train_mask": {
-                "upon": [
-                    {
-                        "filename": "train_mask.pkl",
-                        "md5": "66ea36bae024aaaed289e1998fe894bd",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-            "val_mask": {
-                "upon": [
-                    {
-                        "filename": "val_mask.pkl",
-                        "md5": "6c0d3d8b752e3955c64788cc65dcd018",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-            "test_mask": {
-                "upon": [
-                    {
-                        "filename": "test_mask.pkl",
-                        "md5": "0e1564904551ba493e1f8a09d103461e",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-        }
+from typing import Optional
+
+from easygraph.datapipe import load_from_pickle
+from easygraph.datapipe import to_bool_tensor
+from easygraph.datapipe import to_long_tensor
+from easygraph.datasets.hypergraph.hypergraph_dataset_base import BaseData
+
+
+class Cooking200(BaseData):
+    r"""The Cooking 200 dataset is collected from `Yummly.com <https://www.yummly.com/>`_ for vertex classification task.
+    It is a hypergraph dataset, in which vertex denotes the dish and hyperedge denotes
+    the ingredient. Each dish is also associated with category information, which indicates the dish's cuisine like
+    Chinese, Japanese, French, and Russian.
+
+    The content of the Cooking200 dataset includes the following:
+
+    - ``num_classes``: The number of classes: :math:`20`.
+    - ``num_vertices``: The number of vertices: :math:`7,403`.
+    - ``num_edges``: The number of edges: :math:`2,755`.
+    - ``edge_list``: The edge list. ``List`` with length :math:`(2,755)`.
+    - ``labels``: The label list. ``torch.LongTensor`` with size :math:`(7,403)`.
+    - ``train_mask``: The train mask. ``torch.BoolTensor`` with size :math:`(7,403)`.
+    - ``val_mask``: The validation mask. ``torch.BoolTensor`` with size :math:`(7,403)`.
+    - ``test_mask``: The test mask. ``torch.BoolTensor`` with size :math:`(7,403)`.
+
+    Args:
+        ``data_root`` (``str``, optional): The ``data_root`` has stored the data. If set to ``None``, this function will auto-download from server and save into the default direction ``~/.dhg/datasets/``. Defaults to ``None``.
+    """
+
+    def __init__(self, data_root: Optional[str] = None) -> None:
+        super().__init__("cooking_200", data_root)
+        self._content = {
+            "num_classes": 20,
+            "num_vertices": 7403,
+            "num_edges": 2755,
+            "edge_list": {
+                "upon": [
+                    {
+                        "filename": "edge_list.pkl",
+                        "md5": "2cd32e13dd4e33576c43936542975220",
+                    }
+                ],
+                "loader": load_from_pickle,
+            },
+            "labels": {
+                "upon": [
+                    {
+                        "filename": "labels.pkl",
+                        "md5": "f1f3c0399c9c28547088f44e0bfd5c81",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_long_tensor],
+            },
+            "train_mask": {
+                "upon": [
+                    {
+                        "filename": "train_mask.pkl",
+                        "md5": "66ea36bae024aaaed289e1998fe894bd",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+            "val_mask": {
+                "upon": [
+                    {
+                        "filename": "val_mask.pkl",
+                        "md5": "6c0d3d8b752e3955c64788cc65dcd018",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+            "test_mask": {
+                "upon": [
+                    {
+                        "filename": "test_mask.pkl",
+                        "md5": "0e1564904551ba493e1f8a09d103461e",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+        }
```

## easygraph/datasets/hypergraph/trivago_clicks.py

 * *Ordering differences only*

```diff
@@ -1,104 +1,104 @@
-import requests
-
-from easygraph.utils.exception import EasyGraphError
-
-
-def request_text_from_url(url):
-    try:
-        r = requests.get(url)
-    except requests.ConnectionError:
-        raise EasyGraphError("Connection Error!")
-
-    if r.ok:
-        return r.text
-    else:
-        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
-
-
-class trivago_clicks:
-    def __init__(self, data_root=None):
-        self.data_root = "https://" if data_root is not None else data_root
-        self.hyperedges_path = "https://gitlab.com/easy-graph/easygraph-data-trivago-clicks/-/raw/main/hyperedges-trivago-clicks.txt?inline=false"
-        self.node_labels_path = "https://gitlab.com/easy-graph/easygraph-data-trivago-clicks/-/raw/main/node-labels-trivago-clicks.txt?ref_type=heads&inline=false"
-        # self.node_names_path = "https://gitlab.com/easy-graph/easygraph-data-trivago-clicks/-/raw/main/node-names-house-committees.txt?ref_type=heads&inline=false"
-        self.label_names_path = "https://gitlab.com/easy-graph/easygraph-data-trivago-clicks/-/raw/main/label-names-trivago-clicks.txt?ref_type=heads&inline=false"
-        self._hyperedges = []
-        self._node_labels = []
-        self._label_names = []
-        self._node_names = []
-        self.generate_hypergraph(
-            hyperedges_path=self.hyperedges_path,
-            node_labels_path=self.node_labels_path,
-            # node_names_path=self.node_names_path,
-            label_names_path=self.label_names_path,
-        )
-        self._content = {
-            "num_classes": len(self._label_names),
-            "num_vertices": len(self._node_labels),
-            "num_edges": len(self._hyperedges),
-            "edge_list": self._hyperedges,
-            "labels": self._node_labels,
-        }
-
-    def __getitem__(self, key: str):
-        return self._content[key]
-
-    def process_label_txt(self, data_str, delimiter="\n", transform_fun=str):
-        data_str = data_str.strip()
-        data_lst = data_str.split(delimiter)
-        final_lst = []
-        for data in data_lst:
-            data = data.strip()
-            data = transform_fun(data)
-            final_lst.append(data)
-        return final_lst
-
-    @property
-    def node_labels(self):
-        return self._node_labels
-
-    """
-    @property
-    def node_names(self):
-        return self._node_names
-    """
-
-    @property
-    def label_names(self):
-        return self._label_names
-
-    @property
-    def hyperedges(self):
-        return self._hyperedges
-
-    def generate_hypergraph(
-        self,
-        hyperedges_path=None,
-        node_labels_path=None,
-        # node_names_path=None,
-        label_names_path=None,
-    ):
-        def fun(data):
-            data = int(data) - 1
-            return data
-
-        hyperedges_info = request_text_from_url(hyperedges_path)
-        hyperedges_info = hyperedges_info.strip()
-        hyperedges_lst = hyperedges_info.split("\n")
-        for hyperedge in hyperedges_lst:
-            hyperedge = hyperedge.strip()
-            hyperedge = [int(i) - 1 for i in hyperedge.split(",")]
-            self._hyperedges.append(tuple(hyperedge))
-        # print(self.hyperedges)
-
-        node_labels_info = request_text_from_url(node_labels_path)
-
-        process_node_labels_info = self.process_label_txt(
-            node_labels_info, transform_fun=fun
-        )
-        self._node_labels = process_node_labels_info
-        # print("process_node_labels_info:", process_node_labels_info)
-        # print("process_node_names_info:", process_node_names_info)
-        label_names_info = request_text_from_url(label_names_path)
-        process_label_names_info = self.process_label_txt(label_names_info)
-        self._label_names = process_label_names_info
+import requests
+
+from easygraph.utils.exception import EasyGraphError
+
+
+def request_text_from_url(url):
+    try:
+        r = requests.get(url)
+    except requests.ConnectionError:
+        raise EasyGraphError("Connection Error!")
+
+    if r.ok:
+        return r.text
+    else:
+        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
+
+
+class trivago_clicks:
+    def __init__(self, data_root=None):
+        self.data_root = "https://" if data_root is not None else data_root
+        self.hyperedges_path = "https://gitlab.com/easy-graph/easygraph-data-trivago-clicks/-/raw/main/hyperedges-trivago-clicks.txt?inline=false"
+        self.node_labels_path = "https://gitlab.com/easy-graph/easygraph-data-trivago-clicks/-/raw/main/node-labels-trivago-clicks.txt?ref_type=heads&inline=false"
+        # self.node_names_path = "https://gitlab.com/easy-graph/easygraph-data-trivago-clicks/-/raw/main/node-names-house-committees.txt?ref_type=heads&inline=false"
+        self.label_names_path = "https://gitlab.com/easy-graph/easygraph-data-trivago-clicks/-/raw/main/label-names-trivago-clicks.txt?ref_type=heads&inline=false"
+        self._hyperedges = []
+        self._node_labels = []
+        self._label_names = []
+        self._node_names = []
+        self.generate_hypergraph(
+            hyperedges_path=self.hyperedges_path,
+            node_labels_path=self.node_labels_path,
+            # node_names_path=self.node_names_path,
+            label_names_path=self.label_names_path,
+        )
+        self._content = {
+            "num_classes": len(self._label_names),
+            "num_vertices": len(self._node_labels),
+            "num_edges": len(self._hyperedges),
+            "edge_list": self._hyperedges,
+            "labels": self._node_labels,
+        }
+
+    def __getitem__(self, key: str):
+        return self._content[key]
+
+    def process_label_txt(self, data_str, delimiter="\n", transform_fun=str):
+        data_str = data_str.strip()
+        data_lst = data_str.split(delimiter)
+        final_lst = []
+        for data in data_lst:
+            data = data.strip()
+            data = transform_fun(data)
+            final_lst.append(data)
+        return final_lst
+
+    @property
+    def node_labels(self):
+        return self._node_labels
+
+    """
+    @property
+    def node_names(self):
+        return self._node_names
+    """
+
+    @property
+    def label_names(self):
+        return self._label_names
+
+    @property
+    def hyperedges(self):
+        return self._hyperedges
+
+    def generate_hypergraph(
+        self,
+        hyperedges_path=None,
+        node_labels_path=None,
+        # node_names_path=None,
+        label_names_path=None,
+    ):
+        def fun(data):
+            data = int(data) - 1
+            return data
+
+        hyperedges_info = request_text_from_url(hyperedges_path)
+        hyperedges_info = hyperedges_info.strip()
+        hyperedges_lst = hyperedges_info.split("\n")
+        for hyperedge in hyperedges_lst:
+            hyperedge = hyperedge.strip()
+            hyperedge = [int(i) - 1 for i in hyperedge.split(",")]
+            self._hyperedges.append(tuple(hyperedge))
+        # print(self.hyperedges)
+
+        node_labels_info = request_text_from_url(node_labels_path)
+
+        process_node_labels_info = self.process_label_txt(
+            node_labels_info, transform_fun=fun
+        )
+        self._node_labels = process_node_labels_info
+        # print("process_node_labels_info:", process_node_labels_info)
+        # print("process_node_names_info:", process_node_names_info)
+        label_names_info = request_text_from_url(label_names_path)
+        process_label_names_info = self.process_label_txt(label_names_info)
+        self._label_names = process_label_names_info
```

## easygraph/datasets/hypergraph/__init__.py

 * *Ordering differences only*

```diff
@@ -1,9 +1,9 @@
-from .cat_edge_Cooking import *
-from .cocitation import *
-from .contact_primary_school import *
-from .House_Committees import *
-from .mathoverflow_answers import *
-from .senate_committees import *
-from .trivago_clicks import *
-from .walmart_trips import *
-from .Yelp import *
+from .cat_edge_Cooking import *
+from .cocitation import *
+from .contact_primary_school import *
+from .House_Committees import *
+from .mathoverflow_answers import *
+from .senate_committees import *
+from .trivago_clicks import *
+from .walmart_trips import *
+from .Yelp import *
```

## easygraph/datasets/hypergraph/coauthorship.py

 * *Ordering differences only*

```diff
@@ -1,189 +1,189 @@
-from functools import partial
-from typing import Optional
-
-from easygraph.datapipe import load_from_pickle
-from easygraph.datapipe import norm_ft
-from easygraph.datapipe import to_bool_tensor
-from easygraph.datapipe import to_long_tensor
-from easygraph.datapipe import to_tensor
-from easygraph.datasets.hypergraph.hypergraph_dataset_base import BaseData
-
-
-class CoauthorshipCora(BaseData):
-    r"""The Co-authorship Cora dataset is a citation network dataset for vertex classification task.
-    More details see the `HyperGCN <https://papers.nips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf>`_ paper.
-
-    The content of the Co-authorship Cora dataset includes the following:
-
-    - ``num_classes``: The number of classes: :math:`7`.
-    - ``num_vertices``: The number of vertices: :math:`2,708`.
-    - ``num_edges``: The number of edges: :math:`1,072`.
-    - ``dim_features``: The dimension of features: :math:`1,433`.
-    - ``features``: The vertex feature matrix. ``torch.Tensor`` with size :math:`(2,708 \times 1,433)`.
-    - ``edge_list``: The edge list. ``List`` with length :math:`1,072`.
-    - ``labels``: The label list. ``torch.LongTensor`` with size :math:`(2,708, )`.
-    - ``train_mask``: The train mask. ``torch.BoolTensor`` with size :math:`(2,708, )`.
-    - ``val_mask``: The validation mask. ``torch.BoolTensor`` with size :math:`(2,708, )`.
-    - ``test_mask``: The test mask. ``torch.BoolTensor`` with size :math:`(2,708, )`.
-
-    Args:
-        ``data_root`` (``str``, optional): The ``data_root`` has stored the data. If set to ``None``, this function will auto-download from server and save into the default direction ``~/.dhg/datasets/``. Defaults to ``None``.
-    """
-
-    def __init__(self, data_root: Optional[str] = None) -> None:
-        super().__init__("coauthorship_cora", data_root)
-        self._content = {
-            "num_classes": 7,
-            "num_vertices": 2708,
-            "num_edges": 1072,
-            "dim_features": 1433,
-            "features": {
-                "upon": [
-                    {
-                        "filename": "features.pkl",
-                        "md5": "14257c0e24b4eb741b469a351e524785",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_tensor, partial(norm_ft, ord=1)],
-            },
-            "edge_list": {
-                "upon": [
-                    {
-                        "filename": "edge_list.pkl",
-                        "md5": "a17ff337f1b9099f5a9d4d670674e146",
-                    }
-                ],
-                "loader": load_from_pickle,
-            },
-            "labels": {
-                "upon": [
-                    {
-                        "filename": "labels.pkl",
-                        "md5": "c8d11c452e0be69f79a47dd839279117",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_long_tensor],
-            },
-            "train_mask": {
-                "upon": [
-                    {
-                        "filename": "train_mask.pkl",
-                        "md5": "111db6c6f986be2908378df7bdca7a9b",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-            "val_mask": {
-                "upon": [
-                    {
-                        "filename": "val_mask.pkl",
-                        "md5": "ffab1055193ffb2fe74822bb575d332a",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-            "test_mask": {
-                "upon": [
-                    {
-                        "filename": "test_mask.pkl",
-                        "md5": "ffab1055193ffb2fe74822bb575d332a",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-        }
-
-
-class CoauthorshipDBLP(BaseData):
-    r"""The Co-authorship DBLP dataset is a citation network dataset for vertex classification task.
-    More details see the `HyperGCN <https://papers.nips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf>`_ paper.
-
-    The content of the Co-authorship DBLP dataset includes the following:
-
-    - ``num_classes``: The number of classes: :math:`6`.
-    - ``num_vertices``: The number of vertices: :math:`41,302`.
-    - ``num_edges``: The number of edges: :math:`22,363`.
-    - ``dim_features``: The dimension of features: :math:`1,425`.
-    - ``features``: The vertex feature matrix. ``torch.Tensor`` with size :math:`(41,302 \times 1,425)`.
-    - ``edge_list``: The edge list. ``List`` with length :math:`22,363`.
-    - ``labels``: The label list. ``torch.LongTensor`` with size :math:`(41,302, )`.
-    - ``train_mask``: The train mask. ``torch.BoolTensor`` with size :math:`(41,302, )`.
-    - ``val_mask``: The validation mask. ``torch.BoolTensor`` with size :math:`(41,302, )`.
-    - ``test_mask``: The test mask. ``torch.BoolTensor`` with size :math:`(41,302, )`.
-
-    Args:
-        ``data_root`` (``str``, optional): The ``data_root`` has stored the data. If set to ``None``, this function will auto-download from server and save into the default direction ``~/.dhg/datasets/``. Defaults to None.
-    """
-
-    def __init__(self, data_root: Optional[str] = None) -> None:
-        super().__init__("coauthorship_dblp", data_root)
-        self._content = {
-            "num_classes": 6,
-            "num_vertices": 41302,
-            "num_edges": 22363,
-            "dim_features": 1425,
-            "features": {
-                "upon": [
-                    {
-                        "filename": "features.pkl",
-                        "md5": "b78fd31b2586d1e19a40b3f6cd9cc2e7",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_tensor, partial(norm_ft, ord=1)],
-            },
-            "edge_list": {
-                "upon": [
-                    {
-                        "filename": "edge_list.pkl",
-                        "md5": "c6bf5f9f3b9683bcc9b7bcc9eb8707d8",
-                    }
-                ],
-                "loader": load_from_pickle,
-            },
-            "labels": {
-                "upon": [
-                    {
-                        "filename": "labels.pkl",
-                        "md5": "2e7a792ea018028d582af8f02f2058ca",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_long_tensor],
-            },
-            "train_mask": {
-                "upon": [
-                    {
-                        "filename": "train_mask.pkl",
-                        "md5": "a842b795c7cac4c2f98a56cf599bc1de",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-            "val_mask": {
-                "upon": [
-                    {
-                        "filename": "val_mask.pkl",
-                        "md5": "2ec4b7df7c5e6b355067a22c391ad578",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-            "test_mask": {
-                "upon": [
-                    {
-                        "filename": "test_mask.pkl",
-                        "md5": "2ec4b7df7c5e6b355067a22c391ad578",
-                    }
-                ],
-                "loader": load_from_pickle,
-                "preprocess": [to_bool_tensor],
-            },
-        }
+from functools import partial
+from typing import Optional
+
+from easygraph.datapipe import load_from_pickle
+from easygraph.datapipe import norm_ft
+from easygraph.datapipe import to_bool_tensor
+from easygraph.datapipe import to_long_tensor
+from easygraph.datapipe import to_tensor
+from easygraph.datasets.hypergraph.hypergraph_dataset_base import BaseData
+
+
+class CoauthorshipCora(BaseData):
+    r"""The Co-authorship Cora dataset is a citation network dataset for vertex classification task.
+    More details see the `HyperGCN <https://papers.nips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf>`_ paper.
+
+    The content of the Co-authorship Cora dataset includes the following:
+
+    - ``num_classes``: The number of classes: :math:`7`.
+    - ``num_vertices``: The number of vertices: :math:`2,708`.
+    - ``num_edges``: The number of edges: :math:`1,072`.
+    - ``dim_features``: The dimension of features: :math:`1,433`.
+    - ``features``: The vertex feature matrix. ``torch.Tensor`` with size :math:`(2,708 \times 1,433)`.
+    - ``edge_list``: The edge list. ``List`` with length :math:`1,072`.
+    - ``labels``: The label list. ``torch.LongTensor`` with size :math:`(2,708, )`.
+    - ``train_mask``: The train mask. ``torch.BoolTensor`` with size :math:`(2,708, )`.
+    - ``val_mask``: The validation mask. ``torch.BoolTensor`` with size :math:`(2,708, )`.
+    - ``test_mask``: The test mask. ``torch.BoolTensor`` with size :math:`(2,708, )`.
+
+    Args:
+        ``data_root`` (``str``, optional): The ``data_root`` has stored the data. If set to ``None``, this function will auto-download from server and save into the default direction ``~/.dhg/datasets/``. Defaults to ``None``.
+    """
+
+    def __init__(self, data_root: Optional[str] = None) -> None:
+        super().__init__("coauthorship_cora", data_root)
+        self._content = {
+            "num_classes": 7,
+            "num_vertices": 2708,
+            "num_edges": 1072,
+            "dim_features": 1433,
+            "features": {
+                "upon": [
+                    {
+                        "filename": "features.pkl",
+                        "md5": "14257c0e24b4eb741b469a351e524785",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_tensor, partial(norm_ft, ord=1)],
+            },
+            "edge_list": {
+                "upon": [
+                    {
+                        "filename": "edge_list.pkl",
+                        "md5": "a17ff337f1b9099f5a9d4d670674e146",
+                    }
+                ],
+                "loader": load_from_pickle,
+            },
+            "labels": {
+                "upon": [
+                    {
+                        "filename": "labels.pkl",
+                        "md5": "c8d11c452e0be69f79a47dd839279117",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_long_tensor],
+            },
+            "train_mask": {
+                "upon": [
+                    {
+                        "filename": "train_mask.pkl",
+                        "md5": "111db6c6f986be2908378df7bdca7a9b",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+            "val_mask": {
+                "upon": [
+                    {
+                        "filename": "val_mask.pkl",
+                        "md5": "ffab1055193ffb2fe74822bb575d332a",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+            "test_mask": {
+                "upon": [
+                    {
+                        "filename": "test_mask.pkl",
+                        "md5": "ffab1055193ffb2fe74822bb575d332a",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+        }
+
+
+class CoauthorshipDBLP(BaseData):
+    r"""The Co-authorship DBLP dataset is a citation network dataset for vertex classification task.
+    More details see the `HyperGCN <https://papers.nips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf>`_ paper.
+
+    The content of the Co-authorship DBLP dataset includes the following:
+
+    - ``num_classes``: The number of classes: :math:`6`.
+    - ``num_vertices``: The number of vertices: :math:`41,302`.
+    - ``num_edges``: The number of edges: :math:`22,363`.
+    - ``dim_features``: The dimension of features: :math:`1,425`.
+    - ``features``: The vertex feature matrix. ``torch.Tensor`` with size :math:`(41,302 \times 1,425)`.
+    - ``edge_list``: The edge list. ``List`` with length :math:`22,363`.
+    - ``labels``: The label list. ``torch.LongTensor`` with size :math:`(41,302, )`.
+    - ``train_mask``: The train mask. ``torch.BoolTensor`` with size :math:`(41,302, )`.
+    - ``val_mask``: The validation mask. ``torch.BoolTensor`` with size :math:`(41,302, )`.
+    - ``test_mask``: The test mask. ``torch.BoolTensor`` with size :math:`(41,302, )`.
+
+    Args:
+        ``data_root`` (``str``, optional): The ``data_root`` has stored the data. If set to ``None``, this function will auto-download from server and save into the default direction ``~/.dhg/datasets/``. Defaults to None.
+    """
+
+    def __init__(self, data_root: Optional[str] = None) -> None:
+        super().__init__("coauthorship_dblp", data_root)
+        self._content = {
+            "num_classes": 6,
+            "num_vertices": 41302,
+            "num_edges": 22363,
+            "dim_features": 1425,
+            "features": {
+                "upon": [
+                    {
+                        "filename": "features.pkl",
+                        "md5": "b78fd31b2586d1e19a40b3f6cd9cc2e7",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_tensor, partial(norm_ft, ord=1)],
+            },
+            "edge_list": {
+                "upon": [
+                    {
+                        "filename": "edge_list.pkl",
+                        "md5": "c6bf5f9f3b9683bcc9b7bcc9eb8707d8",
+                    }
+                ],
+                "loader": load_from_pickle,
+            },
+            "labels": {
+                "upon": [
+                    {
+                        "filename": "labels.pkl",
+                        "md5": "2e7a792ea018028d582af8f02f2058ca",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_long_tensor],
+            },
+            "train_mask": {
+                "upon": [
+                    {
+                        "filename": "train_mask.pkl",
+                        "md5": "a842b795c7cac4c2f98a56cf599bc1de",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+            "val_mask": {
+                "upon": [
+                    {
+                        "filename": "val_mask.pkl",
+                        "md5": "2ec4b7df7c5e6b355067a22c391ad578",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+            "test_mask": {
+                "upon": [
+                    {
+                        "filename": "test_mask.pkl",
+                        "md5": "2ec4b7df7c5e6b355067a22c391ad578",
+                    }
+                ],
+                "loader": load_from_pickle,
+                "preprocess": [to_bool_tensor],
+            },
+        }
```

## easygraph/datasets/dynamic/hospital_lyon.py

 * *Ordering differences only*

```diff
@@ -1,132 +1,132 @@
-import json
-import os
-
-from easygraph.classes.hypergraph import Hypergraph
-from easygraph.datasets.dynamic.load_dataset import request_json_from_url
-from easygraph.datasets.graph_dataset_base import EasyGraphDataset
-from easygraph.datasets.utils import _get_eg_url
-from easygraph.datasets.utils import tensor
-
-
-class Hospital_Lyon(EasyGraphDataset):
-    _urls = {
-        "hospital_lyon": "easygraph-data-hospital-lyon/-/raw/main/hospital-lyon.json?ref_type=heads&inline=false",
-    }
-
-    def __init__(
-        self,
-        raw_dir=None,
-        force_reload=False,
-        verbose=True,
-        transform=None,
-        save_dir="./",
-    ):
-        name = "hospital_lyon"
-        self.url = _get_eg_url(self._urls[name])
-        super(Hospital_Lyon, self).__init__(
-            name=name,
-            url=self.url,
-            raw_dir=raw_dir,
-            force_reload=force_reload,
-            verbose=verbose,
-            transform=transform,
-            save_dir=save_dir,
-        )
-
-    def preprocess(self, data, max_order=None, is_dynamic=True):
-        # The index of the nodes in this dataset are not continuous and therefore require special processing
-        timestamp_lst = list()
-        node_data = data["node-data"]
-        node_num = len(node_data)
-        G = Hypergraph(num_v=node_num)
-        id = 0
-        name_dict = {}
-        for k, v in data["node-data"].items():
-            name_dict[k] = id
-            v["name"] = k
-            G.v_property[id] = v
-            id = id + 1
-        e_property_dict = data["edge-data"]
-        rows = []
-        cols = []
-        edge_flag_dict = {}
-        edge_id = 0
-        for id, edge in data["edge-dict"].items():
-            if max_order and len(edge) > max_order + 1:
-                continue
-
-            try:
-                id = int(id)
-            except ValueError as e:
-                raise TypeError(
-                    f"Failed to convert the edge with ID {id} to type int."
-                ) from e
-
-            try:
-                edge = [name_dict[n] for n in edge]
-                rows.extend(edge)
-                cols.extend(len(edge) * [edge_id])
-                edge_id += 1
-            except ValueError as e:
-                raise TypeError(f"Failed to convert nodes to type int.") from e
-            if is_dynamic:
-                G.add_hyperedges(
-                    e_list=edge,
-                    e_property=e_property_dict[str(id)],
-                    group_name=e_property_dict[str(id)]["timestamp"],
-                )
-                timestamp_lst.append(e_property_dict[str(id)]["timestamp"])
-            else:
-                G.add_hyperedges(e_list=edge, e_property=e_property_dict[str(id)])
-        G._rows = rows
-        G._cols = cols
-        return G, timestamp_lst
-
-    @property
-    def url(self):
-        return self._url
-
-    @property
-    def save_name(self):
-        return self.name
-
-    def __getitem__(self, idx):
-        assert idx == 0, "This dataset has only one graph"
-        if self._transform is None:
-            return self._g
-        else:
-            return self._transform(self._g)
-
-    def load(self):
-        graph_path = os.path.join(self.save_path, self.save_name + ".json")
-        with open(graph_path, "r") as f:
-            self.load_data = json.load(f)
-
-    def has_cache(self):
-        graph_path = os.path.join(self.save_path, self.save_name + ".json")
-        if os.path.exists(graph_path):
-            return True
-        return False
-
-    def download(self):
-        if self.has_cache():
-            self.load()
-        else:
-            root = self.raw_dir
-            data = request_json_from_url(self.url)
-            with open(os.path.join(root, self.save_name + ".json"), "w") as f:
-                json.dump(data, f)
-            self.load_data = data
-
-    def process(self):
-        """Loads input data from data directory and transfer to target graph for better analysis
-        """
-
-        self._g, edge_feature_list = self.preprocess(self.load_data, is_dynamic=True)
-        self._g.ndata["hyperedge_feature"] = tensor(
-            range(1, len(edge_feature_list) + 1)
-        )
-
-    @url.setter
-    def url(self, value):
-        self._url = value
+import json
+import os
+
+from easygraph.classes.hypergraph import Hypergraph
+from easygraph.datasets.dynamic.load_dataset import request_json_from_url
+from easygraph.datasets.graph_dataset_base import EasyGraphDataset
+from easygraph.datasets.utils import _get_eg_url
+from easygraph.datasets.utils import tensor
+
+
+class Hospital_Lyon(EasyGraphDataset):
+    _urls = {
+        "hospital_lyon": "easygraph-data-hospital-lyon/-/raw/main/hospital-lyon.json?ref_type=heads&inline=false",
+    }
+
+    def __init__(
+        self,
+        raw_dir=None,
+        force_reload=False,
+        verbose=True,
+        transform=None,
+        save_dir="./",
+    ):
+        name = "hospital_lyon"
+        self.url = _get_eg_url(self._urls[name])
+        super(Hospital_Lyon, self).__init__(
+            name=name,
+            url=self.url,
+            raw_dir=raw_dir,
+            force_reload=force_reload,
+            verbose=verbose,
+            transform=transform,
+            save_dir=save_dir,
+        )
+
+    def preprocess(self, data, max_order=None, is_dynamic=True):
+        # The index of the nodes in this dataset are not continuous and therefore require special processing
+        timestamp_lst = list()
+        node_data = data["node-data"]
+        node_num = len(node_data)
+        G = Hypergraph(num_v=node_num)
+        id = 0
+        name_dict = {}
+        for k, v in data["node-data"].items():
+            name_dict[k] = id
+            v["name"] = k
+            G.v_property[id] = v
+            id = id + 1
+        e_property_dict = data["edge-data"]
+        rows = []
+        cols = []
+        edge_flag_dict = {}
+        edge_id = 0
+        for id, edge in data["edge-dict"].items():
+            if max_order and len(edge) > max_order + 1:
+                continue
+
+            try:
+                id = int(id)
+            except ValueError as e:
+                raise TypeError(
+                    f"Failed to convert the edge with ID {id} to type int."
+                ) from e
+
+            try:
+                edge = [name_dict[n] for n in edge]
+                rows.extend(edge)
+                cols.extend(len(edge) * [edge_id])
+                edge_id += 1
+            except ValueError as e:
+                raise TypeError(f"Failed to convert nodes to type int.") from e
+            if is_dynamic:
+                G.add_hyperedges(
+                    e_list=edge,
+                    e_property=e_property_dict[str(id)],
+                    group_name=e_property_dict[str(id)]["timestamp"],
+                )
+                timestamp_lst.append(e_property_dict[str(id)]["timestamp"])
+            else:
+                G.add_hyperedges(e_list=edge, e_property=e_property_dict[str(id)])
+        G._rows = rows
+        G._cols = cols
+        return G, timestamp_lst
+
+    @property
+    def url(self):
+        return self._url
+
+    @property
+    def save_name(self):
+        return self.name
+
+    def __getitem__(self, idx):
+        assert idx == 0, "This dataset has only one graph"
+        if self._transform is None:
+            return self._g
+        else:
+            return self._transform(self._g)
+
+    def load(self):
+        graph_path = os.path.join(self.save_path, self.save_name + ".json")
+        with open(graph_path, "r") as f:
+            self.load_data = json.load(f)
+
+    def has_cache(self):
+        graph_path = os.path.join(self.save_path, self.save_name + ".json")
+        if os.path.exists(graph_path):
+            return True
+        return False
+
+    def download(self):
+        if self.has_cache():
+            self.load()
+        else:
+            root = self.raw_dir
+            data = request_json_from_url(self.url)
+            with open(os.path.join(root, self.save_name + ".json"), "w") as f:
+                json.dump(data, f)
+            self.load_data = data
+
+    def process(self):
+        """Loads input data from data directory and transfer to target graph for better analysis
+        """
+
+        self._g, edge_feature_list = self.preprocess(self.load_data, is_dynamic=True)
+        self._g.ndata["hyperedge_feature"] = tensor(
+            range(1, len(edge_feature_list) + 1)
+        )
+
+    @url.setter
+    def url(self, value):
+        self._url = value
```

## easygraph/datasets/dynamic/load_dataset.py

 * *Ordering differences only*

```diff
@@ -1,94 +1,94 @@
-import json
-import os
-
-from warnings import warn
-
-import requests
-
-from easygraph.convert import dict_to_hypergraph
-from easygraph.utils.exception import EasyGraphError
-
-
-__all__ = [
-    "load_dynamic_hypergraph_dataset",
-]
-
-dataset_index_url = "https://gitlab.com/easy-graph/easygraph-data/-/raw/main/dataset_index.json?inline=false"
-
-
-def request_json_from_url(url):
-    try:
-        r = requests.get(url)
-    except requests.ConnectionError:
-        raise EasyGraphError("Connection Error!")
-
-    if r.ok:
-        return r.json()
-    else:
-        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
-
-
-def _request_from_eg_data(dataset=None, cache=True):
-    """Request a dataset from eg-data.
-
-    Parameters
-    ----------
-    dataset : str, optional
-        Dataset name. Valid options are the top-level tags of the
-        index.json file in the xgi-data repository. If None, prints
-        the list of available datasets.
-    cache : bool, optional
-        Whether or not to cache the output
-
-    Returns
-    -------
-    Data
-        The requested data loaded from a json file.
-
-    Raises
-    ------
-    EasyGraphError
-        If the HTTP request is not successful or the dataset does not exist.
-
-
-    """
-
-    index_data = request_json_from_url(dataset_index_url)
-
-    key = dataset.lower()
-    if key not in index_data:
-        print("Valid dataset names:")
-        print(*index_data, sep="\n")
-        raise EasyGraphError("Must choose a valid dataset name!")
-
-    return request_json_from_url(index_data[key]["url"])
-
-
-def load_dynamic_hypergraph_dataset(
-    dataset=None,
-    local_read=False,
-    path="",
-    max_order=None,
-):
-    index_datasets = request_json_from_url(dataset_index_url)
-    if dataset is None:
-        print("Please refer to available list")
-
-        print(*index_datasets, sep="\n")
-        return
-
-    if local_read:
-        cfp = os.path.join(path, dataset + ".json")
-        if os.path.exists(cfp):
-            data = json.load(open(cfp, "r"))
-            return dict_to_hypergraph(data, max_order=max_order)
-        else:
-            warn(
-                f"No local copy was found at {cfp}. The data is requested "
-                "from the xgi-data repository instead. To download a local "
-                "copy, use `download_xgi_data`."
-            )
-    data = _request_from_eg_data(dataset)
-    return dict_to_hypergraph(
-        data, max_order=max_order, is_dynamic=index_datasets[dataset]["is_dynamic"]
-    )
+import json
+import os
+
+from warnings import warn
+
+import requests
+
+from easygraph.convert import dict_to_hypergraph
+from easygraph.utils.exception import EasyGraphError
+
+
+__all__ = [
+    "load_dynamic_hypergraph_dataset",
+]
+
+dataset_index_url = "https://gitlab.com/easy-graph/easygraph-data/-/raw/main/dataset_index.json?inline=false"
+
+
+def request_json_from_url(url):
+    try:
+        r = requests.get(url)
+    except requests.ConnectionError:
+        raise EasyGraphError("Connection Error!")
+
+    if r.ok:
+        return r.json()
+    else:
+        raise EasyGraphError(f"Error: HTTP response {r.status_code}")
+
+
+def _request_from_eg_data(dataset=None, cache=True):
+    """Request a dataset from eg-data.
+
+    Parameters
+    ----------
+    dataset : str, optional
+        Dataset name. Valid options are the top-level tags of the
+        index.json file in the xgi-data repository. If None, prints
+        the list of available datasets.
+    cache : bool, optional
+        Whether or not to cache the output
+
+    Returns
+    -------
+    Data
+        The requested data loaded from a json file.
+
+    Raises
+    ------
+    EasyGraphError
+        If the HTTP request is not successful or the dataset does not exist.
+
+
+    """
+
+    index_data = request_json_from_url(dataset_index_url)
+
+    key = dataset.lower()
+    if key not in index_data:
+        print("Valid dataset names:")
+        print(*index_data, sep="\n")
+        raise EasyGraphError("Must choose a valid dataset name!")
+
+    return request_json_from_url(index_data[key]["url"])
+
+
+def load_dynamic_hypergraph_dataset(
+    dataset=None,
+    local_read=False,
+    path="",
+    max_order=None,
+):
+    index_datasets = request_json_from_url(dataset_index_url)
+    if dataset is None:
+        print("Please refer to available list")
+
+        print(*index_datasets, sep="\n")
+        return
+
+    if local_read:
+        cfp = os.path.join(path, dataset + ".json")
+        if os.path.exists(cfp):
+            data = json.load(open(cfp, "r"))
+            return dict_to_hypergraph(data, max_order=max_order)
+        else:
+            warn(
+                f"No local copy was found at {cfp}. The data is requested "
+                "from the xgi-data repository instead. To download a local "
+                "copy, use `download_xgi_data`."
+            )
+    data = _request_from_eg_data(dataset)
+    return dict_to_hypergraph(
+        data, max_order=max_order, is_dynamic=index_datasets[dataset]["is_dynamic"]
+    )
```

## easygraph/datasets/dynamic/email_enron.py

 * *Ordering differences only*

```diff
@@ -1,87 +1,87 @@
-import json
-import os
-
-from easygraph.convert import dict_to_hypergraph
-from easygraph.datasets.dynamic.load_dataset import request_json_from_url
-from easygraph.datasets.graph_dataset_base import EasyGraphDataset
-from easygraph.datasets.utils import _get_eg_url
-from easygraph.datasets.utils import tensor
-
-
-class Email_Enron(EasyGraphDataset):
-    _urls = {
-        "email-enron": (
-            "easygraph-data-email-enron/-/raw/main/email-enron.json?inline=false"
-        ),
-        "email-eu": "easygraph-data-email-eu/-/raw/main/email-eu.json?inline=false",
-    }
-
-    def __init__(
-        self,
-        raw_dir=None,
-        force_reload=False,
-        verbose=True,
-        transform=None,
-        save_dir="./",
-    ):
-        name = "email-enron"
-        self.url = _get_eg_url(self._urls[name])
-        super(Email_Enron, self).__init__(
-            name=name,
-            url=self.url,
-            raw_dir=raw_dir,
-            force_reload=force_reload,
-            verbose=verbose,
-            transform=transform,
-            save_dir=save_dir,
-        )
-
-    @property
-    def url(self):
-        return self._url
-
-    @property
-    def save_name(self):
-        return self.name
-
-    def __getitem__(self, idx):
-        assert idx == 0, "This dataset has only one graph"
-        if self._transform is None:
-            return self._g
-        else:
-            return self._transform(self._g)
-
-    def load(self):
-        graph_path = os.path.join(self.save_path, self.save_name + ".json")
-        with open(graph_path, "r") as f:
-            self.load_data = json.load(f)
-
-    def has_cache(self):
-        graph_path = os.path.join(self.save_path, self.save_name + ".json")
-        if os.path.exists(graph_path):
-            return True
-        return False
-
-    def download(self):
-        if self.has_cache():
-            self.load()
-        else:
-            root = self.raw_dir
-            data = request_json_from_url(self.url)
-            with open(os.path.join(root, self.save_name + ".json"), "w") as f:
-                json.dump(data, f)
-            self.load_data = data
-
-    def process(self):
-        """Loads input data from data directory and transfer to target graph for better analysis
-        """
-
-        self._g, edge_feature_list = dict_to_hypergraph(self.load_data, is_dynamic=True)
-
-        self._g.ndata["hyperedge_feature"] = tensor(
-            range(1, len(edge_feature_list) + 1)
-        )
-
-    @url.setter
-    def url(self, value):
-        self._url = value
+import json
+import os
+
+from easygraph.convert import dict_to_hypergraph
+from easygraph.datasets.dynamic.load_dataset import request_json_from_url
+from easygraph.datasets.graph_dataset_base import EasyGraphDataset
+from easygraph.datasets.utils import _get_eg_url
+from easygraph.datasets.utils import tensor
+
+
+class Email_Enron(EasyGraphDataset):
+    _urls = {
+        "email-enron": (
+            "easygraph-data-email-enron/-/raw/main/email-enron.json?inline=false"
+        ),
+        "email-eu": "easygraph-data-email-eu/-/raw/main/email-eu.json?inline=false",
+    }
+
+    def __init__(
+        self,
+        raw_dir=None,
+        force_reload=False,
+        verbose=True,
+        transform=None,
+        save_dir="./",
+    ):
+        name = "email-enron"
+        self.url = _get_eg_url(self._urls[name])
+        super(Email_Enron, self).__init__(
+            name=name,
+            url=self.url,
+            raw_dir=raw_dir,
+            force_reload=force_reload,
+            verbose=verbose,
+            transform=transform,
+            save_dir=save_dir,
+        )
+
+    @property
+    def url(self):
+        return self._url
+
+    @property
+    def save_name(self):
+        return self.name
+
+    def __getitem__(self, idx):
+        assert idx == 0, "This dataset has only one graph"
+        if self._transform is None:
+            return self._g
+        else:
+            return self._transform(self._g)
+
+    def load(self):
+        graph_path = os.path.join(self.save_path, self.save_name + ".json")
+        with open(graph_path, "r") as f:
+            self.load_data = json.load(f)
+
+    def has_cache(self):
+        graph_path = os.path.join(self.save_path, self.save_name + ".json")
+        if os.path.exists(graph_path):
+            return True
+        return False
+
+    def download(self):
+        if self.has_cache():
+            self.load()
+        else:
+            root = self.raw_dir
+            data = request_json_from_url(self.url)
+            with open(os.path.join(root, self.save_name + ".json"), "w") as f:
+                json.dump(data, f)
+            self.load_data = data
+
+    def process(self):
+        """Loads input data from data directory and transfer to target graph for better analysis
+        """
+
+        self._g, edge_feature_list = dict_to_hypergraph(self.load_data, is_dynamic=True)
+
+        self._g.ndata["hyperedge_feature"] = tensor(
+            range(1, len(edge_feature_list) + 1)
+        )
+
+    @url.setter
+    def url(self, value):
+        self._url = value
```

## easygraph/datasets/dynamic/__init__.py

 * *Ordering differences only*

```diff
@@ -1,4 +1,4 @@
-from .email_enron import *
-from .email_eu import *
-from .hospital_lyon import *
-from .load_dataset import *
+from .email_enron import *
+from .email_eu import *
+from .hospital_lyon import *
+from .load_dataset import *
```

## easygraph/datasets/dynamic/email_eu.py

 * *Ordering differences only*

```diff
@@ -1,82 +1,82 @@
-import json
-import os
-
-from easygraph.convert import dict_to_hypergraph
-from easygraph.datasets.dynamic.load_dataset import request_json_from_url
-from easygraph.datasets.graph_dataset_base import EasyGraphDataset
-from easygraph.datasets.utils import _get_eg_url
-from easygraph.datasets.utils import tensor
-
-
-class Email_Eu(EasyGraphDataset):
-    _urls = {
-        "email-eu": "easygraph-data-email-eu/-/raw/main/email-eu.json?inline=false",
-    }
-
-    def __init__(
-        self,
-        raw_dir=None,
-        force_reload=False,
-        verbose=True,
-        transform=None,
-        save_dir="./",
-    ):
-        name = "email-eu"
-        self.url = _get_eg_url(self._urls[name])
-        super(Email_Eu, self).__init__(
-            name=name,
-            url=self.url,
-            raw_dir=raw_dir,
-            force_reload=force_reload,
-            verbose=verbose,
-            transform=transform,
-            save_dir=save_dir,
-        )
-
-    @property
-    def url(self):
-        return self._url
-
-    @property
-    def save_name(self):
-        return self.name
-
-    def __getitem__(self, idx):
-        assert idx == 0, "This dataset has only one graph"
-        if self._transform is None:
-            return self._g
-        else:
-            return self._transform(self._g)
-
-    def load(self):
-        graph_path = os.path.join(self.save_path, self.save_name + ".json")
-        with open(graph_path, "r") as f:
-            self.load_data = json.load(f)
-
-    def has_cache(self):
-        graph_path = os.path.join(self.save_path, self.save_name + ".json")
-        if os.path.exists(graph_path):
-            return True
-        return False
-
-    def download(self):
-        if self.has_cache():
-            self.load()
-        else:
-            root = self.raw_dir
-            data = request_json_from_url(self.url)
-            with open(os.path.join(root, self.save_name + ".json"), "w") as f:
-                json.dump(data, f)
-            self.load_data = data
-
-    def process(self):
-        """Loads input data from data directory and transfer to target graph for better analysis
-        """
-        self._g, edge_feature_list = dict_to_hypergraph(self.load_data, is_dynamic=True)
-        self._g.ndata["hyperedge_feature"] = tensor(
-            range(1, len(edge_feature_list) + 1)
-        )
-
-    @url.setter
-    def url(self, value):
-        self._url = value
+import json
+import os
+
+from easygraph.convert import dict_to_hypergraph
+from easygraph.datasets.dynamic.load_dataset import request_json_from_url
+from easygraph.datasets.graph_dataset_base import EasyGraphDataset
+from easygraph.datasets.utils import _get_eg_url
+from easygraph.datasets.utils import tensor
+
+
+class Email_Eu(EasyGraphDataset):
+    _urls = {
+        "email-eu": "easygraph-data-email-eu/-/raw/main/email-eu.json?inline=false",
+    }
+
+    def __init__(
+        self,
+        raw_dir=None,
+        force_reload=False,
+        verbose=True,
+        transform=None,
+        save_dir="./",
+    ):
+        name = "email-eu"
+        self.url = _get_eg_url(self._urls[name])
+        super(Email_Eu, self).__init__(
+            name=name,
+            url=self.url,
+            raw_dir=raw_dir,
+            force_reload=force_reload,
+            verbose=verbose,
+            transform=transform,
+            save_dir=save_dir,
+        )
+
+    @property
+    def url(self):
+        return self._url
+
+    @property
+    def save_name(self):
+        return self.name
+
+    def __getitem__(self, idx):
+        assert idx == 0, "This dataset has only one graph"
+        if self._transform is None:
+            return self._g
+        else:
+            return self._transform(self._g)
+
+    def load(self):
+        graph_path = os.path.join(self.save_path, self.save_name + ".json")
+        with open(graph_path, "r") as f:
+            self.load_data = json.load(f)
+
+    def has_cache(self):
+        graph_path = os.path.join(self.save_path, self.save_name + ".json")
+        if os.path.exists(graph_path):
+            return True
+        return False
+
+    def download(self):
+        if self.has_cache():
+            self.load()
+        else:
+            root = self.raw_dir
+            data = request_json_from_url(self.url)
+            with open(os.path.join(root, self.save_name + ".json"), "w") as f:
+                json.dump(data, f)
+            self.load_data = data
+
+    def process(self):
+        """Loads input data from data directory and transfer to target graph for better analysis
+        """
+        self._g, edge_feature_list = dict_to_hypergraph(self.load_data, is_dynamic=True)
+        self._g.ndata["hyperedge_feature"] = tensor(
+            range(1, len(edge_feature_list) + 1)
+        )
+
+    @url.setter
+    def url(self, value):
+        self._url = value
```

## easygraph/functions/isolate.py

 * *Ordering differences only*

```diff
@@ -1,103 +1,103 @@
-"""
-Functions for identifying isolate (degree zero) nodes.
-"""
-
-__all__ = ["is_isolate", "isolates", "number_of_isolates"]
-
-
-def is_isolate(G, n):
-    """Determines whether a node is an isolate.
-
-    An *isolate* is a node with no neighbors (that is, with degree
-    zero). For directed graphs, this means no in-neighbors and no
-    out-neighbors.
-
-    Parameters
-    ----------
-    G : EasyGraph graph
-
-    n : node
-        A node in `G`.
-
-    Returns
-    -------
-    is_isolate : bool
-       True if and only if `n` has no neighbors.
-
-    Examples
-    --------
-    >>> G = eg.Graph()
-    >>> G.add_edge(1, 2)
-    >>> G.add_node(3)
-    >>> eg.is_isolate(G, 2)
-    False
-    >>> eg.is_isolate(G, 3)
-    True
-    """
-    return G.degree()[n] == 0
-
-
-def isolates(G):
-    """Iterator over isolates in the graph.
-
-    An *isolate* is a node with no neighbors (that is, with degree
-    zero). For directed graphs, this means no in-neighbors and no
-    out-neighbors.
-
-    Parameters
-    ----------
-    G : EasyGraph graph
-
-    Returns
-    -------
-    iterator
-        An iterator over the isolates of `G`.
-
-    Examples
-    --------
-    To get a list of all isolates of a graph, use the :class:`list`
-    constructor::
-
-        >>> G = eg.Graph()
-        >>> G.add_edge(1, 2)
-        >>> G.add_node(3)
-        >>> list(eg.isolates(G))
-        [3]
-
-    To remove all isolates in the graph, first create a list of the
-    isolates, then use :meth:`Graph.remove_nodes_from`::
-
-        >>> G.remove_nodes_from(list(eg.isolates(G)))
-        >>> list(G)
-        [1, 2]
-
-    For digraphs, isolates have zero in-degree and zero out_degre::
-
-        >>> G = eg.DiGraph([(0, 1), (1, 2)])
-        >>> G.add_node(3)
-        >>> list(eg.isolates(G))
-        [3]
-
-    """
-    return (n for n, d in G.degree().items() if d == 0)
-
-
-def number_of_isolates(G):
-    """Returns the number of isolates in the graph.
-
-    An *isolate* is a node with no neighbors (that is, with degree
-    zero). For directed graphs, this means no in-neighbors and no
-    out-neighbors.
-
-    Parameters
-    ----------
-    G : EasyGraph graph
-
-    Returns
-    -------
-    int
-        The number of degree zero nodes in the graph `G`.
-
-    """
-    # TODO This can be parallelized.
-    return sum(1 for v in isolates(G))
+"""
+Functions for identifying isolate (degree zero) nodes.
+"""
+
+__all__ = ["is_isolate", "isolates", "number_of_isolates"]
+
+
+def is_isolate(G, n):
+    """Determines whether a node is an isolate.
+
+    An *isolate* is a node with no neighbors (that is, with degree
+    zero). For directed graphs, this means no in-neighbors and no
+    out-neighbors.
+
+    Parameters
+    ----------
+    G : EasyGraph graph
+
+    n : node
+        A node in `G`.
+
+    Returns
+    -------
+    is_isolate : bool
+       True if and only if `n` has no neighbors.
+
+    Examples
+    --------
+    >>> G = eg.Graph()
+    >>> G.add_edge(1, 2)
+    >>> G.add_node(3)
+    >>> eg.is_isolate(G, 2)
+    False
+    >>> eg.is_isolate(G, 3)
+    True
+    """
+    return G.degree()[n] == 0
+
+
+def isolates(G):
+    """Iterator over isolates in the graph.
+
+    An *isolate* is a node with no neighbors (that is, with degree
+    zero). For directed graphs, this means no in-neighbors and no
+    out-neighbors.
+
+    Parameters
+    ----------
+    G : EasyGraph graph
+
+    Returns
+    -------
+    iterator
+        An iterator over the isolates of `G`.
+
+    Examples
+    --------
+    To get a list of all isolates of a graph, use the :class:`list`
+    constructor::
+
+        >>> G = eg.Graph()
+        >>> G.add_edge(1, 2)
+        >>> G.add_node(3)
+        >>> list(eg.isolates(G))
+        [3]
+
+    To remove all isolates in the graph, first create a list of the
+    isolates, then use :meth:`Graph.remove_nodes_from`::
+
+        >>> G.remove_nodes_from(list(eg.isolates(G)))
+        >>> list(G)
+        [1, 2]
+
+    For digraphs, isolates have zero in-degree and zero out_degre::
+
+        >>> G = eg.DiGraph([(0, 1), (1, 2)])
+        >>> G.add_node(3)
+        >>> list(eg.isolates(G))
+        [3]
+
+    """
+    return (n for n, d in G.degree().items() if d == 0)
+
+
+def number_of_isolates(G):
+    """Returns the number of isolates in the graph.
+
+    An *isolate* is a node with no neighbors (that is, with degree
+    zero). For directed graphs, this means no in-neighbors and no
+    out-neighbors.
+
+    Parameters
+    ----------
+    G : EasyGraph graph
+
+    Returns
+    -------
+    int
+        The number of degree zero nodes in the graph `G`.
+
+    """
+    # TODO This can be parallelized.
+    return sum(1 for v in isolates(G))
```

## easygraph/functions/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-from easygraph.exception import *
-from easygraph.functions.basic import *
-from easygraph.functions.centrality import *
-from easygraph.functions.community import *
-from easygraph.functions.components import *
-from easygraph.functions.core import *
-from easygraph.functions.drawing import *
-from easygraph.functions.graph_embedding import *
-from easygraph.functions.graph_generator import *
-from easygraph.functions.hypergraph import *
-from easygraph.functions.isolate import *
-from easygraph.functions.path import *
-from easygraph.functions.structural_holes import *
+from easygraph.exception import *
+from easygraph.functions.basic import *
+from easygraph.functions.centrality import *
+from easygraph.functions.community import *
+from easygraph.functions.components import *
+from easygraph.functions.core import *
+from easygraph.functions.drawing import *
+from easygraph.functions.graph_embedding import *
+from easygraph.functions.graph_generator import *
+from easygraph.functions.hypergraph import *
+from easygraph.functions.isolate import *
+from easygraph.functions.path import *
+from easygraph.functions.structural_holes import *
```

## easygraph/functions/centrality/closeness.py

 * *Ordering differences only*

```diff
@@ -1,102 +1,102 @@
-from easygraph.functions.basic import *
-from easygraph.functions.path import single_source_bfs
-from easygraph.functions.path import single_source_dijkstra
-from easygraph.utils import *
-
-
-__all__ = [
-    "closeness_centrality",
-]
-
-
-def closeness_centrality_parallel(nodes, G, path_length):
-    ret = []
-    length = len(G)
-    for node in nodes:
-        x = path_length(G, node)
-        dist = sum(x.values())
-        cnt = len(x)
-        if dist == 0:
-            ret.append([node, 0])
-        else:
-            ret.append([node, (cnt - 1) * (cnt - 1) / (dist * (length - 1))])
-    return ret
-
-
-@not_implemented_for("multigraph")
-@hybrid("cpp_closeness_centrality")
-def closeness_centrality(G, weight=None, sources=None, n_workers=None):
-    r"""
-    Compute closeness centrality for nodes.
-
-    .. math::
-
-        C_{WF}(u) = \frac{n-1}{N-1} \frac{n - 1}{\sum_{v=1}^{n-1} d(v, u)},
-
-    Notice that the closeness distance function computes the
-    outcoming distance to `u` for directed graphs. To use
-    incoming distance, act on `G.reverse()`.
-
-    Parameters
-    ----------
-    G : graph
-      A easygraph graph
-
-    weight : None or string, optional (default=None)
-      If None, all edge weights are considered equal.
-      Otherwise holds the name of the edge attribute used as weight.
-
-    sources : None or nodes list, optional (default=None)
-      If None, all nodes are returned
-      Otherwise,the set of source vertices to creturn.
-
-    Returns
-    -------
-    nodes : dictionary
-      Dictionary of nodes with closeness centrality as the value.
-    """
-    closeness = dict()
-    if sources is not None:
-        nodes = sources
-    else:
-        nodes = G.nodes
-    length = len(G)
-    import functools
-
-    if weight is not None:
-        path_length = functools.partial(single_source_dijkstra, weight=weight)
-    else:
-        path_length = functools.partial(single_source_bfs)
-
-    if n_workers is not None:
-        # use parallel version for large graph
-        import random
-
-        from functools import partial
-        from multiprocessing import Pool
-
-        nodes = list(nodes)
-        random.shuffle(nodes)
-
-        if len(nodes) > n_workers * 30000:
-            nodes = split_len(nodes, step=30000)
-        else:
-            nodes = split(nodes, n_workers)
-        local_function = partial(
-            closeness_centrality_parallel, G=G, path_length=path_length
-        )
-        with Pool(n_workers) as p:
-            ret = p.imap(local_function, nodes)
-            res = [x for i in ret for x in i]
-        closeness = dict(res)
-    else:
-        # use np-parallel version for small graph
-        for node in nodes:
-            x = path_length(G, node)
-            dist = sum(x.values())
-            cnt = len(x)
-            if dist == 0:
-                closeness[node] = 0
-            else:
-                closeness[node] = (cnt - 1) * (cnt - 1) / (dist * (length - 1))
-    return list(closeness.values())
+from easygraph.functions.basic import *
+from easygraph.functions.path import single_source_bfs
+from easygraph.functions.path import single_source_dijkstra
+from easygraph.utils import *
+
+
+__all__ = [
+    "closeness_centrality",
+]
+
+
+def closeness_centrality_parallel(nodes, G, path_length):
+    ret = []
+    length = len(G)
+    for node in nodes:
+        x = path_length(G, node)
+        dist = sum(x.values())
+        cnt = len(x)
+        if dist == 0:
+            ret.append([node, 0])
+        else:
+            ret.append([node, (cnt - 1) * (cnt - 1) / (dist * (length - 1))])
+    return ret
+
+
+@not_implemented_for("multigraph")
+@hybrid("cpp_closeness_centrality")
+def closeness_centrality(G, weight=None, sources=None, n_workers=None):
+    r"""
+    Compute closeness centrality for nodes.
+
+    .. math::
+
+        C_{WF}(u) = \frac{n-1}{N-1} \frac{n - 1}{\sum_{v=1}^{n-1} d(v, u)},
+
+    Notice that the closeness distance function computes the
+    outcoming distance to `u` for directed graphs. To use
+    incoming distance, act on `G.reverse()`.
+
+    Parameters
+    ----------
+    G : graph
+      A easygraph graph
+
+    weight : None or string, optional (default=None)
+      If None, all edge weights are considered equal.
+      Otherwise holds the name of the edge attribute used as weight.
+
+    sources : None or nodes list, optional (default=None)
+      If None, all nodes are returned
+      Otherwise,the set of source vertices to creturn.
+
+    Returns
+    -------
+    nodes : dictionary
+      Dictionary of nodes with closeness centrality as the value.
+    """
+    closeness = dict()
+    if sources is not None:
+        nodes = sources
+    else:
+        nodes = G.nodes
+    length = len(G)
+    import functools
+
+    if weight is not None:
+        path_length = functools.partial(single_source_dijkstra, weight=weight)
+    else:
+        path_length = functools.partial(single_source_bfs)
+
+    if n_workers is not None:
+        # use parallel version for large graph
+        import random
+
+        from functools import partial
+        from multiprocessing import Pool
+
+        nodes = list(nodes)
+        random.shuffle(nodes)
+
+        if len(nodes) > n_workers * 30000:
+            nodes = split_len(nodes, step=30000)
+        else:
+            nodes = split(nodes, n_workers)
+        local_function = partial(
+            closeness_centrality_parallel, G=G, path_length=path_length
+        )
+        with Pool(n_workers) as p:
+            ret = p.imap(local_function, nodes)
+            res = [x for i in ret for x in i]
+        closeness = dict(res)
+    else:
+        # use np-parallel version for small graph
+        for node in nodes:
+            x = path_length(G, node)
+            dist = sum(x.values())
+            cnt = len(x)
+            if dist == 0:
+                closeness[node] = 0
+            else:
+                closeness[node] = (cnt - 1) * (cnt - 1) / (dist * (length - 1))
+    return list(closeness.values())
```

## easygraph/functions/centrality/pagerank.py

 * *Ordering differences only*

```diff
@@ -1,55 +1,55 @@
-import easygraph as eg
-
-from easygraph.utils import *
-
-
-__all__ = ["pagerank"]
-
-
-@not_implemented_for("multigraph")
-@hybrid("cpp_pagerank")
-def pagerank(G, alpha=0.85):
-    """
-    Returns the PageRank value of each node in G.
-
-    Parameters
-    ----------
-    G : graph
-        Undirected graph will be considered as directed graph with two directed edges for each undirected edge.
-
-    alpha : float
-        The damping factor. Default is 0.85
-
-    """
-    import numpy as np
-
-    if len(G) == 0:
-        return {}
-    M = google_matrix(G, alpha=alpha)
-
-    # use numpy LAPACK solver
-    eigenvalues, eigenvectors = np.linalg.eig(M.T)
-    ind = np.argmax(eigenvalues)
-    # eigenvector of largest eigenvalue is at ind, normalized
-    largest = np.array(eigenvectors[:, ind]).flatten().real
-    norm = float(largest.sum())
-    return dict(zip(G, map(float, largest / norm)))
-
-
-def google_matrix(G, alpha):
-    import numpy as np
-
-    M = eg.to_numpy_array(G)
-    N = len(G)
-    if N == 0:
-        return M
-
-    # Get dangling nodes(nodes with no out link)
-    dangling_nodes = np.where(M.sum(axis=1) == 0)[0]
-    dangling_weights = np.repeat(1.0 / N, N)
-    for node in dangling_nodes:
-        M[node] = dangling_weights
-
-    M /= M.sum(axis=1)[:, np.newaxis]
-
-    return alpha * M + (1 - alpha) * np.repeat(1.0 / N, N)
+import easygraph as eg
+
+from easygraph.utils import *
+
+
+__all__ = ["pagerank"]
+
+
+@not_implemented_for("multigraph")
+@hybrid("cpp_pagerank")
+def pagerank(G, alpha=0.85):
+    """
+    Returns the PageRank value of each node in G.
+
+    Parameters
+    ----------
+    G : graph
+        Undirected graph will be considered as directed graph with two directed edges for each undirected edge.
+
+    alpha : float
+        The damping factor. Default is 0.85
+
+    """
+    import numpy as np
+
+    if len(G) == 0:
+        return {}
+    M = google_matrix(G, alpha=alpha)
+
+    # use numpy LAPACK solver
+    eigenvalues, eigenvectors = np.linalg.eig(M.T)
+    ind = np.argmax(eigenvalues)
+    # eigenvector of largest eigenvalue is at ind, normalized
+    largest = np.array(eigenvectors[:, ind]).flatten().real
+    norm = float(largest.sum())
+    return dict(zip(G, map(float, largest / norm)))
+
+
+def google_matrix(G, alpha):
+    import numpy as np
+
+    M = eg.to_numpy_array(G)
+    N = len(G)
+    if N == 0:
+        return M
+
+    # Get dangling nodes(nodes with no out link)
+    dangling_nodes = np.where(M.sum(axis=1) == 0)[0]
+    dangling_weights = np.repeat(1.0 / N, N)
+    for node in dangling_nodes:
+        M[node] = dangling_weights
+
+    M /= M.sum(axis=1)[:, np.newaxis]
+
+    return alpha * M + (1 - alpha) * np.repeat(1.0 / N, N)
```

## easygraph/functions/centrality/laplacian.py

 * *Ordering differences only*

```diff
@@ -1,134 +1,134 @@
-from easygraph.utils import *
-
-
-__all__ = ["laplacian"]
-
-
-@not_implemented_for("multigraph")
-def laplacian(G, n_workers=None):
-    """Returns the laplacian centrality of each node in the weighted graph
-
-    Parameters
-    ----------
-    G : graph
-        weighted graph
-
-    Returns
-    -------
-    CL : dict
-        the laplacian centrality of each node in the weighted graph
-
-    Examples
-    --------
-    Returns the laplacian centrality of each node in the weighted graph G
-
-    >>> laplacian(G)
-
-    Reference
-    ---------
-    .. [1] Xingqin Qi, Eddie Fuller, Qin Wu, Yezhou Wu, Cun-Quan Zhang.
-    "Laplacian centrality: A new centrality measure for weighted networks."
-    Information Sciences, Volume 194, Pages 240-253, 2012.
-
-    """
-    adj = G.adj
-    from collections import defaultdict
-
-    X = defaultdict(int)
-    W = defaultdict(int)
-    CL = {}
-
-    if n_workers is not None:
-        # use the parallel version for large graph
-        import random
-
-        from functools import partial
-        from multiprocessing import Pool
-
-        nodes = list(G.nodes)
-        random.shuffle(nodes)
-
-        if len(nodes) > n_workers * 30000:
-            nodes = split_len(nodes, step=30000)
-        else:
-            nodes = split(nodes, n_workers)
-
-        local_function = partial(initialize_parallel, G=G, adj=adj)
-        with Pool(n_workers) as p:
-            ret = p.imap(local_function, nodes)
-            resX, resW = [], []
-            for i in ret:
-                for x in i:
-                    resX.append(x[0])
-                    resW.append(x[1])
-            X = dict(resX)
-            W = dict(resW)
-            ELG = sum(X[i] * X[i] for i in G) + sum(W[i] for i in G)
-        local_function = partial(laplacian_parallel, G=G, X=X, W=W, adj=adj, ELG=ELG)
-        with Pool(n_workers) as p:
-            ret = p.imap(local_function, nodes)
-            res = [x for i in ret for x in i]
-        CL = dict(res)
-
-    else:
-        # use np-parallel version for small graph
-        for i in G:
-            for j in G:
-                if i in G and j in G[i]:
-                    X[i] += adj[i][j].get("weight", 1)
-                    W[i] += adj[i][j].get("weight", 1) * adj[i][j].get("weight", 1)
-        ELG = sum(X[i] * X[i] for i in G) + sum(W[i] for i in G)
-        for i in G:
-            import copy
-
-            Xi = copy.deepcopy(X)
-            for j in G:
-                if j in adj.keys() and i in adj[j].keys():
-                    Xi[j] -= adj[j][i].get("weight", 1)
-            Xi[i] = 0
-            ELGi = sum(Xi[i] * Xi[i] for i in G) + sum(W[i] for i in G) - 2 * W[i]
-            if ELG:
-                CL[i] = (float)(ELG - ELGi) / ELG
-    return CL
-
-
-def initialize_parallel(nodes, G, adj):
-    ret = []
-    for i in nodes:
-        X = 0
-        W = 0
-        for j in G:
-            if j in G[i]:
-                X += adj[i][j].get("weight", 1)
-                W += adj[i][j].get("weight", 1) * adj[i][j].get("weight", 1)
-        ret.append([[i, X], [i, W]])
-    return ret
-
-
-def laplacian_parallel(nodes, G, X, W, adj, ELG):
-    ret = []
-    for i in nodes:
-        import copy
-
-        Xi = copy.deepcopy(X)
-        for j in G:
-            if j in adj.keys() and i in adj[j].keys():
-                Xi[j] -= adj[j][i].get("weight", 1)
-        Xi[i] = 0
-        ELGi = sum(Xi[i] * Xi[i] for i in G) + sum(W[i] for i in G) - 2 * W[i]
-        if ELG:
-            ret.append([i, (float)(ELG - ELGi) / ELG])
-    return ret
-
-
-def sort(data):
-    return dict(sorted(data.items(), key=lambda x: x[0], reverse=True))
-
-
-def output(data, path):
-    import json
-
-    data = sort(data)
-    json_str = json.dumps(data, ensure_ascii=False, indent=4)
-    with open(path, "w", encoding="utf-8") as json_file:
-        json_file.write(json_str)
+from easygraph.utils import *
+
+
+__all__ = ["laplacian"]
+
+
+@not_implemented_for("multigraph")
+def laplacian(G, n_workers=None):
+    """Returns the laplacian centrality of each node in the weighted graph
+
+    Parameters
+    ----------
+    G : graph
+        weighted graph
+
+    Returns
+    -------
+    CL : dict
+        the laplacian centrality of each node in the weighted graph
+
+    Examples
+    --------
+    Returns the laplacian centrality of each node in the weighted graph G
+
+    >>> laplacian(G)
+
+    Reference
+    ---------
+    .. [1] Xingqin Qi, Eddie Fuller, Qin Wu, Yezhou Wu, Cun-Quan Zhang.
+    "Laplacian centrality: A new centrality measure for weighted networks."
+    Information Sciences, Volume 194, Pages 240-253, 2012.
+
+    """
+    adj = G.adj
+    from collections import defaultdict
+
+    X = defaultdict(int)
+    W = defaultdict(int)
+    CL = {}
+
+    if n_workers is not None:
+        # use the parallel version for large graph
+        import random
+
+        from functools import partial
+        from multiprocessing import Pool
+
+        nodes = list(G.nodes)
+        random.shuffle(nodes)
+
+        if len(nodes) > n_workers * 30000:
+            nodes = split_len(nodes, step=30000)
+        else:
+            nodes = split(nodes, n_workers)
+
+        local_function = partial(initialize_parallel, G=G, adj=adj)
+        with Pool(n_workers) as p:
+            ret = p.imap(local_function, nodes)
+            resX, resW = [], []
+            for i in ret:
+                for x in i:
+                    resX.append(x[0])
+                    resW.append(x[1])
+            X = dict(resX)
+            W = dict(resW)
+            ELG = sum(X[i] * X[i] for i in G) + sum(W[i] for i in G)
+        local_function = partial(laplacian_parallel, G=G, X=X, W=W, adj=adj, ELG=ELG)
+        with Pool(n_workers) as p:
+            ret = p.imap(local_function, nodes)
+            res = [x for i in ret for x in i]
+        CL = dict(res)
+
+    else:
+        # use np-parallel version for small graph
+        for i in G:
+            for j in G:
+                if i in G and j in G[i]:
+                    X[i] += adj[i][j].get("weight", 1)
+                    W[i] += adj[i][j].get("weight", 1) * adj[i][j].get("weight", 1)
+        ELG = sum(X[i] * X[i] for i in G) + sum(W[i] for i in G)
+        for i in G:
+            import copy
+
+            Xi = copy.deepcopy(X)
+            for j in G:
+                if j in adj.keys() and i in adj[j].keys():
+                    Xi[j] -= adj[j][i].get("weight", 1)
+            Xi[i] = 0
+            ELGi = sum(Xi[i] * Xi[i] for i in G) + sum(W[i] for i in G) - 2 * W[i]
+            if ELG:
+                CL[i] = (float)(ELG - ELGi) / ELG
+    return CL
+
+
+def initialize_parallel(nodes, G, adj):
+    ret = []
+    for i in nodes:
+        X = 0
+        W = 0
+        for j in G:
+            if j in G[i]:
+                X += adj[i][j].get("weight", 1)
+                W += adj[i][j].get("weight", 1) * adj[i][j].get("weight", 1)
+        ret.append([[i, X], [i, W]])
+    return ret
+
+
+def laplacian_parallel(nodes, G, X, W, adj, ELG):
+    ret = []
+    for i in nodes:
+        import copy
+
+        Xi = copy.deepcopy(X)
+        for j in G:
+            if j in adj.keys() and i in adj[j].keys():
+                Xi[j] -= adj[j][i].get("weight", 1)
+        Xi[i] = 0
+        ELGi = sum(Xi[i] * Xi[i] for i in G) + sum(W[i] for i in G) - 2 * W[i]
+        if ELG:
+            ret.append([i, (float)(ELG - ELGi) / ELG])
+    return ret
+
+
+def sort(data):
+    return dict(sorted(data.items(), key=lambda x: x[0], reverse=True))
+
+
+def output(data, path):
+    import json
+
+    data = sort(data)
+    json_str = json.dumps(data, ensure_ascii=False, indent=4)
+    with open(path, "w", encoding="utf-8") as json_file:
+        json_file.write(json_str)
```

## easygraph/functions/centrality/betweenness.py

 * *Ordering differences only*

```diff
@@ -1,242 +1,242 @@
-from easygraph.utils import *
-from easygraph.utils.decorators import *
-
-
-__all__ = [
-    "betweenness_centrality",
-]
-
-
-def betweenness_centrality_parallel(nodes, G, path_length, accumulate):
-    betweenness = {node: 0.0 for node in G}
-    for node in nodes:
-        S, P, sigma = path_length(G, source=node)
-        betweenness = accumulate(betweenness, S, P, sigma, node)
-    return betweenness
-
-
-@not_implemented_for("multigraph")
-@hybrid("cpp_betweenness_centrality")
-def betweenness_centrality(
-    G, weight=None, sources=None, normalized=True, endpoints=False, n_workers=None
-):
-    r"""Compute the shortest-basic betweenness centrality for nodes.
-
-    .. math::
-
-        c_B(v)  = \sum_{s,t \in V} \frac{\sigma(s, t|v)}{\sigma(s, t)}
-
-    where V is the set of nodes,
-
-    .. math::
-        \sigma(s, t)
-
-    is the number of shortest (s, t)-paths, and
-
-    .. math::
-
-        \sigma(s, t|v)
-
-    is the number of those paths  passing through some node v other than s, t.
-
-    .. math::
-
-        If\ s\ =\ t,\ \sigma(s, t) = 1, and\ if\ v \in {s, t}, \sigma(s, t|v) = 0 [2]_.
-
-    Parameters
-    ----------
-    G : graph
-      A easygraph graph.
-
-    weight : None or string, optional (default=None)
-      If None, all edge weights are considered equal.
-      Otherwise holds the name of the edge attribute used as weight.
-
-    sources : None or nodes list, optional (default=None)
-      If None, all nodes are considered.
-      Otherwise,the set of source vertices to consider when calculating shortest paths.
-
-    normalized : bool, optional
-      If True the betweenness values are normalized by `2/((n-1)(n-2))`
-      for graphs, and `1/((n-1)(n-2))` for directed graphs where `n`
-      is the number of nodes in G.
-
-    endpoints : bool, optional
-      If True include the endpoints in the shortest basic counts.
-
-    Returns
-    -------
-
-    nodes : dictionary
-       Dictionary of nodes with betweenness centrality as the value.
-
-    >>> betweenness_centrality(G,weight="weight")
-    """
-
-    import functools
-
-    if weight is not None:
-        path_length = functools.partial(_single_source_dijkstra_path, weight=weight)
-    else:
-        path_length = functools.partial(_single_source_bfs_path)
-
-    if endpoints:
-        accumulate = functools.partial(_accumulate_endpoints)
-    else:
-        accumulate = functools.partial(_accumulate_basic)
-
-    if sources is not None:
-        nodes = sources
-    else:
-        nodes = G.nodes
-    betweenness = dict.fromkeys(G, 0.0)
-
-    if n_workers is not None:
-        #  use the parallel version for large graph
-        import random
-
-        from functools import partial
-        from multiprocessing import Pool
-
-        nodes = list(nodes)
-        random.shuffle(nodes)
-
-        if len(nodes) > n_workers * 30000:
-            nodes = split_len(nodes, step=30000)
-        else:
-            nodes = split(nodes, n_workers)
-        local_function = partial(
-            betweenness_centrality_parallel,
-            G=G,
-            path_length=path_length,
-            accumulate=accumulate,
-        )
-        with Pool(n_workers) as p:
-            ret = p.imap(local_function, nodes)
-            for res in ret:
-                for key in res:
-                    betweenness[key] += res[key]
-    else:
-        # use np-parallel version for small graph
-        for node in nodes:
-            S, P, sigma = path_length(G, source=node)
-            betweenness = accumulate(betweenness, S, P, sigma, node)
-
-    betweenness = _rescale(
-        betweenness,
-        len(G),
-        normalized=normalized,
-        directed=G.is_directed(),
-        endpoints=endpoints,
-    )
-    return list(betweenness.values())
-
-
-def _rescale(betweenness, n, normalized, directed=False, endpoints=False):
-    if normalized:
-        if endpoints:
-            if n < 2:
-                scale = None  # no normalization
-            else:
-                # Scale factor should include endpoint nodes
-                scale = 1 / (n * (n - 1))
-        elif n <= 2:
-            scale = None  # no normalization b=0 for all nodes
-        else:
-            scale = 1 / ((n - 1) * (n - 2))
-    else:  # rescale by 2 for undirected graphs
-        if not directed:
-            scale = 0.5
-        else:
-            scale = None
-    if scale is not None:
-        for v in betweenness:
-            betweenness[v] *= scale
-    return betweenness
-
-
-def _single_source_bfs_path(G, source):
-    S = []
-    P = {v: [] for v in G}
-    sigma = dict.fromkeys(G, 0.0)
-    D = {}
-    sigma[source] = 1.0
-    D[source] = 0
-    Q = [source]
-    adj = G.adj
-    while Q:
-        v = Q.pop(0)
-        S.append(v)
-        Dv = D[v]
-        sigmav = sigma[v]
-        for w in adj[v]:
-            if w not in D:
-                Q.append(w)
-                D[w] = Dv + 1
-            if D[w] == Dv + 1:
-                sigma[w] += sigmav
-                P[w].append(v)
-    return S, P, sigma
-
-
-def _single_source_dijkstra_path(G, source, weight="weight"):
-    from heapq import heappop
-    from heapq import heappush
-
-    push = heappush
-    pop = heappop
-    S = []
-    P = {v: [] for v in G}
-    sigma = dict.fromkeys(G, 0.0)
-    D = {}
-    sigma[source] = 1.0
-    seen = {source: 0}
-    Q = []
-    from itertools import count
-
-    c = count()
-    adj = G.adj
-    push(Q, (0, next(c), source, source))
-    while Q:
-        (dist, _, pred, v) = pop(Q)
-        if v in D:
-            continue
-        sigma[v] += sigma[pred]
-        S.append(v)
-        D[v] = dist
-        for w in adj[v]:
-            vw_dist = dist + adj[v][w].get(weight, 1)
-            if w not in D and (w not in seen or vw_dist < seen[w]):
-                seen[w] = vw_dist
-                push(Q, (vw_dist, next(c), v, w))
-                sigma[w] = 0.0
-                P[w] = [v]
-            elif vw_dist == seen[w]:  # handle equal paths
-                sigma[w] += sigma[v]
-                P[w].append(v)
-    return S, P, sigma
-
-
-def _accumulate_endpoints(betweenness, S, P, sigma, s):
-    betweenness[s] += len(S) - 1
-    delta = dict.fromkeys(S, 0)
-    while S:
-        w = S.pop()
-        coeff = (1 + delta[w]) / sigma[w]
-        for v in P[w]:
-            delta[v] += sigma[v] * coeff
-        if w != s:
-            betweenness[w] += delta[w] + 1
-    return betweenness
-
-
-def _accumulate_basic(betweenness, S, P, sigma, s):
-    delta = dict.fromkeys(S, 0)
-    while S:
-        w = S.pop()
-        coeff = (1 + delta[w]) / sigma[w]
-        for v in P[w]:
-            delta[v] += sigma[v] * coeff
-        if w != s:
-            betweenness[w] += delta[w]
-    return betweenness
+from easygraph.utils import *
+from easygraph.utils.decorators import *
+
+
+__all__ = [
+    "betweenness_centrality",
+]
+
+
+def betweenness_centrality_parallel(nodes, G, path_length, accumulate):
+    betweenness = {node: 0.0 for node in G}
+    for node in nodes:
+        S, P, sigma = path_length(G, source=node)
+        betweenness = accumulate(betweenness, S, P, sigma, node)
+    return betweenness
+
+
+@not_implemented_for("multigraph")
+@hybrid("cpp_betweenness_centrality")
+def betweenness_centrality(
+    G, weight=None, sources=None, normalized=True, endpoints=False, n_workers=None
+):
+    r"""Compute the shortest-basic betweenness centrality for nodes.
+
+    .. math::
+
+        c_B(v)  = \sum_{s,t \in V} \frac{\sigma(s, t|v)}{\sigma(s, t)}
+
+    where V is the set of nodes,
+
+    .. math::
+        \sigma(s, t)
+
+    is the number of shortest (s, t)-paths, and
+
+    .. math::
+
+        \sigma(s, t|v)
+
+    is the number of those paths  passing through some node v other than s, t.
+
+    .. math::
+
+        If\ s\ =\ t,\ \sigma(s, t) = 1, and\ if\ v \in {s, t}, \sigma(s, t|v) = 0 [2]_.
+
+    Parameters
+    ----------
+    G : graph
+      A easygraph graph.
+
+    weight : None or string, optional (default=None)
+      If None, all edge weights are considered equal.
+      Otherwise holds the name of the edge attribute used as weight.
+
+    sources : None or nodes list, optional (default=None)
+      If None, all nodes are considered.
+      Otherwise,the set of source vertices to consider when calculating shortest paths.
+
+    normalized : bool, optional
+      If True the betweenness values are normalized by `2/((n-1)(n-2))`
+      for graphs, and `1/((n-1)(n-2))` for directed graphs where `n`
+      is the number of nodes in G.
+
+    endpoints : bool, optional
+      If True include the endpoints in the shortest basic counts.
+
+    Returns
+    -------
+
+    nodes : dictionary
+       Dictionary of nodes with betweenness centrality as the value.
+
+    >>> betweenness_centrality(G,weight="weight")
+    """
+
+    import functools
+
+    if weight is not None:
+        path_length = functools.partial(_single_source_dijkstra_path, weight=weight)
+    else:
+        path_length = functools.partial(_single_source_bfs_path)
+
+    if endpoints:
+        accumulate = functools.partial(_accumulate_endpoints)
+    else:
+        accumulate = functools.partial(_accumulate_basic)
+
+    if sources is not None:
+        nodes = sources
+    else:
+        nodes = G.nodes
+    betweenness = dict.fromkeys(G, 0.0)
+
+    if n_workers is not None:
+        #  use the parallel version for large graph
+        import random
+
+        from functools import partial
+        from multiprocessing import Pool
+
+        nodes = list(nodes)
+        random.shuffle(nodes)
+
+        if len(nodes) > n_workers * 30000:
+            nodes = split_len(nodes, step=30000)
+        else:
+            nodes = split(nodes, n_workers)
+        local_function = partial(
+            betweenness_centrality_parallel,
+            G=G,
+            path_length=path_length,
+            accumulate=accumulate,
+        )
+        with Pool(n_workers) as p:
+            ret = p.imap(local_function, nodes)
+            for res in ret:
+                for key in res:
+                    betweenness[key] += res[key]
+    else:
+        # use np-parallel version for small graph
+        for node in nodes:
+            S, P, sigma = path_length(G, source=node)
+            betweenness = accumulate(betweenness, S, P, sigma, node)
+
+    betweenness = _rescale(
+        betweenness,
+        len(G),
+        normalized=normalized,
+        directed=G.is_directed(),
+        endpoints=endpoints,
+    )
+    return list(betweenness.values())
+
+
+def _rescale(betweenness, n, normalized, directed=False, endpoints=False):
+    if normalized:
+        if endpoints:
+            if n < 2:
+                scale = None  # no normalization
+            else:
+                # Scale factor should include endpoint nodes
+                scale = 1 / (n * (n - 1))
+        elif n <= 2:
+            scale = None  # no normalization b=0 for all nodes
+        else:
+            scale = 1 / ((n - 1) * (n - 2))
+    else:  # rescale by 2 for undirected graphs
+        if not directed:
+            scale = 0.5
+        else:
+            scale = None
+    if scale is not None:
+        for v in betweenness:
+            betweenness[v] *= scale
+    return betweenness
+
+
+def _single_source_bfs_path(G, source):
+    S = []
+    P = {v: [] for v in G}
+    sigma = dict.fromkeys(G, 0.0)
+    D = {}
+    sigma[source] = 1.0
+    D[source] = 0
+    Q = [source]
+    adj = G.adj
+    while Q:
+        v = Q.pop(0)
+        S.append(v)
+        Dv = D[v]
+        sigmav = sigma[v]
+        for w in adj[v]:
+            if w not in D:
+                Q.append(w)
+                D[w] = Dv + 1
+            if D[w] == Dv + 1:
+                sigma[w] += sigmav
+                P[w].append(v)
+    return S, P, sigma
+
+
+def _single_source_dijkstra_path(G, source, weight="weight"):
+    from heapq import heappop
+    from heapq import heappush
+
+    push = heappush
+    pop = heappop
+    S = []
+    P = {v: [] for v in G}
+    sigma = dict.fromkeys(G, 0.0)
+    D = {}
+    sigma[source] = 1.0
+    seen = {source: 0}
+    Q = []
+    from itertools import count
+
+    c = count()
+    adj = G.adj
+    push(Q, (0, next(c), source, source))
+    while Q:
+        (dist, _, pred, v) = pop(Q)
+        if v in D:
+            continue
+        sigma[v] += sigma[pred]
+        S.append(v)
+        D[v] = dist
+        for w in adj[v]:
+            vw_dist = dist + adj[v][w].get(weight, 1)
+            if w not in D and (w not in seen or vw_dist < seen[w]):
+                seen[w] = vw_dist
+                push(Q, (vw_dist, next(c), v, w))
+                sigma[w] = 0.0
+                P[w] = [v]
+            elif vw_dist == seen[w]:  # handle equal paths
+                sigma[w] += sigma[v]
+                P[w].append(v)
+    return S, P, sigma
+
+
+def _accumulate_endpoints(betweenness, S, P, sigma, s):
+    betweenness[s] += len(S) - 1
+    delta = dict.fromkeys(S, 0)
+    while S:
+        w = S.pop()
+        coeff = (1 + delta[w]) / sigma[w]
+        for v in P[w]:
+            delta[v] += sigma[v] * coeff
+        if w != s:
+            betweenness[w] += delta[w] + 1
+    return betweenness
+
+
+def _accumulate_basic(betweenness, S, P, sigma, s):
+    delta = dict.fromkeys(S, 0)
+    while S:
+        w = S.pop()
+        coeff = (1 + delta[w]) / sigma[w]
+        for v in P[w]:
+            delta[v] += sigma[v] * coeff
+        if w != s:
+            betweenness[w] += delta[w]
+    return betweenness
```

## easygraph/functions/centrality/flowbetweenness.py

 * *Ordering differences only*

```diff
@@ -1,146 +1,146 @@
-import collections
-import copy
-
-from easygraph.utils.decorators import *
-
-
-__all__ = [
-    "flowbetweenness_centrality",
-]
-
-
-@not_implemented_for("multigraph")
-def flowbetweenness_centrality(G):
-    """Compute the independent-basic betweenness centrality for nodes in a flow network.
-
-    .. math::
-
-       c_B(v) =\\sum_{s,t \\in V} \frac{\\sigma(s, t|v)}{\\sigma(s, t)}
-
-    where V is the set of nodes,
-
-    .. math::
-
-        \\sigma(s, t)\\ is\\ the\\ number\\ of\\ independent\\ (s, t)-paths,
-
-    .. math::
-
-        \\sigma(s, t|v)\\ is\\ the\\ maximum\\ number\\ possible\\ of\\ those\\ paths\\ passing\\ through\\ some\\ node\\ v\\ other\\ than\\ s, t.\
-
-    .. math::
-
-        If\\ s\\ =\\ t,\\ \\sigma(s, t)\\ =\\ 1,\\ and\\ if\\ v \\in \\{s, t\\},\\ \\sigma(s, t|v)\\ =\\ 0\\ [2]_.
-
-    Parameters
-    ----------
-    G : graph
-      A easygraph directed graph.
-
-    Returns
-    -------
-    nodes : dictionary
-       Dictionary of nodes with independent-basic betweenness centrality as the value.
-
-    Notes
-    -----
-    A flow network is a directed graph where each edge has a capacity and each edge receives a flow.
-    """
-    if G.is_directed() == False:
-        print("Please input a directed graph")
-        return
-    flow_dict = NumberOfFlow(G)
-    nodes = G.nodes
-    result_dict = dict()
-    for node, _ in nodes.items():
-        result_dict[node] = 0
-    for node_v, _ in nodes.items():
-        for node_s, _ in nodes.items():
-            for node_t, _ in nodes.items():
-                num = 1
-                num_v = 0
-                if node_s == node_t:
-                    num_v = 0
-                    num = 1
-                if node_v in [node_s, node_t]:
-                    num_v = 0
-                    num = 1
-                if node_v != node_s and node_v != node_t and node_s != node_t:
-                    num = flow_dict[node_s][node_t]
-                    num_v = min(flow_dict[node_s][node_v], flow_dict[node_v][node_t])
-                if num == 0:
-                    pass
-                else:
-                    result_dict[node_v] = result_dict[node_v] + num_v / num
-    return result_dict
-
-
-# flow betweenness
-def NumberOfFlow(G):
-    nodes = G.nodes
-    result_dict = dict()
-    for node1, _ in nodes.items():
-        result_dict[node1] = dict()
-        for node2, _ in nodes.items():
-            if node1 == node2:
-                pass
-            else:
-                result_dict[node1][node2] = edmonds_karp(G, node1, node2)
-    return result_dict
-
-
-def edmonds_karp(G, source, sink):
-    nodes = G.nodes
-    parent = dict()
-    for node, _ in nodes.items():
-        parent[node] = -1
-
-    adj = copy.deepcopy(G.adj)
-    max_flow = 0
-    while bfs(G, source, sink, parent, adj):
-        path_flow = float("inf")
-        s = sink
-        while s != source:
-            path_flow = min(path_flow, adj[parent[s]][s].get("weight", 1))
-            s = parent[s]
-        max_flow += path_flow
-        v = sink
-        while v != source:
-            u = parent[v]
-            x = adj[u][v].get("weight", 1)
-            adj[u][v].update({"weight": x})
-            adj[u][v]["weight"] -= path_flow
-
-            flag = 0
-            if v not in adj:
-                adj[v] = dict()
-            if u not in adj[v]:
-                adj[v][u] = dict()
-                flag = 1
-            if flag == 1:
-                x = 0
-            else:
-                x = adj[v][u].get("weight", 1)
-            adj[v][u].update({"weight": x})
-            adj[v][u]["weight"] += path_flow
-            v = parent[v]
-    return max_flow
-
-
-def bfs(G, source, sink, parent, adj):
-    nodes = G.nodes
-    visited = dict()
-    for node, _ in nodes.items():
-        visited[node] = 0
-    queue = collections.deque()
-    queue.append(source)
-    visited[source] = True
-    while queue:
-        u = queue.popleft()
-        if u not in adj:
-            continue
-        for v, attr in adj[u].items():
-            if (visited[v] == False) and (attr.get("weight", 1) > 0):
-                queue.append(v)
-                visited[v] = True
-                parent[v] = u
-    return visited[sink]
+import collections
+import copy
+
+from easygraph.utils.decorators import *
+
+
+__all__ = [
+    "flowbetweenness_centrality",
+]
+
+
+@not_implemented_for("multigraph")
+def flowbetweenness_centrality(G):
+    """Compute the independent-basic betweenness centrality for nodes in a flow network.
+
+    .. math::
+
+       c_B(v) =\\sum_{s,t \\in V} \frac{\\sigma(s, t|v)}{\\sigma(s, t)}
+
+    where V is the set of nodes,
+
+    .. math::
+
+        \\sigma(s, t)\\ is\\ the\\ number\\ of\\ independent\\ (s, t)-paths,
+
+    .. math::
+
+        \\sigma(s, t|v)\\ is\\ the\\ maximum\\ number\\ possible\\ of\\ those\\ paths\\ passing\\ through\\ some\\ node\\ v\\ other\\ than\\ s, t.\
+
+    .. math::
+
+        If\\ s\\ =\\ t,\\ \\sigma(s, t)\\ =\\ 1,\\ and\\ if\\ v \\in \\{s, t\\},\\ \\sigma(s, t|v)\\ =\\ 0\\ [2]_.
+
+    Parameters
+    ----------
+    G : graph
+      A easygraph directed graph.
+
+    Returns
+    -------
+    nodes : dictionary
+       Dictionary of nodes with independent-basic betweenness centrality as the value.
+
+    Notes
+    -----
+    A flow network is a directed graph where each edge has a capacity and each edge receives a flow.
+    """
+    if G.is_directed() == False:
+        print("Please input a directed graph")
+        return
+    flow_dict = NumberOfFlow(G)
+    nodes = G.nodes
+    result_dict = dict()
+    for node, _ in nodes.items():
+        result_dict[node] = 0
+    for node_v, _ in nodes.items():
+        for node_s, _ in nodes.items():
+            for node_t, _ in nodes.items():
+                num = 1
+                num_v = 0
+                if node_s == node_t:
+                    num_v = 0
+                    num = 1
+                if node_v in [node_s, node_t]:
+                    num_v = 0
+                    num = 1
+                if node_v != node_s and node_v != node_t and node_s != node_t:
+                    num = flow_dict[node_s][node_t]
+                    num_v = min(flow_dict[node_s][node_v], flow_dict[node_v][node_t])
+                if num == 0:
+                    pass
+                else:
+                    result_dict[node_v] = result_dict[node_v] + num_v / num
+    return result_dict
+
+
+# flow betweenness
+def NumberOfFlow(G):
+    nodes = G.nodes
+    result_dict = dict()
+    for node1, _ in nodes.items():
+        result_dict[node1] = dict()
+        for node2, _ in nodes.items():
+            if node1 == node2:
+                pass
+            else:
+                result_dict[node1][node2] = edmonds_karp(G, node1, node2)
+    return result_dict
+
+
+def edmonds_karp(G, source, sink):
+    nodes = G.nodes
+    parent = dict()
+    for node, _ in nodes.items():
+        parent[node] = -1
+
+    adj = copy.deepcopy(G.adj)
+    max_flow = 0
+    while bfs(G, source, sink, parent, adj):
+        path_flow = float("inf")
+        s = sink
+        while s != source:
+            path_flow = min(path_flow, adj[parent[s]][s].get("weight", 1))
+            s = parent[s]
+        max_flow += path_flow
+        v = sink
+        while v != source:
+            u = parent[v]
+            x = adj[u][v].get("weight", 1)
+            adj[u][v].update({"weight": x})
+            adj[u][v]["weight"] -= path_flow
+
+            flag = 0
+            if v not in adj:
+                adj[v] = dict()
+            if u not in adj[v]:
+                adj[v][u] = dict()
+                flag = 1
+            if flag == 1:
+                x = 0
+            else:
+                x = adj[v][u].get("weight", 1)
+            adj[v][u].update({"weight": x})
+            adj[v][u]["weight"] += path_flow
+            v = parent[v]
+    return max_flow
+
+
+def bfs(G, source, sink, parent, adj):
+    nodes = G.nodes
+    visited = dict()
+    for node, _ in nodes.items():
+        visited[node] = 0
+    queue = collections.deque()
+    queue.append(source)
+    visited[source] = True
+    while queue:
+        u = queue.popleft()
+        if u not in adj:
+            continue
+        for v, attr in adj[u].items():
+            if (visited[v] == False) and (attr.get("weight", 1) > 0):
+                queue.append(v)
+                visited[v] = True
+                parent[v] = u
+    return visited[sink]
```

## easygraph/functions/centrality/degree.py

```diff
@@ -1,122 +1,122 @@
-from easygraph.utils.decorators import *
-
-
-__all__ = ["degree_centrality", "in_degree_centrality", "out_degree_centrality"]
-
-
-@not_implemented_for("multigraph")
-def degree_centrality(G):
-    """Compute the degree centrality for nodes in a bipartite network.
-
-    The degree centrality for a node v is the fraction of nodes it
-    is connected to.
-
-    parameters
-    ----------
-    G : graph
-      A easygraph graph
-
-    Returns
-    -------
-    nodes : dictionary
-       Dictionary of nodes with degree centrality as the value.
-
-    Notes
-    -----
-    The degree centrality are normalized by dividing by n-1 where
-    n is number of nodes in G.
-    """
-    if len(G) <= 1:
-        return {n: 1 for n in G}
-
-    s = 1.0 / (len(G) - 1.0)
-    centrality = {n: d * s for n, d in (G.degree()).items()}
-    return centrality
-
-
-@not_implemented_for("multigraph")
-@only_implemented_for_Directed_graph
-def in_degree_centrality(G):
-    """Compute the in-degree centrality for nodes.
-
-    The in-degree centrality for a node v is the fraction of nodes its
-    incoming edges are connected to.
-
-    Parameters
-    ----------
-    G : graph
-        A EasyGraph graph
-
-    Returns
-    -------
-    nodes : dictionary
-        Dictionary of nodes with in-degree centrality as values.
-
-    Raises
-    ------
-    EasyGraphNotImplemented:
-        If G is undirected.
-
-    See Also
-    --------
-    degree_centrality, out_degree_centrality
-
-    Notes
-    -----
-    The degree centrality values are normalized by dividing by the maximum
-    possible degree in a simple graph n-1 where n is the number of nodes in G.
-
-    For multigraphs or graphs with self loops the maximum degree might
-    be higher than n-1 and values of degree centrality greater than 1
-    are possible.
-    """
-    if len(G) <= 1:
-        return {n: 1 for n in G}
-
-    s = 1.0 / (len(G) - 1.0)
-    centrality = {n: d * s for n, d in G.in_degree()}
-    return centrality
-
-
-@not_implemented_for("multigraph")
-@only_implemented_for_Directed_graph
-def out_degree_centrality(G):
-    """Compute the out-degree centrality for nodes.
-
-    The out-degree centrality for a node v is the fraction of nodes its
-    outgoing edges are connected to.
-
-    Parameters
-    ----------
-    G : graph
-        A EasyGraph graph
-
-    Returns
-    -------
-    nodes : dictionary
-        Dictionary of nodes with out-degree centrality as values.
-
-    Raises
-    ------
-    EasyGraphNotImplemented:
-        If G is undirected.
-
-    See Also
-    --------
-    degree_centrality, in_degree_centrality
-
-    Notes
-    -----
-    The degree centrality values are normalized by dividing by the maximum
-    possible degree in a simple graph n-1 where n is the number of nodes in G.
-
-    For multigraphs or graphs with self loops the maximum degree might
-    be higher than n-1 and values of degree centrality greater than 1
-    are possible.
-    """
-    if len(G) <= 1:
-        return {n: 1 for n in G}
-
-    s = 1.0 / (len(G) - 1.0)
-    centrality = {n: d * s for n, d in G.out_degree()}
-    return centrality
+from easygraph.utils.decorators import *
+
+
+__all__ = ["degree_centrality", "in_degree_centrality", "out_degree_centrality"]
+
+
+@not_implemented_for("multigraph")
+def degree_centrality(G):
+    """Compute the degree centrality for nodes in a bipartite network.
+
+    The degree centrality for a node v is the fraction of nodes it
+    is connected to.
+
+    parameters
+    ----------
+    G : graph
+      A easygraph graph
+
+    Returns
+    -------
+    nodes : dictionary
+       Dictionary of nodes with degree centrality as the value.
+
+    Notes
+    -----
+    The degree centrality are normalized by dividing by n-1 where
+    n is number of nodes in G.
+    """
+    if len(G) <= 1:
+        return {n: 1 for n in G}
+
+    s = 1.0 / (len(G) - 1.0)
+    centrality = {n: d * s for n, d in (G.degree()).items()}
+    return centrality
+
+
+@not_implemented_for("multigraph")
+@only_implemented_for_Directed_graph
+def in_degree_centrality(G):
+    """Compute the in-degree centrality for nodes.
+
+    The in-degree centrality for a node v is the fraction of nodes its
+    incoming edges are connected to.
+
+    Parameters
+    ----------
+    G : graph
+        A EasyGraph graph
+
+    Returns
+    -------
+    nodes : dictionary
+        Dictionary of nodes with in-degree centrality as values.
+
+    Raises
+    ------
+    EasyGraphNotImplemented:
+        If G is undirected.
+
+    See Also
+    --------
+    degree_centrality, out_degree_centrality
+
+    Notes
+    -----
+    The degree centrality values are normalized by dividing by the maximum
+    possible degree in a simple graph n-1 where n is the number of nodes in G.
+
+    For multigraphs or graphs with self loops the maximum degree might
+    be higher than n-1 and values of degree centrality greater than 1
+    are possible.
+    """
+    if len(G) <= 1:
+        return {n: 1 for n in G}
+
+    s = 1.0 / (len(G) - 1.0)
+    centrality = {n: d * s for n, d in G.in_degree().items()}
+    return centrality
+
+
+@not_implemented_for("multigraph")
+@only_implemented_for_Directed_graph
+def out_degree_centrality(G):
+    """Compute the out-degree centrality for nodes.
+
+    The out-degree centrality for a node v is the fraction of nodes its
+    outgoing edges are connected to.
+
+    Parameters
+    ----------
+    G : graph
+        A EasyGraph graph
+
+    Returns
+    -------
+    nodes : dictionary
+        Dictionary of nodes with out-degree centrality as values.
+
+    Raises
+    ------
+    EasyGraphNotImplemented:
+        If G is undirected.
+
+    See Also
+    --------
+    degree_centrality, in_degree_centrality
+
+    Notes
+    -----
+    The degree centrality values are normalized by dividing by the maximum
+    possible degree in a simple graph n-1 where n is the number of nodes in G.
+
+    For multigraphs or graphs with self loops the maximum degree might
+    be higher than n-1 and values of degree centrality greater than 1
+    are possible.
+    """
+    if len(G) <= 1:
+        return {n: 1 for n in G}
+
+    s = 1.0 / (len(G) - 1.0)
+    centrality = {n: d * s for n, d in G.out_degree().items()}
+    return centrality
```

## easygraph/functions/centrality/ego_betweenness.py

 * *Ordering differences only*

```diff
@@ -1,52 +1,52 @@
-__all__ = ["ego_betweenness"]
-import numpy as np
-
-from easygraph.utils import *
-
-
-@not_implemented_for("multigraph")
-def ego_betweenness(G, node):
-    """
-    ego networks are networks consisting of a single actor (ego) together with the actors they are connected to (alters) and all the links among those alters.[1]
-    Burt (1992), in his book Structural Holes, provides ample evidence that having high betweenness centrality, which is highly correlated with having many structural holes, can bring benefits to ego.[1]
-    Returns the betweenness centrality of a ego network whose ego is set
-
-    Parameters
-    ----------
-    G : graph
-    node : int
-
-    Returns
-    -------
-    sum : float
-        the betweenness centrality of a ego network whose ego is set
-
-    Examples
-    --------
-    Returns the betwenness centrality of node 1.
-
-    >>> ego_betweenness(G,node=1)
-
-    Reference
-    ---------
-    .. [1] Martin Everett, Stephen P. Borgatti. "Ego network betweenness." Social Networks, Volume 27, Issue 1, Pages 31-38, 2005.
-
-    """
-    g = G.ego_subgraph(node)
-    n = len(g) + 1
-    A = np.matlib.zeros((n, n))
-    for i in range(n):
-        for j in range(n):
-            if g.has_edge(i, j):
-                A[i, j] = 1
-    B = A * A
-    C = 1 - A
-    sum = 0
-    flag = G.is_directed()
-    for i in range(n):
-        for j in range(n):
-            if i != j and C[i, j] == 1 and B[i, j] != 0:
-                sum += 1.0 / B[i, j]
-    if flag == False:
-        sum /= 2
-    return sum
+__all__ = ["ego_betweenness"]
+import numpy as np
+
+from easygraph.utils import *
+
+
+@not_implemented_for("multigraph")
+def ego_betweenness(G, node):
+    """
+    ego networks are networks consisting of a single actor (ego) together with the actors they are connected to (alters) and all the links among those alters.[1]
+    Burt (1992), in his book Structural Holes, provides ample evidence that having high betweenness centrality, which is highly correlated with having many structural holes, can bring benefits to ego.[1]
+    Returns the betweenness centrality of a ego network whose ego is set
+
+    Parameters
+    ----------
+    G : graph
+    node : int
+
+    Returns
+    -------
+    sum : float
+        the betweenness centrality of a ego network whose ego is set
+
+    Examples
+    --------
+    Returns the betwenness centrality of node 1.
+
+    >>> ego_betweenness(G,node=1)
+
+    Reference
+    ---------
+    .. [1] Martin Everett, Stephen P. Borgatti. "Ego network betweenness." Social Networks, Volume 27, Issue 1, Pages 31-38, 2005.
+
+    """
+    g = G.ego_subgraph(node)
+    n = len(g) + 1
+    A = np.matlib.zeros((n, n))
+    for i in range(n):
+        for j in range(n):
+            if g.has_edge(i, j):
+                A[i, j] = 1
+    B = A * A
+    C = 1 - A
+    sum = 0
+    flag = G.is_directed()
+    for i in range(n):
+        for j in range(n):
+            if i != j and C[i, j] == 1 and B[i, j] != 0:
+                sum += 1.0 / B[i, j]
+    if flag == False:
+        sum /= 2
+    return sum
```

## easygraph/functions/centrality/__init__.py

 * *Ordering differences only*

```diff
@@ -1,7 +1,7 @@
-from .betweenness import *
-from .closeness import *
-from .degree import *
-from .ego_betweenness import *
-from .flowbetweenness import *
-from .laplacian import *
-from .pagerank import *
+from .betweenness import *
+from .closeness import *
+from .degree import *
+from .ego_betweenness import *
+from .flowbetweenness import *
+from .laplacian import *
+from .pagerank import *
```

## easygraph/functions/basic/avg_degree.py

 * *Ordering differences only*

```diff
@@ -1,31 +1,31 @@
-__all__ = [
-    "average_degree",
-]
-
-
-def average_degree(G) -> float:
-    """Returns the average degree of the graph.
-
-    Parameters
-    ----------
-    G : graph
-        A EasyGraph graph
-
-    Returns
-    -------
-    average degree : float
-        The average degree of the graph.
-
-    Notes
-    -----
-    Self loops are counted twice in the total degree of a node.
-
-    Examples
-    --------
-    >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
-    >>> G.add_edge(1, 2)
-    >>> G.add_edge(2, 3)
-    >>> eg.average_degree(G)
-    1.3333333333333333
-    """
-    return G.number_of_edges() / G.number_of_nodes() * 2
+__all__ = [
+    "average_degree",
+]
+
+
+def average_degree(G) -> float:
+    """Returns the average degree of the graph.
+
+    Parameters
+    ----------
+    G : graph
+        A EasyGraph graph
+
+    Returns
+    -------
+    average degree : float
+        The average degree of the graph.
+
+    Notes
+    -----
+    Self loops are counted twice in the total degree of a node.
+
+    Examples
+    --------
+    >>> G = eg.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc
+    >>> G.add_edge(1, 2)
+    >>> G.add_edge(2, 3)
+    >>> eg.average_degree(G)
+    1.3333333333333333
+    """
+    return G.number_of_edges() / G.number_of_nodes() * 2
```

## easygraph/functions/basic/localassort.py

 * *Ordering differences only*

```diff
@@ -1,226 +1,226 @@
-import easygraph as eg
-import numpy as np
-import scipy.sparse as sparse
-
-
-__all__ = [
-    "localAssort",
-]
-
-
-def localAssort(
-    edgelist, node_attr, pr=np.arange(0.0, 1.0, 0.1), undir=True, missingValue=-1
-):
-    """Calculate the multiscale assortativity.
-    You must ensure that the node index and node attribute index start from 0
-    Parameters
-    ----------
-    edgelist : array_like
-        the network represented as an edge list,
-        i.e., a E x 2 array of node pairs
-    node_attr : array_like
-        n length array of node attribute values
-    pr : array, optional
-        array of one minus restart probabilities for the random walk in
-        calculating the personalised pagerank. The largest of these values
-        determines the accuracy of the TotalRank vector max(pr) -> 1 is more
-        accurate (default: [0, .1, .2, .3, .4, .5, .6, .7, .8, .9])
-    undir : bool, optional
-        indicate if network is undirected (default: True)
-    missingValue : int, optional
-        token to indicate missing attribute values (default: -1)
-    Returns
-    -------
-    assortM : array_like
-        n x len(pr) array of local assortativities, each column corresponds to
-        a value of the input restart probabilities, pr. Note if only number of
-        restart probabilties is greater than one (i.e., len(pr) > 1).
-    assortT : array_like
-        n length array of multiscale assortativities
-    Z : array_like
-        N length array of per-node confidence scores
-    References
-    ----------
-    For full details see [1]_
-    .. [1] Peel, L., Delvenne, J. C., & Lambiotte, R. (2018). "Multiscale
-        mixing patterns in networks.' PNAS, 115(16), 4057-4062.
-    """
-    # number of nodes
-    n = len(node_attr)
-
-    # number of nodes with complete attribute
-    ncomp = (node_attr != missingValue).sum()
-    # number of edges
-    m = len(edgelist)
-    # construct adjacency matrix and calculate degree sequence
-    A, degree = createA(edgelist, n, undir)
-
-    # construct diagonal inverse degree matrix
-    D = sparse.diags(1.0 / degree, 0, format="csc")
-
-    # construct transition matrix (row normalised adjacency matrix)
-    W = D @ A
-
-    # number of distinct node categories
-    c = len(np.unique(node_attr))
-    if ncomp < n:
-        c -= 1
-
-    # calculate node weights for how "complete" the
-    # metadata is around the node
-    Z = np.zeros(n)
-
-    Z[node_attr == missingValue] = 1.0
-
-    Z = (W @ Z) / degree
-
-    # indicator array if node has attribute data (or missing)
-    hasAttribute = node_attr != missingValue
-
-    # calculate global expected values
-    values = np.ones(ncomp)
-
-    yi = (hasAttribute).nonzero()[0]
-
-    yj = node_attr[hasAttribute]
-    Y = sparse.coo_matrix((values, (yi, yj)), shape=(n, c)).tocsc()
-    eij_glob = np.array(Y.T @ (A @ Y).todense())
-
-    eij_glob /= np.sum(eij_glob)
-
-    ab_glob = np.sum(eij_glob.sum(1) * eij_glob.sum(0))
-    # initialise outputs
-    assortM = np.empty((n, len(pr)))
-    assortT = np.empty(n)
-    WY = (W @ Y).tocsc()
-
-    for i in range(n):
-        pis, ti, it = calculateRWRrange(W, i, pr, n)
-        if len(pr) > 1:
-            for ii, pri in enumerate(pr):
-                pi = pis[:, ii]
-
-                YPI = sparse.coo_matrix(
-                    (
-                        pi[hasAttribute],
-                        (node_attr[hasAttribute], np.arange(n)[hasAttribute]),
-                    ),
-                    shape=(c, n),
-                ).tocsr()
-                trace_e = (YPI.dot(WY).toarray()).trace()
-                assortM[i, ii] = trace_e
-        YPI = sparse.coo_matrix(
-            (ti[hasAttribute], (node_attr[hasAttribute], np.arange(n)[hasAttribute])),
-            shape=(c, n),
-        ).tocsr()
-        e_gh = (YPI @ WY).toarray()
-        e_gh_sum = e_gh.sum()
-        Z[i] = e_gh_sum
-        e_gh /= e_gh_sum
-        trace_e = e_gh.trace()
-        assortT[i] = trace_e
-
-    assortT -= ab_glob
-    np.divide(assortT, 1.0 - ab_glob, out=assortT, where=ab_glob != 0)
-
-    if len(pr) > 1:
-        assortM -= ab_glob
-        np.divide(assortM, 1.0 - ab_glob, out=assortM, where=ab_glob != 0)
-        return assortM, assortT, Z
-    return None, assortT, Z
-
-
-def createA(E, n, undir=True):
-    """Create adjacency matrix and degree sequence."""
-    if undir:
-        G = eg.Graph()
-    else:
-        G = eg.DiGraph()
-    G.add_nodes_from(range(n))
-
-    for e in E:
-        G.add_edge(e[0], e[1])
-
-    A = eg.to_scipy_sparse_matrix(G)
-
-    degree = np.array(A.sum(1)).flatten()
-
-    return A, degree
-
-
-def calculateRWRrange(W, i, alphas, n, maxIter=1000):
-    """
-    Calculate the personalised TotalRank and personalised PageRank vectors.
-    Parameters
-    ----------
-    W : array_like
-        transition matrix (row normalised adjacency matrix)
-    i : int
-        index of the personalisation node
-    alphas : array_like
-        array of (1 - restart probabilties)
-    n : int
-        number of nodes in the network
-    maxIter : int, optional
-        maximum number of interations (default: 1000)
-    Returns
-    -------
-    pPageRank_all : array_like
-        personalised PageRank for all input alpha values (only calculated if
-        more than one alpha given as input, i.e., len(alphas) > 1)
-    pTotalRank : array_like
-        personalised TotalRank (personalised PageRank with alpha integrated
-        out)
-
-    it : int
-        number of iterations
-    References
-    ----------
-    See [2]_ and [3]_ for further details.
-    .. [2] Boldi, P. (2005). "TotalRank: Ranking without damping." In Special
-        interest tracks and posters of the 14th international conference on
-        World Wide Web (pp. 898-899).
-    .. [3] Boldi, P., Santini, M., & Vigna, S. (2007). "A deeper investigation
-        of PageRank as a function of the damping factor." In Dagstuhl Seminar
-        Proceedings. Schloss Dagstuhl-Leibniz-Zentrum für Informatik.
-    """
-    alpha0 = alphas.max()
-    WT = alpha0 * W.T
-    diff = 1
-    it = 1
-
-    # initialise PageRank vectors
-    pPageRank = np.zeros(n)
-
-    pPageRank_all = np.zeros((n, len(alphas)))
-    pPageRank[i] = 1
-
-    pPageRank_all[i, :] = 1
-
-    pPageRank_old = pPageRank.copy()
-    pTotalRank = pPageRank.copy()
-
-    oneminusalpha0 = 1 - alpha0
-
-    while diff > 1e-9:
-        # calculate personalised PageRank via power iteration
-        pPageRank = WT @ pPageRank
-        pPageRank[i] += oneminusalpha0
-        # calculate difference in pPageRank from previous iteration
-        delta_pPageRank = pPageRank - pPageRank_old
-        # Eq. [S23] Ref. [1]
-        pTotalRank += (delta_pPageRank) / ((it + 1) * (alpha0**it))
-        # only calculate personalised pageranks if more than one alpha
-        if len(alphas) > 1:
-            pPageRank_all += np.outer((delta_pPageRank), (alphas / alpha0) ** it)
-
-        # calculate convergence criteria
-        diff = np.sum((delta_pPageRank) ** 2) / n
-        it += 1
-
-        if it > maxIter:
-            print(i, "max iterations exceeded")
-            diff = 0
-        pPageRank_old = pPageRank.copy()
-
-    return pPageRank_all, pTotalRank, it
+import easygraph as eg
+import numpy as np
+import scipy.sparse as sparse
+
+
+__all__ = [
+    "localAssort",
+]
+
+
+def localAssort(
+    edgelist, node_attr, pr=np.arange(0.0, 1.0, 0.1), undir=True, missingValue=-1
+):
+    """Calculate the multiscale assortativity.
+    You must ensure that the node index and node attribute index start from 0
+    Parameters
+    ----------
+    edgelist : array_like
+        the network represented as an edge list,
+        i.e., a E x 2 array of node pairs
+    node_attr : array_like
+        n length array of node attribute values
+    pr : array, optional
+        array of one minus restart probabilities for the random walk in
+        calculating the personalised pagerank. The largest of these values
+        determines the accuracy of the TotalRank vector max(pr) -> 1 is more
+        accurate (default: [0, .1, .2, .3, .4, .5, .6, .7, .8, .9])
+    undir : bool, optional
+        indicate if network is undirected (default: True)
+    missingValue : int, optional
+        token to indicate missing attribute values (default: -1)
+    Returns
+    -------
+    assortM : array_like
+        n x len(pr) array of local assortativities, each column corresponds to
+        a value of the input restart probabilities, pr. Note if only number of
+        restart probabilties is greater than one (i.e., len(pr) > 1).
+    assortT : array_like
+        n length array of multiscale assortativities
+    Z : array_like
+        N length array of per-node confidence scores
+    References
+    ----------
+    For full details see [1]_
+    .. [1] Peel, L., Delvenne, J. C., & Lambiotte, R. (2018). "Multiscale
+        mixing patterns in networks.' PNAS, 115(16), 4057-4062.
+    """
+    # number of nodes
+    n = len(node_attr)
+
+    # number of nodes with complete attribute
+    ncomp = (node_attr != missingValue).sum()
+    # number of edges
+    m = len(edgelist)
+    # construct adjacency matrix and calculate degree sequence
+    A, degree = createA(edgelist, n, undir)
+
+    # construct diagonal inverse degree matrix
+    D = sparse.diags(1.0 / degree, 0, format="csc")
+
+    # construct transition matrix (row normalised adjacency matrix)
+    W = D @ A
+
+    # number of distinct node categories
+    c = len(np.unique(node_attr))
+    if ncomp < n:
+        c -= 1
+
+    # calculate node weights for how "complete" the
+    # metadata is around the node
+    Z = np.zeros(n)
+
+    Z[node_attr == missingValue] = 1.0
+
+    Z = (W @ Z) / degree
+
+    # indicator array if node has attribute data (or missing)
+    hasAttribute = node_attr != missingValue
+
+    # calculate global expected values
+    values = np.ones(ncomp)
+
+    yi = (hasAttribute).nonzero()[0]
+
+    yj = node_attr[hasAttribute]
+    Y = sparse.coo_matrix((values, (yi, yj)), shape=(n, c)).tocsc()
+    eij_glob = np.array(Y.T @ (A @ Y).todense())
+
+    eij_glob /= np.sum(eij_glob)
+
+    ab_glob = np.sum(eij_glob.sum(1) * eij_glob.sum(0))
+    # initialise outputs
+    assortM = np.empty((n, len(pr)))
+    assortT = np.empty(n)
+    WY = (W @ Y).tocsc()
+
+    for i in range(n):
+        pis, ti, it = calculateRWRrange(W, i, pr, n)
+        if len(pr) > 1:
+            for ii, pri in enumerate(pr):
+                pi = pis[:, ii]
+
+                YPI = sparse.coo_matrix(
+                    (
+                        pi[hasAttribute],
+                        (node_attr[hasAttribute], np.arange(n)[hasAttribute]),
+                    ),
+                    shape=(c, n),
+                ).tocsr()
+                trace_e = (YPI.dot(WY).toarray()).trace()
+                assortM[i, ii] = trace_e
+        YPI = sparse.coo_matrix(
+            (ti[hasAttribute], (node_attr[hasAttribute], np.arange(n)[hasAttribute])),
+            shape=(c, n),
+        ).tocsr()
+        e_gh = (YPI @ WY).toarray()
+        e_gh_sum = e_gh.sum()
+        Z[i] = e_gh_sum
+        e_gh /= e_gh_sum
+        trace_e = e_gh.trace()
+        assortT[i] = trace_e
+
+    assortT -= ab_glob
+    np.divide(assortT, 1.0 - ab_glob, out=assortT, where=ab_glob != 0)
+
+    if len(pr) > 1:
+        assortM -= ab_glob
+        np.divide(assortM, 1.0 - ab_glob, out=assortM, where=ab_glob != 0)
+        return assortM, assortT, Z
+    return None, assortT, Z
+
+
+def createA(E, n, undir=True):
+    """Create adjacency matrix and degree sequence."""
+    if undir:
+        G = eg.Graph()
+    else:
+        G = eg.DiGraph()
+    G.add_nodes_from(range(n))
+
+    for e in E:
+        G.add_edge(e[0], e[1])
+
+    A = eg.to_scipy_sparse_matrix(G)
+
+    degree = np.array(A.sum(1)).flatten()
+
+    return A, degree
+
+
+def calculateRWRrange(W, i, alphas, n, maxIter=1000):
+    """
+    Calculate the personalised TotalRank and personalised PageRank vectors.
+    Parameters
+    ----------
+    W : array_like
+        transition matrix (row normalised adjacency matrix)
+    i : int
+        index of the personalisation node
+    alphas : array_like
+        array of (1 - restart probabilties)
+    n : int
+        number of nodes in the network
+    maxIter : int, optional
+        maximum number of interations (default: 1000)
+    Returns
+    -------
+    pPageRank_all : array_like
+        personalised PageRank for all input alpha values (only calculated if
+        more than one alpha given as input, i.e., len(alphas) > 1)
+    pTotalRank : array_like
+        personalised TotalRank (personalised PageRank with alpha integrated
+        out)
+
+    it : int
+        number of iterations
+    References
+    ----------
+    See [2]_ and [3]_ for further details.
+    .. [2] Boldi, P. (2005). "TotalRank: Ranking without damping." In Special
+        interest tracks and posters of the 14th international conference on
+        World Wide Web (pp. 898-899).
+    .. [3] Boldi, P., Santini, M., & Vigna, S. (2007). "A deeper investigation
+        of PageRank as a function of the damping factor." In Dagstuhl Seminar
+        Proceedings. Schloss Dagstuhl-Leibniz-Zentrum für Informatik.
+    """
+    alpha0 = alphas.max()
+    WT = alpha0 * W.T
+    diff = 1
+    it = 1
+
+    # initialise PageRank vectors
+    pPageRank = np.zeros(n)
+
+    pPageRank_all = np.zeros((n, len(alphas)))
+    pPageRank[i] = 1
+
+    pPageRank_all[i, :] = 1
+
+    pPageRank_old = pPageRank.copy()
+    pTotalRank = pPageRank.copy()
+
+    oneminusalpha0 = 1 - alpha0
+
+    while diff > 1e-9:
+        # calculate personalised PageRank via power iteration
+        pPageRank = WT @ pPageRank
+        pPageRank[i] += oneminusalpha0
+        # calculate difference in pPageRank from previous iteration
+        delta_pPageRank = pPageRank - pPageRank_old
+        # Eq. [S23] Ref. [1]
+        pTotalRank += (delta_pPageRank) / ((it + 1) * (alpha0**it))
+        # only calculate personalised pageranks if more than one alpha
+        if len(alphas) > 1:
+            pPageRank_all += np.outer((delta_pPageRank), (alphas / alpha0) ** it)
+
+        # calculate convergence criteria
+        diff = np.sum((delta_pPageRank) ** 2) / n
+        it += 1
+
+        if it > maxIter:
+            print(i, "max iterations exceeded")
+            diff = 0
+        pPageRank_old = pPageRank.copy()
+
+    return pPageRank_all, pTotalRank, it
```

## easygraph/functions/basic/predecessor_path_based.py

 * *Ordering differences only*

```diff
@@ -1,101 +1,101 @@
-import easygraph as eg
-
-
-__all__ = [
-    "predecessor",
-]
-
-
-def predecessor(G, source, target=None, cutoff=None, return_seen=None):
-    """Returns dict of predecessors for the path from source to all nodes in G.
-
-    Parameters
-    ----------
-    G : EasyGraph graph
-
-    source : node label
-       Starting node for path
-
-    target : node label, optional
-       Ending node for path. If provided only predecessors between
-       source and target are returned
-
-    cutoff : integer, optional
-        Depth to stop the search. Only paths of length <= cutoff are returned.
-
-    return_seen : bool, optional (default=None)
-        Whether to return a dictionary, keyed by node, of the level (number of
-        hops) to reach the node (as seen during breadth-first-search).
-
-    Returns
-    -------
-    pred : dictionary
-        Dictionary, keyed by node, of predecessors in the shortest path.
-
-
-    (pred, seen): tuple of dictionaries
-        If `return_seen` argument is set to `True`, then a tuple of dictionaries
-        is returned. The first element is the dictionary, keyed by node, of
-        predecessors in the shortest path. The second element is the dictionary,
-        keyed by node, of the level (number of hops) to reach the node (as seen
-        during breadth-first-search).
-
-    Examples
-    --------
-    >>> G = eg.path_graph(4)
-    >>> list(G)
-    [0, 1, 2, 3]
-    >>> eg.predecessor(G, 0)
-    {0: [], 1: [0], 2: [1], 3: [2]}
-    >>> eg.predecessor(G, 0, return_seen=True)
-    ({0: [], 1: [0], 2: [1], 3: [2]}, {0: 0, 1: 1, 2: 2, 3: 3})
-
-
-    """
-
-    if source not in G:
-        raise eg.NodeNotFound(f"Source {source} not in G")
-    level = 0  # the current level
-    nextlevel = [source]  # list of nodes to check at next level
-    seen = {source: level}  # level (number of hops) when seen in BFS
-    pred = {source: []}  # predecessor dictionary
-    while nextlevel:
-        level = level + 1
-        thislevel = nextlevel
-        nextlevel = []
-        for v in thislevel:
-            for w in list(G.neighbors(v)):
-                if w not in seen:
-                    pred[w] = [v]
-                    seen[w] = level
-                    nextlevel.append(w)
-                elif seen[w] == level:  # add v to predecessor list if it
-                    pred[w].append(v)  # is at the correct level
-        if cutoff and cutoff <= level:
-            break
-
-    if target is not None:
-        if return_seen:
-            if target not in pred:
-                return ([], -1)  # No predecessor
-            return (pred[target], seen[target])
-        else:
-            if target not in pred:
-                return []  # No predecessor
-            return pred[target]
-    else:
-        if return_seen:
-            return (pred, seen)
-        else:
-            return pred
-
-
-# def main():
-#     G = eg.path_graph(4)
-#     print(G.edges)
-
-#     print(predecessor(G, 0))
-
-
-# if __name__ == "__main__":
-#     main()
+import easygraph as eg
+
+
+__all__ = [
+    "predecessor",
+]
+
+
+def predecessor(G, source, target=None, cutoff=None, return_seen=None):
+    """Returns dict of predecessors for the path from source to all nodes in G.
+
+    Parameters
+    ----------
+    G : EasyGraph graph
+
+    source : node label
+       Starting node for path
+
+    target : node label, optional
+       Ending node for path. If provided only predecessors between
+       source and target are returned
+
+    cutoff : integer, optional
+        Depth to stop the search. Only paths of length <= cutoff are returned.
+
+    return_seen : bool, optional (default=None)
+        Whether to return a dictionary, keyed by node, of the level (number of
+        hops) to reach the node (as seen during breadth-first-search).
+
+    Returns
+    -------
+    pred : dictionary
+        Dictionary, keyed by node, of predecessors in the shortest path.
+
+
+    (pred, seen): tuple of dictionaries
+        If `return_seen` argument is set to `True`, then a tuple of dictionaries
+        is returned. The first element is the dictionary, keyed by node, of
+        predecessors in the shortest path. The second element is the dictionary,
+        keyed by node, of the level (number of hops) to reach the node (as seen
+        during breadth-first-search).
+
+    Examples
+    --------
+    >>> G = eg.path_graph(4)
+    >>> list(G)
+    [0, 1, 2, 3]
+    >>> eg.predecessor(G, 0)
+    {0: [], 1: [0], 2: [1], 3: [2]}
+    >>> eg.predecessor(G, 0, return_seen=True)
+    ({0: [], 1: [0], 2: [1], 3: [2]}, {0: 0, 1: 1, 2: 2, 3: 3})
+
+
+    """
+
+    if source not in G:
+        raise eg.NodeNotFound(f"Source {source} not in G")
+    level = 0  # the current level
+    nextlevel = [source]  # list of nodes to check at next level
+    seen = {source: level}  # level (number of hops) when seen in BFS
+    pred = {source: []}  # predecessor dictionary
+    while nextlevel:
+        level = level + 1
+        thislevel = nextlevel
+        nextlevel = []
+        for v in thislevel:
+            for w in list(G.neighbors(v)):
+                if w not in seen:
+                    pred[w] = [v]
+                    seen[w] = level
+                    nextlevel.append(w)
+                elif seen[w] == level:  # add v to predecessor list if it
+                    pred[w].append(v)  # is at the correct level
+        if cutoff and cutoff <= level:
+            break
+
+    if target is not None:
+        if return_seen:
+            if target not in pred:
+                return ([], -1)  # No predecessor
+            return (pred[target], seen[target])
+        else:
+            if target not in pred:
+                return []  # No predecessor
+            return pred[target]
+    else:
+        if return_seen:
+            return (pred, seen)
+        else:
+            return pred
+
+
+# def main():
+#     G = eg.path_graph(4)
+#     print(G.edges)
+
+#     print(predecessor(G, 0))
+
+
+# if __name__ == "__main__":
+#     main()
```

## easygraph/functions/basic/cluster.py

 * *Ordering differences only*

```diff
@@ -1,559 +1,559 @@
-from collections import Counter
-from itertools import chain
-
-import numpy as np
-
-from easygraph.utils.decorators import hybrid
-from easygraph.utils.decorators import not_implemented_for
-from easygraph.utils.misc import split
-from easygraph.utils.misc import split_len
-
-
-__all__ = ["average_clustering", "clustering"]
-
-
-def _local_weighted_triangles_and_degree_iter_parallel(
-    nodes_nbrs, G, weight, max_weight
-):
-    ret = []
-
-    def wt(u, v):
-        return G[u][v].get(weight, 1) / max_weight
-
-    for i, nbrs in nodes_nbrs:
-        inbrs = set(nbrs) - {i}
-        weighted_triangles = 0
-        seen = set()
-        for j in inbrs:
-            seen.add(j)
-            # This avoids counting twice -- we double at the end.
-            jnbrs = set(G[j]) - seen
-            # Only compute the edge weight once, before the inner inner
-            # loop.
-            wij = wt(i, j)
-            weighted_triangles += sum(
-                np.cbrt([(wij * wt(j, k) * wt(k, i)) for k in inbrs & jnbrs])
-            )
-        ret.append((i, len(inbrs), 2 * weighted_triangles))
-    return ret
-
-
-@not_implemented_for("multigraph")
-def _weighted_triangles_and_degree_iter(G, nodes=None, weight="weight", n_workers=None):
-    """Return an iterator of (node, degree, weighted_triangles).
-
-    Used for weighted clustering.
-    Note: this returns the geometric average weight of edges in the triangle.
-    Also, each triangle is counted twice (each direction).
-    So you may want to divide by 2.
-
-    """
-
-    if weight is None or G.number_of_edges() == 0:
-        max_weight = 1
-    else:
-        max_weight = max(d.get(weight, 1) for u, v, d in G.edges)
-    if nodes is None:
-        nodes_nbrs = G.adj.items()
-    else:
-        nodes_nbrs = ((n, G[n]) for n in G.nbunch_iter(nodes))
-
-    def wt(u, v):
-        return G[u][v].get(weight, 1) / max_weight
-
-    if n_workers is not None:
-        import random
-
-        from functools import partial
-        from multiprocessing import Pool
-
-        _local_weighted_triangles_and_degree_iter_function = partial(
-            _local_weighted_triangles_and_degree_iter_parallel,
-            G=G,
-            weight=weight,
-            max_weight=max_weight,
-        )
-        nodes_nbrs = list(nodes_nbrs)
-        random.shuffle(nodes_nbrs)
-        if len(nodes_nbrs) > n_workers * 30000:
-            nodes_nbrs = split_len(nodes, step=30000)
-        else:
-            nodes_nbrs = split(nodes_nbrs, n_workers)
-        with Pool(n_workers) as p:
-            ret = p.imap(_local_weighted_triangles_and_degree_iter_function, nodes_nbrs)
-            for r in ret:
-                for x in r:
-                    yield x
-    else:
-        for i, nbrs in nodes_nbrs:
-            inbrs = set(nbrs) - {i}
-            weighted_triangles = 0
-            seen = set()
-            for j in inbrs:
-                seen.add(j)
-                # This avoids counting twice -- we double at the end.
-                jnbrs = set(G[j]) - seen
-                # Only compute the edge weight once, before the inner inner
-                # loop.
-                wij = wt(i, j)
-                weighted_triangles += sum(
-                    np.cbrt([(wij * wt(j, k) * wt(k, i)) for k in inbrs & jnbrs])
-                )
-            yield (i, len(inbrs), 2 * weighted_triangles)
-
-
-def _local_directed_weighted_triangles_and_degree_parallel(
-    nodes_nbrs, G, weight, max_weight
-):
-    ret = []
-
-    def wt(u, v):
-        return G[u][v].get(weight, 1) / max_weight
-
-    for i, preds, succs in nodes_nbrs:
-        ipreds = set(preds) - {i}
-        isuccs = set(succs) - {i}
-
-        directed_triangles = 0
-        for j in ipreds:
-            jpreds = set(G._pred[j]) - {j}
-            jsuccs = set(G._adj[j]) - {j}
-            directed_triangles += sum(
-                np.cbrt([(wt(j, i) * wt(k, i) * wt(k, j)) for k in ipreds & jpreds])
-            )
-            directed_triangles += sum(
-                np.cbrt([(wt(j, i) * wt(k, i) * wt(j, k)) for k in ipreds & jsuccs])
-            )
-            directed_triangles += sum(
-                np.cbrt([(wt(j, i) * wt(i, k) * wt(k, j)) for k in isuccs & jpreds])
-            )
-            directed_triangles += sum(
-                np.cbrt([(wt(j, i) * wt(i, k) * wt(j, k)) for k in isuccs & jsuccs])
-            )
-
-        for j in isuccs:
-            jpreds = set(G._pred[j]) - {j}
-            jsuccs = set(G._adj[j]) - {j}
-            directed_triangles += sum(
-                np.cbrt([(wt(i, j) * wt(k, i) * wt(k, j)) for k in ipreds & jpreds])
-            )
-            directed_triangles += sum(
-                np.cbrt([(wt(i, j) * wt(k, i) * wt(j, k)) for k in ipreds & jsuccs])
-            )
-            directed_triangles += sum(
-                np.cbrt([(wt(i, j) * wt(i, k) * wt(k, j)) for k in isuccs & jpreds])
-            )
-            directed_triangles += sum(
-                np.cbrt([(wt(i, j) * wt(i, k) * wt(j, k)) for k in isuccs & jsuccs])
-            )
-
-        dtotal = len(ipreds) + len(isuccs)
-        dbidirectional = len(ipreds & isuccs)
-        ret.append([i, dtotal, dbidirectional, directed_triangles])
-    return ret
-
-
-@not_implemented_for("multigraph")
-def _directed_weighted_triangles_and_degree_iter(
-    G, nodes=None, weight="weight", n_workers=None
-):
-    """Return an iterator of
-    (node, total_degree, reciprocal_degree, directed_weighted_triangles).
-
-    Used for directed weighted clustering.
-    Note that unlike `_weighted_triangles_and_degree_iter()`, this function counts
-    directed triangles so does not count triangles twice.
-
-    """
-
-    if weight is None or G.number_of_edges() == 0:
-        max_weight = 1
-    else:
-        max_weight = max(d.get(weight, 1) for u, v, d in G.edges)
-
-    nodes_nbrs = ((n, G._pred[n], G._adj[n]) for n in G.nbunch_iter(nodes))
-
-    def wt(u, v):
-        return G[u][v].get(weight, 1) / max_weight
-
-    if n_workers is not None:
-        import random
-
-        from functools import partial
-        from multiprocessing import Pool
-
-        _local_directed_weighted_triangles_and_degree_function = partial(
-            _local_directed_weighted_triangles_and_degree_parallel,
-            G=G,
-            weight=weight,
-            max_weight=max_weight,
-        )
-        nodes_nbrs = list(nodes_nbrs)
-        random.shuffle(nodes_nbrs)
-        if len(nodes_nbrs) > n_workers * 30000:
-            nodes_nbrs = split_len(nodes, step=30000)
-        else:
-            nodes_nbrs = split(nodes_nbrs, n_workers)
-        with Pool(n_workers) as p:
-            ret = p.imap(
-                _local_directed_weighted_triangles_and_degree_function, nodes_nbrs
-            )
-            for r in ret:
-                for x in r:
-                    yield x
-
-    else:
-        for i, preds, succs in nodes_nbrs:
-            ipreds = set(preds) - {i}
-            isuccs = set(succs) - {i}
-
-            directed_triangles = 0
-            for j in ipreds:
-                jpreds = set(G._pred[j]) - {j}
-                jsuccs = set(G._adj[j]) - {j}
-                directed_triangles += sum(
-                    np.cbrt([(wt(j, i) * wt(k, i) * wt(k, j)) for k in ipreds & jpreds])
-                )
-                directed_triangles += sum(
-                    np.cbrt([(wt(j, i) * wt(k, i) * wt(j, k)) for k in ipreds & jsuccs])
-                )
-                directed_triangles += sum(
-                    np.cbrt([(wt(j, i) * wt(i, k) * wt(k, j)) for k in isuccs & jpreds])
-                )
-                directed_triangles += sum(
-                    np.cbrt([(wt(j, i) * wt(i, k) * wt(j, k)) for k in isuccs & jsuccs])
-                )
-
-            for j in isuccs:
-                jpreds = set(G._pred[j]) - {j}
-                jsuccs = set(G._adj[j]) - {j}
-                directed_triangles += sum(
-                    np.cbrt([(wt(i, j) * wt(k, i) * wt(k, j)) for k in ipreds & jpreds])
-                )
-                directed_triangles += sum(
-                    np.cbrt([(wt(i, j) * wt(k, i) * wt(j, k)) for k in ipreds & jsuccs])
-                )
-                directed_triangles += sum(
-                    np.cbrt([(wt(i, j) * wt(i, k) * wt(k, j)) for k in isuccs & jpreds])
-                )
-                directed_triangles += sum(
-                    np.cbrt([(wt(i, j) * wt(i, k) * wt(j, k)) for k in isuccs & jsuccs])
-                )
-
-            dtotal = len(ipreds) + len(isuccs)
-            dbidirectional = len(ipreds & isuccs)
-            yield (i, dtotal, dbidirectional, directed_triangles)
-
-
-def average_clustering(G, nodes=None, weight=None, count_zeros=True, n_workers=None):
-    r"""Compute the average clustering coefficient for the graph G.
-
-    The clustering coefficient for the graph is the average,
-
-    .. math::
-
-       C = \frac{1}{n}\sum_{v \in G} c_v,
-
-    where :math:`n` is the number of nodes in `G`.
-
-    Parameters
-    ----------
-    G : graph
-
-    nodes : container of nodes, optional (default=all nodes in G)
-       Compute average clustering for nodes in this container.
-
-    weight : string or None, optional (default=None)
-       The edge attribute that holds the numerical value used as a weight.
-       If None, then each edge has weight 1.
-
-    count_zeros : bool
-       If False include only the nodes with nonzero clustering in the average.
-
-    Returns
-    -------
-    avg : float
-       Average clustering
-
-    Examples
-    --------
-    >>> G = eg.complete_graph(5)
-    >>> print(eg.average_clustering(G))
-    1.0
-
-    Notes
-    -----
-    This is a space saving routine; it might be faster
-    to use the clustering function to get a list and then take the average.
-
-    Self loops are ignored.
-
-    References
-    ----------
-    .. [1] Generalizations of the clustering coefficient to weighted
-       complex networks by J. Saramäki, M. Kivelä, J.-P. Onnela,
-       K. Kaski, and J. Kertész, Physical Review E, 75 027105 (2007).
-       http://jponnela.com/web_documents/a9.pdf
-    .. [2] Marcus Kaiser,  Mean clustering coefficients: the role of isolated
-       nodes and leafs on clustering measures for small-world networks.
-       https://arxiv.org/abs/0802.2512
-    """
-    c = clustering(G, nodes, weight=weight, n_workers=n_workers).values()
-    if not count_zeros:
-        c = [v for v in c if abs(v) > 0]
-    return sum(c) / len(c)
-
-
-def _local_directed_triangles_and_degree_iter_parallel(nodes_nbrs, G):
-    ret = []
-    for i, preds, succs in nodes_nbrs:
-        ipreds = set(preds) - {i}
-        isuccs = set(succs) - {i}
-
-        directed_triangles = 0
-        for j in chain(ipreds, isuccs):
-            jpreds = set(G._pred[j]) - {j}
-            jsuccs = set(G._adj[j]) - {j}
-            directed_triangles += sum(
-                1
-                for k in chain(
-                    (ipreds & jpreds),
-                    (ipreds & jsuccs),
-                    (isuccs & jpreds),
-                    (isuccs & jsuccs),
-                )
-            )
-        dtotal = len(ipreds) + len(isuccs)
-        dbidirectional = len(ipreds & isuccs)
-        ret.append((i, dtotal, dbidirectional, directed_triangles))
-    return ret
-
-
-@not_implemented_for("multigraph")
-def _directed_triangles_and_degree_iter(G, nodes=None, n_workers=None):
-    """Return an iterator of
-    (node, total_degree, reciprocal_degree, directed_triangles).
-
-    Used for directed clustering.
-    Note that unlike `_triangles_and_degree_iter()`, this function counts
-    directed triangles so does not count triangles twice.
-
-    """
-    nodes_nbrs = ((n, G._pred[n], G._adj[n]) for n in G.nbunch_iter(nodes))
-
-    if n_workers is not None:
-        import random
-
-        from functools import partial
-        from multiprocessing import Pool
-
-        _local_directed_triangles_and_degree_iter_parallel_function = partial(
-            _local_directed_triangles_and_degree_iter_parallel, G=G
-        )
-        nodes_nbrs = list(nodes_nbrs)
-        random.shuffle(nodes_nbrs)
-        if len(nodes_nbrs) > n_workers * 30000:
-            nodes_nbrs = split_len(nodes_nbrs, step=30000)
-        else:
-            nodes_nbrs = split(nodes_nbrs, n_workers)
-
-        with Pool(n_workers) as p:
-            ret = p.imap(
-                _local_directed_triangles_and_degree_iter_parallel_function, nodes_nbrs
-            )
-            for r in ret:
-                for x in r:
-                    yield x
-    else:
-        for i, preds, succs in nodes_nbrs:
-            ipreds = set(preds) - {i}
-            isuccs = set(succs) - {i}
-
-            directed_triangles = 0
-            for j in chain(ipreds, isuccs):
-                jpreds = set(G._pred[j]) - {j}
-                jsuccs = set(G._adj[j]) - {j}
-                directed_triangles += sum(
-                    1
-                    for k in chain(
-                        (ipreds & jpreds),
-                        (ipreds & jsuccs),
-                        (isuccs & jpreds),
-                        (isuccs & jsuccs),
-                    )
-                )
-            dtotal = len(ipreds) + len(isuccs)
-            dbidirectional = len(ipreds & isuccs)
-            yield (i, dtotal, dbidirectional, directed_triangles)
-
-
-def _local_triangles_and_degree_iter_function_parallel(nodes_nbrs, G):
-    ret = []
-    for v, v_nbrs in nodes_nbrs:
-        vs = set(v_nbrs) - {v}
-        gen_degree = Counter(len(vs & (set(G[w]) - {w})) for w in vs)
-        ntriangles = sum(k * val for k, val in gen_degree.items())
-        ret.append((v, len(vs), ntriangles, gen_degree))
-    return ret
-
-
-@not_implemented_for("multigraph")
-def _triangles_and_degree_iter(G, nodes=None, n_workers=None):
-    """Return an iterator of (node, degree, triangles, generalized degree).
-
-    This double counts triangles so you may want to divide by 2.
-    See degree(), triangles() and generalized_degree() for definitions
-    and details.
-
-    """
-    if nodes is None:
-        nodes_nbrs = G.adj.items()
-    else:
-        nodes_nbrs = ((n, G[n]) for n in G.nbunch_iter(nodes))
-
-    if n_workers is not None:
-        import random
-
-        from functools import partial
-        from multiprocessing import Pool
-
-        _local_triangles_and_degree_iter_function = partial(
-            _local_triangles_and_degree_iter_function_parallel, G=G
-        )
-        nodes_nbrs = list(nodes_nbrs)
-        random.shuffle(nodes_nbrs)
-        if len(nodes_nbrs) > n_workers * 30000:
-            nodes_nbrs = split_len(nodes_nbrs, step=30000)
-        else:
-            nodes_nbrs = split(nodes_nbrs, n_workers)
-
-        with Pool(n_workers) as p:
-            ret = p.imap(_local_triangles_and_degree_iter_function, nodes_nbrs)
-            for r in ret:
-                for x in r:
-                    yield x
-    else:
-        for v, v_nbrs in nodes_nbrs:
-            vs = set(v_nbrs) - {v}
-            gen_degree = Counter(len(vs & (set(G[w]) - {w})) for w in vs)
-            ntriangles = sum(k * val for k, val in gen_degree.items())
-            yield (v, len(vs), ntriangles, gen_degree)
-
-
-@hybrid("cpp_clustering")
-def clustering(G, nodes=None, weight=None, n_workers=None):
-    r"""Compute the clustering coefficient for nodes.
-
-    For unweighted graphs, the clustering of a node :math:`u`
-    is the fraction of possible triangles through that node that exist,
-
-    .. math::
-
-      c_u = \frac{2 T(u)}{deg(u)(deg(u)-1)},
-
-    where :math:`T(u)` is the number of triangles through node :math:`u` and
-    :math:`deg(u)` is the degree of :math:`u`.
-
-    For weighted graphs, there are several ways to define clustering [1]_.
-    the one used here is defined
-    as the geometric average of the subgraph edge weights [2]_,
-
-    .. math::
-
-       c_u = \frac{1}{deg(u)(deg(u)-1))}
-             \sum_{vw} (\hat{w}_{uv} \hat{w}_{uw} \hat{w}_{vw})^{1/3}.
-
-    The edge weights :math:`\hat{w}_{uv}` are normalized by the maximum weight
-    in the network :math:`\hat{w}_{uv} = w_{uv}/\max(w)`.
-
-    The value of :math:`c_u` is assigned to 0 if :math:`deg(u) < 2`.
-
-    Additionally, this weighted definition has been generalized to support negative edge weights [3]_.
-
-    For directed graphs, the clustering is similarly defined as the fraction
-    of all possible directed triangles or geometric average of the subgraph
-    edge weights for unweighted and weighted directed graph respectively [4]_.
-
-    .. math::
-
-       c_u = \frac{2}{deg^{tot}(u)(deg^{tot}(u)-1) - 2deg^{\leftrightarrow}(u)}
-             T(u),
-
-    where :math:`T(u)` is the number of directed triangles through node
-    :math:`u`, :math:`deg^{tot}(u)` is the sum of in degree and out degree of
-    :math:`u` and :math:`deg^{\leftrightarrow}(u)` is the reciprocal degree of
-    :math:`u`.
-
-
-    Parameters
-    ----------
-    G : graph
-
-    nodes : container of nodes, optional (default=all nodes in G)
-       Compute clustering for nodes in this container.
-
-    weight : string or None, optional (default=None)
-       The edge attribute that holds the numerical value used as a weight.
-       If None, then each edge has weight 1.
-
-    Returns
-    -------
-    out : float, or dictionary
-       Clustering coefficient at specified nodes
-
-    Examples
-    --------
-    >>> G = eg.complete_graph(5)
-    >>> print(eg.clustering(G, 0))
-    1.0
-    >>> print(eg.clustering(G))
-    {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}
-
-    Notes
-    -----
-    Self loops are ignored.
-
-        References
-        ----------
-        .. [1] Generalizations of the clustering coefficient to weighted
-           complex networks by J. Saramäki, M. Kivelä, J.-P. Onnela,
-           K. Kaski, and J. Kertész, Physical Review E, 75 027105 (2007).
-           http://jponnela.com/web_documents/a9.pdf
-        .. [2] Intensity and coherence of motifs in weighted complex
-           networks by J. P. Onnela, J. Saramäki, J. Kertész, and K. Kaski,
-           Physical Review E, 71(6), 065103 (2005).
-        .. [3] Generalization of Clustering Coefficients to Signed Correlation Networks
-           by G. Costantini and M. Perugini, PloS one, 9(2), e88669 (2014).
-        .. [4] Clustering in complex directed networks by G. Fagiolo,
-           Physical Review E, 76(2), 026107 (2007).
-    """
-
-    if G.is_directed():
-        if weight is not None:
-            td_iter = _directed_weighted_triangles_and_degree_iter(
-                G, nodes, weight, n_workers=n_workers
-            )
-            clusterc = {
-                v: 0 if t == 0 else t / ((dt * (dt - 1) - 2 * db) * 2)
-                for v, dt, db, t in td_iter
-            }
-        else:
-            td_iter = _directed_triangles_and_degree_iter(G, nodes, n_workers=n_workers)
-            clusterc = {
-                v: 0 if t == 0 else t / ((dt * (dt - 1) - 2 * db) * 2)
-                for v, dt, db, t in td_iter
-            }
-    else:
-        # The formula 2*T/(d*(d-1)) from docs is t/(d*(d-1)) here b/c t==2*T
-        if weight is not None:
-            td_iter = _weighted_triangles_and_degree_iter(
-                G, nodes, weight, n_workers=n_workers
-            )
-            clusterc = {v: 0 if t == 0 else t / (d * (d - 1)) for v, d, t in td_iter}
-        else:
-            td_iter = _triangles_and_degree_iter(G, nodes, n_workers=n_workers)
-            clusterc = {v: 0 if t == 0 else t / (d * (d - 1)) for v, d, t, _ in td_iter}
-    if nodes in G:
-        # Return the value of the sole entry in the dictionary.
-        return clusterc[nodes]
-    return clusterc
+from collections import Counter
+from itertools import chain
+
+import numpy as np
+
+from easygraph.utils.decorators import hybrid
+from easygraph.utils.decorators import not_implemented_for
+from easygraph.utils.misc import split
+from easygraph.utils.misc import split_len
+
+
+__all__ = ["average_clustering", "clustering"]
+
+
+def _local_weighted_triangles_and_degree_iter_parallel(
+    nodes_nbrs, G, weight, max_weight
+):
+    ret = []
+
+    def wt(u, v):
+        return G[u][v].get(weight, 1) / max_weight
+
+    for i, nbrs in nodes_nbrs:
+        inbrs = set(nbrs) - {i}
+        weighted_triangles = 0
+        seen = set()
+        for j in inbrs:
+            seen.add(j)
+            # This avoids counting twice -- we double at the end.
+            jnbrs = set(G[j]) - seen
+            # Only compute the edge weight once, before the inner inner
+            # loop.
+            wij = wt(i, j)
+            weighted_triangles += sum(
+                np.cbrt([(wij * wt(j, k) * wt(k, i)) for k in inbrs & jnbrs])
+            )
+        ret.append((i, len(inbrs), 2 * weighted_triangles))
+    return ret
+
+
+@not_implemented_for("multigraph")
+def _weighted_triangles_and_degree_iter(G, nodes=None, weight="weight", n_workers=None):
+    """Return an iterator of (node, degree, weighted_triangles).
+
+    Used for weighted clustering.
+    Note: this returns the geometric average weight of edges in the triangle.
+    Also, each triangle is counted twice (each direction).
+    So you may want to divide by 2.
+
+    """
+
+    if weight is None or G.number_of_edges() == 0:
+        max_weight = 1
+    else:
+        max_weight = max(d.get(weight, 1) for u, v, d in G.edges)
+    if nodes is None:
+        nodes_nbrs = G.adj.items()
+    else:
+        nodes_nbrs = ((n, G[n]) for n in G.nbunch_iter(nodes))
+
+    def wt(u, v):
+        return G[u][v].get(weight, 1) / max_weight
+
+    if n_workers is not None:
+        import random
+
+        from functools import partial
+        from multiprocessing import Pool
+
+        _local_weighted_triangles_and_degree_iter_function = partial(
+            _local_weighted_triangles_and_degree_iter_parallel,
+            G=G,
+            weight=weight,
+            max_weight=max_weight,
+        )
+        nodes_nbrs = list(nodes_nbrs)
+        random.shuffle(nodes_nbrs)
+        if len(nodes_nbrs) > n_workers * 30000:
+            nodes_nbrs = split_len(nodes, step=30000)
+        else:
+            nodes_nbrs = split(nodes_nbrs, n_workers)
+        with Pool(n_workers) as p:
+            ret = p.imap(_local_weighted_triangles_and_degree_iter_function, nodes_nbrs)
+            for r in ret:
+                for x in r:
+                    yield x
+    else:
+        for i, nbrs in nodes_nbrs:
+            inbrs = set(nbrs) - {i}
+            weighted_triangles = 0
+            seen = set()
+            for j in inbrs:
+                seen.add(j)
+                # This avoids counting twice -- we double at the end.
+                jnbrs = set(G[j]) - seen
+                # Only compute the edge weight once, before the inner inner
+                # loop.
+                wij = wt(i, j)
+                weighted_triangles += sum(
+                    np.cbrt([(wij * wt(j, k) * wt(k, i)) for k in inbrs & jnbrs])
+                )
+            yield (i, len(inbrs), 2 * weighted_triangles)
+
+
+def _local_directed_weighted_triangles_and_degree_parallel(
+    nodes_nbrs, G, weight, max_weight
+):
+    ret = []
+
+    def wt(u, v):
+        return G[u][v].get(weight, 1) / max_weight
+
+    for i, preds, succs in nodes_nbrs:
+        ipreds = set(preds) - {i}
+        isuccs = set(succs) - {i}
+
+        directed_triangles = 0
+        for j in ipreds:
+            jpreds = set(G._pred[j]) - {j}
+            jsuccs = set(G._adj[j]) - {j}
+            directed_triangles += sum(
+                np.cbrt([(wt(j, i) * wt(k, i) * wt(k, j)) for k in ipreds & jpreds])
+            )
+            directed_triangles += sum(
+                np.cbrt([(wt(j, i) * wt(k, i) * wt(j, k)) for k in ipreds & jsuccs])
+            )
+            directed_triangles += sum(
+                np.cbrt([(wt(j, i) * wt(i, k) * wt(k, j)) for k in isuccs & jpreds])
+            )
+            directed_triangles += sum(
+                np.cbrt([(wt(j, i) * wt(i, k) * wt(j, k)) for k in isuccs & jsuccs])
+            )
+
+        for j in isuccs:
+            jpreds = set(G._pred[j]) - {j}
+            jsuccs = set(G._adj[j]) - {j}
+            directed_triangles += sum(
+                np.cbrt([(wt(i, j) * wt(k, i) * wt(k, j)) for k in ipreds & jpreds])
+            )
+            directed_triangles += sum(
+                np.cbrt([(wt(i, j) * wt(k, i) * wt(j, k)) for k in ipreds & jsuccs])
+            )
+            directed_triangles += sum(
+                np.cbrt([(wt(i, j) * wt(i, k) * wt(k, j)) for k in isuccs & jpreds])
+            )
+            directed_triangles += sum(
+                np.cbrt([(wt(i, j) * wt(i, k) * wt(j, k)) for k in isuccs & jsuccs])
+            )
+
+        dtotal = len(ipreds) + len(isuccs)
+        dbidirectional = len(ipreds & isuccs)
+        ret.append([i, dtotal, dbidirectional, directed_triangles])
+    return ret
+
+
+@not_implemented_for("multigraph")
+def _directed_weighted_triangles_and_degree_iter(
+    G, nodes=None, weight="weight", n_workers=None
+):
+    """Return an iterator of
+    (node, total_degree, reciprocal_degree, directed_weighted_triangles).
+
+    Used for directed weighted clustering.
+    Note that unlike `_weighted_triangles_and_degree_iter()`, this function counts
+    directed triangles so does not count triangles twice.
+
+    """
+
+    if weight is None or G.number_of_edges() == 0:
+        max_weight = 1
+    else:
+        max_weight = max(d.get(weight, 1) for u, v, d in G.edges)
+
+    nodes_nbrs = ((n, G._pred[n], G._adj[n]) for n in G.nbunch_iter(nodes))
+
+    def wt(u, v):
+        return G[u][v].get(weight, 1) / max_weight
+
+    if n_workers is not None:
+        import random
+
+        from functools import partial
+        from multiprocessing import Pool
+
+        _local_directed_weighted_triangles_and_degree_function = partial(
+            _local_directed_weighted_triangles_and_degree_parallel,
+            G=G,
+            weight=weight,
+            max_weight=max_weight,
+        )
+        nodes_nbrs = list(nodes_nbrs)
+        random.shuffle(nodes_nbrs)
+        if len(nodes_nbrs) > n_workers * 30000:
+            nodes_nbrs = split_len(nodes, step=30000)
+        else:
+            nodes_nbrs = split(nodes_nbrs, n_workers)
+        with Pool(n_workers) as p:
+            ret = p.imap(
+                _local_directed_weighted_triangles_and_degree_function, nodes_nbrs
+            )
+            for r in ret:
+                for x in r:
+                    yield x
+
+    else:
+        for i, preds, succs in nodes_nbrs:
+            ipreds = set(preds) - {i}
+            isuccs = set(succs) - {i}
+
+            directed_triangles = 0
+            for j in ipreds:
+                jpreds = set(G._pred[j]) - {j}
+                jsuccs = set(G._adj[j]) - {j}
+                directed_triangles += sum(
+                    np.cbrt([(wt(j, i) * wt(k, i) * wt(k, j)) for k in ipreds & jpreds])
+                )
+                directed_triangles += sum(
+                    np.cbrt([(wt(j, i) * wt(k, i) * wt(j, k)) for k in ipreds & jsuccs])
+                )
+                directed_triangles += sum(
+                    np.cbrt([(wt(j, i) * wt(i, k) * wt(k, j)) for k in isuccs & jpreds])
+                )
+                directed_triangles += sum(
+                    np.cbrt([(wt(j, i) * wt(i, k) * wt(j, k)) for k in isuccs & jsuccs])
+                )
+
+            for j in isuccs:
+                jpreds = set(G._pred[j]) - {j}
+                jsuccs = set(G._adj[j]) - {j}
+                directed_triangles += sum(
+                    np.cbrt([(wt(i, j) * wt(k, i) * wt(k, j)) for k in ipreds & jpreds])
+                )
+                directed_triangles += sum(
+                    np.cbrt([(wt(i, j) * wt(k, i) * wt(j, k)) for k in ipreds & jsuccs])
+                )
+                directed_triangles += sum(
+                    np.cbrt([(wt(i, j) * wt(i, k) * wt(k, j)) for k in isuccs & jpreds])
+                )
+                directed_triangles += sum(
+                    np.cbrt([(wt(i, j) * wt(i, k) * wt(j, k)) for k in isuccs & jsuccs])
+                )
+
+            dtotal = len(ipreds) + len(isuccs)
+            dbidirectional = len(ipreds & isuccs)
+            yield (i, dtotal, dbidirectional, directed_triangles)
+
+
+def average_clustering(G, nodes=None, weight=None, count_zeros=True, n_workers=None):
+    r"""Compute the average clustering coefficient for the graph G.
+
+    The clustering coefficient for the graph is the average,
+
+    .. math::
+
+       C = \frac{1}{n}\sum_{v \in G} c_v,
+
+    where :math:`n` is the number of nodes in `G`.
+
+    Parameters
+    ----------
+    G : graph
+
+    nodes : container of nodes, optional (default=all nodes in G)
+       Compute average clustering for nodes in this container.
+
+    weight : string or None, optional (default=None)
+       The edge attribute that holds the numerical value used as a weight.
+       If None, then each edge has weight 1.
+
+    count_zeros : bool
+       If False include only the nodes with nonzero clustering in the average.
+
+    Returns
+    -------
+    avg : float
+       Average clustering
+
+    Examples
+    --------
+    >>> G = eg.complete_graph(5)
+    >>> print(eg.average_clustering(G))
+    1.0
+
+    Notes
+    -----
+    This is a space saving routine; it might be faster
+    to use the clustering function to get a list and then take the average.
+
+    Self loops are ignored.
+
+    References
+    ----------
+    .. [1] Generalizations of the clustering coefficient to weighted
+       complex networks by J. Saramäki, M. Kivelä, J.-P. Onnela,
+       K. Kaski, and J. Kertész, Physical Review E, 75 027105 (2007).
+       http://jponnela.com/web_documents/a9.pdf
+    .. [2] Marcus Kaiser,  Mean clustering coefficients: the role of isolated
+       nodes and leafs on clustering measures for small-world networks.
+       https://arxiv.org/abs/0802.2512
+    """
+    c = clustering(G, nodes, weight=weight, n_workers=n_workers).values()
+    if not count_zeros:
+        c = [v for v in c if abs(v) > 0]
+    return sum(c) / len(c)
+
+
+def _local_directed_triangles_and_degree_iter_parallel(nodes_nbrs, G):
+    ret = []
+    for i, preds, succs in nodes_nbrs:
+        ipreds = set(preds) - {i}
+        isuccs = set(succs) - {i}
+
+        directed_triangles = 0
+        for j in chain(ipreds, isuccs):
+            jpreds = set(G._pred[j]) - {j}
+            jsuccs = set(G._adj[j]) - {j}
+            directed_triangles += sum(
+                1
+                for k in chain(
+                    (ipreds & jpreds),
+                    (ipreds & jsuccs),
+                    (isuccs & jpreds),
+                    (isuccs & jsuccs),
+                )
+            )
+        dtotal = len(ipreds) + len(isuccs)
+        dbidirectional = len(ipreds & isuccs)
+        ret.append((i, dtotal, dbidirectional, directed_triangles))
+    return ret
+
+
+@not_implemented_for("multigraph")
+def _directed_triangles_and_degree_iter(G, nodes=None, n_workers=None):
+    """Return an iterator of
+    (node, total_degree, reciprocal_degree, directed_triangles).
+
+    Used for directed clustering.
+    Note that unlike `_triangles_and_degree_iter()`, this function counts
+    directed triangles so does not count triangles twice.
+
+    """
+    nodes_nbrs = ((n, G._pred[n], G._adj[n]) for n in G.nbunch_iter(nodes))
+
+    if n_workers is not None:
+        import random
+
+        from functools import partial
+        from multiprocessing import Pool
+
+        _local_directed_triangles_and_degree_iter_parallel_function = partial(
+            _local_directed_triangles_and_degree_iter_parallel, G=G
+        )
+        nodes_nbrs = list(nodes_nbrs)
+        random.shuffle(nodes_nbrs)
+        if len(nodes_nbrs) > n_workers * 30000:
+            nodes_nbrs = split_len(nodes_nbrs, step=30000)
+        else:
+            nodes_nbrs = split(nodes_nbrs, n_workers)
+
+        with Pool(n_workers) as p:
+            ret = p.imap(
+                _local_directed_triangles_and_degree_iter_parallel_function, nodes_nbrs
+            )
+            for r in ret:
+                for x in r:
+                    yield x
+    else:
+        for i, preds, succs in nodes_nbrs:
+            ipreds = set(preds) - {i}
+            isuccs = set(succs) - {i}
+
+            directed_triangles = 0
+            for j in chain(ipreds, isuccs):
+                jpreds = set(G._pred[j]) - {j}
+                jsuccs = set(G._adj[j]) - {j}
+                directed_triangles += sum(
+                    1
+                    for k in chain(
+                        (ipreds & jpreds),
+                        (ipreds & jsuccs),
+                        (isuccs & jpreds),
+                        (isuccs & jsuccs),
+                    )
+                )
+            dtotal = len(ipreds) + len(isuccs)
+            dbidirectional = len(ipreds & isuccs)
+            yield (i, dtotal, dbidirectional, directed_triangles)
+
+
+def _local_triangles_and_degree_iter_function_parallel(nodes_nbrs, G):
+    ret = []
+    for v, v_nbrs in nodes_nbrs:
+        vs = set(v_nbrs) - {v}
+        gen_degree = Counter(len(vs & (set(G[w]) - {w})) for w in vs)
+        ntriangles = sum(k * val for k, val in gen_degree.items())
+        ret.append((v, len(vs), ntriangles, gen_degree))
+    return ret
+
+
+@not_implemented_for("multigraph")
+def _triangles_and_degree_iter(G, nodes=None, n_workers=None):
+    """Return an iterator of (node, degree, triangles, generalized degree).
+
+    This double counts triangles so you may want to divide by 2.
+    See degree(), triangles() and generalized_degree() for definitions
+    and details.
+
+    """
+    if nodes is None:
+        nodes_nbrs = G.adj.items()
+    else:
+        nodes_nbrs = ((n, G[n]) for n in G.nbunch_iter(nodes))
+
+    if n_workers is not None:
+        import random
+
+        from functools import partial
+        from multiprocessing import Pool
+
+        _local_triangles_and_degree_iter_function = partial(
+            _local_triangles_and_degree_iter_function_parallel, G=G
+        )
+        nodes_nbrs = list(nodes_nbrs)
+        random.shuffle(nodes_nbrs)
+        if len(nodes_nbrs) > n_workers * 30000:
+            nodes_nbrs = split_len(nodes_nbrs, step=30000)
+        else:
+            nodes_nbrs = split(nodes_nbrs, n_workers)
+
+        with Pool(n_workers) as p:
+            ret = p.imap(_local_triangles_and_degree_iter_function, nodes_nbrs)
+            for r in ret:
+                for x in r:
+                    yield x
+    else:
+        for v, v_nbrs in nodes_nbrs:
+            vs = set(v_nbrs) - {v}
+            gen_degree = Counter(len(vs & (set(G[w]) - {w})) for w in vs)
+            ntriangles = sum(k * val for k, val in gen_degree.items())
+            yield (v, len(vs), ntriangles, gen_degree)
+
+
+@hybrid("cpp_clustering")
+def clustering(G, nodes=None, weight=None, n_workers=None):
+    r"""Compute the clustering coefficient for nodes.
+
+    For unweighted graphs, the clustering of a node :math:`u`
+    is the fraction of possible triangles through that node that exist,
+
+    .. math::
+
+      c_u = \frac{2 T(u)}{deg(u)(deg(u)-1)},
+
+    where :math:`T(u)` is the number of triangles through node :math:`u` and
+    :math:`deg(u)` is the degree of :math:`u`.
+
+    For weighted graphs, there are several ways to define clustering [1]_.
+    the one used here is defined
+    as the geometric average of the subgraph edge weights [2]_,
+
+    .. math::
+
+       c_u = \frac{1}{deg(u)(deg(u)-1))}
+             \sum_{vw} (\hat{w}_{uv} \hat{w}_{uw} \hat{w}_{vw})^{1/3}.
+
+    The edge weights :math:`\hat{w}_{uv}` are normalized by the maximum weight
+    in the network :math:`\hat{w}_{uv} = w_{uv}/\max(w)`.
+
+    The value of :math:`c_u` is assigned to 0 if :math:`deg(u) < 2`.
+
+    Additionally, this weighted definition has been generalized to support negative edge weights [3]_.
+
+    For directed graphs, the clustering is similarly defined as the fraction
+    of all possible directed triangles or geometric average of the subgraph
+    edge weights for unweighted and weighted directed graph respectively [4]_.
+
+    .. math::
+
+       c_u = \frac{2}{deg^{tot}(u)(deg^{tot}(u)-1) - 2deg^{\leftrightarrow}(u)}
+             T(u),
+
+    where :math:`T(u)` is the number of directed triangles through node
+    :math:`u`, :math:`deg^{tot}(u)` is the sum of in degree and out degree of
+    :math:`u` and :math:`deg^{\leftrightarrow}(u)` is the reciprocal degree of
+    :math:`u`.
+
+
+    Parameters
+    ----------
+    G : graph
+
+    nodes : container of nodes, optional (default=all nodes in G)
+       Compute clustering for nodes in this container.
+
+    weight : string or None, optional (default=None)
+       The edge attribute that holds the numerical value used as a weight.
+       If None, then each edge has weight 1.
+
+    Returns
+    -------
+    out : float, or dictionary
+       Clustering coefficient at specified nodes
+
+    Examples
+    --------
+    >>> G = eg.complete_graph(5)
+    >>> print(eg.clustering(G, 0))
+    1.0
+    >>> print(eg.clustering(G))
+    {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}
+
+    Notes
+    -----
+    Self loops are ignored.
+
+        References
+        ----------
+        .. [1] Generalizations of the clustering coefficient to weighted
+           complex networks by J. Saramäki, M. Kivelä, J.-P. Onnela,
+           K. Kaski, and J. Kertész, Physical Review E, 75 027105 (2007).
+           http://jponnela.com/web_documents/a9.pdf
+        .. [2] Intensity and coherence of motifs in weighted complex
+           networks by J. P. Onnela, J. Saramäki, J. Kertész, and K. Kaski,
+           Physical Review E, 71(6), 065103 (2005).
+        .. [3] Generalization of Clustering Coefficients to Signed Correlation Networks
+           by G. Costantini and M. Perugini, PloS one, 9(2), e88669 (2014).
+        .. [4] Clustering in complex directed networks by G. Fagiolo,
+           Physical Review E, 76(2), 026107 (2007).
+    """
+
+    if G.is_directed():
+        if weight is not None:
+            td_iter = _directed_weighted_triangles_and_degree_iter(
+                G, nodes, weight, n_workers=n_workers
+            )
+            clusterc = {
+                v: 0 if t == 0 else t / ((dt * (dt - 1) - 2 * db) * 2)
+                for v, dt, db, t in td_iter
+            }
+        else:
+            td_iter = _directed_triangles_and_degree_iter(G, nodes, n_workers=n_workers)
+            clusterc = {
+                v: 0 if t == 0 else t / ((dt * (dt - 1) - 2 * db) * 2)
+                for v, dt, db, t in td_iter
+            }
+    else:
+        # The formula 2*T/(d*(d-1)) from docs is t/(d*(d-1)) here b/c t==2*T
+        if weight is not None:
+            td_iter = _weighted_triangles_and_degree_iter(
+                G, nodes, weight, n_workers=n_workers
+            )
+            clusterc = {v: 0 if t == 0 else t / (d * (d - 1)) for v, d, t in td_iter}
+        else:
+            td_iter = _triangles_and_degree_iter(G, nodes, n_workers=n_workers)
+            clusterc = {v: 0 if t == 0 else t / (d * (d - 1)) for v, d, t, _ in td_iter}
+    if nodes in G:
+        # Return the value of the sole entry in the dictionary.
+        return clusterc[nodes]
+    return clusterc
```

## easygraph/functions/basic/__init__.py

 * *Ordering differences only*

```diff
@@ -1,4 +1,4 @@
-from .avg_degree import *
-from .cluster import *
-from .localassort import *
-from .predecessor_path_based import *
+from .avg_degree import *
+from .cluster import *
+from .localassort import *
+from .predecessor_path_based import *
```

## easygraph/functions/basic/tests/test_predecessor.py

 * *Ordering differences only*

```diff
@@ -1,18 +1,18 @@
-import easygraph as eg
-import pytest
-
-
-class TestPredecessor:
-    # @classmethod
-    # def setup_class(self):
-    #     pytest.importskip("numpy")
-
-    def test_predecessor(self):
-        G = eg.path_graph(4)
-        for source in G:
-            assert eg.predecessor(G, source) in [
-                {0: [], 1: [0], 2: [1], 3: [2]},
-                {1: [], 0: [1], 2: [1], 3: [2]},
-                {2: [], 1: [2], 3: [2], 0: [1]},
-                {3: [], 2: [3], 1: [2], 0: [1]},
-            ]
+import easygraph as eg
+import pytest
+
+
+class TestPredecessor:
+    # @classmethod
+    # def setup_class(self):
+    #     pytest.importskip("numpy")
+
+    def test_predecessor(self):
+        G = eg.path_graph(4)
+        for source in G:
+            assert eg.predecessor(G, source) in [
+                {0: [], 1: [0], 2: [1], 3: [2]},
+                {1: [], 0: [1], 2: [1], 3: [2]},
+                {2: [], 1: [2], 3: [2], 0: [1]},
+                {3: [], 2: [3], 1: [2], 0: [1]},
+            ]
```

## easygraph/functions/basic/tests/test_cluster.py

 * *Ordering differences only*

```diff
@@ -1,379 +1,379 @@
-import easygraph as eg
-import pytest
-
-
-class TestClustering:
-    @classmethod
-    def setup_class(cls):
-        pytest.importorskip("numpy")
-
-    def test_clustering(self):
-        G = eg.DiGraph()
-        G.add_edge("1", "2", weight=16)
-        G.add_edge("2", "3", weight=16)
-        G.add_edge("4", "3", weight=16)
-        G.add_edge("3", "4", weight=23)
-        G.add_edge("3", "5", weight=16)
-        G.add_edge("4", "2", weight=20)
-        print("clustering" in dir(eg))
-        assert eg.clustering(G) == {
-            "1": 0,
-            "2": 0.3333333333333333,
-            "3": 0.2,
-            "4": 0.5,
-            "5": 0,
-        }
-
-    def test_path(self):
-        G = eg.path_graph(10)
-        assert list(eg.clustering(G).values()) == [
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-        ]
-        assert eg.clustering(G) == {
-            0: 0,
-            1: 0,
-            2: 0,
-            3: 0,
-            4: 0,
-            5: 0,
-            6: 0,
-            7: 0,
-            8: 0,
-            9: 0,
-        }
-
-    def test_k5(self):
-        G = eg.complete_graph(5)
-        assert list(eg.clustering(G).values()) == [1, 1, 1, 1, 1]
-        assert eg.average_clustering(G) == 1
-        G.remove_edge(1, 2)
-        assert list(eg.clustering(G).values()) == [
-            5 / 6,
-            1,
-            1,
-            5 / 6,
-            5 / 6,
-        ]
-        assert eg.clustering(G, [1, 4]) == {1: 1, 4: 0.83333333333333337}
-
-    def test_k5_signed(self):
-        G = eg.complete_graph(5)
-        assert list(eg.clustering(G).values()) == [1, 1, 1, 1, 1]
-        assert eg.average_clustering(G) == 1
-        G.remove_edge(1, 2)
-        G.add_edge(0, 1, weight=-1)
-        assert list(eg.clustering(G, weight="weight").values()) == [
-            1 / 6,
-            -1 / 3,
-            1,
-            3 / 6,
-            3 / 6,
-        ]
-
-
-class TestDirectedClustering:
-    def test_clustering(self):
-        G = eg.DiGraph()
-        assert list(eg.clustering(G).values()) == []
-        assert eg.clustering(G) == {}
-
-    def test_path(self):
-        G = eg.path_graph(10, create_using=eg.DiGraph())
-        assert list(eg.clustering(G).values()) == [
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-        ]
-        assert eg.clustering(G) == {
-            0: 0,
-            1: 0,
-            2: 0,
-            3: 0,
-            4: 0,
-            5: 0,
-            6: 0,
-            7: 0,
-            8: 0,
-            9: 0,
-        }
-        assert eg.clustering(G, 0) == 0
-
-    def test_k5(self):
-        G = eg.complete_graph(5, create_using=eg.DiGraph())
-        assert list(eg.clustering(G).values()) == [1, 1, 1, 1, 1]
-        assert eg.average_clustering(G) == 1
-        G.remove_edge(1, 2)
-        assert list(eg.clustering(G).values()) == [
-            11 / 12,
-            1,
-            1,
-            11 / 12,
-            11 / 12,
-        ]
-        assert eg.clustering(G, [1, 4]) == {1: 1, 4: 11 / 12}
-        G.remove_edge(2, 1)
-        assert list(eg.clustering(G).values()) == [
-            5 / 6,
-            1,
-            1,
-            5 / 6,
-            5 / 6,
-        ]
-        assert eg.clustering(G, [1, 4]) == {1: 1, 4: 0.83333333333333337}
-        assert eg.clustering(G, 4) == 5 / 6
-
-    def test_triangle_and_edge(self):
-        G = eg.empty_graph(range(3), eg.DiGraph())
-        G.add_edges_from(eg.pairwise(range(3), cyclic=True))
-        G.add_edge(0, 4)
-        assert eg.clustering(G)[0] == 1 / 6
-
-
-class TestDirectedAverageClustering:
-    @classmethod
-    def setup_class(cls):
-        pytest.importorskip("numpy")
-
-    def test_empty(self):
-        G = eg.DiGraph()
-        with pytest.raises(ZeroDivisionError):
-            eg.average_clustering(G)
-
-    def test_average_clustering(self):
-        G = eg.empty_graph(range(3), eg.DiGraph())
-        G.add_edges_from(eg.pairwise(range(3), cyclic=True))
-        G.add_edge(2, 3)
-        assert eg.average_clustering(G) == (1 + 1 + 1 / 3) / 8
-        assert eg.average_clustering(G, count_zeros=True) == (1 + 1 + 1 / 3) / 8
-        assert eg.average_clustering(G, count_zeros=False) == (1 + 1 + 1 / 3) / 6
-        assert eg.average_clustering(G, [1, 2, 3]) == (1 + 1 / 3) / 6
-        assert eg.average_clustering(G, [1, 2, 3], count_zeros=True) == (1 + 1 / 3) / 6
-        assert eg.average_clustering(G, [1, 2, 3], count_zeros=False) == (1 + 1 / 3) / 4
-
-
-class TestAverageClustering:
-    @classmethod
-    def setup_class(cls):
-        pytest.importorskip("numpy")
-
-    def test_empty(self):
-        G = eg.Graph()
-        with pytest.raises(ZeroDivisionError):
-            eg.average_clustering(G)
-
-    def test_average_clustering(self):
-        G = eg.complete_graph(3)
-        G.add_edge(2, 3)
-
-        assert eg.average_clustering(G) == (1 + 1 + 1 / 3) / 4
-        assert eg.average_clustering(G, count_zeros=True) == (1 + 1 + 1 / 3) / 4
-        assert eg.average_clustering(G, count_zeros=False) == (1 + 1 + 1 / 3) / 3
-        assert eg.average_clustering(G, [1, 2, 3]) == (1 + 1 / 3) / 3
-        assert eg.average_clustering(G, [1, 2, 3], count_zeros=True) == (1 + 1 / 3) / 3
-        assert eg.average_clustering(G, [1, 2, 3], count_zeros=False) == (1 + 1 / 3) / 2
-
-    def test_average_clustering_signed(self):
-        G = eg.complete_graph(3)
-        G.add_edge(2, 3)
-        G.add_edge(0, 1, weight=-1)
-        assert eg.average_clustering(G, weight="weight") == (-1 - 1 - 1 / 3) / 4
-        assert (
-            eg.average_clustering(G, weight="weight", count_zeros=True)
-            == (-1 - 1 - 1 / 3) / 4
-        )
-        assert (
-            eg.average_clustering(G, weight="weight", count_zeros=False)
-            == (-1 - 1 - 1 / 3) / 3
-        )
-
-
-class TestDirectedWeightedClustering:
-    @classmethod
-    def setup_class(cls):
-        global np
-        np = pytest.importorskip("numpy")
-
-    def test_clustering(self):
-        G = eg.DiGraph()
-        assert list(eg.clustering(G, weight="weight").values()) == []
-        assert eg.clustering(G) == {}
-
-    def test_path(self):
-        G = eg.path_graph(10, create_using=eg.DiGraph())
-        print("type:", eg.clustering(G, weight="weight"))
-        assert list(eg.clustering(G, weight="weight").values()) == [
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-        ]
-        assert eg.clustering(G, weight="weight") == {
-            0: 0,
-            1: 0,
-            2: 0,
-            3: 0,
-            4: 0,
-            5: 0,
-            6: 0,
-            7: 0,
-            8: 0,
-            9: 0,
-        }
-
-    def test_k5(self):
-        G = eg.complete_graph(5, create_using=eg.DiGraph())
-        assert list(eg.clustering(G, weight="weight").values()) == [1, 1, 1, 1, 1]
-        assert eg.average_clustering(G, weight="weight") == 1
-        G.remove_edge(1, 2)
-        assert list(eg.clustering(G, weight="weight").values()) == [
-            11 / 12,
-            1,
-            1,
-            11 / 12,
-            11 / 12,
-        ]
-        assert eg.clustering(G, [1, 4], weight="weight") == {1: 1, 4: 11 / 12}
-        G.remove_edge(2, 1)
-        assert list(eg.clustering(G, weight="weight").values()) == [
-            5 / 6,
-            1,
-            1,
-            5 / 6,
-            5 / 6,
-        ]
-        assert eg.clustering(G, [1, 4], weight="weight") == {
-            1: 1,
-            4: 0.83333333333333337,
-        }
-
-    def test_triangle_and_edge(self):
-        G = eg.empty_graph(range(3), create_using=eg.DiGraph())
-        G.add_edges_from(eg.pairwise(range(3), cyclic=True))
-        G.add_edge(0, 4, weight=2)
-        assert eg.clustering(G)[0] == 1 / 6
-        # Relaxed comparisons to allow graphblas-algorithms to pass tests
-        np.testing.assert_allclose(eg.clustering(G, weight="weight")[0], 1 / 12)
-        np.testing.assert_allclose(eg.clustering(G, 0, weight="weight"), 1 / 12)
-
-
-class TestWeightedClustering:
-    @classmethod
-    def setup_class(cls):
-        global np
-        np = pytest.importorskip("numpy")
-
-    def test_clustering(self):
-        G = eg.Graph()
-        assert list(eg.clustering(G, weight="weight").values()) == []
-        assert eg.clustering(G) == {}
-
-    def test_path(self):
-        G = eg.path_graph(10)
-        assert list(eg.clustering(G, weight="weight").values()) == [
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-        ]
-        assert eg.clustering(G, weight="weight") == {
-            0: 0,
-            1: 0,
-            2: 0,
-            3: 0,
-            4: 0,
-            5: 0,
-            6: 0,
-            7: 0,
-            8: 0,
-            9: 0,
-        }
-
-    def test_cubical(self):
-        G = eg.from_dict_of_lists(
-            {
-                0: [1, 3, 4],
-                1: [0, 2, 7],
-                2: [1, 3, 6],
-                3: [0, 2, 5],
-                4: [0, 5, 7],
-                5: [3, 4, 6],
-                6: [2, 5, 7],
-                7: [1, 4, 6],
-            },
-            create_using=None,
-        )
-        assert list(eg.clustering(G, weight="weight").values()) == [
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-        ]
-        assert eg.clustering(G, 1) == 0
-        assert list(eg.clustering(G, [1, 2], weight="weight").values()) == [0, 0]
-        assert eg.clustering(G, 1, weight="weight") == 0
-        assert eg.clustering(G, [1, 2], weight="weight") == {1: 0, 2: 0}
-
-    def test_k5(self):
-        G = eg.complete_graph(5)
-        assert list(eg.clustering(G, weight="weight").values()) == [1, 1, 1, 1, 1]
-        assert eg.average_clustering(G, weight="weight") == 1
-        G.remove_edge(1, 2)
-        assert list(eg.clustering(G, weight="weight").values()) == [
-            5 / 6,
-            1,
-            1,
-            5 / 6,
-            5 / 6,
-        ]
-        assert eg.clustering(G, [1, 4], weight="weight") == {
-            1: 1,
-            4: 0.83333333333333337,
-        }
-
-    def test_triangle_and_edge(self):
-        G = eg.empty_graph(range(3), None)
-        G.add_edges_from(eg.pairwise(range(3), cyclic=True))
-        G.add_edge(0, 4, weight=2)
-        assert eg.clustering(G)[0] == 1 / 3
-        np.testing.assert_allclose(eg.clustering(G, weight="weight")[0], 1 / 6)
-        np.testing.assert_allclose(eg.clustering(G, 0, weight="weight"), 1 / 6)
-
-    def test_triangle_and_signed_edge(self):
-        G = eg.empty_graph(range(3), None)
-        G.add_edges_from(eg.pairwise(range(3), cyclic=True))
-        G.add_edge(0, 1, weight=-1)
-        G.add_edge(3, 0, weight=0)
-        assert eg.clustering(G)[0] == 1 / 3
-        assert eg.clustering(G, weight="weight")[0] == -1 / 3
+import easygraph as eg
+import pytest
+
+
+class TestClustering:
+    @classmethod
+    def setup_class(cls):
+        pytest.importorskip("numpy")
+
+    def test_clustering(self):
+        G = eg.DiGraph()
+        G.add_edge("1", "2", weight=16)
+        G.add_edge("2", "3", weight=16)
+        G.add_edge("4", "3", weight=16)
+        G.add_edge("3", "4", weight=23)
+        G.add_edge("3", "5", weight=16)
+        G.add_edge("4", "2", weight=20)
+        print("clustering" in dir(eg))
+        assert eg.clustering(G) == {
+            "1": 0,
+            "2": 0.3333333333333333,
+            "3": 0.2,
+            "4": 0.5,
+            "5": 0,
+        }
+
+    def test_path(self):
+        G = eg.path_graph(10)
+        assert list(eg.clustering(G).values()) == [
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+        ]
+        assert eg.clustering(G) == {
+            0: 0,
+            1: 0,
+            2: 0,
+            3: 0,
+            4: 0,
+            5: 0,
+            6: 0,
+            7: 0,
+            8: 0,
+            9: 0,
+        }
+
+    def test_k5(self):
+        G = eg.complete_graph(5)
+        assert list(eg.clustering(G).values()) == [1, 1, 1, 1, 1]
+        assert eg.average_clustering(G) == 1
+        G.remove_edge(1, 2)
+        assert list(eg.clustering(G).values()) == [
+            5 / 6,
+            1,
+            1,
+            5 / 6,
+            5 / 6,
+        ]
+        assert eg.clustering(G, [1, 4]) == {1: 1, 4: 0.83333333333333337}
+
+    def test_k5_signed(self):
+        G = eg.complete_graph(5)
+        assert list(eg.clustering(G).values()) == [1, 1, 1, 1, 1]
+        assert eg.average_clustering(G) == 1
+        G.remove_edge(1, 2)
+        G.add_edge(0, 1, weight=-1)
+        assert list(eg.clustering(G, weight="weight").values()) == [
+            1 / 6,
+            -1 / 3,
+            1,
+            3 / 6,
+            3 / 6,
+        ]
+
+
+class TestDirectedClustering:
+    def test_clustering(self):
+        G = eg.DiGraph()
+        assert list(eg.clustering(G).values()) == []
+        assert eg.clustering(G) == {}
+
+    def test_path(self):
+        G = eg.path_graph(10, create_using=eg.DiGraph())
+        assert list(eg.clustering(G).values()) == [
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+        ]
+        assert eg.clustering(G) == {
+            0: 0,
+            1: 0,
+            2: 0,
+            3: 0,
+            4: 0,
+            5: 0,
+            6: 0,
+            7: 0,
+            8: 0,
+            9: 0,
+        }
+        assert eg.clustering(G, 0) == 0
+
+    def test_k5(self):
+        G = eg.complete_graph(5, create_using=eg.DiGraph())
+        assert list(eg.clustering(G).values()) == [1, 1, 1, 1, 1]
+        assert eg.average_clustering(G) == 1
+        G.remove_edge(1, 2)
+        assert list(eg.clustering(G).values()) == [
+            11 / 12,
+            1,
+            1,
+            11 / 12,
+            11 / 12,
+        ]
+        assert eg.clustering(G, [1, 4]) == {1: 1, 4: 11 / 12}
+        G.remove_edge(2, 1)
+        assert list(eg.clustering(G).values()) == [
+            5 / 6,
+            1,
+            1,
+            5 / 6,
+            5 / 6,
+        ]
+        assert eg.clustering(G, [1, 4]) == {1: 1, 4: 0.83333333333333337}
+        assert eg.clustering(G, 4) == 5 / 6
+
+    def test_triangle_and_edge(self):
+        G = eg.empty_graph(range(3), eg.DiGraph())
+        G.add_edges_from(eg.pairwise(range(3), cyclic=True))
+        G.add_edge(0, 4)
+        assert eg.clustering(G)[0] == 1 / 6
+
+
+class TestDirectedAverageClustering:
+    @classmethod
+    def setup_class(cls):
+        pytest.importorskip("numpy")
+
+    def test_empty(self):
+        G = eg.DiGraph()
+        with pytest.raises(ZeroDivisionError):
+            eg.average_clustering(G)
+
+    def test_average_clustering(self):
+        G = eg.empty_graph(range(3), eg.DiGraph())
+        G.add_edges_from(eg.pairwise(range(3), cyclic=True))
+        G.add_edge(2, 3)
+        assert eg.average_clustering(G) == (1 + 1 + 1 / 3) / 8
+        assert eg.average_clustering(G, count_zeros=True) == (1 + 1 + 1 / 3) / 8
+        assert eg.average_clustering(G, count_zeros=False) == (1 + 1 + 1 / 3) / 6
+        assert eg.average_clustering(G, [1, 2, 3]) == (1 + 1 / 3) / 6
+        assert eg.average_clustering(G, [1, 2, 3], count_zeros=True) == (1 + 1 / 3) / 6
+        assert eg.average_clustering(G, [1, 2, 3], count_zeros=False) == (1 + 1 / 3) / 4
+
+
+class TestAverageClustering:
+    @classmethod
+    def setup_class(cls):
+        pytest.importorskip("numpy")
+
+    def test_empty(self):
+        G = eg.Graph()
+        with pytest.raises(ZeroDivisionError):
+            eg.average_clustering(G)
+
+    def test_average_clustering(self):
+        G = eg.complete_graph(3)
+        G.add_edge(2, 3)
+
+        assert eg.average_clustering(G) == (1 + 1 + 1 / 3) / 4
+        assert eg.average_clustering(G, count_zeros=True) == (1 + 1 + 1 / 3) / 4
+        assert eg.average_clustering(G, count_zeros=False) == (1 + 1 + 1 / 3) / 3
+        assert eg.average_clustering(G, [1, 2, 3]) == (1 + 1 / 3) / 3
+        assert eg.average_clustering(G, [1, 2, 3], count_zeros=True) == (1 + 1 / 3) / 3
+        assert eg.average_clustering(G, [1, 2, 3], count_zeros=False) == (1 + 1 / 3) / 2
+
+    def test_average_clustering_signed(self):
+        G = eg.complete_graph(3)
+        G.add_edge(2, 3)
+        G.add_edge(0, 1, weight=-1)
+        assert eg.average_clustering(G, weight="weight") == (-1 - 1 - 1 / 3) / 4
+        assert (
+            eg.average_clustering(G, weight="weight", count_zeros=True)
+            == (-1 - 1 - 1 / 3) / 4
+        )
+        assert (
+            eg.average_clustering(G, weight="weight", count_zeros=False)
+            == (-1 - 1 - 1 / 3) / 3
+        )
+
+
+class TestDirectedWeightedClustering:
+    @classmethod
+    def setup_class(cls):
+        global np
+        np = pytest.importorskip("numpy")
+
+    def test_clustering(self):
+        G = eg.DiGraph()
+        assert list(eg.clustering(G, weight="weight").values()) == []
+        assert eg.clustering(G) == {}
+
+    def test_path(self):
+        G = eg.path_graph(10, create_using=eg.DiGraph())
+        print("type:", eg.clustering(G, weight="weight"))
+        assert list(eg.clustering(G, weight="weight").values()) == [
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+        ]
+        assert eg.clustering(G, weight="weight") == {
+            0: 0,
+            1: 0,
+            2: 0,
+            3: 0,
+            4: 0,
+            5: 0,
+            6: 0,
+            7: 0,
+            8: 0,
+            9: 0,
+        }
+
+    def test_k5(self):
+        G = eg.complete_graph(5, create_using=eg.DiGraph())
+        assert list(eg.clustering(G, weight="weight").values()) == [1, 1, 1, 1, 1]
+        assert eg.average_clustering(G, weight="weight") == 1
+        G.remove_edge(1, 2)
+        assert list(eg.clustering(G, weight="weight").values()) == [
+            11 / 12,
+            1,
+            1,
+            11 / 12,
+            11 / 12,
+        ]
+        assert eg.clustering(G, [1, 4], weight="weight") == {1: 1, 4: 11 / 12}
+        G.remove_edge(2, 1)
+        assert list(eg.clustering(G, weight="weight").values()) == [
+            5 / 6,
+            1,
+            1,
+            5 / 6,
+            5 / 6,
+        ]
+        assert eg.clustering(G, [1, 4], weight="weight") == {
+            1: 1,
+            4: 0.83333333333333337,
+        }
+
+    def test_triangle_and_edge(self):
+        G = eg.empty_graph(range(3), create_using=eg.DiGraph())
+        G.add_edges_from(eg.pairwise(range(3), cyclic=True))
+        G.add_edge(0, 4, weight=2)
+        assert eg.clustering(G)[0] == 1 / 6
+        # Relaxed comparisons to allow graphblas-algorithms to pass tests
+        np.testing.assert_allclose(eg.clustering(G, weight="weight")[0], 1 / 12)
+        np.testing.assert_allclose(eg.clustering(G, 0, weight="weight"), 1 / 12)
+
+
+class TestWeightedClustering:
+    @classmethod
+    def setup_class(cls):
+        global np
+        np = pytest.importorskip("numpy")
+
+    def test_clustering(self):
+        G = eg.Graph()
+        assert list(eg.clustering(G, weight="weight").values()) == []
+        assert eg.clustering(G) == {}
+
+    def test_path(self):
+        G = eg.path_graph(10)
+        assert list(eg.clustering(G, weight="weight").values()) == [
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+        ]
+        assert eg.clustering(G, weight="weight") == {
+            0: 0,
+            1: 0,
+            2: 0,
+            3: 0,
+            4: 0,
+            5: 0,
+            6: 0,
+            7: 0,
+            8: 0,
+            9: 0,
+        }
+
+    def test_cubical(self):
+        G = eg.from_dict_of_lists(
+            {
+                0: [1, 3, 4],
+                1: [0, 2, 7],
+                2: [1, 3, 6],
+                3: [0, 2, 5],
+                4: [0, 5, 7],
+                5: [3, 4, 6],
+                6: [2, 5, 7],
+                7: [1, 4, 6],
+            },
+            create_using=None,
+        )
+        assert list(eg.clustering(G, weight="weight").values()) == [
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+            0,
+        ]
+        assert eg.clustering(G, 1) == 0
+        assert list(eg.clustering(G, [1, 2], weight="weight").values()) == [0, 0]
+        assert eg.clustering(G, 1, weight="weight") == 0
+        assert eg.clustering(G, [1, 2], weight="weight") == {1: 0, 2: 0}
+
+    def test_k5(self):
+        G = eg.complete_graph(5)
+        assert list(eg.clustering(G, weight="weight").values()) == [1, 1, 1, 1, 1]
+        assert eg.average_clustering(G, weight="weight") == 1
+        G.remove_edge(1, 2)
+        assert list(eg.clustering(G, weight="weight").values()) == [
+            5 / 6,
+            1,
+            1,
+            5 / 6,
+            5 / 6,
+        ]
+        assert eg.clustering(G, [1, 4], weight="weight") == {
+            1: 1,
+            4: 0.83333333333333337,
+        }
+
+    def test_triangle_and_edge(self):
+        G = eg.empty_graph(range(3), None)
+        G.add_edges_from(eg.pairwise(range(3), cyclic=True))
+        G.add_edge(0, 4, weight=2)
+        assert eg.clustering(G)[0] == 1 / 3
+        np.testing.assert_allclose(eg.clustering(G, weight="weight")[0], 1 / 6)
+        np.testing.assert_allclose(eg.clustering(G, 0, weight="weight"), 1 / 6)
+
+    def test_triangle_and_signed_edge(self):
+        G = eg.empty_graph(range(3), None)
+        G.add_edges_from(eg.pairwise(range(3), cyclic=True))
+        G.add_edge(0, 1, weight=-1)
+        G.add_edge(3, 0, weight=0)
+        assert eg.clustering(G)[0] == 1 / 3
+        assert eg.clustering(G, weight="weight")[0] == -1 / 3
```

## easygraph/functions/basic/tests/test_localassort.py

 * *Ordering differences only*

```diff
@@ -1,36 +1,36 @@
-import random
-import sys
-
-import easygraph as eg
-import numpy as np
-import pytest
-
-
-class TestLocalAssort:
-    @classmethod
-    def setup_class(self):
-        self.G = eg.get_graph_karateclub()
-        random_value = [0, 1, 2, 3, 4, 5]
-        edgelist = []
-        valuelist = []
-        node_num = len(self.G.nodes)
-        for e in self.G.edges:
-            edgelist.append([e[0] - 1, e[1] - 1])
-        for i in range(0, node_num):
-            valuelist.append(random.choice(random_value))
-        self.edgelist = np.int32(edgelist)
-        valuelist = np.int32(valuelist)
-        self.valuelist = valuelist
-
-    @pytest.mark.skipif(
-        sys.version_info.major <= 3 and sys.version_info.minor <= 7,
-        reason="python version should higher than 3.7",
-    )
-    def test_karateclub(self):
-        assortM, assortT, Z = eg.localAssort(
-            self.edgelist, self.valuelist, pr=np.arange(0, 1, 0.1)
-        )
-
-        _, assortT, Z = eg.functions.basic.localassort.localAssort(
-            self.edgelist, self.valuelist, pr=np.array([0.9])
-        )
+import random
+import sys
+
+import easygraph as eg
+import numpy as np
+import pytest
+
+
+class TestLocalAssort:
+    @classmethod
+    def setup_class(self):
+        self.G = eg.get_graph_karateclub()
+        random_value = [0, 1, 2, 3, 4, 5]
+        edgelist = []
+        valuelist = []
+        node_num = len(self.G.nodes)
+        for e in self.G.edges:
+            edgelist.append([e[0] - 1, e[1] - 1])
+        for i in range(0, node_num):
+            valuelist.append(random.choice(random_value))
+        self.edgelist = np.int32(edgelist)
+        valuelist = np.int32(valuelist)
+        self.valuelist = valuelist
+
+    @pytest.mark.skipif(
+        sys.version_info.major <= 3 and sys.version_info.minor <= 7,
+        reason="python version should higher than 3.7",
+    )
+    def test_karateclub(self):
+        assortM, assortT, Z = eg.localAssort(
+            self.edgelist, self.valuelist, pr=np.arange(0, 1, 0.1)
+        )
+
+        _, assortT, Z = eg.functions.basic.localassort.localAssort(
+            self.edgelist, self.valuelist, pr=np.array([0.9])
+        )
```

## easygraph/functions/graph_embedding/line.py

 * *Ordering differences only*

```diff
@@ -1,303 +1,303 @@
-import time
-import warnings
-
-import easygraph as eg
-import numpy as np
-import torch
-import torch.nn as nn
-
-from easygraph.utils import alias_draw
-from easygraph.utils import alias_setup
-from sklearn import preprocessing
-
-# from easygraph.functions.graph_embedding import *
-from tqdm import tqdm
-
-
-warnings.filterwarnings("ignore")
-
-
-class LINE(nn.Module):
-    """Graph embedding via LINE.
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-    dimension: int
-    walk_length: int
-
-    walk_num: int
-
-    negative: int
-    batch_size: int
-
-    init_alpha: float
-    order: int
-    Returns
-    -------
-    embedding_vector : dict
-        The embedding vector of each node
-    Examples
-    --------
-    >>> model = LINE(
-    ...          dimension=128,
-    ...          walk_length=80,
-    ...          walk_num=20,
-    ...          negative=5,
-    ...          batch_size=128,
-    ...          init_alpha=0.025,
-    ...          order=3  )
-    >>> model.train()
-    >>> emb = model(g, return_dict=True) # g: easygraph.Graph or easygraph.DiGraph
-
-    References
-    ----------
-
-    .. [1] Tang, J., Qu, M., Wang, M., Zhang, M., Yan, J., & Mei, Q. (2015, May). Line: Large-scale information network embedding. In Proceedings of the 24th international conference on world wide web (pp. 1067-1077).
-
-    https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/frp0228-Tang.pdf
-
-    """
-
-    @staticmethod
-    def add_args(parser):
-        """Add model-specific arguments to the parser."""
-        parser.add_argument(
-            "--walk-length",
-            type=int,
-            default=80,
-            help="Length of walk per source. Default is 80.",
-        )
-        parser.add_argument(
-            "--walk-num",
-            type=int,
-            default=20,
-            help="Number of walks per source. Default is 20.",
-        )
-        parser.add_argument(
-            "--negative",
-            type=int,
-            default=5,
-            help="Number of negative node in sampling. Default is 5.",
-        )
-        parser.add_argument(
-            "--batch-size",
-            type=int,
-            default=1000,
-            help="Batch size in SGD training process. Default is 1000.",
-        )
-        parser.add_argument(
-            "--alpha",
-            type=float,
-            default=0.025,
-            help="Initial learning rate of SGD. Default is 0.025.",
-        )
-        parser.add_argument(
-            "--order",
-            type=int,
-            default=3,
-            help="Order of proximity in LINE. Default is 3 for 1+2.",
-        )
-        parser.add_argument("--hidden-size", type=int, default=128)
-
-    @classmethod
-    def build_model_from_args(cls, args):
-        return cls(
-            args.hidden_size,
-            args.walk_length,
-            args.walk_num,
-            args.negative,
-            args.batch_size,
-            args.alpha,
-            args.order,
-        )
-
-    def __init__(
-        self,
-        dimension=128,
-        walk_length=80,
-        walk_num=20,
-        negative=5,
-        batch_size=128,
-        init_alpha=0.025,
-        order=3,
-    ):
-        super(LINE, self).__init__()
-        self.dimension = dimension
-        self.walk_length = walk_length
-        self.walk_num = walk_num
-        self.negative = negative
-        self.batch_size = batch_size
-        self.init_alpha = init_alpha
-        self.order = order
-
-    def forward(self, g, return_dict=True):
-        # run LINE algorithm, 1-order, 2-order or 3(1-order + 2-order)
-
-        self.G = g
-        self.is_directed = g.is_directed()
-        self.num_node = g.size()
-        self.num_edge = g.number_of_edges()
-        self.num_sampling_edge = self.walk_length * self.walk_num * self.num_node
-
-        node2id = dict([(node, vid) for vid, node in enumerate(g.nodes)])
-        self.edges = [[node2id[e[0]], node2id[e[1]]] for e in self.G.edges]
-
-        self.edges_prob = np.asarray([1.0 for e in g.edges])
-        self.edges_prob /= np.sum(self.edges_prob)
-        self.edges_table, self.edges_prob = alias_setup(self.edges_prob)
-
-        degree_weight = np.asarray([0] * self.num_node)
-        degree_weight = np.array(list(g.degree(node2id[u] for u in g.nodes).values()))
-        # for u,v in g.edges:
-
-        #     degree_weight[node2id[u]] += 1.0
-        #     if not self.is_directed:
-        #         degree_weight[node2id[v]] += 1.0
-        self.node_prob = np.power(degree_weight, 0.75)
-        self.node_prob /= np.sum(self.node_prob)
-        self.node_table, self.node_prob = alias_setup(self.node_prob)
-
-        if self.order == 3:
-            self.dimension = int(self.dimension / 2)
-        if self.order == 1 or self.order == 3:
-            print("train line with 1-order")
-            print(type(self.dimension))
-            self.emb_vertex = (
-                np.random.random((self.num_node, self.dimension)) - 0.5
-            ) / self.dimension
-            self._train_line(order=1)
-            embedding1 = preprocessing.normalize(self.emb_vertex, "l2")
-
-        if self.order == 2 or self.order == 3:
-            print("train line with 2-order")
-            self.emb_vertex = (
-                np.random.random((self.num_node, self.dimension)) - 0.5
-            ) / self.dimension
-            self.emb_context = self.emb_vertex
-            self._train_line(order=2)
-            embedding2 = preprocessing.normalize(self.emb_vertex, "l2")
-
-        if self.order == 1:
-            embeddings = embedding1
-        elif self.order == 2:
-            embeddings = embedding2
-        else:
-            print("concatenate two embedding...")
-            embeddings = np.hstack((embedding1, embedding2))
-
-        if return_dict:
-            features_matrix = dict()
-            for vid, node in enumerate(g.nodes):
-                features_matrix[node] = embeddings[vid]
-        else:
-            features_matrix = np.zeros((g.num_nodes, embeddings.shape[1]))
-            nx_nodes = g.nodes()
-            features_matrix[nx_nodes] = embeddings[np.arange(g.num_nodes)]
-        return features_matrix
-
-    def _update(self, vec_u, vec_v, vec_error, label):
-        # update vetex embedding and vec_error
-        f = 1 / (1 + np.exp(-np.sum(vec_u * vec_v, axis=1)))
-        g = (self.alpha * (label - f)).reshape((len(label), 1))
-        vec_error += g * vec_v
-        vec_v += g * vec_u
-
-    def _train_line(self, order):
-        # train Line model with order
-        self.alpha = self.init_alpha
-        batch_size = self.batch_size
-        t0 = time.time()
-        num_batch = int(self.num_sampling_edge / batch_size)
-        epoch_iter = tqdm(range(num_batch))
-        for b in epoch_iter:
-            if b % 100 == 0:
-                epoch_iter.set_description(
-                    #    f"Progress: {b * 1.0 / num_batch * 100:.4f}, alpha: {self.alpha:.6f}, time: {time.time() - t0:.4f}"
-                )
-                self.alpha = self.init_alpha * max((1 - b * 1.0 / num_batch), 0.0001)
-            u, v = [0] * batch_size, [0] * batch_size
-            for i in range(batch_size):
-                edge_id = alias_draw(self.edges_table, self.edges_prob)
-                u[i], v[i] = self.edges[edge_id]
-                if not self.is_directed and np.random.rand() > 0.5:
-                    v[i], u[i] = self.edges[edge_id]
-
-            vec_error = np.zeros((batch_size, self.dimension))
-            label, target = np.asarray([1 for i in range(batch_size)]), np.asarray(v)
-            for j in range(1 + self.negative):
-                if j != 0:
-                    label = np.asarray([0 for i in range(batch_size)])
-                    for i in range(batch_size):
-                        target[i] = alias_draw(self.node_table, self.node_prob)
-                if order == 1:
-                    self._update(
-                        self.emb_vertex[u], self.emb_vertex[target], vec_error, label
-                    )
-                else:
-                    self._update(
-                        self.emb_vertex[u], self.emb_context[target], vec_error, label
-                    )
-            self.emb_vertex[u] += vec_error
-
-
-if __name__ == "__main__":
-    dataset = eg.CiteseerGraphDataset(
-        force_reload=True
-    )  # Download CiteseerGraphDataset contained in EasyGraph
-    num_classes = dataset.num_classes
-    g = dataset[0]
-    labels = g.ndata["label"]
-    edge_list = []
-    for i in g.edges:
-        edge_list.append((i[0], i[1]))
-    g1 = eg.Graph()
-    g1.add_edges_from(edge_list)
-    # print(g.edges)
-    # print(g.__dir__())
-
-    model = LINE(
-        dimension=128,
-        walk_length=80,
-        walk_num=20,
-        negative=5,
-        batch_size=128,
-        init_alpha=0.025,
-        order=3,
-    )
-    print(model)
-
-    model.train()
-    out = model(g1, return_dict=True)
-
-    keylist = sorted(out)
-    tmp = torch.cat(
-        (
-            torch.unsqueeze(torch.tensor(out[keylist[0]]), -2),
-            torch.unsqueeze(torch.tensor(out[keylist[1]]), -2),
-        ),
-        0,
-    )
-
-    for i in range(2, len(keylist)):
-        tmp = torch.cat((tmp, torch.unsqueeze(torch.tensor(out[keylist[i]]), -2)), 0)
-    torch.save(tmp, "line.emb")
-    print(tmp, tmp.shape)
-
-    line_emb = []
-    for i in range(0, len(tmp)):
-        line_emb.append(list(tmp[i]))
-    line_emb = np.array(line_emb)
-
-# tsne = TSNE(n_components=2)
-# z = tsne.fit_transform(line_emb)
-# z_data = np.vstack((z.T, labels)).T
-# df_tsne = pd.DataFrame(z_data, columns=['Dim1', 'Dim2', 'class'])
-# df_tsne['class'] = df_tsne['class'].astype(int)
-# df_tsne.head()
-#
-# plt.figure(figsize=(8, 8))
-# sns.scatterplot(data=df_tsne, hue='class', x='Dim1', y='Dim2', palette=['green','orange','brown','red', 'blue','black'])
-# plt.savefig('torch_line_citeseer.pdf', bbox_inches='tight')
-# plt.show()
-#
-#
+import time
+import warnings
+
+import easygraph as eg
+import numpy as np
+import torch
+import torch.nn as nn
+
+from easygraph.utils import alias_draw
+from easygraph.utils import alias_setup
+from sklearn import preprocessing
+
+# from easygraph.functions.graph_embedding import *
+from tqdm import tqdm
+
+
+warnings.filterwarnings("ignore")
+
+
+class LINE(nn.Module):
+    """Graph embedding via LINE.
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+    dimension: int
+    walk_length: int
+
+    walk_num: int
+
+    negative: int
+    batch_size: int
+
+    init_alpha: float
+    order: int
+    Returns
+    -------
+    embedding_vector : dict
+        The embedding vector of each node
+    Examples
+    --------
+    >>> model = LINE(
+    ...          dimension=128,
+    ...          walk_length=80,
+    ...          walk_num=20,
+    ...          negative=5,
+    ...          batch_size=128,
+    ...          init_alpha=0.025,
+    ...          order=3  )
+    >>> model.train()
+    >>> emb = model(g, return_dict=True) # g: easygraph.Graph or easygraph.DiGraph
+
+    References
+    ----------
+
+    .. [1] Tang, J., Qu, M., Wang, M., Zhang, M., Yan, J., & Mei, Q. (2015, May). Line: Large-scale information network embedding. In Proceedings of the 24th international conference on world wide web (pp. 1067-1077).
+
+    https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/frp0228-Tang.pdf
+
+    """
+
+    @staticmethod
+    def add_args(parser):
+        """Add model-specific arguments to the parser."""
+        parser.add_argument(
+            "--walk-length",
+            type=int,
+            default=80,
+            help="Length of walk per source. Default is 80.",
+        )
+        parser.add_argument(
+            "--walk-num",
+            type=int,
+            default=20,
+            help="Number of walks per source. Default is 20.",
+        )
+        parser.add_argument(
+            "--negative",
+            type=int,
+            default=5,
+            help="Number of negative node in sampling. Default is 5.",
+        )
+        parser.add_argument(
+            "--batch-size",
+            type=int,
+            default=1000,
+            help="Batch size in SGD training process. Default is 1000.",
+        )
+        parser.add_argument(
+            "--alpha",
+            type=float,
+            default=0.025,
+            help="Initial learning rate of SGD. Default is 0.025.",
+        )
+        parser.add_argument(
+            "--order",
+            type=int,
+            default=3,
+            help="Order of proximity in LINE. Default is 3 for 1+2.",
+        )
+        parser.add_argument("--hidden-size", type=int, default=128)
+
+    @classmethod
+    def build_model_from_args(cls, args):
+        return cls(
+            args.hidden_size,
+            args.walk_length,
+            args.walk_num,
+            args.negative,
+            args.batch_size,
+            args.alpha,
+            args.order,
+        )
+
+    def __init__(
+        self,
+        dimension=128,
+        walk_length=80,
+        walk_num=20,
+        negative=5,
+        batch_size=128,
+        init_alpha=0.025,
+        order=3,
+    ):
+        super(LINE, self).__init__()
+        self.dimension = dimension
+        self.walk_length = walk_length
+        self.walk_num = walk_num
+        self.negative = negative
+        self.batch_size = batch_size
+        self.init_alpha = init_alpha
+        self.order = order
+
+    def forward(self, g, return_dict=True):
+        # run LINE algorithm, 1-order, 2-order or 3(1-order + 2-order)
+
+        self.G = g
+        self.is_directed = g.is_directed()
+        self.num_node = g.size()
+        self.num_edge = g.number_of_edges()
+        self.num_sampling_edge = self.walk_length * self.walk_num * self.num_node
+
+        node2id = dict([(node, vid) for vid, node in enumerate(g.nodes)])
+        self.edges = [[node2id[e[0]], node2id[e[1]]] for e in self.G.edges]
+
+        self.edges_prob = np.asarray([1.0 for e in g.edges])
+        self.edges_prob /= np.sum(self.edges_prob)
+        self.edges_table, self.edges_prob = alias_setup(self.edges_prob)
+
+        degree_weight = np.asarray([0] * self.num_node)
+        degree_weight = np.array(list(g.degree(node2id[u] for u in g.nodes).values()))
+        # for u,v in g.edges:
+
+        #     degree_weight[node2id[u]] += 1.0
+        #     if not self.is_directed:
+        #         degree_weight[node2id[v]] += 1.0
+        self.node_prob = np.power(degree_weight, 0.75)
+        self.node_prob /= np.sum(self.node_prob)
+        self.node_table, self.node_prob = alias_setup(self.node_prob)
+
+        if self.order == 3:
+            self.dimension = int(self.dimension / 2)
+        if self.order == 1 or self.order == 3:
+            print("train line with 1-order")
+            print(type(self.dimension))
+            self.emb_vertex = (
+                np.random.random((self.num_node, self.dimension)) - 0.5
+            ) / self.dimension
+            self._train_line(order=1)
+            embedding1 = preprocessing.normalize(self.emb_vertex, "l2")
+
+        if self.order == 2 or self.order == 3:
+            print("train line with 2-order")
+            self.emb_vertex = (
+                np.random.random((self.num_node, self.dimension)) - 0.5
+            ) / self.dimension
+            self.emb_context = self.emb_vertex
+            self._train_line(order=2)
+            embedding2 = preprocessing.normalize(self.emb_vertex, "l2")
+
+        if self.order == 1:
+            embeddings = embedding1
+        elif self.order == 2:
+            embeddings = embedding2
+        else:
+            print("concatenate two embedding...")
+            embeddings = np.hstack((embedding1, embedding2))
+
+        if return_dict:
+            features_matrix = dict()
+            for vid, node in enumerate(g.nodes):
+                features_matrix[node] = embeddings[vid]
+        else:
+            features_matrix = np.zeros((g.num_nodes, embeddings.shape[1]))
+            nx_nodes = g.nodes()
+            features_matrix[nx_nodes] = embeddings[np.arange(g.num_nodes)]
+        return features_matrix
+
+    def _update(self, vec_u, vec_v, vec_error, label):
+        # update vetex embedding and vec_error
+        f = 1 / (1 + np.exp(-np.sum(vec_u * vec_v, axis=1)))
+        g = (self.alpha * (label - f)).reshape((len(label), 1))
+        vec_error += g * vec_v
+        vec_v += g * vec_u
+
+    def _train_line(self, order):
+        # train Line model with order
+        self.alpha = self.init_alpha
+        batch_size = self.batch_size
+        t0 = time.time()
+        num_batch = int(self.num_sampling_edge / batch_size)
+        epoch_iter = tqdm(range(num_batch))
+        for b in epoch_iter:
+            if b % 100 == 0:
+                epoch_iter.set_description(
+                    #    f"Progress: {b * 1.0 / num_batch * 100:.4f}, alpha: {self.alpha:.6f}, time: {time.time() - t0:.4f}"
+                )
+                self.alpha = self.init_alpha * max((1 - b * 1.0 / num_batch), 0.0001)
+            u, v = [0] * batch_size, [0] * batch_size
+            for i in range(batch_size):
+                edge_id = alias_draw(self.edges_table, self.edges_prob)
+                u[i], v[i] = self.edges[edge_id]
+                if not self.is_directed and np.random.rand() > 0.5:
+                    v[i], u[i] = self.edges[edge_id]
+
+            vec_error = np.zeros((batch_size, self.dimension))
+            label, target = np.asarray([1 for i in range(batch_size)]), np.asarray(v)
+            for j in range(1 + self.negative):
+                if j != 0:
+                    label = np.asarray([0 for i in range(batch_size)])
+                    for i in range(batch_size):
+                        target[i] = alias_draw(self.node_table, self.node_prob)
+                if order == 1:
+                    self._update(
+                        self.emb_vertex[u], self.emb_vertex[target], vec_error, label
+                    )
+                else:
+                    self._update(
+                        self.emb_vertex[u], self.emb_context[target], vec_error, label
+                    )
+            self.emb_vertex[u] += vec_error
+
+
+if __name__ == "__main__":
+    dataset = eg.CiteseerGraphDataset(
+        force_reload=True
+    )  # Download CiteseerGraphDataset contained in EasyGraph
+    num_classes = dataset.num_classes
+    g = dataset[0]
+    labels = g.ndata["label"]
+    edge_list = []
+    for i in g.edges:
+        edge_list.append((i[0], i[1]))
+    g1 = eg.Graph()
+    g1.add_edges_from(edge_list)
+    # print(g.edges)
+    # print(g.__dir__())
+
+    model = LINE(
+        dimension=128,
+        walk_length=80,
+        walk_num=20,
+        negative=5,
+        batch_size=128,
+        init_alpha=0.025,
+        order=3,
+    )
+    print(model)
+
+    model.train()
+    out = model(g1, return_dict=True)
+
+    keylist = sorted(out)
+    tmp = torch.cat(
+        (
+            torch.unsqueeze(torch.tensor(out[keylist[0]]), -2),
+            torch.unsqueeze(torch.tensor(out[keylist[1]]), -2),
+        ),
+        0,
+    )
+
+    for i in range(2, len(keylist)):
+        tmp = torch.cat((tmp, torch.unsqueeze(torch.tensor(out[keylist[i]]), -2)), 0)
+    torch.save(tmp, "line.emb")
+    print(tmp, tmp.shape)
+
+    line_emb = []
+    for i in range(0, len(tmp)):
+        line_emb.append(list(tmp[i]))
+    line_emb = np.array(line_emb)
+
+# tsne = TSNE(n_components=2)
+# z = tsne.fit_transform(line_emb)
+# z_data = np.vstack((z.T, labels)).T
+# df_tsne = pd.DataFrame(z_data, columns=['Dim1', 'Dim2', 'class'])
+# df_tsne['class'] = df_tsne['class'].astype(int)
+# df_tsne.head()
+#
+# plt.figure(figsize=(8, 8))
+# sns.scatterplot(data=df_tsne, hue='class', x='Dim1', y='Dim2', palette=['green','orange','brown','red', 'blue','black'])
+# plt.savefig('torch_line_citeseer.pdf', bbox_inches='tight')
+# plt.show()
+#
+#
```

## easygraph/functions/graph_embedding/sdne.py

 * *Ordering differences only*

```diff
@@ -1,281 +1,281 @@
-from argparse import ArgumentDefaultsHelpFormatter
-from argparse import ArgumentParser
-
-import easygraph as eg
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-
-from torch.utils import data
-from torch.utils.data.dataloader import DataLoader
-
-
-def parse_args():
-    parser = ArgumentParser(
-        formatter_class=ArgumentDefaultsHelpFormatter, conflict_handler="resolve"
-    )
-    parser.add_argument(
-        "--output", default="node.emb", help="Output representation file"
-    )
-    parser.add_argument(
-        "--workers", default=8, type=int, help="Number of parallel processes."
-    )
-    parser.add_argument(
-        "--weighted", action="store_true", default=False, help="Treat graph as weighted"
-    )
-    parser.add_argument(
-        "--epochs", default=400, type=int, help="The training epochs of SDNE"
-    )
-    parser.add_argument(
-        "--dropout",
-        default=0.05,
-        type=float,
-        help="Dropout rate (1 - keep probability)",
-    )
-    parser.add_argument(
-        "--weight-decay",
-        type=float,
-        default=5e-4,
-        help="Weight for L2 loss on embedding matrix",
-    )
-    parser.add_argument("--lr", default=0.006, type=float, help="learning rate")
-    parser.add_argument(
-        "--alpha", default=1e-2, type=float, help="alhpa is a hyperparameter in SDNE"
-    )
-    parser.add_argument(
-        "--beta", default=5.0, type=float, help="beta is a hyperparameter in SDNE"
-    )
-    parser.add_argument(
-        "--nu1", default=1e-5, type=float, help="nu1 is a hyperparameter in SDNE"
-    )
-    parser.add_argument(
-        "--nu2", default=1e-4, type=float, help="nu2 is a hyperparameter in SDNE"
-    )
-    parser.add_argument("--bs", default=100, type=int, help="batch size of SDNE")
-    parser.add_argument("--nhid0", default=1000, type=int, help="The first dim")
-    parser.add_argument("--nhid1", default=128, type=int, help="The second dim")
-    parser.add_argument(
-        "--step_size", default=10, type=int, help="The step size for lr"
-    )
-    parser.add_argument("--gamma", default=0.9, type=int, help="The gamma for lr")
-    args = parser.parse_args()
-
-    return args
-
-
-class Dataload(data.Dataset):
-    def __init__(self, Adj, Node):
-        self.Adj = Adj
-        self.Node = Node
-
-    def __getitem__(self, index):
-        return index
-        # adj_batch = self.Adj[index]
-        # adj_mat = adj_batch[index]
-        # b_mat = torch.ones_like(adj_batch)
-        # b_mat[adj_batch != 0] = self.Beta
-        # return adj_batch, adj_mat, b_mat
-
-    def __len__(self):
-        return self.Node
-
-
-def get_adj(g):
-    edges = list(g.edges)
-    edges = [(edges[i][0], edges[i][1]) for i in range(len(edges))]
-    # print(edges)
-    edges = np.array([np.array(i) for i in edges])
-    min_node, max_node = edges.min(), edges.max()
-    if min_node == 0:
-        Node = max_node + 1
-    else:
-        Node = max_node
-
-    Adj = np.zeros([Node, Node], dtype=int)
-    for i in range(edges.shape[0]):
-        g.add_edge(edges[i][0], edges[i][1])
-        if min_node == 0:
-            Adj[edges[i][0], edges[i][1]] = 1
-            Adj[edges[i][1], edges[i][0]] = 1
-
-        else:
-            Adj[edges[i][0] - 1, edges[i][1] - 1] = 1
-            Adj[edges[i][1] - 1, edges[i][0] - 1] = 1
-    Adj = torch.FloatTensor(Adj)
-    return Adj, Node
-
-
-class SDNE(nn.Module):
-    """
-    Graph embedding via SDNE.
-
-        Parameters
-        ----------
-        graph : easygraph.Graph or easygraph.DiGraph
-
-        node: Size of nodes
-
-        nhid0, nhid1: Two dimensions of two hiddenlayers, default: 128, 64
-
-        dropout: One parameter for regularization, default: 0.025
-
-        alpha, beta:  Twe parameters
-        graph=g: : easygraph.Graph or easygraph.DiGraph
-
-    Examples
-    --------
-    >>> import easygraph as eg
-    >>> model = eg.SDNE(graph=g, node_size= len(g.nodes), nhid0=128, nhid1=64, dropout=0.025, alpha=2e-2, beta=10)
-    >>> emb = model.train(model, epochs, lr, bs, step_size, gamma, nu1, nu2, device, output)
-
-
-    epochs,  "--epochs", default=400, type=int, help="The training epochs of SDNE"
-
-    alpha,   "--alpha", default=2e-2, type=float, help="alhpa is a hyperparameter in SDNE"
-
-    beta, "--beta", default=10.0, type=float, help="beta is a hyperparameter in SDNE"
-
-    lr, "--lr", default=0.006, type=float, help="learning rate"
-
-    bs, "--bs", default=100, type=int, help="batch size of SDNE"
-
-    step_size,  "--step_size", default=10, type=int, help="The step size for lr"
-
-    gamma, # "--gamma", default=0.9, type=int, help="The gamma for lr"
-
-    step_size, "--step_size", default=10, type=int, help="The step size for lr"
-
-    nu1, # "--nu1", default=1e-5, type=float, help="nu1 is a hyperparameter in SDNE"
-
-    nu2,  "--nu2", default=1e-4, type=float, help="nu2 is a hyperparameter in SDNE"
-
-    device, "-- device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") "
-
-    output  "--output", default="node.emb", help="Output representation file"
-
-
-    Reference
-        ----------
-        .. [1] Wang, D., Cui, P., & Zhu, W. (2016, August). Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1225-1234).
-
-        https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf
-    """
-
-    def __init__(
-        self, graph, node_size, nhid0, nhid1, dropout=0.06, alpha=2e-2, beta=10.0
-    ):
-        super(SDNE, self).__init__()
-        self.encode0 = nn.Linear(node_size, nhid0)
-        self.encode1 = nn.Linear(nhid0, nhid1)
-        self.decode0 = nn.Linear(nhid1, nhid0)
-        self.decode1 = nn.Linear(nhid0, node_size)
-        self.droput = dropout
-        self.alpha = alpha
-        self.beta = beta
-        self.graph = graph
-
-    def forward(self, adj_batch, adj_mat, b_mat):
-        t0 = F.leaky_relu(self.encode0(adj_batch))
-        t0 = F.leaky_relu(self.encode1(t0))
-        embedding = t0
-        t0 = F.leaky_relu(self.decode0(t0))
-        t0 = F.leaky_relu(self.decode1(t0))
-        embedding_norm = torch.sum(embedding * embedding, dim=1, keepdim=True)
-        L_1st = torch.sum(
-            adj_mat
-            * (
-                embedding_norm
-                - 2 * torch.mm(embedding, torch.transpose(embedding, dim0=0, dim1=1))
-                + torch.transpose(embedding_norm, dim0=0, dim1=1)
-            )
-        )
-        L_2nd = torch.sum(((adj_batch - t0) * b_mat) * ((adj_batch - t0) * b_mat))
-        return L_1st, self.alpha * L_2nd, L_1st + self.alpha * L_2nd
-
-    def train(
-        self,
-        model,
-        epochs=100,
-        lr=0.006,
-        bs=100,
-        step_size=10,
-        gamma=0.9,
-        nu1=1e-5,
-        nu2=1e-4,
-        device="cpu",
-        output="out.emb",
-    ):
-        Adj, Node = get_adj(self.graph)
-        model = model.to(device)
-
-        opt = optim.Adam(model.parameters(), lr=lr)
-        scheduler = torch.optim.lr_scheduler.StepLR(
-            opt, step_size=step_size, gamma=gamma
-        )
-        Data = Dataload(Adj, Node)
-        Data = DataLoader(
-            Data,
-            batch_size=bs,
-            shuffle=True,
-        )
-
-        for epoch in range(1, epochs + 1):
-            loss_sum, loss_L1, loss_L2, loss_reg = 0, 0, 0, 0
-            for index in Data:
-                adj_batch = Adj[index]
-                adj_mat = adj_batch[:, index]
-                b_mat = torch.ones_like(adj_batch)
-                b_mat[adj_batch != 0] = self.beta
-
-                opt.zero_grad()
-                L_1st, L_2nd, L_all = model(adj_batch, adj_mat, b_mat)
-                L_reg = 0
-                for param in model.parameters():
-                    L_reg += nu1 * torch.sum(torch.abs(param)) + nu2 * torch.sum(
-                        param * param
-                    )
-                Loss = L_all + L_reg
-                Loss.backward()
-                opt.step()
-                loss_sum += Loss
-                loss_L1 += L_1st
-                loss_L2 += L_2nd
-                loss_reg += L_reg
-            scheduler.step(epoch)
-            # print("The lr for epoch %d is %f" %(epoch, scheduler.get_lr()[0]))
-            print("loss for epoch %d is:" % epoch)
-            print("loss_sum is %f" % loss_sum)
-            print("loss_L1 is %f" % loss_L1)
-            print("loss_L2 is %f" % loss_L2)
-            print("loss_reg is %f" % loss_reg)
-
-        # model.eval()
-        embedding = model.savector(Adj)
-        outVec = embedding.detach().numpy()
-        np.savetxt(output, outVec)
-
-        return outVec
-
-    def savector(self, adj):
-        t0 = self.encode0(adj)
-        t0 = self.encode1(t0)
-        return t0
-
-
-# if __name__ == '__main__':
-#     args = parse_args()
-#     print(args)
-#     dataset = eg.CiteseerGraphDataset(force_reload=True) # Download CiteseerGraphDataset contained in EasyGraph
-#     num_classes = dataset.num_classes
-#     g = dataset[0]
-#     print(g)
-#     device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
-#     adj, node = get_adj(g)
-#     # labels = g.ndata['label']
-#     nhid0, nhid1, dropout, alpha = args.nhid0, args.nhid1, args.dropout, args.alpha
-#     model = SDNE(node, nhid0, nhid1, dropout, alpha, graph=g)
-#     print(model)
-#
-#     emb = model.train(args, device)
+from argparse import ArgumentDefaultsHelpFormatter
+from argparse import ArgumentParser
+
+import easygraph as eg
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+
+from torch.utils import data
+from torch.utils.data.dataloader import DataLoader
+
+
+def parse_args():
+    parser = ArgumentParser(
+        formatter_class=ArgumentDefaultsHelpFormatter, conflict_handler="resolve"
+    )
+    parser.add_argument(
+        "--output", default="node.emb", help="Output representation file"
+    )
+    parser.add_argument(
+        "--workers", default=8, type=int, help="Number of parallel processes."
+    )
+    parser.add_argument(
+        "--weighted", action="store_true", default=False, help="Treat graph as weighted"
+    )
+    parser.add_argument(
+        "--epochs", default=400, type=int, help="The training epochs of SDNE"
+    )
+    parser.add_argument(
+        "--dropout",
+        default=0.05,
+        type=float,
+        help="Dropout rate (1 - keep probability)",
+    )
+    parser.add_argument(
+        "--weight-decay",
+        type=float,
+        default=5e-4,
+        help="Weight for L2 loss on embedding matrix",
+    )
+    parser.add_argument("--lr", default=0.006, type=float, help="learning rate")
+    parser.add_argument(
+        "--alpha", default=1e-2, type=float, help="alhpa is a hyperparameter in SDNE"
+    )
+    parser.add_argument(
+        "--beta", default=5.0, type=float, help="beta is a hyperparameter in SDNE"
+    )
+    parser.add_argument(
+        "--nu1", default=1e-5, type=float, help="nu1 is a hyperparameter in SDNE"
+    )
+    parser.add_argument(
+        "--nu2", default=1e-4, type=float, help="nu2 is a hyperparameter in SDNE"
+    )
+    parser.add_argument("--bs", default=100, type=int, help="batch size of SDNE")
+    parser.add_argument("--nhid0", default=1000, type=int, help="The first dim")
+    parser.add_argument("--nhid1", default=128, type=int, help="The second dim")
+    parser.add_argument(
+        "--step_size", default=10, type=int, help="The step size for lr"
+    )
+    parser.add_argument("--gamma", default=0.9, type=int, help="The gamma for lr")
+    args = parser.parse_args()
+
+    return args
+
+
+class Dataload(data.Dataset):
+    def __init__(self, Adj, Node):
+        self.Adj = Adj
+        self.Node = Node
+
+    def __getitem__(self, index):
+        return index
+        # adj_batch = self.Adj[index]
+        # adj_mat = adj_batch[index]
+        # b_mat = torch.ones_like(adj_batch)
+        # b_mat[adj_batch != 0] = self.Beta
+        # return adj_batch, adj_mat, b_mat
+
+    def __len__(self):
+        return self.Node
+
+
+def get_adj(g):
+    edges = list(g.edges)
+    edges = [(edges[i][0], edges[i][1]) for i in range(len(edges))]
+    # print(edges)
+    edges = np.array([np.array(i) for i in edges])
+    min_node, max_node = edges.min(), edges.max()
+    if min_node == 0:
+        Node = max_node + 1
+    else:
+        Node = max_node
+
+    Adj = np.zeros([Node, Node], dtype=int)
+    for i in range(edges.shape[0]):
+        g.add_edge(edges[i][0], edges[i][1])
+        if min_node == 0:
+            Adj[edges[i][0], edges[i][1]] = 1
+            Adj[edges[i][1], edges[i][0]] = 1
+
+        else:
+            Adj[edges[i][0] - 1, edges[i][1] - 1] = 1
+            Adj[edges[i][1] - 1, edges[i][0] - 1] = 1
+    Adj = torch.FloatTensor(Adj)
+    return Adj, Node
+
+
+class SDNE(nn.Module):
+    """
+    Graph embedding via SDNE.
+
+        Parameters
+        ----------
+        graph : easygraph.Graph or easygraph.DiGraph
+
+        node: Size of nodes
+
+        nhid0, nhid1: Two dimensions of two hiddenlayers, default: 128, 64
+
+        dropout: One parameter for regularization, default: 0.025
+
+        alpha, beta:  Twe parameters
+        graph=g: : easygraph.Graph or easygraph.DiGraph
+
+    Examples
+    --------
+    >>> import easygraph as eg
+    >>> model = eg.SDNE(graph=g, node_size= len(g.nodes), nhid0=128, nhid1=64, dropout=0.025, alpha=2e-2, beta=10)
+    >>> emb = model.train(model, epochs, lr, bs, step_size, gamma, nu1, nu2, device, output)
+
+
+    epochs,  "--epochs", default=400, type=int, help="The training epochs of SDNE"
+
+    alpha,   "--alpha", default=2e-2, type=float, help="alhpa is a hyperparameter in SDNE"
+
+    beta, "--beta", default=10.0, type=float, help="beta is a hyperparameter in SDNE"
+
+    lr, "--lr", default=0.006, type=float, help="learning rate"
+
+    bs, "--bs", default=100, type=int, help="batch size of SDNE"
+
+    step_size,  "--step_size", default=10, type=int, help="The step size for lr"
+
+    gamma, # "--gamma", default=0.9, type=int, help="The gamma for lr"
+
+    step_size, "--step_size", default=10, type=int, help="The step size for lr"
+
+    nu1, # "--nu1", default=1e-5, type=float, help="nu1 is a hyperparameter in SDNE"
+
+    nu2,  "--nu2", default=1e-4, type=float, help="nu2 is a hyperparameter in SDNE"
+
+    device, "-- device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") "
+
+    output  "--output", default="node.emb", help="Output representation file"
+
+
+    Reference
+        ----------
+        .. [1] Wang, D., Cui, P., & Zhu, W. (2016, August). Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1225-1234).
+
+        https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf
+    """
+
+    def __init__(
+        self, graph, node_size, nhid0, nhid1, dropout=0.06, alpha=2e-2, beta=10.0
+    ):
+        super(SDNE, self).__init__()
+        self.encode0 = nn.Linear(node_size, nhid0)
+        self.encode1 = nn.Linear(nhid0, nhid1)
+        self.decode0 = nn.Linear(nhid1, nhid0)
+        self.decode1 = nn.Linear(nhid0, node_size)
+        self.droput = dropout
+        self.alpha = alpha
+        self.beta = beta
+        self.graph = graph
+
+    def forward(self, adj_batch, adj_mat, b_mat):
+        t0 = F.leaky_relu(self.encode0(adj_batch))
+        t0 = F.leaky_relu(self.encode1(t0))
+        embedding = t0
+        t0 = F.leaky_relu(self.decode0(t0))
+        t0 = F.leaky_relu(self.decode1(t0))
+        embedding_norm = torch.sum(embedding * embedding, dim=1, keepdim=True)
+        L_1st = torch.sum(
+            adj_mat
+            * (
+                embedding_norm
+                - 2 * torch.mm(embedding, torch.transpose(embedding, dim0=0, dim1=1))
+                + torch.transpose(embedding_norm, dim0=0, dim1=1)
+            )
+        )
+        L_2nd = torch.sum(((adj_batch - t0) * b_mat) * ((adj_batch - t0) * b_mat))
+        return L_1st, self.alpha * L_2nd, L_1st + self.alpha * L_2nd
+
+    def train(
+        self,
+        model,
+        epochs=100,
+        lr=0.006,
+        bs=100,
+        step_size=10,
+        gamma=0.9,
+        nu1=1e-5,
+        nu2=1e-4,
+        device="cpu",
+        output="out.emb",
+    ):
+        Adj, Node = get_adj(self.graph)
+        model = model.to(device)
+
+        opt = optim.Adam(model.parameters(), lr=lr)
+        scheduler = torch.optim.lr_scheduler.StepLR(
+            opt, step_size=step_size, gamma=gamma
+        )
+        Data = Dataload(Adj, Node)
+        Data = DataLoader(
+            Data,
+            batch_size=bs,
+            shuffle=True,
+        )
+
+        for epoch in range(1, epochs + 1):
+            loss_sum, loss_L1, loss_L2, loss_reg = 0, 0, 0, 0
+            for index in Data:
+                adj_batch = Adj[index]
+                adj_mat = adj_batch[:, index]
+                b_mat = torch.ones_like(adj_batch)
+                b_mat[adj_batch != 0] = self.beta
+
+                opt.zero_grad()
+                L_1st, L_2nd, L_all = model(adj_batch, adj_mat, b_mat)
+                L_reg = 0
+                for param in model.parameters():
+                    L_reg += nu1 * torch.sum(torch.abs(param)) + nu2 * torch.sum(
+                        param * param
+                    )
+                Loss = L_all + L_reg
+                Loss.backward()
+                opt.step()
+                loss_sum += Loss
+                loss_L1 += L_1st
+                loss_L2 += L_2nd
+                loss_reg += L_reg
+            scheduler.step(epoch)
+            # print("The lr for epoch %d is %f" %(epoch, scheduler.get_lr()[0]))
+            print("loss for epoch %d is:" % epoch)
+            print("loss_sum is %f" % loss_sum)
+            print("loss_L1 is %f" % loss_L1)
+            print("loss_L2 is %f" % loss_L2)
+            print("loss_reg is %f" % loss_reg)
+
+        # model.eval()
+        embedding = model.savector(Adj)
+        outVec = embedding.detach().numpy()
+        np.savetxt(output, outVec)
+
+        return outVec
+
+    def savector(self, adj):
+        t0 = self.encode0(adj)
+        t0 = self.encode1(t0)
+        return t0
+
+
+# if __name__ == '__main__':
+#     args = parse_args()
+#     print(args)
+#     dataset = eg.CiteseerGraphDataset(force_reload=True) # Download CiteseerGraphDataset contained in EasyGraph
+#     num_classes = dataset.num_classes
+#     g = dataset[0]
+#     print(g)
+#     device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+#     adj, node = get_adj(g)
+#     # labels = g.ndata['label']
+#     nhid0, nhid1, dropout, alpha = args.nhid0, args.nhid1, args.dropout, args.alpha
+#     model = SDNE(node, nhid0, nhid1, dropout, alpha, graph=g)
+#     print(model)
+#
+#     emb = model.train(args, device)
```

## easygraph/functions/graph_embedding/deepwalk.py

 * *Ordering differences only*

```diff
@@ -1,103 +1,103 @@
-import random
-
-from easygraph.functions.graph_embedding.node2vec import (
-    _get_embedding_result_from_gensim_skipgram_model,
-)
-from easygraph.functions.graph_embedding.node2vec import learn_embeddings
-from easygraph.utils import *
-from tqdm import tqdm
-
-
-__all__ = ["deepwalk"]
-
-
-@not_implemented_for("multigraph")
-def deepwalk(G, dimensions=128, walk_length=80, num_walks=10, **skip_gram_params):
-    """Graph embedding via DeepWalk.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-
-    dimensions : int
-        Embedding dimensions, optional(default: 128)
-
-    walk_length : int
-        Number of nodes in each walk, optional(default: 80)
-
-    num_walks : int
-        Number of walks per node, optional(default: 10)
-
-    skip_gram_params : dict
-        Parameters for gensim.models.Word2Vec - do not supply `size`, it is taken from the `dimensions` parameter
-
-    Returns
-    -------
-    embedding_vector : dict
-        The embedding vector of each node
-
-    most_similar_nodes_of_node : dict
-        The most similar nodes of each node and its similarity
-
-    Examples
-    --------
-
-    >>> deepwalk(G,
-    ...          dimensions=128, # The graph embedding dimensions.
-    ...          walk_length=80, # Walk length of each random walks.
-    ...          num_walks=10, # Number of random walks.
-    ...          skip_gram_params = dict( # The skip_gram parameters in Python package gensim.
-    ...          window=10,
-    ...             min_count=1,
-    ...             batch_words=4,
-    ...             iter=15
-    ...          ))
-
-    References
-    ----------
-    .. [1] https://arxiv.org/abs/1403.6652
-
-    """
-    G_index, index_of_node, node_of_index = G.to_index_node_graph()
-
-    walks = simulate_walks(G_index, walk_length=walk_length, num_walks=num_walks)
-    model = learn_embeddings(walks=walks, dimensions=dimensions, **skip_gram_params)
-
-    (
-        embedding_vector,
-        most_similar_nodes_of_node,
-    ) = _get_embedding_result_from_gensim_skipgram_model(
-        G=G, index_of_node=index_of_node, node_of_index=node_of_index, model=model
-    )
-
-    del G_index
-    return embedding_vector, most_similar_nodes_of_node
-
-
-def simulate_walks(G, walk_length, num_walks):
-    walks = []
-    nodes = list(G.nodes)
-    print("Walk iteration:")
-    for walk_iter in tqdm(range(num_walks)):
-        random.shuffle(nodes)
-        for node in nodes:
-            walks.append(_deepwalk_walk(G, walk_length=walk_length, start_node=node))
-
-    return walks
-
-
-def _deepwalk_walk(G, walk_length, start_node):
-    """
-    Simulate a random walk starting from start node.
-    """
-    walk = [start_node]
-
-    while len(walk) < walk_length:
-        cur = walk[-1]
-        cur_nbrs = sorted(G.neighbors(cur))
-        if len(cur_nbrs) > 0:
-            pick_node = random.choice(cur_nbrs)
-            walk.append(pick_node)
-        else:
-            break
-    return walk
+import random
+
+from easygraph.functions.graph_embedding.node2vec import (
+    _get_embedding_result_from_gensim_skipgram_model,
+)
+from easygraph.functions.graph_embedding.node2vec import learn_embeddings
+from easygraph.utils import *
+from tqdm import tqdm
+
+
+__all__ = ["deepwalk"]
+
+
+@not_implemented_for("multigraph")
+def deepwalk(G, dimensions=128, walk_length=80, num_walks=10, **skip_gram_params):
+    """Graph embedding via DeepWalk.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+
+    dimensions : int
+        Embedding dimensions, optional(default: 128)
+
+    walk_length : int
+        Number of nodes in each walk, optional(default: 80)
+
+    num_walks : int
+        Number of walks per node, optional(default: 10)
+
+    skip_gram_params : dict
+        Parameters for gensim.models.Word2Vec - do not supply `size`, it is taken from the `dimensions` parameter
+
+    Returns
+    -------
+    embedding_vector : dict
+        The embedding vector of each node
+
+    most_similar_nodes_of_node : dict
+        The most similar nodes of each node and its similarity
+
+    Examples
+    --------
+
+    >>> deepwalk(G,
+    ...          dimensions=128, # The graph embedding dimensions.
+    ...          walk_length=80, # Walk length of each random walks.
+    ...          num_walks=10, # Number of random walks.
+    ...          skip_gram_params = dict( # The skip_gram parameters in Python package gensim.
+    ...          window=10,
+    ...             min_count=1,
+    ...             batch_words=4,
+    ...             iter=15
+    ...          ))
+
+    References
+    ----------
+    .. [1] https://arxiv.org/abs/1403.6652
+
+    """
+    G_index, index_of_node, node_of_index = G.to_index_node_graph()
+
+    walks = simulate_walks(G_index, walk_length=walk_length, num_walks=num_walks)
+    model = learn_embeddings(walks=walks, dimensions=dimensions, **skip_gram_params)
+
+    (
+        embedding_vector,
+        most_similar_nodes_of_node,
+    ) = _get_embedding_result_from_gensim_skipgram_model(
+        G=G, index_of_node=index_of_node, node_of_index=node_of_index, model=model
+    )
+
+    del G_index
+    return embedding_vector, most_similar_nodes_of_node
+
+
+def simulate_walks(G, walk_length, num_walks):
+    walks = []
+    nodes = list(G.nodes)
+    print("Walk iteration:")
+    for walk_iter in tqdm(range(num_walks)):
+        random.shuffle(nodes)
+        for node in nodes:
+            walks.append(_deepwalk_walk(G, walk_length=walk_length, start_node=node))
+
+    return walks
+
+
+def _deepwalk_walk(G, walk_length, start_node):
+    """
+    Simulate a random walk starting from start node.
+    """
+    walk = [start_node]
+
+    while len(walk) < walk_length:
+        cur = walk[-1]
+        cur_nbrs = sorted(G.neighbors(cur))
+        if len(cur_nbrs) > 0:
+            pick_node = random.choice(cur_nbrs)
+            walk.append(pick_node)
+        else:
+            break
+    return walk
```

## easygraph/functions/graph_embedding/node2vec.py

 * *Ordering differences only*

```diff
@@ -1,308 +1,308 @@
-import random
-
-import numpy as np
-
-from easygraph.utils import *
-from tqdm import tqdm
-
-
-__all__ = ["node2vec"]
-
-
-@not_implemented_for("multigraph")
-def node2vec(
-    G,
-    dimensions=128,
-    walk_length=80,
-    num_walks=10,
-    p=1.0,
-    q=1.0,
-    weight_key=None,
-    workers=None,
-    **skip_gram_params,
-):
-    """Graph embedding via Node2Vec.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-
-    dimensions : int
-        Embedding dimensions, optional(default: 128)
-
-    walk_length : int
-        Number of nodes in each walk, optional(default: 80)
-
-    num_walks : int
-        Number of walks per node, optional(default: 10)
-
-    p : float
-        The return hyper parameter, optional(default: 1.0)
-
-    q : float
-        The input parameter, optional(default: 1.0)
-
-    weight_key : string or None (default: None)
-        On weighted graphs, this is the key for the weight attribute
-
-    workers : int or None, optional(default : None)
-        The number of workers generating random walks (default: None). None if not using only one worker.
-
-    skip_gram_params : dict
-        Parameters for gensim.models.Word2Vec - do not supply 'size', it is taken from the 'dimensions' parameter
-
-    Returns
-    -------
-    embedding_vector : dict
-        The embedding vector of each node
-
-    most_similar_nodes_of_node : dict
-        The most similar nodes of each node and its similarity
-
-    Examples
-    --------
-
-    >>> node2vec(G,
-    ...          dimensions=128, # The graph embedding dimensions.
-    ...          walk_length=80, # Walk length of each random walks.
-    ...          num_walks=10, # Number of random walks.
-    ...          p=1.0, # The `p` possibility in random walk in [1]_
-    ...          q=1.0, # The `q` possibility in random walk in [1]_
-    ...          weight_key='weight',
-    ...          skip_gram_params=dict( # The skip_gram parameters in Python package gensim.
-    ...          window=10,
-    ...             min_count=1,
-    ...             batch_words=4
-    ...          ))
-
-    References
-    ----------
-    .. [1] https://arxiv.org/abs/1607.00653
-
-    """
-    G_index, index_of_node, node_of_index = G.to_index_node_graph()
-
-    if workers is None:
-        walks = simulate_walks(
-            G_index,
-            walk_length=walk_length,
-            num_walks=num_walks,
-            p=p,
-            q=q,
-            weight_key=weight_key,
-        )
-    else:
-        from joblib import Parallel
-        from joblib import delayed
-
-        num_walks_lists = np.array_split(range(num_walks), workers)
-        walks = Parallel(n_jobs=workers)(
-            delayed(simulate_walks)(
-                G_index, walk_length, len(num_walks), p, q, weight_key
-            )
-            for num_walks in num_walks_lists
-        )
-        # Change multidimensional array to one dimensional array
-        walks = [walk for walk_group in walks for walk in walk_group]
-
-    model = learn_embeddings(walks=walks, dimensions=dimensions, **skip_gram_params)
-
-    (
-        embedding_vector,
-        most_similar_nodes_of_node,
-    ) = _get_embedding_result_from_gensim_skipgram_model(
-        G=G, index_of_node=index_of_node, node_of_index=node_of_index, model=model
-    )
-
-    del G_index
-    return embedding_vector, most_similar_nodes_of_node
-
-
-def _get_embedding_result_from_gensim_skipgram_model(
-    G, index_of_node, node_of_index, model
-):
-    embedding_vector = dict()
-    most_similar_nodes_of_node = dict()
-
-    def change_string_to_node_from_gensim_return_value(value_including_str):
-        # As the return value of gensim model.wv.most_similar includes string index in G_index,
-        # the string index should be changed to the original node element in G.
-        result = []
-        for node_index, value in value_including_str:
-            node_index = int(node_index)
-            node = node_of_index[node_index]
-            result.append((node, value))
-        return result
-
-    for node in G.nodes:
-        # Output node names are always strings in gensim
-        embedding_vector[node] = model.wv[str(index_of_node[node])]
-
-        most_similar_nodes = model.wv.most_similar(str(index_of_node[node]))
-        most_similar_nodes_of_node[
-            node
-        ] = change_string_to_node_from_gensim_return_value(most_similar_nodes)
-
-    return embedding_vector, most_similar_nodes_of_node
-
-
-def simulate_walks(G, walk_length, num_walks, p, q, weight_key=None):
-    alias_nodes, alias_edges = _preprocess_transition_probs(G, p, q, weight_key)
-    walks = []
-    nodes = list(G.nodes)
-    for walk_iter in tqdm(range(num_walks)):
-        random.shuffle(nodes)
-        for node in nodes:
-            walks.append(
-                _node2vec_walk(
-                    G,
-                    walk_length=walk_length,
-                    start_node=node,
-                    alias_nodes=alias_nodes,
-                    alias_edges=alias_edges,
-                )
-            )
-
-    return walks
-
-
-def _preprocess_transition_probs(G, p, q, weight_key=None):
-    is_directed = G.is_directed()
-    alias_nodes = {}
-
-    for node in G.nodes:
-        if weight_key is None:
-            unnormalized_probs = [1.0 for nbr in sorted(G.neighbors(node))]
-        else:
-            unnormalized_probs = [
-                G[node][nbr][weight_key] for nbr in sorted(G.neighbors(node))
-            ]
-        norm_const = sum(unnormalized_probs)
-        normalized_probs = [float(u_prob) / norm_const for u_prob in unnormalized_probs]
-        alias_nodes[node] = _alias_setup(normalized_probs)
-
-    alias_edges = {}
-    triads = {}
-
-    if is_directed:
-        for edge in G.edges:
-            alias_edges[(edge[0], edge[1])] = _get_alias_edge(
-                G, edge[0], edge[1], p, q, weight_key
-            )
-    else:
-        for edge in G.edges:
-            alias_edges[(edge[0], edge[1])] = _get_alias_edge(
-                G, edge[0], edge[1], p, q, weight_key
-            )
-            alias_edges[(edge[1], edge[0])] = _get_alias_edge(
-                G, edge[1], edge[0], p, q, weight_key
-            )
-
-    return alias_nodes, alias_edges
-
-
-def _get_alias_edge(G, src, dst, p, q, weight_key=None):
-    unnormalized_probs = []
-
-    if weight_key is None:
-        for dst_nbr in sorted(G.neighbors(dst)):
-            if dst_nbr == src:
-                unnormalized_probs.append(1.0 / p)
-            elif G.has_edge(dst_nbr, src):
-                unnormalized_probs.append(1.0)
-            else:
-                unnormalized_probs.append(1.0 / q)
-    else:
-        for dst_nbr in sorted(G.neighbors(dst)):
-            if dst_nbr == src:
-                unnormalized_probs.append(G[dst][dst_nbr][weight_key] / p)
-            elif G.has_edge(dst_nbr, src):
-                unnormalized_probs.append(G[dst][dst_nbr][weight_key])
-            else:
-                unnormalized_probs.append(G[dst][dst_nbr][weight_key] / q)
-
-    norm_const = sum(unnormalized_probs)
-    normalized_probs = [float(u_prob) / norm_const for u_prob in unnormalized_probs]
-
-    return _alias_setup(normalized_probs)
-
-
-def _alias_setup(probs):
-    K = len(probs)
-    q = np.zeros(K)
-    J = np.zeros(K, dtype=int)
-
-    smaller = []
-    larger = []
-    for kk, prob in enumerate(probs):
-        q[kk] = K * prob
-        if q[kk] < 1.0:
-            smaller.append(kk)
-        else:
-            larger.append(kk)
-
-    while len(smaller) > 0 and len(larger) > 0:
-        small = smaller.pop()
-        large = larger.pop()
-
-        J[small] = large
-        q[large] = q[large] + q[small] - 1.0
-        if q[large] < 1.0:
-            smaller.append(large)
-        else:
-            larger.append(large)
-
-    return J, q
-
-
-def _node2vec_walk(G, walk_length, start_node, alias_nodes, alias_edges):
-    """
-    Simulate a random walk starting from start node.
-    """
-    walk = [start_node]
-
-    while len(walk) < walk_length:
-        cur = walk[-1]
-        cur_nbrs = sorted(G.neighbors(cur))
-        if len(cur_nbrs) > 0:
-            if len(walk) == 1:
-                walk.append(
-                    cur_nbrs[_alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])]
-                )
-            else:
-                prev = walk[-2]
-                next_node = cur_nbrs[
-                    _alias_draw(
-                        alias_edges[(prev, cur)][0], alias_edges[(prev, cur)][1]
-                    )
-                ]
-                walk.append(next_node)
-        else:
-            break
-
-    return walk
-
-
-def _alias_draw(J, q):
-    K = len(J)
-    kk = int(np.floor(np.random.rand() * K))
-    if np.random.rand() < q[kk]:
-        return kk
-    else:
-        return J[kk]
-
-
-def learn_embeddings(walks, dimensions, **skip_gram_params):
-    """
-    Learn embeddings with Word2Vec.
-    """
-    from gensim.models import Word2Vec
-
-    walks = [list(map(str, walk)) for walk in walks]
-
-    if "vector_size" not in skip_gram_params:
-        skip_gram_params["vector_size"] = dimensions
-
-    model = Word2Vec(walks, **skip_gram_params)
-
-    return model
+import random
+
+import numpy as np
+
+from easygraph.utils import *
+from tqdm import tqdm
+
+
+__all__ = ["node2vec"]
+
+
+@not_implemented_for("multigraph")
+def node2vec(
+    G,
+    dimensions=128,
+    walk_length=80,
+    num_walks=10,
+    p=1.0,
+    q=1.0,
+    weight_key=None,
+    workers=None,
+    **skip_gram_params,
+):
+    """Graph embedding via Node2Vec.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+
+    dimensions : int
+        Embedding dimensions, optional(default: 128)
+
+    walk_length : int
+        Number of nodes in each walk, optional(default: 80)
+
+    num_walks : int
+        Number of walks per node, optional(default: 10)
+
+    p : float
+        The return hyper parameter, optional(default: 1.0)
+
+    q : float
+        The input parameter, optional(default: 1.0)
+
+    weight_key : string or None (default: None)
+        On weighted graphs, this is the key for the weight attribute
+
+    workers : int or None, optional(default : None)
+        The number of workers generating random walks (default: None). None if not using only one worker.
+
+    skip_gram_params : dict
+        Parameters for gensim.models.Word2Vec - do not supply 'size', it is taken from the 'dimensions' parameter
+
+    Returns
+    -------
+    embedding_vector : dict
+        The embedding vector of each node
+
+    most_similar_nodes_of_node : dict
+        The most similar nodes of each node and its similarity
+
+    Examples
+    --------
+
+    >>> node2vec(G,
+    ...          dimensions=128, # The graph embedding dimensions.
+    ...          walk_length=80, # Walk length of each random walks.
+    ...          num_walks=10, # Number of random walks.
+    ...          p=1.0, # The `p` possibility in random walk in [1]_
+    ...          q=1.0, # The `q` possibility in random walk in [1]_
+    ...          weight_key='weight',
+    ...          skip_gram_params=dict( # The skip_gram parameters in Python package gensim.
+    ...          window=10,
+    ...             min_count=1,
+    ...             batch_words=4
+    ...          ))
+
+    References
+    ----------
+    .. [1] https://arxiv.org/abs/1607.00653
+
+    """
+    G_index, index_of_node, node_of_index = G.to_index_node_graph()
+
+    if workers is None:
+        walks = simulate_walks(
+            G_index,
+            walk_length=walk_length,
+            num_walks=num_walks,
+            p=p,
+            q=q,
+            weight_key=weight_key,
+        )
+    else:
+        from joblib import Parallel
+        from joblib import delayed
+
+        num_walks_lists = np.array_split(range(num_walks), workers)
+        walks = Parallel(n_jobs=workers)(
+            delayed(simulate_walks)(
+                G_index, walk_length, len(num_walks), p, q, weight_key
+            )
+            for num_walks in num_walks_lists
+        )
+        # Change multidimensional array to one dimensional array
+        walks = [walk for walk_group in walks for walk in walk_group]
+
+    model = learn_embeddings(walks=walks, dimensions=dimensions, **skip_gram_params)
+
+    (
+        embedding_vector,
+        most_similar_nodes_of_node,
+    ) = _get_embedding_result_from_gensim_skipgram_model(
+        G=G, index_of_node=index_of_node, node_of_index=node_of_index, model=model
+    )
+
+    del G_index
+    return embedding_vector, most_similar_nodes_of_node
+
+
+def _get_embedding_result_from_gensim_skipgram_model(
+    G, index_of_node, node_of_index, model
+):
+    embedding_vector = dict()
+    most_similar_nodes_of_node = dict()
+
+    def change_string_to_node_from_gensim_return_value(value_including_str):
+        # As the return value of gensim model.wv.most_similar includes string index in G_index,
+        # the string index should be changed to the original node element in G.
+        result = []
+        for node_index, value in value_including_str:
+            node_index = int(node_index)
+            node = node_of_index[node_index]
+            result.append((node, value))
+        return result
+
+    for node in G.nodes:
+        # Output node names are always strings in gensim
+        embedding_vector[node] = model.wv[str(index_of_node[node])]
+
+        most_similar_nodes = model.wv.most_similar(str(index_of_node[node]))
+        most_similar_nodes_of_node[
+            node
+        ] = change_string_to_node_from_gensim_return_value(most_similar_nodes)
+
+    return embedding_vector, most_similar_nodes_of_node
+
+
+def simulate_walks(G, walk_length, num_walks, p, q, weight_key=None):
+    alias_nodes, alias_edges = _preprocess_transition_probs(G, p, q, weight_key)
+    walks = []
+    nodes = list(G.nodes)
+    for walk_iter in tqdm(range(num_walks)):
+        random.shuffle(nodes)
+        for node in nodes:
+            walks.append(
+                _node2vec_walk(
+                    G,
+                    walk_length=walk_length,
+                    start_node=node,
+                    alias_nodes=alias_nodes,
+                    alias_edges=alias_edges,
+                )
+            )
+
+    return walks
+
+
+def _preprocess_transition_probs(G, p, q, weight_key=None):
+    is_directed = G.is_directed()
+    alias_nodes = {}
+
+    for node in G.nodes:
+        if weight_key is None:
+            unnormalized_probs = [1.0 for nbr in sorted(G.neighbors(node))]
+        else:
+            unnormalized_probs = [
+                G[node][nbr][weight_key] for nbr in sorted(G.neighbors(node))
+            ]
+        norm_const = sum(unnormalized_probs)
+        normalized_probs = [float(u_prob) / norm_const for u_prob in unnormalized_probs]
+        alias_nodes[node] = _alias_setup(normalized_probs)
+
+    alias_edges = {}
+    triads = {}
+
+    if is_directed:
+        for edge in G.edges:
+            alias_edges[(edge[0], edge[1])] = _get_alias_edge(
+                G, edge[0], edge[1], p, q, weight_key
+            )
+    else:
+        for edge in G.edges:
+            alias_edges[(edge[0], edge[1])] = _get_alias_edge(
+                G, edge[0], edge[1], p, q, weight_key
+            )
+            alias_edges[(edge[1], edge[0])] = _get_alias_edge(
+                G, edge[1], edge[0], p, q, weight_key
+            )
+
+    return alias_nodes, alias_edges
+
+
+def _get_alias_edge(G, src, dst, p, q, weight_key=None):
+    unnormalized_probs = []
+
+    if weight_key is None:
+        for dst_nbr in sorted(G.neighbors(dst)):
+            if dst_nbr == src:
+                unnormalized_probs.append(1.0 / p)
+            elif G.has_edge(dst_nbr, src):
+                unnormalized_probs.append(1.0)
+            else:
+                unnormalized_probs.append(1.0 / q)
+    else:
+        for dst_nbr in sorted(G.neighbors(dst)):
+            if dst_nbr == src:
+                unnormalized_probs.append(G[dst][dst_nbr][weight_key] / p)
+            elif G.has_edge(dst_nbr, src):
+                unnormalized_probs.append(G[dst][dst_nbr][weight_key])
+            else:
+                unnormalized_probs.append(G[dst][dst_nbr][weight_key] / q)
+
+    norm_const = sum(unnormalized_probs)
+    normalized_probs = [float(u_prob) / norm_const for u_prob in unnormalized_probs]
+
+    return _alias_setup(normalized_probs)
+
+
+def _alias_setup(probs):
+    K = len(probs)
+    q = np.zeros(K)
+    J = np.zeros(K, dtype=int)
+
+    smaller = []
+    larger = []
+    for kk, prob in enumerate(probs):
+        q[kk] = K * prob
+        if q[kk] < 1.0:
+            smaller.append(kk)
+        else:
+            larger.append(kk)
+
+    while len(smaller) > 0 and len(larger) > 0:
+        small = smaller.pop()
+        large = larger.pop()
+
+        J[small] = large
+        q[large] = q[large] + q[small] - 1.0
+        if q[large] < 1.0:
+            smaller.append(large)
+        else:
+            larger.append(large)
+
+    return J, q
+
+
+def _node2vec_walk(G, walk_length, start_node, alias_nodes, alias_edges):
+    """
+    Simulate a random walk starting from start node.
+    """
+    walk = [start_node]
+
+    while len(walk) < walk_length:
+        cur = walk[-1]
+        cur_nbrs = sorted(G.neighbors(cur))
+        if len(cur_nbrs) > 0:
+            if len(walk) == 1:
+                walk.append(
+                    cur_nbrs[_alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])]
+                )
+            else:
+                prev = walk[-2]
+                next_node = cur_nbrs[
+                    _alias_draw(
+                        alias_edges[(prev, cur)][0], alias_edges[(prev, cur)][1]
+                    )
+                ]
+                walk.append(next_node)
+        else:
+            break
+
+    return walk
+
+
+def _alias_draw(J, q):
+    K = len(J)
+    kk = int(np.floor(np.random.rand() * K))
+    if np.random.rand() < q[kk]:
+        return kk
+    else:
+        return J[kk]
+
+
+def learn_embeddings(walks, dimensions, **skip_gram_params):
+    """
+    Learn embeddings with Word2Vec.
+    """
+    from gensim.models import Word2Vec
+
+    walks = [list(map(str, walk)) for walk in walks]
+
+    if "vector_size" not in skip_gram_params:
+        skip_gram_params["vector_size"] = dimensions
+
+    model = Word2Vec(walks, **skip_gram_params)
+
+    return model
```

## easygraph/functions/graph_embedding/NOBE.py

 * *Ordering differences only*

```diff
@@ -1,184 +1,184 @@
-import easygraph as eg
-import numpy as np
-
-from easygraph.utils import *
-
-
-__all__ = ["NOBE", "NOBE_GA"]
-
-
-@not_implemented_for("multigraph")
-def NOBE(G, K):
-    """Graph embedding via NOBE[1].
-
-    Parameters
-    ----------
-    G : easygraph.Graph
-        An unweighted and undirected graph.
-
-    K : int
-        Embedding dimension k
-
-    Returns
-    -------
-    Y : list
-        list of embedding vectors (y1, y2, · · · , yn)
-
-    Examples
-    --------
-    >>> NOBE(G,K=15)
-
-    References
-    ----------
-    .. [1] https://www.researchgate.net/publication/325004496_On_Spectral_Graph_Embedding_A_Non-Backtracking_Perspective_and_Graph_Approximation
-
-    """
-    dict = {}
-    a = 0
-    for i in G.nodes:
-        dict[i] = a
-        a += 1
-    LG = graph_to_d_atleast2(G)
-    N = len(G)
-    P, pair = Transition(LG)
-    V = eigs_nodes(P, K)
-    Y = embedding(V, pair, K, N, dict, G)
-    return Y
-
-
-@not_implemented_for("multigraph")
-def NOBE_GA(G, K):
-    """Graph embedding via NOBE-GA[1].
-
-    Parameters
-    ----------
-    G : easygraph.Graph
-        An unweighted and undirected graph.
-
-    K : int
-        Embedding dimension k
-
-    Returns
-    -------
-    Y : list
-        list of embedding vectors (y1, y2, · · · , yn)
-
-    Examples
-    --------
-    >>> NOBE_GA(G,K=15)
-
-    References
-    ----------
-    .. [1] https://www.researchgate.net/publication/325004496_On_Spectral_Graph_Embedding_A_Non-Backtracking_Perspective_and_Graph_Approximation
-
-    """
-    from scipy.sparse.linalg import eigs
-
-    N = len(G)
-    A = np.eye(N, N)
-    for i in G.edges:
-        (u, v, t) = i
-        u = int(u) - 1
-        v = int(v) - 1
-        A[u, v] = 1
-    degree = G.degree()
-    D_inv = np.zeros([N, N])
-    a = 0
-    for i in degree:
-        D_inv[a, a] = 1 / degree[i]
-        a += 1
-    D_I_inv = np.zeros([N, N])
-    b = 0
-    for i in degree:
-        if degree[i] > 1:
-            D_I_inv[b, b] = 1 / (degree[i] - 1)
-        b += 1
-    I = np.identity(N)
-    M_D = 0.5 * A * D_I_inv * (I - D_inv)
-    D_D = 0.5 * I
-    T_ua = np.zeros([2 * N, 2 * N])
-    T_ua[0:N, 0:N] = M_D
-    T_ua[N : 2 * N, N : 2 * N] = M_D
-    T_ua[N : 2 * N, 0:N] = D_D
-    T_ua[0:N, N : 2 * N] = D_D
-    Y1, Y = eigs(T_ua, K + 1, which="LR")
-    Y = Y[0:N, :-1]
-    return Y
-
-
-def graph_to_d_atleast2(G):
-    n = len(G)
-    LG = eg.Graph()
-    LG = G.copy()
-    new_node = n
-    degree = LG.degree()
-    node = LG.nodes.copy()
-    for i in node:
-        if degree[i] == 1:
-            for neighbors in LG.neighbors(node=i):
-                LG.add_edge(i, new_node)
-                LG.add_edge(new_node, neighbors)
-                break
-            new_node = new_node + 1
-    return LG
-
-
-def Transition(LG):
-    N = len(LG)
-    M = LG.size()
-    LLG = eg.DiGraph()
-    for i in LG.edges:
-        (u, v, t) = i
-        LLG.add_edge(u, v)
-        LLG.add_edge(v, u)
-    degree = LLG.degree()
-    P = np.zeros([2 * M, 2 * M])
-    pair = []
-    k = 0
-    l = 0
-    for i in LLG.edges:
-        l = 0
-        for j in LLG.edges:
-            (u, v, t) = i
-            (x, y, z) = j
-            if v == x and u != y:
-                P[k][l] = 1 / (degree[v] - 1)
-            l += 1
-        k += 1
-    a = 0
-    for i in LLG.edges:
-        (u, v, t) = i
-        pair.append([u, v])
-        a += 1
-    return P, pair
-
-
-def eigs_nodes(P, K):
-    from scipy.sparse.linalg import eigs
-
-    M = np.size(P, 0)
-    L = np.zeros([M, M])
-    I = np.identity(M)
-    P_T = P.T
-    L = I - (P + P_T) / 2
-    U, D = eigs(L, K + 1, which="LR")
-    D = D[:, :-1]
-    V = np.zeros([M, K], dtype=complex)
-    a = 0
-    for i in D:
-        V[a] = i
-        a += 1
-    return V
-
-
-def embedding(V, pair, K, N, dict, G):
-    Y = np.zeros([N, K], dtype=complex)
-    idx = 0
-    for i in pair:
-        [v, u] = i
-        if u in G.nodes:
-            t = dict[u]
-            for j in range(0, len(V[idx])):
-                Y[t, j] += V[idx, j]
-            idx += 1
-    return Y
+import easygraph as eg
+import numpy as np
+
+from easygraph.utils import *
+
+
+__all__ = ["NOBE", "NOBE_GA"]
+
+
+@not_implemented_for("multigraph")
+def NOBE(G, K):
+    """Graph embedding via NOBE[1].
+
+    Parameters
+    ----------
+    G : easygraph.Graph
+        An unweighted and undirected graph.
+
+    K : int
+        Embedding dimension k
+
+    Returns
+    -------
+    Y : list
+        list of embedding vectors (y1, y2, · · · , yn)
+
+    Examples
+    --------
+    >>> NOBE(G,K=15)
+
+    References
+    ----------
+    .. [1] https://www.researchgate.net/publication/325004496_On_Spectral_Graph_Embedding_A_Non-Backtracking_Perspective_and_Graph_Approximation
+
+    """
+    dict = {}
+    a = 0
+    for i in G.nodes:
+        dict[i] = a
+        a += 1
+    LG = graph_to_d_atleast2(G)
+    N = len(G)
+    P, pair = Transition(LG)
+    V = eigs_nodes(P, K)
+    Y = embedding(V, pair, K, N, dict, G)
+    return Y
+
+
+@not_implemented_for("multigraph")
+def NOBE_GA(G, K):
+    """Graph embedding via NOBE-GA[1].
+
+    Parameters
+    ----------
+    G : easygraph.Graph
+        An unweighted and undirected graph.
+
+    K : int
+        Embedding dimension k
+
+    Returns
+    -------
+    Y : list
+        list of embedding vectors (y1, y2, · · · , yn)
+
+    Examples
+    --------
+    >>> NOBE_GA(G,K=15)
+
+    References
+    ----------
+    .. [1] https://www.researchgate.net/publication/325004496_On_Spectral_Graph_Embedding_A_Non-Backtracking_Perspective_and_Graph_Approximation
+
+    """
+    from scipy.sparse.linalg import eigs
+
+    N = len(G)
+    A = np.eye(N, N)
+    for i in G.edges:
+        (u, v, t) = i
+        u = int(u) - 1
+        v = int(v) - 1
+        A[u, v] = 1
+    degree = G.degree()
+    D_inv = np.zeros([N, N])
+    a = 0
+    for i in degree:
+        D_inv[a, a] = 1 / degree[i]
+        a += 1
+    D_I_inv = np.zeros([N, N])
+    b = 0
+    for i in degree:
+        if degree[i] > 1:
+            D_I_inv[b, b] = 1 / (degree[i] - 1)
+        b += 1
+    I = np.identity(N)
+    M_D = 0.5 * A * D_I_inv * (I - D_inv)
+    D_D = 0.5 * I
+    T_ua = np.zeros([2 * N, 2 * N])
+    T_ua[0:N, 0:N] = M_D
+    T_ua[N : 2 * N, N : 2 * N] = M_D
+    T_ua[N : 2 * N, 0:N] = D_D
+    T_ua[0:N, N : 2 * N] = D_D
+    Y1, Y = eigs(T_ua, K + 1, which="LR")
+    Y = Y[0:N, :-1]
+    return Y
+
+
+def graph_to_d_atleast2(G):
+    n = len(G)
+    LG = eg.Graph()
+    LG = G.copy()
+    new_node = n
+    degree = LG.degree()
+    node = LG.nodes.copy()
+    for i in node:
+        if degree[i] == 1:
+            for neighbors in LG.neighbors(node=i):
+                LG.add_edge(i, new_node)
+                LG.add_edge(new_node, neighbors)
+                break
+            new_node = new_node + 1
+    return LG
+
+
+def Transition(LG):
+    N = len(LG)
+    M = LG.size()
+    LLG = eg.DiGraph()
+    for i in LG.edges:
+        (u, v, t) = i
+        LLG.add_edge(u, v)
+        LLG.add_edge(v, u)
+    degree = LLG.degree()
+    P = np.zeros([2 * M, 2 * M])
+    pair = []
+    k = 0
+    l = 0
+    for i in LLG.edges:
+        l = 0
+        for j in LLG.edges:
+            (u, v, t) = i
+            (x, y, z) = j
+            if v == x and u != y:
+                P[k][l] = 1 / (degree[v] - 1)
+            l += 1
+        k += 1
+    a = 0
+    for i in LLG.edges:
+        (u, v, t) = i
+        pair.append([u, v])
+        a += 1
+    return P, pair
+
+
+def eigs_nodes(P, K):
+    from scipy.sparse.linalg import eigs
+
+    M = np.size(P, 0)
+    L = np.zeros([M, M])
+    I = np.identity(M)
+    P_T = P.T
+    L = I - (P + P_T) / 2
+    U, D = eigs(L, K + 1, which="LR")
+    D = D[:, :-1]
+    V = np.zeros([M, K], dtype=complex)
+    a = 0
+    for i in D:
+        V[a] = i
+        a += 1
+    return V
+
+
+def embedding(V, pair, K, N, dict, G):
+    Y = np.zeros([N, K], dtype=complex)
+    idx = 0
+    for i in pair:
+        [v, u] = i
+        if u in G.nodes:
+            t = dict[u]
+            for j in range(0, len(V[idx])):
+                Y[t, j] += V[idx, j]
+            idx += 1
+    return Y
```

## easygraph/functions/graph_embedding/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-from .deepwalk import *
-from .NOBE import *
-from .node2vec import *
-
-
-try:
-    from .line import *
-    from .sdne import *
-except:
-    print(
-        "Warning raise in module:graph_embedding. Please install packages pytorch"
-        "before you use functions related to graph_embedding"
-    )
+from .deepwalk import *
+from .NOBE import *
+from .node2vec import *
+
+
+try:
+    from .line import *
+    from .sdne import *
+except:
+    print(
+        "Warning raise in module:graph_embedding. Please install packages pytorch"
+        "before you use functions related to graph_embedding"
+    )
```

## easygraph/functions/graph_embedding/net_emb_example_citeseer.py

 * *Ordering differences only*

```diff
@@ -1,175 +1,175 @@
-from __future__ import print_function
-
-import argparse
-import csv
-import time
-import warnings
-
-from datetime import datetime
-
-import easygraph as eg
-import matplotlib.pyplot as plt
-import numpy as np
-import pandas as pd
-import seaborn as sns
-import torch
-
-from easygraph.datasets.citation_graph import CiteseerGraphDataset
-from easygraph.functions.community import greedy_modularity_communities
-from easygraph.functions.community import modularity
-from easygraph.functions.graph_embedding import *
-from mpl_toolkits.mplot3d import Axes3D
-from sklearn.decomposition import PCA
-from sklearn.manifold import TSNE
-
-
-warnings.filterwarnings("ignore")
-
-
-if __name__ == "__main__":
-    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
-    dataset = CiteseerGraphDataset(
-        force_reload=True
-    )  # Download CiteseerGraphDataset contained in EasyGraph
-    num_classes = dataset.num_classes
-    g = dataset[0]
-    labels = g.ndata["label"]
-    print(labels, labels.shape, len(g.nodes))
-
-    print("Graph embedding via DeepWalk...........")
-    deepwalk_emb, _ = deepwalk(g, dimensions=128, walk_length=80, num_walks=10)
-    # print(deepwalk_emb, len(deepwalk_emb))
-
-    dw_emb = []
-    for i in range(0, len(deepwalk_emb)):
-        dw_emb.append(list(deepwalk_emb[i]))
-    #   print(len(dw_emb))
-    dw_emb = np.array(dw_emb)
-    print(dw_emb)
-
-    tsne = TSNE(n_components=2, verbose=1, random_state=0)
-    z = tsne.fit_transform(dw_emb)
-    z_data = np.vstack((z.T, labels)).T
-    df_tsne = pd.DataFrame(z_data, columns=["Dim1", "Dim2", "class"])
-    df_tsne["class"] = df_tsne["class"].astype(int)
-    plt.figure(figsize=(8, 8))
-    sns.scatterplot(
-        data=df_tsne,
-        hue="class",
-        x="Dim1",
-        y="Dim2",
-        palette=["green", "orange", "brown", "red", "blue", "black"],
-    )
-    plt.savefig(
-        "figs/dw_citeseer.pdf", bbox_inches="tight"
-    )  # save embeddings if needed
-    plt.savefig("figs/dw_citeseer.png", bbox_inches="tight")
-    plt.show()
-
-    print("Graph embedding via Node2Vec..............")
-    node2vec_emb, _ = node2vec(
-        g, dimensions=128, walk_length=80, num_walks=10, p=4, q=0.25
-    )
-    # print(node2vec_emb, len(node2vec_emb))
-
-    n2v_emb = []
-    for i in range(0, len(node2vec_emb)):
-        n2v_emb.append(list(node2vec_emb[i]))
-    # print(len(n2v_emb))
-    n2v_emb = np.array(n2v_emb)
-    print(n2v_emb)
-
-    tsne = TSNE(n_components=2, verbose=1, random_state=0)
-    z = tsne.fit_transform(n2v_emb)
-    z_data = np.vstack((z.T, labels)).T
-    df_tsne = pd.DataFrame(z_data, columns=["Dim1", "Dim2", "class"])
-    df_tsne["class"] = df_tsne["class"].astype(int)
-    plt.figure(figsize=(8, 8))
-    sns.scatterplot(
-        data=df_tsne,
-        hue="class",
-        x="Dim1",
-        y="Dim2",
-        palette=["green", "orange", "brown", "red", "blue", "black"],
-    )
-
-    plt.savefig("figs/n2v_citeseer.pdf", bbox_inches="tight")
-    plt.savefig("figs/n2v_citeseer.png", bbox_inches="tight")
-    plt.show()
-
-    print("Graph embedding via LINE........")
-
-    model = LINE(
-        dimension=128,
-        walk_length=80,
-        walk_num=10,
-        negative=5,
-        batch_size=128,
-        init_alpha=0.025,
-        order=2,
-    )
-
-    model.train()
-    line_emb = model(g, return_dict=True)
-
-    l_emb = []
-    for i in range(0, len(line_emb)):
-        l_emb.append(list(line_emb[i]))
-    #   print(len(l_emb))
-    l_emb = np.array(l_emb)
-    print(l_emb)
-
-    tsne = TSNE(n_components=2, verbose=1, random_state=0)
-    z = tsne.fit_transform(l_emb)
-    z_data = np.vstack((z.T, labels)).T
-    df_tsne = pd.DataFrame(z_data, columns=["Dim1", "Dim2", "class"])
-    df_tsne["class"] = df_tsne["class"].astype(int)
-    plt.figure(figsize=(8, 8))
-    sns.scatterplot(
-        data=df_tsne,
-        hue="class",
-        x="Dim1",
-        y="Dim2",
-        palette=["green", "orange", "brown", "red", "blue", "black"],
-    )
-
-    plt.savefig("figs/line_citeseer.pdf", bbox_inches="tight")
-    plt.savefig("figs/line_citeseer.png", bbox_inches="tight")
-    plt.show()
-
-    print("Graph embedding via SDNE...........")
-    model = eg.SDNE(
-        g,
-        node_size=len(g.nodes),
-        nhid0=256,
-        nhid1=32,
-        dropout=0.025,
-        alpha=5e-4,
-        beta=10,
-    )
-    sdne_emb = model.train(model)
-
-    sd_emb = []
-    for i in range(0, len(sdne_emb)):
-        sd_emb.append(list(sdne_emb[i]))
-    #   print(len(sd_emb))
-    sd_emb = np.array(sd_emb)
-    print(sd_emb)
-
-    tsne = TSNE(n_components=2, verbose=1, random_state=0)
-    z = tsne.fit_transform(sd_emb)
-    z_data = np.vstack((z.T, labels)).T
-    df_tsne = pd.DataFrame(z_data, columns=["Dim1", "Dim2", "class"])
-    df_tsne["class"] = df_tsne["class"].astype(int)
-    plt.figure(figsize=(8, 8))
-    sns.scatterplot(
-        data=df_tsne,
-        hue="class",
-        x="Dim1",
-        y="Dim2",
-        palette=["green", "orange", "brown", "red", "blue", "black"],
-    )
-
-    plt.savefig("figs/sdne_citeseer2.pdf", bbox_inches="tight")
-    plt.savefig("figs/sdne_citeseer2.png", bbox_inches="tight")
-    plt.show()
+from __future__ import print_function
+
+import argparse
+import csv
+import time
+import warnings
+
+from datetime import datetime
+
+import easygraph as eg
+import matplotlib.pyplot as plt
+import numpy as np
+import pandas as pd
+import seaborn as sns
+import torch
+
+from easygraph.datasets.citation_graph import CiteseerGraphDataset
+from easygraph.functions.community import greedy_modularity_communities
+from easygraph.functions.community import modularity
+from easygraph.functions.graph_embedding import *
+from mpl_toolkits.mplot3d import Axes3D
+from sklearn.decomposition import PCA
+from sklearn.manifold import TSNE
+
+
+warnings.filterwarnings("ignore")
+
+
+if __name__ == "__main__":
+    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+    dataset = CiteseerGraphDataset(
+        force_reload=True
+    )  # Download CiteseerGraphDataset contained in EasyGraph
+    num_classes = dataset.num_classes
+    g = dataset[0]
+    labels = g.ndata["label"]
+    print(labels, labels.shape, len(g.nodes))
+
+    print("Graph embedding via DeepWalk...........")
+    deepwalk_emb, _ = deepwalk(g, dimensions=128, walk_length=80, num_walks=10)
+    # print(deepwalk_emb, len(deepwalk_emb))
+
+    dw_emb = []
+    for i in range(0, len(deepwalk_emb)):
+        dw_emb.append(list(deepwalk_emb[i]))
+    #   print(len(dw_emb))
+    dw_emb = np.array(dw_emb)
+    print(dw_emb)
+
+    tsne = TSNE(n_components=2, verbose=1, random_state=0)
+    z = tsne.fit_transform(dw_emb)
+    z_data = np.vstack((z.T, labels)).T
+    df_tsne = pd.DataFrame(z_data, columns=["Dim1", "Dim2", "class"])
+    df_tsne["class"] = df_tsne["class"].astype(int)
+    plt.figure(figsize=(8, 8))
+    sns.scatterplot(
+        data=df_tsne,
+        hue="class",
+        x="Dim1",
+        y="Dim2",
+        palette=["green", "orange", "brown", "red", "blue", "black"],
+    )
+    plt.savefig(
+        "figs/dw_citeseer.pdf", bbox_inches="tight"
+    )  # save embeddings if needed
+    plt.savefig("figs/dw_citeseer.png", bbox_inches="tight")
+    plt.show()
+
+    print("Graph embedding via Node2Vec..............")
+    node2vec_emb, _ = node2vec(
+        g, dimensions=128, walk_length=80, num_walks=10, p=4, q=0.25
+    )
+    # print(node2vec_emb, len(node2vec_emb))
+
+    n2v_emb = []
+    for i in range(0, len(node2vec_emb)):
+        n2v_emb.append(list(node2vec_emb[i]))
+    # print(len(n2v_emb))
+    n2v_emb = np.array(n2v_emb)
+    print(n2v_emb)
+
+    tsne = TSNE(n_components=2, verbose=1, random_state=0)
+    z = tsne.fit_transform(n2v_emb)
+    z_data = np.vstack((z.T, labels)).T
+    df_tsne = pd.DataFrame(z_data, columns=["Dim1", "Dim2", "class"])
+    df_tsne["class"] = df_tsne["class"].astype(int)
+    plt.figure(figsize=(8, 8))
+    sns.scatterplot(
+        data=df_tsne,
+        hue="class",
+        x="Dim1",
+        y="Dim2",
+        palette=["green", "orange", "brown", "red", "blue", "black"],
+    )
+
+    plt.savefig("figs/n2v_citeseer.pdf", bbox_inches="tight")
+    plt.savefig("figs/n2v_citeseer.png", bbox_inches="tight")
+    plt.show()
+
+    print("Graph embedding via LINE........")
+
+    model = LINE(
+        dimension=128,
+        walk_length=80,
+        walk_num=10,
+        negative=5,
+        batch_size=128,
+        init_alpha=0.025,
+        order=2,
+    )
+
+    model.train()
+    line_emb = model(g, return_dict=True)
+
+    l_emb = []
+    for i in range(0, len(line_emb)):
+        l_emb.append(list(line_emb[i]))
+    #   print(len(l_emb))
+    l_emb = np.array(l_emb)
+    print(l_emb)
+
+    tsne = TSNE(n_components=2, verbose=1, random_state=0)
+    z = tsne.fit_transform(l_emb)
+    z_data = np.vstack((z.T, labels)).T
+    df_tsne = pd.DataFrame(z_data, columns=["Dim1", "Dim2", "class"])
+    df_tsne["class"] = df_tsne["class"].astype(int)
+    plt.figure(figsize=(8, 8))
+    sns.scatterplot(
+        data=df_tsne,
+        hue="class",
+        x="Dim1",
+        y="Dim2",
+        palette=["green", "orange", "brown", "red", "blue", "black"],
+    )
+
+    plt.savefig("figs/line_citeseer.pdf", bbox_inches="tight")
+    plt.savefig("figs/line_citeseer.png", bbox_inches="tight")
+    plt.show()
+
+    print("Graph embedding via SDNE...........")
+    model = eg.SDNE(
+        g,
+        node_size=len(g.nodes),
+        nhid0=256,
+        nhid1=32,
+        dropout=0.025,
+        alpha=5e-4,
+        beta=10,
+    )
+    sdne_emb = model.train(model)
+
+    sd_emb = []
+    for i in range(0, len(sdne_emb)):
+        sd_emb.append(list(sdne_emb[i]))
+    #   print(len(sd_emb))
+    sd_emb = np.array(sd_emb)
+    print(sd_emb)
+
+    tsne = TSNE(n_components=2, verbose=1, random_state=0)
+    z = tsne.fit_transform(sd_emb)
+    z_data = np.vstack((z.T, labels)).T
+    df_tsne = pd.DataFrame(z_data, columns=["Dim1", "Dim2", "class"])
+    df_tsne["class"] = df_tsne["class"].astype(int)
+    plt.figure(figsize=(8, 8))
+    sns.scatterplot(
+        data=df_tsne,
+        hue="class",
+        x="Dim1",
+        y="Dim2",
+        palette=["green", "orange", "brown", "red", "blue", "black"],
+    )
+
+    plt.savefig("figs/sdne_citeseer2.pdf", bbox_inches="tight")
+    plt.savefig("figs/sdne_citeseer2.png", bbox_inches="tight")
+    plt.show()
```

## easygraph/functions/hypergraph/hypergraph_operation.py

 * *Ordering differences only*

```diff
@@ -1,70 +1,70 @@
-from easygraph.exception import EasyGraphError
-
-
-__all__ = [
-    "hypergraph_density",
-]
-
-
-def hypergraph_density(hg, ignore_singletons=False):
-    r"""Hypergraph density.
-
-    The density of a hypergraph is the number of existing edges divided by the number of
-    possible edges.
-
-    Let `H` have :math:`n` nodes and :math:`m` hyperedges. Then,
-
-    * `density(H) =` :math:`\frac{m}{2^n - 1}`,
-    * `density(H, ignore_singletons=True) =` :math:`\frac{m}{2^n - 1 - n}`.
-
-    Here, :math:`2^n` is the total possible number of hyperedges on `H`, from which we
-    subtract :math:`1` because the empty hyperedge is not considered.  We subtract an
-    additional :math:`n` when singletons are not considered.
-
-    Now assume `H` has :math:`a` edges with order :math:`1` and :math:`b` edges with
-    order :math:`2`.  Then,
-
-    * `density(H, order=1) =` :math:`\frac{a}{{n \choose 2}}`,
-    * `density(H, order=2) =` :math:`\frac{b}{{n \choose 3}}`,
-    * `density(H, max_order=1) =` :math:`\frac{a}{{n \choose 1} + {n \choose 2}}`,
-    * `density(H, max_order=1, ignore_singletons=True) =` :math:`\frac{a}{{n \choose 2}}`,
-    * `density(H, max_order=2) =` :math:`\frac{m}{{n \choose 1} + {n \choose 2} + {n \choose 3}}`,
-    * `density(H, max_order=2, ignore_singletons=True) =` :math:`\frac{m}{{n \choose 2} + {n \choose 3}}`,
-
-    Parameters
-    ---------
-    order : int, optional
-        If not None, only count edges of the specified order.
-        By default, None.
-
-    max_order : int, optional
-        If not None, only count edges of order up to this value, inclusive.
-        By default, None.
-
-    ignore_singletons : bool, optional
-        Whether to consider singleton edges.  Ignored if `order` is not None and
-        different from :math:`0`. By default, False.
-
-    See Also
-    --------
-    :func:`incidence_density`
-
-    Notes
-    -----
-    If both `order` and `max_order` are not None, `max_order` is ignored.
-
-    """
-    n = hg.num_v
-    numer = len(hg.e[0])
-    if n < 1:
-        raise EasyGraphError("Density not defined for empty hypergraph")
-    if numer < 1:
-        return 0.0
-
-    denom = 2**n - 1
-    if ignore_singletons:
-        denom -= n
-    try:
-        return numer / float(denom)
-    except ZeroDivisionError:
-        return 0.0
+from easygraph.exception import EasyGraphError
+
+
+__all__ = [
+    "hypergraph_density",
+]
+
+
+def hypergraph_density(hg, ignore_singletons=False):
+    r"""Hypergraph density.
+
+    The density of a hypergraph is the number of existing edges divided by the number of
+    possible edges.
+
+    Let `H` have :math:`n` nodes and :math:`m` hyperedges. Then,
+
+    * `density(H) =` :math:`\frac{m}{2^n - 1}`,
+    * `density(H, ignore_singletons=True) =` :math:`\frac{m}{2^n - 1 - n}`.
+
+    Here, :math:`2^n` is the total possible number of hyperedges on `H`, from which we
+    subtract :math:`1` because the empty hyperedge is not considered.  We subtract an
+    additional :math:`n` when singletons are not considered.
+
+    Now assume `H` has :math:`a` edges with order :math:`1` and :math:`b` edges with
+    order :math:`2`.  Then,
+
+    * `density(H, order=1) =` :math:`\frac{a}{{n \choose 2}}`,
+    * `density(H, order=2) =` :math:`\frac{b}{{n \choose 3}}`,
+    * `density(H, max_order=1) =` :math:`\frac{a}{{n \choose 1} + {n \choose 2}}`,
+    * `density(H, max_order=1, ignore_singletons=True) =` :math:`\frac{a}{{n \choose 2}}`,
+    * `density(H, max_order=2) =` :math:`\frac{m}{{n \choose 1} + {n \choose 2} + {n \choose 3}}`,
+    * `density(H, max_order=2, ignore_singletons=True) =` :math:`\frac{m}{{n \choose 2} + {n \choose 3}}`,
+
+    Parameters
+    ---------
+    order : int, optional
+        If not None, only count edges of the specified order.
+        By default, None.
+
+    max_order : int, optional
+        If not None, only count edges of order up to this value, inclusive.
+        By default, None.
+
+    ignore_singletons : bool, optional
+        Whether to consider singleton edges.  Ignored if `order` is not None and
+        different from :math:`0`. By default, False.
+
+    See Also
+    --------
+    :func:`incidence_density`
+
+    Notes
+    -----
+    If both `order` and `max_order` are not None, `max_order` is ignored.
+
+    """
+    n = hg.num_v
+    numer = len(hg.e[0])
+    if n < 1:
+        raise EasyGraphError("Density not defined for empty hypergraph")
+    if numer < 1:
+        return 0.0
+
+    denom = 2**n - 1
+    if ignore_singletons:
+        denom -= n
+    try:
+        return numer / float(denom)
+    except ZeroDivisionError:
+        return 0.0
```

## easygraph/functions/hypergraph/hypergraph_clustering.py

 * *Ordering differences only*

```diff
@@ -1,295 +1,295 @@
-"""Algorithms for computing nodal clustering coefficients."""
-
-import numpy as np
-
-from easygraph.utils.exception import EasyGraphError
-
-
-__all__ = [
-    "hypergraph_clustering_coefficient",
-    "hypergraph_local_clustering_coefficient",
-    "hypergraph_two_node_clustering_coefficient",
-]
-
-
-def hypergraph_clustering_coefficient(H):
-    r"""Return the clustering coefficients for
-    each node in a Hypergraph.
-
-    This clustering coefficient is defined as the
-    clustering coefficient of the unweighted pairwise
-    projection of the hypergraph, i.e.,
-    :math:`c = A^3_{i,i}/\binom{k}{2},`
-    where :math:`A` is the adjacency matrix of the network
-    and :math:`k` is the pairwise degree of :math:`i`.
-
-    Parameters
-    ----------
-    H : Hypergraph
-        Hypergraph
-
-    Returns
-    -------
-    dict
-        nodes are keys, clustering coefficients are values.
-
-    Notes
-    -----
-    The clustering coefficient is undefined when the number of
-    neighbors is 0 or 1, but we set the clustering coefficient
-    to 0 in these cases. For more discussion, see
-    https://arxiv.org/abs/0802.2512
-
-    See Also
-    --------
-    local_clustering_coefficient
-    two_node_clustering_coefficient
-
-    References
-    ----------
-    "Clustering Coefficients in Protein Interaction Hypernetworks"
-    by Suzanne Gallagher and Debra Goldberg.
-    DOI: 10.1145/2506583.2506635
-
-    Example
-    -------
-    >>> import easygraph as eg
-    >>> H = eg.random_hypergraph(3, [1, 1])
-    >>> cc = eg.clustering_coefficient(H)
-    >>> cc
-    {0: 1.0, 1: 1.0, 2: 1.0}
-    """
-    adj = H.adjacency_matrix()
-    k = np.array(adj.sum(axis=1))
-    l = []
-    for i in k:
-        l.append(i[0])
-    k = np.array(l)
-    denom = k * (k - 1) / 2
-    mat = adj.dot(adj).dot(adj)
-    with np.errstate(divide="ignore", invalid="ignore"):
-        result = np.nan_to_num(0.5 * mat.diagonal() / denom)
-    r = {}
-    for i in range(0, len(H.v)):
-        r[i] = result[i]
-    return r
-
-
-def hypergraph_local_clustering_coefficient(H):
-    """Compute the local clustering coefficient.
-
-    This clustering coefficient is based on the
-    overlap of the edges connected to a given node,
-    normalized by the size of the node's neighborhood.
-
-    Parameters
-    ----------
-    H : Hypergraph
-        Hypergraph
-
-    Returns
-    -------
-    dict
-        keys are node IDs and values are the
-        clustering coefficients.
-
-    Notes
-    -----
-    The clustering coefficient is undefined when the number of
-    neighbors is 0 or 1, but we set the clustering coefficient
-    to 0 in these cases. For more discussion, see
-    https://arxiv.org/abs/0802.2512
-
-    See Also
-    --------
-    clustering_coefficient
-    two_node_clustering_coefficient
-
-    References
-    ----------
-    "Properties of metabolic graphs: biological organization or representation
-    artifacts?"  by Wanding Zhou and Luay Nakhleh.
-    https://doi.org/10.1186/1471-2105-12-132
-
-    "Hypergraphs for predicting essential genes using multiprotein complex data"
-    by Florian Klimm, Charlotte M. Deane, and Gesine Reinert.
-    https://doi.org/10.1093/comnet/cnaa028
-
-    Example
-    -------
-    >>> import easygraph as eg
-    >>> H = eg.random_hypergraph(3, [1, 1])
-    >>> cc = eg.local_clustering_coefficient(H)
-    >>> cc
-    {0: 1.0, 1: 1.0, 2: 1.0}
-
-    """
-    result = {}
-    # 节点属于哪些边
-    memberships = []
-    for n in H.v:
-        tmp = set()
-        for index, e in enumerate(H.e[0]):
-            if n in e:
-                tmp.add(index)
-        memberships.append(tmp)
-
-    # 每条边包含哪些节点
-    members = H.e[0]
-    for n in H.v:
-        ev = memberships[n]
-        dv = len(ev)
-        if dv <= 1:
-            result[n] = 0
-        else:
-            total_eo = 0
-            # go over all pairs of edges pairwise
-            for e1 in range(dv):
-                edge1 = members[e1]
-                for e2 in range(e1):
-                    edge2 = members[e2]
-                    # set differences for the hyperedges
-                    D1 = set(edge1) - set(edge2)
-                    D2 = set(edge2) - set(edge1)
-                    # if edges are the same by definition the extra overlap is zero
-                    if len(D1.union(D2)) == 0:
-                        eo = 0
-                    else:
-                        # otherwise we have to look at their neighbors
-                        # the neighbors of D1 and D2, respectively.
-                        neighD1 = {i for d in D1 for i in H.neighbor_of_node(d)}
-                        neighD2 = {i for d in D2 for i in H.neighbor_of_node(d)}
-                        # compute extra overlap [len() is used for cardinality of edges]
-                        eo = (
-                            len(neighD1.intersection(D2))
-                            + len(neighD2.intersection(D1))
-                        ) / len(
-                            D1.union(D2)
-                        )  # add it up
-                    # add it up
-                    total_eo = total_eo + eo
-
-            # include normalization by degree k*(k-1)/2
-            result[n] = 2 * total_eo / (dv * (dv - 1))
-    return result
-
-
-def hypergraph_two_node_clustering_coefficient(H, kind="union"):
-    """Return the clustering coefficients for
-    each node in a Hypergraph.
-
-    This definition averages over all of the
-    two-node clustering coefficients involving the node.
-
-    Parameters
-    ----------
-    H : Hypergraph
-        Hypergraph
-    kind : string, optional
-        The type of two node clustering coefficient. Options
-        are "union", "max", and "min". By default, "union".
-
-    Returns
-    -------
-    dict
-        nodes are keys, clustering coefficients are values.
-
-    Notes
-    -----
-    The clustering coefficient is undefined when the number of
-    neighbors is 0 or 1, but we set the clustering coefficient
-    to 0 in these cases. For more discussion, see
-    https://arxiv.org/abs/0802.2512
-
-    See Also
-    --------
-    clustering_coefficient
-    local_clustering_coefficient
-
-    References
-    ----------
-    "Clustering Coefficients in Protein Interaction Hypernetworks"
-    by Suzanne Gallagher and Debra Goldberg.
-    DOI: 10.1145/2506583.2506635
-
-    Example
-    -------
-    >>> import easygraph as eg
-    >>> H = eg.random_hypergraph(3, [1, 1])
-    >>> cc = eg.two_node_clustering_coefficient(H, kind="union")
-    >>> cc
-    {0: 0.5, 1: 0.5, 2: 0.5}
-    """
-    result = {}
-    memberships = {}
-    for n in H.v:
-        tmp = set()
-        for index, e in enumerate(H.e[0]):
-            if n in e:
-                tmp.add(index)
-        memberships[n] = tmp
-
-    for n in H.v:
-        neighbors = H.neighbor_of_node(n)
-        result[n] = 0.0
-        for v in neighbors:
-            result[n] += _uv_cc(n, v, memberships, kind=kind) / len(neighbors)
-    return result
-
-
-def _uv_cc(u, v, memberships, kind="union"):
-    """Helper function to compute the two-node
-    clustering coefficient.
-
-    Parameters
-    ----------
-    u : hashable
-        First node
-    v : hashable
-        Second node
-    memberships : dict
-        node IDs are keys, edge IDs to which they belong
-        are values.
-    kind : str, optional
-        Type of clustering coefficient to compute, by default "union".
-        Options:
-
-        - "union"
-        - "max"
-        - "min"
-
-    Returns
-    -------
-    float
-        The clustering coefficient
-
-    Raises
-    ------
-    EasyGraphError
-        If an invalid clustering coefficient kind
-        is specified.
-
-    References
-    ----------
-    "Clustering Coefficients in Protein Interaction Hypernetworks"
-    by Suzanne Gallagher and Debra Goldberg.
-    DOI: 10.1145/2506583.2506635
-    """
-    m_u = memberships[u]
-    m_v = memberships[v]
-
-    num = len(m_u.intersection(m_v))
-
-    if kind == "union":
-        denom = len(m_u.union(m_v))
-    elif kind == "min":
-        denom = min(len(m_u), len(m_v))
-    elif kind == "max":
-        denom = max(len(m_u), len(m_v))
-    else:
-        raise EasyGraphError("Invalid kind of clustering.")
-
-    if denom == 0:
-        return np.nan
-
-    return num / denom
+"""Algorithms for computing nodal clustering coefficients."""
+
+import numpy as np
+
+from easygraph.utils.exception import EasyGraphError
+
+
+__all__ = [
+    "hypergraph_clustering_coefficient",
+    "hypergraph_local_clustering_coefficient",
+    "hypergraph_two_node_clustering_coefficient",
+]
+
+
+def hypergraph_clustering_coefficient(H):
+    r"""Return the clustering coefficients for
+    each node in a Hypergraph.
+
+    This clustering coefficient is defined as the
+    clustering coefficient of the unweighted pairwise
+    projection of the hypergraph, i.e.,
+    :math:`c = A^3_{i,i}/\binom{k}{2},`
+    where :math:`A` is the adjacency matrix of the network
+    and :math:`k` is the pairwise degree of :math:`i`.
+
+    Parameters
+    ----------
+    H : Hypergraph
+        Hypergraph
+
+    Returns
+    -------
+    dict
+        nodes are keys, clustering coefficients are values.
+
+    Notes
+    -----
+    The clustering coefficient is undefined when the number of
+    neighbors is 0 or 1, but we set the clustering coefficient
+    to 0 in these cases. For more discussion, see
+    https://arxiv.org/abs/0802.2512
+
+    See Also
+    --------
+    local_clustering_coefficient
+    two_node_clustering_coefficient
+
+    References
+    ----------
+    "Clustering Coefficients in Protein Interaction Hypernetworks"
+    by Suzanne Gallagher and Debra Goldberg.
+    DOI: 10.1145/2506583.2506635
+
+    Example
+    -------
+    >>> import easygraph as eg
+    >>> H = eg.random_hypergraph(3, [1, 1])
+    >>> cc = eg.clustering_coefficient(H)
+    >>> cc
+    {0: 1.0, 1: 1.0, 2: 1.0}
+    """
+    adj = H.adjacency_matrix()
+    k = np.array(adj.sum(axis=1))
+    l = []
+    for i in k:
+        l.append(i[0])
+    k = np.array(l)
+    denom = k * (k - 1) / 2
+    mat = adj.dot(adj).dot(adj)
+    with np.errstate(divide="ignore", invalid="ignore"):
+        result = np.nan_to_num(0.5 * mat.diagonal() / denom)
+    r = {}
+    for i in range(0, len(H.v)):
+        r[i] = result[i]
+    return r
+
+
+def hypergraph_local_clustering_coefficient(H):
+    """Compute the local clustering coefficient.
+
+    This clustering coefficient is based on the
+    overlap of the edges connected to a given node,
+    normalized by the size of the node's neighborhood.
+
+    Parameters
+    ----------
+    H : Hypergraph
+        Hypergraph
+
+    Returns
+    -------
+    dict
+        keys are node IDs and values are the
+        clustering coefficients.
+
+    Notes
+    -----
+    The clustering coefficient is undefined when the number of
+    neighbors is 0 or 1, but we set the clustering coefficient
+    to 0 in these cases. For more discussion, see
+    https://arxiv.org/abs/0802.2512
+
+    See Also
+    --------
+    clustering_coefficient
+    two_node_clustering_coefficient
+
+    References
+    ----------
+    "Properties of metabolic graphs: biological organization or representation
+    artifacts?"  by Wanding Zhou and Luay Nakhleh.
+    https://doi.org/10.1186/1471-2105-12-132
+
+    "Hypergraphs for predicting essential genes using multiprotein complex data"
+    by Florian Klimm, Charlotte M. Deane, and Gesine Reinert.
+    https://doi.org/10.1093/comnet/cnaa028
+
+    Example
+    -------
+    >>> import easygraph as eg
+    >>> H = eg.random_hypergraph(3, [1, 1])
+    >>> cc = eg.local_clustering_coefficient(H)
+    >>> cc
+    {0: 1.0, 1: 1.0, 2: 1.0}
+
+    """
+    result = {}
+    # 节点属于哪些边
+    memberships = []
+    for n in H.v:
+        tmp = set()
+        for index, e in enumerate(H.e[0]):
+            if n in e:
+                tmp.add(index)
+        memberships.append(tmp)
+
+    # 每条边包含哪些节点
+    members = H.e[0]
+    for n in H.v:
+        ev = memberships[n]
+        dv = len(ev)
+        if dv <= 1:
+            result[n] = 0
+        else:
+            total_eo = 0
+            # go over all pairs of edges pairwise
+            for e1 in range(dv):
+                edge1 = members[e1]
+                for e2 in range(e1):
+                    edge2 = members[e2]
+                    # set differences for the hyperedges
+                    D1 = set(edge1) - set(edge2)
+                    D2 = set(edge2) - set(edge1)
+                    # if edges are the same by definition the extra overlap is zero
+                    if len(D1.union(D2)) == 0:
+                        eo = 0
+                    else:
+                        # otherwise we have to look at their neighbors
+                        # the neighbors of D1 and D2, respectively.
+                        neighD1 = {i for d in D1 for i in H.neighbor_of_node(d)}
+                        neighD2 = {i for d in D2 for i in H.neighbor_of_node(d)}
+                        # compute extra overlap [len() is used for cardinality of edges]
+                        eo = (
+                            len(neighD1.intersection(D2))
+                            + len(neighD2.intersection(D1))
+                        ) / len(
+                            D1.union(D2)
+                        )  # add it up
+                    # add it up
+                    total_eo = total_eo + eo
+
+            # include normalization by degree k*(k-1)/2
+            result[n] = 2 * total_eo / (dv * (dv - 1))
+    return result
+
+
+def hypergraph_two_node_clustering_coefficient(H, kind="union"):
+    """Return the clustering coefficients for
+    each node in a Hypergraph.
+
+    This definition averages over all of the
+    two-node clustering coefficients involving the node.
+
+    Parameters
+    ----------
+    H : Hypergraph
+        Hypergraph
+    kind : string, optional
+        The type of two node clustering coefficient. Options
+        are "union", "max", and "min". By default, "union".
+
+    Returns
+    -------
+    dict
+        nodes are keys, clustering coefficients are values.
+
+    Notes
+    -----
+    The clustering coefficient is undefined when the number of
+    neighbors is 0 or 1, but we set the clustering coefficient
+    to 0 in these cases. For more discussion, see
+    https://arxiv.org/abs/0802.2512
+
+    See Also
+    --------
+    clustering_coefficient
+    local_clustering_coefficient
+
+    References
+    ----------
+    "Clustering Coefficients in Protein Interaction Hypernetworks"
+    by Suzanne Gallagher and Debra Goldberg.
+    DOI: 10.1145/2506583.2506635
+
+    Example
+    -------
+    >>> import easygraph as eg
+    >>> H = eg.random_hypergraph(3, [1, 1])
+    >>> cc = eg.two_node_clustering_coefficient(H, kind="union")
+    >>> cc
+    {0: 0.5, 1: 0.5, 2: 0.5}
+    """
+    result = {}
+    memberships = {}
+    for n in H.v:
+        tmp = set()
+        for index, e in enumerate(H.e[0]):
+            if n in e:
+                tmp.add(index)
+        memberships[n] = tmp
+
+    for n in H.v:
+        neighbors = H.neighbor_of_node(n)
+        result[n] = 0.0
+        for v in neighbors:
+            result[n] += _uv_cc(n, v, memberships, kind=kind) / len(neighbors)
+    return result
+
+
+def _uv_cc(u, v, memberships, kind="union"):
+    """Helper function to compute the two-node
+    clustering coefficient.
+
+    Parameters
+    ----------
+    u : hashable
+        First node
+    v : hashable
+        Second node
+    memberships : dict
+        node IDs are keys, edge IDs to which they belong
+        are values.
+    kind : str, optional
+        Type of clustering coefficient to compute, by default "union".
+        Options:
+
+        - "union"
+        - "max"
+        - "min"
+
+    Returns
+    -------
+    float
+        The clustering coefficient
+
+    Raises
+    ------
+    EasyGraphError
+        If an invalid clustering coefficient kind
+        is specified.
+
+    References
+    ----------
+    "Clustering Coefficients in Protein Interaction Hypernetworks"
+    by Suzanne Gallagher and Debra Goldberg.
+    DOI: 10.1145/2506583.2506635
+    """
+    m_u = memberships[u]
+    m_v = memberships[v]
+
+    num = len(m_u.intersection(m_v))
+
+    if kind == "union":
+        denom = len(m_u.union(m_v))
+    elif kind == "min":
+        denom = min(len(m_u), len(m_v))
+    elif kind == "max":
+        denom = max(len(m_u), len(m_v))
+    else:
+        raise EasyGraphError("Invalid kind of clustering.")
+
+    if denom == 0:
+        return np.nan
+
+    return num / denom
```

## easygraph/functions/hypergraph/assortativity.py

 * *Ordering differences only*

```diff
@@ -1,185 +1,185 @@
-"""Algorithms for finding the degree assortativity of a hypergraph."""
-
-import random
-
-from itertools import combinations
-
-import numpy
-import numpy as np
-
-from easygraph.utils.exception import EasyGraphError
-
-
-__all__ = ["dynamical_assortativity", "degree_assortativity"]
-
-
-def dynamical_assortativity(H):
-    """Computes the dynamical assortativity of a uniform hypergraph.
-
-    Parameters
-    ----------
-    H : eg.Hypergraph
-        Hypergraph of interest
-
-    Returns
-    -------
-    float
-        The dynamical assortativity
-
-    See Also
-    --------
-    degree_assortativity
-
-    Raises
-    ------
-    EasyGraphError
-        If the hypergraph is not uniform, or if there are no nodes
-        or no edges
-
-    References
-    ----------
-    Nicholas Landry and Juan G. Restrepo,
-    Hypergraph assortativity: A dynamical systems perspective,
-    Chaos 2022.
-    DOI: 10.1063/5.0086905
-
-    """
-    if len(H.v) == 0:
-        raise EasyGraphError("Hypergraph must contain nodes")
-    elif len(H.e[0]) == 0:
-        raise EasyGraphError("Hypergraph must contain edges!")
-
-    if not H.is_uniform():
-        raise EasyGraphError("Hypergraph must be uniform!")
-
-    if 1 in H.unique_edge_sizes():
-        raise EasyGraphError("No singleton edges!")
-
-    degs = H.deg_v
-    k1 = sum(degs) / len(degs)
-    k2 = np.mean(numpy.array(degs) ** 2)
-    kk1 = np.mean(
-        [degs[n1] * degs[n2] for e in H.e[0] for n1, n2 in combinations(e, 2)]
-    )
-
-    return kk1 * k1**2 / k2**2 - 1
-
-
-def degree_assortativity(H, kind="uniform", exact=False, num_samples=1000):
-    """Computes the degree assortativity of a hypergraph
-
-    Parameters
-    ----------
-    H : Hypergraph
-        The hypergraph of interest
-    kind : str, optional
-        the type of degree assortativity. valid choices are
-        "uniform", "top-2", and "top-bottom". By default, "uniform".
-    exact : bool, optional
-        whether to compute over all edges or sample randomly from the
-        set of edges. By default, False.
-    num_samples : int, optional
-        if not exact, specify the number of samples for the computation.
-        By default, 1000.
-
-    Returns
-    -------
-    float
-        the degree assortativity
-
-    Raises
-    ------
-    EasyGraphError
-        If there are no nodes or no edges
-
-    See Also
-    --------
-    dynamical_assortativity
-
-    References
-    ----------
-    Phil Chodrow,
-    Configuration models of random hypergraphs,
-    Journal of Complex Networks 2020.
-    DOI: 10.1093/comnet/cnaa018
-    """
-
-    if len(H.v) == 0:
-        raise EasyGraphError("Hypergraph must contain nodes")
-    elif len(H.e[0]) == 0:
-        raise EasyGraphError("Hypergraph must contain edges!")
-
-    degs = H.deg_v
-    if exact:
-        k1k2 = [_choose_degrees(e, degs, kind) for e in H.e[0] if len(e) > 1]
-    else:
-        edges = [e for e in H.e[0] if len(e) > 1]
-        k1k2 = [
-            _choose_degrees(random.choice(H.e[0]), degs, kind)
-            for _ in range(num_samples)
-        ]
-
-    rho = np.corrcoef(np.array(k1k2).T)[0, 1]
-    if np.isnan(rho):
-        return 0
-    return rho
-
-
-def _choose_degrees(e, k, kind="uniform"):
-    """Choose the degrees of two nodes in a hyperedge.
-
-    Parameters
-    ----------
-    e : iterable
-        the members in a hyperedge
-    k : dict
-        the degrees where keys are node IDs and values are degrees
-    kind : str, optional
-        the type of degree assortativity, options are "uniform", "top-2",
-        and "top-bottom". By default, "uniform".
-
-    Returns
-    -------
-    tuple
-        two degrees selected from the edge
-
-    Raises
-    ------
-    EasyGraphError
-        if invalid assortativity function chosen
-
-    See Also
-    --------
-    degree_assortativity
-
-    References
-    ----------
-    Phil Chodrow,
-    Configuration models of random hypergraphs,
-    Journal of Complex Networks 2020.
-    DOI: 10.1093/comnet/cnaa018
-    """
-    e = list(e)
-    if len(e) > 1:
-        if kind == "uniform":
-            i = np.random.randint(len(e))
-            j = i
-            while i == j:
-                j = np.random.randint(len(e))
-            return (k[e[i]], k[e[j]])
-
-        elif kind == "top-2":
-            degs = sorted([k[i] for i in e])[-2:]
-            random.shuffle(degs)
-            return degs
-
-        elif kind == "top-bottom":
-            # this selects the largest and smallest degrees in one line
-            degs = sorted([k[i] for i in e])[:: len(e) - 1]
-            random.shuffle(degs)
-            return degs
-
-        else:
-            raise EasyGraphError("Invalid choice function!")
-    else:
-        raise EasyGraphError("Edge must have more than one member!")
+"""Algorithms for finding the degree assortativity of a hypergraph."""
+
+import random
+
+from itertools import combinations
+
+import numpy
+import numpy as np
+
+from easygraph.utils.exception import EasyGraphError
+
+
+__all__ = ["dynamical_assortativity", "degree_assortativity"]
+
+
+def dynamical_assortativity(H):
+    """Computes the dynamical assortativity of a uniform hypergraph.
+
+    Parameters
+    ----------
+    H : eg.Hypergraph
+        Hypergraph of interest
+
+    Returns
+    -------
+    float
+        The dynamical assortativity
+
+    See Also
+    --------
+    degree_assortativity
+
+    Raises
+    ------
+    EasyGraphError
+        If the hypergraph is not uniform, or if there are no nodes
+        or no edges
+
+    References
+    ----------
+    Nicholas Landry and Juan G. Restrepo,
+    Hypergraph assortativity: A dynamical systems perspective,
+    Chaos 2022.
+    DOI: 10.1063/5.0086905
+
+    """
+    if len(H.v) == 0:
+        raise EasyGraphError("Hypergraph must contain nodes")
+    elif len(H.e[0]) == 0:
+        raise EasyGraphError("Hypergraph must contain edges!")
+
+    if not H.is_uniform():
+        raise EasyGraphError("Hypergraph must be uniform!")
+
+    if 1 in H.unique_edge_sizes():
+        raise EasyGraphError("No singleton edges!")
+
+    degs = H.deg_v
+    k1 = sum(degs) / len(degs)
+    k2 = np.mean(numpy.array(degs) ** 2)
+    kk1 = np.mean(
+        [degs[n1] * degs[n2] for e in H.e[0] for n1, n2 in combinations(e, 2)]
+    )
+
+    return kk1 * k1**2 / k2**2 - 1
+
+
+def degree_assortativity(H, kind="uniform", exact=False, num_samples=1000):
+    """Computes the degree assortativity of a hypergraph
+
+    Parameters
+    ----------
+    H : Hypergraph
+        The hypergraph of interest
+    kind : str, optional
+        the type of degree assortativity. valid choices are
+        "uniform", "top-2", and "top-bottom". By default, "uniform".
+    exact : bool, optional
+        whether to compute over all edges or sample randomly from the
+        set of edges. By default, False.
+    num_samples : int, optional
+        if not exact, specify the number of samples for the computation.
+        By default, 1000.
+
+    Returns
+    -------
+    float
+        the degree assortativity
+
+    Raises
+    ------
+    EasyGraphError
+        If there are no nodes or no edges
+
+    See Also
+    --------
+    dynamical_assortativity
+
+    References
+    ----------
+    Phil Chodrow,
+    Configuration models of random hypergraphs,
+    Journal of Complex Networks 2020.
+    DOI: 10.1093/comnet/cnaa018
+    """
+
+    if len(H.v) == 0:
+        raise EasyGraphError("Hypergraph must contain nodes")
+    elif len(H.e[0]) == 0:
+        raise EasyGraphError("Hypergraph must contain edges!")
+
+    degs = H.deg_v
+    if exact:
+        k1k2 = [_choose_degrees(e, degs, kind) for e in H.e[0] if len(e) > 1]
+    else:
+        edges = [e for e in H.e[0] if len(e) > 1]
+        k1k2 = [
+            _choose_degrees(random.choice(H.e[0]), degs, kind)
+            for _ in range(num_samples)
+        ]
+
+    rho = np.corrcoef(np.array(k1k2).T)[0, 1]
+    if np.isnan(rho):
+        return 0
+    return rho
+
+
+def _choose_degrees(e, k, kind="uniform"):
+    """Choose the degrees of two nodes in a hyperedge.
+
+    Parameters
+    ----------
+    e : iterable
+        the members in a hyperedge
+    k : dict
+        the degrees where keys are node IDs and values are degrees
+    kind : str, optional
+        the type of degree assortativity, options are "uniform", "top-2",
+        and "top-bottom". By default, "uniform".
+
+    Returns
+    -------
+    tuple
+        two degrees selected from the edge
+
+    Raises
+    ------
+    EasyGraphError
+        if invalid assortativity function chosen
+
+    See Also
+    --------
+    degree_assortativity
+
+    References
+    ----------
+    Phil Chodrow,
+    Configuration models of random hypergraphs,
+    Journal of Complex Networks 2020.
+    DOI: 10.1093/comnet/cnaa018
+    """
+    e = list(e)
+    if len(e) > 1:
+        if kind == "uniform":
+            i = np.random.randint(len(e))
+            j = i
+            while i == j:
+                j = np.random.randint(len(e))
+            return (k[e[i]], k[e[j]])
+
+        elif kind == "top-2":
+            degs = sorted([k[i] for i in e])[-2:]
+            random.shuffle(degs)
+            return degs
+
+        elif kind == "top-bottom":
+            # this selects the largest and smallest degrees in one line
+            degs = sorted([k[i] for i in e])[:: len(e) - 1]
+            random.shuffle(degs)
+            return degs
+
+        else:
+            raise EasyGraphError("Invalid choice function!")
+    else:
+        raise EasyGraphError("Edge must have more than one member!")
```

## easygraph/functions/hypergraph/__init__.py

```diff
@@ -1,5 +1,5 @@
-from .assortativity import *
-from .centrality import *
-from .hypergraph_clustering import *
-from .hypergraph_generator import *
-from .hypergraph_operation import *
+from .assortativity import *
+from .centrality import *
+from .hypergraph_clustering import *
+from .hypergraph_operation import *
+from .null_model import *
```

## easygraph/functions/hypergraph/centrality/cycle_ratio.py

 * *Ordering differences only*

```diff
@@ -1,201 +1,201 @@
-import copy
-import itertools
-
-import easygraph as eg
-
-
-__all__ = [
-    "my_all_shortest_paths",
-    "getandJudgeSimpleCircle",
-    "getSmallestCycles",
-    "StatisticsAndCalculateIndicators",
-    "cycle_ratio_centrality",
-]
-
-
-SmallestCycles = set()
-CycleRatio = {}
-
-
-def my_all_shortest_paths(G, source, target):
-    pred = eg.predecessor(G, source)
-    if target not in pred:
-        raise eg.EasyGraphNoPath(
-            f"Target {target} cannot be reached from given sources"
-        )
-    sources = {source}
-    seen = {target}
-    stack = [[target, 0]]
-    top = 0
-    while top >= 0:
-        node, i = stack[top]
-        if node in sources:
-            yield [p for p, n in reversed(stack[: top + 1])]
-        if len(pred[node]) > i:
-            stack[top][1] = i + 1
-            next = pred[node][i]
-            if next in seen:
-                continue
-            else:
-                seen.add(next)
-            top += 1
-            if top == len(stack):
-                stack.append([next, 0])
-            else:
-                stack[top][:] = [next, 0]
-        else:
-            seen.discard(node)
-            top -= 1
-
-
-def getandJudgeSimpleCircle(objectList):  #
-    numEdge = 0
-    for eleArr in list(itertools.combinations(objectList, 2)):
-        if G.has_edge(eleArr[0], eleArr[1]):
-            numEdge += 1
-    if numEdge != len(objectList):
-        return False
-    else:
-        return True
-
-
-def getSmallestCycles(G, NodeGirth, Coreness, DEF_IMPOSSLEN):
-    NodeList = list(G.nodes)
-    # print(NodeList)
-    NodeList.sort()
-    # setp 1
-    curCyc = list()
-    for ix in NodeList[:-2]:  # v1
-        if NodeGirth[ix] == 0:
-            continue
-        curCyc.append(ix)
-        for jx in NodeList[NodeList.index(ix) + 1 : -1]:  # v2
-            if NodeGirth[jx] == 0:
-                continue
-            curCyc.append(jx)
-            if G.has_edge(ix, jx):
-                for kx in NodeList[NodeList.index(jx) + 1 :]:  # v3
-                    if NodeGirth[kx] == 0:
-                        continue
-                    if G.has_edge(kx, ix):
-                        curCyc.append(kx)
-                        if G.has_edge(kx, jx):
-                            SmallestCycles.add(tuple(curCyc))
-                            for i in curCyc:
-                                NodeGirth[i] = 3
-                        curCyc.pop()
-            curCyc.pop()
-        curCyc.pop()
-
-    # setp 2
-    ResiNodeList = []  # Residual Node List
-    for nod in NodeList:
-        if NodeGirth[nod] == DEF_IMPOSSLEN:
-            ResiNodeList.append(nod)
-    if len(ResiNodeList) == 0:
-        return
-    else:
-        visitedNodes = dict.fromkeys(ResiNodeList, set())
-        for nod in ResiNodeList:
-            if Coreness[nod] == 2 and NodeGirth[nod] < DEF_IMPOSSLEN:
-                continue
-            for nei in list(G.neighbors(nod)):
-                if Coreness[nei] == 2 and NodeGirth[nei] < DEF_IMPOSSLEN:
-                    continue
-                if not nei in visitedNodes.keys() or not nod in visitedNodes[nei]:
-                    visitedNodes[nod].add(nei)
-                    if nei not in visitedNodes.keys():
-                        visitedNodes[nei] = set([nod])
-                    else:
-                        visitedNodes[nei].add(nod)
-                    if Coreness[nei] == 2 and NodeGirth[nei] < DEF_IMPOSSLEN:
-                        continue
-                    G.remove_edge(nod, nei)
-                    if eg.single_source_dijkstra(G, nod, nei):
-                        for path in my_all_shortest_paths(G, nod, nei):
-                            lenPath = len(path)
-                            path.sort()
-                            SmallestCycles.add(tuple(path))
-                            for i in path:
-                                if NodeGirth[i] > lenPath:
-                                    NodeGirth[i] = lenPath
-                    G.add_edge(nod, nei)
-
-    return SmallestCycles
-
-
-def StatisticsAndCalculateIndicators(SmallestCyclesOfNodes, CycLenDict):  #
-    global NumSmallCycles
-    NumSmallCycles = len(SmallestCycles)
-    for cyc in SmallestCycles:
-        lenCyc = len(cyc)
-        CycLenDict[lenCyc] += 1
-        for nod in cyc:
-            SmallestCyclesOfNodes[nod].add(cyc)
-    for objNode, SmaCycs in SmallestCyclesOfNodes.items():
-        if len(SmaCycs) == 0:
-            continue
-        cycleNeighbors = set()
-        NeiOccurTimes = {}
-        for cyc in SmaCycs:
-            for n in cyc:
-                if n in NeiOccurTimes.keys():
-                    NeiOccurTimes[n] += 1
-                else:
-                    NeiOccurTimes[n] = 1
-            cycleNeighbors = cycleNeighbors.union(cyc)
-        cycleNeighbors.remove(objNode)
-        del NeiOccurTimes[objNode]
-        sum = 0
-        for nei in cycleNeighbors:
-            sum += float(NeiOccurTimes[nei]) / len(SmallestCyclesOfNodes[nei])
-        CycleRatio[objNode] = sum + 1
-    return CycleRatio
-
-
-def cycle_ratio_centrality(G):
-    """
-    Parameters
-    ----------
-    G :   eg.Graph
-
-    Returns
-    -------
-    cycle ratio centrality of each node in G : dict
-
-    Example
-    -------
-    >>> G = eg.Graph()
-    >>> G.add_edges([(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4), (1, 5), (2, 5)])
-    >>> cycle_ratio_centrality(G)
-    {1: 4.083333333333333, 2: 4.083333333333333, 3: 2.6666666666666665, 4: 2.6666666666666665, 5: 1.5}
-
-    """
-    CycLenDict = dict()
-    NumNode = G.number_of_nodes()  # update
-    DEF_IMPOSSLEN = NumNode + 1  # Impossible simple cycle length
-    NodeGirth = dict()
-    CycLenDict = dict()
-
-    SmallestCyclesOfNodes = {}  #
-    removeNodes = set()
-    Coreness = dict(zip(list(G.nodes), eg.k_core(G)))
-    for i in list(G.nodes):  #
-        SmallestCyclesOfNodes[i] = set()
-        CycleRatio[i] = 0
-        if G.degree()[i] <= 1 or Coreness[i] <= 1:
-            NodeGirth[i] = 0
-            removeNodes.add(i)
-        else:
-            NodeGirth[i] = DEF_IMPOSSLEN
-    # print('NodeGirth:', NodeGirth)
-
-    G.remove_nodes_from(removeNodes)
-
-    NodeNum = G.number_of_nodes()
-    for i in range(3, NodeNum + 2):
-        CycLenDict[i] = 0
-
-    getSmallestCycles(G, NodeGirth, Coreness, DEF_IMPOSSLEN)
-    cycle_ratio = StatisticsAndCalculateIndicators(SmallestCyclesOfNodes, CycLenDict)
-    return cycle_ratio
+import copy
+import itertools
+
+import easygraph as eg
+
+
+__all__ = [
+    "my_all_shortest_paths",
+    "getandJudgeSimpleCircle",
+    "getSmallestCycles",
+    "StatisticsAndCalculateIndicators",
+    "cycle_ratio_centrality",
+]
+
+
+SmallestCycles = set()
+CycleRatio = {}
+
+
+def my_all_shortest_paths(G, source, target):
+    pred = eg.predecessor(G, source)
+    if target not in pred:
+        raise eg.EasyGraphNoPath(
+            f"Target {target} cannot be reached from given sources"
+        )
+    sources = {source}
+    seen = {target}
+    stack = [[target, 0]]
+    top = 0
+    while top >= 0:
+        node, i = stack[top]
+        if node in sources:
+            yield [p for p, n in reversed(stack[: top + 1])]
+        if len(pred[node]) > i:
+            stack[top][1] = i + 1
+            next = pred[node][i]
+            if next in seen:
+                continue
+            else:
+                seen.add(next)
+            top += 1
+            if top == len(stack):
+                stack.append([next, 0])
+            else:
+                stack[top][:] = [next, 0]
+        else:
+            seen.discard(node)
+            top -= 1
+
+
+def getandJudgeSimpleCircle(objectList):  #
+    numEdge = 0
+    for eleArr in list(itertools.combinations(objectList, 2)):
+        if G.has_edge(eleArr[0], eleArr[1]):
+            numEdge += 1
+    if numEdge != len(objectList):
+        return False
+    else:
+        return True
+
+
+def getSmallestCycles(G, NodeGirth, Coreness, DEF_IMPOSSLEN):
+    NodeList = list(G.nodes)
+    # print(NodeList)
+    NodeList.sort()
+    # setp 1
+    curCyc = list()
+    for ix in NodeList[:-2]:  # v1
+        if NodeGirth[ix] == 0:
+            continue
+        curCyc.append(ix)
+        for jx in NodeList[NodeList.index(ix) + 1 : -1]:  # v2
+            if NodeGirth[jx] == 0:
+                continue
+            curCyc.append(jx)
+            if G.has_edge(ix, jx):
+                for kx in NodeList[NodeList.index(jx) + 1 :]:  # v3
+                    if NodeGirth[kx] == 0:
+                        continue
+                    if G.has_edge(kx, ix):
+                        curCyc.append(kx)
+                        if G.has_edge(kx, jx):
+                            SmallestCycles.add(tuple(curCyc))
+                            for i in curCyc:
+                                NodeGirth[i] = 3
+                        curCyc.pop()
+            curCyc.pop()
+        curCyc.pop()
+
+    # setp 2
+    ResiNodeList = []  # Residual Node List
+    for nod in NodeList:
+        if NodeGirth[nod] == DEF_IMPOSSLEN:
+            ResiNodeList.append(nod)
+    if len(ResiNodeList) == 0:
+        return
+    else:
+        visitedNodes = dict.fromkeys(ResiNodeList, set())
+        for nod in ResiNodeList:
+            if Coreness[nod] == 2 and NodeGirth[nod] < DEF_IMPOSSLEN:
+                continue
+            for nei in list(G.neighbors(nod)):
+                if Coreness[nei] == 2 and NodeGirth[nei] < DEF_IMPOSSLEN:
+                    continue
+                if not nei in visitedNodes.keys() or not nod in visitedNodes[nei]:
+                    visitedNodes[nod].add(nei)
+                    if nei not in visitedNodes.keys():
+                        visitedNodes[nei] = set([nod])
+                    else:
+                        visitedNodes[nei].add(nod)
+                    if Coreness[nei] == 2 and NodeGirth[nei] < DEF_IMPOSSLEN:
+                        continue
+                    G.remove_edge(nod, nei)
+                    if eg.single_source_dijkstra(G, nod, nei):
+                        for path in my_all_shortest_paths(G, nod, nei):
+                            lenPath = len(path)
+                            path.sort()
+                            SmallestCycles.add(tuple(path))
+                            for i in path:
+                                if NodeGirth[i] > lenPath:
+                                    NodeGirth[i] = lenPath
+                    G.add_edge(nod, nei)
+
+    return SmallestCycles
+
+
+def StatisticsAndCalculateIndicators(SmallestCyclesOfNodes, CycLenDict):  #
+    global NumSmallCycles
+    NumSmallCycles = len(SmallestCycles)
+    for cyc in SmallestCycles:
+        lenCyc = len(cyc)
+        CycLenDict[lenCyc] += 1
+        for nod in cyc:
+            SmallestCyclesOfNodes[nod].add(cyc)
+    for objNode, SmaCycs in SmallestCyclesOfNodes.items():
+        if len(SmaCycs) == 0:
+            continue
+        cycleNeighbors = set()
+        NeiOccurTimes = {}
+        for cyc in SmaCycs:
+            for n in cyc:
+                if n in NeiOccurTimes.keys():
+                    NeiOccurTimes[n] += 1
+                else:
+                    NeiOccurTimes[n] = 1
+            cycleNeighbors = cycleNeighbors.union(cyc)
+        cycleNeighbors.remove(objNode)
+        del NeiOccurTimes[objNode]
+        sum = 0
+        for nei in cycleNeighbors:
+            sum += float(NeiOccurTimes[nei]) / len(SmallestCyclesOfNodes[nei])
+        CycleRatio[objNode] = sum + 1
+    return CycleRatio
+
+
+def cycle_ratio_centrality(G):
+    """
+    Parameters
+    ----------
+    G :   eg.Graph
+
+    Returns
+    -------
+    cycle ratio centrality of each node in G : dict
+
+    Example
+    -------
+    >>> G = eg.Graph()
+    >>> G.add_edges([(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4), (1, 5), (2, 5)])
+    >>> cycle_ratio_centrality(G)
+    {1: 4.083333333333333, 2: 4.083333333333333, 3: 2.6666666666666665, 4: 2.6666666666666665, 5: 1.5}
+
+    """
+    CycLenDict = dict()
+    NumNode = G.number_of_nodes()  # update
+    DEF_IMPOSSLEN = NumNode + 1  # Impossible simple cycle length
+    NodeGirth = dict()
+    CycLenDict = dict()
+
+    SmallestCyclesOfNodes = {}  #
+    removeNodes = set()
+    Coreness = dict(zip(list(G.nodes), eg.k_core(G)))
+    for i in list(G.nodes):  #
+        SmallestCyclesOfNodes[i] = set()
+        CycleRatio[i] = 0
+        if G.degree()[i] <= 1 or Coreness[i] <= 1:
+            NodeGirth[i] = 0
+            removeNodes.add(i)
+        else:
+            NodeGirth[i] = DEF_IMPOSSLEN
+    # print('NodeGirth:', NodeGirth)
+
+    G.remove_nodes_from(removeNodes)
+
+    NodeNum = G.number_of_nodes()
+    for i in range(3, NodeNum + 2):
+        CycLenDict[i] = 0
+
+    getSmallestCycles(G, NodeGirth, Coreness, DEF_IMPOSSLEN)
+    cycle_ratio = StatisticsAndCalculateIndicators(SmallestCyclesOfNodes, CycLenDict)
+    return cycle_ratio
```

## easygraph/functions/hypergraph/centrality/hypercoreness.py

 * *Ordering differences only*

```diff
@@ -1,351 +1,351 @@
-from itertools import compress
-
-import easygraph as eg
-import numpy as np
-
-
-__all__ = ["size_independent_hypercoreness", "frequency_based_hypercoreness"]
-
-
-def size_independent_hypercoreness(h):
-    """The size_independent_hypercoreness of nodes in hypergraph.
-
-    Parameters
-    ----------
-    h : eg.Hypergraph.
-
-
-    Returns
-    ----------
-    dict
-        Centrality, where keys are node IDs and values are lists of centralities.
-
-    References
-    ----------
-    Mancastroppa, M., Iacopini, I., Petri, G. et al. Hyper-cores promote localization and efficient seeding in higher-order processes. Nat Commun 14, 6223 (2023). https://doi.org/10.1038/s41467-023-41887-2.
-
-    """
-    e_list = h.e[0]
-    initial_node_num = h.num_v
-    data = [e_list[i] for i in range(len(e_list)) if len(e_list[i]) > 1]
-
-    data.sort(key=len)
-    L = len(data)
-    size_max = len(data[L - 1])
-
-    size = list([len(data[j]) for j in range(L)])
-
-    X = eg.Hypergraph(num_v=initial_node_num, e_list=data)
-    IDX = list(range(1, X.num_v))
-
-    M = range(2, size_max + 1)
-    k_step = 1
-    K = range(1, 1200, k_step)
-    k_shell_dict = {}
-    idx_orig = IDX
-
-    IDX_size = range(len(size))
-    k_max = np.zeros(len(M))
-
-    for j in idx_orig:
-        k_shell_dict[j] = np.zeros(len(M))
-
-    for x in range(len(M)):
-        m = M[x]
-
-        D = np.zeros(len(K))
-
-        # consider only hyperedges of size >=m
-        idx_size = list(
-            compress(IDX_size, np.greater_equal(size, m * np.ones(len(size))))
-        )
-        int_sel = list([data[i] for i in idx_size])
-        # build hypergraph with only interactions of size >=m
-        X = eg.Hypergraph(num_v=initial_node_num, e_list=int_sel)
-        node_set = set()
-        for sublist in int_sel:
-            for element in sublist:
-                node_set.add(element)
-        IDX = list(node_set)
-        # IDX_e = list(X.e[0])
-
-        for y in range(len(K)):
-            kk = K[y]
-
-            d_tot_m = np.zeros(len(IDX))
-            prev_shell = IDX
-
-            for i in range(len(IDX)):
-                d_tot_m[i] = X.degree_node[IDX[i]]
-
-            idx_n_remove = list(
-                compress(IDX, np.greater(kk * np.ones(len(d_tot_m)), d_tot_m))
-            )  # nodes with degree<k are removed
-            # X.remove_nodes_from(idx_n_remove)
-            now_e_list = X.e[0]
-            new_e_list = []
-            for e in now_e_list:
-                new_e = []
-                for n in e:
-                    if n not in idx_n_remove:
-                        new_e.append(n)
-                if len(new_e) > 0:
-                    new_e_list.append(new_e)
-
-            X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
-
-            IDX_e = list(range(0, len(X.e[0])))
-
-            sizes = [
-                len(X.e[0][i]) for i in IDX_e
-            ]  # hyperedges with size <m are removed
-            idx_e_remove = [IDX_e[i] for i in range(len(IDX_e)) if sizes[i] < m]
-            now_e_list = X.e[0]
-            new_e_list = []
-            for i in range(len(now_e_list)):
-                if i not in idx_e_remove:
-                    new_e_list.append(now_e_list[i])
-
-            X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
-
-            node_set = set()
-            for sublist in X.e[0]:
-                for element in sublist:
-                    node_set.add(element)
-            IDX = list(node_set)
-
-            while len(idx_n_remove) > 0 or len(idx_e_remove) > 0:
-                d_tot_m = np.zeros(len(IDX))
-
-                for i in range(len(IDX)):
-                    d_tot_m[i] = X.degree_node[IDX[i]]
-
-                idx_n_remove = list(
-                    compress(IDX, np.greater(kk * np.ones(len(d_tot_m)), d_tot_m))
-                )  # nodes with degree<k are removed
-                # X.remove_nodes_from(idx_n_remove)
-                now_e_list = X.e[0]
-                new_e_list = []
-                for e in now_e_list:
-                    new_e = []
-                    for n in e:
-                        if n not in idx_n_remove:
-                            new_e.append(n)
-                    if len(new_e) > 0:
-                        new_e_list.append(new_e)
-                X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
-
-                IDX_e = list(range(len(X.e[0])))
-                sizes = [
-                    len(X.e[0][i]) for i in IDX_e
-                ]  # hyperedges with size <m are removed
-
-                idx_e_remove = [IDX_e[i] for i in range(len(IDX_e)) if sizes[i] < m]
-                now_e_list = X.e[0]
-                new_e_list = []
-                for i in range(len(now_e_list)):
-                    if i not in idx_e_remove:
-                        new_e_list.append(now_e_list[i])
-                X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
-
-                node_set = set()
-                for sublist in X.e[0]:
-                    for element in sublist:
-                        node_set.add(element)
-                IDX = list(node_set)
-
-            shell_kk = list(sorted(set(prev_shell) - set(IDX)))
-            for j in shell_kk:
-                k_shell_dict[j][x] = kk - k_step
-
-            node_set = set()
-            for sublist in X.e[0]:
-                for element in sublist:
-                    node_set.add(element)
-            IDX = list(node_set)
-
-            D[y] = len(node_set)
-            if y > 0:
-                if D[y] == 0 and D[y - 1] != 0:
-                    # maximum connectivity at order m
-                    k_max[x] = kk - k_step
-            # stop the decomposition when the (k,m)-core is empty
-            if D[y] == 0:
-                break
-
-    # size-independent hypercoreness
-    R_dict = {}
-    for y in k_shell_dict:
-        R_dict[y] = sum(np.array(k_shell_dict[y]) / np.array(k_max))
-
-    return R_dict
-
-
-def frequency_based_hypercoreness(h):
-    r"""The frequency-based hypercoreness of nodes in hypergraph.
-
-     Parameters
-     ----------
-     h : easygraph.Hypergraph
-
-
-    Returns
-    -------
-    dict : Centrality, where keys are node IDs and values are lists of centralities.
-
-    References
-    ----------
-    Mancastroppa, M., Iacopini, I., Petri, G. et al. Hyper-cores promote localization and efficient seeding in higher-order processes. Nat Commun 14, 6223 (2023). https://doi.org/10.1038/s41467-023-41887-2
-
-    """
-    e_list = h.e[0]
-    initial_node_num = h.num_v
-    data = [e_list[i] for i in range(len(e_list)) if len(e_list[i]) > 1]
-
-    data.sort(key=len)
-    L = len(data)
-    size_max = len(data[L - 1])
-
-    size = list([len(data[j]) for j in range(L)])
-
-    X = eg.Hypergraph(num_v=initial_node_num, e_list=data)
-    IDX = list(range(1, X.num_v))
-
-    M = range(2, size_max + 1)
-    k_step = 1
-    K = range(1, 1200, k_step)
-    k_shell_dict = {}
-    idx_orig = IDX
-
-    IDX_size = range(len(size))
-    k_max = np.zeros(len(M))
-
-    for j in idx_orig:
-        k_shell_dict[j] = np.zeros(len(M))
-
-    for x in range(len(M)):
-        m = M[x]
-
-        D = np.zeros(len(K))
-        # consider only hyperedges of size >=m
-        idx_size = list(
-            compress(IDX_size, np.greater_equal(size, m * np.ones(len(size))))
-        )
-        int_sel = list([data[i] for i in idx_size])
-        # build hypergraph with only interactions of size >=m
-        X = eg.Hypergraph(num_v=initial_node_num, e_list=int_sel)
-        node_set = set()
-        for sublist in int_sel:
-            for element in sublist:
-                node_set.add(element)
-        IDX = list(node_set)
-
-        for y in range(len(K)):
-            kk = K[y]
-
-            d_tot_m = np.zeros(len(IDX))
-            prev_shell = IDX
-
-            for i in range(len(IDX)):
-                d_tot_m[i] = X.degree_node[IDX[i]]
-
-            idx_n_remove = list(
-                compress(IDX, np.greater(kk * np.ones(len(d_tot_m)), d_tot_m))
-            )  # nodes with degree<k are removed
-            now_e_list = X.e[0]
-            new_e_list = []
-            for e in now_e_list:
-                new_e = []
-                for n in e:
-                    if n not in idx_n_remove:
-                        new_e.append(n)
-                if len(new_e) > 0:
-                    new_e_list.append(new_e)
-
-            X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
-
-            IDX_e = list(range(0, len(X.e[0])))
-
-            # hyperedges with size <m are removed
-            sizes = [len(X.e[0][i]) for i in IDX_e]
-            idx_e_remove = [IDX_e[i] for i in range(len(IDX_e)) if sizes[i] < m]
-            now_e_list = X.e[0]
-            new_e_list = []
-            for i in range(len(now_e_list)):
-                if i not in idx_e_remove:
-                    new_e_list.append(now_e_list[i])
-
-            X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
-
-            node_set = set()
-            for sublist in X.e[0]:
-                for element in sublist:
-                    node_set.add(element)
-            IDX = list(node_set)
-
-            while len(idx_n_remove) > 0 or len(idx_e_remove) > 0:
-                d_tot_m = np.zeros(len(IDX))
-
-                for i in range(len(IDX)):
-                    d_tot_m[i] = X.degree_node[IDX[i]]
-                # nodes with degree<k are removed
-                idx_n_remove = list(
-                    compress(IDX, np.greater(kk * np.ones(len(d_tot_m)), d_tot_m))
-                )
-                now_e_list = X.e[0]
-                new_e_list = []
-                for e in now_e_list:
-                    new_e = []
-                    for n in e:
-                        if n not in idx_n_remove:
-                            new_e.append(n)
-                    if len(new_e) > 0:
-                        new_e_list.append(new_e)
-                X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
-
-                IDX_e = list(range(len(X.e[0])))
-                # hyperedges with size <m are removed
-                sizes = [len(X.e[0][i]) for i in IDX_e]
-
-                idx_e_remove = [IDX_e[i] for i in range(len(IDX_e)) if sizes[i] < m]
-                # print("m:",m)
-                now_e_list = X.e[0]
-                new_e_list = []
-                for i in range(len(now_e_list)):
-                    if i not in idx_e_remove:
-                        new_e_list.append(now_e_list[i])
-                X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
-
-                node_set = set()
-                for sublist in X.e[0]:
-                    for element in sublist:
-                        node_set.add(element)
-                IDX = list(node_set)
-
-            shell_kk = list(sorted(set(prev_shell) - set(IDX)))
-            # print("fun shell_kk:",len(shell_kk))
-            for j in shell_kk:
-                k_shell_dict[j][x] = kk - k_step
-
-            node_set = set()
-            for sublist in X.e[0]:
-                for element in sublist:
-                    node_set.add(element)
-            IDX = list(node_set)
-
-            D[y] = len(node_set)
-            if y > 0:
-                if D[y] == 0 and D[y - 1] != 0:
-                    k_max[x] = kk - k_step  # maximum connectivity at order m
-            if D[y] == 0:
-                break  # stop the decomposition when the (k,m)-core is empty
-
-    # Psi(m)  distribution of hyperedges size
-    Psi = []
-    for m in range(2, size_max + 1):
-        Psi.append(size.count(m) / len(size))
-    # frequency-based hypercoreness
-    R_w_dict = {}
-    for y in k_shell_dict:
-        R_w_dict[y] = sum(np.array(Psi) * np.array(k_shell_dict[y]) / np.array(k_max))
-    return R_w_dict
+from itertools import compress
+
+import easygraph as eg
+import numpy as np
+
+
+__all__ = ["size_independent_hypercoreness", "frequency_based_hypercoreness"]
+
+
+def size_independent_hypercoreness(h):
+    """The size_independent_hypercoreness of nodes in hypergraph.
+
+    Parameters
+    ----------
+    h : eg.Hypergraph.
+
+
+    Returns
+    ----------
+    dict
+        Centrality, where keys are node IDs and values are lists of centralities.
+
+    References
+    ----------
+    Mancastroppa, M., Iacopini, I., Petri, G. et al. Hyper-cores promote localization and efficient seeding in higher-order processes. Nat Commun 14, 6223 (2023). https://doi.org/10.1038/s41467-023-41887-2.
+
+    """
+    e_list = h.e[0]
+    initial_node_num = h.num_v
+    data = [e_list[i] for i in range(len(e_list)) if len(e_list[i]) > 1]
+
+    data.sort(key=len)
+    L = len(data)
+    size_max = len(data[L - 1])
+
+    size = list([len(data[j]) for j in range(L)])
+
+    X = eg.Hypergraph(num_v=initial_node_num, e_list=data)
+    IDX = list(range(1, X.num_v))
+
+    M = range(2, size_max + 1)
+    k_step = 1
+    K = range(1, 1200, k_step)
+    k_shell_dict = {}
+    idx_orig = IDX
+
+    IDX_size = range(len(size))
+    k_max = np.zeros(len(M))
+
+    for j in idx_orig:
+        k_shell_dict[j] = np.zeros(len(M))
+
+    for x in range(len(M)):
+        m = M[x]
+
+        D = np.zeros(len(K))
+
+        # consider only hyperedges of size >=m
+        idx_size = list(
+            compress(IDX_size, np.greater_equal(size, m * np.ones(len(size))))
+        )
+        int_sel = list([data[i] for i in idx_size])
+        # build hypergraph with only interactions of size >=m
+        X = eg.Hypergraph(num_v=initial_node_num, e_list=int_sel)
+        node_set = set()
+        for sublist in int_sel:
+            for element in sublist:
+                node_set.add(element)
+        IDX = list(node_set)
+        # IDX_e = list(X.e[0])
+
+        for y in range(len(K)):
+            kk = K[y]
+
+            d_tot_m = np.zeros(len(IDX))
+            prev_shell = IDX
+
+            for i in range(len(IDX)):
+                d_tot_m[i] = X.degree_node[IDX[i]]
+
+            idx_n_remove = list(
+                compress(IDX, np.greater(kk * np.ones(len(d_tot_m)), d_tot_m))
+            )  # nodes with degree<k are removed
+            # X.remove_nodes_from(idx_n_remove)
+            now_e_list = X.e[0]
+            new_e_list = []
+            for e in now_e_list:
+                new_e = []
+                for n in e:
+                    if n not in idx_n_remove:
+                        new_e.append(n)
+                if len(new_e) > 0:
+                    new_e_list.append(new_e)
+
+            X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
+
+            IDX_e = list(range(0, len(X.e[0])))
+
+            sizes = [
+                len(X.e[0][i]) for i in IDX_e
+            ]  # hyperedges with size <m are removed
+            idx_e_remove = [IDX_e[i] for i in range(len(IDX_e)) if sizes[i] < m]
+            now_e_list = X.e[0]
+            new_e_list = []
+            for i in range(len(now_e_list)):
+                if i not in idx_e_remove:
+                    new_e_list.append(now_e_list[i])
+
+            X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
+
+            node_set = set()
+            for sublist in X.e[0]:
+                for element in sublist:
+                    node_set.add(element)
+            IDX = list(node_set)
+
+            while len(idx_n_remove) > 0 or len(idx_e_remove) > 0:
+                d_tot_m = np.zeros(len(IDX))
+
+                for i in range(len(IDX)):
+                    d_tot_m[i] = X.degree_node[IDX[i]]
+
+                idx_n_remove = list(
+                    compress(IDX, np.greater(kk * np.ones(len(d_tot_m)), d_tot_m))
+                )  # nodes with degree<k are removed
+                # X.remove_nodes_from(idx_n_remove)
+                now_e_list = X.e[0]
+                new_e_list = []
+                for e in now_e_list:
+                    new_e = []
+                    for n in e:
+                        if n not in idx_n_remove:
+                            new_e.append(n)
+                    if len(new_e) > 0:
+                        new_e_list.append(new_e)
+                X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
+
+                IDX_e = list(range(len(X.e[0])))
+                sizes = [
+                    len(X.e[0][i]) for i in IDX_e
+                ]  # hyperedges with size <m are removed
+
+                idx_e_remove = [IDX_e[i] for i in range(len(IDX_e)) if sizes[i] < m]
+                now_e_list = X.e[0]
+                new_e_list = []
+                for i in range(len(now_e_list)):
+                    if i not in idx_e_remove:
+                        new_e_list.append(now_e_list[i])
+                X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
+
+                node_set = set()
+                for sublist in X.e[0]:
+                    for element in sublist:
+                        node_set.add(element)
+                IDX = list(node_set)
+
+            shell_kk = list(sorted(set(prev_shell) - set(IDX)))
+            for j in shell_kk:
+                k_shell_dict[j][x] = kk - k_step
+
+            node_set = set()
+            for sublist in X.e[0]:
+                for element in sublist:
+                    node_set.add(element)
+            IDX = list(node_set)
+
+            D[y] = len(node_set)
+            if y > 0:
+                if D[y] == 0 and D[y - 1] != 0:
+                    # maximum connectivity at order m
+                    k_max[x] = kk - k_step
+            # stop the decomposition when the (k,m)-core is empty
+            if D[y] == 0:
+                break
+
+    # size-independent hypercoreness
+    R_dict = {}
+    for y in k_shell_dict:
+        R_dict[y] = sum(np.array(k_shell_dict[y]) / np.array(k_max))
+
+    return R_dict
+
+
+def frequency_based_hypercoreness(h):
+    r"""The frequency-based hypercoreness of nodes in hypergraph.
+
+     Parameters
+     ----------
+     h : easygraph.Hypergraph
+
+
+    Returns
+    -------
+    dict : Centrality, where keys are node IDs and values are lists of centralities.
+
+    References
+    ----------
+    Mancastroppa, M., Iacopini, I., Petri, G. et al. Hyper-cores promote localization and efficient seeding in higher-order processes. Nat Commun 14, 6223 (2023). https://doi.org/10.1038/s41467-023-41887-2
+
+    """
+    e_list = h.e[0]
+    initial_node_num = h.num_v
+    data = [e_list[i] for i in range(len(e_list)) if len(e_list[i]) > 1]
+
+    data.sort(key=len)
+    L = len(data)
+    size_max = len(data[L - 1])
+
+    size = list([len(data[j]) for j in range(L)])
+
+    X = eg.Hypergraph(num_v=initial_node_num, e_list=data)
+    IDX = list(range(1, X.num_v))
+
+    M = range(2, size_max + 1)
+    k_step = 1
+    K = range(1, 1200, k_step)
+    k_shell_dict = {}
+    idx_orig = IDX
+
+    IDX_size = range(len(size))
+    k_max = np.zeros(len(M))
+
+    for j in idx_orig:
+        k_shell_dict[j] = np.zeros(len(M))
+
+    for x in range(len(M)):
+        m = M[x]
+
+        D = np.zeros(len(K))
+        # consider only hyperedges of size >=m
+        idx_size = list(
+            compress(IDX_size, np.greater_equal(size, m * np.ones(len(size))))
+        )
+        int_sel = list([data[i] for i in idx_size])
+        # build hypergraph with only interactions of size >=m
+        X = eg.Hypergraph(num_v=initial_node_num, e_list=int_sel)
+        node_set = set()
+        for sublist in int_sel:
+            for element in sublist:
+                node_set.add(element)
+        IDX = list(node_set)
+
+        for y in range(len(K)):
+            kk = K[y]
+
+            d_tot_m = np.zeros(len(IDX))
+            prev_shell = IDX
+
+            for i in range(len(IDX)):
+                d_tot_m[i] = X.degree_node[IDX[i]]
+
+            idx_n_remove = list(
+                compress(IDX, np.greater(kk * np.ones(len(d_tot_m)), d_tot_m))
+            )  # nodes with degree<k are removed
+            now_e_list = X.e[0]
+            new_e_list = []
+            for e in now_e_list:
+                new_e = []
+                for n in e:
+                    if n not in idx_n_remove:
+                        new_e.append(n)
+                if len(new_e) > 0:
+                    new_e_list.append(new_e)
+
+            X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
+
+            IDX_e = list(range(0, len(X.e[0])))
+
+            # hyperedges with size <m are removed
+            sizes = [len(X.e[0][i]) for i in IDX_e]
+            idx_e_remove = [IDX_e[i] for i in range(len(IDX_e)) if sizes[i] < m]
+            now_e_list = X.e[0]
+            new_e_list = []
+            for i in range(len(now_e_list)):
+                if i not in idx_e_remove:
+                    new_e_list.append(now_e_list[i])
+
+            X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
+
+            node_set = set()
+            for sublist in X.e[0]:
+                for element in sublist:
+                    node_set.add(element)
+            IDX = list(node_set)
+
+            while len(idx_n_remove) > 0 or len(idx_e_remove) > 0:
+                d_tot_m = np.zeros(len(IDX))
+
+                for i in range(len(IDX)):
+                    d_tot_m[i] = X.degree_node[IDX[i]]
+                # nodes with degree<k are removed
+                idx_n_remove = list(
+                    compress(IDX, np.greater(kk * np.ones(len(d_tot_m)), d_tot_m))
+                )
+                now_e_list = X.e[0]
+                new_e_list = []
+                for e in now_e_list:
+                    new_e = []
+                    for n in e:
+                        if n not in idx_n_remove:
+                            new_e.append(n)
+                    if len(new_e) > 0:
+                        new_e_list.append(new_e)
+                X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
+
+                IDX_e = list(range(len(X.e[0])))
+                # hyperedges with size <m are removed
+                sizes = [len(X.e[0][i]) for i in IDX_e]
+
+                idx_e_remove = [IDX_e[i] for i in range(len(IDX_e)) if sizes[i] < m]
+                # print("m:",m)
+                now_e_list = X.e[0]
+                new_e_list = []
+                for i in range(len(now_e_list)):
+                    if i not in idx_e_remove:
+                        new_e_list.append(now_e_list[i])
+                X = eg.Hypergraph(num_v=initial_node_num, e_list=new_e_list)
+
+                node_set = set()
+                for sublist in X.e[0]:
+                    for element in sublist:
+                        node_set.add(element)
+                IDX = list(node_set)
+
+            shell_kk = list(sorted(set(prev_shell) - set(IDX)))
+            # print("fun shell_kk:",len(shell_kk))
+            for j in shell_kk:
+                k_shell_dict[j][x] = kk - k_step
+
+            node_set = set()
+            for sublist in X.e[0]:
+                for element in sublist:
+                    node_set.add(element)
+            IDX = list(node_set)
+
+            D[y] = len(node_set)
+            if y > 0:
+                if D[y] == 0 and D[y - 1] != 0:
+                    k_max[x] = kk - k_step  # maximum connectivity at order m
+            if D[y] == 0:
+                break  # stop the decomposition when the (k,m)-core is empty
+
+    # Psi(m)  distribution of hyperedges size
+    Psi = []
+    for m in range(2, size_max + 1):
+        Psi.append(size.count(m) / len(size))
+    # frequency-based hypercoreness
+    R_w_dict = {}
+    for y in k_shell_dict:
+        R_w_dict[y] = sum(np.array(Psi) * np.array(k_shell_dict[y]) / np.array(k_max))
+    return R_w_dict
```

## easygraph/functions/hypergraph/centrality/vector_centrality.py

 * *Ordering differences only*

```diff
@@ -1,66 +1,66 @@
-import easygraph as eg
-import numpy as np
-
-from easygraph.exception import EasyGraphError
-
-
-__all__ = ["vector_centrality"]
-
-
-def vector_centrality(H):
-    """The vector centrality of nodes in the line graph of the hypergraph.
-
-    Parameters
-    ----------
-    H : eg.Hypergraph
-
-
-    Returns
-    -------
-    dict
-        Centrality, where keys are node IDs and values are lists of centralities.
-
-    References
-    ----------
-    "Vector centrality in hypergraphs", K. Kovalenko, M. Romance, E. Vasilyeva,
-    D. Aleja, R. Criado, D. Musatov, A.M. Raigorodskii, J. Flores, I. Samoylenko,
-    K. Alfaro-Bittner, M. Perc, S. Boccaletti,
-    https://doi.org/10.1016/j.chaos.2022.112397
-
-    """
-
-    # If the hypergraph is empty, then return an empty dictionary
-    if H.num_v == 0:
-        return dict()
-
-    LG = H.get_linegraph()
-    if not eg.is_connected(LG):
-        raise EasyGraphError("This method is not defined for disconnected hypergraphs.")
-    LGcent = eg.eigenvector_centrality(LG)
-
-    vc = {node: [] for node in range(0, H.num_v)}
-
-    edge_label_dict = {tuple(edge): index for index, edge in enumerate(H.e[0])}
-
-    hyperedge_dims = {tuple(edge): len(edge) for edge in H.e[0]}
-
-    D = max([len(e) for e in H.e[0]])
-
-    for k in range(2, D + 1):
-        c_i = np.zeros(H.num_v)
-
-        for edge, _ in list(filter(lambda x: x[1] == k, hyperedge_dims.items())):
-            for node in edge:
-                try:
-                    c_i[node] += LGcent[edge_label_dict[edge]]
-                except IndexError:
-                    raise Exception(
-                        "Nodes must be written with the Pythonic indexing (0,1,2...)"
-                    )
-
-        c_i *= 1 / k
-
-        for node in range(H.num_v):
-            vc[node].append(c_i[node])
-
-    return vc
+import easygraph as eg
+import numpy as np
+
+from easygraph.exception import EasyGraphError
+
+
+__all__ = ["vector_centrality"]
+
+
+def vector_centrality(H):
+    """The vector centrality of nodes in the line graph of the hypergraph.
+
+    Parameters
+    ----------
+    H : eg.Hypergraph
+
+
+    Returns
+    -------
+    dict
+        Centrality, where keys are node IDs and values are lists of centralities.
+
+    References
+    ----------
+    "Vector centrality in hypergraphs", K. Kovalenko, M. Romance, E. Vasilyeva,
+    D. Aleja, R. Criado, D. Musatov, A.M. Raigorodskii, J. Flores, I. Samoylenko,
+    K. Alfaro-Bittner, M. Perc, S. Boccaletti,
+    https://doi.org/10.1016/j.chaos.2022.112397
+
+    """
+
+    # If the hypergraph is empty, then return an empty dictionary
+    if H.num_v == 0:
+        return dict()
+
+    LG = H.get_linegraph()
+    if not eg.is_connected(LG):
+        raise EasyGraphError("This method is not defined for disconnected hypergraphs.")
+    LGcent = eg.eigenvector_centrality(LG)
+
+    vc = {node: [] for node in range(0, H.num_v)}
+
+    edge_label_dict = {tuple(edge): index for index, edge in enumerate(H.e[0])}
+
+    hyperedge_dims = {tuple(edge): len(edge) for edge in H.e[0]}
+
+    D = max([len(e) for e in H.e[0]])
+
+    for k in range(2, D + 1):
+        c_i = np.zeros(H.num_v)
+
+        for edge, _ in list(filter(lambda x: x[1] == k, hyperedge_dims.items())):
+            for node in edge:
+                try:
+                    c_i[node] += LGcent[edge_label_dict[edge]]
+                except IndexError:
+                    raise Exception(
+                        "Nodes must be written with the Pythonic indexing (0,1,2...)"
+                    )
+
+        c_i *= 1 / k
+
+        for node in range(H.num_v):
+            vc[node].append(c_i[node])
+
+    return vc
```

## easygraph/functions/hypergraph/centrality/s_centrality.py

 * *Ordering differences only*

```diff
@@ -1,89 +1,89 @@
-import easygraph as eg
-
-
-__all__ = ["s_betweenness", "s_closeness", "s_eccentricity"]
-
-
-def s_betweenness(H, s=1, n_workers=None):
-    """Computes the betweenness centrality for each edge in the hypergraph.
-
-    Computes the betweenness centrality for each edge in the hypergraph.
-
-    Parameters
-    ----------
-    H : eg.Hypergraph.
-        The hypergraph to compute
-
-    s : int, optional.
-
-    Returns
-    ----------
-    dict
-    The keys are the edges and the values are the betweenness centrality.
-    The betweenness centrality for each edge in the hypergraph.
-
-
-    """
-
-    linegraph = H.get_linegraph(s=s)
-    results = eg.betweenness_centrality(linegraph, n_workers=n_workers)
-    return results
-
-
-def s_closeness(H, s=1, n_workers=None):
-    """
-    Compute the closeness centrality for each edge in the hypergraph.
-
-    Parameters
-    ----------
-    H : eg.Hypergraph.
-    s : int, optional
-
-    Returns
-    -------
-    dict. The closeness centrality for each edge in the hypergraph. The keys are the edges and the values are the closeness centrality.
-    """
-    linegraph = H.get_linegraph(s=s)
-    results = eg.closeness_centrality(linegraph, n_workers=n_workers)
-    return results
-
-
-def s_eccentricity(H, s=1, edges=True, source=None):
-    r"""
-    The length of the longest shortest path from a vertex $u$ to every other vertex in
-    the s-linegraph.
-    $V$ = set of vertices in the s-linegraph
-    $d$ = shortest path distance
-
-    .. math::
-
-        \text{s-ecc}(u) = \text{max}\{d(u,v): v \in V\}
-
-    Parameters
-    ----------
-    H : eg.Hypergraph
-
-    s : int, optional
-
-    edges : bool, optional
-        Indicates if method should compute edge linegraph (default) or node linegraph.
-
-    source : str, optional
-        Identifier of node or edge of interest for computing centrality
-
-    Returns
-    -------
-    dict or float
-        returns the s-eccentricity value of the edges(nodes).
-        If source=None a dictionary of values for each s-edge in H is returned.
-        If source then a single value is returned.
-        If the s-linegraph is disconnected, np.inf is returned.
-
-    """
-
-    g = H.get_linegraph(s=s, edges=edges)
-    result = eg.eccentricity(g)
-    if source:
-        return result[source]
-    else:
-        return result
+import easygraph as eg
+
+
+__all__ = ["s_betweenness", "s_closeness", "s_eccentricity"]
+
+
+def s_betweenness(H, s=1, n_workers=None):
+    """Computes the betweenness centrality for each edge in the hypergraph.
+
+    Computes the betweenness centrality for each edge in the hypergraph.
+
+    Parameters
+    ----------
+    H : eg.Hypergraph.
+        The hypergraph to compute
+
+    s : int, optional.
+
+    Returns
+    ----------
+    dict
+    The keys are the edges and the values are the betweenness centrality.
+    The betweenness centrality for each edge in the hypergraph.
+
+
+    """
+
+    linegraph = H.get_linegraph(s=s)
+    results = eg.betweenness_centrality(linegraph, n_workers=n_workers)
+    return results
+
+
+def s_closeness(H, s=1, n_workers=None):
+    """
+    Compute the closeness centrality for each edge in the hypergraph.
+
+    Parameters
+    ----------
+    H : eg.Hypergraph.
+    s : int, optional
+
+    Returns
+    -------
+    dict. The closeness centrality for each edge in the hypergraph. The keys are the edges and the values are the closeness centrality.
+    """
+    linegraph = H.get_linegraph(s=s)
+    results = eg.closeness_centrality(linegraph, n_workers=n_workers)
+    return results
+
+
+def s_eccentricity(H, s=1, edges=True, source=None):
+    r"""
+    The length of the longest shortest path from a vertex $u$ to every other vertex in
+    the s-linegraph.
+    $V$ = set of vertices in the s-linegraph
+    $d$ = shortest path distance
+
+    .. math::
+
+        \text{s-ecc}(u) = \text{max}\{d(u,v): v \in V\}
+
+    Parameters
+    ----------
+    H : eg.Hypergraph
+
+    s : int, optional
+
+    edges : bool, optional
+        Indicates if method should compute edge linegraph (default) or node linegraph.
+
+    source : str, optional
+        Identifier of node or edge of interest for computing centrality
+
+    Returns
+    -------
+    dict or float
+        returns the s-eccentricity value of the edges(nodes).
+        If source=None a dictionary of values for each s-edge in H is returned.
+        If source then a single value is returned.
+        If the s-linegraph is disconnected, np.inf is returned.
+
+    """
+
+    g = H.get_linegraph(s=s, edges=edges)
+    result = eg.eccentricity(g)
+    if source:
+        return result[source]
+    else:
+        return result
```

## easygraph/functions/hypergraph/centrality/degree.py

 * *Ordering differences only*

```diff
@@ -1,28 +1,28 @@
-__all__ = ["hyepergraph_degree_centrality"]
-
-
-def hyepergraph_degree_centrality(G):
-    """
-
-    Parameters
-    ----------
-    G : eg.Hypergraph
-        The target hypergraph
-
-    Returns
-    ----------
-    degree centrality of each node in G : dict
-
-    """
-    res = {}
-    node_list = G.v
-    # Get hyperedge list
-    edge_list = G.e[0]
-    for node in node_list:
-        res[node] = 0
-
-    for e in edge_list:
-        for n in e:
-            res[n] += 1
-
-    return res
+__all__ = ["hyepergraph_degree_centrality"]
+
+
+def hyepergraph_degree_centrality(G):
+    """
+
+    Parameters
+    ----------
+    G : eg.Hypergraph
+        The target hypergraph
+
+    Returns
+    ----------
+    degree centrality of each node in G : dict
+
+    """
+    res = {}
+    node_list = G.v
+    # Get hyperedge list
+    edge_list = G.e[0]
+    for node in node_list:
+        res[node] = 0
+
+    for e in edge_list:
+        for n in e:
+            res[n] += 1
+
+    return res
```

## easygraph/functions/hypergraph/centrality/__init__.py

 * *Ordering differences only*

```diff
@@ -1,5 +1,5 @@
-from .cycle_ratio import *
-from .degree import *
-from .hypercoreness import *
-from .s_centrality import *
-from .vector_centrality import *
+from .cycle_ratio import *
+from .degree import *
+from .hypercoreness import *
+from .s_centrality import *
+from .vector_centrality import *
```

## easygraph/functions/path/diameter.py

 * *Ordering differences only*

```diff
@@ -1,105 +1,105 @@
-import easygraph as eg
-import easygraph.functions.path
-
-
-def eccentricity(G, v=None, sp=None):
-    """Returns the eccentricity of nodes in G.
-
-    The eccentricity of a node v is the maximum distance from v to
-    all other nodes in G.
-
-    Parameters
-    ----------
-    G : EasyGraph graph
-       A graph
-
-    v : node, optional
-       Return value of specified node
-
-    sp : dict of dicts, optional
-       All pairs shortest path lengths as a dictionary of dictionaries
-
-    Returns
-    -------
-    ecc : dictionary
-       A dictionary of eccentricity values keyed by node.
-
-    Examples
-    --------
-    >>> G = eg.Graph([(1, 2), (1, 3), (1, 4), (3, 4), (3, 5), (4, 5)])
-    >>> dict(eg.eccentricity(G))
-    {1: 2, 2: 3, 3: 2, 4: 2, 5: 3}
-
-    >>> dict(eg.eccentricity(G, v=[1, 5]))  # This returns the eccentrity of node 1 & 5
-    {1: 2, 5: 3}
-
-    """
-    #    if v is None:                # none, use entire graph
-    #        nodes=G.nodes()
-    #    elif v in G:               # is v a single node
-    #        nodes=[v]
-    #    else:                      # assume v is a container of nodes
-    #        nodes=v
-    order = G.order()
-
-    e = {}
-    for n in G.nbunch_iter(v):
-        if sp is None:
-            length = eg.single_source_dijkstra(G, n)
-            L = len(length)
-        else:
-            try:
-                length = sp[n]
-                L = len(length)
-            except TypeError as err:
-                raise eg.EasyGraphError('Format of "sp" is invalid.') from err
-        if L != order:
-            if G.is_directed():
-                msg = (
-                    "Found infinite path length because the digraph is not"
-                    " strongly connected"
-                )
-            else:
-                msg = "Found infinite path length because the graph is not connected"
-            raise eg.EasyGraphError(msg)
-
-        e[n] = max(length.values())
-
-    if v in G:
-        return e[v]  # return single value
-    else:
-        return e
-
-
-def diameter(G, e=None):
-    """Returns the diameter of the graph G.
-
-    The diameter is the maximum eccentricity.
-
-    Parameters
-    ----------
-    G : EasyGraph graph
-       A graph
-
-    e : eccentricity dictionary, optional
-      A precomputed dictionary of eccentricities.
-
-    Returns
-    -------
-    d : integer
-       Diameter of graph
-
-    Examples
-    --------
-    >>> G = eg.Graph([(1, 2), (1, 3), (1, 4), (3, 4), (3, 5), (4, 5)])
-    >>> eg.diameter(G)
-    3
-
-    See Also
-    --------
-    eccentricity
-    """
-
-    if e is None:
-        e = eccentricity(G)
-    return max(e.values())
+import easygraph as eg
+import easygraph.functions.path
+
+
+def eccentricity(G, v=None, sp=None):
+    """Returns the eccentricity of nodes in G.
+
+    The eccentricity of a node v is the maximum distance from v to
+    all other nodes in G.
+
+    Parameters
+    ----------
+    G : EasyGraph graph
+       A graph
+
+    v : node, optional
+       Return value of specified node
+
+    sp : dict of dicts, optional
+       All pairs shortest path lengths as a dictionary of dictionaries
+
+    Returns
+    -------
+    ecc : dictionary
+       A dictionary of eccentricity values keyed by node.
+
+    Examples
+    --------
+    >>> G = eg.Graph([(1, 2), (1, 3), (1, 4), (3, 4), (3, 5), (4, 5)])
+    >>> dict(eg.eccentricity(G))
+    {1: 2, 2: 3, 3: 2, 4: 2, 5: 3}
+
+    >>> dict(eg.eccentricity(G, v=[1, 5]))  # This returns the eccentrity of node 1 & 5
+    {1: 2, 5: 3}
+
+    """
+    #    if v is None:                # none, use entire graph
+    #        nodes=G.nodes()
+    #    elif v in G:               # is v a single node
+    #        nodes=[v]
+    #    else:                      # assume v is a container of nodes
+    #        nodes=v
+    order = G.order()
+
+    e = {}
+    for n in G.nbunch_iter(v):
+        if sp is None:
+            length = eg.single_source_dijkstra(G, n)
+            L = len(length)
+        else:
+            try:
+                length = sp[n]
+                L = len(length)
+            except TypeError as err:
+                raise eg.EasyGraphError('Format of "sp" is invalid.') from err
+        if L != order:
+            if G.is_directed():
+                msg = (
+                    "Found infinite path length because the digraph is not"
+                    " strongly connected"
+                )
+            else:
+                msg = "Found infinite path length because the graph is not connected"
+            raise eg.EasyGraphError(msg)
+
+        e[n] = max(length.values())
+
+    if v in G:
+        return e[v]  # return single value
+    else:
+        return e
+
+
+def diameter(G, e=None):
+    """Returns the diameter of the graph G.
+
+    The diameter is the maximum eccentricity.
+
+    Parameters
+    ----------
+    G : EasyGraph graph
+       A graph
+
+    e : eccentricity dictionary, optional
+      A precomputed dictionary of eccentricities.
+
+    Returns
+    -------
+    d : integer
+       Diameter of graph
+
+    Examples
+    --------
+    >>> G = eg.Graph([(1, 2), (1, 3), (1, 4), (3, 4), (3, 5), (4, 5)])
+    >>> eg.diameter(G)
+    3
+
+    See Also
+    --------
+    eccentricity
+    """
+
+    if e is None:
+        e = eccentricity(G)
+    return max(e.values())
```

## easygraph/functions/path/mst.py

 * *Ordering differences only*

```diff
@@ -1,685 +1,685 @@
-from heapq import heappop
-from heapq import heappush
-from itertools import count
-from math import isnan
-from operator import itemgetter
-
-from easygraph.utils.decorators import *
-
-
-__all__ = [
-    "minimum_spanning_edges",
-    "maximum_spanning_edges",
-    "minimum_spanning_tree",
-    "maximum_spanning_tree",
-]
-
-
-def boruvka_mst_edges(G, minimum=True, weight="weight", data=True, ignore_nan=False):
-    """Iterate over edges of a Borůvka's algorithm min/max spanning tree.
-
-    Parameters
-    ----------
-    G : EasyGraph Graph
-        The edges of `G` must have distinct weights,
-        otherwise the edges may not form a tree.
-
-    minimum : bool (default: True)
-        Find the minimum (True) or maximum (False) spanning tree.
-
-    weight : string (default: 'weight')
-        The name of the edge attribute holding the edge weights.
-
-    data : bool (default: True)
-        Flag for whether to yield edge attribute dicts.
-        If True, yield edges `(u, v, d)`, where `d` is the attribute dict.
-        If False, yield edges `(u, v)`.
-
-    ignore_nan : bool (default: False)
-        If a NaN is found as an edge weight normally an exception is raised.
-        If `ignore_nan is True` then that edge is ignored instead.
-
-    """
-    # Initialize a forest, assuming initially that it is the discrete
-    # partition of the nodes of the graph.
-    forest = UnionFind(G)
-
-    def best_edge(component):
-        """Returns the optimum (minimum or maximum) edge on the edge
-        boundary of the given set of nodes.
-
-        A return value of ``None`` indicates an empty boundary.
-
-        """
-        sign = 1 if minimum else -1
-        minwt = float("inf")
-        boundary = None
-        for e in edge_boundary(G, component, data=True):
-            wt = e[-1].get(weight, 1) * sign
-            if isnan(wt):
-                if ignore_nan:
-                    continue
-                msg = f"NaN found as an edge weight. Edge {e}"
-                raise ValueError(msg)
-            if wt < minwt:
-                minwt = wt
-                boundary = e
-        return boundary
-
-    # Determine the optimum edge in the edge boundary of each component
-    # in the forest.
-    best_edges = (best_edge(component) for component in forest.to_sets())
-    best_edges = [edge for edge in best_edges if edge is not None]
-    # If each entry was ``None``, that means the graph was disconnected,
-    # so we are done generating the forest.
-    while best_edges:
-        # Determine the optimum edge in the edge boundary of each
-        # component in the forest.
-        #
-        # This must be a sequence, not an iterator. In this list, the
-        # same edge may appear twice, in different orientations (but
-        # that's okay, since a union operation will be called on the
-        # endpoints the first time it is seen, but not the second time).
-        #
-        # Any ``None`` indicates that the edge boundary for that
-        # component was empty, so that part of the forest has been
-        # completed.
-        #
-        # TODO This can be parallelized, both in the outer loop over
-        # each component in the forest and in the computation of the
-        # minimum. (Same goes for the identical lines outside the loop.)
-        best_edges = (best_edge(component) for component in forest.to_sets())
-        best_edges = [edge for edge in best_edges if edge is not None]
-        # Join trees in the forest using the best edges, and yield that
-        # edge, since it is part of the spanning tree.
-        #
-        # TODO This loop can be parallelized, to an extent (the union
-        # operation must be atomic).
-        for u, v, d in best_edges:
-            if forest[u] != forest[v]:
-                if data:
-                    yield u, v, d
-                else:
-                    yield u, v
-                forest.union(u, v)
-
-
-@hybrid("cpp_kruskal_mst_edges")
-def kruskal_mst_edges(G, minimum=True, weight="weight", data=True, ignore_nan=False):
-    """Iterate over edges of a Kruskal's algorithm min/max spanning tree.
-
-    Parameters
-    ----------
-    G : EasyGraph Graph
-        The graph holding the tree of interest.
-
-    minimum : bool (default: True)
-        Find the minimum (True) or maximum (False) spanning tree.
-
-    weight : string (default: 'weight')
-        The name of the edge attribute holding the edge weights.
-
-    data : bool (default: True)
-        Flag for whether to yield edge attribute dicts.
-        If True, yield edges `(u, v, d)`, where `d` is the attribute dict.
-        If False, yield edges `(u, v)`.
-
-    ignore_nan : bool (default: False)
-        If a NaN is found as an edge weight normally an exception is raised.
-        If `ignore_nan is True` then that edge is ignored instead.
-
-    """
-    subtrees = UnionFind()
-    edges = []
-    for u, v, t in G.edges:
-        edges.append((u, v, t))
-
-    def filter_nan_edges(edges=edges, weight=weight):
-        sign = 1 if minimum else -1
-        for u, v, d in edges:
-            wt = d.get(weight, 1) * sign
-            if isnan(wt):
-                if ignore_nan:
-                    continue
-                msg = f"NaN found as an edge weight. Edge {(u, v, d)}"
-                raise ValueError(msg)
-            yield wt, u, v, d
-
-    edges = sorted(filter_nan_edges(), key=itemgetter(0))
-    for wt, u, v, d in edges:
-        if subtrees[u] != subtrees[v]:
-            if data:
-                yield (u, v, d)
-            else:
-                yield (u, v)
-            subtrees.union(u, v)
-
-
-@hybrid("cpp_prim_mst_edges")
-def prim_mst_edges(G, minimum=True, weight="weight", data=True, ignore_nan=False):
-    """Iterate over edges of Prim's algorithm min/max spanning tree.
-
-    Parameters
-    ----------
-    G : EasyGraph Graph
-        The graph holding the tree of interest.
-
-    minimum : bool (default: True)
-        Find the minimum (True) or maximum (False) spanning tree.
-
-    weight : string (default: 'weight')
-        The name of the edge attribute holding the edge weights.
-
-    data : bool (default: True)
-        Flag for whether to yield edge attribute dicts.
-        If True, yield edges `(u, v, d)`, where `d` is the attribute dict.
-        If False, yield edges `(u, v)`.
-
-    ignore_nan : bool (default: False)
-        If a NaN is found as an edge weight normally an exception is raised.
-        If `ignore_nan is True` then that edge is ignored instead.
-
-    """
-    push = heappush
-    pop = heappop
-
-    nodes = set(G)
-    c = count()
-
-    sign = 1 if minimum else -1
-
-    while nodes:
-        u = nodes.pop()
-        frontier = []
-        visited = {u}
-        for v, d in G.adj[u].items():
-            wt = d.get(weight, 1) * sign
-            if isnan(wt):
-                if ignore_nan:
-                    continue
-                msg = f"NaN found as an edge weight. Edge {(u, v, d)}"
-                raise ValueError(msg)
-            push(frontier, (wt, next(c), u, v, d))
-        while frontier:
-            W, _, u, v, d = pop(frontier)
-            if v in visited or v not in nodes:
-                continue
-            if data:
-                yield u, v, d
-            else:
-                yield u, v
-            # update frontier
-            visited.add(v)
-            nodes.discard(v)
-            for w, d2 in G.adj[v].items():
-                if w in visited:
-                    continue
-                new_weight = d2.get(weight, 1) * sign
-                push(frontier, (new_weight, next(c), v, w, d2))
-
-
-ALGORITHMS = {
-    "boruvka": boruvka_mst_edges,
-    "borůvka": boruvka_mst_edges,
-    "kruskal": kruskal_mst_edges,
-    "prim": prim_mst_edges,
-}
-
-
-@not_implemented_for("multigraph")
-@only_implemented_for_UnDirected_graph
-def minimum_spanning_edges(
-    G, algorithm="kruskal", weight="weight", data=True, ignore_nan=False
-):
-    """Generate edges in a minimum spanning forest of an undirected
-    weighted graph.
-
-    A minimum spanning tree is a subgraph of the graph (a tree)
-    with the minimum sum of edge weights.  A spanning forest is a
-    union of the spanning trees for each connected component of the graph.
-
-    Parameters
-    ----------
-    G : undirected Graph
-       An undirected graph. If `G` is connected, then the algorithm finds a
-       spanning tree. Otherwise, a spanning forest is found.
-
-    algorithm : string
-       The algorithm to use when finding a minimum spanning tree. Valid
-       choices are 'kruskal', 'prim', or 'boruvka'. The default is 'kruskal'.
-
-    weight : string
-       Edge data key to use for weight (default 'weight').
-
-    data : bool, optional
-       If True yield the edge data along with the edge.
-
-    ignore_nan : bool (default: False)
-        If a NaN is found as an edge weight normally an exception is raised.
-        If `ignore_nan is True` then that edge is ignored instead.
-
-    Returns
-    -------
-    edges : iterator
-       An iterator over edges in a maximum spanning tree of `G`.
-       Edges connecting nodes `u` and `v` are represented as tuples:
-       `(u, v, k, d)` or `(u, v, k)` or `(u, v, d)` or `(u, v)`
-
-    Examples
-    --------
-    >>> from easygraph.functions.basic import mst
-
-    Find minimum spanning edges by Kruskal's algorithm
-
-    >>> G.add_edge(0, 3, weight=2)
-    >>> mst = mst.minimum_spanning_edges(G, algorithm="kruskal", data=False)
-    >>> edgelist = list(mst)
-    >>> sorted(sorted(e) for e in edgelist)
-    [[0, 1], [1, 2], [2, 3]]
-
-    Find minimum spanning edges by Prim's algorithm
-
-    >>> G.add_edge(0, 3, weight=2)
-    >>> mst = mst.minimum_spanning_edges(G, algorithm="prim", data=False)
-    >>> edgelist = list(mst)
-    >>> sorted(sorted(e) for e in edgelist)
-    [[0, 1], [1, 2], [2, 3]]
-
-    Notes
-    -----
-    For Borůvka's algorithm, each edge must have a weight attribute, and
-    each edge weight must be distinct.
-
-    For the other algorithms, if the graph edges do not have a weight
-    attribute a default weight of 1 will be used.
-
-    Modified code from David Eppstein, April 2006
-    http://www.ics.uci.edu/~eppstein/PADS/
-
-    """
-    try:
-        algo = ALGORITHMS[algorithm]
-    except KeyError as e:
-        msg = f"{algorithm} is not a valid choice for an algorithm."
-        raise ValueError(msg) from e
-
-    return algo(G, minimum=True, weight=weight, data=data, ignore_nan=ignore_nan)
-
-
-@not_implemented_for("multigraph")
-@only_implemented_for_UnDirected_graph
-def maximum_spanning_edges(
-    G, algorithm="kruskal", weight="weight", data=True, ignore_nan=False
-):
-    """Generate edges in a maximum spanning forest of an undirected
-    weighted graph.
-
-    A maximum spanning tree is a subgraph of the graph (a tree)
-    with the maximum possible sum of edge weights.  A spanning forest is a
-    union of the spanning trees for each connected component of the graph.
-
-    Parameters
-    ----------
-    G : undirected Graph
-       An undirected graph. If `G` is connected, then the algorithm finds a
-       spanning tree. Otherwise, a spanning forest is found.
-
-    algorithm : string
-       The algorithm to use when finding a maximum spanning tree. Valid
-       choices are 'kruskal', 'prim', or 'boruvka'. The default is 'kruskal'.
-
-    weight : string
-       Edge data key to use for weight (default 'weight').
-
-    data : bool, optional
-       If True yield the edge data along with the edge.
-
-    ignore_nan : bool (default: False)
-        If a NaN is found as an edge weight normally an exception is raised.
-        If `ignore_nan is True` then that edge is ignored instead.
-
-    Returns
-    -------
-    edges : iterator
-       An iterator over edges in a maximum spanning tree of `G`.
-       Edges connecting nodes `u` and `v` are represented as tuples:
-       `(u, v, k, d)` or `(u, v, k)` or `(u, v, d)` or `(u, v)`
-
-    Examples
-    --------
-    >>> from easygraph.functions.basic import mst
-
-    Find maximum spanning edges by Kruskal's algorithm
-
-    >>> G.add_edge(0, 3, weight=2)
-    >>> mst = mst.maximum_spanning_edges(G, algorithm="kruskal", data=False)
-    >>> edgelist = list(mst)
-    >>> sorted(sorted(e) for e in edgelist)
-    [[0, 1], [0, 3], [1, 2]]
-
-    Find maximum spanning edges by Prim's algorithm
-
-    >>> G.add_edge(0, 3, weight=2)  # assign weight 2 to edge 0-3
-    >>> mst = mst.maximum_spanning_edges(G, algorithm="prim", data=False)
-    >>> edgelist = list(mst)
-    >>> sorted(sorted(e) for e in edgelist)
-    [[0, 1], [0, 3], [2, 3]]
-
-    Notes
-    -----
-    For Borůvka's algorithm, each edge must have a weight attribute, and
-    each edge weight must be distinct.
-
-    For the other algorithms, if the graph edges do not have a weight
-    attribute a default weight of 1 will be used.
-
-    Modified code from David Eppstein, April 2006
-    http://www.ics.uci.edu/~eppstein/PADS/
-    """
-    try:
-        algo = ALGORITHMS[algorithm]
-    except KeyError as e:
-        msg = f"{algorithm} is not a valid choice for an algorithm."
-        raise ValueError(msg) from e
-
-    return algo(G, minimum=False, weight=weight, data=data, ignore_nan=ignore_nan)
-
-
-@not_implemented_for("multigraph")
-def minimum_spanning_tree(G, weight="weight", algorithm="kruskal", ignore_nan=False):
-    """Returns a minimum spanning tree or forest on an undirected graph `G`.
-
-    Parameters
-    ----------
-    G : undirected graph
-        An undirected graph. If `G` is connected, then the algorithm finds a
-        spanning tree. Otherwise, a spanning forest is found.
-
-    weight : str
-       Data key to use for edge weights.
-
-    algorithm : string
-       The algorithm to use when finding a minimum spanning tree. Valid
-       choices are 'kruskal', 'prim', or 'boruvka'. The default is
-       'kruskal'.
-
-    ignore_nan : bool (default: False)
-        If a NaN is found as an edge weight normally an exception is raised.
-        If `ignore_nan is True` then that edge is ignored instead.
-
-    Returns
-    -------
-    G : EasyGraph Graph
-       A minimum spanning tree or forest.
-
-    Examples
-    --------
-    >>> G.add_edge(0, 3, weight=2)
-    >>> T = eg.minimum_spanning_tree(G)
-    >>> sorted(T.edges(data=True))
-    [(0, 1, {}), (1, 2, {}), (2, 3, {})]
-
-
-    Notes
-    -----
-    For Borůvka's algorithm, each edge must have a weight attribute, and
-    each edge weight must be distinct.
-
-    For the other algorithms, if the graph edges do not have a weight
-    attribute a default weight of 1 will be used.
-
-    Isolated nodes with self-loops are in the tree as edgeless isolated nodes.
-
-    """
-    edges = list(
-        minimum_spanning_edges(G, algorithm, weight, data=True, ignore_nan=ignore_nan)
-    )
-    T = G.__class__()  # Same graph class as G
-    for i in G.nodes:
-        T.add_node(i)
-    for i in edges:
-        (u, v, t) = i
-        T.add_edge(u, v, **t)
-    return T
-
-
-@not_implemented_for("multigraph")
-def maximum_spanning_tree(G, weight="weight", algorithm="kruskal", ignore_nan=False):
-    """Returns a maximum spanning tree or forest on an undirected graph `G`.
-
-    Parameters
-    ----------
-    G : undirected graph
-        An undirected graph. If `G` is connected, then the algorithm finds a
-        spanning tree. Otherwise, a spanning forest is found.
-
-    weight : str
-       Data key to use for edge weights.
-
-    algorithm : string
-       The algorithm to use when finding a maximum spanning tree. Valid
-       choices are 'kruskal', 'prim', or 'boruvka'. The default is
-       'kruskal'.
-
-    ignore_nan : bool (default: False)
-        If a NaN is found as an edge weight normally an exception is raised.
-        If `ignore_nan is True` then that edge is ignored instead.
-
-
-    Returns
-    -------
-    G : EasyGraph Graph
-       A maximum spanning tree or forest.
-
-
-    Examples
-    --------
-    >>> G.add_edge(0, 3, weight=2)
-    >>> T = eg.maximum_spanning_tree(G)
-    >>> sorted(T.edges(data=True))
-    [(0, 1, {}), (0, 3, {'weight': 2}), (1, 2, {})]
-
-
-    Notes
-    -----
-    For Borůvka's algorithm, each edge must have a weight attribute, and
-    each edge weight must be distinct.
-
-    For the other algorithms, if the graph edges do not have a weight
-    attribute a default weight of 1 will be used.
-
-    There may be more than one tree with the same minimum or maximum weight.
-    See :mod:`easygraph.tree.recognition` for more detailed definitions.
-
-    Isolated nodes with self-loops are in the tree as edgeless isolated nodes.
-
-    """
-    edges = list(
-        maximum_spanning_edges(G, algorithm, weight, data=True, ignore_nan=ignore_nan)
-    )
-    T = G.__class__()  # Same graph class as G
-    for i in G.nodes:
-        T.add_node(i)
-    for i in edges:
-        (u, v, t) = i
-        T.add_edge(u, v, **t)
-    return T
-
-
-def edge_boundary(G, nbunch1, nbunch2=None, data=False, default=None):
-    """Returns the edge boundary of `nbunch1`.
-
-    The *edge boundary* of a set *S* with respect to a set *T* is the
-    set of edges (*u*, *v*) such that *u* is in *S* and *v* is in *T*.
-    If *T* is not specified, it is assumed to be the set of all nodes
-    not in *S*.
-
-    Parameters
-    ----------
-    G : EasyGraph graph
-
-    nbunch1 : iterable
-        Iterable of nodes in the graph representing the set of nodes
-        whose edge boundary will be returned. (This is the set *S* from
-        the definition above.)
-
-    nbunch2 : iterable
-        Iterable of nodes representing the target (or "exterior") set of
-        nodes. (This is the set *T* from the definition above.) If not
-        specified, this is assumed to be the set of all nodes in `G`
-        not in `nbunch1`.
-
-    data : bool or object
-        This parameter has the same meaning as in
-        :meth:`MultiGraph.edges`.
-
-    default : object
-        This parameter has the same meaning as in
-        :meth:`MultiGraph.edges`.
-
-    Returns
-    -------
-    iterator
-        An iterator over the edges in the boundary of `nbunch1` with
-        respect to `nbunch2`. If `keys`, `data`, or `default`
-        are specified and `G` is a multigraph, then edges are returned
-        with keys and/or data, as in :meth:`MultiGraph.edges`.
-
-    Notes
-    -----
-    Any element of `nbunch` that is not in the graph `G` will be
-    ignored.
-
-    `nbunch1` and `nbunch2` are usually meant to be disjoint, but in
-    the interest of speed and generality, that is not required here.
-
-    """
-    nset1 = {v for v in G if v in nbunch1}
-    # Here we create an iterator over edges incident to nodes in the set
-    # `nset1`. The `Graph.edges()` method does not provide a guarantee
-    # on the orientation of the edges, so our algorithm below must
-    # handle the case in which exactly one orientation, either (u, v) or
-    # (v, u), appears in this iterable.
-    edges = G.edges(nset1, data=data, default=default)
-    # If `nbunch2` is not provided, then it is assumed to be the set
-    # complement of `nbunch1`. For the sake of efficiency, this is
-    # implemented by using the `not in` operator, instead of by creating
-    # an additional set and using the `in` operator.
-    if nbunch2 is None:
-        return (e for e in edges if (e[0] in nset1) ^ (e[1] in nset1))
-    nset2 = set(nbunch2)
-    return (
-        e
-        for e in edges
-        if (e[0] in nset1 and e[1] in nset2) or (e[1] in nset1 and e[0] in nset2)
-    )
-
-
-"""
-Union-find data structure.
-"""
-
-
-class UnionFind:
-    """Union-find data structure.
-
-    Each unionFind instance X maintains a family of disjoint sets of
-    hashable objects, supporting the following two methods:
-
-    - X[item] returns a name for the set containing the given item.
-      Each set is named by an arbitrarily-chosen one of its members; as
-      long as the set remains unchanged it will keep the same name. If
-      the item is not yet part of a set in X, a new singleton set is
-      created for it.
-
-    - X.union(item1, item2, ...) merges the sets containing each item
-      into a single larger set.  If any item is not yet part of a set
-      in X, it is added to X as one of the members of the merged set.
-
-      Union-find data structure. Based on Josiah Carlson's code,
-      http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/215912
-      with significant additional changes by D. Eppstein.
-      http://www.ics.uci.edu/~eppstein/PADS/UnionFind.py
-
-    """
-
-    def __init__(self, elements=None):
-        """Create a new empty union-find structure.
-
-        If *elements* is an iterable, this structure will be initialized
-        with the discrete partition on the given set of elements.
-
-        """
-        if elements is None:
-            elements = ()
-        self.parents = {}
-        self.weights = {}
-        for x in elements:
-            self.weights[x] = 1
-            self.parents[x] = x
-
-    def __getitem__(self, object):
-        """Find and return the name of the set containing the object."""
-
-        # check for previously unknown object
-        if object not in self.parents:
-            self.parents[object] = object
-            self.weights[object] = 1
-            return object
-
-        # find basic of objects leading to the root
-        path = [object]
-        root = self.parents[object]
-        while root != path[-1]:
-            path.append(root)
-            root = self.parents[root]
-
-        # compress the basic and return
-        for ancestor in path:
-            self.parents[ancestor] = root
-        return root
-
-    def __iter__(self):
-        """Iterate through all items ever found or unioned by this structure."""
-        return iter(self.parents)
-
-    def to_sets(self):
-        """Iterates over the sets stored in this structure.
-
-        For example::
-
-            >>> partition = UnionFind("xyz")
-            >>> sorted(map(sorted, partition.to_sets()))
-            [['x'], ['y'], ['z']]
-            >>> partition.union("x", "y")
-            >>> sorted(map(sorted, partition.to_sets()))
-            [['x', 'y'], ['z']]
-
-        """
-        # Ensure fully pruned paths
-
-        def groups(parents: dict):
-            sets = {}
-            for v, k in parents.items():
-                if k not in sets:
-                    sets[k] = set()
-                sets[k].add(v)
-            return sets
-
-        for x in self.parents.keys():
-            _ = self[x]  # Evaluated for side-effect only
-
-        yield from groups(self.parents).values()
-
-    def union(self, *objects):
-        """Find the sets containing the objects and merge them all."""
-        # Find the heaviest root according to its weight.
-        roots = iter(sorted({self[x] for x in objects}, key=lambda r: self.weights[r]))
-        try:
-            root = next(roots)
-        except StopIteration:
-            return
-
-        for r in roots:
-            self.weights[root] += self.weights[r]
-            self.parents[r] = root
+from heapq import heappop
+from heapq import heappush
+from itertools import count
+from math import isnan
+from operator import itemgetter
+
+from easygraph.utils.decorators import *
+
+
+__all__ = [
+    "minimum_spanning_edges",
+    "maximum_spanning_edges",
+    "minimum_spanning_tree",
+    "maximum_spanning_tree",
+]
+
+
+def boruvka_mst_edges(G, minimum=True, weight="weight", data=True, ignore_nan=False):
+    """Iterate over edges of a Borůvka's algorithm min/max spanning tree.
+
+    Parameters
+    ----------
+    G : EasyGraph Graph
+        The edges of `G` must have distinct weights,
+        otherwise the edges may not form a tree.
+
+    minimum : bool (default: True)
+        Find the minimum (True) or maximum (False) spanning tree.
+
+    weight : string (default: 'weight')
+        The name of the edge attribute holding the edge weights.
+
+    data : bool (default: True)
+        Flag for whether to yield edge attribute dicts.
+        If True, yield edges `(u, v, d)`, where `d` is the attribute dict.
+        If False, yield edges `(u, v)`.
+
+    ignore_nan : bool (default: False)
+        If a NaN is found as an edge weight normally an exception is raised.
+        If `ignore_nan is True` then that edge is ignored instead.
+
+    """
+    # Initialize a forest, assuming initially that it is the discrete
+    # partition of the nodes of the graph.
+    forest = UnionFind(G)
+
+    def best_edge(component):
+        """Returns the optimum (minimum or maximum) edge on the edge
+        boundary of the given set of nodes.
+
+        A return value of ``None`` indicates an empty boundary.
+
+        """
+        sign = 1 if minimum else -1
+        minwt = float("inf")
+        boundary = None
+        for e in edge_boundary(G, component, data=True):
+            wt = e[-1].get(weight, 1) * sign
+            if isnan(wt):
+                if ignore_nan:
+                    continue
+                msg = f"NaN found as an edge weight. Edge {e}"
+                raise ValueError(msg)
+            if wt < minwt:
+                minwt = wt
+                boundary = e
+        return boundary
+
+    # Determine the optimum edge in the edge boundary of each component
+    # in the forest.
+    best_edges = (best_edge(component) for component in forest.to_sets())
+    best_edges = [edge for edge in best_edges if edge is not None]
+    # If each entry was ``None``, that means the graph was disconnected,
+    # so we are done generating the forest.
+    while best_edges:
+        # Determine the optimum edge in the edge boundary of each
+        # component in the forest.
+        #
+        # This must be a sequence, not an iterator. In this list, the
+        # same edge may appear twice, in different orientations (but
+        # that's okay, since a union operation will be called on the
+        # endpoints the first time it is seen, but not the second time).
+        #
+        # Any ``None`` indicates that the edge boundary for that
+        # component was empty, so that part of the forest has been
+        # completed.
+        #
+        # TODO This can be parallelized, both in the outer loop over
+        # each component in the forest and in the computation of the
+        # minimum. (Same goes for the identical lines outside the loop.)
+        best_edges = (best_edge(component) for component in forest.to_sets())
+        best_edges = [edge for edge in best_edges if edge is not None]
+        # Join trees in the forest using the best edges, and yield that
+        # edge, since it is part of the spanning tree.
+        #
+        # TODO This loop can be parallelized, to an extent (the union
+        # operation must be atomic).
+        for u, v, d in best_edges:
+            if forest[u] != forest[v]:
+                if data:
+                    yield u, v, d
+                else:
+                    yield u, v
+                forest.union(u, v)
+
+
+@hybrid("cpp_kruskal_mst_edges")
+def kruskal_mst_edges(G, minimum=True, weight="weight", data=True, ignore_nan=False):
+    """Iterate over edges of a Kruskal's algorithm min/max spanning tree.
+
+    Parameters
+    ----------
+    G : EasyGraph Graph
+        The graph holding the tree of interest.
+
+    minimum : bool (default: True)
+        Find the minimum (True) or maximum (False) spanning tree.
+
+    weight : string (default: 'weight')
+        The name of the edge attribute holding the edge weights.
+
+    data : bool (default: True)
+        Flag for whether to yield edge attribute dicts.
+        If True, yield edges `(u, v, d)`, where `d` is the attribute dict.
+        If False, yield edges `(u, v)`.
+
+    ignore_nan : bool (default: False)
+        If a NaN is found as an edge weight normally an exception is raised.
+        If `ignore_nan is True` then that edge is ignored instead.
+
+    """
+    subtrees = UnionFind()
+    edges = []
+    for u, v, t in G.edges:
+        edges.append((u, v, t))
+
+    def filter_nan_edges(edges=edges, weight=weight):
+        sign = 1 if minimum else -1
+        for u, v, d in edges:
+            wt = d.get(weight, 1) * sign
+            if isnan(wt):
+                if ignore_nan:
+                    continue
+                msg = f"NaN found as an edge weight. Edge {(u, v, d)}"
+                raise ValueError(msg)
+            yield wt, u, v, d
+
+    edges = sorted(filter_nan_edges(), key=itemgetter(0))
+    for wt, u, v, d in edges:
+        if subtrees[u] != subtrees[v]:
+            if data:
+                yield (u, v, d)
+            else:
+                yield (u, v)
+            subtrees.union(u, v)
+
+
+@hybrid("cpp_prim_mst_edges")
+def prim_mst_edges(G, minimum=True, weight="weight", data=True, ignore_nan=False):
+    """Iterate over edges of Prim's algorithm min/max spanning tree.
+
+    Parameters
+    ----------
+    G : EasyGraph Graph
+        The graph holding the tree of interest.
+
+    minimum : bool (default: True)
+        Find the minimum (True) or maximum (False) spanning tree.
+
+    weight : string (default: 'weight')
+        The name of the edge attribute holding the edge weights.
+
+    data : bool (default: True)
+        Flag for whether to yield edge attribute dicts.
+        If True, yield edges `(u, v, d)`, where `d` is the attribute dict.
+        If False, yield edges `(u, v)`.
+
+    ignore_nan : bool (default: False)
+        If a NaN is found as an edge weight normally an exception is raised.
+        If `ignore_nan is True` then that edge is ignored instead.
+
+    """
+    push = heappush
+    pop = heappop
+
+    nodes = set(G)
+    c = count()
+
+    sign = 1 if minimum else -1
+
+    while nodes:
+        u = nodes.pop()
+        frontier = []
+        visited = {u}
+        for v, d in G.adj[u].items():
+            wt = d.get(weight, 1) * sign
+            if isnan(wt):
+                if ignore_nan:
+                    continue
+                msg = f"NaN found as an edge weight. Edge {(u, v, d)}"
+                raise ValueError(msg)
+            push(frontier, (wt, next(c), u, v, d))
+        while frontier:
+            W, _, u, v, d = pop(frontier)
+            if v in visited or v not in nodes:
+                continue
+            if data:
+                yield u, v, d
+            else:
+                yield u, v
+            # update frontier
+            visited.add(v)
+            nodes.discard(v)
+            for w, d2 in G.adj[v].items():
+                if w in visited:
+                    continue
+                new_weight = d2.get(weight, 1) * sign
+                push(frontier, (new_weight, next(c), v, w, d2))
+
+
+ALGORITHMS = {
+    "boruvka": boruvka_mst_edges,
+    "borůvka": boruvka_mst_edges,
+    "kruskal": kruskal_mst_edges,
+    "prim": prim_mst_edges,
+}
+
+
+@not_implemented_for("multigraph")
+@only_implemented_for_UnDirected_graph
+def minimum_spanning_edges(
+    G, algorithm="kruskal", weight="weight", data=True, ignore_nan=False
+):
+    """Generate edges in a minimum spanning forest of an undirected
+    weighted graph.
+
+    A minimum spanning tree is a subgraph of the graph (a tree)
+    with the minimum sum of edge weights.  A spanning forest is a
+    union of the spanning trees for each connected component of the graph.
+
+    Parameters
+    ----------
+    G : undirected Graph
+       An undirected graph. If `G` is connected, then the algorithm finds a
+       spanning tree. Otherwise, a spanning forest is found.
+
+    algorithm : string
+       The algorithm to use when finding a minimum spanning tree. Valid
+       choices are 'kruskal', 'prim', or 'boruvka'. The default is 'kruskal'.
+
+    weight : string
+       Edge data key to use for weight (default 'weight').
+
+    data : bool, optional
+       If True yield the edge data along with the edge.
+
+    ignore_nan : bool (default: False)
+        If a NaN is found as an edge weight normally an exception is raised.
+        If `ignore_nan is True` then that edge is ignored instead.
+
+    Returns
+    -------
+    edges : iterator
+       An iterator over edges in a maximum spanning tree of `G`.
+       Edges connecting nodes `u` and `v` are represented as tuples:
+       `(u, v, k, d)` or `(u, v, k)` or `(u, v, d)` or `(u, v)`
+
+    Examples
+    --------
+    >>> from easygraph.functions.basic import mst
+
+    Find minimum spanning edges by Kruskal's algorithm
+
+    >>> G.add_edge(0, 3, weight=2)
+    >>> mst = mst.minimum_spanning_edges(G, algorithm="kruskal", data=False)
+    >>> edgelist = list(mst)
+    >>> sorted(sorted(e) for e in edgelist)
+    [[0, 1], [1, 2], [2, 3]]
+
+    Find minimum spanning edges by Prim's algorithm
+
+    >>> G.add_edge(0, 3, weight=2)
+    >>> mst = mst.minimum_spanning_edges(G, algorithm="prim", data=False)
+    >>> edgelist = list(mst)
+    >>> sorted(sorted(e) for e in edgelist)
+    [[0, 1], [1, 2], [2, 3]]
+
+    Notes
+    -----
+    For Borůvka's algorithm, each edge must have a weight attribute, and
+    each edge weight must be distinct.
+
+    For the other algorithms, if the graph edges do not have a weight
+    attribute a default weight of 1 will be used.
+
+    Modified code from David Eppstein, April 2006
+    http://www.ics.uci.edu/~eppstein/PADS/
+
+    """
+    try:
+        algo = ALGORITHMS[algorithm]
+    except KeyError as e:
+        msg = f"{algorithm} is not a valid choice for an algorithm."
+        raise ValueError(msg) from e
+
+    return algo(G, minimum=True, weight=weight, data=data, ignore_nan=ignore_nan)
+
+
+@not_implemented_for("multigraph")
+@only_implemented_for_UnDirected_graph
+def maximum_spanning_edges(
+    G, algorithm="kruskal", weight="weight", data=True, ignore_nan=False
+):
+    """Generate edges in a maximum spanning forest of an undirected
+    weighted graph.
+
+    A maximum spanning tree is a subgraph of the graph (a tree)
+    with the maximum possible sum of edge weights.  A spanning forest is a
+    union of the spanning trees for each connected component of the graph.
+
+    Parameters
+    ----------
+    G : undirected Graph
+       An undirected graph. If `G` is connected, then the algorithm finds a
+       spanning tree. Otherwise, a spanning forest is found.
+
+    algorithm : string
+       The algorithm to use when finding a maximum spanning tree. Valid
+       choices are 'kruskal', 'prim', or 'boruvka'. The default is 'kruskal'.
+
+    weight : string
+       Edge data key to use for weight (default 'weight').
+
+    data : bool, optional
+       If True yield the edge data along with the edge.
+
+    ignore_nan : bool (default: False)
+        If a NaN is found as an edge weight normally an exception is raised.
+        If `ignore_nan is True` then that edge is ignored instead.
+
+    Returns
+    -------
+    edges : iterator
+       An iterator over edges in a maximum spanning tree of `G`.
+       Edges connecting nodes `u` and `v` are represented as tuples:
+       `(u, v, k, d)` or `(u, v, k)` or `(u, v, d)` or `(u, v)`
+
+    Examples
+    --------
+    >>> from easygraph.functions.basic import mst
+
+    Find maximum spanning edges by Kruskal's algorithm
+
+    >>> G.add_edge(0, 3, weight=2)
+    >>> mst = mst.maximum_spanning_edges(G, algorithm="kruskal", data=False)
+    >>> edgelist = list(mst)
+    >>> sorted(sorted(e) for e in edgelist)
+    [[0, 1], [0, 3], [1, 2]]
+
+    Find maximum spanning edges by Prim's algorithm
+
+    >>> G.add_edge(0, 3, weight=2)  # assign weight 2 to edge 0-3
+    >>> mst = mst.maximum_spanning_edges(G, algorithm="prim", data=False)
+    >>> edgelist = list(mst)
+    >>> sorted(sorted(e) for e in edgelist)
+    [[0, 1], [0, 3], [2, 3]]
+
+    Notes
+    -----
+    For Borůvka's algorithm, each edge must have a weight attribute, and
+    each edge weight must be distinct.
+
+    For the other algorithms, if the graph edges do not have a weight
+    attribute a default weight of 1 will be used.
+
+    Modified code from David Eppstein, April 2006
+    http://www.ics.uci.edu/~eppstein/PADS/
+    """
+    try:
+        algo = ALGORITHMS[algorithm]
+    except KeyError as e:
+        msg = f"{algorithm} is not a valid choice for an algorithm."
+        raise ValueError(msg) from e
+
+    return algo(G, minimum=False, weight=weight, data=data, ignore_nan=ignore_nan)
+
+
+@not_implemented_for("multigraph")
+def minimum_spanning_tree(G, weight="weight", algorithm="kruskal", ignore_nan=False):
+    """Returns a minimum spanning tree or forest on an undirected graph `G`.
+
+    Parameters
+    ----------
+    G : undirected graph
+        An undirected graph. If `G` is connected, then the algorithm finds a
+        spanning tree. Otherwise, a spanning forest is found.
+
+    weight : str
+       Data key to use for edge weights.
+
+    algorithm : string
+       The algorithm to use when finding a minimum spanning tree. Valid
+       choices are 'kruskal', 'prim', or 'boruvka'. The default is
+       'kruskal'.
+
+    ignore_nan : bool (default: False)
+        If a NaN is found as an edge weight normally an exception is raised.
+        If `ignore_nan is True` then that edge is ignored instead.
+
+    Returns
+    -------
+    G : EasyGraph Graph
+       A minimum spanning tree or forest.
+
+    Examples
+    --------
+    >>> G.add_edge(0, 3, weight=2)
+    >>> T = eg.minimum_spanning_tree(G)
+    >>> sorted(T.edges(data=True))
+    [(0, 1, {}), (1, 2, {}), (2, 3, {})]
+
+
+    Notes
+    -----
+    For Borůvka's algorithm, each edge must have a weight attribute, and
+    each edge weight must be distinct.
+
+    For the other algorithms, if the graph edges do not have a weight
+    attribute a default weight of 1 will be used.
+
+    Isolated nodes with self-loops are in the tree as edgeless isolated nodes.
+
+    """
+    edges = list(
+        minimum_spanning_edges(G, algorithm, weight, data=True, ignore_nan=ignore_nan)
+    )
+    T = G.__class__()  # Same graph class as G
+    for i in G.nodes:
+        T.add_node(i)
+    for i in edges:
+        (u, v, t) = i
+        T.add_edge(u, v, **t)
+    return T
+
+
+@not_implemented_for("multigraph")
+def maximum_spanning_tree(G, weight="weight", algorithm="kruskal", ignore_nan=False):
+    """Returns a maximum spanning tree or forest on an undirected graph `G`.
+
+    Parameters
+    ----------
+    G : undirected graph
+        An undirected graph. If `G` is connected, then the algorithm finds a
+        spanning tree. Otherwise, a spanning forest is found.
+
+    weight : str
+       Data key to use for edge weights.
+
+    algorithm : string
+       The algorithm to use when finding a maximum spanning tree. Valid
+       choices are 'kruskal', 'prim', or 'boruvka'. The default is
+       'kruskal'.
+
+    ignore_nan : bool (default: False)
+        If a NaN is found as an edge weight normally an exception is raised.
+        If `ignore_nan is True` then that edge is ignored instead.
+
+
+    Returns
+    -------
+    G : EasyGraph Graph
+       A maximum spanning tree or forest.
+
+
+    Examples
+    --------
+    >>> G.add_edge(0, 3, weight=2)
+    >>> T = eg.maximum_spanning_tree(G)
+    >>> sorted(T.edges(data=True))
+    [(0, 1, {}), (0, 3, {'weight': 2}), (1, 2, {})]
+
+
+    Notes
+    -----
+    For Borůvka's algorithm, each edge must have a weight attribute, and
+    each edge weight must be distinct.
+
+    For the other algorithms, if the graph edges do not have a weight
+    attribute a default weight of 1 will be used.
+
+    There may be more than one tree with the same minimum or maximum weight.
+    See :mod:`easygraph.tree.recognition` for more detailed definitions.
+
+    Isolated nodes with self-loops are in the tree as edgeless isolated nodes.
+
+    """
+    edges = list(
+        maximum_spanning_edges(G, algorithm, weight, data=True, ignore_nan=ignore_nan)
+    )
+    T = G.__class__()  # Same graph class as G
+    for i in G.nodes:
+        T.add_node(i)
+    for i in edges:
+        (u, v, t) = i
+        T.add_edge(u, v, **t)
+    return T
+
+
+def edge_boundary(G, nbunch1, nbunch2=None, data=False, default=None):
+    """Returns the edge boundary of `nbunch1`.
+
+    The *edge boundary* of a set *S* with respect to a set *T* is the
+    set of edges (*u*, *v*) such that *u* is in *S* and *v* is in *T*.
+    If *T* is not specified, it is assumed to be the set of all nodes
+    not in *S*.
+
+    Parameters
+    ----------
+    G : EasyGraph graph
+
+    nbunch1 : iterable
+        Iterable of nodes in the graph representing the set of nodes
+        whose edge boundary will be returned. (This is the set *S* from
+        the definition above.)
+
+    nbunch2 : iterable
+        Iterable of nodes representing the target (or "exterior") set of
+        nodes. (This is the set *T* from the definition above.) If not
+        specified, this is assumed to be the set of all nodes in `G`
+        not in `nbunch1`.
+
+    data : bool or object
+        This parameter has the same meaning as in
+        :meth:`MultiGraph.edges`.
+
+    default : object
+        This parameter has the same meaning as in
+        :meth:`MultiGraph.edges`.
+
+    Returns
+    -------
+    iterator
+        An iterator over the edges in the boundary of `nbunch1` with
+        respect to `nbunch2`. If `keys`, `data`, or `default`
+        are specified and `G` is a multigraph, then edges are returned
+        with keys and/or data, as in :meth:`MultiGraph.edges`.
+
+    Notes
+    -----
+    Any element of `nbunch` that is not in the graph `G` will be
+    ignored.
+
+    `nbunch1` and `nbunch2` are usually meant to be disjoint, but in
+    the interest of speed and generality, that is not required here.
+
+    """
+    nset1 = {v for v in G if v in nbunch1}
+    # Here we create an iterator over edges incident to nodes in the set
+    # `nset1`. The `Graph.edges()` method does not provide a guarantee
+    # on the orientation of the edges, so our algorithm below must
+    # handle the case in which exactly one orientation, either (u, v) or
+    # (v, u), appears in this iterable.
+    edges = G.edges(nset1, data=data, default=default)
+    # If `nbunch2` is not provided, then it is assumed to be the set
+    # complement of `nbunch1`. For the sake of efficiency, this is
+    # implemented by using the `not in` operator, instead of by creating
+    # an additional set and using the `in` operator.
+    if nbunch2 is None:
+        return (e for e in edges if (e[0] in nset1) ^ (e[1] in nset1))
+    nset2 = set(nbunch2)
+    return (
+        e
+        for e in edges
+        if (e[0] in nset1 and e[1] in nset2) or (e[1] in nset1 and e[0] in nset2)
+    )
+
+
+"""
+Union-find data structure.
+"""
+
+
+class UnionFind:
+    """Union-find data structure.
+
+    Each unionFind instance X maintains a family of disjoint sets of
+    hashable objects, supporting the following two methods:
+
+    - X[item] returns a name for the set containing the given item.
+      Each set is named by an arbitrarily-chosen one of its members; as
+      long as the set remains unchanged it will keep the same name. If
+      the item is not yet part of a set in X, a new singleton set is
+      created for it.
+
+    - X.union(item1, item2, ...) merges the sets containing each item
+      into a single larger set.  If any item is not yet part of a set
+      in X, it is added to X as one of the members of the merged set.
+
+      Union-find data structure. Based on Josiah Carlson's code,
+      http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/215912
+      with significant additional changes by D. Eppstein.
+      http://www.ics.uci.edu/~eppstein/PADS/UnionFind.py
+
+    """
+
+    def __init__(self, elements=None):
+        """Create a new empty union-find structure.
+
+        If *elements* is an iterable, this structure will be initialized
+        with the discrete partition on the given set of elements.
+
+        """
+        if elements is None:
+            elements = ()
+        self.parents = {}
+        self.weights = {}
+        for x in elements:
+            self.weights[x] = 1
+            self.parents[x] = x
+
+    def __getitem__(self, object):
+        """Find and return the name of the set containing the object."""
+
+        # check for previously unknown object
+        if object not in self.parents:
+            self.parents[object] = object
+            self.weights[object] = 1
+            return object
+
+        # find basic of objects leading to the root
+        path = [object]
+        root = self.parents[object]
+        while root != path[-1]:
+            path.append(root)
+            root = self.parents[root]
+
+        # compress the basic and return
+        for ancestor in path:
+            self.parents[ancestor] = root
+        return root
+
+    def __iter__(self):
+        """Iterate through all items ever found or unioned by this structure."""
+        return iter(self.parents)
+
+    def to_sets(self):
+        """Iterates over the sets stored in this structure.
+
+        For example::
+
+            >>> partition = UnionFind("xyz")
+            >>> sorted(map(sorted, partition.to_sets()))
+            [['x'], ['y'], ['z']]
+            >>> partition.union("x", "y")
+            >>> sorted(map(sorted, partition.to_sets()))
+            [['x', 'y'], ['z']]
+
+        """
+        # Ensure fully pruned paths
+
+        def groups(parents: dict):
+            sets = {}
+            for v, k in parents.items():
+                if k not in sets:
+                    sets[k] = set()
+                sets[k].add(v)
+            return sets
+
+        for x in self.parents.keys():
+            _ = self[x]  # Evaluated for side-effect only
+
+        yield from groups(self.parents).values()
+
+    def union(self, *objects):
+        """Find the sets containing the objects and merge them all."""
+        # Find the heaviest root according to its weight.
+        roots = iter(sorted({self[x] for x in objects}, key=lambda r: self.weights[r]))
+        try:
+            root = next(roots)
+        except StopIteration:
+            return
+
+        for r in roots:
+            self.weights[root] += self.weights[r]
+            self.parents[r] = root
```

## easygraph/functions/path/bridges.py

 * *Ordering differences only*

```diff
@@ -1,199 +1,199 @@
-from itertools import chain
-
-import easygraph as eg
-
-from easygraph.utils.decorators import *
-
-
-__all__ = ["bridges", "has_bridges"]
-
-
-@not_implemented_for("multigraph")
-@only_implemented_for_UnDirected_graph
-def bridges(G, root=None):
-    """Generate all bridges in a graph.
-
-    A *bridge* in a graph is an edge whose removal causes the number of
-    connected components of the graph to increase.  Equivalently, a bridge is an
-    edge that does not belong to any cycle.
-
-    Parameters
-    ----------
-    G : undirected graph
-
-    root : node (optional)
-       A node in the graph `G`. If specified, only the bridges in the
-       connected component containing this node will be returned.
-
-    Yields
-    ------
-    e : edge
-       An edge in the graph whose removal disconnects the graph (or
-       causes the number of connected components to increase).
-
-    Raises
-    ------
-    NodeNotFound
-       If `root` is not in the graph `G`.
-
-    Examples
-    --------
-
-    >>> list(eg.bridges(G))
-    [(9, 10)]
-
-    Notes
-    -----
-    This is an implementation of the algorithm described in _[1].  An edge is a
-    bridge if and only if it is not contained in any chain. Chains are found
-    using the :func:`chain_decomposition` function.
-
-    Ignoring polylogarithmic factors, the worst-case time complexity is the
-    same as the :func:`chain_decomposition` function,
-    $O(m + n)$, where $n$ is the number of nodes in the graph and $m$ is
-    the number of edges.
-
-    References
-    ----------
-    .. [1] https://en.wikipedia.org/wiki/Bridge_%28graph_theory%29#Bridge-Finding_with_Chain_Decompositions
-    """
-    chains = chain_decomposition(G, root=root)
-    chain_edges = set(chain.from_iterable(chains))
-    for u, v, t in G.edges:
-        if (u, v) not in chain_edges and (v, u) not in chain_edges:
-            yield u, v
-
-
-@not_implemented_for("multigraph")
-@only_implemented_for_UnDirected_graph
-def has_bridges(G, root=None):
-    """Decide whether a graph has any bridges.
-
-    A *bridge* in a graph is an edge whose removal causes the number of
-    connected components of the graph to increase.
-
-    Parameters
-    ----------
-    G : undirected graph
-
-    root : node (optional)
-       A node in the graph `G`. If specified, only the bridges in the
-       connected component containing this node will be considered.
-
-    Returns
-    -------
-    bool
-       Whether the graph (or the connected component containing `root`)
-       has any bridges.
-
-    Raises
-    ------
-    NodeNotFound
-       If `root` is not in the graph `G`.
-
-    Examples
-    --------
-
-    >>> eg.has_bridges(G)
-    True
-
-    Notes
-    -----
-    This implementation uses the :func:`easygraph.bridges` function, so
-    it shares its worst-case time complexity, $O(m + n)$, ignoring
-    polylogarithmic factors, where $n$ is the number of nodes in the
-    graph and $m$ is the number of edges.
-
-    """
-    try:
-        next(bridges(G))
-    except StopIteration:
-        return False
-    else:
-        return True
-
-
-def chain_decomposition(G, root=None):
-    def _dfs_cycle_forest(G, root=None):
-        H = eg.DiGraph()
-        nodes = []
-        for u, v, d in dfs_labeled_edges(G, source=root):
-            if d == "forward":
-                # `dfs_labeled_edges()` yields (root, root, 'forward')
-                # if it is beginning the search on a new connected
-                # component.
-                if u == v:
-                    H.add_node(v, parent=None)
-                    nodes.append(v)
-                else:
-                    H.add_node(v, parent=u)
-                    H.add_edge(v, u, nontree=False)
-                    nodes.append(v)
-            # `dfs_labeled_edges` considers nontree edges in both
-            # orientations, so we need to not add the edge if it its
-            # other orientation has been added.
-            elif d == "nontree" and v not in H[u]:
-                H.add_edge(v, u, nontree=True)
-            else:
-                # Do nothing on 'reverse' edges; we only care about
-                # forward and nontree edges.
-                pass
-        return H, nodes
-
-    def _build_chain(G, u, v, visited):
-        while v not in visited:
-            yield u, v
-            visited.add(v)
-            u, v = v, G.nodes[v]["parent"]
-        yield u, v
-
-    H, nodes = _dfs_cycle_forest(G, root)
-
-    visited = set()
-    for u in nodes:
-        visited.add(u)
-        # For each nontree edge going out of node u...
-        edges = []
-        for w, v, d in H.edges:
-            if w == u and d["nontree"] == True:
-                edges.append((w, v))
-        # edges = ((u, v) for u, v, d in H.out_edges(u, data="nontree") if d)
-        for u, v in edges:
-            # Create the cycle or cycle prefix starting with the
-            # nontree edge.
-            chain = list(_build_chain(H, u, v, visited))
-            yield chain
-
-
-def dfs_labeled_edges(G, source=None, depth_limit=None):
-    if source is None:
-        # edges for all components
-        nodes = G
-    else:
-        # edges for components with source
-        nodes = [source]
-    visited = set()
-    if depth_limit is None:
-        depth_limit = len(G)
-    for start in nodes:
-        if start in visited:
-            continue
-        yield start, start, "forward"
-        visited.add(start)
-        stack = [(start, depth_limit, iter(G[start]))]
-        while stack:
-            parent, depth_now, children = stack[-1]
-            try:
-                child = next(children)
-                if child in visited:
-                    yield parent, child, "nontree"
-                else:
-                    yield parent, child, "forward"
-                    visited.add(child)
-                    if depth_now > 1:
-                        stack.append((child, depth_now - 1, iter(G[child])))
-            except StopIteration:
-                stack.pop()
-                if stack:
-                    yield stack[-1][0], parent, "reverse"
-        yield start, start, "reverse"
+from itertools import chain
+
+import easygraph as eg
+
+from easygraph.utils.decorators import *
+
+
+__all__ = ["bridges", "has_bridges"]
+
+
+@not_implemented_for("multigraph")
+@only_implemented_for_UnDirected_graph
+def bridges(G, root=None):
+    """Generate all bridges in a graph.
+
+    A *bridge* in a graph is an edge whose removal causes the number of
+    connected components of the graph to increase.  Equivalently, a bridge is an
+    edge that does not belong to any cycle.
+
+    Parameters
+    ----------
+    G : undirected graph
+
+    root : node (optional)
+       A node in the graph `G`. If specified, only the bridges in the
+       connected component containing this node will be returned.
+
+    Yields
+    ------
+    e : edge
+       An edge in the graph whose removal disconnects the graph (or
+       causes the number of connected components to increase).
+
+    Raises
+    ------
+    NodeNotFound
+       If `root` is not in the graph `G`.
+
+    Examples
+    --------
+
+    >>> list(eg.bridges(G))
+    [(9, 10)]
+
+    Notes
+    -----
+    This is an implementation of the algorithm described in _[1].  An edge is a
+    bridge if and only if it is not contained in any chain. Chains are found
+    using the :func:`chain_decomposition` function.
+
+    Ignoring polylogarithmic factors, the worst-case time complexity is the
+    same as the :func:`chain_decomposition` function,
+    $O(m + n)$, where $n$ is the number of nodes in the graph and $m$ is
+    the number of edges.
+
+    References
+    ----------
+    .. [1] https://en.wikipedia.org/wiki/Bridge_%28graph_theory%29#Bridge-Finding_with_Chain_Decompositions
+    """
+    chains = chain_decomposition(G, root=root)
+    chain_edges = set(chain.from_iterable(chains))
+    for u, v, t in G.edges:
+        if (u, v) not in chain_edges and (v, u) not in chain_edges:
+            yield u, v
+
+
+@not_implemented_for("multigraph")
+@only_implemented_for_UnDirected_graph
+def has_bridges(G, root=None):
+    """Decide whether a graph has any bridges.
+
+    A *bridge* in a graph is an edge whose removal causes the number of
+    connected components of the graph to increase.
+
+    Parameters
+    ----------
+    G : undirected graph
+
+    root : node (optional)
+       A node in the graph `G`. If specified, only the bridges in the
+       connected component containing this node will be considered.
+
+    Returns
+    -------
+    bool
+       Whether the graph (or the connected component containing `root`)
+       has any bridges.
+
+    Raises
+    ------
+    NodeNotFound
+       If `root` is not in the graph `G`.
+
+    Examples
+    --------
+
+    >>> eg.has_bridges(G)
+    True
+
+    Notes
+    -----
+    This implementation uses the :func:`easygraph.bridges` function, so
+    it shares its worst-case time complexity, $O(m + n)$, ignoring
+    polylogarithmic factors, where $n$ is the number of nodes in the
+    graph and $m$ is the number of edges.
+
+    """
+    try:
+        next(bridges(G))
+    except StopIteration:
+        return False
+    else:
+        return True
+
+
+def chain_decomposition(G, root=None):
+    def _dfs_cycle_forest(G, root=None):
+        H = eg.DiGraph()
+        nodes = []
+        for u, v, d in dfs_labeled_edges(G, source=root):
+            if d == "forward":
+                # `dfs_labeled_edges()` yields (root, root, 'forward')
+                # if it is beginning the search on a new connected
+                # component.
+                if u == v:
+                    H.add_node(v, parent=None)
+                    nodes.append(v)
+                else:
+                    H.add_node(v, parent=u)
+                    H.add_edge(v, u, nontree=False)
+                    nodes.append(v)
+            # `dfs_labeled_edges` considers nontree edges in both
+            # orientations, so we need to not add the edge if it its
+            # other orientation has been added.
+            elif d == "nontree" and v not in H[u]:
+                H.add_edge(v, u, nontree=True)
+            else:
+                # Do nothing on 'reverse' edges; we only care about
+                # forward and nontree edges.
+                pass
+        return H, nodes
+
+    def _build_chain(G, u, v, visited):
+        while v not in visited:
+            yield u, v
+            visited.add(v)
+            u, v = v, G.nodes[v]["parent"]
+        yield u, v
+
+    H, nodes = _dfs_cycle_forest(G, root)
+
+    visited = set()
+    for u in nodes:
+        visited.add(u)
+        # For each nontree edge going out of node u...
+        edges = []
+        for w, v, d in H.edges:
+            if w == u and d["nontree"] == True:
+                edges.append((w, v))
+        # edges = ((u, v) for u, v, d in H.out_edges(u, data="nontree") if d)
+        for u, v in edges:
+            # Create the cycle or cycle prefix starting with the
+            # nontree edge.
+            chain = list(_build_chain(H, u, v, visited))
+            yield chain
+
+
+def dfs_labeled_edges(G, source=None, depth_limit=None):
+    if source is None:
+        # edges for all components
+        nodes = G
+    else:
+        # edges for components with source
+        nodes = [source]
+    visited = set()
+    if depth_limit is None:
+        depth_limit = len(G)
+    for start in nodes:
+        if start in visited:
+            continue
+        yield start, start, "forward"
+        visited.add(start)
+        stack = [(start, depth_limit, iter(G[start]))]
+        while stack:
+            parent, depth_now, children = stack[-1]
+            try:
+                child = next(children)
+                if child in visited:
+                    yield parent, child, "nontree"
+                else:
+                    yield parent, child, "forward"
+                    visited.add(child)
+                    if depth_now > 1:
+                        stack.append((child, depth_now - 1, iter(G[child])))
+            except StopIteration:
+                stack.pop()
+                if stack:
+                    yield stack[-1][0], parent, "reverse"
+        yield start, start, "reverse"
```

## easygraph/functions/path/average_shortest_path_length.py

 * *Ordering differences only*

```diff
@@ -1,115 +1,115 @@
-import warnings
-
-import easygraph as eg
-
-from easygraph.functions.path.path import *
-
-
-def average_shortest_path_length(G, weight=None, method=None):
-    r"""Returns the average shortest path length.
-
-    The average shortest path length is
-
-    .. math::
-
-       a =\sum_{\substack{s,t \in V \\ s\neq t}} \frac{d(s, t)}{n(n-1)}
-
-    where `V` is the set of nodes in `G`,
-    `d(s, t)` is the shortest path from `s` to `t`,
-    and `n` is the number of nodes in `G`.
-
-    .. versionchanged:: 3.0
-       An exception is raised for directed graphs that are not strongly
-       connected.
-
-    Parameters
-    ----------
-    G : EasyGraph graph
-
-    weight : None, string or function, optional (default = None)
-        If None, every edge has weight/distance/cost 1.
-        If a string, use this edge attribute as the edge weight.
-        Any edge attribute not present defaults to 1.
-        If this is a function, the weight of an edge is the value
-        returned by the function. The function must accept exactly
-        three positional arguments: the two endpoints of an edge and
-        the dictionary of edge attributes for that edge.
-        The function must return a number.
-
-    method : string, optional (default = 'unweighted' or 'dijkstra')
-        The algorithm to use to compute the path lengths.
-        Supported options are 'unweighted', 'dijkstra', 'bellman-ford',
-        'floyd-warshall' and 'floyd-warshall-numpy'.
-        Other method values produce a ValueError.
-        The default method is 'unweighted' if `weight` is None,
-        otherwise the default method is 'dijkstra'.
-
-    Raises
-    ------
-    NetworkXPointlessConcept
-        If `G` is the null graph (that is, the graph on zero nodes).
-
-    NetworkXError
-        If `G` is not connected (or not strongly connected, in the case
-        of a directed graph).
-
-    ValueError
-        If `method` is not among the supported options.
-
-    Examples
-    --------
-    >>> G = eg.path_graph(5)
-    >>> eg.average_shortest_path_length(G)
-    2.0
-
-    For disconnected graphs, you can compute the average shortest path
-    length for each component
-
-    >>> G = eg.Graph([(1, 2), (3, 4)])
-    >>> for C in (G.subgraph(c).copy() for c in eg.connected_components(G)):
-    ...     print(eg.average_shortest_path_length(C))
-    1.0
-    1.0
-
-    """
-    single_source_methods = ["single_source_bfs", "Dijkstra"]
-    all_pairs_methods = ["Floyed"]
-    supported_methods = single_source_methods + all_pairs_methods
-
-    if method is None:
-        method = "single_source_bfs" if weight is None else "dijkstra"
-    if method not in supported_methods:
-        raise ValueError(f"method not supported: {method}")
-
-    n = len(G)
-    # For the special case of the null graph, raise an exception, since
-    # there are no paths in the null graph.
-    if n == 0:
-        msg = (
-            "the null graph has no paths, thus there is no average shortest path length"
-        )
-        raise eg.EasyGraphPointlessConcept(msg)
-    # For the special case of the trivial graph, return zero immediately.
-    if n == 1:
-        return 0
-    # Shortest path length is undefined if the graph is not strongly connected.
-    if G.is_directed() and not eg.is_strongly_connected(G):
-        raise eg.EasyGraphError("Graph is not strongly connected.")
-    # Shortest path length is undefined if the graph is not connected.
-    if not G.is_directed() and not eg.is_connected(G):
-        raise eg.EasyGraphError("Graph is not connected.")
-
-    # Compute all-pairs shortest paths.
-    def path_length(v):
-        if method == "single_source_bfs":
-            return eg.single_source_bfs(G, v)
-        elif method == "dijkstra":
-            return eg.Dijkstra(G, v, weight=weight)
-
-    if method in single_source_methods:
-        # Sum the distances for each (ordered) pair of source and target node.
-        s = sum(l for u in G for l in path_length(u).values())
-    else:
-        all_pairs = eg.Floyed(G, weight=weight)
-        s = sum(sum(t.values()) for t in all_pairs.values())
-    return s / (n * (n - 1))
+import warnings
+
+import easygraph as eg
+
+from easygraph.functions.path.path import *
+
+
+def average_shortest_path_length(G, weight=None, method=None):
+    r"""Returns the average shortest path length.
+
+    The average shortest path length is
+
+    .. math::
+
+       a =\sum_{\substack{s,t \in V \\ s\neq t}} \frac{d(s, t)}{n(n-1)}
+
+    where `V` is the set of nodes in `G`,
+    `d(s, t)` is the shortest path from `s` to `t`,
+    and `n` is the number of nodes in `G`.
+
+    .. versionchanged:: 3.0
+       An exception is raised for directed graphs that are not strongly
+       connected.
+
+    Parameters
+    ----------
+    G : EasyGraph graph
+
+    weight : None, string or function, optional (default = None)
+        If None, every edge has weight/distance/cost 1.
+        If a string, use this edge attribute as the edge weight.
+        Any edge attribute not present defaults to 1.
+        If this is a function, the weight of an edge is the value
+        returned by the function. The function must accept exactly
+        three positional arguments: the two endpoints of an edge and
+        the dictionary of edge attributes for that edge.
+        The function must return a number.
+
+    method : string, optional (default = 'unweighted' or 'dijkstra')
+        The algorithm to use to compute the path lengths.
+        Supported options are 'unweighted', 'dijkstra', 'bellman-ford',
+        'floyd-warshall' and 'floyd-warshall-numpy'.
+        Other method values produce a ValueError.
+        The default method is 'unweighted' if `weight` is None,
+        otherwise the default method is 'dijkstra'.
+
+    Raises
+    ------
+    NetworkXPointlessConcept
+        If `G` is the null graph (that is, the graph on zero nodes).
+
+    NetworkXError
+        If `G` is not connected (or not strongly connected, in the case
+        of a directed graph).
+
+    ValueError
+        If `method` is not among the supported options.
+
+    Examples
+    --------
+    >>> G = eg.path_graph(5)
+    >>> eg.average_shortest_path_length(G)
+    2.0
+
+    For disconnected graphs, you can compute the average shortest path
+    length for each component
+
+    >>> G = eg.Graph([(1, 2), (3, 4)])
+    >>> for C in (G.subgraph(c).copy() for c in eg.connected_components(G)):
+    ...     print(eg.average_shortest_path_length(C))
+    1.0
+    1.0
+
+    """
+    single_source_methods = ["single_source_bfs", "Dijkstra"]
+    all_pairs_methods = ["Floyed"]
+    supported_methods = single_source_methods + all_pairs_methods
+
+    if method is None:
+        method = "single_source_bfs" if weight is None else "dijkstra"
+    if method not in supported_methods:
+        raise ValueError(f"method not supported: {method}")
+
+    n = len(G)
+    # For the special case of the null graph, raise an exception, since
+    # there are no paths in the null graph.
+    if n == 0:
+        msg = (
+            "the null graph has no paths, thus there is no average shortest path length"
+        )
+        raise eg.EasyGraphPointlessConcept(msg)
+    # For the special case of the trivial graph, return zero immediately.
+    if n == 1:
+        return 0
+    # Shortest path length is undefined if the graph is not strongly connected.
+    if G.is_directed() and not eg.is_strongly_connected(G):
+        raise eg.EasyGraphError("Graph is not strongly connected.")
+    # Shortest path length is undefined if the graph is not connected.
+    if not G.is_directed() and not eg.is_connected(G):
+        raise eg.EasyGraphError("Graph is not connected.")
+
+    # Compute all-pairs shortest paths.
+    def path_length(v):
+        if method == "single_source_bfs":
+            return eg.single_source_bfs(G, v)
+        elif method == "dijkstra":
+            return eg.Dijkstra(G, v, weight=weight)
+
+    if method in single_source_methods:
+        # Sum the distances for each (ordered) pair of source and target node.
+        s = sum(l for u in G for l in path_length(u).values())
+    else:
+        all_pairs = eg.Floyed(G, weight=weight)
+        s = sum(sum(t.values()) for t in all_pairs.values())
+    return s / (n * (n - 1))
```

## easygraph/functions/path/path.py

 * *Ordering differences only*

```diff
@@ -1,262 +1,262 @@
-from easygraph.utils import *
-from easygraph.utils.decorators import *
-
-
-__all__ = [
-    "Dijkstra",
-    "Floyd",
-    "Prim",
-    "Kruskal",
-    "Spfa",
-    "single_source_bfs",
-    "single_source_dijkstra",
-    "multi_source_dijkstra",
-]
-
-
-@hybrid("cpp_spfa")
-def Spfa(G, node, weight="weight"):
-    raise EasyGraphError("Please input GraphC or DiGraphC.")
-
-
-@not_implemented_for("multigraph")
-def Dijkstra(G, node, weight="weight"):
-    """Returns the length of paths from the certain node to remaining nodes
-
-    Parameters
-    ----------
-    G : graph
-        weighted graph
-    node : int
-
-    Returns
-    -------
-    result_dict : dict
-        the length of paths from the certain node to remaining nodes
-
-    Examples
-    --------
-    Returns the length of paths from node 1 to remaining nodes
-
-    >>> Dijkstra(G,node=1,weight="weight")
-
-    """
-    return single_source_dijkstra(G, node, weight=weight)
-
-
-@not_implemented_for("multigraph")
-@only_implemented_for_UnDirected_graph
-@hybrid("cpp_Floyd")
-def Floyd(G, weight="weight"):
-    """Returns the length of paths from all nodes to remaining nodes
-
-    Parameters
-    ----------
-    G : graph
-        weighted graph
-
-    Returns
-    -------
-    result_dict : dict
-        the length of paths from all nodes to remaining nodes
-
-    Examples
-    --------
-    Returns the length of paths from all nodes to remaining nodes
-
-    >>> Floyd(G,weight="weight")
-
-    """
-    adj = G.adj.copy()
-    result_dict = {}
-    for i in G:
-        result_dict[i] = {}
-    for i in G:
-        temp_key = adj[i].keys()
-        for j in G:
-            if j in temp_key:
-                result_dict[i][j] = adj[i][j].get(weight, 1)
-            else:
-                result_dict[i][j] = float("inf")
-            if i == j:
-                result_dict[i][i] = 0
-    for k in G:
-        for i in G:
-            for j in G:
-                temp = result_dict[i][k] + result_dict[k][j]
-                if result_dict[i][j] > temp:
-                    result_dict[i][j] = temp
-    return result_dict
-
-
-@not_implemented_for("multigraph")
-@only_implemented_for_UnDirected_graph
-@hybrid("cpp_Prim")
-def Prim(G, weight="weight"):
-    """Returns the edges that make up the minimum spanning tree
-
-    Parameters
-    ----------
-    G : graph
-        weighted graph
-
-    Returns
-    -------
-    result_dict : dict
-        the edges that make up the minimum spanning tree
-
-    Examples
-    --------
-    Returns the edges that make up the minimum spanning tree
-
-    >>> Prim(G,weight="weight")
-
-    """
-    adj = G.adj.copy()
-    result_dict = {}
-    for i in G:
-        result_dict[i] = {}
-    selected = []
-    candidate = []
-    for i in G:
-        if not selected:
-            selected.append(i)
-        else:
-            candidate.append(i)
-    while len(candidate):
-        start = None
-        end = None
-        min_weight = float("inf")
-        for i in selected:
-            for j in candidate:
-                if i in G and j in G[i] and adj[i][j].get(weight, 1) < min_weight:
-                    start = i
-                    end = j
-                    min_weight = adj[i][j].get(weight, 1)
-        if start != None and end != None:
-            result_dict[start][end] = min_weight
-            selected.append(end)
-            candidate.remove(end)
-        else:
-            break
-    return result_dict
-
-
-@not_implemented_for("multigraph")
-@only_implemented_for_UnDirected_graph
-@hybrid("cpp_Kruskal")
-def Kruskal(G, weight="weight"):
-    """Returns the edges that make up the minimum spanning tree
-
-    Parameters
-    ----------
-    G : graph
-        weighted graph
-
-    Returns
-    -------
-    result_dict : dict
-        the edges that make up the minimum spanning tree
-
-    Examples
-    --------
-    Returns the edges that make up the minimum spanning tree
-
-    >>> Kruskal(G,weight="weight")
-
-    """
-    adj = G.adj.copy()
-    result_dict = {}
-    edge_list = []
-    for i in G:
-        result_dict[i] = {}
-    for i in G:
-        for j in G[i]:
-            wt = adj[i][j].get(weight, 1)
-            edge_list.append([i, j, wt])
-    edge_list.sort(key=lambda a: a[2])
-    group = [[i] for i in G]
-    for edge in edge_list:
-        for i in range(len(group)):
-            if edge[0] in group[i]:
-                m = i
-            if edge[1] in group[i]:
-                n = i
-        if m != n:
-            result_dict[edge[0]][edge[1]] = edge[2]
-            group[m] = group[m] + group[n]
-            group[n] = []
-    return result_dict
-
-
-@not_implemented_for("multigraph")
-def single_source_bfs(G, source, target=None):
-    nextlevel = {source: 0}
-    return dict(_single_source_bfs(G.adj, nextlevel, target=target))
-
-
-def _single_source_bfs(adj, firstlevel, target=None):
-    seen = {}
-    level = 0
-    nextlevel = firstlevel
-
-    while nextlevel:
-        thislevel = nextlevel
-        nextlevel = {}
-        for v in thislevel:
-            if v not in seen:
-                seen[v] = level
-                nextlevel.update(adj[v])
-                yield (v, level)
-                if v == target:
-                    break
-        level += 1
-    del seen
-
-
-@not_implemented_for("multigraph")
-def single_source_dijkstra(G, source, weight="weight", target=None):
-    return multi_source_dijkstra(G, {source}, weight, target=target)
-
-
-@not_implemented_for("multigraph")
-def multi_source_dijkstra(G, sources, weight="weight", target=None):
-    return _dijkstra_multisource(G, sources, weight, target=target)
-
-
-@hybrid("cpp_dijkstra_multisource")
-def _dijkstra_multisource(G, sources, weight="weight", target=None):
-    from heapq import heappop
-    from heapq import heappush
-
-    push = heappush
-    pop = heappop
-    adj = G.adj
-    dist = {}
-    seen = {}
-    from itertools import count
-
-    c = count()
-    Q = []
-    for source in sources:
-        seen[source] = 0
-        push(Q, (0, next(c), source))
-    while Q:
-        (d, _, v) = pop(Q)
-        if v in dist:
-            continue
-        dist[v] = d
-        if v == target:
-            break
-        for u in adj[v]:
-            cost = adj[v][u].get(weight, 1)
-            vu_dist = dist[v] + cost
-            if u in dist:
-                if vu_dist < dist[u]:
-                    raise ValueError("Contradictory paths found:", "negative weights?")
-            elif u not in seen or vu_dist < seen[u]:
-                seen[u] = vu_dist
-                push(Q, (vu_dist, next(c), u))
-            else:
-                continue
-    return dist
+from easygraph.utils import *
+from easygraph.utils.decorators import *
+
+
+__all__ = [
+    "Dijkstra",
+    "Floyd",
+    "Prim",
+    "Kruskal",
+    "Spfa",
+    "single_source_bfs",
+    "single_source_dijkstra",
+    "multi_source_dijkstra",
+]
+
+
+@hybrid("cpp_spfa")
+def Spfa(G, node, weight="weight"):
+    raise EasyGraphError("Please input GraphC or DiGraphC.")
+
+
+@not_implemented_for("multigraph")
+def Dijkstra(G, node, weight="weight"):
+    """Returns the length of paths from the certain node to remaining nodes
+
+    Parameters
+    ----------
+    G : graph
+        weighted graph
+    node : int
+
+    Returns
+    -------
+    result_dict : dict
+        the length of paths from the certain node to remaining nodes
+
+    Examples
+    --------
+    Returns the length of paths from node 1 to remaining nodes
+
+    >>> Dijkstra(G,node=1,weight="weight")
+
+    """
+    return single_source_dijkstra(G, node, weight=weight)
+
+
+@not_implemented_for("multigraph")
+@only_implemented_for_UnDirected_graph
+@hybrid("cpp_Floyd")
+def Floyd(G, weight="weight"):
+    """Returns the length of paths from all nodes to remaining nodes
+
+    Parameters
+    ----------
+    G : graph
+        weighted graph
+
+    Returns
+    -------
+    result_dict : dict
+        the length of paths from all nodes to remaining nodes
+
+    Examples
+    --------
+    Returns the length of paths from all nodes to remaining nodes
+
+    >>> Floyd(G,weight="weight")
+
+    """
+    adj = G.adj.copy()
+    result_dict = {}
+    for i in G:
+        result_dict[i] = {}
+    for i in G:
+        temp_key = adj[i].keys()
+        for j in G:
+            if j in temp_key:
+                result_dict[i][j] = adj[i][j].get(weight, 1)
+            else:
+                result_dict[i][j] = float("inf")
+            if i == j:
+                result_dict[i][i] = 0
+    for k in G:
+        for i in G:
+            for j in G:
+                temp = result_dict[i][k] + result_dict[k][j]
+                if result_dict[i][j] > temp:
+                    result_dict[i][j] = temp
+    return result_dict
+
+
+@not_implemented_for("multigraph")
+@only_implemented_for_UnDirected_graph
+@hybrid("cpp_Prim")
+def Prim(G, weight="weight"):
+    """Returns the edges that make up the minimum spanning tree
+
+    Parameters
+    ----------
+    G : graph
+        weighted graph
+
+    Returns
+    -------
+    result_dict : dict
+        the edges that make up the minimum spanning tree
+
+    Examples
+    --------
+    Returns the edges that make up the minimum spanning tree
+
+    >>> Prim(G,weight="weight")
+
+    """
+    adj = G.adj.copy()
+    result_dict = {}
+    for i in G:
+        result_dict[i] = {}
+    selected = []
+    candidate = []
+    for i in G:
+        if not selected:
+            selected.append(i)
+        else:
+            candidate.append(i)
+    while len(candidate):
+        start = None
+        end = None
+        min_weight = float("inf")
+        for i in selected:
+            for j in candidate:
+                if i in G and j in G[i] and adj[i][j].get(weight, 1) < min_weight:
+                    start = i
+                    end = j
+                    min_weight = adj[i][j].get(weight, 1)
+        if start != None and end != None:
+            result_dict[start][end] = min_weight
+            selected.append(end)
+            candidate.remove(end)
+        else:
+            break
+    return result_dict
+
+
+@not_implemented_for("multigraph")
+@only_implemented_for_UnDirected_graph
+@hybrid("cpp_Kruskal")
+def Kruskal(G, weight="weight"):
+    """Returns the edges that make up the minimum spanning tree
+
+    Parameters
+    ----------
+    G : graph
+        weighted graph
+
+    Returns
+    -------
+    result_dict : dict
+        the edges that make up the minimum spanning tree
+
+    Examples
+    --------
+    Returns the edges that make up the minimum spanning tree
+
+    >>> Kruskal(G,weight="weight")
+
+    """
+    adj = G.adj.copy()
+    result_dict = {}
+    edge_list = []
+    for i in G:
+        result_dict[i] = {}
+    for i in G:
+        for j in G[i]:
+            wt = adj[i][j].get(weight, 1)
+            edge_list.append([i, j, wt])
+    edge_list.sort(key=lambda a: a[2])
+    group = [[i] for i in G]
+    for edge in edge_list:
+        for i in range(len(group)):
+            if edge[0] in group[i]:
+                m = i
+            if edge[1] in group[i]:
+                n = i
+        if m != n:
+            result_dict[edge[0]][edge[1]] = edge[2]
+            group[m] = group[m] + group[n]
+            group[n] = []
+    return result_dict
+
+
+@not_implemented_for("multigraph")
+def single_source_bfs(G, source, target=None):
+    nextlevel = {source: 0}
+    return dict(_single_source_bfs(G.adj, nextlevel, target=target))
+
+
+def _single_source_bfs(adj, firstlevel, target=None):
+    seen = {}
+    level = 0
+    nextlevel = firstlevel
+
+    while nextlevel:
+        thislevel = nextlevel
+        nextlevel = {}
+        for v in thislevel:
+            if v not in seen:
+                seen[v] = level
+                nextlevel.update(adj[v])
+                yield (v, level)
+                if v == target:
+                    break
+        level += 1
+    del seen
+
+
+@not_implemented_for("multigraph")
+def single_source_dijkstra(G, source, weight="weight", target=None):
+    return multi_source_dijkstra(G, {source}, weight, target=target)
+
+
+@not_implemented_for("multigraph")
+def multi_source_dijkstra(G, sources, weight="weight", target=None):
+    return _dijkstra_multisource(G, sources, weight, target=target)
+
+
+@hybrid("cpp_dijkstra_multisource")
+def _dijkstra_multisource(G, sources, weight="weight", target=None):
+    from heapq import heappop
+    from heapq import heappush
+
+    push = heappush
+    pop = heappop
+    adj = G.adj
+    dist = {}
+    seen = {}
+    from itertools import count
+
+    c = count()
+    Q = []
+    for source in sources:
+        seen[source] = 0
+        push(Q, (0, next(c), source))
+    while Q:
+        (d, _, v) = pop(Q)
+        if v in dist:
+            continue
+        dist[v] = d
+        if v == target:
+            break
+        for u in adj[v]:
+            cost = adj[v][u].get(weight, 1)
+            vu_dist = dist[v] + cost
+            if u in dist:
+                if vu_dist < dist[u]:
+                    raise ValueError("Contradictory paths found:", "negative weights?")
+            elif u not in seen or vu_dist < seen[u]:
+                seen[u] = vu_dist
+                push(Q, (vu_dist, next(c), u))
+            else:
+                continue
+    return dist
```

## easygraph/functions/path/__init__.py

 * *Ordering differences only*

```diff
@@ -1,5 +1,5 @@
-from .average_shortest_path_length import *
-from .bridges import *
-from .diameter import *
-from .mst import *
-from .path import *
+from .average_shortest_path_length import *
+from .bridges import *
+from .diameter import *
+from .mst import *
+from .path import *
```

## easygraph/functions/community/motif.py

 * *Ordering differences only*

```diff
@@ -1,120 +1,120 @@
-import random
-
-import easygraph as eg
-
-from easygraph.utils import *
-
-
-__all__ = ["enumerate_subgraph", "random_enumerate_subgraph"]
-
-
-@not_implemented_for("multigraph")
-def enumerate_subgraph(G, k: int):
-    """
-    Returns the motifs.
-    Motifs are small weakly connected induced subgraphs of a given structure in a graph.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph.
-
-    k : int
-        The size of the motifs to search for.
-
-    Returns
-    ----------
-    k_subgraphs : list
-        The motifs.
-
-    References
-    ----------
-    .. [1] Wernicke, Sebastian. "Efficient detection of network motifs."
-        IEEE/ACM transactions on computational biology and bioinformatics 3.4 (2006): 347-359.
-
-    """
-    k_subgraphs = []
-    for v, _ in G.nodes.items():
-        Vextension = {u for u in G.adj[v] if u > v}
-        extend_subgraph(G, {v}, Vextension, v, k, k_subgraphs)
-    return k_subgraphs
-
-
-def extend_subgraph(
-    G, Vsubgraph: set, Vextension: set, v: int, k: int, k_subgraphs: list
-):
-    if len(Vsubgraph) == k:
-        k_subgraphs.append(Vsubgraph)
-        return
-    while len(Vextension) > 0:
-        w = random.choice(tuple(Vextension))
-        Vextension.remove(w)
-        NexclwVsubgraph = exclusive_neighborhood(G, w, Vsubgraph)
-        VpExtension = Vextension | {u for u in NexclwVsubgraph if u > v}
-        extend_subgraph(G, Vsubgraph | {w}, VpExtension, v, k, k_subgraphs)
-
-
-def exclusive_neighborhood(G, v: int, vp: set):
-    Nv = set(G.adj[v])
-    NVp = {u for n in vp for u in G.adj[n]} | vp
-    return Nv - NVp
-
-
-@not_implemented_for("multigraph")
-def random_enumerate_subgraph(G, k: int, cut_prob: list):
-    """
-    Returns the motifs.
-    Motifs are small weakly connected induced subgraphs of a given structure in a graph.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph.
-
-    k : int
-        The size of the motifs to search for.
-
-    cut_prob : list
-        list of probabilities for cutting the search tree at a given level.
-
-    Returns
-    ----------
-    k_subgraphs : list
-        The motifs.
-
-    References
-    ----------
-    .. [1] Wernicke, Sebastian. "A faster algorithm for detecting network motifs."
-    International Workshop on Algorithms in Bioinformatics. Springer, Berlin, Heidelberg, 2005.
-
-    """
-    if len(cut_prob) != k:
-        raise eg.EasyGraphError("length of cut_prob invalid, should equal to k")
-
-    k_subgraphs = []
-    for v, _ in G.nodes.items():
-        if random.random() > cut_prob[0]:
-            continue
-        Vextension = {u for u in G.adj[v] if u > v}
-        random_extend_subgraph(G, {v}, Vextension, v, k, k_subgraphs, cut_prob)
-    return k_subgraphs
-
-
-def random_extend_subgraph(
-    G,
-    Vsubgraph: set,
-    Vextension: set,
-    v: int,
-    k: int,
-    k_subgraphs: list,
-    cut_prob: list,
-):
-    if len(Vsubgraph) == k:
-        k_subgraphs.append(Vsubgraph)
-        return
-    while len(Vextension) > 0:
-        w = random.choice(tuple(Vextension))
-        Vextension.remove(w)
-        NexclwVsubgraph = exclusive_neighborhood(G, w, Vsubgraph)
-        VpExtension = Vextension | {u for u in NexclwVsubgraph if u > v}
-        if random.random() > cut_prob[len(Vsubgraph)]:
-            continue
-        random_extend_subgraph(G, Vsubgraph | {w}, VpExtension, v, k, k_subgraphs)
+import random
+
+import easygraph as eg
+
+from easygraph.utils import *
+
+
+__all__ = ["enumerate_subgraph", "random_enumerate_subgraph"]
+
+
+@not_implemented_for("multigraph")
+def enumerate_subgraph(G, k: int):
+    """
+    Returns the motifs.
+    Motifs are small weakly connected induced subgraphs of a given structure in a graph.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph.
+
+    k : int
+        The size of the motifs to search for.
+
+    Returns
+    ----------
+    k_subgraphs : list
+        The motifs.
+
+    References
+    ----------
+    .. [1] Wernicke, Sebastian. "Efficient detection of network motifs."
+        IEEE/ACM transactions on computational biology and bioinformatics 3.4 (2006): 347-359.
+
+    """
+    k_subgraphs = []
+    for v, _ in G.nodes.items():
+        Vextension = {u for u in G.adj[v] if u > v}
+        extend_subgraph(G, {v}, Vextension, v, k, k_subgraphs)
+    return k_subgraphs
+
+
+def extend_subgraph(
+    G, Vsubgraph: set, Vextension: set, v: int, k: int, k_subgraphs: list
+):
+    if len(Vsubgraph) == k:
+        k_subgraphs.append(Vsubgraph)
+        return
+    while len(Vextension) > 0:
+        w = random.choice(tuple(Vextension))
+        Vextension.remove(w)
+        NexclwVsubgraph = exclusive_neighborhood(G, w, Vsubgraph)
+        VpExtension = Vextension | {u for u in NexclwVsubgraph if u > v}
+        extend_subgraph(G, Vsubgraph | {w}, VpExtension, v, k, k_subgraphs)
+
+
+def exclusive_neighborhood(G, v: int, vp: set):
+    Nv = set(G.adj[v])
+    NVp = {u for n in vp for u in G.adj[n]} | vp
+    return Nv - NVp
+
+
+@not_implemented_for("multigraph")
+def random_enumerate_subgraph(G, k: int, cut_prob: list):
+    """
+    Returns the motifs.
+    Motifs are small weakly connected induced subgraphs of a given structure in a graph.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph.
+
+    k : int
+        The size of the motifs to search for.
+
+    cut_prob : list
+        list of probabilities for cutting the search tree at a given level.
+
+    Returns
+    ----------
+    k_subgraphs : list
+        The motifs.
+
+    References
+    ----------
+    .. [1] Wernicke, Sebastian. "A faster algorithm for detecting network motifs."
+    International Workshop on Algorithms in Bioinformatics. Springer, Berlin, Heidelberg, 2005.
+
+    """
+    if len(cut_prob) != k:
+        raise eg.EasyGraphError("length of cut_prob invalid, should equal to k")
+
+    k_subgraphs = []
+    for v, _ in G.nodes.items():
+        if random.random() > cut_prob[0]:
+            continue
+        Vextension = {u for u in G.adj[v] if u > v}
+        random_extend_subgraph(G, {v}, Vextension, v, k, k_subgraphs, cut_prob)
+    return k_subgraphs
+
+
+def random_extend_subgraph(
+    G,
+    Vsubgraph: set,
+    Vextension: set,
+    v: int,
+    k: int,
+    k_subgraphs: list,
+    cut_prob: list,
+):
+    if len(Vsubgraph) == k:
+        k_subgraphs.append(Vsubgraph)
+        return
+    while len(Vextension) > 0:
+        w = random.choice(tuple(Vextension))
+        Vextension.remove(w)
+        NexclwVsubgraph = exclusive_neighborhood(G, w, Vsubgraph)
+        VpExtension = Vextension | {u for u in NexclwVsubgraph if u > v}
+        if random.random() > cut_prob[len(Vsubgraph)]:
+            continue
+        random_extend_subgraph(G, Vsubgraph | {w}, VpExtension, v, k, k_subgraphs)
```

## easygraph/functions/community/louvain.py

 * *Ordering differences only*

```diff
@@ -1,350 +1,350 @@
-from collections import defaultdict
-from collections import deque
-
-import easygraph as eg
-
-from easygraph.functions.community.modularity import *
-
-
-__all__ = ["louvain_communities", "louvain_partitions"]
-
-
-def louvain_communities(G, weight="weight", threshold=0.00002):
-    r"""Find the best partition of a graph using the Louvain Community Detection
-    Algorithm.
-
-    Louvain Community Detection Algorithm is a simple method to extract the community
-    structure of a network. This is a heuristic method based on modularity optimization. [1]_
-
-    The algorithm works in 2 steps. On the first step it assigns every node to be
-    in its own community and then for each node it tries to find the maximum positive
-    modularity gain by moving each node to all of its neighbor communities. If no positive
-    gain is achieved the node remains in its original community.
-
-    The modularity gain obtained by moving an isolated node $i$ into a community $C$ can
-    easily be calculated by the following formula (combining [1]_ [2]_ and some algebra):
-
-    .. math::
-        \Delta Q = \frac{k_{i,in}}{2m} - \gamma\frac{ \Sigma_{tot} \cdot k_i}{2m^2}
-
-    where $m$ is the size of the graph, $k_{i,in}$ is the sum of the weights of the links
-    from $i$ to nodes in $C$, $k_i$ is the sum of the weights of the links incident to node $i$,
-    $\Sigma_{tot}$ is the sum of the weights of the links incident to nodes in $C$ and $\gamma$
-    is the resolution parameter.
-
-    For the directed case the modularity gain can be computed using this formula according to [3]_
-
-    .. math::
-        \Delta Q = \frac{k_{i,in}}{m}
-        - \gamma\frac{k_i^{out} \cdot\Sigma_{tot}^{in} + k_i^{in} \cdot \Sigma_{tot}^{out}}{m^2}
-
-    where $k_i^{out}$, $k_i^{in}$ are the outer and inner weighted degrees of node $i$ and
-    $\Sigma_{tot}^{in}$, $\Sigma_{tot}^{out}$ are the sum of in-going and out-going links incident
-    to nodes in $C$.
-
-    The first phase continues until no individual move can improve the modularity.
-
-    The second phase consists in building a new network whose nodes are now the communities
-    found in the first phase. To do so, the weights of the links between the new nodes are given by
-    the sum of the weight of the links between nodes in the corresponding two communities. Once this
-    phase is complete it is possible to reapply the first phase creating bigger communities with
-    increased modularity.
-
-    The above two phases are executed until no modularity gain is achieved (or is less than
-    the `threshold`).
-
-    Parameters
-    ----------
-    threshold
-    G : easygraph
-    weight : string or None, optional (default="weight")
-        The name of an edge attribute that holds the numerical value
-        used as a weight. If None then each edge has weight 1.
-
-    Returns
-    -------
-    list
-        A list of sets (partition of `G`). Each set represents one community and contains
-        all the nodes that constitute it.
-
-    Notes
-    -----
-    The order in which the nodes are considered can affect the final output. In the algorithm
-    the ordering happens using a random shuffle.
-
-    References
-    ----------
-    .. [1] Blondel, V.D. et al. Fast unfolding of communities in
-       large networks. J. Stat. Mech 10008, 1-12(2008). https://doi.org/10.1088/1742-5468/2008/10/P10008
-    .. [2] Traag, V.A., Waltman, L. & van Eck, N.J. From Louvain to Leiden: guaranteeing
-       well-connected communities. Sci Rep 9, 5233 (2019). https://doi.org/10.1038/s41598-019-41695-z
-    .. [3] Nicolas Dugu��, Anthony Perez. Directed Louvain : maximizing modularity in directed networks.
-        [Research Report] Universit�� d��Orl��ans. 2015. hal-01231784. https://hal.archives-ouvertes.fr/hal-01231784
-
-    See Also
-    --------
-    louvain_partitions
-    """
-    d = louvain_partitions(G, weight, threshold)
-    q = deque(maxlen=1)
-    q.append(d)
-    return q.pop()
-
-
-def louvain_partitions(G, weight="weight", threshold=0.0000001):
-    """Yields partitions for each level of the Louvain Community Detection Algorithm
-
-    Louvain Community Detection Algorithm is a simple method to extract the community
-    structure of a network. This is a heuristic method based on modularity optimization. [1]_
-
-    The partitions at each level (step of the algorithm) form a dendogram of communities.
-    A dendrogram is a diagram representing a tree and each level represents
-    a partition of the G graph. The top level contains the smallest communities
-    and as you traverse to the bottom of the tree the communities get bigger
-    and the overall modularity increases making the partition better.
-
-    Each level is generated by executing the two phases of the Louvain Community
-    Detection Algorithm.
-
-    Parameters
-    ----------
-    threshold
-    G : NetworkX graph
-    weight : string or None, optional (default="weight")
-     The name of an edge attribute that holds the numerical value
-     used as a weight. If None then each edge has weight 1.
-
-    Yields
-    ------
-    list
-        A list of sets (partition of `G`). Each set represents one community and contains
-        all the nodes that constitute it.
-
-    References
-    ----------
-    .. [1] Blondel, V.D. et al. Fast unfolding of communities in
-       large networks. J. Stat. Mech 10008, 1-12(2008)
-
-    See Also
-    --------
-    louvain_communities
-    """
-    partition = [{u} for u in G.nodes]
-    mod = modularity(G, partition)
-    is_directed = G.is_directed()
-    if G.is_multigraph():
-        G = _convert_multigraph(G, weight, is_directed)
-    else:
-        graph = G.__class__()
-        graph.add_nodes_from(G)
-        graph.add_edges_from(G.edges, weight=1)
-        G = graph
-
-    m = G.size(weight="weight")
-    partition, inner_partition, improvement = _one_level(G, m, partition, is_directed)
-    improvement = True
-    while improvement:
-        # gh-5901 protect the sets in the yielded list from further manipulation here
-
-        yield [s.copy() for s in partition]
-        new_mod = modularity(G, inner_partition, weight="weight")
-        if new_mod - mod <= threshold:
-            return
-        mod = new_mod
-        """
-        for node1, node2, wt in G.edges:
-            print(node1,node2,wt)
-        print("\n")
-        """
-        G = _gen_graph(G, inner_partition)
-        """
-        for node1, node2, wt in G.edges:
-            print(node1,node2,wt)
-        """
-        partition, inner_partition, improvement = _one_level(
-            G, m, partition, is_directed, 1
-        )
-
-
-def _one_level(G, m, partition, resolution=1, is_directed=False, seed=None, tes=0):
-    """Calculate one level of the Louvain partitions tree
-
-    Parameters
-    ----------
-    G : NetworkX Graph/DiGraph
-        The graph from which to detect communities
-    m : number
-        The size of the graph `G`.
-    partition : list of sets of nodes
-        A valid partition of the graph `G`
-    resolution : positive number
-        The resolution parameter for computing the modularity of a partition
-    is_directed : bool
-        True if `G` is a directed graph.
-    seed : integer, random_state, or None (default)
-        Indicator of random number generation state.
-        See :ref:`Randomness<randomness>`.
-
-    """
-    node2com = {u: i for i, u in enumerate(G.nodes)}
-    inner_partition = [{u} for u in G.nodes]
-    """
-    if is_directed:
-        in_degrees = dict(G.in_degree(weight="weight"))
-        out_degrees = dict(G.out_degree(weight="weight"))
-        Stot_in = list(in_degrees.values())
-        Stot_out = list(out_degrees.values())
-        # Calculate weights for both in and out neighbours
-        nbrs = {}
-        for u in G:
-            nbrs[u] = defaultdict(float)
-            for _, n, wt in G.out_edges(u, data="weight"):
-                nbrs[u][n] += wt
-            for n, _, wt in G.in_edges(u, data="weight"):
-                nbrs[u][n] += wt
-        pass
-    else:
-    """
-    degrees = dict(G.degree(weight="weight"))
-    Stot = []
-    for i in G:
-        Stot.append(len(G[i]))
-
-    # for c in Stot:
-    #    print(c)
-
-    nbrs = {u: {v: data["weight"] for v, data in G[u].items() if v != u} for u in G}
-    rand_nodes = list(G.nodes)
-    # seed.shuffle(rand_nodes)
-    nb_moves = 1
-    improvement = False
-    while nb_moves > 0:
-        # print(nb_moves)
-
-        nb_moves = 0
-        for u in rand_nodes:
-            best_mod = 0
-            best_com = node2com[u]
-            weights2com = _neighbor_weights(nbrs[u], node2com)
-            """
-            if is_directed:
-                in_degree = in_degrees[u]
-                out_degree = out_degrees[u]
-                Stot_in[best_com] -= in_degree
-                Stot_out[best_com] -= out_degree
-                remove_cost = (
-                    -weights2com[best_com] / m
-                    + (out_degree * Stot_in[best_com] + in_degree * Stot_out[best_com])
-                    / m**2
-                )
-            else:
-            """
-            degree = degrees[u]
-            Stot[best_com] -= degree
-            remove_cost = -weights2com[best_com] / m + (Stot[best_com] * degree) / (
-                2 * m**2
-            )
-            for nbr_com, wt in weights2com.items():
-                """
-                if is_directed:
-                    gain = (
-                        remove_cost
-                        + wt / m
-                        - (
-                            out_degree * Stot_in[nbr_com]
-                            + in_degree * Stot_out[nbr_com]
-                        )
-                        / m**2
-                    )
-                else:
-                """
-                gain = remove_cost + wt / m - (Stot[nbr_com] * degree) / (2 * m**2)
-                if gain > best_mod:
-                    best_mod = gain
-                    best_com = nbr_com
-            """
-            if is_directed:
-                Stot_in[best_com] += in_degree
-                Stot_out[best_com] += out_degree
-            else:
-            """
-            Stot[best_com] += degree
-
-            if best_com != node2com[u]:
-                com = G.nodes[u].get("nodes", {u})
-                partition[node2com[u]].difference_update(com)
-                inner_partition[node2com[u]].remove(u)
-                partition[best_com].update(com)
-                inner_partition[best_com].add(u)
-                improvement = True
-                nb_moves += 1
-                node2com[u] = best_com
-    partition = list(filter(len, partition))
-    inner_partition = list(filter(len, inner_partition))
-
-    # for c in partition:
-    # print(c)
-
-    return partition, inner_partition, improvement
-
-
-def _neighbor_weights(nbrs, node2com):
-    """Calculate weights between node and its neighbor communities.
-
-    Parameters
-    ----------
-    nbrs : dictionary
-           Dictionary with nodes' neighbours as keys and their edge weight as value.
-    node2com : dictionary
-           Dictionary with all graph's nodes as keys and their community index as value.
-
-    """
-    weights = defaultdict(float)
-    for nbr, wt in nbrs.items():
-        weights[node2com[nbr]] += wt
-    return weights
-
-
-def _gen_graph(G, partition):
-    """Generate a new graph based on the partitions of a given graph"""
-    H = G.__class__()
-    node2com = {}
-    for i, part in enumerate(partition):
-        nodes = set()
-        for node in part:
-            node2com[node] = i
-            nodes.update(G.nodes[node].get("nodes", {node}))
-        H.add_node(i, nodes=nodes)
-
-    for node1, node2, wt in G.edges:
-        com1 = node2com[node1]
-        com2 = node2com[node2]
-        wt = wt["weight"]
-        try:
-            temp = H[com1][com2]["weight"]
-        except KeyError:
-            temp = 0
-        H.add_edge(com1, com2, weight=wt + temp)
-        """
-        if wt:
-            wt = wt["weight"]
-            H.add_edge(com1, com2, weight=wt)
-        else:
-            H.add_edge(com1, com2, weight=1)
-        """
-    return H
-
-
-def _convert_multigraph(G, weight, is_directed):
-    """Convert a Multigraph to normal Graph"""
-    if is_directed:
-        H = eg.DiGraph()
-    else:
-        H = eg.Graph()
-    H.add_nodes_from(G)
-    for u, v, wt in G.edges(data=weight, default=1):
-        if H.has_edge(u, v):
-            H[u][v]["weight"] += wt
-        else:
-            H.add_edge(u, v, weight=wt)
-    return H
+from collections import defaultdict
+from collections import deque
+
+import easygraph as eg
+
+from easygraph.functions.community.modularity import *
+
+
+__all__ = ["louvain_communities", "louvain_partitions"]
+
+
+def louvain_communities(G, weight="weight", threshold=0.00002):
+    r"""Find the best partition of a graph using the Louvain Community Detection
+    Algorithm.
+
+    Louvain Community Detection Algorithm is a simple method to extract the community
+    structure of a network. This is a heuristic method based on modularity optimization. [1]_
+
+    The algorithm works in 2 steps. On the first step it assigns every node to be
+    in its own community and then for each node it tries to find the maximum positive
+    modularity gain by moving each node to all of its neighbor communities. If no positive
+    gain is achieved the node remains in its original community.
+
+    The modularity gain obtained by moving an isolated node $i$ into a community $C$ can
+    easily be calculated by the following formula (combining [1]_ [2]_ and some algebra):
+
+    .. math::
+        \Delta Q = \frac{k_{i,in}}{2m} - \gamma\frac{ \Sigma_{tot} \cdot k_i}{2m^2}
+
+    where $m$ is the size of the graph, $k_{i,in}$ is the sum of the weights of the links
+    from $i$ to nodes in $C$, $k_i$ is the sum of the weights of the links incident to node $i$,
+    $\Sigma_{tot}$ is the sum of the weights of the links incident to nodes in $C$ and $\gamma$
+    is the resolution parameter.
+
+    For the directed case the modularity gain can be computed using this formula according to [3]_
+
+    .. math::
+        \Delta Q = \frac{k_{i,in}}{m}
+        - \gamma\frac{k_i^{out} \cdot\Sigma_{tot}^{in} + k_i^{in} \cdot \Sigma_{tot}^{out}}{m^2}
+
+    where $k_i^{out}$, $k_i^{in}$ are the outer and inner weighted degrees of node $i$ and
+    $\Sigma_{tot}^{in}$, $\Sigma_{tot}^{out}$ are the sum of in-going and out-going links incident
+    to nodes in $C$.
+
+    The first phase continues until no individual move can improve the modularity.
+
+    The second phase consists in building a new network whose nodes are now the communities
+    found in the first phase. To do so, the weights of the links between the new nodes are given by
+    the sum of the weight of the links between nodes in the corresponding two communities. Once this
+    phase is complete it is possible to reapply the first phase creating bigger communities with
+    increased modularity.
+
+    The above two phases are executed until no modularity gain is achieved (or is less than
+    the `threshold`).
+
+    Parameters
+    ----------
+    threshold
+    G : easygraph
+    weight : string or None, optional (default="weight")
+        The name of an edge attribute that holds the numerical value
+        used as a weight. If None then each edge has weight 1.
+
+    Returns
+    -------
+    list
+        A list of sets (partition of `G`). Each set represents one community and contains
+        all the nodes that constitute it.
+
+    Notes
+    -----
+    The order in which the nodes are considered can affect the final output. In the algorithm
+    the ordering happens using a random shuffle.
+
+    References
+    ----------
+    .. [1] Blondel, V.D. et al. Fast unfolding of communities in
+       large networks. J. Stat. Mech 10008, 1-12(2008). https://doi.org/10.1088/1742-5468/2008/10/P10008
+    .. [2] Traag, V.A., Waltman, L. & van Eck, N.J. From Louvain to Leiden: guaranteeing
+       well-connected communities. Sci Rep 9, 5233 (2019). https://doi.org/10.1038/s41598-019-41695-z
+    .. [3] Nicolas Dugu��, Anthony Perez. Directed Louvain : maximizing modularity in directed networks.
+        [Research Report] Universit�� d��Orl��ans. 2015. hal-01231784. https://hal.archives-ouvertes.fr/hal-01231784
+
+    See Also
+    --------
+    louvain_partitions
+    """
+    d = louvain_partitions(G, weight, threshold)
+    q = deque(maxlen=1)
+    q.append(d)
+    return q.pop()
+
+
+def louvain_partitions(G, weight="weight", threshold=0.0000001):
+    """Yields partitions for each level of the Louvain Community Detection Algorithm
+
+    Louvain Community Detection Algorithm is a simple method to extract the community
+    structure of a network. This is a heuristic method based on modularity optimization. [1]_
+
+    The partitions at each level (step of the algorithm) form a dendogram of communities.
+    A dendrogram is a diagram representing a tree and each level represents
+    a partition of the G graph. The top level contains the smallest communities
+    and as you traverse to the bottom of the tree the communities get bigger
+    and the overall modularity increases making the partition better.
+
+    Each level is generated by executing the two phases of the Louvain Community
+    Detection Algorithm.
+
+    Parameters
+    ----------
+    threshold
+    G : NetworkX graph
+    weight : string or None, optional (default="weight")
+     The name of an edge attribute that holds the numerical value
+     used as a weight. If None then each edge has weight 1.
+
+    Yields
+    ------
+    list
+        A list of sets (partition of `G`). Each set represents one community and contains
+        all the nodes that constitute it.
+
+    References
+    ----------
+    .. [1] Blondel, V.D. et al. Fast unfolding of communities in
+       large networks. J. Stat. Mech 10008, 1-12(2008)
+
+    See Also
+    --------
+    louvain_communities
+    """
+    partition = [{u} for u in G.nodes]
+    mod = modularity(G, partition)
+    is_directed = G.is_directed()
+    if G.is_multigraph():
+        G = _convert_multigraph(G, weight, is_directed)
+    else:
+        graph = G.__class__()
+        graph.add_nodes_from(G)
+        graph.add_edges_from(G.edges, weight=1)
+        G = graph
+
+    m = G.size(weight="weight")
+    partition, inner_partition, improvement = _one_level(G, m, partition, is_directed)
+    improvement = True
+    while improvement:
+        # gh-5901 protect the sets in the yielded list from further manipulation here
+
+        yield [s.copy() for s in partition]
+        new_mod = modularity(G, inner_partition, weight="weight")
+        if new_mod - mod <= threshold:
+            return
+        mod = new_mod
+        """
+        for node1, node2, wt in G.edges:
+            print(node1,node2,wt)
+        print("\n")
+        """
+        G = _gen_graph(G, inner_partition)
+        """
+        for node1, node2, wt in G.edges:
+            print(node1,node2,wt)
+        """
+        partition, inner_partition, improvement = _one_level(
+            G, m, partition, is_directed, 1
+        )
+
+
+def _one_level(G, m, partition, resolution=1, is_directed=False, seed=None, tes=0):
+    """Calculate one level of the Louvain partitions tree
+
+    Parameters
+    ----------
+    G : NetworkX Graph/DiGraph
+        The graph from which to detect communities
+    m : number
+        The size of the graph `G`.
+    partition : list of sets of nodes
+        A valid partition of the graph `G`
+    resolution : positive number
+        The resolution parameter for computing the modularity of a partition
+    is_directed : bool
+        True if `G` is a directed graph.
+    seed : integer, random_state, or None (default)
+        Indicator of random number generation state.
+        See :ref:`Randomness<randomness>`.
+
+    """
+    node2com = {u: i for i, u in enumerate(G.nodes)}
+    inner_partition = [{u} for u in G.nodes]
+    """
+    if is_directed:
+        in_degrees = dict(G.in_degree(weight="weight"))
+        out_degrees = dict(G.out_degree(weight="weight"))
+        Stot_in = list(in_degrees.values())
+        Stot_out = list(out_degrees.values())
+        # Calculate weights for both in and out neighbours
+        nbrs = {}
+        for u in G:
+            nbrs[u] = defaultdict(float)
+            for _, n, wt in G.out_edges(u, data="weight"):
+                nbrs[u][n] += wt
+            for n, _, wt in G.in_edges(u, data="weight"):
+                nbrs[u][n] += wt
+        pass
+    else:
+    """
+    degrees = dict(G.degree(weight="weight"))
+    Stot = []
+    for i in G:
+        Stot.append(len(G[i]))
+
+    # for c in Stot:
+    #    print(c)
+
+    nbrs = {u: {v: data["weight"] for v, data in G[u].items() if v != u} for u in G}
+    rand_nodes = list(G.nodes)
+    # seed.shuffle(rand_nodes)
+    nb_moves = 1
+    improvement = False
+    while nb_moves > 0:
+        # print(nb_moves)
+
+        nb_moves = 0
+        for u in rand_nodes:
+            best_mod = 0
+            best_com = node2com[u]
+            weights2com = _neighbor_weights(nbrs[u], node2com)
+            """
+            if is_directed:
+                in_degree = in_degrees[u]
+                out_degree = out_degrees[u]
+                Stot_in[best_com] -= in_degree
+                Stot_out[best_com] -= out_degree
+                remove_cost = (
+                    -weights2com[best_com] / m
+                    + (out_degree * Stot_in[best_com] + in_degree * Stot_out[best_com])
+                    / m**2
+                )
+            else:
+            """
+            degree = degrees[u]
+            Stot[best_com] -= degree
+            remove_cost = -weights2com[best_com] / m + (Stot[best_com] * degree) / (
+                2 * m**2
+            )
+            for nbr_com, wt in weights2com.items():
+                """
+                if is_directed:
+                    gain = (
+                        remove_cost
+                        + wt / m
+                        - (
+                            out_degree * Stot_in[nbr_com]
+                            + in_degree * Stot_out[nbr_com]
+                        )
+                        / m**2
+                    )
+                else:
+                """
+                gain = remove_cost + wt / m - (Stot[nbr_com] * degree) / (2 * m**2)
+                if gain > best_mod:
+                    best_mod = gain
+                    best_com = nbr_com
+            """
+            if is_directed:
+                Stot_in[best_com] += in_degree
+                Stot_out[best_com] += out_degree
+            else:
+            """
+            Stot[best_com] += degree
+
+            if best_com != node2com[u]:
+                com = G.nodes[u].get("nodes", {u})
+                partition[node2com[u]].difference_update(com)
+                inner_partition[node2com[u]].remove(u)
+                partition[best_com].update(com)
+                inner_partition[best_com].add(u)
+                improvement = True
+                nb_moves += 1
+                node2com[u] = best_com
+    partition = list(filter(len, partition))
+    inner_partition = list(filter(len, inner_partition))
+
+    # for c in partition:
+    # print(c)
+
+    return partition, inner_partition, improvement
+
+
+def _neighbor_weights(nbrs, node2com):
+    """Calculate weights between node and its neighbor communities.
+
+    Parameters
+    ----------
+    nbrs : dictionary
+           Dictionary with nodes' neighbours as keys and their edge weight as value.
+    node2com : dictionary
+           Dictionary with all graph's nodes as keys and their community index as value.
+
+    """
+    weights = defaultdict(float)
+    for nbr, wt in nbrs.items():
+        weights[node2com[nbr]] += wt
+    return weights
+
+
+def _gen_graph(G, partition):
+    """Generate a new graph based on the partitions of a given graph"""
+    H = G.__class__()
+    node2com = {}
+    for i, part in enumerate(partition):
+        nodes = set()
+        for node in part:
+            node2com[node] = i
+            nodes.update(G.nodes[node].get("nodes", {node}))
+        H.add_node(i, nodes=nodes)
+
+    for node1, node2, wt in G.edges:
+        com1 = node2com[node1]
+        com2 = node2com[node2]
+        wt = wt["weight"]
+        try:
+            temp = H[com1][com2]["weight"]
+        except KeyError:
+            temp = 0
+        H.add_edge(com1, com2, weight=wt + temp)
+        """
+        if wt:
+            wt = wt["weight"]
+            H.add_edge(com1, com2, weight=wt)
+        else:
+            H.add_edge(com1, com2, weight=1)
+        """
+    return H
+
+
+def _convert_multigraph(G, weight, is_directed):
+    """Convert a Multigraph to normal Graph"""
+    if is_directed:
+        H = eg.DiGraph()
+    else:
+        H = eg.Graph()
+    H.add_nodes_from(G)
+    for u, v, wt in G.edges(data=weight, default=1):
+        if H.has_edge(u, v):
+            H[u][v]["weight"] += wt
+        else:
+            H.add_edge(u, v, weight=wt)
+    return H
```

## easygraph/functions/community/ego_graph.py

```diff
@@ -1,65 +1,66 @@
-__all__ = ["ego_graph"]
-
-import easygraph as eg
-
-
-def ego_graph(G, n, radius=1, center=True, undirected=False, distance=None):
-    """Returns induced subgraph of neighbors centered at node n within
-    a given radius.
-
-    Parameters
-    ----------
-    G : graph
-      A NetworkX Graph or DiGraph
-
-    n : node
-      A single node
-
-    radius : number, optional
-      Include all neighbors of distance<=radius from n.
-
-    center : bool, optional
-      If False, do not include center node in graph
-
-    undirected : bool, optional
-      If True use both in- and out-neighbors of directed graphs.
-
-    distance : key, optional
-      Use specified edge data key as distance.  For example, setting
-      distance='weight' will use the edge weight to measure the
-      distance from the node n.
-
-    Notes
-    -----
-    For directed graphs D this produces the "out" neighborhood
-    or successors.  If you want the neighborhood of predecessors
-    first reverse the graph with D.reverse().  If you want both
-    directions use the keyword argument undirected=True.
-
-    Node, edge, and graph attributes are copied to the returned subgraph.
-    """
-    if undirected:
-        """
-        if distance is not None:
-            sp, _ = eg.single_source_dijkstra(
-                G.to_undirected(), n, cutoff=radius, weight=distance
-            )
-        else:
-            sp = dict(
-                eg.single_source_shortest_path_length(
-                    G.to_undirected(), n, cutoff=radius
-                )
-            )
-        """
-    else:
-        if distance is not None:
-            sp = eg.single_source_dijkstra(G, n, cutoff=radius, weight=distance)
-        else:
-            sp = eg.single_source_dijkstra(G, n, cutoff=radius, weight=distance)
-    nodes = list(sp.keys())
-    # for i,wd in sp.items():
-    #    print(i,wd)
-    H = G.nodes_subgraph(nodes)
-    if not center:
-        H.remove_node(n)
-    return H
+__all__ = ["ego_graph"]
+
+import easygraph as eg
+
+
+def ego_graph(G, n, radius=1, center=True, undirected=False, distance=None):
+    """Returns induced subgraph of neighbors centered at node n within
+    a given radius.
+
+    Parameters
+    ----------
+    G : graph
+      A NetworkX Graph or DiGraph
+
+    n : node
+      A single node
+
+    radius : number, optional
+      Include all neighbors of distance<=radius from n.
+
+    center : bool, optional
+      If False, do not include center node in graph
+
+    undirected : bool, optional
+      If True use both in- and out-neighbors of directed graphs.
+
+    distance : key, optional
+      Use specified edge data key as distance.  For example, setting
+      distance='weight' will use the edge weight to measure the
+      distance from the node n.
+
+    Notes
+    -----
+    For directed graphs D this produces the "out" neighborhood
+    or successors.  If you want the neighborhood of predecessors
+    first reverse the graph with D.reverse().  If you want both
+    directions use the keyword argument undirected=True.
+
+    Node, edge, and graph attributes are copied to the returned subgraph.
+    """
+    if undirected:
+        """
+        if distance is not None:
+            sp, _ = eg.single_source_dijkstra(
+                G.to_undirected(), n, cutoff=radius, weight=distance
+            )
+        else:
+            sp = dict(
+                eg.single_source_shortest_path_length(
+                    G.to_undirected(), n, cutoff=radius
+                )
+            )
+        """
+    else:
+        if distance is not None:
+            sp = eg.single_source_dijkstra(G, n, weight=distance)
+        else:
+            sp = eg.single_source_dijkstra(G, n)
+    nodes = [key for key, value in sp.items() if value <= radius]
+    nodes = list(nodes)
+    # for i,wd in sp.items():
+    #    print(i,wd)
+    H = G.nodes_subgraph(nodes)
+    if not center:
+        H.remove_node(n)
+    return H
```

## easygraph/functions/community/LPA.py

 * *Ordering differences only*

```diff
@@ -1,761 +1,761 @@
-import copy
-import random
-
-from collections import defaultdict
-from queue import Queue
-
-import easygraph as eg
-import numpy as np
-
-from easygraph.utils import *
-
-
-__all__ = [
-    "LPA",
-    "SLPA",
-    "HANP",
-    "BMLPA",
-]
-
-
-@not_implemented_for("multigraph")
-def LPA(G):
-    """Detect community by label propagation algorithm
-    Return the detected communities. But the result is random.
-    Each node in the network is initially assigned to its own community. At every iteration,nodes have
-    a label that the maximum number of their neighbors have. If there are more than one nodes fit and
-    available, choose a label randomly. Finally, nodes having the same labels are grouped together as
-    communities. In case two or more disconnected groups of nodes have the same label, we run a simple
-    breadth-first search to separate the disconnected communities
-
-    Parameters
-    ----------
-    G : graph
-      A easygraph graph
-
-    Returns
-    ----------
-    communities : dictionary
-      key: serial number of community , value: nodes in the community.
-
-    Examples
-    ----------
-    >>> LPA(G)
-
-    References
-    ----------
-    .. [1] Usha Nandini Raghavan, Réka Albert, and Soundar Kumara:
-        Near linear time algorithm to detect community structures in large-scale networks
-    """
-    i = 0
-    label_dict = dict()
-    cluster_community = dict()
-    Next_label_dict = dict()
-    nodes = list(G.nodes.keys())
-    if len(nodes) == 1:
-        return {1: [nodes[0]]}
-    for node in nodes:
-        label_dict[node] = i
-        i = i + 1
-    loop_count = 0
-    while True:
-        loop_count += 1
-        random.shuffle(nodes)
-        for node in nodes:
-            labels = SelectLabels(G, node, label_dict)
-            if labels == []:
-                Next_label_dict[node] = label_dict[node]
-                continue
-            Next_label_dict[node] = random.choice(labels)
-            # Asynchronous updates. If you want to use synchronous updates, comment the line below
-            label_dict[node] = Next_label_dict[node]
-        label_dict = Next_label_dict
-        if estimate_stop_cond(G, label_dict) is True:
-            break
-    for node in label_dict.keys():
-        label = label_dict[node]
-        if label not in cluster_community.keys():
-            cluster_community[label] = [node]
-        else:
-            cluster_community[label].append(node)
-
-    result_community = CheckConnectivity(G, cluster_community)
-    return result_community
-
-
-@not_implemented_for("multigraph")
-def SLPA(G, T, r):
-    """Detect Overlapping Communities by Speaker-listener Label Propagation Algorithm
-    Return the detected Overlapping communities. But the result is random.
-
-    Parameters
-    ----------
-    G : graph
-      A easygraph graph.
-    T : int
-      The number of iterations, In general, T is set greater than 20, which produces relatively stable outputs.
-    r : int
-      a threshold between 0 and 1.
-
-    Returns
-    -------
-    communities : dictionary
-      key: serial number of community , value: nodes in the community.
-
-    Examples
-    ----------
-    >>> SLPA(G,
-    ...     T = 20,
-    ...     r = 0.05
-    ...     )
-
-    References
-    ----------
-    .. [1] Jierui Xie, Boleslaw K. Szymanski, Xiaoming Liu:
-        SLPA: Uncovering Overlapping Communities in Social Networks via A Speaker-listener Interaction Dynamic Process
-    """
-    nodes = list(G.nodes.keys())
-    if len(nodes) == 1:
-        return {1: [nodes[0]]}
-    nodes = G.nodes
-    adj = G.adj
-    memory = {i: {i: 1} for i in nodes}
-    for i in range(0, T):
-        listenerslist = list(G.nodes)
-        random.shuffle(listenerslist)
-        for listener in listenerslist:
-            speakerlist = adj[listener]
-            if len(speakerlist) == 0:
-                continue
-            labels = defaultdict(int)
-            for speaker in speakerlist:
-                # Speaker Rule
-                total = float(sum(memory[speaker].values()))
-                keys = list(memory[speaker].keys())
-                index = np.random.multinomial(
-                    1, [round(freq / total, 2) for freq in memory[speaker].values()]
-                ).argmax()
-                chosen_label = keys[index]
-                labels[chosen_label] += 1
-            # Listener Rule
-            maxlabel = max(labels.items(), key=lambda x: x[1])[0]
-            if maxlabel in memory[listener]:
-                memory[listener][maxlabel] += 1
-            else:
-                memory[listener][maxlabel] = 1
-
-    for node, labels in memory.items():
-        name_list = []
-        for label_name, label_number in labels.items():
-            if round(label_number / float(T + 1), 2) < r:
-                name_list.append(label_name)
-        for name in name_list:
-            del labels[name]
-
-    # Find nodes membership
-    communities = {}
-    for node, labels in memory.items():
-        for label in labels:
-            if label in communities:
-                communities[label].add(node)
-            else:
-                communities[label] = {node}
-
-    # Remove nested communities
-    RemoveNested(communities)
-
-    # Check Connectivity
-    result_community = CheckConnectivity(G, communities)
-    return result_community
-
-
-@not_implemented_for("multigraph")
-def HANP(G, m, delta, threshod=1, hier_open=0, combine_open=0):
-    """Detect community by Hop attenuation & node preference algorithm
-
-    Return the detected communities. But the result is random.
-
-    Implement the basic HANP algorithm and give more freedom through the parameters, e.g., you can use threshod
-    to set the condition for node updating. If network are known to be Hierarchical and overlapping communities,
-    it's recommended to choose geodesic distance as the measure(instead of receiving the current hop scores
-    from the neighborhood and carry out a subtraction) and When an equilibrium is reached, treat newly combined
-    communities as a single node.
-
-    For using Floyd to get the shortest distance, the time complexity is a little high.
-
-    Parameters
-    ----------
-    G : graph
-      A easygraph graph
-    m : float
-      Used to calculate score, when m > 0, more preference is given to node with more neighbors; m < 0, less
-    delta : float
-      Hop attenuation
-    threshod : float
-      Between 0 and 1, only update node whose number of neighbors sharing the maximal label is less than the threshod.
-      e.g., threshod == 1 means updating all nodes.
-    hier_open :
-      1 means using geodesic distance as the score measure.
-      0 means not.
-    combine_open :
-      this option is valid only when hier_open = 1
-      1 means When an equilibrium is reached, treat newly combined communities as a single node.
-      0 means not.
-
-    Returns
-    ----------
-    communities : dictionary
-      key: serial number of community , value: nodes in the community.
-
-    Examples
-    ----------
-    >>> HANP(G,
-    ...     m = 0.1,
-    ...     delta = 0.05,
-    ...     threshod = 1,
-    ...     hier_open = 0,
-    ...     combine_open = 0
-    ...     )
-
-    References
-    ----------
-    .. [1] Ian X. Y. Leung, Pan Hui, Pietro Liò, and Jon Crowcrof:
-        Towards real-time community detection in large networks
-
-    """
-    nodes = list(G.nodes.keys())
-    if len(nodes) == 1:
-        return {1: [nodes[0]]}
-    label_dict = dict()
-    score_dict = dict()
-    node_dict = dict()
-    Next_label_dict = dict()
-    cluster_community = dict()
-    nodes = list(G.nodes.keys())
-    degrees = G.degree()
-    records = []
-    loop_count = 0
-    i = 0
-    old_score = 1
-    ori_G = G
-    if hier_open == 1:
-        distance_dict = eg.Floyd(G)
-    for node in nodes:
-        label_dict[node] = i
-        score_dict[i] = 1
-        node_dict[i] = node
-        i = i + 1
-    while True:
-        loop_count += 1
-        random.shuffle(nodes)
-        score = 1
-        for node in nodes:
-            labels = SelectLabels_HANP(
-                G, node, label_dict, score_dict, degrees, m, threshod
-            )
-            if labels == []:
-                Next_label_dict[node] = label_dict[node]
-                continue
-            old_label = label_dict[node]
-            Next_label_dict[node] = random.choice(labels)
-            # Asynchronous updates. If you want to use synchronous updates, comment the line below
-            label_dict[node] = Next_label_dict[node]
-            if hier_open == 1:
-                score_dict[Next_label_dict[node]] = UpdateScore_Hier(
-                    G, node, label_dict, node_dict, distance_dict
-                )
-                score = min(score, score_dict[Next_label_dict[node]])
-            else:
-                if old_label == Next_label_dict[node]:
-                    cdelta = 0
-                else:
-                    cdelta = delta
-                score_dict[Next_label_dict[node]] = UpdateScore(
-                    G, node, label_dict, score_dict, cdelta
-                )
-        if hier_open == 1 and combine_open == 1:
-            if old_score - score > 1 / 3:
-                old_score = score
-                (
-                    records,
-                    G,
-                    label_dict,
-                    score_dict,
-                    node_dict,
-                    Next_label_dict,
-                    nodes,
-                    degrees,
-                    distance_dict,
-                ) = CombineNodes(
-                    records,
-                    G,
-                    label_dict,
-                    score_dict,
-                    node_dict,
-                    Next_label_dict,
-                    nodes,
-                    degrees,
-                    distance_dict,
-                )
-        label_dict = Next_label_dict
-        if (
-            estimate_stop_cond_HANP(G, label_dict, score_dict, degrees, m, threshod)
-            is True
-        ):
-            break
-        """As mentioned in the paper, it's suggested that the number of iterations
-        required is independent to the number of nodes and that after
-        five iterations, 95% of their nodes are already accurately clustered
-        """
-        if loop_count > 20:
-            break
-    print("After %d iterations, HANP complete." % loop_count)
-    for node in label_dict.keys():
-        label = label_dict[node]
-        if label not in cluster_community.keys():
-            cluster_community[label] = [node]
-        else:
-            cluster_community[label].append(node)
-    if hier_open == 1 and combine_open == 1:
-        records.append(cluster_community)
-        cluster_community = ShowRecord(records)
-    result_community = CheckConnectivity(ori_G, cluster_community)
-    return result_community
-
-
-@not_implemented_for("multigraph")
-def BMLPA(G, p):
-    """Detect community by Balanced Multi-Label Propagation algorithm
-
-    Return the detected communities.
-
-    Firstly, initialize 'old' using cores generated by RC function, the propagate label till the number and size
-    of communities stay no change, check if there are subcommunity and delete it. Finally, split discontinuous
-    communities.
-
-    For some directed graphs lead to oscillations of labels, modify the stop condition.
-
-    Parameters
-    ----------
-    G : graph
-      A easygraph graph
-    p : float
-      Between 0 and 1, judge Whether a community identifier should be retained
-
-    Returns
-    ----------
-    communities : dictionary
-      key: serial number of community , value: nodes in the community.
-
-    Examples
-    ----------
-    >>> BMLPA(G,
-    ...     p = 0.1,
-    ...     )
-
-    References
-    ----------
-    .. [1] Wu Zhihao, Lin You-Fang, Gregory Steve, Wan Huai-Yu, Tian Sheng-Feng
-        Balanced Multi-Label Propagation for Overlapping Community Detection in Social Networks
-
-    """
-    nodes = list(G.nodes.keys())
-    if len(nodes) == 1:
-        return {1: [nodes[0]]}
-    cores = Rough_Cores(G)
-    nodes = G.nodes
-    i = 0
-    old_label_dict = dict()
-    new_label_dict = dict()
-    for core in cores:
-        for node in core:
-            if node not in old_label_dict:
-                old_label_dict[node] = {i: 1}
-            else:
-                old_label_dict[node][i] = 1
-            i += 1
-    oldMin = dict()
-    loop_count = 0
-    old_label_dictx = dict()
-    while True:
-        loop_count += 1
-        old_label_dictx = old_label_dict
-        for node in nodes:
-            Propagate_bbc(G, node, old_label_dict, new_label_dict, p)
-        if loop_count > 50 and old_label_dict == old_label_dictx:
-            break
-        Min = dict()
-        if Id(old_label_dict) == Id(new_label_dict):
-            Min = mc(count(old_label_dict), count(new_label_dict))
-        else:
-            Min = count(new_label_dict)
-        if loop_count > 500:
-            break
-        if Min != oldMin:
-            old_label_dict = copy.deepcopy(new_label_dict)
-            oldMin = copy.deepcopy(Min)
-        else:
-            break
-    print("After %d iterations, BMLPA complete." % loop_count)
-    communities = dict()
-    for node in nodes:
-        for label, _ in old_label_dict[node].items():
-            if label in communities:
-                communities[label].add(node)
-            else:
-                communities[label] = {node}
-    RemoveNested(communities)
-    result_community = CheckConnectivity(G, communities)
-    return result_community
-
-
-def RemoveNested(communities):
-    nestedCommunities = set()
-    keys = list(communities.keys())
-    for i, label0 in enumerate(keys[:-1]):
-        comm0 = communities[label0]
-        for label1 in keys[i + 1 :]:
-            comm1 = communities[label1]
-            if comm0.issubset(comm1):
-                nestedCommunities.add(label0)
-            elif comm0.issuperset(comm1):
-                nestedCommunities.add(label1)
-    for comm in nestedCommunities:
-        del communities[comm]
-
-
-def SelectLabels(G, node, label_dict):
-    adj = G.adj
-    count = {}
-    count_items = []
-    for neighbor in adj[node]:
-        neighbor_label = label_dict[neighbor]
-        count[neighbor_label] = count.get(neighbor_label, 0) + 1
-        count_items = sorted(count.items(), key=lambda x: x[1], reverse=True)
-    labels = [k for k, v in count_items if v == count_items[0][1]]
-    return labels
-
-
-def estimate_stop_cond(G, label_dict):
-    for node in G.nodes:
-        if SelectLabels(G, node, label_dict) != [] and (
-            label_dict[node] not in SelectLabels(G, node, label_dict)
-        ):
-            return False
-    return True
-
-
-def SelectLabels_HANP(G, node, label_dict, score_dict, degrees, m, threshod):
-    adj = G.adj
-    count = defaultdict(float)
-    cnt = defaultdict(int)
-    for neighbor in adj[node]:
-        neighbor_label = label_dict[neighbor]
-        cnt[neighbor_label] += 1
-        count[neighbor_label] += (
-            score_dict[neighbor_label]
-            * (degrees[neighbor] ** m)
-            * adj[node][neighbor].get("weight", 1)
-        )
-    count_items = sorted(count.items(), key=lambda x: x[1], reverse=True)
-    labels = [k for k, v in count_items if v == count_items[0][1]]
-    # only update node whose number of neighbors sharing the maximal label is less than a certain percentage.
-    if count_items == []:
-        return []
-    if round(cnt[count_items[0][0]] / len(adj[node]), 2) > threshod:
-        return [label_dict[node]]
-    return labels
-
-
-def HopAttenuation_Hier(G, node, label_dict, node_dict, distance_dict):
-    distance = float("inf")
-    Max_distance = 0
-    adj = G.adj
-    label = label_dict[node]
-    ori_node = node_dict[label]
-    for _, distancex in distance_dict[ori_node].items():
-        Max_distance = max(Max_distance, distancex)
-    for neighbor in adj[node]:
-        if label_dict[neighbor] == label:
-            distance = min(distance, distance_dict[ori_node][neighbor])
-    return round((1 + distance) / Max_distance, 2)
-
-
-def UpdateScore_Hier(G, node, label_dict, node_dict, distance_dict):
-    return 1 - HopAttenuation_Hier(G, node, label_dict, node_dict, distance_dict)
-
-
-def UpdateScore(G, node, label_dict, score_dict, delta):
-    adj = G.adj
-    Max_score = 0
-    label = label_dict[node]
-    for neighbor in adj[node]:
-        if label_dict[neighbor] == label:
-            Max_score = max(Max_score, score_dict[label_dict[neighbor]])
-    return Max_score - delta
-
-
-def estimate_stop_cond_HANP(G, label_dict, score_dict, degrees, m, threshod):
-    for node in G.nodes:
-        if SelectLabels_HANP(
-            G, node, label_dict, score_dict, degrees, m, threshod
-        ) != [] and label_dict[node] not in SelectLabels_HANP(
-            G, node, label_dict, score_dict, degrees, m, threshod
-        ):
-            return False
-    return True
-
-
-def CombineNodes(
-    records,
-    G,
-    label_dict,
-    score_dict,
-    node_dict,
-    Next_label_dict,
-    nodes,
-    degrees,
-    distance_dict,
-):
-    onerecord = dict()
-    for node, label in label_dict.items():
-        if label in onerecord:
-            onerecord[label].append(node)
-        else:
-            onerecord[label] = [node]
-    records.append(onerecord)
-    Gx = eg.Graph()
-    label_dictx = dict()
-    score_dictx = dict()
-    node_dictx = dict()
-    nodesx = []
-    cnt = 0
-    for record_label in onerecord:
-        nodesx.append(cnt)
-        label_dictx[cnt] = record_label
-        score_dictx[record_label] = score_dict[record_label]
-        node_dictx[record_label] = cnt
-        cnt += 1
-    record_labels = list(onerecord.keys())
-    i = 0
-    edge = dict()
-    adj = G.adj
-    for i in range(0, len(record_labels)):
-        edge[i] = dict()
-        for j in range(0, len(record_labels)):
-            if i == j:
-                continue
-            inodes = onerecord[record_labels[i]]
-            jnodes = onerecord[record_labels[j]]
-            for unode in inodes:
-                for vnode in jnodes:
-                    if unode in adj and vnode in adj[unode]:
-                        if j not in edge[i]:
-                            edge[i][j] = 0
-                        edge[i][j] += adj[unode][vnode].get("weight", 1)
-    for unode in edge:
-        for vnode, w in edge[unode].items():
-            if unode < vnode:
-                Gx.add_edge(unode, vnode, weight=w)
-    G = Gx
-    label_dict = label_dictx
-    score_dict = score_dictx
-    node_dict = node_dictx
-    Next_label_dict = label_dictx
-    nodes = nodesx
-    degrees = G.degree()
-    distance_dict = eg.Floyd(G)
-    return (
-        records,
-        G,
-        label_dict,
-        score_dict,
-        node_dict,
-        Next_label_dict,
-        nodes,
-        degrees,
-        distance_dict,
-    )
-
-
-def ShowRecord(records):
-    """
-    e.g.
-        records : [ {1:[1,2,3,4],2:[5,6,7,8],3:[9],4:[10],5:[11],6:[12]},
-                        {2:[0,1,3],3:[2,4,5]},
-                            {2:[0,1]} ]
-
-        process :   {1:[1,2,3,4],2:[5,6,7,8],3:[9],4:[10],5:[11],6:[12]} ->
-                        {2:[ [1,2,3,4] + [5,6,7,8] + [10] ], 3:[ [9] + [11] + [12] ]} ->
-                            {2:[ ([ [1,2,3,4] + [5,6,7,8] + [10] ]) + ([ [9] + [11] + [12] ] ]) } ->
-
-        return :    {2:[1,2,3,4,5,6,7,8,10,9,11,12]}
-    """
-    result = dict()
-    first = records[0]
-    for i in range(1, len(records)):
-        keys = list(first.keys())
-        onerecord = records[i]
-        result = {}
-        for label, nodes in onerecord.items():
-            for unode in nodes:
-                for vnode in first[keys[unode]]:
-                    if label not in result:
-                        result[label] = []
-                    result[label].append(vnode)
-        first = result
-    return first
-
-
-def CheckConnectivity(G, communities):
-    result_community = dict()
-    community = [list(community) for label, community in communities.items()]
-    communityx = []
-    for nodes in community:
-        BFS(G, nodes, communityx)
-    i = 0
-    for com in communityx:
-        i += 1
-        result_community[i] = com
-    return result_community
-
-
-def BFS(G, nodes, result):
-    # check the nodes in G are connected or not. if not, desperate the nodes into different connected subgraphs.
-    if len(nodes) == 0:
-        return
-    if len(nodes) == 1:
-        result.append(nodes)
-        return
-    adj = G.adj
-    queue = Queue()
-    queue.put(nodes[0])
-    seen = set()
-    seen.add(nodes[0])
-    count = 0
-    while queue.empty() == 0:
-        vertex = queue.get()
-        count += 1
-        for w in adj[vertex]:
-            if w in nodes and w not in seen:
-                queue.put(w)
-                seen.add(w)
-    if count != len(nodes):
-        result.append([w for w in seen])
-        return BFS(G, [w for w in nodes if w not in seen], result)
-    else:
-        result.append(nodes)
-        return
-
-
-def Rough_Cores(G):
-    nodes = G.nodes
-    degrees = G.degree()
-    adj = G.adj
-    seen_dict = dict()
-    label_dict = dict()
-    cores = []
-    i = 0
-    for node in nodes:
-        label_dict[node] = i
-        seen_dict[node] = 1
-        i += 1
-    degree_list = sorted(degrees.items(), key=lambda x: x[1], reverse=True)
-    for node, _ in degree_list:
-        core = []
-        if degrees[node] >= 3 and seen_dict[node] == 1:
-            for neighbor in adj[node]:
-                max_degree = 0
-                j = node
-                if seen_dict[neighbor] == 1:
-                    if degrees[neighbor] > max_degree:
-                        max_degree = degrees[neighbor]
-                        j = neighbor
-                    elif degrees[neighbor] == max_degree:
-                        pass
-                if j != []:
-                    core = [node] + [j]
-                    commNeiber = [i for i in adj[node] if i in adj[j]]
-                    commNeiber = [node for node, _ in degree_list if node in commNeiber]
-                    commNeiber = commNeiber[::-1]
-                    while commNeiber != []:
-                        for h in commNeiber:
-                            core.append(h)
-                            for x in commNeiber:
-                                if x not in adj[h]:
-                                    commNeiber.remove(x)
-                            if h in commNeiber:
-                                commNeiber.remove(h)
-        if len(core) >= 3:
-            for i in core:
-                seen_dict[i] = 0
-            cores.append(core)
-    core_node = []
-    for core in cores:
-        core_node += core
-    for node in nodes:
-        if node not in core_node:
-            cores.append([node])
-    return cores
-
-
-def Normalizer(l):
-    Sum = 0
-    for identifier, coefficient in l.items():
-        Sum += coefficient
-    for identifier, coefficient in l.items():
-        l[identifier] = round(coefficient / Sum, 2)
-
-
-def Propagate_bbc(G, x, source, dest, p):
-    adj = G.adj
-    dest[x] = dict()
-    max_b = 0
-    for y in adj[x]:
-        for identifier, coefficient in source[y].items():
-            b = coefficient
-            if identifier in dest[x]:
-                dest[x][identifier] += b
-            else:
-                dest[x][identifier] = b
-            max_b = max(dest[x][identifier], max_b)
-    if max_b == 0:
-        dest[x] = source[x]
-        return
-    for identifier in list(dest[x].keys()):
-        if dest[x][identifier] / max_b < p:
-            del dest[x][identifier]
-    Normalizer(dest[x])
-
-
-def Id(l):
-    ids = dict()
-    for x in l:
-        ids[x] = Id1(l[x])
-    return ids
-
-
-def Id1(x):
-    ids = []
-    for identifier, _ in x.items():
-        if identifier not in ids:
-            ids.append(identifier)
-    return ids
-
-
-def count(l):
-    counts = dict()
-    for x in l:
-        for identifier, _ in l[x].items():
-            if identifier in counts:
-                counts[identifier] += 1
-            else:
-                counts[identifier] = 1
-    return counts
-
-
-def mc(cs1, cs2):
-    cs = dict()
-    for identifier, _ in cs1.items():
-        cs[identifier] = min(cs1[identifier], cs2[identifier])
-    return cs
+import copy
+import random
+
+from collections import defaultdict
+from queue import Queue
+
+import easygraph as eg
+import numpy as np
+
+from easygraph.utils import *
+
+
+__all__ = [
+    "LPA",
+    "SLPA",
+    "HANP",
+    "BMLPA",
+]
+
+
+@not_implemented_for("multigraph")
+def LPA(G):
+    """Detect community by label propagation algorithm
+    Return the detected communities. But the result is random.
+    Each node in the network is initially assigned to its own community. At every iteration,nodes have
+    a label that the maximum number of their neighbors have. If there are more than one nodes fit and
+    available, choose a label randomly. Finally, nodes having the same labels are grouped together as
+    communities. In case two or more disconnected groups of nodes have the same label, we run a simple
+    breadth-first search to separate the disconnected communities
+
+    Parameters
+    ----------
+    G : graph
+      A easygraph graph
+
+    Returns
+    ----------
+    communities : dictionary
+      key: serial number of community , value: nodes in the community.
+
+    Examples
+    ----------
+    >>> LPA(G)
+
+    References
+    ----------
+    .. [1] Usha Nandini Raghavan, Réka Albert, and Soundar Kumara:
+        Near linear time algorithm to detect community structures in large-scale networks
+    """
+    i = 0
+    label_dict = dict()
+    cluster_community = dict()
+    Next_label_dict = dict()
+    nodes = list(G.nodes.keys())
+    if len(nodes) == 1:
+        return {1: [nodes[0]]}
+    for node in nodes:
+        label_dict[node] = i
+        i = i + 1
+    loop_count = 0
+    while True:
+        loop_count += 1
+        random.shuffle(nodes)
+        for node in nodes:
+            labels = SelectLabels(G, node, label_dict)
+            if labels == []:
+                Next_label_dict[node] = label_dict[node]
+                continue
+            Next_label_dict[node] = random.choice(labels)
+            # Asynchronous updates. If you want to use synchronous updates, comment the line below
+            label_dict[node] = Next_label_dict[node]
+        label_dict = Next_label_dict
+        if estimate_stop_cond(G, label_dict) is True:
+            break
+    for node in label_dict.keys():
+        label = label_dict[node]
+        if label not in cluster_community.keys():
+            cluster_community[label] = [node]
+        else:
+            cluster_community[label].append(node)
+
+    result_community = CheckConnectivity(G, cluster_community)
+    return result_community
+
+
+@not_implemented_for("multigraph")
+def SLPA(G, T, r):
+    """Detect Overlapping Communities by Speaker-listener Label Propagation Algorithm
+    Return the detected Overlapping communities. But the result is random.
+
+    Parameters
+    ----------
+    G : graph
+      A easygraph graph.
+    T : int
+      The number of iterations, In general, T is set greater than 20, which produces relatively stable outputs.
+    r : int
+      a threshold between 0 and 1.
+
+    Returns
+    -------
+    communities : dictionary
+      key: serial number of community , value: nodes in the community.
+
+    Examples
+    ----------
+    >>> SLPA(G,
+    ...     T = 20,
+    ...     r = 0.05
+    ...     )
+
+    References
+    ----------
+    .. [1] Jierui Xie, Boleslaw K. Szymanski, Xiaoming Liu:
+        SLPA: Uncovering Overlapping Communities in Social Networks via A Speaker-listener Interaction Dynamic Process
+    """
+    nodes = list(G.nodes.keys())
+    if len(nodes) == 1:
+        return {1: [nodes[0]]}
+    nodes = G.nodes
+    adj = G.adj
+    memory = {i: {i: 1} for i in nodes}
+    for i in range(0, T):
+        listenerslist = list(G.nodes)
+        random.shuffle(listenerslist)
+        for listener in listenerslist:
+            speakerlist = adj[listener]
+            if len(speakerlist) == 0:
+                continue
+            labels = defaultdict(int)
+            for speaker in speakerlist:
+                # Speaker Rule
+                total = float(sum(memory[speaker].values()))
+                keys = list(memory[speaker].keys())
+                index = np.random.multinomial(
+                    1, [round(freq / total, 2) for freq in memory[speaker].values()]
+                ).argmax()
+                chosen_label = keys[index]
+                labels[chosen_label] += 1
+            # Listener Rule
+            maxlabel = max(labels.items(), key=lambda x: x[1])[0]
+            if maxlabel in memory[listener]:
+                memory[listener][maxlabel] += 1
+            else:
+                memory[listener][maxlabel] = 1
+
+    for node, labels in memory.items():
+        name_list = []
+        for label_name, label_number in labels.items():
+            if round(label_number / float(T + 1), 2) < r:
+                name_list.append(label_name)
+        for name in name_list:
+            del labels[name]
+
+    # Find nodes membership
+    communities = {}
+    for node, labels in memory.items():
+        for label in labels:
+            if label in communities:
+                communities[label].add(node)
+            else:
+                communities[label] = {node}
+
+    # Remove nested communities
+    RemoveNested(communities)
+
+    # Check Connectivity
+    result_community = CheckConnectivity(G, communities)
+    return result_community
+
+
+@not_implemented_for("multigraph")
+def HANP(G, m, delta, threshod=1, hier_open=0, combine_open=0):
+    """Detect community by Hop attenuation & node preference algorithm
+
+    Return the detected communities. But the result is random.
+
+    Implement the basic HANP algorithm and give more freedom through the parameters, e.g., you can use threshod
+    to set the condition for node updating. If network are known to be Hierarchical and overlapping communities,
+    it's recommended to choose geodesic distance as the measure(instead of receiving the current hop scores
+    from the neighborhood and carry out a subtraction) and When an equilibrium is reached, treat newly combined
+    communities as a single node.
+
+    For using Floyd to get the shortest distance, the time complexity is a little high.
+
+    Parameters
+    ----------
+    G : graph
+      A easygraph graph
+    m : float
+      Used to calculate score, when m > 0, more preference is given to node with more neighbors; m < 0, less
+    delta : float
+      Hop attenuation
+    threshod : float
+      Between 0 and 1, only update node whose number of neighbors sharing the maximal label is less than the threshod.
+      e.g., threshod == 1 means updating all nodes.
+    hier_open :
+      1 means using geodesic distance as the score measure.
+      0 means not.
+    combine_open :
+      this option is valid only when hier_open = 1
+      1 means When an equilibrium is reached, treat newly combined communities as a single node.
+      0 means not.
+
+    Returns
+    ----------
+    communities : dictionary
+      key: serial number of community , value: nodes in the community.
+
+    Examples
+    ----------
+    >>> HANP(G,
+    ...     m = 0.1,
+    ...     delta = 0.05,
+    ...     threshod = 1,
+    ...     hier_open = 0,
+    ...     combine_open = 0
+    ...     )
+
+    References
+    ----------
+    .. [1] Ian X. Y. Leung, Pan Hui, Pietro Liò, and Jon Crowcrof:
+        Towards real-time community detection in large networks
+
+    """
+    nodes = list(G.nodes.keys())
+    if len(nodes) == 1:
+        return {1: [nodes[0]]}
+    label_dict = dict()
+    score_dict = dict()
+    node_dict = dict()
+    Next_label_dict = dict()
+    cluster_community = dict()
+    nodes = list(G.nodes.keys())
+    degrees = G.degree()
+    records = []
+    loop_count = 0
+    i = 0
+    old_score = 1
+    ori_G = G
+    if hier_open == 1:
+        distance_dict = eg.Floyd(G)
+    for node in nodes:
+        label_dict[node] = i
+        score_dict[i] = 1
+        node_dict[i] = node
+        i = i + 1
+    while True:
+        loop_count += 1
+        random.shuffle(nodes)
+        score = 1
+        for node in nodes:
+            labels = SelectLabels_HANP(
+                G, node, label_dict, score_dict, degrees, m, threshod
+            )
+            if labels == []:
+                Next_label_dict[node] = label_dict[node]
+                continue
+            old_label = label_dict[node]
+            Next_label_dict[node] = random.choice(labels)
+            # Asynchronous updates. If you want to use synchronous updates, comment the line below
+            label_dict[node] = Next_label_dict[node]
+            if hier_open == 1:
+                score_dict[Next_label_dict[node]] = UpdateScore_Hier(
+                    G, node, label_dict, node_dict, distance_dict
+                )
+                score = min(score, score_dict[Next_label_dict[node]])
+            else:
+                if old_label == Next_label_dict[node]:
+                    cdelta = 0
+                else:
+                    cdelta = delta
+                score_dict[Next_label_dict[node]] = UpdateScore(
+                    G, node, label_dict, score_dict, cdelta
+                )
+        if hier_open == 1 and combine_open == 1:
+            if old_score - score > 1 / 3:
+                old_score = score
+                (
+                    records,
+                    G,
+                    label_dict,
+                    score_dict,
+                    node_dict,
+                    Next_label_dict,
+                    nodes,
+                    degrees,
+                    distance_dict,
+                ) = CombineNodes(
+                    records,
+                    G,
+                    label_dict,
+                    score_dict,
+                    node_dict,
+                    Next_label_dict,
+                    nodes,
+                    degrees,
+                    distance_dict,
+                )
+        label_dict = Next_label_dict
+        if (
+            estimate_stop_cond_HANP(G, label_dict, score_dict, degrees, m, threshod)
+            is True
+        ):
+            break
+        """As mentioned in the paper, it's suggested that the number of iterations
+        required is independent to the number of nodes and that after
+        five iterations, 95% of their nodes are already accurately clustered
+        """
+        if loop_count > 20:
+            break
+    print("After %d iterations, HANP complete." % loop_count)
+    for node in label_dict.keys():
+        label = label_dict[node]
+        if label not in cluster_community.keys():
+            cluster_community[label] = [node]
+        else:
+            cluster_community[label].append(node)
+    if hier_open == 1 and combine_open == 1:
+        records.append(cluster_community)
+        cluster_community = ShowRecord(records)
+    result_community = CheckConnectivity(ori_G, cluster_community)
+    return result_community
+
+
+@not_implemented_for("multigraph")
+def BMLPA(G, p):
+    """Detect community by Balanced Multi-Label Propagation algorithm
+
+    Return the detected communities.
+
+    Firstly, initialize 'old' using cores generated by RC function, the propagate label till the number and size
+    of communities stay no change, check if there are subcommunity and delete it. Finally, split discontinuous
+    communities.
+
+    For some directed graphs lead to oscillations of labels, modify the stop condition.
+
+    Parameters
+    ----------
+    G : graph
+      A easygraph graph
+    p : float
+      Between 0 and 1, judge Whether a community identifier should be retained
+
+    Returns
+    ----------
+    communities : dictionary
+      key: serial number of community , value: nodes in the community.
+
+    Examples
+    ----------
+    >>> BMLPA(G,
+    ...     p = 0.1,
+    ...     )
+
+    References
+    ----------
+    .. [1] Wu Zhihao, Lin You-Fang, Gregory Steve, Wan Huai-Yu, Tian Sheng-Feng
+        Balanced Multi-Label Propagation for Overlapping Community Detection in Social Networks
+
+    """
+    nodes = list(G.nodes.keys())
+    if len(nodes) == 1:
+        return {1: [nodes[0]]}
+    cores = Rough_Cores(G)
+    nodes = G.nodes
+    i = 0
+    old_label_dict = dict()
+    new_label_dict = dict()
+    for core in cores:
+        for node in core:
+            if node not in old_label_dict:
+                old_label_dict[node] = {i: 1}
+            else:
+                old_label_dict[node][i] = 1
+            i += 1
+    oldMin = dict()
+    loop_count = 0
+    old_label_dictx = dict()
+    while True:
+        loop_count += 1
+        old_label_dictx = old_label_dict
+        for node in nodes:
+            Propagate_bbc(G, node, old_label_dict, new_label_dict, p)
+        if loop_count > 50 and old_label_dict == old_label_dictx:
+            break
+        Min = dict()
+        if Id(old_label_dict) == Id(new_label_dict):
+            Min = mc(count(old_label_dict), count(new_label_dict))
+        else:
+            Min = count(new_label_dict)
+        if loop_count > 500:
+            break
+        if Min != oldMin:
+            old_label_dict = copy.deepcopy(new_label_dict)
+            oldMin = copy.deepcopy(Min)
+        else:
+            break
+    print("After %d iterations, BMLPA complete." % loop_count)
+    communities = dict()
+    for node in nodes:
+        for label, _ in old_label_dict[node].items():
+            if label in communities:
+                communities[label].add(node)
+            else:
+                communities[label] = {node}
+    RemoveNested(communities)
+    result_community = CheckConnectivity(G, communities)
+    return result_community
+
+
+def RemoveNested(communities):
+    nestedCommunities = set()
+    keys = list(communities.keys())
+    for i, label0 in enumerate(keys[:-1]):
+        comm0 = communities[label0]
+        for label1 in keys[i + 1 :]:
+            comm1 = communities[label1]
+            if comm0.issubset(comm1):
+                nestedCommunities.add(label0)
+            elif comm0.issuperset(comm1):
+                nestedCommunities.add(label1)
+    for comm in nestedCommunities:
+        del communities[comm]
+
+
+def SelectLabels(G, node, label_dict):
+    adj = G.adj
+    count = {}
+    count_items = []
+    for neighbor in adj[node]:
+        neighbor_label = label_dict[neighbor]
+        count[neighbor_label] = count.get(neighbor_label, 0) + 1
+        count_items = sorted(count.items(), key=lambda x: x[1], reverse=True)
+    labels = [k for k, v in count_items if v == count_items[0][1]]
+    return labels
+
+
+def estimate_stop_cond(G, label_dict):
+    for node in G.nodes:
+        if SelectLabels(G, node, label_dict) != [] and (
+            label_dict[node] not in SelectLabels(G, node, label_dict)
+        ):
+            return False
+    return True
+
+
+def SelectLabels_HANP(G, node, label_dict, score_dict, degrees, m, threshod):
+    adj = G.adj
+    count = defaultdict(float)
+    cnt = defaultdict(int)
+    for neighbor in adj[node]:
+        neighbor_label = label_dict[neighbor]
+        cnt[neighbor_label] += 1
+        count[neighbor_label] += (
+            score_dict[neighbor_label]
+            * (degrees[neighbor] ** m)
+            * adj[node][neighbor].get("weight", 1)
+        )
+    count_items = sorted(count.items(), key=lambda x: x[1], reverse=True)
+    labels = [k for k, v in count_items if v == count_items[0][1]]
+    # only update node whose number of neighbors sharing the maximal label is less than a certain percentage.
+    if count_items == []:
+        return []
+    if round(cnt[count_items[0][0]] / len(adj[node]), 2) > threshod:
+        return [label_dict[node]]
+    return labels
+
+
+def HopAttenuation_Hier(G, node, label_dict, node_dict, distance_dict):
+    distance = float("inf")
+    Max_distance = 0
+    adj = G.adj
+    label = label_dict[node]
+    ori_node = node_dict[label]
+    for _, distancex in distance_dict[ori_node].items():
+        Max_distance = max(Max_distance, distancex)
+    for neighbor in adj[node]:
+        if label_dict[neighbor] == label:
+            distance = min(distance, distance_dict[ori_node][neighbor])
+    return round((1 + distance) / Max_distance, 2)
+
+
+def UpdateScore_Hier(G, node, label_dict, node_dict, distance_dict):
+    return 1 - HopAttenuation_Hier(G, node, label_dict, node_dict, distance_dict)
+
+
+def UpdateScore(G, node, label_dict, score_dict, delta):
+    adj = G.adj
+    Max_score = 0
+    label = label_dict[node]
+    for neighbor in adj[node]:
+        if label_dict[neighbor] == label:
+            Max_score = max(Max_score, score_dict[label_dict[neighbor]])
+    return Max_score - delta
+
+
+def estimate_stop_cond_HANP(G, label_dict, score_dict, degrees, m, threshod):
+    for node in G.nodes:
+        if SelectLabels_HANP(
+            G, node, label_dict, score_dict, degrees, m, threshod
+        ) != [] and label_dict[node] not in SelectLabels_HANP(
+            G, node, label_dict, score_dict, degrees, m, threshod
+        ):
+            return False
+    return True
+
+
+def CombineNodes(
+    records,
+    G,
+    label_dict,
+    score_dict,
+    node_dict,
+    Next_label_dict,
+    nodes,
+    degrees,
+    distance_dict,
+):
+    onerecord = dict()
+    for node, label in label_dict.items():
+        if label in onerecord:
+            onerecord[label].append(node)
+        else:
+            onerecord[label] = [node]
+    records.append(onerecord)
+    Gx = eg.Graph()
+    label_dictx = dict()
+    score_dictx = dict()
+    node_dictx = dict()
+    nodesx = []
+    cnt = 0
+    for record_label in onerecord:
+        nodesx.append(cnt)
+        label_dictx[cnt] = record_label
+        score_dictx[record_label] = score_dict[record_label]
+        node_dictx[record_label] = cnt
+        cnt += 1
+    record_labels = list(onerecord.keys())
+    i = 0
+    edge = dict()
+    adj = G.adj
+    for i in range(0, len(record_labels)):
+        edge[i] = dict()
+        for j in range(0, len(record_labels)):
+            if i == j:
+                continue
+            inodes = onerecord[record_labels[i]]
+            jnodes = onerecord[record_labels[j]]
+            for unode in inodes:
+                for vnode in jnodes:
+                    if unode in adj and vnode in adj[unode]:
+                        if j not in edge[i]:
+                            edge[i][j] = 0
+                        edge[i][j] += adj[unode][vnode].get("weight", 1)
+    for unode in edge:
+        for vnode, w in edge[unode].items():
+            if unode < vnode:
+                Gx.add_edge(unode, vnode, weight=w)
+    G = Gx
+    label_dict = label_dictx
+    score_dict = score_dictx
+    node_dict = node_dictx
+    Next_label_dict = label_dictx
+    nodes = nodesx
+    degrees = G.degree()
+    distance_dict = eg.Floyd(G)
+    return (
+        records,
+        G,
+        label_dict,
+        score_dict,
+        node_dict,
+        Next_label_dict,
+        nodes,
+        degrees,
+        distance_dict,
+    )
+
+
+def ShowRecord(records):
+    """
+    e.g.
+        records : [ {1:[1,2,3,4],2:[5,6,7,8],3:[9],4:[10],5:[11],6:[12]},
+                        {2:[0,1,3],3:[2,4,5]},
+                            {2:[0,1]} ]
+
+        process :   {1:[1,2,3,4],2:[5,6,7,8],3:[9],4:[10],5:[11],6:[12]} ->
+                        {2:[ [1,2,3,4] + [5,6,7,8] + [10] ], 3:[ [9] + [11] + [12] ]} ->
+                            {2:[ ([ [1,2,3,4] + [5,6,7,8] + [10] ]) + ([ [9] + [11] + [12] ] ]) } ->
+
+        return :    {2:[1,2,3,4,5,6,7,8,10,9,11,12]}
+    """
+    result = dict()
+    first = records[0]
+    for i in range(1, len(records)):
+        keys = list(first.keys())
+        onerecord = records[i]
+        result = {}
+        for label, nodes in onerecord.items():
+            for unode in nodes:
+                for vnode in first[keys[unode]]:
+                    if label not in result:
+                        result[label] = []
+                    result[label].append(vnode)
+        first = result
+    return first
+
+
+def CheckConnectivity(G, communities):
+    result_community = dict()
+    community = [list(community) for label, community in communities.items()]
+    communityx = []
+    for nodes in community:
+        BFS(G, nodes, communityx)
+    i = 0
+    for com in communityx:
+        i += 1
+        result_community[i] = com
+    return result_community
+
+
+def BFS(G, nodes, result):
+    # check the nodes in G are connected or not. if not, desperate the nodes into different connected subgraphs.
+    if len(nodes) == 0:
+        return
+    if len(nodes) == 1:
+        result.append(nodes)
+        return
+    adj = G.adj
+    queue = Queue()
+    queue.put(nodes[0])
+    seen = set()
+    seen.add(nodes[0])
+    count = 0
+    while queue.empty() == 0:
+        vertex = queue.get()
+        count += 1
+        for w in adj[vertex]:
+            if w in nodes and w not in seen:
+                queue.put(w)
+                seen.add(w)
+    if count != len(nodes):
+        result.append([w for w in seen])
+        return BFS(G, [w for w in nodes if w not in seen], result)
+    else:
+        result.append(nodes)
+        return
+
+
+def Rough_Cores(G):
+    nodes = G.nodes
+    degrees = G.degree()
+    adj = G.adj
+    seen_dict = dict()
+    label_dict = dict()
+    cores = []
+    i = 0
+    for node in nodes:
+        label_dict[node] = i
+        seen_dict[node] = 1
+        i += 1
+    degree_list = sorted(degrees.items(), key=lambda x: x[1], reverse=True)
+    for node, _ in degree_list:
+        core = []
+        if degrees[node] >= 3 and seen_dict[node] == 1:
+            for neighbor in adj[node]:
+                max_degree = 0
+                j = node
+                if seen_dict[neighbor] == 1:
+                    if degrees[neighbor] > max_degree:
+                        max_degree = degrees[neighbor]
+                        j = neighbor
+                    elif degrees[neighbor] == max_degree:
+                        pass
+                if j != []:
+                    core = [node] + [j]
+                    commNeiber = [i for i in adj[node] if i in adj[j]]
+                    commNeiber = [node for node, _ in degree_list if node in commNeiber]
+                    commNeiber = commNeiber[::-1]
+                    while commNeiber != []:
+                        for h in commNeiber:
+                            core.append(h)
+                            for x in commNeiber:
+                                if x not in adj[h]:
+                                    commNeiber.remove(x)
+                            if h in commNeiber:
+                                commNeiber.remove(h)
+        if len(core) >= 3:
+            for i in core:
+                seen_dict[i] = 0
+            cores.append(core)
+    core_node = []
+    for core in cores:
+        core_node += core
+    for node in nodes:
+        if node not in core_node:
+            cores.append([node])
+    return cores
+
+
+def Normalizer(l):
+    Sum = 0
+    for identifier, coefficient in l.items():
+        Sum += coefficient
+    for identifier, coefficient in l.items():
+        l[identifier] = round(coefficient / Sum, 2)
+
+
+def Propagate_bbc(G, x, source, dest, p):
+    adj = G.adj
+    dest[x] = dict()
+    max_b = 0
+    for y in adj[x]:
+        for identifier, coefficient in source[y].items():
+            b = coefficient
+            if identifier in dest[x]:
+                dest[x][identifier] += b
+            else:
+                dest[x][identifier] = b
+            max_b = max(dest[x][identifier], max_b)
+    if max_b == 0:
+        dest[x] = source[x]
+        return
+    for identifier in list(dest[x].keys()):
+        if dest[x][identifier] / max_b < p:
+            del dest[x][identifier]
+    Normalizer(dest[x])
+
+
+def Id(l):
+    ids = dict()
+    for x in l:
+        ids[x] = Id1(l[x])
+    return ids
+
+
+def Id1(x):
+    ids = []
+    for identifier, _ in x.items():
+        if identifier not in ids:
+            ids.append(identifier)
+    return ids
+
+
+def count(l):
+    counts = dict()
+    for x in l:
+        for identifier, _ in l[x].items():
+            if identifier in counts:
+                counts[identifier] += 1
+            else:
+                counts[identifier] = 1
+    return counts
+
+
+def mc(cs1, cs2):
+    cs = dict()
+    for identifier, _ in cs1.items():
+        cs[identifier] = min(cs1[identifier], cs2[identifier])
+    return cs
```

## easygraph/functions/community/modularity.py

 * *Ordering differences only*

```diff
@@ -1,75 +1,75 @@
-from itertools import product
-
-from easygraph.utils import *
-
-
-__all__ = ["modularity"]
-
-
-@not_implemented_for("multigraph")
-def modularity(G, communities, weight="weight"):
-    r"""
-    Returns the modularity of the given partition of the graph.
-    Modularity is defined in [1]_ as
-
-    .. math::
-
-        Q = \frac{1}{2m} \sum_{ij} \left( A_{ij} - \frac{k_ik_j}{2m}\right)
-            \delta(c_i,c_j)
-
-    where m is the number of edges, A is the adjacency matrix of
-    `G`,
-
-    .. math::
-
-        k_i\ is\ the\ degree\ of\ i\ and\ \delta(c_i, c_j)\ is\ 1\ if\ i\ and\ j\ are\ in\ the\ same\ community\ and\ 0\ otherwise.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-
-    communities : list or iterable of set of nodes
-        These node sets must represent a partition of G's nodes.
-
-    weight : string, optional (default : 'weight')
-        The key for edge weight.
-
-    Returns
-    ----------
-    Q : float
-        The modularity of the partition.
-
-    References
-    ----------
-    .. [1] M. E. J. Newman *Networks: An Introduction*, page 224.
-       Oxford University Press, 2011.
-
-    """
-    # TODO: multigraph not included.
-
-    if not isinstance(communities, list):
-        communities = list(communities)
-
-    directed = G.is_directed()
-    m = G.size(weight=weight)
-    if directed:
-        out_degree = dict(G.out_degree(weight=weight))
-        in_degree = dict(G.in_degree(weight=weight))
-        norm = 1 / m
-    else:
-        out_degree = dict(G.degree(weight=weight))
-        in_degree = out_degree
-        norm = 1 / (2 * m)
-
-    def val(u, v):
-        try:
-            w = G[u][v].get(weight, 1)
-        except KeyError:
-            w = 0
-        # Double count self-loops if the graph is undirected.
-        if u == v and not directed:
-            w *= 2
-        return w - in_degree[u] * out_degree[v] * norm
-
-    Q = sum(val(u, v) for c in communities for u, v in product(c, repeat=2))
-    return Q * norm
+from itertools import product
+
+from easygraph.utils import *
+
+
+__all__ = ["modularity"]
+
+
+@not_implemented_for("multigraph")
+def modularity(G, communities, weight="weight"):
+    r"""
+    Returns the modularity of the given partition of the graph.
+    Modularity is defined in [1]_ as
+
+    .. math::
+
+        Q = \frac{1}{2m} \sum_{ij} \left( A_{ij} - \frac{k_ik_j}{2m}\right)
+            \delta(c_i,c_j)
+
+    where m is the number of edges, A is the adjacency matrix of
+    `G`,
+
+    .. math::
+
+        k_i\ is\ the\ degree\ of\ i\ and\ \delta(c_i, c_j)\ is\ 1\ if\ i\ and\ j\ are\ in\ the\ same\ community\ and\ 0\ otherwise.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+
+    communities : list or iterable of set of nodes
+        These node sets must represent a partition of G's nodes.
+
+    weight : string, optional (default : 'weight')
+        The key for edge weight.
+
+    Returns
+    ----------
+    Q : float
+        The modularity of the partition.
+
+    References
+    ----------
+    .. [1] M. E. J. Newman *Networks: An Introduction*, page 224.
+       Oxford University Press, 2011.
+
+    """
+    # TODO: multigraph not included.
+
+    if not isinstance(communities, list):
+        communities = list(communities)
+
+    directed = G.is_directed()
+    m = G.size(weight=weight)
+    if directed:
+        out_degree = dict(G.out_degree(weight=weight))
+        in_degree = dict(G.in_degree(weight=weight))
+        norm = 1 / m
+    else:
+        out_degree = dict(G.degree(weight=weight))
+        in_degree = out_degree
+        norm = 1 / (2 * m)
+
+    def val(u, v):
+        try:
+            w = G[u][v].get(weight, 1)
+        except KeyError:
+            w = 0
+        # Double count self-loops if the graph is undirected.
+        if u == v and not directed:
+            w *= 2
+        return w - in_degree[u] * out_degree[v] * norm
+
+    Q = sum(val(u, v) for c in communities for u, v in product(c, repeat=2))
+    return Q * norm
```

## easygraph/functions/community/modularity_max_detection.py

 * *Ordering differences only*

```diff
@@ -1,200 +1,200 @@
-from easygraph.functions.community.modularity import modularity
-from easygraph.utils import *
-from easygraph.utils.mapped_queue import MappedQueue
-
-
-__all__ = ["greedy_modularity_communities"]
-
-
-@not_implemented_for("multigraph")
-def greedy_modularity_communities(G, weight="weight"):
-    """Communities detection via greedy modularity method.
-
-    Find communities in graph using Clauset-Newman-Moore greedy modularity
-    maximization. This method currently supports the Graph class.
-
-    Greedy modularity maximization begins with each node in its own community
-    and joins the pair of communities that most increases modularity until no
-    such pair exists.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-
-    weight : string (default : 'weight')
-        The key for edge weight. For undirected graph, it will regard each edge
-        weight as 1.
-
-    Returns
-    ----------
-    Yields sets of nodes, one for each community.
-
-    References
-    ----------
-    .. [1] Newman, M. E. J. "Networks: An Introduction Oxford Univ." (2010).
-    .. [2] Clauset, Aaron, Mark EJ Newman, and Cristopher Moore.
-    "Finding community structure in very large networks." Physical review E 70.6 (2004): 066111.
-    """
-
-    # Count nodes and edges
-
-    N = len(G.nodes)
-    m = sum(d.get(weight, 1) for u, v, d in G.edges)
-    if N == 0 or m == 0:
-        print("Please input the graph which has at least one edge!")
-        exit()
-    q0 = 1.0 / (2.0 * m)
-
-    # Map node labels to contiguous integers
-    label_for_node = {i: v for i, v in enumerate(G.nodes)}
-    node_for_label = {label_for_node[i]: i for i in range(N)}
-
-    # Calculate degrees
-    k_for_label = G.degree(weight=weight)
-    k = [k_for_label[label_for_node[i]] for i in range(N)]
-
-    # Initialize community and merge lists
-    communities = {i: frozenset([i]) for i in range(N)}
-    merges = []
-
-    # Initial modularity
-    partition = [[label_for_node[x] for x in c] for c in communities.values()]
-    q_cnm = modularity(G, partition)
-
-    # Initialize data structures
-    # CNM Eq 8-9 (Eq 8 was missing a factor of 2 (from A_ij + A_ji)
-    # a[i]: fraction of edges within community i
-    # dq_dict[i][j]: dQ for merging community i, j
-    # dq_heap[i][n] : (-dq, i, j) for communitiy i nth largest dQ
-    # H[n]: (-dq, i, j) for community with nth largest max_j(dQ_ij)
-    a = [k[i] * q0 for i in range(N)]
-    dq_dict = {
-        i: {
-            j: 2 * q0 - 2 * k[i] * k[j] * q0 * q0
-            for j in [node_for_label[u] for u in G.neighbors(label_for_node[i])]
-            if j != i
-        }
-        for i in range(N)
-    }
-    dq_heap = [
-        MappedQueue([(-dq, i, j) for j, dq in dq_dict[i].items()]) for i in range(N)
-    ]
-    H = MappedQueue([dq_heap[i].h[0] for i in range(N) if len(dq_heap[i]) > 0])
-
-    # Merge communities until we can't improve modularity
-    while len(H) > 1:
-        # Find best merge
-        # Remove from heap of row maxes
-        # Ties will be broken by choosing the pair with lowest min community id
-        try:
-            dq, i, j = H.pop()
-        except IndexError:
-            break
-        dq = -dq
-        # Remove best merge from row i heap
-        dq_heap[i].pop()
-        # Push new row max onto H
-        if len(dq_heap[i]) > 0:
-            H.push(dq_heap[i].h[0])
-        # If this element was also at the root of row j, we need to remove the
-        # duplicate entry from H
-        if dq_heap[j].h[0] == (-dq, j, i):
-            H.remove((-dq, j, i))
-            # Remove best merge from row j heap
-            dq_heap[j].remove((-dq, j, i))
-            # Push new row max onto H
-            if len(dq_heap[j]) > 0:
-                H.push(dq_heap[j].h[0])
-        else:
-            # Duplicate wasn't in H, just remove from row j heap
-            dq_heap[j].remove((-dq, j, i))
-        # Stop when change is non-positive
-        if dq <= 0:
-            break
-
-        # Perform merge
-        communities[j] = frozenset(communities[i] | communities[j])
-        del communities[i]
-        merges.append((i, j, dq))
-        # New modularity
-        q_cnm += dq
-        # Get list of communities connected to merged communities
-        i_set = set(dq_dict[i].keys())
-        j_set = set(dq_dict[j].keys())
-        all_set = (i_set | j_set) - {i, j}
-        both_set = i_set & j_set
-        # Merge i into j and update dQ
-        for k in all_set:
-            # Calculate new dq value
-            if k in both_set:
-                dq_jk = dq_dict[j][k] + dq_dict[i][k]
-            elif k in j_set:
-                dq_jk = dq_dict[j][k] - 2.0 * a[i] * a[k]
-            else:
-                # k in i_set
-                dq_jk = dq_dict[i][k] - 2.0 * a[j] * a[k]
-            # Update rows j and k
-            for row, col in [(j, k), (k, j)]:
-                # Save old value for finding heap index
-                if k in j_set:
-                    d_old = (-dq_dict[row][col], row, col)
-                else:
-                    d_old = None
-                # Update dict for j,k only (i is removed below)
-                dq_dict[row][col] = dq_jk
-                # Save old max of per-row heap
-                if len(dq_heap[row]) > 0:
-                    d_oldmax = dq_heap[row].h[0]
-                else:
-                    d_oldmax = None
-                # Add/update heaps
-                d = (-dq_jk, row, col)
-                if d_old is None:
-                    # We're creating a new nonzero element, add to heap
-                    dq_heap[row].push(d)
-                else:
-                    # Update existing element in per-row heap
-                    dq_heap[row].update(d_old, d)
-                # Update heap of row maxes if necessary
-                if d_oldmax is None:
-                    # No entries previously in this row, push new max
-                    H.push(d)
-                else:
-                    # We've updated an entry in this row, has the max changed?
-                    if dq_heap[row].h[0] != d_oldmax:
-                        H.update(d_oldmax, dq_heap[row].h[0])
-
-        # Remove row/col i from matrix
-        i_neighbors = dq_dict[i].keys()
-        for k in i_neighbors:
-            # Remove from dict
-            dq_old = dq_dict[k][i]
-            del dq_dict[k][i]
-            # Remove from heaps if we haven't already
-            if k != j:
-                # Remove both row and column
-                for row, col in [(k, i), (i, k)]:
-                    # Check if replaced dq is row max
-                    d_old = (-dq_old, row, col)
-                    if dq_heap[row].h[0] == d_old:
-                        # Update per-row heap and heap of row maxes
-                        dq_heap[row].remove(d_old)
-                        H.remove(d_old)
-                        # Update row max
-                        if len(dq_heap[row]) > 0:
-                            H.push(dq_heap[row].h[0])
-                    else:
-                        # Only update per-row heap
-                        dq_heap[row].remove(d_old)
-
-        del dq_dict[i]
-        # Mark row i as deleted, but keep placeholder
-        dq_heap[i] = MappedQueue()
-        # Merge i into j and update a
-        a[j] += a[i]
-        a[i] = 0
-
-    communities = [
-        frozenset(label_for_node[i] for i in c) for c in communities.values()
-    ]
-    return sorted(communities, key=len, reverse=True)
+from easygraph.functions.community.modularity import modularity
+from easygraph.utils import *
+from easygraph.utils.mapped_queue import MappedQueue
+
+
+__all__ = ["greedy_modularity_communities"]
+
+
+@not_implemented_for("multigraph")
+def greedy_modularity_communities(G, weight="weight"):
+    """Communities detection via greedy modularity method.
+
+    Find communities in graph using Clauset-Newman-Moore greedy modularity
+    maximization. This method currently supports the Graph class.
+
+    Greedy modularity maximization begins with each node in its own community
+    and joins the pair of communities that most increases modularity until no
+    such pair exists.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+
+    weight : string (default : 'weight')
+        The key for edge weight. For undirected graph, it will regard each edge
+        weight as 1.
+
+    Returns
+    ----------
+    Yields sets of nodes, one for each community.
+
+    References
+    ----------
+    .. [1] Newman, M. E. J. "Networks: An Introduction Oxford Univ." (2010).
+    .. [2] Clauset, Aaron, Mark EJ Newman, and Cristopher Moore.
+    "Finding community structure in very large networks." Physical review E 70.6 (2004): 066111.
+    """
+
+    # Count nodes and edges
+
+    N = len(G.nodes)
+    m = sum(d.get(weight, 1) for u, v, d in G.edges)
+    if N == 0 or m == 0:
+        print("Please input the graph which has at least one edge!")
+        exit()
+    q0 = 1.0 / (2.0 * m)
+
+    # Map node labels to contiguous integers
+    label_for_node = {i: v for i, v in enumerate(G.nodes)}
+    node_for_label = {label_for_node[i]: i for i in range(N)}
+
+    # Calculate degrees
+    k_for_label = G.degree(weight=weight)
+    k = [k_for_label[label_for_node[i]] for i in range(N)]
+
+    # Initialize community and merge lists
+    communities = {i: frozenset([i]) for i in range(N)}
+    merges = []
+
+    # Initial modularity
+    partition = [[label_for_node[x] for x in c] for c in communities.values()]
+    q_cnm = modularity(G, partition)
+
+    # Initialize data structures
+    # CNM Eq 8-9 (Eq 8 was missing a factor of 2 (from A_ij + A_ji)
+    # a[i]: fraction of edges within community i
+    # dq_dict[i][j]: dQ for merging community i, j
+    # dq_heap[i][n] : (-dq, i, j) for communitiy i nth largest dQ
+    # H[n]: (-dq, i, j) for community with nth largest max_j(dQ_ij)
+    a = [k[i] * q0 for i in range(N)]
+    dq_dict = {
+        i: {
+            j: 2 * q0 - 2 * k[i] * k[j] * q0 * q0
+            for j in [node_for_label[u] for u in G.neighbors(label_for_node[i])]
+            if j != i
+        }
+        for i in range(N)
+    }
+    dq_heap = [
+        MappedQueue([(-dq, i, j) for j, dq in dq_dict[i].items()]) for i in range(N)
+    ]
+    H = MappedQueue([dq_heap[i].h[0] for i in range(N) if len(dq_heap[i]) > 0])
+
+    # Merge communities until we can't improve modularity
+    while len(H) > 1:
+        # Find best merge
+        # Remove from heap of row maxes
+        # Ties will be broken by choosing the pair with lowest min community id
+        try:
+            dq, i, j = H.pop()
+        except IndexError:
+            break
+        dq = -dq
+        # Remove best merge from row i heap
+        dq_heap[i].pop()
+        # Push new row max onto H
+        if len(dq_heap[i]) > 0:
+            H.push(dq_heap[i].h[0])
+        # If this element was also at the root of row j, we need to remove the
+        # duplicate entry from H
+        if dq_heap[j].h[0] == (-dq, j, i):
+            H.remove((-dq, j, i))
+            # Remove best merge from row j heap
+            dq_heap[j].remove((-dq, j, i))
+            # Push new row max onto H
+            if len(dq_heap[j]) > 0:
+                H.push(dq_heap[j].h[0])
+        else:
+            # Duplicate wasn't in H, just remove from row j heap
+            dq_heap[j].remove((-dq, j, i))
+        # Stop when change is non-positive
+        if dq <= 0:
+            break
+
+        # Perform merge
+        communities[j] = frozenset(communities[i] | communities[j])
+        del communities[i]
+        merges.append((i, j, dq))
+        # New modularity
+        q_cnm += dq
+        # Get list of communities connected to merged communities
+        i_set = set(dq_dict[i].keys())
+        j_set = set(dq_dict[j].keys())
+        all_set = (i_set | j_set) - {i, j}
+        both_set = i_set & j_set
+        # Merge i into j and update dQ
+        for k in all_set:
+            # Calculate new dq value
+            if k in both_set:
+                dq_jk = dq_dict[j][k] + dq_dict[i][k]
+            elif k in j_set:
+                dq_jk = dq_dict[j][k] - 2.0 * a[i] * a[k]
+            else:
+                # k in i_set
+                dq_jk = dq_dict[i][k] - 2.0 * a[j] * a[k]
+            # Update rows j and k
+            for row, col in [(j, k), (k, j)]:
+                # Save old value for finding heap index
+                if k in j_set:
+                    d_old = (-dq_dict[row][col], row, col)
+                else:
+                    d_old = None
+                # Update dict for j,k only (i is removed below)
+                dq_dict[row][col] = dq_jk
+                # Save old max of per-row heap
+                if len(dq_heap[row]) > 0:
+                    d_oldmax = dq_heap[row].h[0]
+                else:
+                    d_oldmax = None
+                # Add/update heaps
+                d = (-dq_jk, row, col)
+                if d_old is None:
+                    # We're creating a new nonzero element, add to heap
+                    dq_heap[row].push(d)
+                else:
+                    # Update existing element in per-row heap
+                    dq_heap[row].update(d_old, d)
+                # Update heap of row maxes if necessary
+                if d_oldmax is None:
+                    # No entries previously in this row, push new max
+                    H.push(d)
+                else:
+                    # We've updated an entry in this row, has the max changed?
+                    if dq_heap[row].h[0] != d_oldmax:
+                        H.update(d_oldmax, dq_heap[row].h[0])
+
+        # Remove row/col i from matrix
+        i_neighbors = dq_dict[i].keys()
+        for k in i_neighbors:
+            # Remove from dict
+            dq_old = dq_dict[k][i]
+            del dq_dict[k][i]
+            # Remove from heaps if we haven't already
+            if k != j:
+                # Remove both row and column
+                for row, col in [(k, i), (i, k)]:
+                    # Check if replaced dq is row max
+                    d_old = (-dq_old, row, col)
+                    if dq_heap[row].h[0] == d_old:
+                        # Update per-row heap and heap of row maxes
+                        dq_heap[row].remove(d_old)
+                        H.remove(d_old)
+                        # Update row max
+                        if len(dq_heap[row]) > 0:
+                            H.push(dq_heap[row].h[0])
+                    else:
+                        # Only update per-row heap
+                        dq_heap[row].remove(d_old)
+
+        del dq_dict[i]
+        # Mark row i as deleted, but keep placeholder
+        dq_heap[i] = MappedQueue()
+        # Merge i into j and update a
+        a[j] += a[i]
+        a[i] = 0
+
+    communities = [
+        frozenset(label_for_node[i] for i in c) for c in communities.values()
+    ]
+    return sorted(communities, key=len, reverse=True)
```

## easygraph/functions/community/__init__.py

 * *Ordering differences only*

```diff
@@ -1,6 +1,6 @@
-from .ego_graph import *
-from .louvain import *
-from .LPA import *
-from .modularity import *
-from .modularity_max_detection import *
-from .motif import *
+from .ego_graph import *
+from .louvain import *
+from .LPA import *
+from .modularity import *
+from .modularity_max_detection import *
+from .motif import *
```

## easygraph/functions/community/tests/test_motif.py

 * *Ordering differences only*

```diff
@@ -1,16 +1,16 @@
-import easygraph as eg
-
-
-class TestMotif:
-    @classmethod
-    def setup_class(self):
-        self.G = eg.Graph()
-        self.G.add_nodes_from([1, 2, 3, 4, 5])
-        self.G.add_edges_from([(1, 3), (2, 3), (3, 4), (4, 5), (3, 5)])
-
-    def test_esu(self):
-        res = eg.enumerate_subgraph(self.G, 3)
-        res = [list(x) for x in res]
-        exp_res = [{1, 3, 4}, {1, 2, 3}, {1, 3, 5}, {2, 3, 5}, {2, 3, 4}, {3, 4, 5}]
-        exp_res = [list(x) for x in exp_res]
-        assert sorted(res) == sorted(exp_res)
+import easygraph as eg
+
+
+class TestMotif:
+    @classmethod
+    def setup_class(self):
+        self.G = eg.Graph()
+        self.G.add_nodes_from([1, 2, 3, 4, 5])
+        self.G.add_edges_from([(1, 3), (2, 3), (3, 4), (4, 5), (3, 5)])
+
+    def test_esu(self):
+        res = eg.enumerate_subgraph(self.G, 3)
+        res = [list(x) for x in res]
+        exp_res = [{1, 3, 4}, {1, 2, 3}, {1, 3, 5}, {2, 3, 5}, {2, 3, 4}, {3, 4, 5}]
+        exp_res = [list(x) for x in exp_res]
+        assert sorted(res) == sorted(exp_res)
```

## easygraph/functions/components/connected.py

 * *Ordering differences only*

```diff
@@ -1,160 +1,160 @@
-from easygraph.utils.decorators import *
-
-
-__all__ = [
-    "is_connected",
-    "number_connected_components",
-    "connected_components",
-    "connected_components_directed",
-    "connected_component_of_node",
-]
-
-
-@not_implemented_for("multigraph")
-def is_connected(G):
-    """Returns whether the graph is connected or not.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-
-    Returns
-    -------
-    is_biconnected : boolean
-        `True` if the graph is connected.
-
-    Examples
-    --------
-
-    >>> is_connected(G)
-
-    """
-    assert len(G) != 0, "No node in the graph."
-    arbitrary_node = next(iter(G))  # Pick an arbitrary node to run BFS
-    return len(G) == sum(1 for node in _plain_bfs(G, arbitrary_node))
-
-
-@not_implemented_for("multigraph")
-def number_connected_components(G):
-    """Returns the number of connected components.
-
-    Parameters
-    ----------
-    G : easygraph.Graph
-
-    Returns
-    -------
-    number_connected_components : int
-        The number of connected components.
-
-    Examples
-    --------
-    >>> number_connected_components(G)
-
-    """
-    return sum(1 for component in _generator_connected_components(G))
-
-
-@not_implemented_for("multigraph")
-@hybrid("cpp_connected_components_undirected")
-def connected_components(G):
-    """Returns a list of connected components, each of which denotes the edges set of a connected component.
-
-    Parameters
-    ----------
-    G : easygraph.Graph
-    Returns
-    -------
-    connected_components : list of list
-        Each element list is the edges set of a connected component.
-
-    Examples
-    --------
-    >>> connected_components(G)
-
-    """
-    seen = set()
-    for v in G:
-        if v not in seen:
-            c = set(_plain_bfs(G, v))
-            seen.update(c)
-            yield c
-
-
-@not_implemented_for("multigraph")
-@hybrid("cpp_connected_components_directed")
-def connected_components_directed(G):
-    """Returns a list of connected components, each of which denotes the edges set of a connected component.
-
-    Parameters
-    ----------
-    G :  easygraph.DiGraph
-    Returns
-    -------
-    connected_components : list of list
-        Each element list is the edges set of a connected component.
-
-    Examples
-    --------
-    >>> connected_components(G)
-
-    """
-    seen = set()
-    for v in G:
-        if v not in seen:
-            c = set(_plain_bfs(G, v))
-            seen.update(c)
-            yield c
-
-
-def _generator_connected_components(G):
-    seen = set()
-    for v in G:
-        if v not in seen:
-            component = set(_plain_bfs(G, v))
-            yield component
-            seen.update(component)
-
-
-@not_implemented_for("multigraph")
-def connected_component_of_node(G, node):
-    """Returns the connected component that *node* belongs to.
-
-    Parameters
-    ----------
-    G : easygraph.Graph
-
-    node : object
-        The target node
-
-    Returns
-    -------
-    connected_component_of_node : set
-        The connected component that *node* belongs to.
-
-    Examples
-    --------
-    Returns the connected component of one node `Jack`.
-
-    >>> connected_component_of_node(G, node='Jack')
-
-    """
-    return set(_plain_bfs(G, node))
-
-
-@hybrid("cpp_plain_bfs")
-def _plain_bfs(G, source):
-    """
-    A fast BFS node generator
-    """
-    G_adj = G.adj
-    seen = set()
-    nextlevel = {source}
-    while nextlevel:
-        thislevel = nextlevel
-        nextlevel = set()
-        for v in thislevel:
-            if v not in seen:
-                yield v
-                seen.add(v)
-                nextlevel.update(G_adj[v])
+from easygraph.utils.decorators import *
+
+
+__all__ = [
+    "is_connected",
+    "number_connected_components",
+    "connected_components",
+    "connected_components_directed",
+    "connected_component_of_node",
+]
+
+
+@not_implemented_for("multigraph")
+def is_connected(G):
+    """Returns whether the graph is connected or not.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+
+    Returns
+    -------
+    is_biconnected : boolean
+        `True` if the graph is connected.
+
+    Examples
+    --------
+
+    >>> is_connected(G)
+
+    """
+    assert len(G) != 0, "No node in the graph."
+    arbitrary_node = next(iter(G))  # Pick an arbitrary node to run BFS
+    return len(G) == sum(1 for node in _plain_bfs(G, arbitrary_node))
+
+
+@not_implemented_for("multigraph")
+def number_connected_components(G):
+    """Returns the number of connected components.
+
+    Parameters
+    ----------
+    G : easygraph.Graph
+
+    Returns
+    -------
+    number_connected_components : int
+        The number of connected components.
+
+    Examples
+    --------
+    >>> number_connected_components(G)
+
+    """
+    return sum(1 for component in _generator_connected_components(G))
+
+
+@not_implemented_for("multigraph")
+@hybrid("cpp_connected_components_undirected")
+def connected_components(G):
+    """Returns a list of connected components, each of which denotes the edges set of a connected component.
+
+    Parameters
+    ----------
+    G : easygraph.Graph
+    Returns
+    -------
+    connected_components : list of list
+        Each element list is the edges set of a connected component.
+
+    Examples
+    --------
+    >>> connected_components(G)
+
+    """
+    seen = set()
+    for v in G:
+        if v not in seen:
+            c = set(_plain_bfs(G, v))
+            seen.update(c)
+            yield c
+
+
+@not_implemented_for("multigraph")
+@hybrid("cpp_connected_components_directed")
+def connected_components_directed(G):
+    """Returns a list of connected components, each of which denotes the edges set of a connected component.
+
+    Parameters
+    ----------
+    G :  easygraph.DiGraph
+    Returns
+    -------
+    connected_components : list of list
+        Each element list is the edges set of a connected component.
+
+    Examples
+    --------
+    >>> connected_components(G)
+
+    """
+    seen = set()
+    for v in G:
+        if v not in seen:
+            c = set(_plain_bfs(G, v))
+            seen.update(c)
+            yield c
+
+
+def _generator_connected_components(G):
+    seen = set()
+    for v in G:
+        if v not in seen:
+            component = set(_plain_bfs(G, v))
+            yield component
+            seen.update(component)
+
+
+@not_implemented_for("multigraph")
+def connected_component_of_node(G, node):
+    """Returns the connected component that *node* belongs to.
+
+    Parameters
+    ----------
+    G : easygraph.Graph
+
+    node : object
+        The target node
+
+    Returns
+    -------
+    connected_component_of_node : set
+        The connected component that *node* belongs to.
+
+    Examples
+    --------
+    Returns the connected component of one node `Jack`.
+
+    >>> connected_component_of_node(G, node='Jack')
+
+    """
+    return set(_plain_bfs(G, node))
+
+
+@hybrid("cpp_plain_bfs")
+def _plain_bfs(G, source):
+    """
+    A fast BFS node generator
+    """
+    G_adj = G.adj
+    seen = set()
+    nextlevel = {source}
+    while nextlevel:
+        thislevel = nextlevel
+        nextlevel = set()
+        for v in thislevel:
+            if v not in seen:
+                yield v
+                seen.add(v)
+                nextlevel.update(G_adj[v])
```

## easygraph/functions/components/weakly_connected.py

 * *Ordering differences only*

```diff
@@ -1,186 +1,186 @@
-"""Weakly connected components."""
-import easygraph as eg
-
-from easygraph.utils.decorators import not_implemented_for
-
-
-__all__ = [
-    "number_weakly_connected_components",
-    "weakly_connected_components",
-    "is_weakly_connected",
-]
-
-
-@not_implemented_for("undirected")
-def weakly_connected_components(G):
-    """Generate weakly connected components of G.
-
-    Parameters
-    ----------
-    G : EasyGraph graph
-        A directed graph
-
-    Returns
-    -------
-    comp : generator of sets
-        A generator of sets of nodes, one for each weakly connected
-        component of G.
-
-    Raises
-    ------
-    EasyGraphNotImplemented
-        If G is undirected.
-
-    Examples
-    --------
-    Generate a sorted list of weakly connected components, largest first.
-
-    >>> G = eg.path_graph(4, create_using=eg.DiGraph())
-    >>> eg.add_path(G, [10, 11, 12])
-    >>> [
-    ...     len(c)
-    ...     for c in sorted(eg.weakly_connected_components(G), key=len, reverse=True)
-    ... ]
-    [4, 3]
-
-    If you only want the largest component, it's more efficient to
-    use max instead of sort:
-
-    >>> largest_cc = max(eg.weakly_connected_components(G), key=len)
-
-    See Also
-    --------
-    connected_components
-    strongly_connected_components
-
-    Notes
-    -----
-    For directed graphs only.
-
-    """
-    seen = set()
-    for v in G:
-        if v not in seen:
-            c = set(_plain_bfs(G, v))
-            seen.update(c)
-            yield c
-
-
-@not_implemented_for("undirected")
-def number_weakly_connected_components(G):
-    """Returns the number of weakly connected components in G.
-
-    Parameters
-    ----------
-    G : EasyGraph graph
-        A directed graph.
-
-    Returns
-    -------
-    n : integer
-        Number of weakly connected components
-
-    Raises
-    ------
-    EasyGraphNotImplemented
-        If G is undirected.
-
-    Examples
-    --------
-    >>> G = eg.DiGraph([(0, 1), (2, 1), (3, 4)])
-    >>> eg.number_weakly_connected_components(G)
-    2
-
-    See Also
-    --------
-    weakly_connected_components
-    number_connected_components
-    number_strongly_connected_components
-
-    Notes
-    -----
-    For directed graphs only.
-
-    """
-    return sum(1 for wcc in weakly_connected_components(G))
-
-
-@not_implemented_for("undirected")
-def is_weakly_connected(G):
-    """Test directed graph for weak connectivity.
-
-    A directed graph is weakly connected if and only if the graph
-    is connected when the direction of the edge between nodes is ignored.
-
-    Note that if a graph is strongly connected (i.e. the graph is connected
-    even when we account for directionality), it is by definition weakly
-    connected as well.
-
-    Parameters
-    ----------
-    G : EasyGraph Graph
-        A directed graph.
-
-    Returns
-    -------
-    connected : bool
-        True if the graph is weakly connected, False otherwise.
-
-    Raises
-    ------
-    EasyGraphNotImplemented
-        If G is undirected.
-
-    Examples
-    --------
-    >>> G = eg.DiGraph([(0, 1), (2, 1)])
-    >>> G.add_node(3)
-    >>> eg.is_weakly_connected(G)  # node 3 is not connected to the graph
-    False
-    >>> G.add_edge(2, 3)
-    >>> eg.is_weakly_connected(G)
-    True
-
-    See Also
-    --------
-    is_strongly_connected
-    is_semiconnected
-    is_connected
-    is_biconnected
-    weakly_connected_components
-
-    Notes
-    -----
-    For directed graphs only.
-
-    """
-    if len(G) == 0:
-        raise eg.EasyGraphPointlessConcept(
-            """Connectivity is undefined for the null graph."""
-        )
-
-    return len(next(weakly_connected_components(G))) == len(G)
-
-
-def _plain_bfs(G, source):
-    """A fast BFS node generator
-
-    The direction of the edge between nodes is ignored.
-
-    For directed graphs only.
-
-    """
-    Gsucc = G.adj
-    Gpred = G.pred
-
-    seen = set()
-    nextlevel = {source}
-    while nextlevel:
-        thislevel = nextlevel
-        nextlevel = set()
-        for v in thislevel:
-            if v not in seen:
-                seen.add(v)
-                nextlevel.update(Gsucc[v])
-                nextlevel.update(Gpred[v])
-                yield v
+"""Weakly connected components."""
+import easygraph as eg
+
+from easygraph.utils.decorators import not_implemented_for
+
+
+__all__ = [
+    "number_weakly_connected_components",
+    "weakly_connected_components",
+    "is_weakly_connected",
+]
+
+
+@not_implemented_for("undirected")
+def weakly_connected_components(G):
+    """Generate weakly connected components of G.
+
+    Parameters
+    ----------
+    G : EasyGraph graph
+        A directed graph
+
+    Returns
+    -------
+    comp : generator of sets
+        A generator of sets of nodes, one for each weakly connected
+        component of G.
+
+    Raises
+    ------
+    EasyGraphNotImplemented
+        If G is undirected.
+
+    Examples
+    --------
+    Generate a sorted list of weakly connected components, largest first.
+
+    >>> G = eg.path_graph(4, create_using=eg.DiGraph())
+    >>> eg.add_path(G, [10, 11, 12])
+    >>> [
+    ...     len(c)
+    ...     for c in sorted(eg.weakly_connected_components(G), key=len, reverse=True)
+    ... ]
+    [4, 3]
+
+    If you only want the largest component, it's more efficient to
+    use max instead of sort:
+
+    >>> largest_cc = max(eg.weakly_connected_components(G), key=len)
+
+    See Also
+    --------
+    connected_components
+    strongly_connected_components
+
+    Notes
+    -----
+    For directed graphs only.
+
+    """
+    seen = set()
+    for v in G:
+        if v not in seen:
+            c = set(_plain_bfs(G, v))
+            seen.update(c)
+            yield c
+
+
+@not_implemented_for("undirected")
+def number_weakly_connected_components(G):
+    """Returns the number of weakly connected components in G.
+
+    Parameters
+    ----------
+    G : EasyGraph graph
+        A directed graph.
+
+    Returns
+    -------
+    n : integer
+        Number of weakly connected components
+
+    Raises
+    ------
+    EasyGraphNotImplemented
+        If G is undirected.
+
+    Examples
+    --------
+    >>> G = eg.DiGraph([(0, 1), (2, 1), (3, 4)])
+    >>> eg.number_weakly_connected_components(G)
+    2
+
+    See Also
+    --------
+    weakly_connected_components
+    number_connected_components
+    number_strongly_connected_components
+
+    Notes
+    -----
+    For directed graphs only.
+
+    """
+    return sum(1 for wcc in weakly_connected_components(G))
+
+
+@not_implemented_for("undirected")
+def is_weakly_connected(G):
+    """Test directed graph for weak connectivity.
+
+    A directed graph is weakly connected if and only if the graph
+    is connected when the direction of the edge between nodes is ignored.
+
+    Note that if a graph is strongly connected (i.e. the graph is connected
+    even when we account for directionality), it is by definition weakly
+    connected as well.
+
+    Parameters
+    ----------
+    G : EasyGraph Graph
+        A directed graph.
+
+    Returns
+    -------
+    connected : bool
+        True if the graph is weakly connected, False otherwise.
+
+    Raises
+    ------
+    EasyGraphNotImplemented
+        If G is undirected.
+
+    Examples
+    --------
+    >>> G = eg.DiGraph([(0, 1), (2, 1)])
+    >>> G.add_node(3)
+    >>> eg.is_weakly_connected(G)  # node 3 is not connected to the graph
+    False
+    >>> G.add_edge(2, 3)
+    >>> eg.is_weakly_connected(G)
+    True
+
+    See Also
+    --------
+    is_strongly_connected
+    is_semiconnected
+    is_connected
+    is_biconnected
+    weakly_connected_components
+
+    Notes
+    -----
+    For directed graphs only.
+
+    """
+    if len(G) == 0:
+        raise eg.EasyGraphPointlessConcept(
+            """Connectivity is undefined for the null graph."""
+        )
+
+    return len(next(weakly_connected_components(G))) == len(G)
+
+
+def _plain_bfs(G, source):
+    """A fast BFS node generator
+
+    The direction of the edge between nodes is ignored.
+
+    For directed graphs only.
+
+    """
+    Gsucc = G.adj
+    Gpred = G.pred
+
+    seen = set()
+    nextlevel = {source}
+    while nextlevel:
+        thislevel = nextlevel
+        nextlevel = set()
+        for v in thislevel:
+            if v not in seen:
+                seen.add(v)
+                nextlevel.update(Gsucc[v])
+                nextlevel.update(Gpred[v])
+                yield v
```

## easygraph/functions/components/strongly_connected.py

 * *Ordering differences only*

```diff
@@ -1,244 +1,244 @@
-import easygraph as eg
-
-from easygraph.utils.decorators import *
-
-
-__all__ = [
-    "number_strongly_connected_components",
-    "strongly_connected_components",
-    "is_strongly_connected",
-    "condensation",
-]
-
-
-@not_implemented_for("undirected")
-@hybrid("cpp_strongly_connected_components")
-def strongly_connected_components(G):
-    """Generate nodes in strongly connected components of graph.
-
-    Parameters
-    ----------
-    G : EasyGraph Graph
-        A directed graph.
-
-    Returns
-    -------
-    comp : generator of sets
-        A generator of sets of nodes, one for each strongly connected
-        component of G.
-
-    Raises
-    ------
-    EasyGraphNotImplemented
-        If G is undirected.
-
-    Examples
-    --------
-    Generate a sorted list of strongly connected components, largest first.
-
-    If you only want the largest component, it's more efficient to
-    use max instead of sort.
-
-    >>> largest = max(eg.strongly_connected_components(G), key=len)
-
-    See Also
-    --------
-    connected_components
-
-    Notes
-    -----
-    Uses Tarjan's algorithm[1]_ with Nuutila's modifications[2]_.
-    Nonrecursive version of algorithm.
-
-    References
-    ----------
-    .. [1] Depth-first search and linear graph algorithms, R. Tarjan
-       SIAM Journal of Computing 1(2):146-160, (1972).
-
-    .. [2] On finding the strongly connected components in a directed graph.
-       E. Nuutila and E. Soisalon-Soinen
-       Information Processing Letters 49(1): 9-14, (1994)..
-
-    """
-    preorder = {}
-    lowlink = {}
-    scc_found = set()
-    scc_queue = []
-    i = 0  # Preorder counter
-    neighbors = {v: iter(G[v]) for v in G}
-    for source in G:
-        if source not in scc_found:
-            queue = [source]
-            while queue:
-                v = queue[-1]
-                if v not in preorder:
-                    i = i + 1
-                    preorder[v] = i
-                done = True
-                for w in neighbors[v]:
-                    if w not in preorder:
-                        queue.append(w)
-                        done = False
-                        break
-                if done:
-                    lowlink[v] = preorder[v]
-                    for w in G[v]:
-                        if w not in scc_found:
-                            if preorder[w] > preorder[v]:
-                                lowlink[v] = min([lowlink[v], lowlink[w]])
-                            else:
-                                lowlink[v] = min([lowlink[v], preorder[w]])
-                    queue.pop()
-                    if lowlink[v] == preorder[v]:
-                        scc = {v}
-                        while scc_queue and preorder[scc_queue[-1]] > preorder[v]:
-                            k = scc_queue.pop()
-                            scc.add(k)
-                        scc_found.update(scc)
-                        yield scc
-                    else:
-                        scc_queue.append(v)
-
-
-def number_strongly_connected_components(G):
-    """Returns number of strongly connected components in graph.
-
-    Parameters
-    ----------
-    G : Easygraph graph
-       A directed graph.
-
-    Returns
-    -------
-    n : integer
-       Number of strongly connected components
-
-    Raises
-    ------
-    EasygraphNotImplemented
-        If G is undirected.
-
-    Examples
-    --------
-    >>> G = eg.DiGraph([(0, 1), (1, 2), (2, 0), (2, 3), (4, 5), (3, 4), (5, 6), (6, 3), (6, 7)])
-    >>> eg.number_strongly_connected_components(G)
-    3
-
-    See Also
-    --------
-    strongly_connected_components
-    number_connected_components
-
-    Notes
-    -----
-    For directed graphs only.
-    """
-    return sum(1 for scc in strongly_connected_components(G))
-
-
-@not_implemented_for("undirected")
-def is_strongly_connected(G):
-    """Test directed graph for strong connectivity.
-
-    A directed graph is strongly connected if and only if every vertex in
-    the graph is reachable from every other vertex.
-
-    Parameters
-    ----------
-    G : EasyGraph Graph
-       A directed graph.
-
-    Returns
-    -------
-    connected : bool
-      True if the graph is strongly connected, False otherwise.
-
-    Examples
-    --------
-    >>> G = eg.DiGraph([(0, 1), (1, 2), (2, 3), (3, 0), (2, 4), (4, 2)])
-    >>> eg.is_strongly_connected(G)
-    True
-    >>> G.remove_edge(2, 3)
-    >>> eg.is_strongly_connected(G)
-    False
-
-    Raises
-    ------
-    EasyGraphNotImplemented
-        If G is undirected.
-
-    See Also
-    --------
-    is_connected
-    is_biconnected
-    strongly_connected_components
-
-    Notes
-    -----
-    For directed graphs only.
-    """
-    if len(G) == 0:
-        raise eg.EasyGraphPointlessConcept(
-            """Connectivity is undefined for the null graph."""
-        )
-
-    return len(next(strongly_connected_components(G))) == len(G)
-
-
-@not_implemented_for("multigraph")
-@only_implemented_for_Directed_graph
-def condensation(G, scc=None):
-    """Returns the condensation of G.
-    The condensation of G is the graph with each of the strongly connected
-    components contracted into a single node.
-    Parameters
-    ----------
-    G : easygraph.DiGraph
-       A directed graph.
-    scc:  list or generator (optional, default=None)
-       Strongly connected components. If provided, the elements in
-       `scc` must partition the nodes in `G`. If not provided, it will be
-       calculated as scc=strongly_connected_components(G).
-    Returns
-    -------
-    C : easygraph.DiGraph
-       The condensation graph C of G.  The node labels are integers
-       corresponding to the index of the component in the list of
-       strongly connected components of G.  C has a graph attribute named
-       'mapping' with a dictionary mapping the original nodes to the
-       nodes in C to which they belong.  Each node in C also has a node
-       attribute 'members' with the set of original nodes in G that
-       form the SCC that the node in C represents.
-    Examples
-    --------
-    # >>> condensation(G)
-    Notes
-    -----
-    After contracting all strongly connected components to a single node,
-    the resulting graph is a directed acyclic graph.
-    """
-    if scc is None:
-        scc = strongly_connected_components(G)
-    mapping = {}
-    incoming_info = {}
-    members = {}
-    C = eg.DiGraph()
-    # Add mapping dict as graph attribute
-    C.graph["mapping"] = mapping
-    if len(G) == 0:
-        return C
-    for i, component in enumerate(scc):
-        members[i] = component
-        mapping.update((n, i) for n in component)
-    number_of_components = i + 1
-    for i in range(number_of_components):
-        C.add_node(i, member=members[i], incoming=set())
-    C.add_nodes(range(number_of_components))
-    for edge in G.edges:
-        if mapping[edge[0]] != mapping[edge[1]]:
-            C.add_edge(mapping[edge[0]], mapping[edge[1]])
-            if edge[1] not in incoming_info.keys():
-                incoming_info[edge[1]] = set()
-            incoming_info[edge[1]].add(edge[0])
-    C.graph["incoming_info"] = incoming_info
-    return C
+import easygraph as eg
+
+from easygraph.utils.decorators import *
+
+
+__all__ = [
+    "number_strongly_connected_components",
+    "strongly_connected_components",
+    "is_strongly_connected",
+    "condensation",
+]
+
+
+@not_implemented_for("undirected")
+@hybrid("cpp_strongly_connected_components")
+def strongly_connected_components(G):
+    """Generate nodes in strongly connected components of graph.
+
+    Parameters
+    ----------
+    G : EasyGraph Graph
+        A directed graph.
+
+    Returns
+    -------
+    comp : generator of sets
+        A generator of sets of nodes, one for each strongly connected
+        component of G.
+
+    Raises
+    ------
+    EasyGraphNotImplemented
+        If G is undirected.
+
+    Examples
+    --------
+    Generate a sorted list of strongly connected components, largest first.
+
+    If you only want the largest component, it's more efficient to
+    use max instead of sort.
+
+    >>> largest = max(eg.strongly_connected_components(G), key=len)
+
+    See Also
+    --------
+    connected_components
+
+    Notes
+    -----
+    Uses Tarjan's algorithm[1]_ with Nuutila's modifications[2]_.
+    Nonrecursive version of algorithm.
+
+    References
+    ----------
+    .. [1] Depth-first search and linear graph algorithms, R. Tarjan
+       SIAM Journal of Computing 1(2):146-160, (1972).
+
+    .. [2] On finding the strongly connected components in a directed graph.
+       E. Nuutila and E. Soisalon-Soinen
+       Information Processing Letters 49(1): 9-14, (1994)..
+
+    """
+    preorder = {}
+    lowlink = {}
+    scc_found = set()
+    scc_queue = []
+    i = 0  # Preorder counter
+    neighbors = {v: iter(G[v]) for v in G}
+    for source in G:
+        if source not in scc_found:
+            queue = [source]
+            while queue:
+                v = queue[-1]
+                if v not in preorder:
+                    i = i + 1
+                    preorder[v] = i
+                done = True
+                for w in neighbors[v]:
+                    if w not in preorder:
+                        queue.append(w)
+                        done = False
+                        break
+                if done:
+                    lowlink[v] = preorder[v]
+                    for w in G[v]:
+                        if w not in scc_found:
+                            if preorder[w] > preorder[v]:
+                                lowlink[v] = min([lowlink[v], lowlink[w]])
+                            else:
+                                lowlink[v] = min([lowlink[v], preorder[w]])
+                    queue.pop()
+                    if lowlink[v] == preorder[v]:
+                        scc = {v}
+                        while scc_queue and preorder[scc_queue[-1]] > preorder[v]:
+                            k = scc_queue.pop()
+                            scc.add(k)
+                        scc_found.update(scc)
+                        yield scc
+                    else:
+                        scc_queue.append(v)
+
+
+def number_strongly_connected_components(G):
+    """Returns number of strongly connected components in graph.
+
+    Parameters
+    ----------
+    G : Easygraph graph
+       A directed graph.
+
+    Returns
+    -------
+    n : integer
+       Number of strongly connected components
+
+    Raises
+    ------
+    EasygraphNotImplemented
+        If G is undirected.
+
+    Examples
+    --------
+    >>> G = eg.DiGraph([(0, 1), (1, 2), (2, 0), (2, 3), (4, 5), (3, 4), (5, 6), (6, 3), (6, 7)])
+    >>> eg.number_strongly_connected_components(G)
+    3
+
+    See Also
+    --------
+    strongly_connected_components
+    number_connected_components
+
+    Notes
+    -----
+    For directed graphs only.
+    """
+    return sum(1 for scc in strongly_connected_components(G))
+
+
+@not_implemented_for("undirected")
+def is_strongly_connected(G):
+    """Test directed graph for strong connectivity.
+
+    A directed graph is strongly connected if and only if every vertex in
+    the graph is reachable from every other vertex.
+
+    Parameters
+    ----------
+    G : EasyGraph Graph
+       A directed graph.
+
+    Returns
+    -------
+    connected : bool
+      True if the graph is strongly connected, False otherwise.
+
+    Examples
+    --------
+    >>> G = eg.DiGraph([(0, 1), (1, 2), (2, 3), (3, 0), (2, 4), (4, 2)])
+    >>> eg.is_strongly_connected(G)
+    True
+    >>> G.remove_edge(2, 3)
+    >>> eg.is_strongly_connected(G)
+    False
+
+    Raises
+    ------
+    EasyGraphNotImplemented
+        If G is undirected.
+
+    See Also
+    --------
+    is_connected
+    is_biconnected
+    strongly_connected_components
+
+    Notes
+    -----
+    For directed graphs only.
+    """
+    if len(G) == 0:
+        raise eg.EasyGraphPointlessConcept(
+            """Connectivity is undefined for the null graph."""
+        )
+
+    return len(next(strongly_connected_components(G))) == len(G)
+
+
+@not_implemented_for("multigraph")
+@only_implemented_for_Directed_graph
+def condensation(G, scc=None):
+    """Returns the condensation of G.
+    The condensation of G is the graph with each of the strongly connected
+    components contracted into a single node.
+    Parameters
+    ----------
+    G : easygraph.DiGraph
+       A directed graph.
+    scc:  list or generator (optional, default=None)
+       Strongly connected components. If provided, the elements in
+       `scc` must partition the nodes in `G`. If not provided, it will be
+       calculated as scc=strongly_connected_components(G).
+    Returns
+    -------
+    C : easygraph.DiGraph
+       The condensation graph C of G.  The node labels are integers
+       corresponding to the index of the component in the list of
+       strongly connected components of G.  C has a graph attribute named
+       'mapping' with a dictionary mapping the original nodes to the
+       nodes in C to which they belong.  Each node in C also has a node
+       attribute 'members' with the set of original nodes in G that
+       form the SCC that the node in C represents.
+    Examples
+    --------
+    # >>> condensation(G)
+    Notes
+    -----
+    After contracting all strongly connected components to a single node,
+    the resulting graph is a directed acyclic graph.
+    """
+    if scc is None:
+        scc = strongly_connected_components(G)
+    mapping = {}
+    incoming_info = {}
+    members = {}
+    C = eg.DiGraph()
+    # Add mapping dict as graph attribute
+    C.graph["mapping"] = mapping
+    if len(G) == 0:
+        return C
+    for i, component in enumerate(scc):
+        members[i] = component
+        mapping.update((n, i) for n in component)
+    number_of_components = i + 1
+    for i in range(number_of_components):
+        C.add_node(i, member=members[i], incoming=set())
+    C.add_nodes(range(number_of_components))
+    for edge in G.edges:
+        if mapping[edge[0]] != mapping[edge[1]]:
+            C.add_edge(mapping[edge[0]], mapping[edge[1]])
+            if edge[1] not in incoming_info.keys():
+                incoming_info[edge[1]] = set()
+            incoming_info[edge[1]].add(edge[0])
+    C.graph["incoming_info"] = incoming_info
+    return C
```

## easygraph/functions/components/biconnected.py

 * *Ordering differences only*

```diff
@@ -1,247 +1,247 @@
-from itertools import chain
-
-from easygraph.utils import *
-
-
-__all__ = [
-    "is_biconnected",
-    "biconnected_components",
-    "generator_biconnected_components_nodes",
-    "generator_biconnected_components_edges",
-    "generator_articulation_points",
-]
-
-
-@not_implemented_for("multigraph", "directed")
-def is_biconnected(G):
-    """Returns whether the graph is biconnected or not.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-
-    Returns
-    -------
-    is_biconnected : boolean
-        `True` if the graph is biconnected.
-
-    Examples
-    --------
-
-    >>> is_biconnected(G)
-
-    """
-    bc_nodes = list(generator_biconnected_components_nodes(G))
-    if len(bc_nodes) == 1:
-        return len(bc_nodes[0]) == len(
-            G
-        )  # avoid situations where there is isolated vertex
-    return False
-
-
-@not_implemented_for("multigraph", "directed")
-# TODO: get the subgraph of each biconnected graph
-def biconnected_components(G):
-    """Returns a list of biconnected components, each of which denotes the edges set of a biconnected component.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-
-    Returns
-    -------
-    biconnected_components : list of list
-        Each element list is the edges set of a biconnected component.
-
-    Examples
-    --------
-    >>> connected_components(G)
-
-    """
-    return list(generator_biconnected_components_edges(G))
-
-
-@not_implemented_for("multigraph", "directed")
-def generator_biconnected_components_nodes(G):
-    """Returns a generator of nodes in each biconnected component.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-
-    Returns
-    -------
-    Yields nodes set of each biconnected component.
-
-    See Also
-    --------
-    generator_biconnected_components_edges
-
-    Examples
-    --------
-    >>> generator_biconnected_components_nodes(G)
-
-
-    """
-    for component in _biconnected_dfs_record_edges(G, need_components=True):
-        # TODO: only one edge = biconnected_component?
-        yield set(chain.from_iterable(component))
-
-
-@not_implemented_for("multigraph", "directed")
-def generator_biconnected_components_edges(G):
-    """Returns a generator of nodes in each biconnected component.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-
-    Returns
-    -------
-    Yields edges set of each biconnected component.
-
-    See Also
-    --------
-    generator_biconnected_components_nodes
-
-    Examples
-    --------
-    >>> generator_biconnected_components_edges(G)
-
-    """
-    yield from _biconnected_dfs_record_edges(G, need_components=True)
-
-
-@not_implemented_for("multigraph", "directed")
-def generator_articulation_points(G):
-    """Returns a generator of articulation points.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-
-    Returns
-    -------
-    Yields the articulation point in *G*.
-
-    Examples
-    --------
-    >>> generator_articulation_points(G)
-
-    """
-    seen = set()
-    for cut_vertex in _biconnected_dfs_record_edges(G, need_components=False):
-        if cut_vertex not in seen:
-            seen.add(cut_vertex)
-            yield cut_vertex
-
-
-@hybrid("cpp_biconnected_dfs_record_edges")
-def _biconnected_dfs_record_edges(G, need_components=True):
-    """
-    References
-    ----------
-    https://www.cnblogs.com/nullzx/p/7968110.html
-    https://blog.csdn.net/gauss_acm/article/details/43493903
-    """
-    # record edges of each biconnected component in traversal
-    # Copied version from EasyGraph
-    # depth-first search algorithm to generate articulation points
-    # and biconnected components
-    visited = set()
-    for start in G:
-        if start in visited:
-            continue
-        discovery = {start: 0}  # time of first discovery of node during search
-        low = {start: 0}
-        root_children = 0
-        visited.add(start)
-        edge_stack = []
-        stack = [(start, start, iter(G[start]))]
-        while stack:
-            grandparent, parent, children = stack[-1]
-            try:
-                child = next(children)
-                if grandparent == child:
-                    continue
-                if child in visited:
-                    if discovery[child] <= discovery[parent]:  # back edge
-                        low[parent] = min(low[parent], discovery[child])
-                        if need_components:
-                            edge_stack.append((parent, child))
-                else:
-                    low[child] = discovery[child] = len(discovery)
-                    visited.add(child)
-                    stack.append((parent, child, iter(G[child])))
-                    if need_components:
-                        edge_stack.append((parent, child))
-            except StopIteration:
-                stack.pop()
-                if len(stack) > 1:
-                    if low[parent] >= discovery[grandparent]:
-                        if need_components:
-                            ind = edge_stack.index((grandparent, parent))
-                            yield edge_stack[ind:]
-                            edge_stack = edge_stack[:ind]
-                        else:
-                            yield grandparent
-                    low[grandparent] = min(low[parent], low[grandparent])
-                elif stack:  # length 1 so grandparent is root
-                    root_children += 1
-                    if need_components:
-                        ind = edge_stack.index((grandparent, parent))
-                        yield edge_stack[ind:]
-        if not need_components:
-            # root node is articulation point if it has more than 1 child
-            if root_children > 1:
-                yield start
-
-
-def _biconnected_dfs_record_nodes(G, need_components=True):
-    # record nodes of each biconnected component in traversal
-    # Not used.
-    visited = set()
-    for start in G:
-        if start in visited:
-            continue
-        discovery = {start: 0}  # time of first discovery of node during search
-        low = {start: 0}
-        root_children = 0
-        visited.add(start)
-        node_stack = [start]
-        stack = [(start, start, iter(G[start]))]
-        while stack:
-            grandparent, parent, children = stack[-1]
-            try:
-                child = next(children)
-                if grandparent == child:
-                    continue
-                if child in visited:
-                    if discovery[child] <= discovery[parent]:  # back edge
-                        low[parent] = min(low[parent], discovery[child])
-                else:
-                    low[child] = discovery[child] = len(discovery)
-                    visited.add(child)
-                    stack.append((parent, child, iter(G[child])))
-                    if need_components:
-                        node_stack.append(child)
-            except StopIteration:
-                stack.pop()
-                if len(stack) > 1:
-                    if low[parent] >= discovery[grandparent]:
-                        if need_components:
-                            ind = node_stack.index(grandparent)
-                            yield node_stack[ind:]
-                            node_stack = node_stack[: ind + 1]
-                        else:
-                            yield grandparent
-                    low[grandparent] = min(low[parent], low[grandparent])
-                elif stack:  # length 1 so grandparent is root
-                    root_children += 1
-                    if need_components:
-                        ind = node_stack.index(grandparent)
-                        yield node_stack[ind:]
-        if not need_components:
-            # root node is articulation point if it has more than 1 child
-            if root_children > 1:
-                yield start
+from itertools import chain
+
+from easygraph.utils import *
+
+
+__all__ = [
+    "is_biconnected",
+    "biconnected_components",
+    "generator_biconnected_components_nodes",
+    "generator_biconnected_components_edges",
+    "generator_articulation_points",
+]
+
+
+@not_implemented_for("multigraph", "directed")
+def is_biconnected(G):
+    """Returns whether the graph is biconnected or not.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+
+    Returns
+    -------
+    is_biconnected : boolean
+        `True` if the graph is biconnected.
+
+    Examples
+    --------
+
+    >>> is_biconnected(G)
+
+    """
+    bc_nodes = list(generator_biconnected_components_nodes(G))
+    if len(bc_nodes) == 1:
+        return len(bc_nodes[0]) == len(
+            G
+        )  # avoid situations where there is isolated vertex
+    return False
+
+
+@not_implemented_for("multigraph", "directed")
+# TODO: get the subgraph of each biconnected graph
+def biconnected_components(G):
+    """Returns a list of biconnected components, each of which denotes the edges set of a biconnected component.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+
+    Returns
+    -------
+    biconnected_components : list of list
+        Each element list is the edges set of a biconnected component.
+
+    Examples
+    --------
+    >>> connected_components(G)
+
+    """
+    return list(generator_biconnected_components_edges(G))
+
+
+@not_implemented_for("multigraph", "directed")
+def generator_biconnected_components_nodes(G):
+    """Returns a generator of nodes in each biconnected component.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+
+    Returns
+    -------
+    Yields nodes set of each biconnected component.
+
+    See Also
+    --------
+    generator_biconnected_components_edges
+
+    Examples
+    --------
+    >>> generator_biconnected_components_nodes(G)
+
+
+    """
+    for component in _biconnected_dfs_record_edges(G, need_components=True):
+        # TODO: only one edge = biconnected_component?
+        yield set(chain.from_iterable(component))
+
+
+@not_implemented_for("multigraph", "directed")
+def generator_biconnected_components_edges(G):
+    """Returns a generator of nodes in each biconnected component.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+
+    Returns
+    -------
+    Yields edges set of each biconnected component.
+
+    See Also
+    --------
+    generator_biconnected_components_nodes
+
+    Examples
+    --------
+    >>> generator_biconnected_components_edges(G)
+
+    """
+    yield from _biconnected_dfs_record_edges(G, need_components=True)
+
+
+@not_implemented_for("multigraph", "directed")
+def generator_articulation_points(G):
+    """Returns a generator of articulation points.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+
+    Returns
+    -------
+    Yields the articulation point in *G*.
+
+    Examples
+    --------
+    >>> generator_articulation_points(G)
+
+    """
+    seen = set()
+    for cut_vertex in _biconnected_dfs_record_edges(G, need_components=False):
+        if cut_vertex not in seen:
+            seen.add(cut_vertex)
+            yield cut_vertex
+
+
+@hybrid("cpp_biconnected_dfs_record_edges")
+def _biconnected_dfs_record_edges(G, need_components=True):
+    """
+    References
+    ----------
+    https://www.cnblogs.com/nullzx/p/7968110.html
+    https://blog.csdn.net/gauss_acm/article/details/43493903
+    """
+    # record edges of each biconnected component in traversal
+    # Copied version from EasyGraph
+    # depth-first search algorithm to generate articulation points
+    # and biconnected components
+    visited = set()
+    for start in G:
+        if start in visited:
+            continue
+        discovery = {start: 0}  # time of first discovery of node during search
+        low = {start: 0}
+        root_children = 0
+        visited.add(start)
+        edge_stack = []
+        stack = [(start, start, iter(G[start]))]
+        while stack:
+            grandparent, parent, children = stack[-1]
+            try:
+                child = next(children)
+                if grandparent == child:
+                    continue
+                if child in visited:
+                    if discovery[child] <= discovery[parent]:  # back edge
+                        low[parent] = min(low[parent], discovery[child])
+                        if need_components:
+                            edge_stack.append((parent, child))
+                else:
+                    low[child] = discovery[child] = len(discovery)
+                    visited.add(child)
+                    stack.append((parent, child, iter(G[child])))
+                    if need_components:
+                        edge_stack.append((parent, child))
+            except StopIteration:
+                stack.pop()
+                if len(stack) > 1:
+                    if low[parent] >= discovery[grandparent]:
+                        if need_components:
+                            ind = edge_stack.index((grandparent, parent))
+                            yield edge_stack[ind:]
+                            edge_stack = edge_stack[:ind]
+                        else:
+                            yield grandparent
+                    low[grandparent] = min(low[parent], low[grandparent])
+                elif stack:  # length 1 so grandparent is root
+                    root_children += 1
+                    if need_components:
+                        ind = edge_stack.index((grandparent, parent))
+                        yield edge_stack[ind:]
+        if not need_components:
+            # root node is articulation point if it has more than 1 child
+            if root_children > 1:
+                yield start
+
+
+def _biconnected_dfs_record_nodes(G, need_components=True):
+    # record nodes of each biconnected component in traversal
+    # Not used.
+    visited = set()
+    for start in G:
+        if start in visited:
+            continue
+        discovery = {start: 0}  # time of first discovery of node during search
+        low = {start: 0}
+        root_children = 0
+        visited.add(start)
+        node_stack = [start]
+        stack = [(start, start, iter(G[start]))]
+        while stack:
+            grandparent, parent, children = stack[-1]
+            try:
+                child = next(children)
+                if grandparent == child:
+                    continue
+                if child in visited:
+                    if discovery[child] <= discovery[parent]:  # back edge
+                        low[parent] = min(low[parent], discovery[child])
+                else:
+                    low[child] = discovery[child] = len(discovery)
+                    visited.add(child)
+                    stack.append((parent, child, iter(G[child])))
+                    if need_components:
+                        node_stack.append(child)
+            except StopIteration:
+                stack.pop()
+                if len(stack) > 1:
+                    if low[parent] >= discovery[grandparent]:
+                        if need_components:
+                            ind = node_stack.index(grandparent)
+                            yield node_stack[ind:]
+                            node_stack = node_stack[: ind + 1]
+                        else:
+                            yield grandparent
+                    low[grandparent] = min(low[parent], low[grandparent])
+                elif stack:  # length 1 so grandparent is root
+                    root_children += 1
+                    if need_components:
+                        ind = node_stack.index(grandparent)
+                        yield node_stack[ind:]
+        if not need_components:
+            # root node is articulation point if it has more than 1 child
+            if root_children > 1:
+                yield start
```

## easygraph/functions/components/__init__.py

 * *Ordering differences only*

```diff
@@ -1,4 +1,4 @@
-from .biconnected import *
-from .connected import *
-from .strongly_connected import *
-from .weakly_connected import *
+from .biconnected import *
+from .connected import *
+from .strongly_connected import *
+from .weakly_connected import *
```

## easygraph/functions/drawing/defaults.py

 * *Ordering differences only*

```diff
@@ -1,251 +1,251 @@
-from typing import Any
-from typing import List
-from typing import Optional
-from typing import Union
-
-
-def default_style(
-    num_v: int,
-    num_e: int,
-    v_color: Union[str, list] = "r",
-    e_color: Union[str, list] = "gray",
-    e_fill_color: Union[str, list] = "whitesmoke",
-):
-    _v_color = "r"
-    _e_color = "gray"
-    _e_fill_color = "whitesmoke"
-
-    v_color = fill_color(v_color, _v_color, num_v)
-    e_color = fill_color(e_color, _e_color, num_e)
-    e_fill_color = fill_color(e_fill_color, _e_fill_color, num_e)
-
-    return v_color, e_color, e_fill_color
-
-
-def default_bipartite_style(
-    num_u: int,
-    num_v: int,
-    num_e: int,
-    u_color: Union[str, list] = "m",
-    v_color: Union[str, list] = "r",
-    e_color: Union[str, list] = "gray",
-    e_fill_color: Union[str, list] = "whitesmoke",
-):
-    _u_color = "m"
-    _v_color = "r"
-    _e_color = "gray"
-    _e_fill_color = "whitesmoke"
-
-    u_color = fill_color(u_color, _u_color, num_u)
-    v_color = fill_color(v_color, _v_color, num_v)
-    e_color = fill_color(e_color, _e_color, num_e)
-    e_fill_color = fill_color(e_fill_color, _e_fill_color, num_e)
-
-    return u_color, v_color, e_color, e_fill_color
-
-
-def default_hypergraph_style(
-    num_v: int,
-    num_e: int,
-    v_color: Union[str, list] = "r",
-    e_color: Union[str, list] = "gray",
-    e_fill_color: Union[str, list] = "whitesmoke",
-):
-    _v_color = "r"
-    _e_color = "gray"
-    _e_fill_color = "whitesmoke"
-
-    v_color = fill_color(v_color, _v_color, num_v)
-    e_color = fill_color(e_color, _e_color, num_e)
-    e_fill_color = fill_color(e_fill_color, _e_fill_color, num_e)
-
-    return v_color, e_color, e_fill_color
-
-
-def default_size(
-    num_v: int,
-    e_list: List[tuple],
-    v_size: Union[float, list] = 1.0,
-    v_line_width: Union[float, list] = 1.0,
-    e_line_width: Union[float, list] = 1.0,
-    font_size: float = None,
-):
-    import numpy as np
-
-    _v_size = 1 / np.sqrt(num_v + 10) * 0.1
-    _v_line_width = 1 * np.exp(-num_v / 50)
-    _e_line_width = 1 * np.exp(-len(e_list) / 120)
-    _font_size = 20 * np.exp(-num_v / 100)
-    v_size = fill_sizes(v_size, _v_size, num_v)
-    v_line_width = fill_sizes(v_line_width, _v_line_width, num_v)
-    e_line_width = fill_sizes(e_line_width, _e_line_width, len(e_list))
-    font_size = _font_size if font_size is None else font_size
-
-    return v_size, v_line_width, e_line_width, font_size
-
-
-def default_bipartite_size(
-    num_u: int,
-    num_v: int,
-    e_list: List[tuple],
-    u_size: Union[float, list] = 1.0,
-    u_line_width: Union[float, list] = 1.0,
-    v_size: Union[float, list] = 1.0,
-    v_line_width: Union[float, list] = 1.0,
-    e_line_width: Union[float, list] = 1.0,
-    u_font_size: float = 1.0,
-    v_font_size: float = 1.0,
-):
-    import numpy as np
-
-    _u_size = 1 / np.sqrt(num_u + 12) * 0.08
-    _u_line_width = 1 * np.exp(-num_u / 50)
-    _v_size = 1 / np.sqrt(num_v + 12) * 0.08
-    _v_line_width = 1 * np.exp(-num_v / 50)
-    _e_line_width = 1 * np.exp(-len(e_list) / 50)
-    _u_font_size = 12 * np.exp(-((num_u / num_v) ** 0.3) * (num_u + num_v) / 100)
-    _v_font_size = 12 * np.exp(-((num_v / num_u) ** 0.3) * (num_u + num_v) / 100)
-
-    u_size = fill_sizes(u_size, _u_size, num_u)
-    u_line_width = fill_sizes(u_line_width, _u_line_width, num_u)
-    v_size = fill_sizes(v_size, _v_size, num_v)
-    v_line_width = fill_sizes(v_line_width, _v_line_width, num_v)
-    e_line_width = fill_sizes(e_line_width, _e_line_width, len(e_list))
-
-    u_font_size = _u_font_size if u_font_size is None else u_font_size * _u_font_size
-    v_font_size = _v_font_size if v_font_size is None else v_font_size * _v_font_size
-
-    return (
-        u_size,
-        u_line_width,
-        v_size,
-        v_line_width,
-        e_line_width,
-        u_font_size,
-        v_font_size,
-    )
-
-
-def default_strength(
-    num_v: int,
-    e_list: List[tuple],
-    push_v_strength: float = 1.0,
-    push_e_strength: float = 1.0,
-    pull_e_strength: float = 1.0,
-    pull_center_strength: float = 1.0,
-):
-    _push_v_strength = 0.006
-    _push_e_strength = 0.0
-    _pull_e_strength = 0.045
-    _pull_center_strength = 0.01
-
-    push_v_strength = fill_strength(push_v_strength, _push_v_strength)
-    push_e_strength = fill_strength(push_e_strength, _push_e_strength)
-    pull_e_strength = fill_strength(pull_e_strength, _pull_e_strength)
-    pull_center_strength = fill_strength(pull_center_strength, _pull_center_strength)
-
-    return push_v_strength, push_e_strength, pull_e_strength, pull_center_strength
-
-
-def default_bipartite_strength(
-    num_u: int,
-    num_v: int,
-    e_list: List[tuple],
-    push_u_strength: float = 1.0,
-    push_v_strength: float = 1.0,
-    push_e_strength: float = 1.0,
-    pull_e_strength: float = 1.0,
-    pull_u_center_strength: float = 1.0,
-    pull_v_center_strength: float = 1.0,
-):
-    _push_u_strength = 0.005
-    _push_v_strength = 0.005
-    _push_e_strength = 0.0
-    _pull_e_strength = 0.03
-    _pull_u_center_strength = 0.04
-    _pull_v_center_strength = 0.04
-
-    push_u_strength = fill_strength(push_u_strength, _push_u_strength)
-    push_v_strength = fill_strength(push_v_strength, _push_v_strength)
-    push_e_strength = fill_strength(push_e_strength, _push_e_strength)
-    pull_e_strength = fill_strength(pull_e_strength, _pull_e_strength)
-    pull_u_center_strength = fill_strength(
-        pull_u_center_strength, _pull_u_center_strength
-    )
-    pull_v_center_strength = fill_strength(
-        pull_v_center_strength, _pull_v_center_strength
-    )
-
-    return (
-        push_u_strength,
-        push_v_strength,
-        push_e_strength,
-        pull_e_strength,
-        pull_u_center_strength,
-        pull_v_center_strength,
-    )
-
-
-def default_hypergraph_strength(
-    num_v: int,
-    e_list: List[tuple],
-    push_v_strength: float = 1.0,
-    push_e_strength: float = 1.0,
-    pull_e_strength: float = 1.0,
-    pull_center_strength: float = 1.0,
-):
-    _push_v_strength = 0.006
-    _push_e_strength = 0.008
-    _pull_e_strength = 0.007
-    _pull_center_strength = 0.001
-
-    push_v_strength = fill_strength(push_v_strength, _push_v_strength)
-    push_e_strength = fill_strength(push_e_strength, _push_e_strength)
-    pull_e_strength = fill_strength(pull_e_strength, _pull_e_strength)
-    pull_center_strength = fill_strength(pull_center_strength, _pull_center_strength)
-
-    return push_v_strength, push_e_strength, pull_e_strength, pull_center_strength
-
-
-def fill_color(
-    custom_color: Optional[Union[str, list]], default_color: Any, length: int
-):
-    if custom_color is None:
-        return [default_color] * length
-    elif isinstance(custom_color, list):
-        if (
-            isinstance(custom_color[0], str)
-            or isinstance(custom_color[0], tuple)
-            or isinstance(custom_color[0], list)
-        ):
-            return custom_color
-        else:
-            return [custom_color] * length
-    elif isinstance(custom_color, str):
-        return [custom_color] * length
-    else:
-        raise ValueError("The specified value is not a valid type.")
-
-
-def fill_sizes(
-    custom_scales: Optional[Union[float, list]], default_value: Any, length: int
-):
-    if custom_scales is None:
-        return [default_value] * length
-    elif isinstance(custom_scales, list):
-        assert (
-            len(custom_scales) == length
-        ), "The specified value list has the wrong length."
-        return [default_value * scale for scale in custom_scales]
-    elif isinstance(custom_scales, float):
-        return [default_value * custom_scales] * length
-    elif isinstance(custom_scales, int):
-        return [default_value * float(custom_scales)] * length
-    else:
-        raise ValueError("The specified value is not a valid type.")
-
-
-def fill_strength(custom_scale: Optional[float], default_value: float):
-    if custom_scale is None:
-        return default_value
-    return custom_scale * default_value
+from typing import Any
+from typing import List
+from typing import Optional
+from typing import Union
+
+
+def default_style(
+    num_v: int,
+    num_e: int,
+    v_color: Union[str, list] = "r",
+    e_color: Union[str, list] = "gray",
+    e_fill_color: Union[str, list] = "whitesmoke",
+):
+    _v_color = "r"
+    _e_color = "gray"
+    _e_fill_color = "whitesmoke"
+
+    v_color = fill_color(v_color, _v_color, num_v)
+    e_color = fill_color(e_color, _e_color, num_e)
+    e_fill_color = fill_color(e_fill_color, _e_fill_color, num_e)
+
+    return v_color, e_color, e_fill_color
+
+
+def default_bipartite_style(
+    num_u: int,
+    num_v: int,
+    num_e: int,
+    u_color: Union[str, list] = "m",
+    v_color: Union[str, list] = "r",
+    e_color: Union[str, list] = "gray",
+    e_fill_color: Union[str, list] = "whitesmoke",
+):
+    _u_color = "m"
+    _v_color = "r"
+    _e_color = "gray"
+    _e_fill_color = "whitesmoke"
+
+    u_color = fill_color(u_color, _u_color, num_u)
+    v_color = fill_color(v_color, _v_color, num_v)
+    e_color = fill_color(e_color, _e_color, num_e)
+    e_fill_color = fill_color(e_fill_color, _e_fill_color, num_e)
+
+    return u_color, v_color, e_color, e_fill_color
+
+
+def default_hypergraph_style(
+    num_v: int,
+    num_e: int,
+    v_color: Union[str, list] = "r",
+    e_color: Union[str, list] = "gray",
+    e_fill_color: Union[str, list] = "whitesmoke",
+):
+    _v_color = "r"
+    _e_color = "gray"
+    _e_fill_color = "whitesmoke"
+
+    v_color = fill_color(v_color, _v_color, num_v)
+    e_color = fill_color(e_color, _e_color, num_e)
+    e_fill_color = fill_color(e_fill_color, _e_fill_color, num_e)
+
+    return v_color, e_color, e_fill_color
+
+
+def default_size(
+    num_v: int,
+    e_list: List[tuple],
+    v_size: Union[float, list] = 1.0,
+    v_line_width: Union[float, list] = 1.0,
+    e_line_width: Union[float, list] = 1.0,
+    font_size: float = None,
+):
+    import numpy as np
+
+    _v_size = 1 / np.sqrt(num_v + 10) * 0.1
+    _v_line_width = 1 * np.exp(-num_v / 50)
+    _e_line_width = 1 * np.exp(-len(e_list) / 120)
+    _font_size = 20 * np.exp(-num_v / 100)
+    v_size = fill_sizes(v_size, _v_size, num_v)
+    v_line_width = fill_sizes(v_line_width, _v_line_width, num_v)
+    e_line_width = fill_sizes(e_line_width, _e_line_width, len(e_list))
+    font_size = _font_size if font_size is None else font_size
+
+    return v_size, v_line_width, e_line_width, font_size
+
+
+def default_bipartite_size(
+    num_u: int,
+    num_v: int,
+    e_list: List[tuple],
+    u_size: Union[float, list] = 1.0,
+    u_line_width: Union[float, list] = 1.0,
+    v_size: Union[float, list] = 1.0,
+    v_line_width: Union[float, list] = 1.0,
+    e_line_width: Union[float, list] = 1.0,
+    u_font_size: float = 1.0,
+    v_font_size: float = 1.0,
+):
+    import numpy as np
+
+    _u_size = 1 / np.sqrt(num_u + 12) * 0.08
+    _u_line_width = 1 * np.exp(-num_u / 50)
+    _v_size = 1 / np.sqrt(num_v + 12) * 0.08
+    _v_line_width = 1 * np.exp(-num_v / 50)
+    _e_line_width = 1 * np.exp(-len(e_list) / 50)
+    _u_font_size = 12 * np.exp(-((num_u / num_v) ** 0.3) * (num_u + num_v) / 100)
+    _v_font_size = 12 * np.exp(-((num_v / num_u) ** 0.3) * (num_u + num_v) / 100)
+
+    u_size = fill_sizes(u_size, _u_size, num_u)
+    u_line_width = fill_sizes(u_line_width, _u_line_width, num_u)
+    v_size = fill_sizes(v_size, _v_size, num_v)
+    v_line_width = fill_sizes(v_line_width, _v_line_width, num_v)
+    e_line_width = fill_sizes(e_line_width, _e_line_width, len(e_list))
+
+    u_font_size = _u_font_size if u_font_size is None else u_font_size * _u_font_size
+    v_font_size = _v_font_size if v_font_size is None else v_font_size * _v_font_size
+
+    return (
+        u_size,
+        u_line_width,
+        v_size,
+        v_line_width,
+        e_line_width,
+        u_font_size,
+        v_font_size,
+    )
+
+
+def default_strength(
+    num_v: int,
+    e_list: List[tuple],
+    push_v_strength: float = 1.0,
+    push_e_strength: float = 1.0,
+    pull_e_strength: float = 1.0,
+    pull_center_strength: float = 1.0,
+):
+    _push_v_strength = 0.006
+    _push_e_strength = 0.0
+    _pull_e_strength = 0.045
+    _pull_center_strength = 0.01
+
+    push_v_strength = fill_strength(push_v_strength, _push_v_strength)
+    push_e_strength = fill_strength(push_e_strength, _push_e_strength)
+    pull_e_strength = fill_strength(pull_e_strength, _pull_e_strength)
+    pull_center_strength = fill_strength(pull_center_strength, _pull_center_strength)
+
+    return push_v_strength, push_e_strength, pull_e_strength, pull_center_strength
+
+
+def default_bipartite_strength(
+    num_u: int,
+    num_v: int,
+    e_list: List[tuple],
+    push_u_strength: float = 1.0,
+    push_v_strength: float = 1.0,
+    push_e_strength: float = 1.0,
+    pull_e_strength: float = 1.0,
+    pull_u_center_strength: float = 1.0,
+    pull_v_center_strength: float = 1.0,
+):
+    _push_u_strength = 0.005
+    _push_v_strength = 0.005
+    _push_e_strength = 0.0
+    _pull_e_strength = 0.03
+    _pull_u_center_strength = 0.04
+    _pull_v_center_strength = 0.04
+
+    push_u_strength = fill_strength(push_u_strength, _push_u_strength)
+    push_v_strength = fill_strength(push_v_strength, _push_v_strength)
+    push_e_strength = fill_strength(push_e_strength, _push_e_strength)
+    pull_e_strength = fill_strength(pull_e_strength, _pull_e_strength)
+    pull_u_center_strength = fill_strength(
+        pull_u_center_strength, _pull_u_center_strength
+    )
+    pull_v_center_strength = fill_strength(
+        pull_v_center_strength, _pull_v_center_strength
+    )
+
+    return (
+        push_u_strength,
+        push_v_strength,
+        push_e_strength,
+        pull_e_strength,
+        pull_u_center_strength,
+        pull_v_center_strength,
+    )
+
+
+def default_hypergraph_strength(
+    num_v: int,
+    e_list: List[tuple],
+    push_v_strength: float = 1.0,
+    push_e_strength: float = 1.0,
+    pull_e_strength: float = 1.0,
+    pull_center_strength: float = 1.0,
+):
+    _push_v_strength = 0.006
+    _push_e_strength = 0.008
+    _pull_e_strength = 0.007
+    _pull_center_strength = 0.001
+
+    push_v_strength = fill_strength(push_v_strength, _push_v_strength)
+    push_e_strength = fill_strength(push_e_strength, _push_e_strength)
+    pull_e_strength = fill_strength(pull_e_strength, _pull_e_strength)
+    pull_center_strength = fill_strength(pull_center_strength, _pull_center_strength)
+
+    return push_v_strength, push_e_strength, pull_e_strength, pull_center_strength
+
+
+def fill_color(
+    custom_color: Optional[Union[str, list]], default_color: Any, length: int
+):
+    if custom_color is None:
+        return [default_color] * length
+    elif isinstance(custom_color, list):
+        if (
+            isinstance(custom_color[0], str)
+            or isinstance(custom_color[0], tuple)
+            or isinstance(custom_color[0], list)
+        ):
+            return custom_color
+        else:
+            return [custom_color] * length
+    elif isinstance(custom_color, str):
+        return [custom_color] * length
+    else:
+        raise ValueError("The specified value is not a valid type.")
+
+
+def fill_sizes(
+    custom_scales: Optional[Union[float, list]], default_value: Any, length: int
+):
+    if custom_scales is None:
+        return [default_value] * length
+    elif isinstance(custom_scales, list):
+        assert (
+            len(custom_scales) == length
+        ), "The specified value list has the wrong length."
+        return [default_value * scale for scale in custom_scales]
+    elif isinstance(custom_scales, float):
+        return [default_value * custom_scales] * length
+    elif isinstance(custom_scales, int):
+        return [default_value * float(custom_scales)] * length
+    else:
+        raise ValueError("The specified value is not a valid type.")
+
+
+def fill_strength(custom_scale: Optional[float], default_value: float):
+    if custom_scale is None:
+        return default_value
+    return custom_scale * default_value
```

## easygraph/functions/drawing/positioning.py

 * *Ordering differences only*

```diff
@@ -1,643 +1,643 @@
-import easygraph as eg
-
-from easygraph.utils.exception import EasyGraphError
-
-
-__all__ = [
-    "random_position",
-    "circular_position",
-    "shell_position",
-    "rescale_position",
-    "kamada_kawai_layout",
-    # "spring_layout",
-    # "fruchterman_reingold_layout",
-    # "_process_params",
-    # "_fruchterman_reingold",
-    # "_sparse_fruchterman_reingold",
-]
-
-
-def random_position(G, center=None, dim=2, random_seed=None):
-    """
-    Returns random position for each node in graph G.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-
-    center : array-like or None, optional (default : None)
-        Coordinate pair around which to center the layout
-
-    dim : int, optional (default : 2)
-        Dimension of layout
-
-    random_seed : int or None, optional (default : None)
-        Seed for RandomState instance
-
-    Returns
-    ----------
-    pos : dict
-        A dictionary of positions keyed by node
-    """
-    import numpy as np
-
-    center = _get_center(center, dim)
-
-    rng = np.random.RandomState(seed=random_seed)
-    pos = rng.rand(len(G), dim) + center
-    pos = pos.astype(np.float32)
-    pos = dict(zip(G, pos))
-
-    return pos
-
-
-def circular_position(G, center=None, scale=1):
-    """
-    Position nodes on a circle, the dimension is 2.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-        A position will be assigned to every node in G
-
-    center : array-like or None, optional (default : None)
-        Coordinate pair around which to center the layout
-
-    scale : number, optional (default : 1)
-        Scale factor for positions
-
-    Returns
-    -------
-    pos : dict
-        A dictionary of positions keyed by node
-    """
-    import numpy as np
-
-    center = _get_center(center, dim=2)
-
-    if len(G) == 0:
-        pos = {}
-    elif len(G) == 1:
-        pos = {G.nodes[0]: center}
-    else:
-        theta = np.linspace(0, 1, len(G), endpoint=False) * 2 * np.pi
-        theta = theta.astype(np.float32)
-        pos = np.column_stack([np.cos(theta), np.sin(theta)])
-        pos = rescale_position(pos, scale=scale) + center
-        pos = dict(zip(G, pos))
-
-    return pos
-
-
-def shell_position(G, nlist=None, scale=1, center=None):
-    """
-    Position nodes in concentric circles, the dimension is 2.
-
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-
-    nlist : list of lists or None, optional (default : None)
-       List of node lists for each shell.
-
-    scale : number, optional (default : 1)
-        Scale factor for positions.
-
-    center : array-like or None, optional (default : None)
-        Coordinate pair around which to center the layout.
-
-
-    Returns
-    -------
-    pos : dict
-        A dictionary of positions keyed by node
-
-    Notes
-    -----
-    This algorithm currently only works in two dimensions and does not
-    try to minimize edge crossings.
-
-    """
-    import numpy as np
-
-    center = _get_center(center, dim=2)
-
-    if len(G) == 0:
-        return {}
-    if len(G) == 1:
-        return {G.nodes[0]: center}
-
-    if nlist is None:
-        # draw the whole graph in one shell
-        nlist = [list(G)]
-
-    if len(nlist[0]) == 1:
-        # single node at center
-        radius = 0.0
-    else:
-        # else start at r=1
-        radius = 1.0
-
-    npos = {}
-    for nodes in nlist:
-        # Discard the extra angle since it matches 0 radians.
-        theta = np.linspace(0, 1, len(nodes), endpoint=False) * 2 * np.pi
-        theta = theta.astype(np.float32)
-        pos = np.column_stack([np.cos(theta), np.sin(theta)])
-        if len(pos) > 1:
-            pos = rescale_position(pos, scale=scale * radius / len(nlist)) + center
-        else:
-            pos = np.array([(scale * radius + center[0], center[1])])
-        npos.update(zip(nodes, pos))
-        radius += 1.0
-
-    return npos
-
-
-def _get_center(center, dim):
-    import numpy as np
-
-    if center is None:
-        center = np.zeros(dim)
-    else:
-        center = np.asarray(center)
-
-    if dim < 2:
-        raise ValueError("cannot handle dimensions < 2")
-
-    if len(center) != dim:
-        msg = "length of center coordinates must match dimension of layout"
-        raise ValueError(msg)
-
-    return center
-
-
-def rescale_position(pos, scale=1):
-    """
-    Returns scaled position array to (-scale, scale) in all axes.
-
-    Parameters
-    ----------
-    pos : numpy array
-        positions to be scaled. Each row is a position.
-
-    scale : number, optional (default : 1)
-        The size of the resulting extent in all directions.
-
-    Returns
-    -------
-    pos : numpy array
-        scaled positions. Each row is a position.
-    """
-    # Find max length over all dimensions
-    lim = 0  # max coordinate for all axes
-    for i in range(pos.shape[1]):
-        pos[:, i] -= pos[:, i].mean()
-        lim = max(abs(pos[:, i]).max(), lim)
-    # rescale to (-scale, scale) in all directions, preserves aspect
-    if lim > 0:
-        for i in range(pos.shape[1]):
-            pos[:, i] *= scale / lim
-    return pos
-
-
-def kamada_kawai_layout(
-    G, dist=None, pos=None, weight="weight", scale=1, center=None, dim=2
-):
-    """Position nodes using Kamada-Kawai basic-length cost-function.
-
-    Parameters
-    ----------
-    G : graph or list of nodes
-        A position will be assigned to every node in G.
-
-    dist : dict (default=None)
-        A two-level dictionary of optimal distances between nodes,
-        indexed by source and destination node.
-        If None, the distance is computed using shortest_path_length().
-
-    pos : dict or None  optional (default=None)
-        Initial positions for nodes as a dictionary with node as keys
-        and values as a coordinate list or tuple.  If None, then use
-        circular_layout() for dim >= 2 and a linear layout for dim == 1.
-
-    weight : string or None   optional (default='weight')
-        The edge attribute that holds the numerical value used for
-        the edge weight.  If None, then all edge weights are 1.
-
-    scale : number (default: 1)
-        Scale factor for positions.
-
-    center : array-like or None
-        Coordinate pair around which to center the layout.
-
-    dim : int
-        Dimension of layout.
-
-    Returns
-    -------
-    pos : dict
-        A dictionary of positions keyed by node
-
-    Examples
-    --------
-    >>> pos = eg.kamada_kawai_layout(G)
-    """
-    import numpy as np
-
-    nNodes = len(G)
-    if nNodes == 0:
-        return {}
-
-    if dist is None:
-        dist = dict(eg.Floyd(G))
-    dist_mtx = 1e6 * np.ones((nNodes, nNodes))
-    for row, nr in enumerate(G):
-        if nr not in dist:
-            continue
-        rdist = dist[nr]
-        for col, nc in enumerate(G):
-            if nc not in rdist:
-                continue
-            dist_mtx[row][col] = rdist[nc]
-
-    if pos is None:
-        if dim >= 3:
-            pos = eg.random_position(G, dim=dim)
-        elif dim == 2:
-            pos = eg.circular_position(G)
-        else:
-            pos = {n: pt for n, pt in zip(G, np.linspace(0, 1, len(G)))}
-
-    pos_arr = np.array([pos[n] for n in G])
-
-    pos = _kamada_kawai_solve(dist_mtx, pos_arr, dim)
-
-    if center is None:
-        center = np.zeros(dim)
-    else:
-        center = np.asarray(center)
-
-    if len(center) != dim:
-        msg = "length of center coordinates must match dimension of layout"
-        raise ValueError(msg)
-
-    pos = eg.rescale_position(pos, scale=scale) + center
-    return dict(zip(G, pos))
-
-
-def _kamada_kawai_solve(dist_mtx, pos_arr, dim):
-    # Anneal node locations based on the Kamada-Kawai cost-function,
-    # using the supplied matrix of preferred inter-node distances,
-    # and starting locations.
-
-    import numpy as np
-
-    from scipy.optimize import minimize
-
-    meanwt = 1e-3
-    costargs = (np, 1 / (dist_mtx + np.eye(dist_mtx.shape[0]) * 1e-3), meanwt, dim)
-
-    optresult = minimize(
-        _kamada_kawai_costfn,
-        pos_arr.ravel(),
-        method="L-BFGS-B",
-        args=costargs,
-        jac=True,
-    )
-
-    return optresult.x.reshape((-1, dim))
-
-
-def _kamada_kawai_costfn(pos_vec, np, invdist, meanweight, dim):
-    # Cost-function and gradient for Kamada-Kawai layout algorithm
-    nNodes = invdist.shape[0]
-    pos_arr = pos_vec.reshape((nNodes, dim))
-
-    delta = pos_arr[:, np.newaxis, :] - pos_arr[np.newaxis, :, :]
-    nodesep = np.linalg.norm(delta, axis=-1)
-    direction = np.einsum("ijk,ij->ijk", delta, 1 / (nodesep + np.eye(nNodes) * 1e-3))
-
-    offset = nodesep * invdist - 1.0
-    offset[np.diag_indices(nNodes)] = 0
-
-    cost = 0.5 * np.sum(offset**2)
-    grad = np.einsum("ij,ij,ijk->ik", invdist, offset, direction) - np.einsum(
-        "ij,ij,ijk->jk", invdist, offset, direction
-    )
-
-    # Additional parabolic term to encourage mean position to be near origin:
-    sumpos = np.sum(pos_arr, axis=0)
-    cost += 0.5 * meanweight * np.sum(sumpos**2)
-    grad += meanweight * sumpos
-
-    return (cost, grad.ravel())
-
-
-# @np_random_state(10)
-# def spring_layout(
-#     G,
-#     k=None,
-#     pos=None,
-#     fixed=None,
-#     iterations=50,
-#     threshold=1e-4,
-#     weight="weight",
-#     scale=1,
-#     center=None,
-#     dim=2,
-#     seed=None,
-# ):
-#     """Position nodes using Fruchterman-Reingold force-directed algorithm.
-#
-#     The algorithm simulates a force-directed representation of the network
-#     treating edges as springs holding nodes close, while treating nodes
-#     as repelling objects, sometimes called an anti-gravity force.
-#     Simulation continues until the positions are close to an equilibrium.
-#
-#     There are some hard-coded values: minimal distance between
-#     nodes (0.01) and "temperature" of 0.1 to ensure nodes don't fly away.
-#     During the simulation, `k` helps determine the distance between nodes,
-#     though `scale` and `center` determine the size and place after
-#     rescaling occurs at the end of the simulation.
-#
-#     Fixing some nodes doesn't allow them to move in the simulation.
-#     It also turns off the rescaling feature at the simulation's end.
-#     In addition, setting `scale` to `None` turns off rescaling.
-#
-#     Parameters
-#     ----------
-#     G : EasyGraph graph or list of nodes
-#         A position will be assigned to every node in G.
-#
-#     k : float (default=None)
-#         Optimal distance between nodes.  If None the distance is set to
-#         1/sqrt(n) where n is the number of nodes.  Increase this value
-#         to move nodes farther apart.
-#
-#     pos : dict or None  optional (default=None)
-#         Initial positions for nodes as a dictionary with node as keys
-#         and values as a coordinate list or tuple.  If None, then use
-#         random initial positions.
-#
-#     fixed : list or None  optional (default=None)
-#         Nodes to keep fixed at initial position.
-#         Nodes not in ``G.nodes`` are ignored.
-#         ValueError raised if `fixed` specified and `pos` not.
-#
-#     iterations : int  optional (default=50)
-#         Maximum number of iterations taken
-#
-#     threshold: float optional (default = 1e-4)
-#         Threshold for relative error in node position changes.
-#         The iteration stops if the error is below this threshold.
-#
-#     weight : string or None   optional (default='weight')
-#         The edge attribute that holds the numerical value used for
-#         the edge weight.  Larger means a stronger attractive force.
-#         If None, then all edge weights are 1.
-#
-#     scale : number or None (default: 1)
-#         Scale factor for positions. Not used unless `fixed is None`.
-#         If scale is None, no rescaling is performed.
-#
-#     center : array-like or None
-#         Coordinate pair around which to center the layout.
-#         Not used unless `fixed is None`.
-#
-#     dim : int
-#         Dimension of layout.
-#
-#     seed : int, RandomState instance or None  optional (default=None)
-#         Set the random state for deterministic node layouts.
-#         If int, `seed` is the seed used by the random number generator,
-#         if numpy.random.RandomState instance, `seed` is the random
-#         number generator,
-#         if None, the random number generator is the RandomState instance used
-#         by numpy.random.
-#
-#     Returns
-#     -------
-#     pos : dict
-#         A dictionary of positions keyed by node
-#
-#     Examples
-#     --------
-#     >>> G = eg.path_graph(4)
-#     >>> pos = eg.spring_layout(G)
-#
-#
-#     """
-#     import numpy as np
-#
-#     G, center = _process_params(G, center, dim)
-#
-#     if fixed is not None:
-#         if pos is None:
-#             raise ValueError("nodes are fixed without positions given")
-#         for node in fixed:
-#             if node not in pos:
-#                 raise ValueError("nodes are fixed without positions given")
-#         nfixed = {node: i for i, node in enumerate(G)}
-#         fixed = np.asarray([nfixed[node] for node in fixed if node in nfixed])
-#
-#     if pos is not None:
-#         # Determine size of existing domain to adjust initial positions
-#         dom_size = max(coord for pos_tup in pos.values() for coord in pos_tup)
-#         if dom_size == 0:
-#             dom_size = 1
-#         pos_arr = seed.rand(len(G), dim) * dom_size + center
-#
-#         for i, n in enumerate(G):
-#             if n in pos:
-#                 pos_arr[i] = np.asarray(pos[n])
-#     else:
-#         pos_arr = None
-#         dom_size = 1
-#
-#     if len(G) == 0:
-#         return {}
-#     if len(G) == 1:
-#         return {eg.utils.arbitrary_element(G.nodes()): center}
-#
-#     try:
-#         # Sparse matrix
-#         if len(G) < 500:  # sparse solver for large graphs
-#             raise ValueError
-#         A = eg.to_scipy_sparse_array(G, weight=weight, dtype="f")
-#         if k is None and fixed is not None:
-#             # We must adjust k by domain size for layouts not near 1x1
-#             nnodes, _ = A.shape
-#             k = dom_size / np.sqrt(nnodes)
-#         pos = _sparse_fruchterman_reingold(
-#             A, k, pos_arr, fixed, iterations, threshold, dim, seed
-#         )
-#     except ValueError:
-#         A = eg.to_numpy_array(G, weight=weight)
-#         if k is None and fixed is not None:
-#             # We must adjust k by domain size for layouts not near 1x1
-#             nnodes, _ = A.shape
-#             k = dom_size / np.sqrt(nnodes)
-#         pos = _fruchterman_reingold(
-#             A, k, pos_arr, fixed, iterations, threshold, dim, seed
-#         )
-#     if fixed is None and scale is not None:
-#         pos = rescale_position(pos, scale=scale) + center
-#     pos = dict(zip(G, pos))
-#     return pos
-#
-# fruchterman_reingold_layout = spring_layout
-#
-# def _process_params(G, center, dim):
-#     # Some boilerplate code.
-#     import numpy as np
-#
-#     if not isinstance(G, eg.Graph):
-#         empty_graph = eg.Graph()
-#         empty_graph.add_nodes_from(G)
-#         G = empty_graph
-#
-#     if center is None:
-#         center = np.zeros(dim)
-#     else:
-#         center = np.asarray(center)
-#
-#     if len(center) != dim:
-#         msg = "length of center coordinates must match dimension of layout"
-#         raise ValueError(msg)
-#
-#     return G, center
-#
-# @np_random_state(7)
-# def _fruchterman_reingold(
-#     A, k=None, pos=None, fixed=None, iterations=50, threshold=1e-4, dim=2, seed=None
-# ):
-#     # Position nodes in adjacency matrix A using Fruchterman-Reingold
-#     # Entry point for NetworkX graph is fruchterman_reingold_layout()
-#     import numpy as np
-#
-#     try:
-#         nnodes, _ = A.shape
-#     except AttributeError as err:
-#         msg = "fruchterman_reingold() takes an adjacency matrix as input"
-#         raise EasyGraphError(msg) from err
-#
-#     if pos is None:
-#         # random initial positions
-#         pos = np.asarray(seed.rand(nnodes, dim), dtype=A.dtype)
-#     else:
-#         # make sure positions are of same type as matrix
-#         pos = pos.astype(A.dtype)
-#
-#     # optimal distance between nodes
-#     if k is None:
-#         k = np.sqrt(1.0 / nnodes)
-#     # the initial "temperature"  is about .1 of domain area (=1x1)
-#     # this is the largest step allowed in the dynamics.
-#     # We need to calculate this in case our fixed positions force our domain
-#     # to be much bigger than 1x1
-#     t = max(max(pos.T[0]) - min(pos.T[0]), max(pos.T[1]) - min(pos.T[1])) * 0.1
-#     # simple cooling scheme.
-#     # linearly step down by dt on each iteration so last iteration is size dt.
-#     dt = t / (iterations + 1)
-#     delta = np.zeros((pos.shape[0], pos.shape[0], pos.shape[1]), dtype=A.dtype)
-#     # the inscrutable (but fast) version
-#     # this is still O(V^2)
-#     # could use multilevel methods to speed this up significantly
-#     for iteration in range(iterations):
-#         # matrix of difference between points
-#         delta = pos[:, np.newaxis, :] - pos[np.newaxis, :, :]
-#         # distance between points
-#         distance = np.linalg.norm(delta, axis=-1)
-#         # enforce minimum distance of 0.01
-#         np.clip(distance, 0.01, None, out=distance)
-#         # displacement "force"
-#         displacement = np.einsum(
-#             "ijk,ij->ik", delta, (k * k / distance**2 - A * distance / k)
-#         )
-#         # update positions
-#         length = np.linalg.norm(displacement, axis=-1)
-#         length = np.where(length < 0.01, 0.1, length)
-#         delta_pos = np.einsum("ij,i->ij", displacement, t / length)
-#         if fixed is not None:
-#             # don't change positions of fixed nodes
-#             delta_pos[fixed] = 0.0
-#         pos += delta_pos
-#         # cool temperature
-#         t -= dt
-#         if (np.linalg.norm(delta_pos) / nnodes) < threshold:
-#             break
-#     return pos
-#
-# @np_random_state(7)
-# def _sparse_fruchterman_reingold(
-#     A, k=None, pos=None, fixed=None, iterations=50, threshold=1e-4, dim=2, seed=None
-# ):
-#     # Position nodes in adjacency matrix A using Fruchterman-Reingold
-#     # Entry point for NetworkX graph is fruchterman_reingold_layout()
-#     # Sparse version
-#     import numpy as np
-#     import scipy as sp
-#     import scipy.sparse  # call as sp.sparse
-#
-#     try:
-#         nnodes, _ = A.shape
-#     except AttributeError as err:
-#         msg = "fruchterman_reingold() takes an adjacency matrix as input"
-#         raise EasyGraphError(msg) from err
-#     # make sure we have a LIst of Lists representation
-#     try:
-#         A = A.tolil()
-#     except AttributeError:
-#         A = (sp.sparse.coo_array(A)).tolil()
-#
-#     if pos is None:
-#         # random initial positions
-#         pos = np.asarray(seed.rand(nnodes, dim), dtype=A.dtype)
-#     else:
-#         # make sure positions are of same type as matrix
-#         pos = pos.astype(A.dtype)
-#
-#     # no fixed nodes
-#     if fixed is None:
-#         fixed = []
-#
-#     # optimal distance between nodes
-#     if k is None:
-#         k = np.sqrt(1.0 / nnodes)
-#     # the initial "temperature"  is about .1 of domain area (=1x1)
-#     # this is the largest step allowed in the dynamics.
-#     t = max(max(pos.T[0]) - min(pos.T[0]), max(pos.T[1]) - min(pos.T[1])) * 0.1
-#     # simple cooling scheme.
-#     # linearly step down by dt on each iteration so last iteration is size dt.
-#     dt = t / (iterations + 1)
-#
-#     displacement = np.zeros((dim, nnodes))
-#     for iteration in range(iterations):
-#         displacement *= 0
-#         # loop over rows
-#         for i in range(A.shape[0]):
-#             if i in fixed:
-#                 continue
-#             # difference between this row's node position and all others
-#             delta = (pos[i] - pos).T
-#             # distance between points
-#             distance = np.sqrt((delta**2).sum(axis=0))
-#             # enforce minimum distance of 0.01
-#             distance = np.where(distance < 0.01, 0.01, distance)
-#             # the adjacency matrix row
-#             Ai = A.getrowview(i).toarray()  # TODO: revisit w/ sparse 1D container
-#             # displacement "force"
-#             displacement[:, i] += (
-#                 delta * (k * k / distance**2 - Ai * distance / k)
-#             ).sum(axis=1)
-#         # update positions
-#         length = np.sqrt((displacement**2).sum(axis=0))
-#         length = np.where(length < 0.01, 0.1, length)
-#         delta_pos = (displacement * t / length).T
-#         pos += delta_pos
-#         # cool temperature
-#         t -= dt
-#         if (np.linalg.norm(delta_pos) / nnodes) < threshold:
-#             break
-#     return pos
+import easygraph as eg
+
+from easygraph.utils.exception import EasyGraphError
+
+
+__all__ = [
+    "random_position",
+    "circular_position",
+    "shell_position",
+    "rescale_position",
+    "kamada_kawai_layout",
+    # "spring_layout",
+    # "fruchterman_reingold_layout",
+    # "_process_params",
+    # "_fruchterman_reingold",
+    # "_sparse_fruchterman_reingold",
+]
+
+
+def random_position(G, center=None, dim=2, random_seed=None):
+    """
+    Returns random position for each node in graph G.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+
+    center : array-like or None, optional (default : None)
+        Coordinate pair around which to center the layout
+
+    dim : int, optional (default : 2)
+        Dimension of layout
+
+    random_seed : int or None, optional (default : None)
+        Seed for RandomState instance
+
+    Returns
+    ----------
+    pos : dict
+        A dictionary of positions keyed by node
+    """
+    import numpy as np
+
+    center = _get_center(center, dim)
+
+    rng = np.random.RandomState(seed=random_seed)
+    pos = rng.rand(len(G), dim) + center
+    pos = pos.astype(np.float32)
+    pos = dict(zip(G, pos))
+
+    return pos
+
+
+def circular_position(G, center=None, scale=1):
+    """
+    Position nodes on a circle, the dimension is 2.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+        A position will be assigned to every node in G
+
+    center : array-like or None, optional (default : None)
+        Coordinate pair around which to center the layout
+
+    scale : number, optional (default : 1)
+        Scale factor for positions
+
+    Returns
+    -------
+    pos : dict
+        A dictionary of positions keyed by node
+    """
+    import numpy as np
+
+    center = _get_center(center, dim=2)
+
+    if len(G) == 0:
+        pos = {}
+    elif len(G) == 1:
+        pos = {G.nodes[0]: center}
+    else:
+        theta = np.linspace(0, 1, len(G), endpoint=False) * 2 * np.pi
+        theta = theta.astype(np.float32)
+        pos = np.column_stack([np.cos(theta), np.sin(theta)])
+        pos = rescale_position(pos, scale=scale) + center
+        pos = dict(zip(G, pos))
+
+    return pos
+
+
+def shell_position(G, nlist=None, scale=1, center=None):
+    """
+    Position nodes in concentric circles, the dimension is 2.
+
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+
+    nlist : list of lists or None, optional (default : None)
+       List of node lists for each shell.
+
+    scale : number, optional (default : 1)
+        Scale factor for positions.
+
+    center : array-like or None, optional (default : None)
+        Coordinate pair around which to center the layout.
+
+
+    Returns
+    -------
+    pos : dict
+        A dictionary of positions keyed by node
+
+    Notes
+    -----
+    This algorithm currently only works in two dimensions and does not
+    try to minimize edge crossings.
+
+    """
+    import numpy as np
+
+    center = _get_center(center, dim=2)
+
+    if len(G) == 0:
+        return {}
+    if len(G) == 1:
+        return {G.nodes[0]: center}
+
+    if nlist is None:
+        # draw the whole graph in one shell
+        nlist = [list(G)]
+
+    if len(nlist[0]) == 1:
+        # single node at center
+        radius = 0.0
+    else:
+        # else start at r=1
+        radius = 1.0
+
+    npos = {}
+    for nodes in nlist:
+        # Discard the extra angle since it matches 0 radians.
+        theta = np.linspace(0, 1, len(nodes), endpoint=False) * 2 * np.pi
+        theta = theta.astype(np.float32)
+        pos = np.column_stack([np.cos(theta), np.sin(theta)])
+        if len(pos) > 1:
+            pos = rescale_position(pos, scale=scale * radius / len(nlist)) + center
+        else:
+            pos = np.array([(scale * radius + center[0], center[1])])
+        npos.update(zip(nodes, pos))
+        radius += 1.0
+
+    return npos
+
+
+def _get_center(center, dim):
+    import numpy as np
+
+    if center is None:
+        center = np.zeros(dim)
+    else:
+        center = np.asarray(center)
+
+    if dim < 2:
+        raise ValueError("cannot handle dimensions < 2")
+
+    if len(center) != dim:
+        msg = "length of center coordinates must match dimension of layout"
+        raise ValueError(msg)
+
+    return center
+
+
+def rescale_position(pos, scale=1):
+    """
+    Returns scaled position array to (-scale, scale) in all axes.
+
+    Parameters
+    ----------
+    pos : numpy array
+        positions to be scaled. Each row is a position.
+
+    scale : number, optional (default : 1)
+        The size of the resulting extent in all directions.
+
+    Returns
+    -------
+    pos : numpy array
+        scaled positions. Each row is a position.
+    """
+    # Find max length over all dimensions
+    lim = 0  # max coordinate for all axes
+    for i in range(pos.shape[1]):
+        pos[:, i] -= pos[:, i].mean()
+        lim = max(abs(pos[:, i]).max(), lim)
+    # rescale to (-scale, scale) in all directions, preserves aspect
+    if lim > 0:
+        for i in range(pos.shape[1]):
+            pos[:, i] *= scale / lim
+    return pos
+
+
+def kamada_kawai_layout(
+    G, dist=None, pos=None, weight="weight", scale=1, center=None, dim=2
+):
+    """Position nodes using Kamada-Kawai basic-length cost-function.
+
+    Parameters
+    ----------
+    G : graph or list of nodes
+        A position will be assigned to every node in G.
+
+    dist : dict (default=None)
+        A two-level dictionary of optimal distances between nodes,
+        indexed by source and destination node.
+        If None, the distance is computed using shortest_path_length().
+
+    pos : dict or None  optional (default=None)
+        Initial positions for nodes as a dictionary with node as keys
+        and values as a coordinate list or tuple.  If None, then use
+        circular_layout() for dim >= 2 and a linear layout for dim == 1.
+
+    weight : string or None   optional (default='weight')
+        The edge attribute that holds the numerical value used for
+        the edge weight.  If None, then all edge weights are 1.
+
+    scale : number (default: 1)
+        Scale factor for positions.
+
+    center : array-like or None
+        Coordinate pair around which to center the layout.
+
+    dim : int
+        Dimension of layout.
+
+    Returns
+    -------
+    pos : dict
+        A dictionary of positions keyed by node
+
+    Examples
+    --------
+    >>> pos = eg.kamada_kawai_layout(G)
+    """
+    import numpy as np
+
+    nNodes = len(G)
+    if nNodes == 0:
+        return {}
+
+    if dist is None:
+        dist = dict(eg.Floyd(G))
+    dist_mtx = 1e6 * np.ones((nNodes, nNodes))
+    for row, nr in enumerate(G):
+        if nr not in dist:
+            continue
+        rdist = dist[nr]
+        for col, nc in enumerate(G):
+            if nc not in rdist:
+                continue
+            dist_mtx[row][col] = rdist[nc]
+
+    if pos is None:
+        if dim >= 3:
+            pos = eg.random_position(G, dim=dim)
+        elif dim == 2:
+            pos = eg.circular_position(G)
+        else:
+            pos = {n: pt for n, pt in zip(G, np.linspace(0, 1, len(G)))}
+
+    pos_arr = np.array([pos[n] for n in G])
+
+    pos = _kamada_kawai_solve(dist_mtx, pos_arr, dim)
+
+    if center is None:
+        center = np.zeros(dim)
+    else:
+        center = np.asarray(center)
+
+    if len(center) != dim:
+        msg = "length of center coordinates must match dimension of layout"
+        raise ValueError(msg)
+
+    pos = eg.rescale_position(pos, scale=scale) + center
+    return dict(zip(G, pos))
+
+
+def _kamada_kawai_solve(dist_mtx, pos_arr, dim):
+    # Anneal node locations based on the Kamada-Kawai cost-function,
+    # using the supplied matrix of preferred inter-node distances,
+    # and starting locations.
+
+    import numpy as np
+
+    from scipy.optimize import minimize
+
+    meanwt = 1e-3
+    costargs = (np, 1 / (dist_mtx + np.eye(dist_mtx.shape[0]) * 1e-3), meanwt, dim)
+
+    optresult = minimize(
+        _kamada_kawai_costfn,
+        pos_arr.ravel(),
+        method="L-BFGS-B",
+        args=costargs,
+        jac=True,
+    )
+
+    return optresult.x.reshape((-1, dim))
+
+
+def _kamada_kawai_costfn(pos_vec, np, invdist, meanweight, dim):
+    # Cost-function and gradient for Kamada-Kawai layout algorithm
+    nNodes = invdist.shape[0]
+    pos_arr = pos_vec.reshape((nNodes, dim))
+
+    delta = pos_arr[:, np.newaxis, :] - pos_arr[np.newaxis, :, :]
+    nodesep = np.linalg.norm(delta, axis=-1)
+    direction = np.einsum("ijk,ij->ijk", delta, 1 / (nodesep + np.eye(nNodes) * 1e-3))
+
+    offset = nodesep * invdist - 1.0
+    offset[np.diag_indices(nNodes)] = 0
+
+    cost = 0.5 * np.sum(offset**2)
+    grad = np.einsum("ij,ij,ijk->ik", invdist, offset, direction) - np.einsum(
+        "ij,ij,ijk->jk", invdist, offset, direction
+    )
+
+    # Additional parabolic term to encourage mean position to be near origin:
+    sumpos = np.sum(pos_arr, axis=0)
+    cost += 0.5 * meanweight * np.sum(sumpos**2)
+    grad += meanweight * sumpos
+
+    return (cost, grad.ravel())
+
+
+# @np_random_state(10)
+# def spring_layout(
+#     G,
+#     k=None,
+#     pos=None,
+#     fixed=None,
+#     iterations=50,
+#     threshold=1e-4,
+#     weight="weight",
+#     scale=1,
+#     center=None,
+#     dim=2,
+#     seed=None,
+# ):
+#     """Position nodes using Fruchterman-Reingold force-directed algorithm.
+#
+#     The algorithm simulates a force-directed representation of the network
+#     treating edges as springs holding nodes close, while treating nodes
+#     as repelling objects, sometimes called an anti-gravity force.
+#     Simulation continues until the positions are close to an equilibrium.
+#
+#     There are some hard-coded values: minimal distance between
+#     nodes (0.01) and "temperature" of 0.1 to ensure nodes don't fly away.
+#     During the simulation, `k` helps determine the distance between nodes,
+#     though `scale` and `center` determine the size and place after
+#     rescaling occurs at the end of the simulation.
+#
+#     Fixing some nodes doesn't allow them to move in the simulation.
+#     It also turns off the rescaling feature at the simulation's end.
+#     In addition, setting `scale` to `None` turns off rescaling.
+#
+#     Parameters
+#     ----------
+#     G : EasyGraph graph or list of nodes
+#         A position will be assigned to every node in G.
+#
+#     k : float (default=None)
+#         Optimal distance between nodes.  If None the distance is set to
+#         1/sqrt(n) where n is the number of nodes.  Increase this value
+#         to move nodes farther apart.
+#
+#     pos : dict or None  optional (default=None)
+#         Initial positions for nodes as a dictionary with node as keys
+#         and values as a coordinate list or tuple.  If None, then use
+#         random initial positions.
+#
+#     fixed : list or None  optional (default=None)
+#         Nodes to keep fixed at initial position.
+#         Nodes not in ``G.nodes`` are ignored.
+#         ValueError raised if `fixed` specified and `pos` not.
+#
+#     iterations : int  optional (default=50)
+#         Maximum number of iterations taken
+#
+#     threshold: float optional (default = 1e-4)
+#         Threshold for relative error in node position changes.
+#         The iteration stops if the error is below this threshold.
+#
+#     weight : string or None   optional (default='weight')
+#         The edge attribute that holds the numerical value used for
+#         the edge weight.  Larger means a stronger attractive force.
+#         If None, then all edge weights are 1.
+#
+#     scale : number or None (default: 1)
+#         Scale factor for positions. Not used unless `fixed is None`.
+#         If scale is None, no rescaling is performed.
+#
+#     center : array-like or None
+#         Coordinate pair around which to center the layout.
+#         Not used unless `fixed is None`.
+#
+#     dim : int
+#         Dimension of layout.
+#
+#     seed : int, RandomState instance or None  optional (default=None)
+#         Set the random state for deterministic node layouts.
+#         If int, `seed` is the seed used by the random number generator,
+#         if numpy.random.RandomState instance, `seed` is the random
+#         number generator,
+#         if None, the random number generator is the RandomState instance used
+#         by numpy.random.
+#
+#     Returns
+#     -------
+#     pos : dict
+#         A dictionary of positions keyed by node
+#
+#     Examples
+#     --------
+#     >>> G = eg.path_graph(4)
+#     >>> pos = eg.spring_layout(G)
+#
+#
+#     """
+#     import numpy as np
+#
+#     G, center = _process_params(G, center, dim)
+#
+#     if fixed is not None:
+#         if pos is None:
+#             raise ValueError("nodes are fixed without positions given")
+#         for node in fixed:
+#             if node not in pos:
+#                 raise ValueError("nodes are fixed without positions given")
+#         nfixed = {node: i for i, node in enumerate(G)}
+#         fixed = np.asarray([nfixed[node] for node in fixed if node in nfixed])
+#
+#     if pos is not None:
+#         # Determine size of existing domain to adjust initial positions
+#         dom_size = max(coord for pos_tup in pos.values() for coord in pos_tup)
+#         if dom_size == 0:
+#             dom_size = 1
+#         pos_arr = seed.rand(len(G), dim) * dom_size + center
+#
+#         for i, n in enumerate(G):
+#             if n in pos:
+#                 pos_arr[i] = np.asarray(pos[n])
+#     else:
+#         pos_arr = None
+#         dom_size = 1
+#
+#     if len(G) == 0:
+#         return {}
+#     if len(G) == 1:
+#         return {eg.utils.arbitrary_element(G.nodes()): center}
+#
+#     try:
+#         # Sparse matrix
+#         if len(G) < 500:  # sparse solver for large graphs
+#             raise ValueError
+#         A = eg.to_scipy_sparse_array(G, weight=weight, dtype="f")
+#         if k is None and fixed is not None:
+#             # We must adjust k by domain size for layouts not near 1x1
+#             nnodes, _ = A.shape
+#             k = dom_size / np.sqrt(nnodes)
+#         pos = _sparse_fruchterman_reingold(
+#             A, k, pos_arr, fixed, iterations, threshold, dim, seed
+#         )
+#     except ValueError:
+#         A = eg.to_numpy_array(G, weight=weight)
+#         if k is None and fixed is not None:
+#             # We must adjust k by domain size for layouts not near 1x1
+#             nnodes, _ = A.shape
+#             k = dom_size / np.sqrt(nnodes)
+#         pos = _fruchterman_reingold(
+#             A, k, pos_arr, fixed, iterations, threshold, dim, seed
+#         )
+#     if fixed is None and scale is not None:
+#         pos = rescale_position(pos, scale=scale) + center
+#     pos = dict(zip(G, pos))
+#     return pos
+#
+# fruchterman_reingold_layout = spring_layout
+#
+# def _process_params(G, center, dim):
+#     # Some boilerplate code.
+#     import numpy as np
+#
+#     if not isinstance(G, eg.Graph):
+#         empty_graph = eg.Graph()
+#         empty_graph.add_nodes_from(G)
+#         G = empty_graph
+#
+#     if center is None:
+#         center = np.zeros(dim)
+#     else:
+#         center = np.asarray(center)
+#
+#     if len(center) != dim:
+#         msg = "length of center coordinates must match dimension of layout"
+#         raise ValueError(msg)
+#
+#     return G, center
+#
+# @np_random_state(7)
+# def _fruchterman_reingold(
+#     A, k=None, pos=None, fixed=None, iterations=50, threshold=1e-4, dim=2, seed=None
+# ):
+#     # Position nodes in adjacency matrix A using Fruchterman-Reingold
+#     # Entry point for NetworkX graph is fruchterman_reingold_layout()
+#     import numpy as np
+#
+#     try:
+#         nnodes, _ = A.shape
+#     except AttributeError as err:
+#         msg = "fruchterman_reingold() takes an adjacency matrix as input"
+#         raise EasyGraphError(msg) from err
+#
+#     if pos is None:
+#         # random initial positions
+#         pos = np.asarray(seed.rand(nnodes, dim), dtype=A.dtype)
+#     else:
+#         # make sure positions are of same type as matrix
+#         pos = pos.astype(A.dtype)
+#
+#     # optimal distance between nodes
+#     if k is None:
+#         k = np.sqrt(1.0 / nnodes)
+#     # the initial "temperature"  is about .1 of domain area (=1x1)
+#     # this is the largest step allowed in the dynamics.
+#     # We need to calculate this in case our fixed positions force our domain
+#     # to be much bigger than 1x1
+#     t = max(max(pos.T[0]) - min(pos.T[0]), max(pos.T[1]) - min(pos.T[1])) * 0.1
+#     # simple cooling scheme.
+#     # linearly step down by dt on each iteration so last iteration is size dt.
+#     dt = t / (iterations + 1)
+#     delta = np.zeros((pos.shape[0], pos.shape[0], pos.shape[1]), dtype=A.dtype)
+#     # the inscrutable (but fast) version
+#     # this is still O(V^2)
+#     # could use multilevel methods to speed this up significantly
+#     for iteration in range(iterations):
+#         # matrix of difference between points
+#         delta = pos[:, np.newaxis, :] - pos[np.newaxis, :, :]
+#         # distance between points
+#         distance = np.linalg.norm(delta, axis=-1)
+#         # enforce minimum distance of 0.01
+#         np.clip(distance, 0.01, None, out=distance)
+#         # displacement "force"
+#         displacement = np.einsum(
+#             "ijk,ij->ik", delta, (k * k / distance**2 - A * distance / k)
+#         )
+#         # update positions
+#         length = np.linalg.norm(displacement, axis=-1)
+#         length = np.where(length < 0.01, 0.1, length)
+#         delta_pos = np.einsum("ij,i->ij", displacement, t / length)
+#         if fixed is not None:
+#             # don't change positions of fixed nodes
+#             delta_pos[fixed] = 0.0
+#         pos += delta_pos
+#         # cool temperature
+#         t -= dt
+#         if (np.linalg.norm(delta_pos) / nnodes) < threshold:
+#             break
+#     return pos
+#
+# @np_random_state(7)
+# def _sparse_fruchterman_reingold(
+#     A, k=None, pos=None, fixed=None, iterations=50, threshold=1e-4, dim=2, seed=None
+# ):
+#     # Position nodes in adjacency matrix A using Fruchterman-Reingold
+#     # Entry point for NetworkX graph is fruchterman_reingold_layout()
+#     # Sparse version
+#     import numpy as np
+#     import scipy as sp
+#     import scipy.sparse  # call as sp.sparse
+#
+#     try:
+#         nnodes, _ = A.shape
+#     except AttributeError as err:
+#         msg = "fruchterman_reingold() takes an adjacency matrix as input"
+#         raise EasyGraphError(msg) from err
+#     # make sure we have a LIst of Lists representation
+#     try:
+#         A = A.tolil()
+#     except AttributeError:
+#         A = (sp.sparse.coo_array(A)).tolil()
+#
+#     if pos is None:
+#         # random initial positions
+#         pos = np.asarray(seed.rand(nnodes, dim), dtype=A.dtype)
+#     else:
+#         # make sure positions are of same type as matrix
+#         pos = pos.astype(A.dtype)
+#
+#     # no fixed nodes
+#     if fixed is None:
+#         fixed = []
+#
+#     # optimal distance between nodes
+#     if k is None:
+#         k = np.sqrt(1.0 / nnodes)
+#     # the initial "temperature"  is about .1 of domain area (=1x1)
+#     # this is the largest step allowed in the dynamics.
+#     t = max(max(pos.T[0]) - min(pos.T[0]), max(pos.T[1]) - min(pos.T[1])) * 0.1
+#     # simple cooling scheme.
+#     # linearly step down by dt on each iteration so last iteration is size dt.
+#     dt = t / (iterations + 1)
+#
+#     displacement = np.zeros((dim, nnodes))
+#     for iteration in range(iterations):
+#         displacement *= 0
+#         # loop over rows
+#         for i in range(A.shape[0]):
+#             if i in fixed:
+#                 continue
+#             # difference between this row's node position and all others
+#             delta = (pos[i] - pos).T
+#             # distance between points
+#             distance = np.sqrt((delta**2).sum(axis=0))
+#             # enforce minimum distance of 0.01
+#             distance = np.where(distance < 0.01, 0.01, distance)
+#             # the adjacency matrix row
+#             Ai = A.getrowview(i).toarray()  # TODO: revisit w/ sparse 1D container
+#             # displacement "force"
+#             displacement[:, i] += (
+#                 delta * (k * k / distance**2 - Ai * distance / k)
+#             ).sum(axis=1)
+#         # update positions
+#         length = np.sqrt((displacement**2).sum(axis=0))
+#         length = np.where(length < 0.01, 0.1, length)
+#         delta_pos = (displacement * t / length).T
+#         pos += delta_pos
+#         # cool temperature
+#         t -= dt
+#         if (np.linalg.norm(delta_pos) / nnodes) < threshold:
+#             break
+#     return pos
```

## easygraph/functions/drawing/utils.py

```diff
@@ -1,581 +1,595 @@
-from itertools import chain
-from typing import List
-from typing import Optional
-from typing import Tuple
-
-import matplotlib
-import matplotlib.pyplot as plt
-import numpy as np
-
-from matplotlib.axes import Axes
-from matplotlib.collections import LineCollection
-from matplotlib.collections import PatchCollection
-from matplotlib.patches import Circle
-from matplotlib.patches import PathPatch
-from matplotlib.path import Path
-from scipy.spatial import ConvexHull
-
-from .geometry import common_tangent_radian
-from .geometry import polar_position
-from .geometry import rad_2_deg
-from .geometry import radian_from_atan
-from .geometry import vlen
-
-
-# from fa2 import ForceAtlas2
-# import bezier
-# import numpy as np
-# from easygraph import to_networkx
-# from easygraph.utils.exception import EasyGraphError
-# import easygraph as eg
-
-
-def safe_div(a: np.ndarray, b: np.ndarray, jitter_scale: float = 0.000001):
-    mask = b == 0
-    b[mask] = 1
-    inv_b = 1.0 / b
-    res = a * inv_b
-    if mask.sum() > 0:
-        res[mask.repeat(2, 2)] = np.random.randn(mask.sum() * 2) * jitter_scale
-    return res
-
-
-def init_pos(num_v: int, center: Tuple[float, float] = (0, 0), scale: float = 1.0):
-    return (np.random.rand(num_v, 2) * 2 - 1) * scale + center
-
-
-def draw_line_edge(
-    ax: Axes,
-    v_coor: np.array,
-    v_size: list,
-    e_list: List[Tuple[int, int]],
-    show_arrow: bool,
-    e_color: list,
-    e_line_width: list,
-):
-    arrow_head_width = (
-        [0.015 * w for w in e_line_width] if show_arrow else [0] * len(e_list)
-    )
-
-    for eidx, e in enumerate(e_list):
-        start_pos = v_coor[e[0]]
-        end_pos = v_coor[e[1]]
-
-        dir = end_pos - start_pos
-        dir = dir / np.linalg.norm(dir)
-
-        start_pos = start_pos + dir * v_size[e[0]]
-        end_pos = end_pos - dir * v_size[e[1]]
-
-        x, y = start_pos[0], start_pos[1]
-        dx, dy = end_pos[0] - x, end_pos[1] - y
-
-        ax.arrow(
-            x,
-            y,
-            dx,
-            dy,
-            head_width=arrow_head_width[eidx],
-            color=e_color[eidx],
-            linewidth=e_line_width[eidx],
-            length_includes_head=True,
-        )
-
-
-def draw_circle_edge(
-    ax: Axes,
-    v_coor: List[Tuple[float, float]],
-    v_size: list,
-    e_list: List[Tuple[int, int]],
-    e_color: list,
-    e_fill_color: list,
-    e_line_width: list,
-):
-    n_v = len(v_coor)
-    line_paths, arc_paths, vertices = hull_layout(n_v, e_list, v_coor, v_size)
-
-    for eidx, lines in enumerate(line_paths):
-        pathdata = []
-        for line in lines:
-            if len(line) == 0:
-                continue
-            start_pos, end_pos = line
-            pathdata.append((Path.MOVETO, start_pos.tolist()))
-            pathdata.append((Path.LINETO, end_pos.tolist()))
-
-        if len(list(zip(*pathdata))) == 0:
-            continue
-        codes, verts = zip(*pathdata)
-        path = Path(verts, codes)
-        ax.add_patch(
-            PathPatch(
-                path,
-                linewidth=e_line_width[eidx],
-                facecolor=e_fill_color[eidx],
-                edgecolor=e_color[eidx],
-            )
-        )
-
-    for eidx, arcs in enumerate(arc_paths):
-        for arc in arcs:
-            center, theta1, theta2, radius = arc
-            x, y = center[0], center[1]
-
-            ax.add_patch(
-                matplotlib.patches.Arc(
-                    (x, y),
-                    2 * radius,
-                    2 * radius,
-                    theta1=theta1,
-                    theta2=theta2,
-                    # color=e_color[eidx],
-                    linewidth=e_line_width[eidx],
-                    edgecolor=e_color[eidx],
-                    facecolor=e_fill_color[eidx],
-                )
-            )
-
-
-def edge_list_to_incidence_matrix(num_v: int, e_list: List[tuple]) -> np.ndarray:
-    v_idx = list(chain(*e_list))
-    e_idx = [[idx] * len(e) for idx, e in enumerate(e_list)]
-    e_idx = list(chain(*e_idx))
-    H = np.zeros((num_v, len(e_list)))
-    H[v_idx, e_idx] = 1
-    return H
-
-
-def draw_vertex(
-    ax: Axes,
-    v_coor: List[Tuple[float, float]],
-    v_label: Optional[List[str]],
-    font_size: int,
-    font_family: str,
-    v_size: list,
-    v_color: list,
-    edgecolors,
-    v_line_width: list,
-):
-    patches = []
-    n = v_coor.shape[0]
-    if v_label is None:
-        v_label = [""] * n
-    for coor, label, size, width in zip(v_coor.tolist(), v_label, v_size, v_line_width):
-        circle = Circle(coor, size)
-        circle.lineWidth = width
-        # circle.label = label
-        if label != "":
-            x, y = coor[0], coor[1]
-            offset = 0, -1.3 * size
-            x += offset[0]
-            y += offset[1]
-            ax.text(
-                x,
-                y,
-                label,
-                fontsize=font_size,
-                fontfamily=font_family,
-                ha="center",
-                va="top",
-            )
-        patches.append(circle)
-    edgecolors = "black" if edgecolors == None else edgecolors
-    p = PatchCollection(patches, facecolors=v_color, edgecolors=edgecolors)
-    ax.add_collection(p)
-
-
-def hull_layout(n_v, e_list, pos, v_size, radius_increment=0.3):
-    line_paths = [None] * len(e_list)
-    arc_paths = [None] * len(e_list)
-
-    polygons_vertices_index = []
-    vertices_radius = np.array(v_size)
-    vertices_increased_radius = vertices_radius * radius_increment
-    vertices_radius += vertices_increased_radius
-
-    e_degree = [len(e) for e in e_list]
-    e_idxs = np.argsort(np.array(e_degree))
-
-    # for edge in e_list:
-    for e_idx in e_idxs:
-        edge = list(e_list[e_idx])
-
-        line_path_for_e = []
-        arc_path_for_e = []
-
-        if len(edge) == 1:
-            arc_path_for_e.append([pos[edge[0]], 0, 360, vertices_radius[edge[0]]])
-
-            vertices_radius[edge] += vertices_increased_radius[edge]
-
-            line_paths[e_idx] = line_path_for_e
-            arc_paths[e_idx] = arc_path_for_e
-            continue
-
-        pos_in_edge = pos[edge]
-        if len(edge) == 2:
-            vertices_index = np.array((0, 1), dtype=np.int64)
-        else:
-            hull = ConvexHull(pos_in_edge)
-            vertices_index = hull.vertices
-
-        n_vertices = vertices_index.shape[0]
-
-        vertices_index = np.append(vertices_index, vertices_index[0])  # close the loop
-
-        thetas = []
-
-        for i in range(n_vertices):
-            # line
-            i1 = edge[vertices_index[i]]
-            i2 = edge[vertices_index[i + 1]]
-
-            r1 = vertices_radius[i1]
-            r2 = vertices_radius[i2]
-
-            p1 = pos[i1]
-            p2 = pos[i2]
-
-            dp = p2 - p1
-            dp_len = vlen(dp)
-
-            beta = radian_from_atan(dp[0], dp[1])
-            alpha = common_tangent_radian(r1, r2, dp_len)
-
-            theta = beta - alpha
-            start_point = polar_position(r1, theta, p1)
-            end_point = polar_position(r2, theta, p2)
-
-            line_path_for_e.append((start_point, end_point))
-            thetas.append(theta)
-
-        for i in range(n_vertices):
-            # arcs
-            theta_1 = thetas[i - 1]
-            theta_2 = thetas[i]
-
-            arc_center = pos[edge[vertices_index[i]]]
-            radius = vertices_radius[edge[vertices_index[i]]]
-
-            theta_1, theta_2 = rad_2_deg(theta_1), rad_2_deg(theta_2)
-            arc_path_for_e.append((arc_center, theta_1, theta_2, radius))
-
-        vertices_radius[edge] += vertices_increased_radius[edge]
-
-        polygons_vertices_index.append(vertices_index.copy())
-
-        # line_paths.append(line_path_for_e)
-        # arc_paths.append(arc_path_for_e)
-        line_paths[e_idx] = line_path_for_e
-        arc_paths[e_idx] = arc_path_for_e
-
-    return line_paths, arc_paths, polygons_vertices_index
-
-
-def apply_alpha(colors, alpha, elem_list, cmap=None, vmin=None, vmax=None):
-    """Apply an alpha (or list of alphas) to the colors provided.
-
-    Parameters
-    ----------
-
-    colors : color string or array of floats (default='r')
-        Color of element. Can be a single color format string,
-        or a sequence of colors with the same length as nodelist.
-        If numeric values are specified they will be mapped to
-        colors using the cmap and vmin,vmax parameters.  See
-        matplotlib.scatter for more details.
-
-    alpha : float or array of floats
-        Alpha values for elements. This can be a single alpha value, in
-        which case it will be applied to all the elements of color. Otherwise,
-        if it is an array, the elements of alpha will be applied to the colors
-        in order (cycling through alpha multiple times if necessary).
-
-    elem_list : array of networkx objects
-        The list of elements which are being colored. These could be nodes,
-        edges or labels.
-
-    cmap : matplotlib colormap
-        Color map for use if colors is a list of floats corresponding to points
-        on a color mapping.
-
-    vmin, vmax : float
-        Minimum and maximum values for normalizing colors if a colormap is used
-
-    Returns
-    -------
-
-    rgba_colors : numpy ndarray
-        Array containing RGBA format values for each of the node colours.
-
-    """
-    from itertools import cycle
-    from itertools import islice
-    from numbers import Number
-
-    import matplotlib as mpl
-    import matplotlib.cm  # call as mpl.cm
-    import matplotlib.colors  # call as mpl.colors
-    import numpy as np
-
-    # If we have been provided with a list of numbers as long as elem_list,
-    # apply the color mapping.
-    if len(colors) == len(elem_list) and isinstance(colors[0], Number):
-        mapper = mpl.cm.ScalarMappable(cmap=cmap)
-        mapper.set_clim(vmin, vmax)
-        rgba_colors = mapper.to_rgba(colors)
-    # Otherwise, convert colors to matplotlib's RGB using the colorConverter
-    # object.  These are converted to numpy ndarrays to be consistent with the
-    # to_rgba method of ScalarMappable.
-    else:
-        try:
-            rgba_colors = np.array([mpl.colors.colorConverter.to_rgba(colors)])
-        except ValueError:
-            rgba_colors = np.array(
-                [mpl.colors.colorConverter.to_rgba(color) for color in colors]
-            )
-    # Set the final column of the rgba_colors to have the relevant alpha values
-    try:
-        # If alpha is longer than the number of colors, resize to the number of
-        # elements.  Also, if rgba_colors.size (the number of elements of
-        # rgba_colors) is the same as the number of elements, resize the array,
-        # to avoid it being interpreted as a colormap by scatter()
-        if len(alpha) > len(rgba_colors) or rgba_colors.size == len(elem_list):
-            rgba_colors = np.resize(rgba_colors, (len(elem_list), 4))
-            rgba_colors[1:, 0] = rgba_colors[0, 0]
-            rgba_colors[1:, 1] = rgba_colors[0, 1]
-            rgba_colors[1:, 2] = rgba_colors[0, 2]
-        rgba_colors[:, 3] = list(islice(cycle(alpha), len(rgba_colors)))
-    except TypeError:
-        rgba_colors[:, -1] = alpha
-    return rgba_colors
-
-
-# def draw_easygraph_nodes(
-#     G,
-#     pos,
-#     nodelist=None,
-#     node_size=300,
-#     node_color="#1f78b4",
-#     node_shape="o",
-#     alpha=None,
-#     cmap=None,
-#     vmin=None,
-#     vmax=None,
-#     ax=None,
-#     linewidths=None,
-#     edgecolors=None,
-#     label=None,
-#     margins=None,
-# ):
-#     """Draw the nodes of the graph G.
-
-#     This draws only the nodes of the graph G.
-
-#     Parameters
-#     ----------
-#     G : graph
-#         A easygraph graph
-
-#     pos : dictionary
-#         A dictionary with nodes as keys and positions as values.
-#         Positions should be sequences of length 2.
-
-#     ax : Matplotlib Axes object, optional
-#         Draw the graph in the specified Matplotlib axes.
-
-#     nodelist : list (default list(G))
-#         Draw only specified nodes
-
-#     node_size : scalar or array (default=300)
-#         Size of nodes.  If an array it must be the same length as nodelist.
-
-#     node_color : color or array of colors (default='#1f78b4')
-#         Node color. Can be a single color or a sequence of colors with the same
-#         length as nodelist. Color can be string or rgb (or rgba) tuple of
-#         floats from 0-1. If numeric values are specified they will be
-#         mapped to colors using the cmap and vmin,vmax parameters. See
-#         matplotlib.scatter for more details.
-
-#     node_shape :  string (default='o')
-#         The shape of the node.  Specification is as matplotlib.scatter
-#         marker, one of 'so^>v<dph8'.
-
-#     alpha : float or array of floats (default=None)
-#         The node transparency.  This can be a single alpha value,
-#         in which case it will be applied to all the nodes of color. Otherwise,
-#         if it is an array, the elements of alpha will be applied to the colors
-#         in order (cycling through alpha multiple times if necessary).
-
-#     cmap : Matplotlib colormap (default=None)
-#         Colormap for mapping intensities of nodes
-
-#     vmin,vmax : floats or None (default=None)
-#         Minimum and maximum for node colormap scaling
-
-#     linewidths : [None | scalar | sequence] (default=1.0)
-#         Line width of symbol border
-
-#     edgecolors : [None | scalar | sequence] (default = node_color)
-#         Colors of node borders
-
-#     label : [None | string]
-#         Label for legend
-
-#     margins : float or 2-tuple, optional
-#         Sets the padding for axis autoscaling. Increase margin to prevent
-#         clipping for nodes that are near the edges of an image. Values should
-#         be in the range ``[0, 1]``. See :meth:`matplotlib.axes.Axes.margins`
-#         for details. The default is `None`, which uses the Matplotlib default.
-
-#     Returns
-#     -------
-#     matplotlib.collections.PathCollection
-#         `PathCollection` of the nodes.
-
-#     Examples
-#     --------
-#     >>> from easygraph.datasets import get_graph_karateclub
-#     >>> import easygraph as eg
-#     >>> G = get_graph_karateclub()
-#     >>> nodes = eg.draw_easygraph_nodes(G, pos=eg.circular_position(G))
-
-
-#     """
-#     from collections.abc import Iterable
-
-#     import matplotlib as mpl
-#     import matplotlib.collections  # call as mpl.collections
-#     import matplotlib.pyplot as plt
-#     import numpy as np
-
-#     if ax is None:
-#         ax = plt.gca()
-
-#     if nodelist is None:
-#         nodelist = list(G)
-
-#     if len(nodelist) == 0:  # empty nodelist, no drawing
-#         return mpl.collections.PathCollection(None)
-
-#     try:
-#         xy = np.asarray([pos[v] for v in nodelist])
-#     except KeyError as err:
-#         raise EasyGraphError(f"Node {err} has no position.") from err
-
-#     if isinstance(alpha, Iterable):
-#         node_color = apply_alpha(node_color, alpha, nodelist, cmap, vmin, vmax)
-#         alpha = None
-
-#     node_collection = ax.scatter(
-#         xy[:, 0],
-#         xy[:, 1],
-#         s=node_size,
-#         c=node_color,
-#         marker=node_shape,
-#         cmap=cmap,
-#         vmin=vmin,
-#         vmax=vmax,
-#         alpha=alpha,
-#         linewidths=linewidths,
-#         edgecolors=edgecolors,
-#         label=label,
-#     )
-#     ax.tick_params(
-#         axis="both",
-#         which="both",
-#         bottom=False,
-#         left=False,
-#         labelbottom=False,
-#         labelleft=False,
-#     )
-
-#     if margins is not None:
-#         if isinstance(margins, Iterable):
-#             ax.margins(*margins)
-#         else:
-#             ax.margins(margins)
-
-#     node_collection.set_zorder(2)
-#     return node_collection
-
-
-# def draw_curved_edges(G, pos, dist_ratio=0.2, bezier_precision=20, polarity='random'):
-#     # Get nodes into np array
-#     edges = np.array(G.edges())
-#     l = edges.shape[0]
-
-#     if polarity == 'random':
-#         # Random polarity of curve
-#         rnd = np.where(np.random.randint(2, size=l)==0, -1, 1)
-#     else:
-#         # Create a fixed (hashed) polarity column in the case we use fixed polarity
-#         # This is useful, e.g., for animations
-#         rnd = np.where(np.mod(np.vectorize(hash)(edges[:,0])+np.vectorize(hash)(edges[:,1]),2)==0,-1,1)
-
-#     # Coordinates (x,y) of both nodes for each edge
-#     # e.g., https://stackoverflow.com/questions/16992713/translate-every-element-in-numpy-array-according-to-key
-#     # Note the np.vectorize method doesn't work for all node position dictionaries for some reason
-#     u, inv = np.unique(edges, return_inverse = True)
-#     coords = np.array([pos[x] for x in u])[inv].reshape([edges.shape[0], 2, edges.shape[1]])
-#     coords_node1 = coords[:,0,:]
-#     coords_node2 = coords[:,1,:]
-
-#     # Swap node1/node2 allocations to make sure the directionality works correctly
-#     should_swap = coords_node1[:,0] > coords_node2[:,0]
-#     coords_node1[should_swap], coords_node2[should_swap] = coords_node2[should_swap], coords_node1[should_swap]
-
-#     # Distance for control points
-#     dist = dist_ratio * np.sqrt(np.sum((coords_node1-coords_node2)**2, axis=1))
-
-#     # Gradients of line connecting node & perpendicular
-#     m1 = (coords_node2[:,1]-coords_node1[:,1])/(coords_node2[:,0]-coords_node1[:,0])
-#     m2 = -1/m1
-
-#     # Temporary points along the line which connects two nodes
-#     # e.g., https://math.stackexchange.com/questions/656500/given-a-point-slope-and-a-distance-along-that-slope-easily-find-a-second-p
-#     t1 = dist/np.sqrt(1+m1**2)
-#     v1 = np.array([np.ones(l),m1])
-#     coords_node1_displace = coords_node1 + (v1*t1).T
-#     coords_node2_displace = coords_node2 - (v1*t1).T
-
-#     # Control points, same distance but along perpendicular line
-#     # rnd gives the 'polarity' to determine which side of the line the curve should arc
-#     t2 = dist/np.sqrt(1+m2**2)
-#     v2 = np.array([np.ones(len(edges)),m2])
-#     coords_node1_ctrl = coords_node1_displace + (rnd*v2*t2).T
-#     coords_node2_ctrl = coords_node2_displace + (rnd*v2*t2).T
-
-#     # Combine all these four (x,y) columns into a 'node matrix'
-#     node_matrix = np.array([coords_node1, coords_node1_ctrl, coords_node2_ctrl, coords_node2])
-
-#     # Create the Bezier curves and store them in a list
-#     curveplots = []
-#     for i in range(l):
-#         nodes = node_matrix[:,i,:].T
-#         curveplots.append(bezier.Curve(nodes, degree=3).evaluate_multi(np.linspace(0,1,bezier_precision)).T)
-#     # Return an array of these curves
-#     curves = np.array(curveplots)
-#     return curves
-
-# def draw_curved_graph(G, colors, ax):
-#     #G = to_networkx(G)
-#     # layout
-#     pos = eg.spring_layout(G, iterations=50)
-#     eg.draw_networkx_nodes(G, pos, ax=ax, node_size=200, node_color=colors[0], alpha=0.5)
-
-#     # 绘制标签
-#     eg.draw_networkx_labels(G, pos, ax=ax, font_size=8, font_family='Arial', font_color='black')
-
-#     # Produce the curves
-#     curves = draw_curved_edges(G, pos)
-#     lc = LineCollection(curves, color=colors[1], alpha=0.4)
-
-#     # 添加连线
-#     ax.add_collection(lc)
-
-#     # 设置坐标轴参数
-#     ax.tick_params(axis='both', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)
-
-#     plt.savefig('Figure.pdf')
-#     plt.show()
+from itertools import chain
+from typing import List
+from typing import Optional
+from typing import Tuple
+
+import matplotlib
+import matplotlib.pyplot as plt
+import numpy as np
+
+from matplotlib.axes import Axes
+from matplotlib.collections import LineCollection
+from matplotlib.collections import PatchCollection
+from matplotlib.patches import Circle
+from matplotlib.patches import PathPatch
+from matplotlib.path import Path
+from scipy.spatial import ConvexHull
+
+from .geometry import common_tangent_radian
+from .geometry import polar_position
+from .geometry import rad_2_deg
+from .geometry import radian_from_atan
+from .geometry import vlen
+
+
+# from fa2 import ForceAtlas2
+# import bezier
+# import numpy as np
+# from easygraph import to_networkx
+# from easygraph.utils.exception import EasyGraphError
+# import easygraph as eg
+
+
+def safe_div(a: np.ndarray, b: np.ndarray, jitter_scale: float = 0.000001):
+    mask = b == 0
+    b[mask] = 1
+    inv_b = 1.0 / b
+    res = a * inv_b
+    if mask.sum() > 0:
+        res[mask.repeat(2, 2)] = np.random.randn(mask.sum() * 2) * jitter_scale
+    return res
+
+
+def init_pos(num_v: int, center: Tuple[float, float] = (0, 0), scale: float = 1.0):
+    return (np.random.rand(num_v, 2) * 2 - 1) * scale + center
+
+
+def draw_line_edge(
+    ax: Axes,
+    v_coor: np.array,
+    v_size: list,
+    e_list: List[Tuple[int, int]],
+    show_arrow: bool,
+    e_color: list,
+    e_line_width: list,
+):
+    arrow_head_width = (
+        [0.015 * w for w in e_line_width] if show_arrow else [0] * len(e_list)
+    )
+
+    for eidx, e in enumerate(e_list):
+        start_pos = v_coor[e[0]]
+        end_pos = v_coor[e[1]]
+
+        dir = end_pos - start_pos
+        dir = dir / np.linalg.norm(dir)
+
+        start_pos = start_pos + dir * v_size[e[0]]
+        end_pos = end_pos - dir * v_size[e[1]]
+
+        x, y = start_pos[0], start_pos[1]
+        dx, dy = end_pos[0] - x, end_pos[1] - y
+
+        ax.arrow(
+            x,
+            y,
+            dx,
+            dy,
+            head_width=arrow_head_width[eidx],
+            color=e_color[eidx],
+            linewidth=e_line_width[eidx],
+            length_includes_head=True,
+        )
+
+
+def draw_circle_edge(
+    ax: Axes,
+    v_coor: List[Tuple[float, float]],
+    v_size: list,
+    e_list: List[Tuple[int, int]],
+    e_color: list,
+    e_fill_color: list,
+    e_line_width: list,
+):
+    n_v = len(v_coor)
+    line_paths, arc_paths, vertices = hull_layout(n_v, e_list, v_coor, v_size)
+    for eidx, lines in enumerate(line_paths):
+        pathdata = []
+        for line in lines:
+            if len(line) == 0:
+                continue
+            start_pos, end_pos = line
+            pathdata.append((Path.MOVETO, start_pos.tolist()))
+            pathdata.append((Path.LINETO, end_pos.tolist()))
+
+        if len(list(zip(*pathdata))) == 0:
+            continue
+        codes, verts = zip(*pathdata)
+        path = Path(verts, codes)
+
+        ax.add_patch(
+            PathPatch(
+                path,
+                linewidth=e_line_width[eidx],
+                facecolor=e_fill_color[eidx],
+                edgecolor=e_color[eidx],
+            )
+        )
+
+    for eidx, arcs in enumerate(arc_paths):
+        for arc in arcs:
+            center, theta1, theta2, radius = arc
+            x, y = center[0], center[1]
+
+            patcjes_arc = matplotlib.patches.Arc(
+                (x, y),
+                2 * radius,
+                2 * radius,
+                theta1=theta1,
+                theta2=theta2,
+                color=e_color[eidx],
+                linewidth=e_line_width[eidx],
+                # edgecolor=e_color[eidx],
+                edgecolor=e_color[eidx],
+                facecolor=e_fill_color[eidx],
+            )
+
+            ax.add_patch(
+                matplotlib.patches.Arc(
+                    (x, y),
+                    2 * radius,
+                    2 * radius,
+                    theta1=theta1,
+                    theta2=theta2,
+                    color=e_color[eidx],
+                    linewidth=e_line_width[eidx],
+                    # edgecolor=e_color[eidx],
+                    edgecolor=e_color[eidx],
+                    facecolor=e_fill_color[eidx],
+                )
+            )
+
+
+def edge_list_to_incidence_matrix(num_v: int, e_list: List[tuple]) -> np.ndarray:
+    v_idx = list(chain(*e_list))
+    e_idx = [[idx] * len(e) for idx, e in enumerate(e_list)]
+    e_idx = list(chain(*e_idx))
+    H = np.zeros((num_v, len(e_list)))
+    H[v_idx, e_idx] = 1
+    return H
+
+
+def draw_vertex(
+    ax: Axes,
+    v_coor: List[Tuple[float, float]],
+    v_label: Optional[List[str]],
+    font_size: int,
+    font_family: str,
+    v_size: list,
+    v_color: list,
+    edgecolors,
+    v_line_width: list,
+):
+    patches = []
+    n = v_coor.shape[0]
+    if v_label is None:
+        v_label = [""] * n
+    for coor, label, size, width in zip(v_coor.tolist(), v_label, v_size, v_line_width):
+        circle = Circle(coor, size)
+        circle.lineWidth = width
+        # circle.label = label
+        if label != "":
+            x, y = coor[0], coor[1]
+            offset = 0, -1.3 * size
+            x += offset[0]
+            y += offset[1]
+            ax.text(
+                x,
+                y,
+                label,
+                fontsize=font_size,
+                fontfamily=font_family,
+                ha="center",
+                va="top",
+            )
+        patches.append(circle)
+    edgecolors = "black" if edgecolors == None else edgecolors
+    p = PatchCollection(patches, facecolors=v_color, edgecolors=edgecolors)
+    ax.add_collection(p)
+
+
+def hull_layout(n_v, e_list, pos, v_size, radius_increment=0.3):
+    line_paths = [None] * len(e_list)
+    arc_paths = [None] * len(e_list)
+
+    polygons_vertices_index = []
+    vertices_radius = np.array(v_size)
+    vertices_increased_radius = vertices_radius * radius_increment
+    vertices_radius += vertices_increased_radius
+
+    e_degree = [len(e) for e in e_list]
+    e_idxs = np.argsort(np.array(e_degree))
+
+    # for edge in e_list:
+    for e_idx in e_idxs:
+        edge = list(e_list[e_idx])
+
+        line_path_for_e = []
+        arc_path_for_e = []
+
+        if len(edge) == 1:
+            arc_path_for_e.append([pos[edge[0]], 0, 360, vertices_radius[edge[0]]])
+
+            vertices_radius[edge] += vertices_increased_radius[edge]
+
+            line_paths[e_idx] = line_path_for_e
+            arc_paths[e_idx] = arc_path_for_e
+            continue
+
+        pos_in_edge = pos[edge]
+        if len(edge) == 2:
+            vertices_index = np.array((0, 1), dtype=np.int64)
+        else:
+            hull = ConvexHull(pos_in_edge)
+            vertices_index = hull.vertices
+
+        n_vertices = vertices_index.shape[0]
+
+        vertices_index = np.append(vertices_index, vertices_index[0])  # close the loop
+
+        thetas = []
+
+        for i in range(n_vertices):
+            # line
+            i1 = edge[vertices_index[i]]
+            i2 = edge[vertices_index[i + 1]]
+
+            r1 = vertices_radius[i1]
+            r2 = vertices_radius[i2]
+
+            p1 = pos[i1]
+            p2 = pos[i2]
+
+            dp = p2 - p1
+            dp_len = vlen(dp)
+
+            beta = radian_from_atan(dp[0], dp[1])
+            alpha = common_tangent_radian(r1, r2, dp_len)
+
+            theta = beta - alpha
+            start_point = polar_position(r1, theta, p1)
+            end_point = polar_position(r2, theta, p2)
+
+            line_path_for_e.append((start_point, end_point))
+            thetas.append(theta)
+
+        for i in range(n_vertices):
+            # arcs
+            theta_1 = thetas[i - 1]
+            theta_2 = thetas[i]
+
+            arc_center = pos[edge[vertices_index[i]]]
+            radius = vertices_radius[edge[vertices_index[i]]]
+
+            theta_1, theta_2 = rad_2_deg(theta_1), rad_2_deg(theta_2)
+            arc_path_for_e.append((arc_center, theta_1, theta_2, radius))
+
+        vertices_radius[edge] += vertices_increased_radius[edge]
+
+        polygons_vertices_index.append(vertices_index.copy())
+
+        # line_paths.append(line_path_for_e)
+        # arc_paths.append(arc_path_for_e)
+        line_paths[e_idx] = line_path_for_e
+        arc_paths[e_idx] = arc_path_for_e
+
+    return line_paths, arc_paths, polygons_vertices_index
+
+
+def apply_alpha(colors, alpha, elem_list, cmap=None, vmin=None, vmax=None):
+    """Apply an alpha (or list of alphas) to the colors provided.
+
+    Parameters
+    ----------
+
+    colors : color string or array of floats (default='r')
+        Color of element. Can be a single color format string,
+        or a sequence of colors with the same length as nodelist.
+        If numeric values are specified they will be mapped to
+        colors using the cmap and vmin,vmax parameters.  See
+        matplotlib.scatter for more details.
+
+    alpha : float or array of floats
+        Alpha values for elements. This can be a single alpha value, in
+        which case it will be applied to all the elements of color. Otherwise,
+        if it is an array, the elements of alpha will be applied to the colors
+        in order (cycling through alpha multiple times if necessary).
+
+    elem_list : array of networkx objects
+        The list of elements which are being colored. These could be nodes,
+        edges or labels.
+
+    cmap : matplotlib colormap
+        Color map for use if colors is a list of floats corresponding to points
+        on a color mapping.
+
+    vmin, vmax : float
+        Minimum and maximum values for normalizing colors if a colormap is used
+
+    Returns
+    -------
+
+    rgba_colors : numpy ndarray
+        Array containing RGBA format values for each of the node colours.
+
+    """
+    from itertools import cycle
+    from itertools import islice
+    from numbers import Number
+
+    import matplotlib as mpl
+    import matplotlib.cm  # call as mpl.cm
+    import matplotlib.colors  # call as mpl.colors
+    import numpy as np
+
+    # If we have been provided with a list of numbers as long as elem_list,
+    # apply the color mapping.
+    if len(colors) == len(elem_list) and isinstance(colors[0], Number):
+        mapper = mpl.cm.ScalarMappable(cmap=cmap)
+        mapper.set_clim(vmin, vmax)
+        rgba_colors = mapper.to_rgba(colors)
+    # Otherwise, convert colors to matplotlib's RGB using the colorConverter
+    # object.  These are converted to numpy ndarrays to be consistent with the
+    # to_rgba method of ScalarMappable.
+    else:
+        try:
+            rgba_colors = np.array([mpl.colors.colorConverter.to_rgba(colors)])
+        except ValueError:
+            rgba_colors = np.array(
+                [mpl.colors.colorConverter.to_rgba(color) for color in colors]
+            )
+    # Set the final column of the rgba_colors to have the relevant alpha values
+    try:
+        # If alpha is longer than the number of colors, resize to the number of
+        # elements.  Also, if rgba_colors.size (the number of elements of
+        # rgba_colors) is the same as the number of elements, resize the array,
+        # to avoid it being interpreted as a colormap by scatter()
+        if len(alpha) > len(rgba_colors) or rgba_colors.size == len(elem_list):
+            rgba_colors = np.resize(rgba_colors, (len(elem_list), 4))
+            rgba_colors[1:, 0] = rgba_colors[0, 0]
+            rgba_colors[1:, 1] = rgba_colors[0, 1]
+            rgba_colors[1:, 2] = rgba_colors[0, 2]
+        rgba_colors[:, 3] = list(islice(cycle(alpha), len(rgba_colors)))
+    except TypeError:
+        rgba_colors[:, -1] = alpha
+    return rgba_colors
+
+
+# def draw_easygraph_nodes(
+#     G,
+#     pos,
+#     nodelist=None,
+#     node_size=300,
+#     node_color="#1f78b4",
+#     node_shape="o",
+#     alpha=None,
+#     cmap=None,
+#     vmin=None,
+#     vmax=None,
+#     ax=None,
+#     linewidths=None,
+#     edgecolors=None,
+#     label=None,
+#     margins=None,
+# ):
+#     """Draw the nodes of the graph G.
+
+#     This draws only the nodes of the graph G.
+
+#     Parameters
+#     ----------
+#     G : graph
+#         A easygraph graph
+
+#     pos : dictionary
+#         A dictionary with nodes as keys and positions as values.
+#         Positions should be sequences of length 2.
+
+#     ax : Matplotlib Axes object, optional
+#         Draw the graph in the specified Matplotlib axes.
+
+#     nodelist : list (default list(G))
+#         Draw only specified nodes
+
+#     node_size : scalar or array (default=300)
+#         Size of nodes.  If an array it must be the same length as nodelist.
+
+#     node_color : color or array of colors (default='#1f78b4')
+#         Node color. Can be a single color or a sequence of colors with the same
+#         length as nodelist. Color can be string or rgb (or rgba) tuple of
+#         floats from 0-1. If numeric values are specified they will be
+#         mapped to colors using the cmap and vmin,vmax parameters. See
+#         matplotlib.scatter for more details.
+
+#     node_shape :  string (default='o')
+#         The shape of the node.  Specification is as matplotlib.scatter
+#         marker, one of 'so^>v<dph8'.
+
+#     alpha : float or array of floats (default=None)
+#         The node transparency.  This can be a single alpha value,
+#         in which case it will be applied to all the nodes of color. Otherwise,
+#         if it is an array, the elements of alpha will be applied to the colors
+#         in order (cycling through alpha multiple times if necessary).
+
+#     cmap : Matplotlib colormap (default=None)
+#         Colormap for mapping intensities of nodes
+
+#     vmin,vmax : floats or None (default=None)
+#         Minimum and maximum for node colormap scaling
+
+#     linewidths : [None | scalar | sequence] (default=1.0)
+#         Line width of symbol border
+
+#     edgecolors : [None | scalar | sequence] (default = node_color)
+#         Colors of node borders
+
+#     label : [None | string]
+#         Label for legend
+
+#     margins : float or 2-tuple, optional
+#         Sets the padding for axis autoscaling. Increase margin to prevent
+#         clipping for nodes that are near the edges of an image. Values should
+#         be in the range ``[0, 1]``. See :meth:`matplotlib.axes.Axes.margins`
+#         for details. The default is `None`, which uses the Matplotlib default.
+
+#     Returns
+#     -------
+#     matplotlib.collections.PathCollection
+#         `PathCollection` of the nodes.
+
+#     Examples
+#     --------
+#     >>> from easygraph.datasets import get_graph_karateclub
+#     >>> import easygraph as eg
+#     >>> G = get_graph_karateclub()
+#     >>> nodes = eg.draw_easygraph_nodes(G, pos=eg.circular_position(G))
+
+
+#     """
+#     from collections.abc import Iterable
+
+#     import matplotlib as mpl
+#     import matplotlib.collections  # call as mpl.collections
+#     import matplotlib.pyplot as plt
+#     import numpy as np
+
+#     if ax is None:
+#         ax = plt.gca()
+
+#     if nodelist is None:
+#         nodelist = list(G)
+
+#     if len(nodelist) == 0:  # empty nodelist, no drawing
+#         return mpl.collections.PathCollection(None)
+
+#     try:
+#         xy = np.asarray([pos[v] for v in nodelist])
+#     except KeyError as err:
+#         raise EasyGraphError(f"Node {err} has no position.") from err
+
+#     if isinstance(alpha, Iterable):
+#         node_color = apply_alpha(node_color, alpha, nodelist, cmap, vmin, vmax)
+#         alpha = None
+
+#     node_collection = ax.scatter(
+#         xy[:, 0],
+#         xy[:, 1],
+#         s=node_size,
+#         c=node_color,
+#         marker=node_shape,
+#         cmap=cmap,
+#         vmin=vmin,
+#         vmax=vmax,
+#         alpha=alpha,
+#         linewidths=linewidths,
+#         edgecolors=edgecolors,
+#         label=label,
+#     )
+#     ax.tick_params(
+#         axis="both",
+#         which="both",
+#         bottom=False,
+#         left=False,
+#         labelbottom=False,
+#         labelleft=False,
+#     )
+
+#     if margins is not None:
+#         if isinstance(margins, Iterable):
+#             ax.margins(*margins)
+#         else:
+#             ax.margins(margins)
+
+#     node_collection.set_zorder(2)
+#     return node_collection
+
+
+# def draw_curved_edges(G, pos, dist_ratio=0.2, bezier_precision=20, polarity='random'):
+#     # Get nodes into np array
+#     edges = np.array(G.edges())
+#     l = edges.shape[0]
+
+#     if polarity == 'random':
+#         # Random polarity of curve
+#         rnd = np.where(np.random.randint(2, size=l)==0, -1, 1)
+#     else:
+#         # Create a fixed (hashed) polarity column in the case we use fixed polarity
+#         # This is useful, e.g., for animations
+#         rnd = np.where(np.mod(np.vectorize(hash)(edges[:,0])+np.vectorize(hash)(edges[:,1]),2)==0,-1,1)
+
+#     # Coordinates (x,y) of both nodes for each edge
+#     # e.g., https://stackoverflow.com/questions/16992713/translate-every-element-in-numpy-array-according-to-key
+#     # Note the np.vectorize method doesn't work for all node position dictionaries for some reason
+#     u, inv = np.unique(edges, return_inverse = True)
+#     coords = np.array([pos[x] for x in u])[inv].reshape([edges.shape[0], 2, edges.shape[1]])
+#     coords_node1 = coords[:,0,:]
+#     coords_node2 = coords[:,1,:]
+
+#     # Swap node1/node2 allocations to make sure the directionality works correctly
+#     should_swap = coords_node1[:,0] > coords_node2[:,0]
+#     coords_node1[should_swap], coords_node2[should_swap] = coords_node2[should_swap], coords_node1[should_swap]
+
+#     # Distance for control points
+#     dist = dist_ratio * np.sqrt(np.sum((coords_node1-coords_node2)**2, axis=1))
+
+#     # Gradients of line connecting node & perpendicular
+#     m1 = (coords_node2[:,1]-coords_node1[:,1])/(coords_node2[:,0]-coords_node1[:,0])
+#     m2 = -1/m1
+
+#     # Temporary points along the line which connects two nodes
+#     # e.g., https://math.stackexchange.com/questions/656500/given-a-point-slope-and-a-distance-along-that-slope-easily-find-a-second-p
+#     t1 = dist/np.sqrt(1+m1**2)
+#     v1 = np.array([np.ones(l),m1])
+#     coords_node1_displace = coords_node1 + (v1*t1).T
+#     coords_node2_displace = coords_node2 - (v1*t1).T
+
+#     # Control points, same distance but along perpendicular line
+#     # rnd gives the 'polarity' to determine which side of the line the curve should arc
+#     t2 = dist/np.sqrt(1+m2**2)
+#     v2 = np.array([np.ones(len(edges)),m2])
+#     coords_node1_ctrl = coords_node1_displace + (rnd*v2*t2).T
+#     coords_node2_ctrl = coords_node2_displace + (rnd*v2*t2).T
+
+#     # Combine all these four (x,y) columns into a 'node matrix'
+#     node_matrix = np.array([coords_node1, coords_node1_ctrl, coords_node2_ctrl, coords_node2])
+
+#     # Create the Bezier curves and store them in a list
+#     curveplots = []
+#     for i in range(l):
+#         nodes = node_matrix[:,i,:].T
+#         curveplots.append(bezier.Curve(nodes, degree=3).evaluate_multi(np.linspace(0,1,bezier_precision)).T)
+#     # Return an array of these curves
+#     curves = np.array(curveplots)
+#     return curves
+
+# def draw_curved_graph(G, colors, ax):
+#     #G = to_networkx(G)
+#     # layout
+#     pos = eg.spring_layout(G, iterations=50)
+#     eg.draw_networkx_nodes(G, pos, ax=ax, node_size=200, node_color=colors[0], alpha=0.5)
+
+#     # 绘制标签
+#     eg.draw_networkx_labels(G, pos, ax=ax, font_size=8, font_family='Arial', font_color='black')
+
+#     # Produce the curves
+#     curves = draw_curved_edges(G, pos)
+#     lc = LineCollection(curves, color=colors[1], alpha=0.4)
+
+#     # 添加连线
+#     ax.add_collection(lc)
+
+#     # 设置坐标轴参数
+#     ax.tick_params(axis='both', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)
+
+#     plt.savefig('Figure.pdf')
+#     plt.show()
```

## easygraph/functions/drawing/plot.py

 * *Ordering differences only*

```diff
@@ -1,229 +1,229 @@
-import easygraph as eg
-
-
-__all__ = [
-    "plot_Followers",
-    "plot_Connected_Communities",
-    "plot_Betweenness_Centrality",
-    "plot_Neighborhood_Followers",
-]
-
-
-# Number of Followers
-def plot_Followers(G, SHS):
-    """
-    Returns the CDF curves of "Number of Followers" of SH spanners and ordinary users in graph G.
-
-    Parameters
-    ----------
-    G : graph
-        A easygraph graph.
-
-    SHS : list
-        The SH Spanners in graph G.
-
-    Returns
-    -------
-    plt : CDF curves
-        the CDF curves of "Number of Followers" of SH spanners and ordinary users in graph G.
-    """
-    import matplotlib.pyplot as plt
-    import numpy as np
-    import statsmodels.api as sm
-
-    OU = []
-    for i in G:
-        if i not in SHS:
-            OU.append(i)
-    degree = G.degree()
-    sample1 = []
-    sample2 = []
-    for i in degree.keys():
-        if i in OU:
-            sample1.append(degree[i])
-        elif i in SHS:
-            sample2.append(degree[i])
-    X1 = np.linspace(min(sample1), max(sample1))
-    ecdf = sm.distributions.ECDF(sample1)
-    Y1 = ecdf(X1)
-    X2 = np.linspace(min(sample2), max(sample2))
-    ecdf = sm.distributions.ECDF(sample2)
-    Y2 = ecdf(X2)
-    plt.plot(X1, Y1, "b--", label="Ordinary User")
-    plt.plot(X2, Y2, "r", label="SH Spanner")
-    plt.title("Number of Followers")
-    plt.xlabel("Number of Followers")
-    plt.ylabel("Cumulative Distribution Function")
-    plt.legend(loc="lower right")
-    plt.show()
-
-
-# Number of Connected Communities
-def plot_Connected_Communities(G, SHS):
-    """
-    Returns the CDF curves of "Number of Connected Communities" of SH spanners and ordinary users in graph G.
-
-    Parameters
-    ----------
-    G : graph
-        A easygraph graph.
-
-    SHS : list
-        The SH Spanners in graph G.
-
-    Returns
-    -------
-    plt : CDF curves
-        the CDF curves of "Number of Connected Communities" of SH spanners and ordinary users in graph G.
-    """
-    import matplotlib.pyplot as plt
-    import numpy as np
-    import statsmodels.api as sm
-
-    OU = []
-    for i in G:
-        if i not in SHS:
-            OU.append(i)
-    sample1 = []
-    sample2 = []
-    cmts = eg.LPA(G)
-    for i in OU:
-        s = set()
-        neighbors = G.neighbors(node=i)
-        for j in neighbors:
-            for k in cmts:
-                if j in cmts[k]:
-                    s.add(k)
-        sample1.append(len(s))
-    for i in SHS:
-        s = set()
-        neighbors = G.neighbors(node=i)
-        for j in neighbors:
-            for k in cmts:
-                if j in cmts[k]:
-                    s.add(k)
-        sample2.append(len(s))
-    print(len(cmts))
-    print(sample1)
-    print(sample2)
-    X1 = np.linspace(min(sample1), max(sample1))
-    ecdf = sm.distributions.ECDF(sample1)
-    Y1 = ecdf(X1)
-    X2 = np.linspace(min(sample2), max(sample2))
-    ecdf = sm.distributions.ECDF(sample2)
-    Y2 = ecdf(X2)
-    plt.plot(X1, Y1, "b--", label="Ordinary User")
-    plt.plot(X2, Y2, "r", label="SH Spanner")
-    plt.title("Number of Connected Communities")
-    plt.xlabel("Number of Connected Communities")
-    plt.ylabel("Cumulative Distribution Function")
-    plt.legend(loc="lower right")
-    plt.show()
-
-
-# Betweenness Centrality
-def plot_Betweenness_Centrality(G, SHS):
-    """
-    Returns the CDF curves of "Betweenness Centralitys" of SH spanners and ordinary users in graph G.
-
-    Parameters
-    ----------
-    G : graph
-        A easygraph graph.
-
-    SHS : list
-        The SH Spanners in graph G.
-
-    Returns
-    -------
-    plt : CDF curves
-        the CDF curves of "Betweenness Centrality" of SH spanners and ordinary users in graph G.
-    """
-    import matplotlib.pyplot as plt
-    import numpy as np
-    import statsmodels.api as sm
-
-    OU = []
-    for i in G:
-        if i not in SHS:
-            OU.append(i)
-    bc = eg.betweenness_centrality(G)
-    sample1 = []
-    sample2 = []
-    for i in bc.keys():
-        if i in OU:
-            sample1.append(bc[i])
-        elif i in SHS:
-            sample2.append(bc[i])
-    X1 = np.linspace(min(sample1), max(sample1))
-    ecdf = sm.distributions.ECDF(sample1)
-    Y1 = ecdf(X1)
-    X2 = np.linspace(min(sample2), max(sample2))
-    ecdf = sm.distributions.ECDF(sample2)
-    Y2 = ecdf(X2)
-    plt.plot(X1, Y1, "b--", label="Ordinary User")
-    plt.plot(X2, Y2, "r", label="SH Spanner")
-    plt.title("Betweenness Centrality")
-    plt.xlabel("Betweenness Centrality")
-    plt.ylabel("Cumulative Distribution Function")
-    plt.legend(loc="lower right")
-    plt.show()
-
-
-# Arg. Number of Followers of the Neighborhood Users
-def plot_Neighborhood_Followers(G, SHS):
-    """
-    Returns the CDF curves of "Arg. Number of Followers of the Neighborhood Users" of SH spanners and ordinary users in graph G.
-
-    Parameters
-    ----------
-    G : graph
-        A easygraph graph.
-
-    SHS : list
-        The SH Spanners in graph G.
-
-    Returns
-    -------
-    plt : CDF curves
-        the CDF curves of "Arg. Number of Followers of the Neighborhood Users
-        " of SH spanners and ordinary users in graph G.
-    """
-    import matplotlib.pyplot as plt
-    import numpy as np
-    import statsmodels.api as sm
-
-    OU = []
-    for i in G:
-        if i not in SHS:
-            OU.append(i)
-    sample1 = []
-    sample2 = []
-    degree = G.degree()
-    for i in OU:
-        num = 0
-        sum = 0
-        for neighbor in G.neighbors(node=i):
-            num = num + 1
-            sum = sum + degree[neighbor]
-        sample1.append(sum / num)
-    for i in SHS:
-        num = 0
-        sum = 0
-        for neighbor in G.neighbors(node=i):
-            num = num + 1
-            sum = sum + degree[neighbor]
-        sample2.append(sum / num)
-    X1 = np.linspace(min(sample1), max(sample1))
-    ecdf = sm.distributions.ECDF(sample1)
-    Y1 = ecdf(X1)
-    X2 = np.linspace(min(sample2), max(sample2))
-    ecdf = sm.distributions.ECDF(sample2)
-    Y2 = ecdf(X2)
-    plt.plot(X1, Y1, "b--", label="Ordinary User")
-    plt.plot(X2, Y2, "r", label="SH Spanner")
-    plt.title("Arg. Number of Followers of the Neighborhood Users")
-    plt.xlabel("Arg. Number of Followers of the Neighborhood Users")
-    plt.ylabel("Cumulative Distribution Function")
-    plt.legend(loc="lower right")
-    plt.show()
+import easygraph as eg
+
+
+__all__ = [
+    "plot_Followers",
+    "plot_Connected_Communities",
+    "plot_Betweenness_Centrality",
+    "plot_Neighborhood_Followers",
+]
+
+
+# Number of Followers
+def plot_Followers(G, SHS):
+    """
+    Returns the CDF curves of "Number of Followers" of SH spanners and ordinary users in graph G.
+
+    Parameters
+    ----------
+    G : graph
+        A easygraph graph.
+
+    SHS : list
+        The SH Spanners in graph G.
+
+    Returns
+    -------
+    plt : CDF curves
+        the CDF curves of "Number of Followers" of SH spanners and ordinary users in graph G.
+    """
+    import matplotlib.pyplot as plt
+    import numpy as np
+    import statsmodels.api as sm
+
+    OU = []
+    for i in G:
+        if i not in SHS:
+            OU.append(i)
+    degree = G.degree()
+    sample1 = []
+    sample2 = []
+    for i in degree.keys():
+        if i in OU:
+            sample1.append(degree[i])
+        elif i in SHS:
+            sample2.append(degree[i])
+    X1 = np.linspace(min(sample1), max(sample1))
+    ecdf = sm.distributions.ECDF(sample1)
+    Y1 = ecdf(X1)
+    X2 = np.linspace(min(sample2), max(sample2))
+    ecdf = sm.distributions.ECDF(sample2)
+    Y2 = ecdf(X2)
+    plt.plot(X1, Y1, "b--", label="Ordinary User")
+    plt.plot(X2, Y2, "r", label="SH Spanner")
+    plt.title("Number of Followers")
+    plt.xlabel("Number of Followers")
+    plt.ylabel("Cumulative Distribution Function")
+    plt.legend(loc="lower right")
+    plt.show()
+
+
+# Number of Connected Communities
+def plot_Connected_Communities(G, SHS):
+    """
+    Returns the CDF curves of "Number of Connected Communities" of SH spanners and ordinary users in graph G.
+
+    Parameters
+    ----------
+    G : graph
+        A easygraph graph.
+
+    SHS : list
+        The SH Spanners in graph G.
+
+    Returns
+    -------
+    plt : CDF curves
+        the CDF curves of "Number of Connected Communities" of SH spanners and ordinary users in graph G.
+    """
+    import matplotlib.pyplot as plt
+    import numpy as np
+    import statsmodels.api as sm
+
+    OU = []
+    for i in G:
+        if i not in SHS:
+            OU.append(i)
+    sample1 = []
+    sample2 = []
+    cmts = eg.LPA(G)
+    for i in OU:
+        s = set()
+        neighbors = G.neighbors(node=i)
+        for j in neighbors:
+            for k in cmts:
+                if j in cmts[k]:
+                    s.add(k)
+        sample1.append(len(s))
+    for i in SHS:
+        s = set()
+        neighbors = G.neighbors(node=i)
+        for j in neighbors:
+            for k in cmts:
+                if j in cmts[k]:
+                    s.add(k)
+        sample2.append(len(s))
+    print(len(cmts))
+    print(sample1)
+    print(sample2)
+    X1 = np.linspace(min(sample1), max(sample1))
+    ecdf = sm.distributions.ECDF(sample1)
+    Y1 = ecdf(X1)
+    X2 = np.linspace(min(sample2), max(sample2))
+    ecdf = sm.distributions.ECDF(sample2)
+    Y2 = ecdf(X2)
+    plt.plot(X1, Y1, "b--", label="Ordinary User")
+    plt.plot(X2, Y2, "r", label="SH Spanner")
+    plt.title("Number of Connected Communities")
+    plt.xlabel("Number of Connected Communities")
+    plt.ylabel("Cumulative Distribution Function")
+    plt.legend(loc="lower right")
+    plt.show()
+
+
+# Betweenness Centrality
+def plot_Betweenness_Centrality(G, SHS):
+    """
+    Returns the CDF curves of "Betweenness Centralitys" of SH spanners and ordinary users in graph G.
+
+    Parameters
+    ----------
+    G : graph
+        A easygraph graph.
+
+    SHS : list
+        The SH Spanners in graph G.
+
+    Returns
+    -------
+    plt : CDF curves
+        the CDF curves of "Betweenness Centrality" of SH spanners and ordinary users in graph G.
+    """
+    import matplotlib.pyplot as plt
+    import numpy as np
+    import statsmodels.api as sm
+
+    OU = []
+    for i in G:
+        if i not in SHS:
+            OU.append(i)
+    bc = eg.betweenness_centrality(G)
+    sample1 = []
+    sample2 = []
+    for i in bc.keys():
+        if i in OU:
+            sample1.append(bc[i])
+        elif i in SHS:
+            sample2.append(bc[i])
+    X1 = np.linspace(min(sample1), max(sample1))
+    ecdf = sm.distributions.ECDF(sample1)
+    Y1 = ecdf(X1)
+    X2 = np.linspace(min(sample2), max(sample2))
+    ecdf = sm.distributions.ECDF(sample2)
+    Y2 = ecdf(X2)
+    plt.plot(X1, Y1, "b--", label="Ordinary User")
+    plt.plot(X2, Y2, "r", label="SH Spanner")
+    plt.title("Betweenness Centrality")
+    plt.xlabel("Betweenness Centrality")
+    plt.ylabel("Cumulative Distribution Function")
+    plt.legend(loc="lower right")
+    plt.show()
+
+
+# Arg. Number of Followers of the Neighborhood Users
+def plot_Neighborhood_Followers(G, SHS):
+    """
+    Returns the CDF curves of "Arg. Number of Followers of the Neighborhood Users" of SH spanners and ordinary users in graph G.
+
+    Parameters
+    ----------
+    G : graph
+        A easygraph graph.
+
+    SHS : list
+        The SH Spanners in graph G.
+
+    Returns
+    -------
+    plt : CDF curves
+        the CDF curves of "Arg. Number of Followers of the Neighborhood Users
+        " of SH spanners and ordinary users in graph G.
+    """
+    import matplotlib.pyplot as plt
+    import numpy as np
+    import statsmodels.api as sm
+
+    OU = []
+    for i in G:
+        if i not in SHS:
+            OU.append(i)
+    sample1 = []
+    sample2 = []
+    degree = G.degree()
+    for i in OU:
+        num = 0
+        sum = 0
+        for neighbor in G.neighbors(node=i):
+            num = num + 1
+            sum = sum + degree[neighbor]
+        sample1.append(sum / num)
+    for i in SHS:
+        num = 0
+        sum = 0
+        for neighbor in G.neighbors(node=i):
+            num = num + 1
+            sum = sum + degree[neighbor]
+        sample2.append(sum / num)
+    X1 = np.linspace(min(sample1), max(sample1))
+    ecdf = sm.distributions.ECDF(sample1)
+    Y1 = ecdf(X1)
+    X2 = np.linspace(min(sample2), max(sample2))
+    ecdf = sm.distributions.ECDF(sample2)
+    Y2 = ecdf(X2)
+    plt.plot(X1, Y1, "b--", label="Ordinary User")
+    plt.plot(X2, Y2, "r", label="SH Spanner")
+    plt.title("Arg. Number of Followers of the Neighborhood Users")
+    plt.xlabel("Arg. Number of Followers of the Neighborhood Users")
+    plt.ylabel("Cumulative Distribution Function")
+    plt.legend(loc="lower right")
+    plt.show()
```

## easygraph/functions/drawing/simulator.py

```diff
@@ -1,197 +1,195 @@
-from copy import deepcopy
-
-from .utils import safe_div
-
-
-class Simulator:
-    NODE_ATTRACTION = 0
-    NODE_REPULSION = 1
-    EDGE_REPULSION = 2
-    CENTER_GRAVITY = 3
-
-    def __init__(self, nums, forces, centers=1, damping_factor=0.999) -> None:
-        self.nums = [nums] if isinstance(nums, int) else nums
-
-        self.node_attraction = forces.get(self.NODE_ATTRACTION, None)
-        self.node_repulsion = forces.get(self.NODE_REPULSION, None)
-        self.edge_repulsion = forces.get(self.EDGE_REPULSION, None)
-        self.center_gravity = forces.get(self.CENTER_GRAVITY, None)
-
-        self.n_centers = len(centers)
-        self.centers = centers
-
-        if self.node_repulsion is not None and isinstance(self.node_repulsion, float):
-            self.node_repulsion = [self.node_repulsion] * self.n_centers
-        if self.center_gravity is not None and isinstance(self.center_gravity, float):
-            self.center_gravity = [self.center_gravity] * self.n_centers
-
-        self.damping_factor = damping_factor
-
-    def simulate(self, init_position, H, max_iter=400, epsilon=0.001, dt=2.0) -> None:
-        import numpy as np
-
-        """
-        Simulate the force-directed layout algorithm.
-        """
-        position = init_position.copy()
-        velocity = np.zeros_like(position)
-        damping = 1.0
-        for it in range(max_iter):
-            position, velocity, stop = self._step(
-                position, velocity, H, epsilon, damping, dt
-            )
-            # np.save("./tmp/position_{}.npy".format(it), position)
-            # np.save("./tmp/velocity_{}.npy".format(it), velocity)
-            if stop:
-                break
-            damping *= self.damping_factor
-        return position
-
-    def _step(self, position, velocity, H, epsilon, damping, dt):
-        import numpy as np
-
-        from sklearn.metrics import euclidean_distances
-
-        """
-        One step of the simulation.
-        """
-        v2v_dist = euclidean_distances(position)
-        e_center = np.matmul(H.T, position) / H.sum(axis=0).reshape(-1, 1)
-        v2e_dist = euclidean_distances(position, e_center) * H
-        e2e_dist = euclidean_distances(e_center)
-
-        centers = self.centers
-
-        force = np.zeros_like(position)
-        if self.node_attraction is not None:
-            f = (
-                self._node_attraction(position, e_center, v2e_dist)
-                * self.node_attraction
-            )
-            assert np.isnan(f).sum() == 0
-            force += f
-        if self.node_repulsion is not None:
-            f = self._node_repulsion(position, v2v_dist)
-            if self.n_centers == 1:
-                f *= self.node_repulsion[0]
-            else:
-                masks = np.zeros((position.shape[0], 1))
-                masks[: self.nums[0]] = self.node_repulsion[0]
-                masks[self.nums[0] :] = self.node_repulsion[1]
-                f *= masks
-            assert np.isnan(f).sum() == 0
-            force += f
-        if self.edge_repulsion is not None:
-            f = self._edge_repulsion(e_center, H, e2e_dist) * self.edge_repulsion
-            assert np.isnan(f).sum() == 0
-            force += f
-        if self.center_gravity is not None:
-            masks = [np.zeros((position.shape[0], 1)), np.zeros((position.shape[0], 1))]
-            masks[0][: self.nums[0]] = 1
-            masks[1][self.nums[0] :] = 1
-            for center, gravity, mask in zip(centers, self.center_gravity, masks):
-                v2c_dist = euclidean_distances(position, center.reshape(1, -1)).reshape(
-                    -1, 1
-                )
-                f = self._center_gravity(position, center, v2c_dist) * gravity * mask
-                assert np.isnan(f).sum() == 0
-                force += f
-
-        force *= damping
-
-        force = np.clip(force, -0.1, 0.1)
-        position += force * dt
-        velocity = force
-
-        return position, velocity, self._stop_condition(velocity, epsilon)
-
-    def _node_attraction(self, position, e_center, v2e_dist, x0=0.1, k=1.0):
-        import numpy as np
-
-        """
-        Node attracted by edge center.
-        """
-        x = deepcopy(v2e_dist)
-        x[v2e_dist > 0] -= x0
-        f_scale = k * x  # (n, m)
-        f_dir = (
-            e_center[np.newaxis, :, :] - position[:, np.newaxis, :]
-        )  # (1, m, 2) - (n, 1, 2) -> (n, m, 2)
-        f_dir_len = np.linalg.norm(f_dir, axis=2)  # (n, m)
-        # f_dir = f_dir / f_dir_len[:, :, np.newaxis]  # (n, m, 2)
-        f_dir = safe_div(f_dir, f_dir_len[:, :, np.newaxis])  # (n, m, 2)
-        f = f_scale[:, :, np.newaxis] * f_dir  # (n, m, 2)
-        f = f.sum(axis=1)  # (n, 2)
-        return f
-
-    def _node_repulsion(self, position, v2v_dist, k=1.0):
-        import numpy as np
-
-        """
-        Node repulsed by other nodes.
-        """
-        dist = v2v_dist.copy()
-        r, c = np.diag_indices_from(dist)
-        dist[r, c] = np.inf
-
-        f_scale = k / (dist**2)  # (n, n) with diag 0
-        f_dir = (
-            position[:, np.newaxis, :] - position[np.newaxis, :, :]
-        )  # (n, 1, 2) - (1, n, 2) -> (n, n, 2)
-        f_dir_len = np.linalg.norm(f_dir, axis=2)  # (n, n)
-        f_dir_len[r, c] = np.inf
-        # f_dir = f_dir / f_dir_len[:, :, np.newaxis]  # (n, n, 2)
-        f_dir = safe_div(f_dir, f_dir_len[:, :, np.newaxis])  # (n, n, 2)
-        f = f_scale[:, :, np.newaxis] * f_dir  # (n, n, 2)
-        f[r, c] = 0
-        f = f.sum(axis=1)  # (n, 2)
-        return f
-
-    def _edge_repulsion(self, e_center, H, e2e_dist, k=1.0):
-        import numpy as np
-
-        """
-        Edge repulsed by other edges.
-        """
-        dist = e2e_dist.copy()
-        r, c = np.diag_indices_from(dist)
-        dist[r, c] = np.inf
-
-        f_scale = k / (dist**2)  # (m, m)
-        f_dir = (
-            e_center[:, np.newaxis, :] - e_center[np.newaxis, :, :]
-        )  # (m, 1, 2) - (1, m, 2) -> (m, m, 2)
-        f_dir_len = np.linalg.norm(f_dir, axis=2)  # (m, m)
-        f_dir_len[r, c] = np.inf
-        # f_dir = f_dir / f_dir_len[:, :, np.newaxis]  # (m, m, 2)
-        f_dir = safe_div(f_dir, f_dir_len[:, :, np.newaxis])  # (m, m, 2)
-        f = f_scale[:, :, np.newaxis] * f_dir  # (m, m, 2)
-        f[r, c] = 0
-        f = f.sum(axis=1)  # (m, 2)
-        return np.matmul(H, f)
-
-    def _center_gravity(self, position, center, v2c_dist, k=1):
-        import numpy as np
-
-        """
-        Node attracted by center.
-        """
-        f_scale = v2c_dist  # (n, 1)
-        f_dir = (
-            center[np.newaxis, np.newaxis, :] - position[:, np.newaxis, :]
-        )  # (1, 1, 2) - (n, 1, 2) -> (n, 1, 2)
-        f_dir_len = np.linalg.norm(f_dir, axis=2)  # (n, 1)
-        # f_dir = f_dir / f_dir_len[:, :, np.newaxis]  # (n, 1, 2)
-        f_dir = safe_div(f_dir, f_dir_len[:, :, np.newaxis])  # (n, 1, 2)
-        f = f_scale[:, :, np.newaxis] * f_dir  # (n, 1, 2)
-        # f = jitter(f)
-        f = f.sum(axis=1) * k
-        return f
-
-    def _stop_condition(self, velocity, epsilon):
-        import numpy as np
-
-        """
-        Stop condition.
-        """
-        return np.linalg.norm(velocity) < epsilon
+from copy import deepcopy
+
+from .utils import safe_div
+
+
+class Simulator:
+    NODE_ATTRACTION = 0
+    NODE_REPULSION = 1
+    EDGE_REPULSION = 2
+    CENTER_GRAVITY = 3
+
+    def __init__(self, nums, forces, centers=1, damping_factor=0.999) -> None:
+        self.nums = [nums] if isinstance(nums, int) else nums
+
+        self.node_attraction = forces.get(self.NODE_ATTRACTION, None)
+        self.node_repulsion = forces.get(self.NODE_REPULSION, None)
+        self.edge_repulsion = forces.get(self.EDGE_REPULSION, None)
+        self.center_gravity = forces.get(self.CENTER_GRAVITY, None)
+
+        self.n_centers = len(centers)
+        self.centers = centers
+
+        if self.node_repulsion is not None and isinstance(self.node_repulsion, float):
+            self.node_repulsion = [self.node_repulsion] * self.n_centers
+        if self.center_gravity is not None and isinstance(self.center_gravity, float):
+            self.center_gravity = [self.center_gravity] * self.n_centers
+
+        self.damping_factor = damping_factor
+
+    def simulate(self, init_position, H, max_iter=400, epsilon=0.001, dt=2.0) -> None:
+        import numpy as np
+
+        """
+        Simulate the force-directed layout algorithm.
+        """
+        position = init_position.copy()
+        velocity = np.zeros_like(position)
+        damping = 1.0
+        for it in range(max_iter):
+            position, velocity, stop = self._step(
+                position, velocity, H, epsilon, damping, dt
+            )
+            if stop:
+                break
+            damping *= self.damping_factor
+        return position
+
+    def _step(self, position, velocity, H, epsilon, damping, dt):
+        import numpy as np
+
+        from sklearn.metrics import euclidean_distances
+
+        """
+        One step of the simulation.
+        """
+        v2v_dist = euclidean_distances(position)
+        e_center = np.matmul(H.T, position) / H.sum(axis=0).reshape(-1, 1)
+        v2e_dist = euclidean_distances(position, e_center) * H
+        e2e_dist = euclidean_distances(e_center)
+
+        centers = self.centers
+
+        force = np.zeros_like(position)
+        if self.node_attraction is not None:
+            f = (
+                self._node_attraction(position, e_center, v2e_dist)
+                * self.node_attraction
+            )
+            assert np.isnan(f).sum() == 0
+            force += f
+        if self.node_repulsion is not None:
+            f = self._node_repulsion(position, v2v_dist)
+            if self.n_centers == 1:
+                f *= self.node_repulsion[0]
+            else:
+                masks = np.zeros((position.shape[0], 1))
+                masks[: self.nums[0]] = self.node_repulsion[0]
+                masks[self.nums[0] :] = self.node_repulsion[1]
+                f *= masks
+            assert np.isnan(f).sum() == 0
+            force += f
+        if self.edge_repulsion is not None:
+            f = self._edge_repulsion(e_center, H, e2e_dist) * self.edge_repulsion
+            assert np.isnan(f).sum() == 0
+            force += f
+        if self.center_gravity is not None:
+            masks = [np.zeros((position.shape[0], 1)), np.zeros((position.shape[0], 1))]
+            masks[0][: self.nums[0]] = 1
+            masks[1][self.nums[0] :] = 1
+            for center, gravity, mask in zip(centers, self.center_gravity, masks):
+                v2c_dist = euclidean_distances(position, center.reshape(1, -1)).reshape(
+                    -1, 1
+                )
+                f = self._center_gravity(position, center, v2c_dist) * gravity * mask
+                assert np.isnan(f).sum() == 0
+                force += f
+
+        force *= damping
+
+        force = np.clip(force, -0.1, 0.1)
+        position += force * dt
+        velocity = force
+
+        return position, velocity, self._stop_condition(velocity, epsilon)
+
+    def _node_attraction(self, position, e_center, v2e_dist, x0=0.1, k=1.0):
+        import numpy as np
+
+        """
+        Node attracted by edge center.
+        """
+        x = deepcopy(v2e_dist)
+        x[v2e_dist > 0] -= x0
+        f_scale = k * x  # (n, m)
+        f_dir = (
+            e_center[np.newaxis, :, :] - position[:, np.newaxis, :]
+        )  # (1, m, 2) - (n, 1, 2) -> (n, m, 2)
+        f_dir_len = np.linalg.norm(f_dir, axis=2)  # (n, m)
+        # f_dir = f_dir / f_dir_len[:, :, np.newaxis]  # (n, m, 2)
+        f_dir = safe_div(f_dir, f_dir_len[:, :, np.newaxis])  # (n, m, 2)
+        f = f_scale[:, :, np.newaxis] * f_dir  # (n, m, 2)
+        f = f.sum(axis=1)  # (n, 2)
+        return f
+
+    def _node_repulsion(self, position, v2v_dist, k=1.0):
+        import numpy as np
+
+        """
+        Node repulsed by other nodes.
+        """
+        dist = v2v_dist.copy()
+        r, c = np.diag_indices_from(dist)
+        dist[r, c] = np.inf
+
+        f_scale = k / (dist**2)  # (n, n) with diag 0
+        f_dir = (
+            position[:, np.newaxis, :] - position[np.newaxis, :, :]
+        )  # (n, 1, 2) - (1, n, 2) -> (n, n, 2)
+        f_dir_len = np.linalg.norm(f_dir, axis=2)  # (n, n)
+        f_dir_len[r, c] = np.inf
+        # f_dir = f_dir / f_dir_len[:, :, np.newaxis]  # (n, n, 2)
+        f_dir = safe_div(f_dir, f_dir_len[:, :, np.newaxis])  # (n, n, 2)
+        f = f_scale[:, :, np.newaxis] * f_dir  # (n, n, 2)
+        f[r, c] = 0
+        f = f.sum(axis=1)  # (n, 2)
+        return f
+
+    def _edge_repulsion(self, e_center, H, e2e_dist, k=1.0):
+        import numpy as np
+
+        """
+        Edge repulsed by other edges.
+        """
+        dist = e2e_dist.copy()
+        r, c = np.diag_indices_from(dist)
+        dist[r, c] = np.inf
+
+        f_scale = k / (dist**2)  # (m, m)
+        f_dir = (
+            e_center[:, np.newaxis, :] - e_center[np.newaxis, :, :]
+        )  # (m, 1, 2) - (1, m, 2) -> (m, m, 2)
+        f_dir_len = np.linalg.norm(f_dir, axis=2)  # (m, m)
+        f_dir_len[r, c] = np.inf
+        # f_dir = f_dir / f_dir_len[:, :, np.newaxis]  # (m, m, 2)
+        f_dir = safe_div(f_dir, f_dir_len[:, :, np.newaxis])  # (m, m, 2)
+        f = f_scale[:, :, np.newaxis] * f_dir  # (m, m, 2)
+        f[r, c] = 0
+        f = f.sum(axis=1)  # (m, 2)
+        return np.matmul(H, f)
+
+    def _center_gravity(self, position, center, v2c_dist, k=1):
+        import numpy as np
+
+        """
+        Node attracted by center.
+        """
+        f_scale = v2c_dist  # (n, 1)
+        f_dir = (
+            center[np.newaxis, np.newaxis, :] - position[:, np.newaxis, :]
+        )  # (1, 1, 2) - (n, 1, 2) -> (n, 1, 2)
+        f_dir_len = np.linalg.norm(f_dir, axis=2)  # (n, 1)
+        # f_dir = f_dir / f_dir_len[:, :, np.newaxis]  # (n, 1, 2)
+        f_dir = safe_div(f_dir, f_dir_len[:, :, np.newaxis])  # (n, 1, 2)
+        f = f_scale[:, :, np.newaxis] * f_dir  # (n, 1, 2)
+        # f = jitter(f)
+        f = f.sum(axis=1) * k
+        return f
+
+    def _stop_condition(self, velocity, epsilon):
+        import numpy as np
+
+        """
+        Stop condition.
+        """
+        return np.linalg.norm(velocity) < epsilon
```

## easygraph/functions/drawing/drawing.py

```diff
@@ -1,1372 +1,1720 @@
-import random
-
-from copy import deepcopy
-from typing import List
-from typing import Optional
-from typing import Union
-
-import easygraph as eg
-
-
-__all__ = [
-    "draw_SHS_center",
-    "draw_SHS_center_kk",
-    "draw_kamada_kawai",
-    "draw_hypergraph",
-    "draw_dynamic_hypergraph",
-    "draw_easygraph_nodes",
-    "draw_easygraph_edges",
-]
-
-from easygraph.functions.drawing.defaults import default_hypergraph_strength
-from easygraph.functions.drawing.defaults import default_hypergraph_style
-from easygraph.functions.drawing.defaults import default_size
-from easygraph.functions.drawing.layout import force_layout
-from easygraph.functions.drawing.utils import draw_circle_edge
-from easygraph.functions.drawing.utils import draw_vertex
-
-
-def draw_hypergraph(
-    hg: "eg.Hypergraph",
-    e_style: str = "circle",
-    v_label: Optional[List[str]] = None,
-    v_size: Union[float, list] = 1.0,
-    v_color: Union[str, list] = "r",
-    v_line_width: Union[str, list] = 1.0,
-    e_color: Union[str, list] = "gray",
-    e_fill_color: Union[str, list] = "whitesmoke",
-    e_line_width: Union[str, list] = 1.0,
-    font_size: float = 1.0,
-    font_family: str = "sans-serif",
-    push_v_strength: float = 1.0,
-    push_e_strength: float = 1.0,
-    pull_e_strength: float = 1.0,
-    pull_center_strength: float = 1.0,
-):
-    r"""Draw the hypergraph structure.
-
-    Args:
-        ``hg`` (``eg.Hypergraph``): The EasyGraph's hypergraph object.
-        ``e_style`` (``str``): The style of hyperedges. The available styles are only ``'circle'``. Defaults to ``'circle'``.
-        ``v_label`` (``list``): The labels of vertices. Defaults to ``None``.
-        ``v_size`` (``float`` or ``list``): The size of vertices. Defaults to ``1.0``.
-        ``v_color`` (``str`` or ``list``): The `color <https://matplotlib.org/stable/gallery/color/named_colors.html>`_ of vertices. Defaults to ``'r'``.
-        ``v_line_width`` (``float`` or ``list``): The line width of vertices. Defaults to ``1.0``.
-        ``e_color`` (``str`` or ``list``): The `color <https://matplotlib.org/stable/gallery/color/named_colors.html>`_ of hyperedges. Defaults to ``'gray'``.
-        ``e_fill_color`` (``str`` or ``list``): The fill `color <https://matplotlib.org/stable/gallery/color/named_colors.html>`_ of hyperedges. Defaults to ``'whitesmoke'``.
-        ``e_line_width`` (``float`` or ``list``): The line width of hyperedges. Defaults to ``1.0``.
-        ``font_size`` (``float``): The font size of labels. Defaults to ``1.0``.
-        ``font_family`` (``str``): The font family of labels. Defaults to ``'sans-serif'``.
-        ``push_v_strength`` (``float``): The strength of pushing vertices. Defaults to ``1.0``.
-        ``push_e_strength`` (``float``): The strength of pushing hyperedges. Defaults to ``1.0``.
-        ``pull_e_strength`` (``float``): The strength of pulling hyperedges. Defaults to ``1.0``.
-        ``pull_center_strength`` (``float``): The strength of pulling vertices to the center. Defaults to ``1.0``.
-    """
-    import matplotlib.pyplot as plt
-
-    assert isinstance(
-        hg, eg.Hypergraph
-    ), "The input object must be a EasyGraph's hypergraph object."
-    assert e_style in ["circle"], "e_style must be 'circle'"
-    assert hg.num_e > 0, "g must be a non-empty structure"
-    fig, ax = plt.subplots(figsize=(6, 6))
-
-    num_v, e_list = hg.num_v, deepcopy(hg.e[0])
-    # default configures
-    v_color, e_color, e_fill_color = default_hypergraph_style(
-        hg.num_v, hg.num_e, v_color, e_color, e_fill_color
-    )
-    v_size, v_line_width, e_line_width, font_size = default_size(
-        num_v, e_list, v_size, v_line_width, e_line_width
-    )
-    (
-        push_v_strength,
-        push_e_strength,
-        pull_e_strength,
-        pull_center_strength,
-    ) = default_hypergraph_strength(
-        num_v,
-        e_list,
-        push_v_strength,
-        push_e_strength,
-        pull_e_strength,
-        pull_center_strength,
-    )
-    # layout
-    v_coor = force_layout(
-        num_v,
-        e_list,
-        push_v_strength,
-        push_e_strength,
-        pull_e_strength,
-        pull_center_strength,
-    )
-    if e_style == "circle":
-        draw_circle_edge(
-            ax,
-            v_coor,
-            v_size,
-            e_list,
-            e_color,
-            e_fill_color,
-            e_line_width,
-        )
-    else:
-        raise ValueError("e_style must be 'circle'")
-
-    draw_vertex(
-        ax,
-        v_coor,
-        v_label,
-        font_size,
-        font_family,
-        v_size,
-        v_color,
-        "black",
-        v_line_width,
-    )
-
-    plt.xlim((0, 1.0))
-    plt.ylim((0, 1.0))
-    plt.axis("off")
-    fig.tight_layout()
-    plt.show()
-
-
-def _draw_single_dynamic_hypergraph(
-    hg: "eg.Hypergraph",
-    ax,
-    title_font_size=4,
-    group_name: str = "main",
-    e_style: str = "circle",
-    v_label: Optional[List[str]] = None,
-    v_size: Union[float, list] = 2.0,
-    v_color: Union[str, list] = "r",
-    v_line_width: Union[str, list] = 1.0,
-    e_color: Union[str, list] = "gray",
-    e_fill_color: Union[str, list] = "whitesmoke",
-    e_line_width: Union[str, list] = 1.0,
-    font_size: float = 1.0,
-    font_family: str = "sans-serif",
-    push_v_strength: float = 1.0,
-    push_e_strength: float = 1.0,
-    pull_e_strength: float = 1.0,
-    pull_center_strength: float = 1.0,
-):
-    import matplotlib.pyplot as plt
-
-    assert isinstance(
-        hg, eg.Hypergraph
-    ), "The input object must be a EasyGraph's hypergraph object."
-    assert e_style in ["circle"], "e_style must be 'circle'"
-    assert hg.num_e > 0, "g must be a non-empty structure"
-
-    num_v, e_list = hg.num_v, deepcopy(hg.e_of_group(group_name)[0])
-    # default configures
-    v_color, e_color, e_fill_color = default_hypergraph_style(
-        hg.num_v, hg.num_e, v_color, e_color, e_fill_color
-    )
-    v_size, v_line_width, e_line_width, font_size = default_size(
-        num_v, e_list, v_size, v_line_width, e_line_width, font_size
-    )
-
-    (
-        push_v_strength,
-        push_e_strength,
-        pull_e_strength,
-        pull_center_strength,
-    ) = default_hypergraph_strength(
-        num_v,
-        e_list,
-        push_v_strength,
-        push_e_strength,
-        pull_e_strength,
-        pull_center_strength,
-    )
-    # layout
-    v_coor = force_layout(
-        num_v,
-        e_list,
-        push_v_strength,
-        push_e_strength,
-        pull_e_strength,
-        pull_center_strength,
-    )
-    if e_style == "circle":
-        draw_circle_edge(
-            ax,
-            v_coor,
-            v_size,
-            e_list,
-            e_color,
-            e_fill_color,
-            e_line_width,
-        )
-    else:
-        raise ValueError("e_style must be 'circle'")
-
-    draw_vertex(
-        ax,
-        v_coor,
-        v_label,
-        font_size,
-        font_family,
-        v_size,
-        v_color,
-        "black",
-        v_line_width,
-    )
-    plt.title(group_name, fontsize=title_font_size)
-    plt.xlim((0, 1.0))
-    plt.ylim((0, 1.0))
-
-    plt.axis("off")
-
-
-def draw_dynamic_hypergraph(
-    G,
-    group_name_list=None,
-    column_size=None,
-    save_path=None,
-    title_font_size=4,
-    e_style: str = "circle",
-    v_label: Optional[List[str]] = None,
-    v_size: Union[float, list] = 2.0,
-    v_color: Union[str, list] = "r",
-    v_line_width: Union[str, list] = 1.0,
-    e_color: Union[str, list] = "gray",
-    e_fill_color: Union[str, list] = "whitesmoke",
-    e_line_width: Union[str, list] = 1.0,
-    font_size: float = 1.0,
-    font_family: str = "sans-serif",
-    push_v_strength: float = 1.0,
-    push_e_strength: float = 1.0,
-    pull_e_strength: float = 1.0,
-    pull_center_strength: float = 1.0,
-):
-    """
-
-    Parameters
-    ----------
-    G eg.Hypergraph
-    group_name_list The groups to visualize
-    column_size The number of subplots placed in each row
-    save_path path to save visualization
-    title_font_size The font size of tilte of each subplot
-
-    """
-    import math
-
-    import matplotlib.pyplot as plt
-
-    if group_name_list == None:
-        group_name_list = G.group_names
-    COLUMN_SIZE = 3 if column_size == None else column_size
-    ROW_SIZE = math.ceil(len(group_name_list) / COLUMN_SIZE)
-    fig = plt.figure()
-
-    sub = 1
-    for gn in group_name_list:
-        if sub > len(group_name_list):
-            break
-        tmp_ax = fig.add_subplot(ROW_SIZE, COLUMN_SIZE, sub)
-        _draw_single_dynamic_hypergraph(
-            G,
-            ax=tmp_ax,
-            group_name=gn,
-            title_font_size=title_font_size,
-            e_style=e_style,
-            v_label=v_label,
-            v_size=v_size,
-            v_color=v_color,
-            v_line_width=v_line_width,
-            e_color=e_color,
-            e_fill_color=e_fill_color,
-            e_line_width=e_line_width,
-            font_size=font_size,
-            font_family=font_family,
-            push_v_strength=push_v_strength,
-            push_e_strength=push_e_strength,
-            pull_e_strength=pull_e_strength,
-            pull_center_strength=pull_center_strength,
-        )
-        sub += 1
-
-    fig.tight_layout()
-    if save_path is not None:
-        plt.savefig(save_path)
-    plt.show()
-
-
-def draw_easygraph_nodes(
-    G,
-    pos,
-    nodelist=None,
-    node_size=300,
-    node_color="#1f78b4",
-    node_shape="o",
-    alpha=None,
-    cmap=None,
-    vmin=None,
-    vmax=None,
-    ax=None,
-    linewidths=None,
-    edgecolors=None,
-    label=None,
-    margins=None,
-):
-    """Draw the nodes of the graph G.
-
-    This draws only the nodes of the graph G.
-
-    Parameters
-    ----------
-    G : graph
-        A EasyGraph graph
-
-    pos : dictionary
-        A dictionary with nodes as keys and positions as values.
-        Positions should be sequences of length 2.
-
-    ax : Matplotlib Axes object, optional
-        Draw the graph in the specified Matplotlib axes.
-
-    nodelist : list (default list(G))
-        Draw only specified nodes
-
-    node_size : scalar or array (default=300)
-        Size of nodes.  If an array it must be the same length as nodelist.
-
-    node_color : color or array of colors (default='#1f78b4')
-        Node color. Can be a single color or a sequence of colors with the same
-        length as nodelist. Color can be string or rgb (or rgba) tuple of
-        floats from 0-1. If numeric values are specified they will be
-        mapped to colors using the cmap and vmin,vmax parameters. See
-        matplotlib.scatter for more details.
-
-    node_shape :  string (default='o')
-        The shape of the node.  Specification is as matplotlib.scatter
-        marker, one of 'so^>v<dph8'.
-
-    alpha : float or array of floats (default=None)
-        The node transparency.  This can be a single alpha value,
-        in which case it will be applied to all the nodes of color. Otherwise,
-        if it is an array, the elements of alpha will be applied to the colors
-        in order (cycling through alpha multiple times if necessary).
-
-    cmap : Matplotlib colormap (default=None)
-        Colormap for mapping intensities of nodes
-
-    vmin,vmax : floats or None (default=None)
-        Minimum and maximum for node colormap scaling
-
-    linewidths : [None | scalar | sequence] (default=1.0)
-        Line width of symbol border
-
-    edgecolors : [None | scalar | sequence] (default = node_color)
-        Colors of node borders. Can be a single color or a sequence of colors with the
-        same length as nodelist. Color can be string or rgb (or rgba) tuple of floats
-        from 0-1. If numeric values are specified they will be mapped to colors
-        using the cmap and vmin,vmax parameters. See `~matplotlib.pyplot.scatter` for more details.
-
-    label : [None | string]
-        Label for legend
-
-    margins : float or 2-tuple, optional
-        Sets the padding for axis autoscaling. Increase margin to prevent
-        clipping for nodes that are near the edges of an image. Values should
-        be in the range ``[0, 1]``. See :meth:`matplotlib.axes.Axes.margins`
-        for details. The default is `None`, which uses the Matplotlib default.
-
-    Returns
-    -------
-    matplotlib.collections.PathCollection
-        `PathCollection` of the nodes.
-
-    Examples
-    --------
-    >>> G = eg.dodecahedral_graph()
-    >>> nodes = eg.draw_easygraph_nodes(G, pos=eg.spring_layout(G))
-
-
-
-    """
-    from collections.abc import Iterable
-
-    import matplotlib as mpl
-    import matplotlib.collections  # call as mpl.collections
-    import matplotlib.pyplot as plt
-    import numpy as np
-
-    if ax is None:
-        ax = plt.gca()
-
-    if nodelist is None:
-        nodelist = list(G)
-
-    if len(nodelist) == 0:  # empty nodelist, no drawing
-        return mpl.collections.PathCollection(None)
-
-    try:
-        xy = np.asarray([pos[v] for v in nodelist])
-    except KeyError as err:
-        raise eg.EasygraphError(f"Node {err} has no position.") from err
-
-    if isinstance(alpha, Iterable):
-        node_color = apply_alpha(node_color, alpha, nodelist, cmap, vmin, vmax)
-        alpha = None
-
-    node_collection = ax.scatter(
-        xy[:, 0],
-        xy[:, 1],
-        s=node_size,
-        c=node_color,
-        marker=node_shape,
-        cmap=cmap,
-        vmin=vmin,
-        vmax=vmax,
-        alpha=alpha,
-        linewidths=linewidths,
-        edgecolors=edgecolors,
-        label=label,
-    )
-    ax.tick_params(
-        axis="both",
-        which="both",
-        bottom=False,
-        left=False,
-        labelbottom=False,
-        labelleft=False,
-    )
-
-    if margins is not None:
-        if isinstance(margins, Iterable):
-            ax.margins(*margins)
-        else:
-            ax.margins(margins)
-
-    node_collection.set_zorder(2)
-    return node_collection
-
-
-def draw_easygraph_edges(
-    G,
-    pos,
-    edgelist=None,
-    width=1.0,
-    edge_color="k",
-    style="solid",
-    alpha=None,
-    arrowstyle=None,
-    arrowsize=10,
-    edge_cmap=None,
-    edge_vmin=None,
-    edge_vmax=None,
-    ax=None,
-    arrows=None,
-    label=None,
-    node_size=300,
-    nodelist=None,
-    node_shape="o",
-    connectionstyle="arc3",
-    min_source_margin=0,
-    min_target_margin=0,
-):
-    r"""Draw the edges of the graph G.
-
-    This draws only the edges of the graph G.
-
-    Parameters
-    ----------
-    G : graph
-        A easygraph graph
-
-    pos : dictionary
-        A dictionary with nodes as keys and positions as values.
-        Positions should be sequences of length 2.
-
-    edgelist : collection of edge tuples (default=G.edges())
-        Draw only specified edges
-
-    width : float or array of floats (default=1.0)
-        Line width of edges
-
-    edge_color : color or array of colors (default='k')
-        Edge color. Can be a single color or a sequence of colors with the same
-        length as edgelist. Color can be string or rgb (or rgba) tuple of
-        floats from 0-1. If numeric values are specified they will be
-        mapped to colors using the edge_cmap and edge_vmin,edge_vmax parameters.
-
-    style : string or array of strings (default='solid')
-        Edge line style e.g.: '-', '--', '-.', ':'
-        or words like 'solid' or 'dashed'.
-        Can be a single style or a sequence of styles with the same
-        length as the edge list.
-        If less styles than edges are given the styles will cycle.
-        If more styles than edges are given the styles will be used sequentially
-        and not be exhausted.
-        Also, `(offset, onoffseq)` tuples can be used as style instead of a strings.
-        (See `matplotlib.patches.FancyArrowPatch`: `linestyle`)
-
-    alpha : float or array of floats (default=None)
-        The edge transparency.  This can be a single alpha value,
-        in which case it will be applied to all specified edges. Otherwise,
-        if it is an array, the elements of alpha will be applied to the colors
-        in order (cycling through alpha multiple times if necessary).
-
-    edge_cmap : Matplotlib colormap, optional
-        Colormap for mapping intensities of edges
-
-    edge_vmin,edge_vmax : floats, optional
-        Minimum and maximum for edge colormap scaling
-
-    ax : Matplotlib Axes object, optional
-        Draw the graph in the specified Matplotlib axes.
-
-    arrows : bool or None, optional (default=None)
-        If `None`, directed graphs draw arrowheads with
-        `~matplotlib.patches.FancyArrowPatch`, while undirected graphs draw edges
-        via `~matplotlib.collections.LineCollection` for speed.
-        If `True`, draw arrowheads with FancyArrowPatches (bendable and stylish).
-        If `False`, draw edges using LineCollection (linear and fast).
-
-        Note: Arrowheads will be the same color as edges.
-
-    arrowstyle : str (default='-\|>' for directed graphs)
-        For directed graphs and `arrows==True` defaults to '-\|>',
-        For undirected graphs default to '-'.
-
-        See `matplotlib.patches.ArrowStyle` for more options.
-
-    arrowsize : int (default=10)
-        For directed graphs, choose the size of the arrow head's length and
-        width. See `matplotlib.patches.FancyArrowPatch` for attribute
-        `mutation_scale` for more info.
-
-    connectionstyle : string (default="arc3")
-        Pass the connectionstyle parameter to create curved arc of rounding
-        radius rad. For example, connectionstyle='arc3,rad=0.2'.
-        See `matplotlib.patches.ConnectionStyle` and
-        `matplotlib.patches.FancyArrowPatch` for more info.
-
-    node_size : scalar or array (default=300)
-        Size of nodes. Though the nodes are not drawn with this function, the
-        node size is used in determining edge positioning.
-
-    nodelist : list, optional (default=G.nodes())
-       This provides the node order for the `node_size` array (if it is an array).
-
-    node_shape :  string (default='o')
-        The marker used for nodes, used in determining edge positioning.
-        Specification is as a `matplotlib.markers` marker, e.g. one of 'so^>v<dph8'.
-
-    label : None or string
-        Label for legend
-
-    min_source_margin : int (default=0)
-        The minimum margin (gap) at the beginning of the edge at the source.
-
-    min_target_margin : int (default=0)
-        The minimum margin (gap) at the end of the edge at the target.
-
-    Returns
-    -------
-     matplotlib.collections.LineCollection or a list of matplotlib.patches.FancyArrowPatch
-        If ``arrows=True``, a list of FancyArrowPatches is returned.
-        If ``arrows=False``, a LineCollection is returned.
-        If ``arrows=None`` (the default), then a LineCollection is returned if
-        `G` is undirected, otherwise returns a list of FancyArrowPatches.
-
-    Notes
-    -----
-    For directed graphs, arrows are drawn at the head end.  Arrows can be
-    turned off with keyword arrows=False or by passing an arrowstyle without
-    an arrow on the end.
-
-    Be sure to include `node_size` as a keyword argument; arrows are
-    drawn considering the size of nodes.
-
-    Self-loops are always drawn with `~matplotlib.patches.FancyArrowPatch`
-    regardless of the value of `arrows` or whether `G` is directed.
-    When ``arrows=False`` or ``arrows=None`` and `G` is undirected, the
-    FancyArrowPatches corresponding to the self-loops are not explicitly
-    returned. They should instead be accessed via the ``Axes.patches``
-    attribute (see examples).
-
-    """
-    import matplotlib as mpl
-    import matplotlib.collections  # call as mpl.collections
-    import matplotlib.colors  # call as mpl.colors
-    import matplotlib.patches  # call as mpl.patches
-    import matplotlib.path  # call as mpl.path
-    import matplotlib.pyplot as plt
-    import numpy as np
-
-    # The default behavior is to use LineCollection to draw edges for
-    # undirected graphs (for performance reasons) and use FancyArrowPatches
-    # for directed graphs.
-    # The `arrows` keyword can be used to override the default behavior
-    use_linecollection = not G.is_directed()
-    if arrows in (True, False):
-        use_linecollection = not arrows
-
-    # Some kwargs only apply to FancyArrowPatches. Warn users when they use
-    # non-default values for these kwargs when LineCollection is being used
-    # instead of silently ignoring the specified option
-    if use_linecollection and any(
-        [
-            arrowstyle is not None,
-            arrowsize != 10,
-            connectionstyle != "arc3",
-            min_source_margin != 0,
-            min_target_margin != 0,
-        ]
-    ):
-        import warnings
-
-        msg = (
-            "\n\nThe {0} keyword argument is not applicable when drawing edges\n"
-            "with LineCollection.\n\n"
-            "To make this warning go away, either specify `arrows=True` to\n"
-            "force FancyArrowPatches or use the default value for {0}.\n"
-            "Note that using FancyArrowPatches may be slow for large graphs.\n"
-        )
-        if arrowstyle is not None:
-            msg = msg.format("arrowstyle")
-        if arrowsize != 10:
-            msg = msg.format("arrowsize")
-        if connectionstyle != "arc3":
-            msg = msg.format("connectionstyle")
-        if min_source_margin != 0:
-            msg = msg.format("min_source_margin")
-        if min_target_margin != 0:
-            msg = msg.format("min_target_margin")
-        warnings.warn(msg, category=UserWarning, stacklevel=2)
-
-    if arrowstyle == None:
-        if G.is_directed():
-            arrowstyle = "-|>"
-        else:
-            arrowstyle = "-"
-
-    if ax is None:
-        ax = plt.gca()
-
-    if edgelist is None:
-        edgelist = list(G.edges)
-
-    if len(edgelist) == 0:  # no edges!
-        return []
-
-    if nodelist is None:
-        nodelist = list(G.nodes)
-
-    # FancyArrowPatch handles color=None different from LineCollection
-    if edge_color is None:
-        edge_color = "k"
-    edgelist_tuple = list(map(tuple, edgelist))
-
-    # set edge positions
-    edge_pos = np.asarray([(pos[e[0]], pos[e[1]]) for e in edgelist])
-
-    # Check if edge_color is an array of floats and map to edge_cmap.
-    # This is the only case handled differently from matplotlib
-    if (
-        np.iterable(edge_color)
-        and (len(edge_color) == len(edge_pos))
-        and np.all([isinstance(c, Number) for c in edge_color])
-    ):
-        if edge_cmap is not None:
-            assert isinstance(edge_cmap, mpl.colors.Colormap)
-        else:
-            edge_cmap = plt.get_cmap()
-        if edge_vmin is None:
-            edge_vmin = min(edge_color)
-        if edge_vmax is None:
-            edge_vmax = max(edge_color)
-        color_normal = mpl.colors.Normalize(vmin=edge_vmin, vmax=edge_vmax)
-        edge_color = [edge_cmap(color_normal(e)) for e in edge_color]
-
-    def _draw_networkx_edges_line_collection():
-        edge_collection = mpl.collections.LineCollection(
-            edge_pos,
-            colors=edge_color,
-            linewidths=width,
-            antialiaseds=(1,),
-            linestyle=style,
-            alpha=alpha,
-        )
-        edge_collection.set_cmap(edge_cmap)
-        edge_collection.set_clim(edge_vmin, edge_vmax)
-        edge_collection.set_zorder(1)  # edges go behind nodes
-        edge_collection.set_label(label)
-        ax.add_collection(edge_collection)
-
-        return edge_collection
-
-    def _draw_networkx_edges_fancy_arrow_patch():
-        # Note: Waiting for someone to implement arrow to intersection with
-        # marker.  Meanwhile, this works well for polygons with more than 4
-        # sides and circle.
-
-        def to_marker_edge(marker_size, marker):
-            if marker in "s^>v<d":  # `large` markers need extra space
-                return np.sqrt(2 * marker_size) / 2
-            else:
-                return np.sqrt(marker_size) / 2
-
-        # Draw arrows with `matplotlib.patches.FancyarrowPatch`
-        arrow_collection = []
-
-        if isinstance(arrowsize, list):
-            if len(arrowsize) != len(edge_pos):
-                raise ValueError("arrowsize should have the same length as edgelist")
-        else:
-            mutation_scale = arrowsize  # scale factor of arrow head
-
-        base_connection_style = mpl.patches.ConnectionStyle(connectionstyle)
-
-        # Fallback for self-loop scale. Left outside of _connectionstyle so it is
-        # only computed once
-        max_nodesize = np.array(node_size).max()
-
-        def _connectionstyle(posA, posB, *args, **kwargs):
-            # check if we need to do a self-loop
-            if np.all(posA == posB):
-                # Self-loops are scaled by view extent, except in cases the extent
-                # is 0, e.g. for a single node. In this case, fall back to scaling
-                # by the maximum node size
-                selfloop_ht = 0.005 * max_nodesize if h == 0 else h
-                # this is called with _screen space_ values so convert back
-                # to data space
-                data_loc = ax.transData.inverted().transform(posA)
-                v_shift = 0.1 * selfloop_ht
-                h_shift = v_shift * 0.5
-                # put the top of the loop first so arrow is not hidden by node
-                path = [
-                    # 1
-                    data_loc + np.asarray([0, v_shift]),
-                    # 4 4 4
-                    data_loc + np.asarray([h_shift, v_shift]),
-                    data_loc + np.asarray([h_shift, 0]),
-                    data_loc,
-                    # 4 4 4
-                    data_loc + np.asarray([-h_shift, 0]),
-                    data_loc + np.asarray([-h_shift, v_shift]),
-                    data_loc + np.asarray([0, v_shift]),
-                ]
-
-                ret = mpl.path.Path(ax.transData.transform(path), [1, 4, 4, 4, 4, 4, 4])
-            # if not, fall back to the user specified behavior
-            else:
-                ret = base_connection_style(posA, posB, *args, **kwargs)
-
-            return ret
-
-        # FancyArrowPatch doesn't handle color strings
-        arrow_colors = mpl.colors.colorConverter.to_rgba_array(edge_color, alpha)
-        for i, (src, dst) in zip(fancy_edges_indices, edge_pos):
-            x1, y1 = src
-            x2, y2 = dst
-            shrink_source = 0  # space from source to tail
-            shrink_target = 0  # space from  head to target
-
-            if isinstance(arrowsize, list):
-                # Scale each factor of each arrow based on arrowsize list
-                mutation_scale = arrowsize[i]
-
-            if np.iterable(node_size):  # many node sizes
-                source, target = edgelist[i][:2]
-                source_node_size = node_size[nodelist.index(source)]
-                target_node_size = node_size[nodelist.index(target)]
-                shrink_source = to_marker_edge(source_node_size, node_shape)
-                shrink_target = to_marker_edge(target_node_size, node_shape)
-            else:
-                shrink_source = shrink_target = to_marker_edge(node_size, node_shape)
-
-            if shrink_source < min_source_margin:
-                shrink_source = min_source_margin
-
-            if shrink_target < min_target_margin:
-                shrink_target = min_target_margin
-
-            if len(arrow_colors) > i:
-                arrow_color = arrow_colors[i]
-            elif len(arrow_colors) == 1:
-                arrow_color = arrow_colors[0]
-            else:  # Cycle through colors
-                arrow_color = arrow_colors[i % len(arrow_colors)]
-
-            if np.iterable(width):
-                if len(width) > i:
-                    line_width = width[i]
-                else:
-                    line_width = width[i % len(width)]
-            else:
-                line_width = width
-
-            if (
-                np.iterable(style)
-                and not isinstance(style, str)
-                and not isinstance(style, tuple)
-            ):
-                if len(style) > i:
-                    linestyle = style[i]
-                else:  # Cycle through styles
-                    linestyle = style[i % len(style)]
-            else:
-                linestyle = style
-
-            arrow = mpl.patches.FancyArrowPatch(
-                (x1, y1),
-                (x2, y2),
-                arrowstyle=arrowstyle,
-                shrinkA=shrink_source,
-                shrinkB=shrink_target,
-                mutation_scale=mutation_scale,
-                color=arrow_color,
-                linewidth=line_width,
-                connectionstyle=_connectionstyle,
-                linestyle=linestyle,
-                zorder=1,
-            )  # arrows go behind nodes
-
-            arrow_collection.append(arrow)
-            ax.add_patch(arrow)
-
-        return arrow_collection
-
-    # compute initial view
-    minx = np.amin(np.ravel(edge_pos[:, :, 0]))
-    maxx = np.amax(np.ravel(edge_pos[:, :, 0]))
-    miny = np.amin(np.ravel(edge_pos[:, :, 1]))
-    maxy = np.amax(np.ravel(edge_pos[:, :, 1]))
-    w = maxx - minx
-    h = maxy - miny
-
-    # Draw the edges
-    if use_linecollection:
-        edge_viz_obj = _draw_networkx_edges_line_collection()
-        # Make sure selfloop edges are also drawn
-        selfloops_to_draw = [loop for loop in eg.selfloop_edges(G) if loop in edgelist]
-        if selfloops_to_draw:
-            fancy_edges_indices = [
-                edgelist_tuple.index(loop) for loop in selfloops_to_draw
-            ]
-            edge_pos = np.asarray([(pos[e[0]], pos[e[1]]) for e in selfloops_to_draw])
-            arrowstyle = "-"
-            _draw_networkx_edges_fancy_arrow_patch()
-    else:
-        fancy_edges_indices = range(len(edgelist))
-        edge_viz_obj = _draw_networkx_edges_fancy_arrow_patch()
-
-    # update view after drawing
-    padx, pady = 0.05 * w, 0.05 * h
-    corners = (minx - padx, miny - pady), (maxx + padx, maxy + pady)
-    ax.update_datalim(corners)
-    ax.autoscale_view()
-
-    ax.tick_params(
-        axis="both",
-        which="both",
-        bottom=False,
-        left=False,
-        labelbottom=False,
-        labelleft=False,
-    )
-
-    return edge_viz_obj
-
-
-def draw_SHS_center(G, SHS, rate=1, style="center"):
-    """
-    Draw the graph whose the SH Spanners are in the center, with random layout.
-
-    Parameters
-    ----------
-    G : graph
-        A easygraph graph.
-
-    SHS : list
-        The SH Spanners in graph G.
-
-    rate : float
-       The proportion of visible points and edges to the total
-
-    style : string
-        "side"- the label is next to the dot
-        "center"- the label is in the center of the dot
-
-    Returns
-    -------
-    graph : network
-        the graph whose the SH Spanners are in the center.
-    """
-    import matplotlib.pyplot as plt
-    import numpy as np
-
-    pos = eg.random_position(G)
-    center = np.zeros((len(SHS), 2), float)
-    node = np.zeros((len(pos) - len(SHS), 2), float)
-    m, n = 0, 0
-    if rate == 1:
-        for i in pos:
-            if i in SHS:
-                center[n][0] = 0.5 + (-1) ** random.randint(1, 2) * pos[i][0] / 5
-                center[n][1] = 0.5 + (-1) ** random.randint(1, 2) * pos[i][1] / 5
-                pos[i][0] = center[n][0]
-                pos[i][1] = center[n][1]
-                n += 1
-            else:
-                node[m][0] = pos[i][0]
-                node[m][1] = pos[i][1]
-                m += 1
-        if style == "side":
-            plt.scatter(node[:, 0], node[:, 1], marker=".", color="b", s=10)
-            plt.scatter(center[:, 0], center[:, 1], marker="*", color="r", s=20)
-        elif style == "center":
-            plt.scatter(
-                node[:, 0],
-                node[:, 1],
-                marker="o",
-                color="lightblue",
-                edgecolors="lightblue",
-                s=300,
-                linewidth=0.5,
-            )
-            plt.scatter(
-                center[:, 0],
-                center[:, 1],
-                marker="o",
-                color="lightblue",
-                edgecolors="lightblue",
-                s=300,
-                linewidth=0.5,
-            )
-            plt.scatter(
-                center[:, 0],
-                center[:, 1],
-                marker="*",
-                color="None",
-                edgecolors="r",
-                s=1000,
-                linewidth=2,
-            )
-        k = 0
-        for i in pos:
-            if style == "side":
-                plt.text(
-                    pos[i][0],
-                    pos[i][1],
-                    i,
-                    fontsize=5,
-                    verticalalignment="top",
-                    horizontalalignment="right",
-                )
-            elif style == "center":
-                plt.text(
-                    pos[i][0],
-                    pos[i][1],
-                    i,
-                    fontsize=10,
-                    verticalalignment="center",
-                    horizontalalignment="center",
-                )
-            k += 1
-        for i in G.edges:
-            p1 = [pos[i[0]][0], pos[i[1]][0]]
-            p2 = [pos[i[0]][1], pos[i[1]][1]]
-            plt.plot(p1, p2, color="lightblue", linestyle="-", alpha=0.3, linewidth=3)
-        plt.show()
-
-    else:
-        degree = G.degree()
-        sorted_degree = sorted(degree.items(), key=lambda d: d[1], reverse=True)
-        l = int(rate * len(G))
-        s = []
-        for i in sorted_degree:
-            if len(s) < l:
-                s.append(i[0])
-        for i in pos:
-            if i in SHS and i in s:
-                center[n][0] = 0.5 + (-1) ** random.randint(1, 2) * pos[i][0] / 5
-                center[n][1] = 0.5 + (-1) ** random.randint(1, 2) * pos[i][1] / 5
-                pos[i][0] = center[n][0]
-                pos[i][1] = center[n][1]
-                n += 1
-            elif i in s:
-                node[m][0] = pos[i][0]
-                node[m][1] = pos[i][1]
-                m += 1
-        node = node[0:m, :]
-        center = center[0:n, :]
-        if style == "side":
-            plt.scatter(node[:, 0], node[:, 1], marker=".", color="b", s=10)
-            plt.scatter(center[:, 0], center[:, 1], marker="*", color="r", s=20)
-        elif style == "center":
-            plt.scatter(
-                node[:, 0],
-                node[:, 1],
-                marker="o",
-                color="None",
-                edgecolors="b",
-                s=50,
-                linewidth=0.5,
-            )
-            plt.scatter(
-                center[:, 0],
-                center[:, 1],
-                marker="o",
-                color="None",
-                edgecolors="r",
-                s=50,
-                linewidth=0.5,
-            )
-        k = 0
-        for i in pos:
-            if i in s:
-                if style == "side":
-                    plt.text(
-                        pos[i][0],
-                        pos[i][1],
-                        i,
-                        fontsize=5,
-                        verticalalignment="top",
-                        horizontalalignment="right",
-                    )
-                elif style == "center":
-                    plt.text(
-                        pos[i][0],
-                        pos[i][1],
-                        i,
-                        fontsize=5,
-                        verticalalignment="center",
-                        horizontalalignment="center",
-                    )
-                k += 1
-        for i in G.edges:
-            (u, v, t) = i
-            if u in s and v in s:
-                p1 = [pos[i[0]][0], pos[i[1]][0]]
-                p2 = [pos[i[0]][1], pos[i[1]][1]]
-                plt.plot(
-                    p1, p2, color="lightblue", linestyle="-", alpha=0.3, linewidth=3
-                )
-        plt.show()
-    return
-
-
-def draw_SHS_center_kk(G, SHS, rate=1, style="center"):
-    """
-    Draw the graph whose the SH Spanners are in the center, with a Kamada-Kawai force-directed layout.
-
-    Parameters
-    ----------
-    G : graph
-        A easygraph graph.
-
-    SHS : list
-        The SH Spanners in graph G.
-
-    rate : float
-       The proportion of visible points and edges to the total
-
-    style : string
-        "side"- the label is next to the dot
-        "center"- the label is in the center of the dot
-
-    Returns
-    -------
-    graph : network
-        the graph whose the SH Spanners are in the center.
-    """
-    import matplotlib.pyplot as plt
-    import numpy as np
-
-    pos = eg.kamada_kawai_layout(G)
-    center = np.zeros((len(SHS), 2), float)
-    node = np.zeros((len(pos) - len(SHS), 2), float)
-    m, n = 0, 0
-    if rate == 1:
-        for i in pos:
-            if i in SHS:
-                center[n][0] = pos[i][0] / 5
-                center[n][1] = pos[i][1] / 5
-                pos[i][0] = center[n][0]
-                pos[i][1] = center[n][1]
-                n += 1
-            else:
-                node[m][0] = pos[i][0]
-                node[m][1] = pos[i][1]
-                m += 1
-        if style == "side":
-            plt.scatter(node[:, 0], node[:, 1], marker=".", color="b", s=50)
-            plt.scatter(center[:, 0], center[:, 1], marker="*", color="r", s=100)
-        elif style == "center":
-            plt.scatter(
-                node[:, 0],
-                node[:, 1],
-                marker="o",
-                color="lightblue",
-                edgecolors="lightblue",
-                s=300,
-                linewidth=0.5,
-            )
-            plt.scatter(
-                center[:, 0],
-                center[:, 1],
-                marker="o",
-                color="lightblue",
-                edgecolors="lightblue",
-                s=300,
-                linewidth=0.5,
-            )
-            plt.scatter(
-                center[:, 0],
-                center[:, 1],
-                marker="*",
-                color="None",
-                edgecolors="r",
-                s=1000,
-                linewidth=2,
-            )
-        k = 0
-        for i in pos:
-            if style == "side":
-                plt.text(
-                    pos[i][0],
-                    pos[i][1],
-                    i,
-                    fontsize=5,
-                    verticalalignment="top",
-                    horizontalalignment="right",
-                )
-            elif style == "center":
-                plt.text(
-                    pos[i][0],
-                    pos[i][1],
-                    i,
-                    fontsize=10,
-                    verticalalignment="center",
-                    horizontalalignment="center",
-                )
-            k += 1
-        for i in G.edges:
-            p1 = [pos[i[0]][0], pos[i[1]][0]]
-            p2 = [pos[i[0]][1], pos[i[1]][1]]
-            plt.plot(p1, p2, color="lightblue", linestyle="-", alpha=0.3, linewidth=3)
-        plt.show()
-    else:
-        degree = G.degree()
-        sorted_degree = sorted(degree.items(), key=lambda d: d[1], reverse=True)
-        l = int(rate * len(G))
-        s = []
-        for i in sorted_degree:
-            if len(s) < l:
-                s.append(i[0])
-        for i in pos:
-            if i in SHS and i in s:
-                center[n][0] = pos[i][0] / 5
-                center[n][1] = pos[i][1] / 5
-                pos[i][0] = center[n][0]
-                pos[i][1] = center[n][1]
-                n += 1
-            elif i in s:
-                node[m][0] = pos[i][0]
-                node[m][1] = pos[i][1]
-                m += 1
-        node = node[0:m, :]
-        center = center[0:n, :]
-        if style == "side":
-            plt.scatter(node[:, 0], node[:, 1], marker=".", color="b", s=10)
-            plt.scatter(center[:, 0], center[:, 1], marker="*", color="r", s=20)
-        elif style == "center":
-            plt.scatter(
-                node[:, 0],
-                node[:, 1],
-                marker="o",
-                color="None",
-                edgecolors="b",
-                s=50,
-                linewidth=0.5,
-            )
-            plt.scatter(
-                center[:, 0],
-                center[:, 1],
-                marker="o",
-                color="None",
-                edgecolors="r",
-                s=50,
-                linewidth=0.5,
-            )
-        k = 0
-        for i in pos:
-            if i in s:
-                if style == "side":
-                    plt.text(
-                        pos[i][0],
-                        pos[i][1],
-                        i,
-                        fontsize=5,
-                        verticalalignment="top",
-                        horizontalalignment="right",
-                    )
-                elif style == "center":
-                    plt.text(
-                        pos[i][0],
-                        pos[i][1],
-                        i,
-                        fontsize=5,
-                        verticalalignment="center",
-                        horizontalalignment="center",
-                    )
-                k += 1
-        for i in G.edges:
-            (u, v, t) = i
-            if u in s and v in s:
-                p1 = [pos[i[0]][0], pos[i[1]][0]]
-                p2 = [pos[i[0]][1], pos[i[1]][1]]
-                plt.plot(
-                    p1, p2, color="lightblue", linestyle="-", alpha=0.3, linewidth=3
-                )
-        plt.show()
-    return
-
-
-def draw_kamada_kawai(G, rate=1, style="side"):
-    """Draw the graph G with a Kamada-Kawai force-directed layout.
-
-    Parameters
-    ----------
-    G : graph
-       A easygraph graph
-
-    rate : float
-       The proportion of visible points and edges to the total
-
-    style : string
-        "side"- the label is next to the dot
-        "center"- the label is in the center of the dot
-
-    """
-    import matplotlib.pyplot as plt
-    import numpy as np
-
-    pos = eg.kamada_kawai_layout(G)
-    node = np.zeros((len(pos), 2), float)
-    m, n = 0, 0
-    if rate == 1:
-        for i in pos:
-            node[m][0] = pos[i][0]
-            node[m][1] = pos[i][1]
-            m += 1
-        if style == "side":
-            plt.scatter(node[:, 0], node[:, 1], marker=".", color="b", s=10)
-        elif style == "center":
-            plt.scatter(
-                node[:, 0],
-                node[:, 1],
-                marker="o",
-                color="None",
-                edgecolors="b",
-                s=50,
-                linewidth=0.5,
-            )
-        k = 0
-        for i in pos:
-            if style == "side":
-                plt.text(
-                    pos[i][0],
-                    pos[i][1],
-                    i,
-                    fontsize=5,
-                    verticalalignment="top",
-                    horizontalalignment="right",
-                )
-            elif style == "center":
-                plt.text(
-                    pos[i][0],
-                    pos[i][1],
-                    i,
-                    fontsize=5,
-                    verticalalignment="center",
-                    horizontalalignment="center",
-                )
-            k += 1
-        for i in G.edges:
-            p1 = [pos[i[0]][0], pos[i[1]][0]]
-            p2 = [pos[i[0]][1], pos[i[1]][1]]
-            plt.plot(p1, p2, "k-", alpha=0.3, linewidth=0.5)
-        plt.show()
-    else:
-        degree = G.degree()
-        sorted_degree = sorted(degree.items(), key=lambda d: d[1], reverse=True)
-        l = int(rate * len(G))
-        s = []
-        for i in sorted_degree:
-            if len(s) < l:
-                s.append(i[0])
-        for i in pos:
-            if i in s:
-                node[m][0] = pos[i][0]
-                node[m][1] = pos[i][1]
-                m += 1
-        node = node[0:m, :]
-        if style == "side":
-            plt.scatter(node[:, 0], node[:, 1], marker=".", color="b", s=10)
-        elif style == "center":
-            plt.scatter(
-                node[:, 0],
-                node[:, 1],
-                marker="o",
-                color="None",
-                edgecolors="b",
-                s=50,
-                linewidth=0.5,
-            )
-        k = 0
-        for i in pos:
-            if i in s:
-                if style == "side":
-                    plt.text(
-                        pos[i][0],
-                        pos[i][1],
-                        i,
-                        fontsize=5,
-                        verticalalignment="top",
-                        horizontalalignment="right",
-                    )
-                elif style == "center":
-                    plt.text(
-                        pos[i][0],
-                        pos[i][1],
-                        i,
-                        fontsize=5,
-                        verticalalignment="center",
-                        horizontalalignment="center",
-                    )
-                k += 1
-        for i in G.edges:
-            (u, v, t) = i
-            if u in s and v in s:
-                p1 = [pos[i[0]][0], pos[i[1]][0]]
-                p2 = [pos[i[0]][1], pos[i[1]][1]]
-                plt.plot(p1, p2, "k-", alpha=0.3, linewidth=0.5)
-        plt.show()
-    return
-
-
-if __name__ == "__main__":
-    G = eg.datasets.get_graph_karateclub()
-    draw_SHS_center(G, [1, 33, 34], style="side")
-    draw_SHS_center(G, [1, 33, 34], style="center")
-    draw_SHS_center_kk(G, [1, 33, 34], style="side")
-    draw_SHS_center_kk(G, [1, 33, 34], style="center")
-    draw_kamada_kawai(G, style="side")
-    draw_kamada_kawai(G, style="center")
-    draw_SHS_center(G, [1, 33, 34], rate=0.8, style="side")
-    draw_SHS_center(G, [1, 33, 34], rate=0.8, style="center")
-    draw_SHS_center_kk(G, [1, 33, 34], rate=0.8, style="side")
-    draw_SHS_center_kk(G, [1, 33, 34], rate=0.8, style="center")
-    draw_kamada_kawai(G, rate=0.8, style="side")
-    draw_kamada_kawai(G, rate=0.8, style="center")
+import random
+
+from copy import deepcopy
+from typing import List
+from typing import Optional
+from typing import Union
+
+import easygraph as eg
+
+
+__all__ = [
+    "draw_SHS_center",
+    "draw_SHS_center_kk",
+    "draw_kamada_kawai",
+    "draw_hypergraph",
+    "draw_dynamic_hypergraph",
+    "draw_easygraph_nodes",
+    "draw_easygraph_edges",
+    "draw_louvain_com",
+    "draw_lpa_com",
+    "draw_gm_com",
+    "draw_ego_graph",
+]
+
+from easygraph.functions.drawing.defaults import default_hypergraph_strength
+from easygraph.functions.drawing.defaults import default_hypergraph_style
+from easygraph.functions.drawing.defaults import default_size
+from easygraph.functions.drawing.layout import force_layout
+from easygraph.functions.drawing.utils import draw_circle_edge
+from easygraph.functions.drawing.utils import draw_vertex
+
+
+def draw_hypergraph(
+    hg: "eg.Hypergraph",
+    e_style: str = "circle",
+    v_label: Optional[List[str]] = None,
+    v_size: Union[float, list] = 1.0,
+    v_color: Union[str, list] = "r",
+    v_line_width: Union[str, list] = 1.0,
+    e_color: Union[str, list] = "gray",
+    e_fill_color: Union[str, list] = "whitesmoke",
+    e_line_width: Union[str, list] = 1.0,
+    font_size: float = 1.0,
+    font_family: str = "sans-serif",
+    push_v_strength: float = 1.0,
+    push_e_strength: float = 1.0,
+    pull_e_strength: float = 1.0,
+    pull_center_strength: float = 1.0,
+):
+    r"""Draw the hypergraph structure.
+
+    Args:
+        ``hg`` (``eg.Hypergraph``): The EasyGraph's hypergraph object.
+        ``e_style`` (``str``): The style of hyperedges. The available styles are only ``'circle'``. Defaults to ``'circle'``.
+        ``v_label`` (``list``): The labels of vertices. Defaults to ``None``.
+        ``v_size`` (``float`` or ``list``): The size of vertices. Defaults to ``1.0``.
+        ``v_color`` (``str`` or ``list``): The `color <https://matplotlib.org/stable/gallery/color/named_colors.html>`_ of vertices. Defaults to ``'r'``.
+        ``v_line_width`` (``float`` or ``list``): The line width of vertices. Defaults to ``1.0``.
+        ``e_color`` (``str`` or ``list``): The `color <https://matplotlib.org/stable/gallery/color/named_colors.html>`_ of hyperedges. Defaults to ``'gray'``.
+        ``e_fill_color`` (``str`` or ``list``): The fill `color <https://matplotlib.org/stable/gallery/color/named_colors.html>`_ of hyperedges. Defaults to ``'whitesmoke'``.
+        ``e_line_width`` (``float`` or ``list``): The line width of hyperedges. Defaults to ``1.0``.
+        ``font_size`` (``float``): The font size of labels. Defaults to ``1.0``.
+        ``font_family`` (``str``): The font family of labels. Defaults to ``'sans-serif'``.
+        ``push_v_strength`` (``float``): The strength of pushing vertices. Defaults to ``1.0``.
+        ``push_e_strength`` (``float``): The strength of pushing hyperedges. Defaults to ``1.0``.
+        ``pull_e_strength`` (``float``): The strength of pulling hyperedges. Defaults to ``1.0``.
+        ``pull_center_strength`` (``float``): The strength of pulling vertices to the center. Defaults to ``1.0``.
+    """
+    import matplotlib.pyplot as plt
+
+    assert isinstance(
+        hg, eg.Hypergraph
+    ), "The input object must be a EasyGraph's hypergraph object."
+    assert e_style in ["circle"], "e_style must be 'circle'"
+    assert hg.num_e > 0, "g must be a non-empty structure"
+    fig, ax = plt.subplots(figsize=(6, 6))
+
+    num_v, e_list = hg.num_v, deepcopy(hg.e[0])
+    # default configures
+    v_color, e_color, e_fill_color = default_hypergraph_style(
+        hg.num_v, hg.num_e, v_color, e_color, e_fill_color
+    )
+    v_size, v_line_width, e_line_width, font_size = default_size(
+        num_v, e_list, v_size, v_line_width, e_line_width
+    )
+    (
+        push_v_strength,
+        push_e_strength,
+        pull_e_strength,
+        pull_center_strength,
+    ) = default_hypergraph_strength(
+        num_v,
+        e_list,
+        push_v_strength,
+        push_e_strength,
+        pull_e_strength,
+        pull_center_strength,
+    )
+    # layout
+    v_coor = force_layout(
+        num_v,
+        e_list,
+        push_v_strength,
+        push_e_strength,
+        pull_e_strength,
+        pull_center_strength,
+    )
+    if e_style == "circle":
+        draw_circle_edge(
+            ax,
+            v_coor,
+            v_size,
+            e_list,
+            e_color,
+            e_fill_color,
+            e_line_width,
+        )
+    else:
+        raise ValueError("e_style must be 'circle'")
+
+    draw_vertex(
+        ax,
+        v_coor,
+        v_label,
+        font_size,
+        font_family,
+        v_size,
+        v_color,
+        v_line_width,
+    )
+
+    plt.xlim((0, 1.0))
+    plt.ylim((0, 1.0))
+    plt.axis("off")
+    fig.tight_layout()
+    plt.show()
+
+
+def _draw_single_dynamic_hypergraph(
+    hg: "eg.Hypergraph",
+    ax,
+    title_font_size=4,
+    group_name: str = "main",
+    e_style: str = "circle",
+    v_label: Optional[List[str]] = None,
+    v_size: Union[float, list] = 2.0,
+    v_color: Union[str, list] = "r",
+    v_line_width: Union[str, list] = 1.0,
+    e_color: Union[str, list] = "gray",
+    e_fill_color: Union[str, list] = "whitesmoke",
+    e_line_width: Union[str, list] = 1.0,
+    font_size: float = 1.0,
+    font_family: str = "sans-serif",
+    push_v_strength: float = 1.0,
+    push_e_strength: float = 1.0,
+    pull_e_strength: float = 1.0,
+    pull_center_strength: float = 1.0,
+):
+    import matplotlib.pyplot as plt
+
+    assert isinstance(
+        hg, eg.Hypergraph
+    ), "The input object must be a EasyGraph's hypergraph object."
+    assert e_style in ["circle"], "e_style must be 'circle'"
+    assert hg.num_e > 0, "g must be a non-empty structure"
+
+    num_v, e_list = hg.num_v, deepcopy(hg.e_of_group(group_name)[0])
+    # default configures
+    v_color, e_color, e_fill_color = default_hypergraph_style(
+        hg.num_v, hg.num_e, v_color, e_color, e_fill_color
+    )
+    v_size, v_line_width, e_line_width, font_size = default_size(
+        num_v, e_list, v_size, v_line_width, e_line_width, font_size
+    )
+
+    (
+        push_v_strength,
+        push_e_strength,
+        pull_e_strength,
+        pull_center_strength,
+    ) = default_hypergraph_strength(
+        num_v,
+        e_list,
+        push_v_strength,
+        push_e_strength,
+        pull_e_strength,
+        pull_center_strength,
+    )
+    # layout
+    v_coor = force_layout(
+        num_v,
+        e_list,
+        push_v_strength,
+        push_e_strength,
+        pull_e_strength,
+        pull_center_strength,
+    )
+    if e_style == "circle":
+        draw_circle_edge(
+            ax,
+            v_coor,
+            v_size,
+            e_list,
+            e_color,
+            e_fill_color,
+            e_line_width,
+        )
+    else:
+        raise ValueError("e_style must be 'circle'")
+
+    draw_vertex(
+        ax,
+        v_coor,
+        v_label,
+        font_size,
+        font_family,
+        v_size,
+        v_color,
+        v_line_width,
+    )
+    plt.title(group_name, fontsize=title_font_size)
+    plt.xlim((0, 1.0))
+    plt.ylim((0, 1.0))
+    plt.axis("off")
+
+
+def draw_dynamic_hypergraph(
+    G,
+    group_name_list=None,
+    column_size=None,
+    save_path=None,
+    title_font_size=4,
+    e_style: str = "circle",
+    v_label: Optional[List[str]] = None,
+    v_size: Union[float, list] = 2.0,
+    v_color: Union[str, list] = "r",
+    v_line_width: Union[str, list] = 1.0,
+    e_color: Union[str, list] = "gray",
+    e_fill_color: Union[str, list] = "whitesmoke",
+    e_line_width: Union[str, list] = 1.0,
+    font_size: float = 1.0,
+    font_family: str = "sans-serif",
+    push_v_strength: float = 1.0,
+    push_e_strength: float = 1.0,
+    pull_e_strength: float = 1.0,
+    pull_center_strength: float = 1.0,
+):
+    """
+
+    Parameters
+    ----------
+    G eg.Hypergraph
+    group_name_list The groups to visualize
+    column_size The number of subplots placed in each row
+    save_path path to save visualization
+    title_font_size The font size of tilte of each subplot
+
+    """
+    import math
+
+    import matplotlib.pyplot as plt
+
+    # if group_name_list == None:
+    #     group_name_list = G.group_names
+    COLUMN_SIZE = 3 if column_size == None else column_size
+    ROW_SIZE = math.ceil(len(group_name_list) / COLUMN_SIZE)
+    fig = plt.figure()
+
+    sub = 1
+    for gn in group_name_list:
+        if sub > len(group_name_list):
+            break
+        tmp_ax = fig.add_subplot(ROW_SIZE, COLUMN_SIZE, sub)
+        _draw_single_dynamic_hypergraph(
+            G,
+            ax=tmp_ax,
+            group_name=gn,
+            title_font_size=title_font_size,
+            e_style=e_style,
+            v_label=v_label,
+            v_size=v_size,
+            v_color=v_color,
+            v_line_width=v_line_width,
+            e_color=e_color,
+            e_fill_color=e_fill_color,
+            e_line_width=e_line_width,
+            font_size=font_size,
+            font_family=font_family,
+            push_v_strength=push_v_strength,
+            push_e_strength=push_e_strength,
+            pull_e_strength=pull_e_strength,
+            pull_center_strength=pull_center_strength,
+        )
+        sub += 1
+    fig.tight_layout()
+    if save_path is not None:
+        plt.savefig(save_path)
+    plt.show()
+
+
+def draw_easygraph_nodes(
+    G,
+    pos,
+    nodelist=None,
+    node_size=300,
+    node_color="#1f78b4",
+    node_shape="o",
+    alpha=None,
+    cmap=None,
+    vmin=None,
+    vmax=None,
+    ax=None,
+    linewidths=None,
+    edgecolors=None,
+    label=None,
+    margins=None,
+):
+    """Draw the nodes of the graph G.
+
+    This draws only the nodes of the graph G.
+
+    Parameters
+    ----------
+    G : graph
+        A EasyGraph graph
+
+    pos : dictionary
+        A dictionary with nodes as keys and positions as values.
+        Positions should be sequences of length 2.
+
+    ax : Matplotlib Axes object, optional
+        Draw the graph in the specified Matplotlib axes.
+
+    nodelist : list (default list(G))
+        Draw only specified nodes
+
+    node_size : scalar or array (default=300)
+        Size of nodes.  If an array it must be the same length as nodelist.
+
+    node_color : color or array of colors (default='#1f78b4')
+        Node color. Can be a single color or a sequence of colors with the same
+        length as nodelist. Color can be string or rgb (or rgba) tuple of
+        floats from 0-1. If numeric values are specified they will be
+        mapped to colors using the cmap and vmin,vmax parameters. See
+        matplotlib.scatter for more details.
+
+    node_shape :  string (default='o')
+        The shape of the node.  Specification is as matplotlib.scatter
+        marker, one of 'so^>v<dph8'.
+
+    alpha : float or array of floats (default=None)
+        The node transparency.  This can be a single alpha value,
+        in which case it will be applied to all the nodes of color. Otherwise,
+        if it is an array, the elements of alpha will be applied to the colors
+        in order (cycling through alpha multiple times if necessary).
+
+    cmap : Matplotlib colormap (default=None)
+        Colormap for mapping intensities of nodes
+
+    vmin,vmax : floats or None (default=None)
+        Minimum and maximum for node colormap scaling
+
+    linewidths : [None | scalar | sequence] (default=1.0)
+        Line width of symbol border
+
+    edgecolors : [None | scalar | sequence] (default = node_color)
+        Colors of node borders. Can be a single color or a sequence of colors with the
+        same length as nodelist. Color can be string or rgb (or rgba) tuple of floats
+        from 0-1. If numeric values are specified they will be mapped to colors
+        using the cmap and vmin,vmax parameters. See `~matplotlib.pyplot.scatter` for more details.
+
+    label : [None | string]
+        Label for legend
+
+    margins : float or 2-tuple, optional
+        Sets the padding for axis autoscaling. Increase margin to prevent
+        clipping for nodes that are near the edges of an image. Values should
+        be in the range ``[0, 1]``. See :meth:`matplotlib.axes.Axes.margins`
+        for details. The default is `None`, which uses the Matplotlib default.
+
+    Returns
+    -------
+    matplotlib.collections.PathCollection
+        `PathCollection` of the nodes.
+
+    Examples
+    --------
+    >>> G = eg.dodecahedral_graph()
+    >>> nodes = eg.draw_easygraph_nodes(G, pos=eg.spring_layout(G))
+
+
+
+    """
+    from collections.abc import Iterable
+
+    import matplotlib as mpl
+    import matplotlib.collections  # call as mpl.collections
+    import matplotlib.pyplot as plt
+    import numpy as np
+
+    if ax is None:
+        ax = plt.gca()
+
+    if nodelist is None:
+        nodelist = list(G)
+
+    if len(nodelist) == 0:  # empty nodelist, no drawing
+        return mpl.collections.PathCollection(None)
+
+    try:
+        xy = np.asarray([pos[v] for v in nodelist])
+    except KeyError as err:
+        raise eg.EasygraphError(f"Node {err} has no position.") from err
+
+    if isinstance(alpha, Iterable):
+        node_color = apply_alpha(node_color, alpha, nodelist, cmap, vmin, vmax)
+        alpha = None
+
+    node_collection = ax.scatter(
+        xy[:, 0],
+        xy[:, 1],
+        s=node_size,
+        c=node_color,
+        marker=node_shape,
+        cmap=cmap,
+        vmin=vmin,
+        vmax=vmax,
+        alpha=alpha,
+        linewidths=linewidths,
+        edgecolors=edgecolors,
+        label=label,
+    )
+    ax.tick_params(
+        axis="both",
+        which="both",
+        bottom=False,
+        left=False,
+        labelbottom=False,
+        labelleft=False,
+    )
+
+    if margins is not None:
+        if isinstance(margins, Iterable):
+            ax.margins(*margins)
+        else:
+            ax.margins(margins)
+
+    node_collection.set_zorder(2)
+    return node_collection
+
+
+def draw_easygraph_edges(
+    G,
+    pos,
+    edgelist=None,
+    width=1.0,
+    edge_color="k",
+    style="solid",
+    alpha=None,
+    arrowstyle=None,
+    arrowsize=10,
+    edge_cmap=None,
+    edge_vmin=None,
+    edge_vmax=None,
+    ax=None,
+    arrows=None,
+    label=None,
+    node_size=300,
+    nodelist=None,
+    node_shape="o",
+    connectionstyle="arc3",
+    min_source_margin=0,
+    min_target_margin=0,
+):
+    r"""Draw the edges of the graph G.
+
+    This draws only the edges of the graph G.
+
+    Parameters
+    ----------
+    G : graph
+        A easygraph graph
+
+    pos : dictionary
+        A dictionary with nodes as keys and positions as values.
+        Positions should be sequences of length 2.
+
+    edgelist : collection of edge tuples (default=G.edges())
+        Draw only specified edges
+
+    width : float or array of floats (default=1.0)
+        Line width of edges
+
+    edge_color : color or array of colors (default='k')
+        Edge color. Can be a single color or a sequence of colors with the same
+        length as edgelist. Color can be string or rgb (or rgba) tuple of
+        floats from 0-1. If numeric values are specified they will be
+        mapped to colors using the edge_cmap and edge_vmin,edge_vmax parameters.
+
+    style : string or array of strings (default='solid')
+        Edge line style e.g.: '-', '--', '-.', ':'
+        or words like 'solid' or 'dashed'.
+        Can be a single style or a sequence of styles with the same
+        length as the edge list.
+        If less styles than edges are given the styles will cycle.
+        If more styles than edges are given the styles will be used sequentially
+        and not be exhausted.
+        Also, `(offset, onoffseq)` tuples can be used as style instead of a strings.
+        (See `matplotlib.patches.FancyArrowPatch`: `linestyle`)
+
+    alpha : float or array of floats (default=None)
+        The edge transparency.  This can be a single alpha value,
+        in which case it will be applied to all specified edges. Otherwise,
+        if it is an array, the elements of alpha will be applied to the colors
+        in order (cycling through alpha multiple times if necessary).
+
+    edge_cmap : Matplotlib colormap, optional
+        Colormap for mapping intensities of edges
+
+    edge_vmin,edge_vmax : floats, optional
+        Minimum and maximum for edge colormap scaling
+
+    ax : Matplotlib Axes object, optional
+        Draw the graph in the specified Matplotlib axes.
+
+    arrows : bool or None, optional (default=None)
+        If `None`, directed graphs draw arrowheads with
+        `~matplotlib.patches.FancyArrowPatch`, while undirected graphs draw edges
+        via `~matplotlib.collections.LineCollection` for speed.
+        If `True`, draw arrowheads with FancyArrowPatches (bendable and stylish).
+        If `False`, draw edges using LineCollection (linear and fast).
+
+        Note: Arrowheads will be the same color as edges.
+
+    arrowstyle : str (default='-\|>' for directed graphs)
+        For directed graphs and `arrows==True` defaults to '-\|>',
+        For undirected graphs default to '-'.
+
+        See `matplotlib.patches.ArrowStyle` for more options.
+
+    arrowsize : int (default=10)
+        For directed graphs, choose the size of the arrow head's length and
+        width. See `matplotlib.patches.FancyArrowPatch` for attribute
+        `mutation_scale` for more info.
+
+    connectionstyle : string (default="arc3")
+        Pass the connectionstyle parameter to create curved arc of rounding
+        radius rad. For example, connectionstyle='arc3,rad=0.2'.
+        See `matplotlib.patches.ConnectionStyle` and
+        `matplotlib.patches.FancyArrowPatch` for more info.
+
+    node_size : scalar or array (default=300)
+        Size of nodes. Though the nodes are not drawn with this function, the
+        node size is used in determining edge positioning.
+
+    nodelist : list, optional (default=G.nodes())
+       This provides the node order for the `node_size` array (if it is an array).
+
+    node_shape :  string (default='o')
+        The marker used for nodes, used in determining edge positioning.
+        Specification is as a `matplotlib.markers` marker, e.g. one of 'so^>v<dph8'.
+
+    label : None or string
+        Label for legend
+
+    min_source_margin : int (default=0)
+        The minimum margin (gap) at the beginning of the edge at the source.
+
+    min_target_margin : int (default=0)
+        The minimum margin (gap) at the end of the edge at the target.
+
+    Returns
+    -------
+     matplotlib.collections.LineCollection or a list of matplotlib.patches.FancyArrowPatch
+        If ``arrows=True``, a list of FancyArrowPatches is returned.
+        If ``arrows=False``, a LineCollection is returned.
+        If ``arrows=None`` (the default), then a LineCollection is returned if
+        `G` is undirected, otherwise returns a list of FancyArrowPatches.
+
+    Notes
+    -----
+    For directed graphs, arrows are drawn at the head end.  Arrows can be
+    turned off with keyword arrows=False or by passing an arrowstyle without
+    an arrow on the end.
+
+    Be sure to include `node_size` as a keyword argument; arrows are
+    drawn considering the size of nodes.
+
+    Self-loops are always drawn with `~matplotlib.patches.FancyArrowPatch`
+    regardless of the value of `arrows` or whether `G` is directed.
+    When ``arrows=False`` or ``arrows=None`` and `G` is undirected, the
+    FancyArrowPatches corresponding to the self-loops are not explicitly
+    returned. They should instead be accessed via the ``Axes.patches``
+    attribute (see examples).
+
+    """
+    import matplotlib as mpl
+    import matplotlib.collections  # call as mpl.collections
+    import matplotlib.colors  # call as mpl.colors
+    import matplotlib.patches  # call as mpl.patches
+    import matplotlib.path  # call as mpl.path
+    import matplotlib.pyplot as plt
+    import numpy as np
+
+    # The default behavior is to use LineCollection to draw edges for
+    # undirected graphs (for performance reasons) and use FancyArrowPatches
+    # for directed graphs.
+    # The `arrows` keyword can be used to override the default behavior
+    use_linecollection = not G.is_directed()
+    if arrows in (True, False):
+        use_linecollection = not arrows
+
+    # Some kwargs only apply to FancyArrowPatches. Warn users when they use
+    # non-default values for these kwargs when LineCollection is being used
+    # instead of silently ignoring the specified option
+    if use_linecollection and any(
+        [
+            arrowstyle is not None,
+            arrowsize != 10,
+            connectionstyle != "arc3",
+            min_source_margin != 0,
+            min_target_margin != 0,
+        ]
+    ):
+        import warnings
+
+        msg = (
+            "\n\nThe {0} keyword argument is not applicable when drawing edges\n"
+            "with LineCollection.\n\n"
+            "To make this warning go away, either specify `arrows=True` to\n"
+            "force FancyArrowPatches or use the default value for {0}.\n"
+            "Note that using FancyArrowPatches may be slow for large graphs.\n"
+        )
+        if arrowstyle is not None:
+            msg = msg.format("arrowstyle")
+        if arrowsize != 10:
+            msg = msg.format("arrowsize")
+        if connectionstyle != "arc3":
+            msg = msg.format("connectionstyle")
+        if min_source_margin != 0:
+            msg = msg.format("min_source_margin")
+        if min_target_margin != 0:
+            msg = msg.format("min_target_margin")
+        warnings.warn(msg, category=UserWarning, stacklevel=2)
+
+    if arrowstyle == None:
+        if G.is_directed():
+            arrowstyle = "-|>"
+        else:
+            arrowstyle = "-"
+
+    if ax is None:
+        ax = plt.gca()
+
+    if edgelist is None:
+        edgelist = list(G.edges)
+
+    if len(edgelist) == 0:  # no edges!
+        return []
+
+    if nodelist is None:
+        nodelist = list(G.nodes)
+
+    # FancyArrowPatch handles color=None different from LineCollection
+    if edge_color is None:
+        edge_color = "k"
+    edgelist_tuple = list(map(tuple, edgelist))
+
+    # set edge positions
+    edge_pos = np.asarray([(pos[e[0]], pos[e[1]]) for e in edgelist])
+
+    # Check if edge_color is an array of floats and map to edge_cmap.
+    # This is the only case handled differently from matplotlib
+    if (
+        np.iterable(edge_color)
+        and (len(edge_color) == len(edge_pos))
+        and np.all([isinstance(c, Number) for c in edge_color])
+    ):
+        if edge_cmap is not None:
+            assert isinstance(edge_cmap, mpl.colors.Colormap)
+        else:
+            edge_cmap = plt.get_cmap()
+        if edge_vmin is None:
+            edge_vmin = min(edge_color)
+        if edge_vmax is None:
+            edge_vmax = max(edge_color)
+        color_normal = mpl.colors.Normalize(vmin=edge_vmin, vmax=edge_vmax)
+        edge_color = [edge_cmap(color_normal(e)) for e in edge_color]
+
+    def _draw_networkx_edges_line_collection():
+        edge_collection = mpl.collections.LineCollection(
+            edge_pos,
+            colors=edge_color,
+            linewidths=width,
+            antialiaseds=(1,),
+            linestyle=style,
+            alpha=alpha,
+        )
+        edge_collection.set_cmap(edge_cmap)
+        edge_collection.set_clim(edge_vmin, edge_vmax)
+        edge_collection.set_zorder(1)  # edges go behind nodes
+        edge_collection.set_label(label)
+        ax.add_collection(edge_collection)
+
+        return edge_collection
+
+    def _draw_networkx_edges_fancy_arrow_patch():
+        # Note: Waiting for someone to implement arrow to intersection with
+        # marker.  Meanwhile, this works well for polygons with more than 4
+        # sides and circle.
+
+        def to_marker_edge(marker_size, marker):
+            if marker in "s^>v<d":  # `large` markers need extra space
+                return np.sqrt(2 * marker_size) / 2
+            else:
+                return np.sqrt(marker_size) / 2
+
+        # Draw arrows with `matplotlib.patches.FancyarrowPatch`
+        arrow_collection = []
+
+        if isinstance(arrowsize, list):
+            if len(arrowsize) != len(edge_pos):
+                raise ValueError("arrowsize should have the same length as edgelist")
+        else:
+            mutation_scale = arrowsize  # scale factor of arrow head
+
+        base_connection_style = mpl.patches.ConnectionStyle(connectionstyle)
+
+        # Fallback for self-loop scale. Left outside of _connectionstyle so it is
+        # only computed once
+        max_nodesize = np.array(node_size).max()
+
+        def _connectionstyle(posA, posB, *args, **kwargs):
+            # check if we need to do a self-loop
+            if np.all(posA == posB):
+                # Self-loops are scaled by view extent, except in cases the extent
+                # is 0, e.g. for a single node. In this case, fall back to scaling
+                # by the maximum node size
+                selfloop_ht = 0.005 * max_nodesize if h == 0 else h
+                # this is called with _screen space_ values so convert back
+                # to data space
+                data_loc = ax.transData.inverted().transform(posA)
+                v_shift = 0.1 * selfloop_ht
+                h_shift = v_shift * 0.5
+                # put the top of the loop first so arrow is not hidden by node
+                path = [
+                    # 1
+                    data_loc + np.asarray([0, v_shift]),
+                    # 4 4 4
+                    data_loc + np.asarray([h_shift, v_shift]),
+                    data_loc + np.asarray([h_shift, 0]),
+                    data_loc,
+                    # 4 4 4
+                    data_loc + np.asarray([-h_shift, 0]),
+                    data_loc + np.asarray([-h_shift, v_shift]),
+                    data_loc + np.asarray([0, v_shift]),
+                ]
+
+                ret = mpl.path.Path(ax.transData.transform(path), [1, 4, 4, 4, 4, 4, 4])
+            # if not, fall back to the user specified behavior
+            else:
+                ret = base_connection_style(posA, posB, *args, **kwargs)
+
+            return ret
+
+        # FancyArrowPatch doesn't handle color strings
+        arrow_colors = mpl.colors.colorConverter.to_rgba_array(edge_color, alpha)
+        for i, (src, dst) in zip(fancy_edges_indices, edge_pos):
+            x1, y1 = src
+            x2, y2 = dst
+            shrink_source = 0  # space from source to tail
+            shrink_target = 0  # space from  head to target
+
+            if isinstance(arrowsize, list):
+                # Scale each factor of each arrow based on arrowsize list
+                mutation_scale = arrowsize[i]
+
+            if np.iterable(node_size):  # many node sizes
+                source, target = edgelist[i][:2]
+                source_node_size = node_size[nodelist.index(source)]
+                target_node_size = node_size[nodelist.index(target)]
+                shrink_source = to_marker_edge(source_node_size, node_shape)
+                shrink_target = to_marker_edge(target_node_size, node_shape)
+            else:
+                shrink_source = shrink_target = to_marker_edge(node_size, node_shape)
+
+            if shrink_source < min_source_margin:
+                shrink_source = min_source_margin
+
+            if shrink_target < min_target_margin:
+                shrink_target = min_target_margin
+
+            if len(arrow_colors) > i:
+                arrow_color = arrow_colors[i]
+            elif len(arrow_colors) == 1:
+                arrow_color = arrow_colors[0]
+            else:  # Cycle through colors
+                arrow_color = arrow_colors[i % len(arrow_colors)]
+
+            if np.iterable(width):
+                if len(width) > i:
+                    line_width = width[i]
+                else:
+                    line_width = width[i % len(width)]
+            else:
+                line_width = width
+
+            if (
+                np.iterable(style)
+                and not isinstance(style, str)
+                and not isinstance(style, tuple)
+            ):
+                if len(style) > i:
+                    linestyle = style[i]
+                else:  # Cycle through styles
+                    linestyle = style[i % len(style)]
+            else:
+                linestyle = style
+
+            arrow = mpl.patches.FancyArrowPatch(
+                (x1, y1),
+                (x2, y2),
+                arrowstyle=arrowstyle,
+                shrinkA=shrink_source,
+                shrinkB=shrink_target,
+                mutation_scale=mutation_scale,
+                color=arrow_color,
+                linewidth=line_width,
+                connectionstyle=_connectionstyle,
+                linestyle=linestyle,
+                zorder=1,
+            )  # arrows go behind nodes
+
+            arrow_collection.append(arrow)
+            ax.add_patch(arrow)
+
+        return arrow_collection
+
+    # compute initial view
+    minx = np.amin(np.ravel(edge_pos[:, :, 0]))
+    maxx = np.amax(np.ravel(edge_pos[:, :, 0]))
+    miny = np.amin(np.ravel(edge_pos[:, :, 1]))
+    maxy = np.amax(np.ravel(edge_pos[:, :, 1]))
+    w = maxx - minx
+    h = maxy - miny
+
+    # Draw the edges
+    if use_linecollection:
+        edge_viz_obj = _draw_networkx_edges_line_collection()
+        # Make sure selfloop edges are also drawn
+        selfloops_to_draw = [loop for loop in eg.selfloop_edges(G) if loop in edgelist]
+        if selfloops_to_draw:
+            fancy_edges_indices = [
+                edgelist_tuple.index(loop) for loop in selfloops_to_draw
+            ]
+            edge_pos = np.asarray([(pos[e[0]], pos[e[1]]) for e in selfloops_to_draw])
+            arrowstyle = "-"
+            _draw_networkx_edges_fancy_arrow_patch()
+    else:
+        fancy_edges_indices = range(len(edgelist))
+        edge_viz_obj = _draw_networkx_edges_fancy_arrow_patch()
+
+    # update view after drawing
+    padx, pady = 0.05 * w, 0.05 * h
+    corners = (minx - padx, miny - pady), (maxx + padx, maxy + pady)
+    ax.update_datalim(corners)
+    ax.autoscale_view()
+
+    ax.tick_params(
+        axis="both",
+        which="both",
+        bottom=False,
+        left=False,
+        labelbottom=False,
+        labelleft=False,
+    )
+
+    return edge_viz_obj
+
+
+def draw_SHS_center(G, SHS, rate=1, style="center"):
+    """
+    Draw the graph whose the SH Spanners are in the center, with random layout.
+
+    Parameters
+    ----------
+    G : graph
+        A easygraph graph.
+
+    SHS : list
+        The SH Spanners in graph G.
+
+    rate : float
+       The proportion of visible points and edges to the total
+
+    style : string
+        "side"- the label is next to the dot
+        "center"- the label is in the center of the dot
+
+    Returns
+    -------
+    graph : network
+        the graph whose the SH Spanners are in the center.
+    """
+    import matplotlib.pyplot as plt
+    import numpy as np
+
+    plt.figure(figsize=(8, 8))
+    pos = eg.random_position(G)
+    center = np.zeros((len(SHS), 2), float)
+    node = np.zeros((len(pos) - len(SHS), 2), float)
+    m, n = 0, 0
+    if rate == 1:
+        for i in pos:
+            if i in SHS:
+                center[n][0] = 0.5 + (-1) ** random.randint(1, 2) * pos[i][0] / 5
+                center[n][1] = 0.5 + (-1) ** random.randint(1, 2) * pos[i][1] / 5
+                pos[i][0] = center[n][0]
+                pos[i][1] = center[n][1]
+                n += 1
+            else:
+                node[m][0] = pos[i][0]
+                node[m][1] = pos[i][1]
+                m += 1
+        if style == "side":
+            plt.scatter(node[:, 0], node[:, 1], marker=".", color="b", s=10)
+            plt.scatter(center[:, 0], center[:, 1], marker="*", color="r", s=20)
+        elif style == "center":
+            plt.scatter(
+                node[:, 0],
+                node[:, 1],
+                marker="o",
+                color="skyblue",
+                edgecolors="skyblue",
+                s=300,
+                linewidth=0.5,
+            )
+            plt.scatter(
+                center[:, 0],
+                center[:, 1],
+                marker="o",
+                color="tomato",
+                edgecolors="tomato",
+                s=500,
+                linewidth=0.5,
+                zorder=2,
+            )
+        k = 0
+        for i in pos:
+            if style == "side":
+                plt.text(
+                    pos[i][0],
+                    pos[i][1],
+                    i,
+                    fontsize=5,
+                    verticalalignment="top",
+                    horizontalalignment="right",
+                )
+            elif style == "center":
+                plt.text(
+                    pos[i][0],
+                    pos[i][1],
+                    i,
+                    fontsize=10,
+                    verticalalignment="center",
+                    horizontalalignment="center",
+                )
+            k += 1
+        for i in G.edges:
+            p1 = [pos[i[0]][0], pos[i[1]][0]]
+            p2 = [pos[i[0]][1], pos[i[1]][1]]
+            plt.plot(
+                p1,
+                p2,
+                color="skyblue",
+                linestyle="-",
+                alpha=0.3,
+                linewidth=1.8,
+                zorder=1,
+            )
+        plt.show()
+
+    else:
+        degree = G.degree()
+        sorted_degree = sorted(degree.items(), key=lambda d: d[1], reverse=True)
+        l = int(rate * len(G))
+        s = []
+        for i in sorted_degree:
+            if len(s) < l:
+                s.append(i[0])
+        for i in pos:
+            if i in SHS and i in s:
+                center[n][0] = 0.5 + (-1) ** random.randint(1, 2) * pos[i][0] / 5
+                center[n][1] = 0.5 + (-1) ** random.randint(1, 2) * pos[i][1] / 5
+                pos[i][0] = center[n][0]
+                pos[i][1] = center[n][1]
+                n += 1
+            elif i in s:
+                node[m][0] = pos[i][0]
+                node[m][1] = pos[i][1]
+                m += 1
+        node = node[0:m, :]
+        center = center[0:n, :]
+        if style == "side":
+            plt.scatter(node[:, 0], node[:, 1], marker=".", color="b", s=10)
+            plt.scatter(center[:, 0], center[:, 1], marker="*", color="r", s=20)
+        elif style == "center":
+            plt.scatter(
+                node[:, 0],
+                node[:, 1],
+                marker="o",
+                color="None",
+                edgecolors="b",
+                s=50,
+                linewidth=0.5,
+            )
+            plt.scatter(
+                center[:, 0],
+                center[:, 1],
+                marker="o",
+                color="None",
+                edgecolors="r",
+                s=50,
+                linewidth=0.5,
+            )
+        k = 0
+        for i in pos:
+            if i in s:
+                if style == "side":
+                    plt.text(
+                        pos[i][0],
+                        pos[i][1],
+                        i,
+                        fontsize=5,
+                        verticalalignment="top",
+                        horizontalalignment="right",
+                    )
+                elif style == "center":
+                    plt.text(
+                        pos[i][0],
+                        pos[i][1],
+                        i,
+                        fontsize=5,
+                        verticalalignment="center",
+                        horizontalalignment="center",
+                    )
+                k += 1
+        for i in G.edges:
+            (u, v, t) = i
+            if u in s and v in s:
+                p1 = [pos[i[0]][0], pos[i[1]][0]]
+                p2 = [pos[i[0]][1], pos[i[1]][1]]
+                plt.plot(p1, p2, color="skyblue", linestyle="-", alpha=0.3, linewidth=3)
+        plt.show()
+    return
+
+
+def draw_SHS_center_kk(G, SHS, rate=1, style="center"):
+    """
+    Draw the graph whose the SH Spanners are in the center, with a Kamada-Kawai force-directed layout.
+
+    Parameters
+    ----------
+    G : graph
+        A easygraph graph.
+
+    SHS : list
+        The SH Spanners in graph G.
+
+    rate : float
+       The proportion of visible points and edges to the total
+
+    style : string
+        "side"- the label is next to the dot
+        "center"- the label is in the center of the dot
+
+    Returns
+    -------
+    graph : network
+        the graph whose the SH Spanners are in the center.
+    """
+    import matplotlib.pyplot as plt
+    import numpy as np
+
+    pos = eg.kamada_kawai_layout(G)
+    center = np.zeros((len(SHS), 2), float)
+    node = np.zeros((len(pos) - len(SHS), 2), float)
+    m, n = 0, 0
+    if rate == 1:
+        for i in pos:
+            if i in SHS:
+                center[n][0] = pos[i][0] / 5
+                center[n][1] = pos[i][1] / 5
+                pos[i][0] = center[n][0]
+                pos[i][1] = center[n][1]
+                n += 1
+            else:
+                node[m][0] = pos[i][0]
+                node[m][1] = pos[i][1]
+                m += 1
+        if style == "side":
+            plt.scatter(node[:, 0], node[:, 1], marker=".", color="b", s=50)
+            plt.scatter(center[:, 0], center[:, 1], marker="*", color="r", s=100)
+        elif style == "center":
+            plt.scatter(
+                node[:, 0],
+                node[:, 1],
+                marker="o",
+                color="skyblue",
+                edgecolors="skyblue",
+                s=300,
+                linewidth=0.5,
+            )
+            plt.scatter(
+                center[:, 0],
+                center[:, 1],
+                marker="o",
+                color="skyblue",
+                edgecolors="skyblue",
+                s=300,
+                linewidth=0.5,
+            )
+            plt.scatter(
+                center[:, 0],
+                center[:, 1],
+                marker="*",
+                color="None",
+                edgecolors="r",
+                s=1000,
+                linewidth=2,
+            )
+        k = 0
+        for i in pos:
+            if style == "side":
+                plt.text(
+                    pos[i][0],
+                    pos[i][1],
+                    i,
+                    fontsize=5,
+                    verticalalignment="top",
+                    horizontalalignment="right",
+                )
+            elif style == "center":
+                plt.text(
+                    pos[i][0],
+                    pos[i][1],
+                    i,
+                    fontsize=10,
+                    verticalalignment="center",
+                    horizontalalignment="center",
+                )
+            k += 1
+        for i in G.edges:
+            p1 = [pos[i[0]][0], pos[i[1]][0]]
+            p2 = [pos[i[0]][1], pos[i[1]][1]]
+            plt.plot(p1, p2, color="skyblue", linestyle="-", alpha=0.3, linewidth=3)
+        plt.show()
+    else:
+        degree = G.degree()
+        sorted_degree = sorted(degree.items(), key=lambda d: d[1], reverse=True)
+        l = int(rate * len(G))
+        s = []
+        for i in sorted_degree:
+            if len(s) < l:
+                s.append(i[0])
+        for i in pos:
+            if i in SHS and i in s:
+                center[n][0] = pos[i][0] / 5
+                center[n][1] = pos[i][1] / 5
+                pos[i][0] = center[n][0]
+                pos[i][1] = center[n][1]
+                n += 1
+            elif i in s:
+                node[m][0] = pos[i][0]
+                node[m][1] = pos[i][1]
+                m += 1
+        node = node[0:m, :]
+        center = center[0:n, :]
+        if style == "side":
+            plt.scatter(node[:, 0], node[:, 1], marker=".", color="b", s=10)
+            plt.scatter(center[:, 0], center[:, 1], marker="*", color="r", s=20)
+        elif style == "center":
+            plt.scatter(
+                node[:, 0],
+                node[:, 1],
+                marker="o",
+                color="None",
+                edgecolors="b",
+                s=50,
+                linewidth=0.5,
+            )
+            plt.scatter(
+                center[:, 0],
+                center[:, 1],
+                marker="o",
+                color="None",
+                edgecolors="r",
+                s=50,
+                linewidth=0.5,
+            )
+        k = 0
+        for i in pos:
+            if i in s:
+                if style == "side":
+                    plt.text(
+                        pos[i][0],
+                        pos[i][1],
+                        i,
+                        fontsize=5,
+                        verticalalignment="top",
+                        horizontalalignment="right",
+                    )
+                elif style == "center":
+                    plt.text(
+                        pos[i][0],
+                        pos[i][1],
+                        i,
+                        fontsize=5,
+                        verticalalignment="center",
+                        horizontalalignment="center",
+                    )
+                k += 1
+        for i in G.edges:
+            (u, v, t) = i
+            if u in s and v in s:
+                p1 = [pos[i[0]][0], pos[i[1]][0]]
+                p2 = [pos[i[0]][1], pos[i[1]][1]]
+                plt.plot(p1, p2, color="skyblue", linestyle="-", alpha=0.3, linewidth=3)
+        plt.show()
+    return
+
+
+def draw_kamada_kawai(G, rate=1, style="side"):
+    """Draw the graph G with a Kamada-Kawai force-directed layout.
+
+    Parameters
+    ----------
+    G : graph
+       A easygraph graph
+
+    rate : float
+       The proportion of visible points and edges to the total
+
+    style : string
+        "side"- the label is next to the dot
+        "center"- the label is in the center of the dot
+
+    """
+    import matplotlib.pyplot as plt
+    import numpy as np
+
+    pos = eg.kamada_kawai_layout(G)
+    node = np.zeros((len(pos), 2), float)
+    m, n = 0, 0
+    if rate == 1:
+        for i in pos:
+            node[m][0] = pos[i][0]
+            node[m][1] = pos[i][1]
+            m += 1
+        if style == "side":
+            plt.scatter(node[:, 0], node[:, 1], marker=".", color="b", s=10)
+        elif style == "center":
+            plt.scatter(
+                node[:, 0],
+                node[:, 1],
+                marker="o",
+                color="None",
+                edgecolors="b",
+                s=50,
+                linewidth=0.5,
+            )
+        k = 0
+        for i in pos:
+            if style == "side":
+                plt.text(
+                    pos[i][0],
+                    pos[i][1],
+                    i,
+                    fontsize=5,
+                    verticalalignment="top",
+                    horizontalalignment="right",
+                )
+            elif style == "center":
+                plt.text(
+                    pos[i][0],
+                    pos[i][1],
+                    i,
+                    fontsize=5,
+                    verticalalignment="center",
+                    horizontalalignment="center",
+                )
+            k += 1
+        for i in G.edges:
+            p1 = [pos[i[0]][0], pos[i[1]][0]]
+            p2 = [pos[i[0]][1], pos[i[1]][1]]
+            plt.plot(p1, p2, "k-", alpha=0.3, linewidth=0.5)
+        plt.show()
+    else:
+        degree = G.degree()
+        sorted_degree = sorted(degree.items(), key=lambda d: d[1], reverse=True)
+        l = int(rate * len(G))
+        s = []
+        for i in sorted_degree:
+            if len(s) < l:
+                s.append(i[0])
+        for i in pos:
+            if i in s:
+                node[m][0] = pos[i][0]
+                node[m][1] = pos[i][1]
+                m += 1
+        node = node[0:m, :]
+        if style == "side":
+            plt.scatter(node[:, 0], node[:, 1], marker=".", color="b", s=10)
+        elif style == "center":
+            plt.scatter(
+                node[:, 0],
+                node[:, 1],
+                marker="o",
+                color="None",
+                edgecolors="b",
+                s=50,
+                linewidth=0.5,
+            )
+        k = 0
+        for i in pos:
+            if i in s:
+                if style == "side":
+                    plt.text(
+                        pos[i][0],
+                        pos[i][1],
+                        i,
+                        fontsize=5,
+                        verticalalignment="top",
+                        horizontalalignment="right",
+                    )
+                elif style == "center":
+                    plt.text(
+                        pos[i][0],
+                        pos[i][1],
+                        i,
+                        fontsize=5,
+                        verticalalignment="center",
+                        horizontalalignment="center",
+                    )
+                k += 1
+        for i in G.edges:
+            (u, v, t) = i
+            if u in s and v in s:
+                p1 = [pos[i[0]][0], pos[i[1]][0]]
+                p2 = [pos[i[0]][1], pos[i[1]][1]]
+                plt.plot(p1, p2, "k-", alpha=0.3, linewidth=0.5)
+        plt.show()
+    return
+
+
+def draw_louvain_com(G, l_com):
+    """
+    Draw the graph and show the communities
+
+    Parameters
+    ----------
+    G : graph
+    l_com : communities created by louvain algorithm
+    """
+    import matplotlib.pyplot as plt
+    import numpy as np
+
+    plt.figure(figsize=(8, 8))
+    n = len(l_com)
+    colors = get_n_colors(n + 1)
+    com_pos = community_pos(n)
+    node = np.zeros((len(G.nodes), 2), float)
+    node_idx = np.zeros(len(G.nodes) + 1)
+    edge_label = edge_partition(G, l_com)
+    k = 0
+
+    for i in range(n):
+        n_pos = node_pos(len(l_com[i]))
+        com_list = list(l_com[i])
+        m = len(com_list)
+        start = k
+        for j in range(m):
+            node[k][0] = com_pos[i][0] + n_pos[j][0]
+            node[k][1] = com_pos[i][1] + n_pos[j][1]
+            node_idx[com_list[j]] = k
+            k += 1
+        plt.scatter(
+            node[start:k, 0],
+            node[start:k, 1],
+            marker="o",
+            color=colors[i],
+            edgecolors=colors[i],
+            s=300,
+            linewidth=0.5,
+            zorder=2,
+        )
+        for j in range(m):
+            x = int(node_idx[com_list[j]])
+            plt.text(
+                node[x][0],
+                node[x][1],
+                com_list[j],
+                fontsize=10,
+                verticalalignment="center",
+                horizontalalignment="center",
+                color="white",
+            )
+    for i in G.edges:
+        x = int(node_idx[int(i[0])])
+        y = int(node_idx[int(i[1])])
+        p1 = [node[x][0], node[y][0]]
+        p2 = [node[x][1], node[y][1]]
+        plt.plot(
+            p1,
+            p2,
+            color=colors[edge_label[(i[0], i[1])]],
+            linestyle="-",
+            alpha=0.3,
+            linewidth=1.5,
+            zorder=1,
+        )
+    plt.show()
+    return
+
+
+def draw_lpa_com(G, lpa_com):
+    """
+    Draw the graph and show the communities
+
+    Parameters
+    ----------
+    G : graph
+    lpa_com : communities created by LPA
+    """
+    import matplotlib.pyplot as plt
+    import numpy as np
+
+    plt.figure(figsize=(8, 8))
+    list_lpa_com = list(lpa_com.values())
+    n = len(list_lpa_com)
+    colors = get_n_colors(n + 1)
+    com_pos = community_pos(n)
+    node = np.zeros((len(G.nodes), 2), float)
+    node_idx = np.zeros(len(G.nodes) + 1)
+    edge_label = edge_partition(G, list_lpa_com)
+    k = 0
+
+    for i in range(n):
+        cur_com = list_lpa_com[i]
+        m = len(cur_com)
+        n_pos = node_pos(m)
+        start = k
+        for j in range(m):
+            node[k][0] = com_pos[i][0] + n_pos[j][0]
+            node[k][1] = com_pos[i][1] + n_pos[j][1]
+            node_idx[cur_com[j]] = k
+            k += 1
+        plt.scatter(
+            node[start:k, 0],
+            node[start:k, 1],
+            marker="o",
+            color=colors[i],
+            edgecolors=colors[i],
+            s=300,
+            linewidth=0.5,
+            zorder=2,
+        )
+        for j in range(m):
+            x = int(node_idx[cur_com[j]])
+            plt.text(
+                node[x][0],
+                node[x][1],
+                cur_com[j],
+                fontsize=10,
+                verticalalignment="center",
+                horizontalalignment="center",
+                color="white",
+            )
+    for i in G.edges:
+        x = int(node_idx[int(i[0])])
+        y = int(node_idx[int(i[1])])
+        p1 = [node[x][0], node[y][0]]
+        p2 = [node[x][1], node[y][1]]
+        plt.plot(
+            p1,
+            p2,
+            color=colors[edge_label[(i[0], i[1])]],
+            linestyle="-",
+            alpha=0.3,
+            linewidth=1.5,
+            zorder=1,
+        )
+    plt.show()
+    return
+
+
+def draw_gm_com(G, gm_com):
+    """
+    Draw the graph and show the communities
+
+    Parameters
+    ----------
+    G : graph
+    gm_com : communities created by greedy modularity
+    """
+    import matplotlib.pyplot as plt
+    import numpy as np
+
+    plt.figure(figsize=(8, 8))
+    list_gm_com = [list(i) for i in gm_com]
+    n = len(list_gm_com)
+    colors = get_n_colors(n + 1)
+    com_pos = community_pos(n)
+    node = np.zeros((len(G.nodes), 2), float)
+    node_idx = np.zeros(len(G.nodes) + 1)
+    edge_label = edge_partition(G, list_gm_com)
+    k = 0
+
+    for i in range(n):
+        cur_com = list_gm_com[i]
+        m = len(cur_com)
+        n_pos = node_pos(m)
+        start = k
+        for j in range(m):
+            node[k][0] = com_pos[i][0] + n_pos[j][0]
+            node[k][1] = com_pos[i][1] + n_pos[j][1]
+            node_idx[cur_com[j]] = k
+            k += 1
+        plt.scatter(
+            node[start:k, 0],
+            node[start:k, 1],
+            marker="o",
+            color=colors[i],
+            edgecolors=colors[i],
+            s=300,
+            linewidth=0.5,
+            zorder=2,
+        )
+        for j in range(m):
+            x = int(node_idx[cur_com[j]])
+            plt.text(
+                node[x][0],
+                node[x][1],
+                cur_com[j],
+                fontsize=10,
+                verticalalignment="center",
+                horizontalalignment="center",
+                color="white",
+            )
+    for i in G.edges:
+        x = int(node_idx[int(i[0])])
+        y = int(node_idx[int(i[1])])
+        p1 = [node[x][0], node[y][0]]
+        p2 = [node[x][1], node[y][1]]
+        plt.plot(
+            p1,
+            p2,
+            color=colors[edge_label[(i[0], i[1])]],
+            linestyle="-",
+            alpha=0.3,
+            linewidth=1.5,
+            zorder=1,
+        )
+    plt.show()
+    return
+
+
+def get_n_colors(n):
+    import numpy as np
+
+    from matplotlib import cm
+
+    viridis = cm.get_cmap("viridis", n)
+    colors = viridis(np.linspace(0, 1, n))
+    return colors
+
+
+def community_pos(n, scale=10):
+    """
+    Set position for every community.
+
+    Parameters
+    ----------
+    n : number of communities
+    scale : parameter for sprint_layout
+    """
+    graph = eg.Graph()
+    graph.add_nodes(range(n))
+    pos = eg.spring_layout(graph, scale=scale)
+    return pos
+
+
+def node_pos(n, scale=2):
+    """
+    Set position for every node in a community
+
+    Parameters
+    ----------
+    n : number of nodes
+    scale : parameter for sprint_layout
+    """
+    graph = eg.Graph()
+    graph.add_nodes(range(n))
+    pos = eg.spring_layout(graph, scale=scale)
+    return pos
+
+
+def edge_partition(G, community):
+    """
+    Label every edge with the community it belongs to.
+
+    Parameters
+    ----------
+    G : the graph
+    community : communities of the graph
+    """
+    edge_label = {}
+    n = len(community)
+    for edge in G.edges:
+        for i in range(n):
+            if edge[0] in community[i] and edge[1] in community[i]:
+                edge_label[(edge[0], edge[1])] = i
+                break
+            elif edge[0] in community[i] or edge[1] in community[i]:
+                edge_label[(edge[0], edge[1])] = n
+                break
+    return edge_label
+
+
+def draw_ego_graph(G, ego_graph):
+    import matplotlib.pyplot as plt
+    import numpy as np
+
+    plt.figure(figsize=(10, 10))
+    pos = eg.random_position(G)
+    center = np.zeros((len(ego_graph), 2), float)
+    node = np.zeros((len(pos) - len(ego_graph), 2), float)
+    m, n = 0, 0
+    for i in pos:
+        if i in list(ego_graph.nodes.keys()):
+            center[n][0] = 0.5 + (-1) ** np.random.randint(1, 2) * pos[i][0] / 3
+            center[n][1] = 0.5 + (-1) ** np.random.randint(1, 2) * pos[i][1] / 3
+            pos[i][0] = center[n][0]
+            pos[i][1] = center[n][1]
+            n += 1
+        else:
+            node[m][0] = pos[i][0]
+            node[m][1] = pos[i][1]
+            m += 1
+    plt.scatter(
+        node[:, 0],
+        node[:, 1],
+        marker="o",
+        color="skyblue",
+        edgecolors="skyblue",
+        s=100,
+        linewidth=0.5,
+    )
+    plt.scatter(
+        center[:, 0],
+        center[:, 1],
+        marker="o",
+        color="tomato",
+        edgecolors="tomato",
+        s=200,
+        linewidth=0.5,
+        zorder=2,
+    )
+    k = 0
+    for i in pos:
+        plt.text(
+            pos[i][0],
+            pos[i][1],
+            i,
+            fontsize=10,
+            verticalalignment="center",
+            horizontalalignment="center",
+        )
+        k += 1
+    for i in G.edges:
+        p1 = [pos[i[0]][0], pos[i[1]][0]]
+        p2 = [pos[i[0]][1], pos[i[1]][1]]
+        if i not in ego_graph.edges:
+            plt.plot(
+                p1,
+                p2,
+                color="skyblue",
+                linestyle="-",
+                alpha=0.3,
+                linewidth=1.8,
+                zorder=1,
+            )
+        else:
+            plt.plot(
+                p1,
+                p2,
+                color="tomato",
+                linestyle="-",
+                alpha=0.3,
+                linewidth=1.8,
+                zorder=1,
+            )
+    plt.show()
+    return
+
+
+if __name__ == "__main__":
+    G = eg.datasets.get_graph_karateclub()
+    draw_SHS_center(G, [1, 33, 34], style="side")
+    draw_SHS_center(G, [1, 33, 34], style="center")
+    draw_SHS_center_kk(G, [1, 33, 34], style="side")
+    draw_SHS_center_kk(G, [1, 33, 34], style="center")
+    draw_kamada_kawai(G, style="side")
+    draw_kamada_kawai(G, style="center")
+    draw_SHS_center(G, [1, 33, 34], rate=0.8, style="side")
+    draw_SHS_center(G, [1, 33, 34], rate=0.8, style="center")
+    draw_SHS_center_kk(G, [1, 33, 34], rate=0.8, style="side")
+    draw_SHS_center_kk(G, [1, 33, 34], rate=0.8, style="center")
+    draw_kamada_kawai(G, rate=0.8, style="side")
+    draw_kamada_kawai(G, rate=0.8, style="center")
```

## easygraph/functions/drawing/layout.py

 * *Ordering differences only*

```diff
@@ -1,33 +1,33 @@
-from typing import List
-
-from .simulator import Simulator
-from .utils import edge_list_to_incidence_matrix
-from .utils import init_pos
-
-
-def force_layout(
-    num_v: int,
-    e_list: List[tuple],
-    push_v_strength: float,
-    push_e_strength: float,
-    pull_e_strength: float,
-    pull_center_strength: float,
-):
-    import numpy as np
-
-    v_coor = init_pos(num_v, scale=5)
-    assert v_coor.max() <= 5.0 and v_coor.min() >= -5.0
-    centers = [np.array([0, 0])]
-    sim = Simulator(
-        nums=num_v,
-        forces={
-            Simulator.NODE_ATTRACTION: pull_e_strength,
-            Simulator.NODE_REPULSION: push_v_strength,
-            Simulator.EDGE_REPULSION: push_e_strength,
-            Simulator.CENTER_GRAVITY: pull_center_strength,
-        },
-        centers=centers,
-    )
-    v_coor = sim.simulate(v_coor, edge_list_to_incidence_matrix(num_v, e_list))
-    v_coor = (v_coor - v_coor.min(0)) / (v_coor.max(0) - v_coor.min(0)) * 0.8 + 0.1
-    return v_coor
+from typing import List
+
+from .simulator import Simulator
+from .utils import edge_list_to_incidence_matrix
+from .utils import init_pos
+
+
+def force_layout(
+    num_v: int,
+    e_list: List[tuple],
+    push_v_strength: float,
+    push_e_strength: float,
+    pull_e_strength: float,
+    pull_center_strength: float,
+):
+    import numpy as np
+
+    v_coor = init_pos(num_v, scale=5)
+    assert v_coor.max() <= 5.0 and v_coor.min() >= -5.0
+    centers = [np.array([0, 0])]
+    sim = Simulator(
+        nums=num_v,
+        forces={
+            Simulator.NODE_ATTRACTION: pull_e_strength,
+            Simulator.NODE_REPULSION: push_v_strength,
+            Simulator.EDGE_REPULSION: push_e_strength,
+            Simulator.CENTER_GRAVITY: pull_center_strength,
+        },
+        centers=centers,
+    )
+    v_coor = sim.simulate(v_coor, edge_list_to_incidence_matrix(num_v, e_list))
+    v_coor = (v_coor - v_coor.min(0)) / (v_coor.max(0) - v_coor.min(0)) * 0.8 + 0.1
+    return v_coor
```

## easygraph/functions/drawing/geometry.py

 * *Ordering differences only*

```diff
@@ -1,41 +1,41 @@
-import math
-
-from math import pi
-
-
-def radian_from_atan(x, y):
-    if x == 0:
-        return pi / 2 if y > 0 else 3 * pi / 2
-    if y == 0:
-        return 0 if x > 0 else pi
-    r = math.atan(y / x)
-    if x > 0 and y > 0:
-        return r
-    elif x > 0 and y < 0:
-        return r + 2 * pi
-    elif x < 0 and y > 0:
-        return r + pi
-    else:
-        return r + pi
-
-
-def vlen(vector):
-    return math.sqrt(vector[0] ** 2 + vector[1] ** 2)
-
-
-def common_tangent_radian(r1, r2, d):
-    alpha = math.acos(abs(r2 - r1) / d)
-    alpha = alpha if r1 > r2 else pi - alpha
-    return alpha
-
-
-def polar_position(r, theta, start_point):
-    import numpy as np
-
-    x = r * math.cos(theta)
-    y = r * math.sin(theta)
-    return np.array([x, y]) + start_point
-
-
-def rad_2_deg(rad):
-    return rad * 180 / pi
+import math
+
+from math import pi
+
+
+def radian_from_atan(x, y):
+    if x == 0:
+        return pi / 2 if y > 0 else 3 * pi / 2
+    if y == 0:
+        return 0 if x > 0 else pi
+    r = math.atan(y / x)
+    if x > 0 and y > 0:
+        return r
+    elif x > 0 and y < 0:
+        return r + 2 * pi
+    elif x < 0 and y > 0:
+        return r + pi
+    else:
+        return r + pi
+
+
+def vlen(vector):
+    return math.sqrt(vector[0] ** 2 + vector[1] ** 2)
+
+
+def common_tangent_radian(r1, r2, d):
+    alpha = math.acos(abs(r2 - r1) / d)
+    alpha = alpha if r1 > r2 else pi - alpha
+    return alpha
+
+
+def polar_position(r, theta, start_point):
+    import numpy as np
+
+    x = r * math.cos(theta)
+    y = r * math.sin(theta)
+    return np.array([x, y]) + start_point
+
+
+def rad_2_deg(rad):
+    return rad * 180 / pi
```

## easygraph/functions/drawing/__init__.py

 * *Ordering differences only*

```diff
@@ -1,3 +1,3 @@
-from .drawing import *
-from .plot import *
-from .positioning import *
+from .drawing import *
+from .plot import *
+from .positioning import *
```

## easygraph/functions/structural_holes/MaxD.py

 * *Ordering differences only*

```diff
@@ -1,447 +1,447 @@
-from typing import List
-
-from easygraph.utils import *
-
-
-__all__ = ["get_structural_holes_MaxD"]
-
-
-@not_implemented_for("multigraph")
-def get_community_kernel(G, C: List[frozenset], weight="weight"):
-    """
-    To get community kernels with most degrees.
-    Parameters
-    ----------
-    G : graph
-        An undirected graph.
-    C : int
-        #communities
-
-    Returns
-    -------
-    kernels
-    """
-    area = []
-    for i in range(len(G)):
-        area.append(0)
-    for i, cc in enumerate(C):
-        for each_node in cc:
-            area[each_node - 1] += 1 << i  # node_id from 1 to n.
-    kernels = []
-    cnt = 0
-    for i in range(len(C)):
-        mask = 1 << i
-        cnt += 1
-        q = []
-        p = []
-        for i in range(len(G)):
-            if (area[i] & mask) == mask:
-                q.append((G.degree(weight=weight)[i + 1], i + 1))
-        q.sort()
-        q.reverse()
-        for i in range(
-            max(int(len(q) / 100), min(2, len(q)))
-        ):  # latter of min for test.
-            p.append(q[i][1])
-        kernels.append(p)
-    if len(kernels) < 2:
-        print("ERROR: WE should have at least 2 communities.")
-    for i in range(len(kernels)):
-        if len(kernels[i]) == 0:
-            print("Community %d is too small." % i)
-            return None
-    return kernels
-
-
-def get_structural_holes_MaxD(G, k, C: List[frozenset]):
-    """Structural hole spanners detection via MaxD method.
-
-    Both **HIS** and **MaxD** are methods in [1]_.
-    The authors developed these two methods to find the structural holes spanners,
-    based on theory of information diffusion.
-
-    Parameters
-    ----------
-
-    k : int
-        Top-`k` structural hole spanners
-
-    C : list of frozenset
-        Each frozenset denotes a community of nodes.
-
-    Returns
-    -------
-    get_structural_holes_MaxD : list
-        Top-`k` structural hole spanners
-
-    Examples
-    --------
-
-    >>> get_structural_holes_MaxD(G,
-    ...                           k = 5, # To find top five structural holes spanners.
-    ...                           C = [frozenset([1,2,3]), frozenset([4,5,6])] # Two communities
-    ...                           )
-
-
-    References
-    ----------
-    .. [1] https://www.aminer.cn/structural-hole
-
-    """
-    _init_data()
-
-    G_index, index_of_node, node_of_index = G.to_index_node_graph(begin_index=1)
-    C_index = []
-    for cmnt in C:
-        cmnt_index = []
-        for node in cmnt:
-            cmnt_index.append(index_of_node[node])
-        C_index.append(frozenset(cmnt_index))
-
-    kernels = get_community_kernel(G_index, C_index)
-    c = len(kernels)
-    save = []
-    for i in range(len(G_index)):
-        save.append(False)
-
-    build_network(kernels, c, G_index)
-
-    n = len(G_index)
-    sflow = []
-    save = []
-    for i in range(n):
-        save.append(True)
-    q = []
-    ans_list = []
-    for step in range(k):
-        q.clear()
-        sflow.clear()
-        for i in range(n):
-            sflow.append(0)
-        max_flow(n, kernels, save)
-        for i in range(n * (c - 1)):
-            k_ = head[i]
-            while k_ >= 0:
-                if flow[k_] > 0:
-                    sflow[i % n] += flow[k_]
-                k_ = nex[k_]
-        for i in range(n):
-            if save[i] == False:
-                q.append((-1, i))
-            else:
-                q.append((sflow[i] + G_index.degree(weight="weight")[i + 1], i))
-        q.sort()
-        q.reverse()
-        candidates = []
-        for i in range(n):
-            if save[q[i][1]] == True and len(candidates) < k:
-                candidates.append(q[i][1])
-        ret = pick_candidates(n, candidates, kernels, save)
-        ans_list.append(ret[1] + 1)
-    del sflow
-    del q
-
-    for i in range(len(ans_list)):
-        ans_list[i] = node_of_index[ans_list[i]]
-
-    return ans_list
-
-
-def pick_candidates(n, candidates, kernels, save):
-    """
-    detect candidates.
-    Parameters
-    ----------
-    n : #nodes
-    candidates : A list of candidates.
-    kernels : A list of kernels
-    save : A bool list of visited candidates for max_flow.
-
-    Returns
-    -------
-    A tuple of min_cut, best_candidate of this round.
-    """
-    for i in range(len(candidates)):
-        save[candidates[i]] = False
-    old_flow = max_flow(n, kernels, save)
-    global prev_flow
-    prev_flow.clear()
-    for i in range(nedge):
-        prev_flow.append(flow[i])
-    mcut = 100000000
-    best_key = -1
-    for i in range(len(candidates)):
-        key = candidates[i]
-        for j in range(len(candidates)):
-            save[candidates[j]] = True
-        save[key] = False
-        tp = max_flow(n, kernels, save, prev_flow)
-        if tp < mcut:
-            mcut = tp
-            best_key = key
-    for i in range(len(candidates)):
-        save[candidates[i]] = True
-        save[best_key] = False
-    return (old_flow + mcut, best_key)
-
-
-head = []
-
-point = []
-nex = []
-flow = []
-capa = []
-
-dist = []
-work = []
-dsave = []
-
-src = 0
-dest = 0
-node = 0
-nedge = 0
-prev_flow = []
-oo = 1000000000
-
-
-def _init_data():
-    global head, point, nex, flow, capa
-    global dist, work, dsave
-    global src, dest, node, nedge, prev_flow, oo
-
-    head = []
-
-    point = []
-    nex = []
-    flow = []
-    capa = []
-
-    dist = []
-    work = []
-    dsave = []
-
-    src = 0
-    dest = 0
-    node = 0
-    nedge = 0
-    prev_flow = []
-    oo = 1000000000
-
-
-def dinic_bfs():
-    """
-    using BFS to find augmenting basic.
-
-    Returns
-    -------
-    A bool, whether found a augmenting basic or not.
-    """
-    global dist, dest, src, node
-    dist.clear()
-    for i in range(node):
-        dist.append(-1)
-    dist[src] = 0
-    Q = []
-    Q.append(src)
-    cl = 0
-    while cl < len(Q):
-        k_ = Q[cl]
-        i = head[k_]
-        while i >= 0:
-            if flow[i] < capa[i] and dsave[point[i]] == True and dist[point[i]] < 0:
-                dist[point[i]] = dist[k_] + 1
-                Q.append(point[i])
-            i = nex[i]
-        cl += 1
-    return dist[dest] >= 0
-
-
-def dinic_dfs(x, exp):
-    """
-    using DFS to calc the augmenting basic and refresh network.
-    Parameters
-    ----------
-    x : current node.
-    exp : current flow.
-
-    Returns
-    -------
-    current flow.
-    """
-    if x == dest:
-        return exp
-    res = 0
-    i = work[x]
-    global flow
-    while i >= 0:
-        v = point[i]
-        tmp = 0
-        if flow[i] < capa[i] and dist[v] == dist[x] + 1:
-            tmp = dinic_dfs(v, min(exp, capa[i] - flow[i]))
-            if tmp > 0:
-                flow[i] += tmp
-                flow[i ^ 1] -= tmp
-                res += tmp
-                exp -= tmp
-                if exp == 0:
-                    break
-        i = nex[i]
-    return res
-
-
-def dinic_flow():
-    """
-    Dinic algorithm to calc max_flow.
-
-    Returns
-    -------
-    max_flow.
-    """
-    result = 0
-    global work
-    while dinic_bfs():
-        work.clear()
-        for i in range(node):
-            work.append(head[i])
-        result += dinic_dfs(src, oo)
-    return result
-
-
-def max_flow(n, kernels, save, prev_flow=None):
-    """
-    Calculate max_flow.
-    Parameters
-    ----------
-    n : #nodes
-    kernels : A list of kernels.
-    save : A bool list of visited nodes.
-    prev_flow : A list of previous flows.
-
-    Returns
-    -------
-    max_flow
-    """
-    global dsave, node
-    dsave.clear()
-    for i in range(node):
-        dsave.append(True)
-
-    if prev_flow != None:
-        for i in range(nedge):
-            flow.append(prev_flow[i])
-    else:
-        for i in range(nedge):
-            flow.append(0)
-
-    c = len(kernels)
-    for i in range(n):
-        for k_ in range(c - 1):
-            dsave[k_ * n + i] = save[i]
-    ret = dinic_flow()
-    return ret
-
-
-def init_MaxD(_node, _src, _dest):
-    """
-    Initialize a network.
-    Parameters
-    ----------
-    _node : #nodes
-    _src : the source node
-    _dest : the destiny node
-
-    Returns
-    -------
-    void
-    """
-    global node, src, dest
-    node = _node
-    src = _src
-    dest = _dest
-    global point, capa, flow, nex, head
-    head.clear()
-    for i in range(node):
-        head.append(-1)
-    nedge = 0
-    point.clear()
-    capa.clear()
-    flow.clear()
-    nex.clear()
-
-    return
-
-
-def addedge(u, v, c1, c2):
-    """
-    Add an edge(u,v) with capacity c1 and inverse capacity c2.
-    Parameters
-    ----------
-    u : node u
-    v : node v
-    c1 : capacity c1
-    c2 : capacity c2
-
-    Returns
-    -------
-    void
-    """
-    global nedge
-    global point, capa, flow, nex, head
-    point.append(v)
-    capa.append(c1)
-    flow.append(0)
-    nex.append(head[u])
-    head[u] = nedge
-    nedge += 1
-
-    point.append(u)
-    capa.append(c2)
-    flow.append(0)
-    nex.append(head[v])
-    head[v] = nedge
-    nedge += 1
-    return
-
-
-def build_network(kernels, c, G):
-    """
-    build a network.
-    Parameters
-    ----------
-    kernels : A list of kernels.
-    c : #communities.
-    G : graph
-        An undirected graph.
-
-    Returns
-    -------
-    void
-    """
-    n = len(G)
-    init_MaxD(n * (c - 1) + 2, n * (c - 1), n * (c - 1) + 1)
-
-    base = 0
-    for k_iter in range(c):
-        S1 = set()
-        S2 = set()
-        for i in range(c):
-            for j in range(len(kernels[i])):
-                if i == k_iter:
-                    S1.add(kernels[i][j])
-                elif i < k_iter:
-                    S2.add(kernels[i][j])
-        if len(S1) == 0 or len(S2) == 0:
-            continue
-
-        for edges in G.edges:
-            addedge(base + edges[0] - 1, base + edges[1] - 1, 1, 1)
-            addedge(base + edges[1] - 1, base + edges[0] - 1, 1, 1)
-
-        for i in S1:
-            if i not in S2:
-                addedge(src, base + i - 1, n, 0)
-        for i in S2:
-            if i not in S1:
-                addedge(base + i - 1, dest, n, 0)
-        base += n
-    return
+from typing import List
+
+from easygraph.utils import *
+
+
+__all__ = ["get_structural_holes_MaxD"]
+
+
+@not_implemented_for("multigraph")
+def get_community_kernel(G, C: List[frozenset], weight="weight"):
+    """
+    To get community kernels with most degrees.
+    Parameters
+    ----------
+    G : graph
+        An undirected graph.
+    C : int
+        #communities
+
+    Returns
+    -------
+    kernels
+    """
+    area = []
+    for i in range(len(G)):
+        area.append(0)
+    for i, cc in enumerate(C):
+        for each_node in cc:
+            area[each_node - 1] += 1 << i  # node_id from 1 to n.
+    kernels = []
+    cnt = 0
+    for i in range(len(C)):
+        mask = 1 << i
+        cnt += 1
+        q = []
+        p = []
+        for i in range(len(G)):
+            if (area[i] & mask) == mask:
+                q.append((G.degree(weight=weight)[i + 1], i + 1))
+        q.sort()
+        q.reverse()
+        for i in range(
+            max(int(len(q) / 100), min(2, len(q)))
+        ):  # latter of min for test.
+            p.append(q[i][1])
+        kernels.append(p)
+    if len(kernels) < 2:
+        print("ERROR: WE should have at least 2 communities.")
+    for i in range(len(kernels)):
+        if len(kernels[i]) == 0:
+            print("Community %d is too small." % i)
+            return None
+    return kernels
+
+
+def get_structural_holes_MaxD(G, k, C: List[frozenset]):
+    """Structural hole spanners detection via MaxD method.
+
+    Both **HIS** and **MaxD** are methods in [1]_.
+    The authors developed these two methods to find the structural holes spanners,
+    based on theory of information diffusion.
+
+    Parameters
+    ----------
+
+    k : int
+        Top-`k` structural hole spanners
+
+    C : list of frozenset
+        Each frozenset denotes a community of nodes.
+
+    Returns
+    -------
+    get_structural_holes_MaxD : list
+        Top-`k` structural hole spanners
+
+    Examples
+    --------
+
+    >>> get_structural_holes_MaxD(G,
+    ...                           k = 5, # To find top five structural holes spanners.
+    ...                           C = [frozenset([1,2,3]), frozenset([4,5,6])] # Two communities
+    ...                           )
+
+
+    References
+    ----------
+    .. [1] https://www.aminer.cn/structural-hole
+
+    """
+    _init_data()
+
+    G_index, index_of_node, node_of_index = G.to_index_node_graph(begin_index=1)
+    C_index = []
+    for cmnt in C:
+        cmnt_index = []
+        for node in cmnt:
+            cmnt_index.append(index_of_node[node])
+        C_index.append(frozenset(cmnt_index))
+
+    kernels = get_community_kernel(G_index, C_index)
+    c = len(kernels)
+    save = []
+    for i in range(len(G_index)):
+        save.append(False)
+
+    build_network(kernels, c, G_index)
+
+    n = len(G_index)
+    sflow = []
+    save = []
+    for i in range(n):
+        save.append(True)
+    q = []
+    ans_list = []
+    for step in range(k):
+        q.clear()
+        sflow.clear()
+        for i in range(n):
+            sflow.append(0)
+        max_flow(n, kernels, save)
+        for i in range(n * (c - 1)):
+            k_ = head[i]
+            while k_ >= 0:
+                if flow[k_] > 0:
+                    sflow[i % n] += flow[k_]
+                k_ = nex[k_]
+        for i in range(n):
+            if save[i] == False:
+                q.append((-1, i))
+            else:
+                q.append((sflow[i] + G_index.degree(weight="weight")[i + 1], i))
+        q.sort()
+        q.reverse()
+        candidates = []
+        for i in range(n):
+            if save[q[i][1]] == True and len(candidates) < k:
+                candidates.append(q[i][1])
+        ret = pick_candidates(n, candidates, kernels, save)
+        ans_list.append(ret[1] + 1)
+    del sflow
+    del q
+
+    for i in range(len(ans_list)):
+        ans_list[i] = node_of_index[ans_list[i]]
+
+    return ans_list
+
+
+def pick_candidates(n, candidates, kernels, save):
+    """
+    detect candidates.
+    Parameters
+    ----------
+    n : #nodes
+    candidates : A list of candidates.
+    kernels : A list of kernels
+    save : A bool list of visited candidates for max_flow.
+
+    Returns
+    -------
+    A tuple of min_cut, best_candidate of this round.
+    """
+    for i in range(len(candidates)):
+        save[candidates[i]] = False
+    old_flow = max_flow(n, kernels, save)
+    global prev_flow
+    prev_flow.clear()
+    for i in range(nedge):
+        prev_flow.append(flow[i])
+    mcut = 100000000
+    best_key = -1
+    for i in range(len(candidates)):
+        key = candidates[i]
+        for j in range(len(candidates)):
+            save[candidates[j]] = True
+        save[key] = False
+        tp = max_flow(n, kernels, save, prev_flow)
+        if tp < mcut:
+            mcut = tp
+            best_key = key
+    for i in range(len(candidates)):
+        save[candidates[i]] = True
+        save[best_key] = False
+    return (old_flow + mcut, best_key)
+
+
+head = []
+
+point = []
+nex = []
+flow = []
+capa = []
+
+dist = []
+work = []
+dsave = []
+
+src = 0
+dest = 0
+node = 0
+nedge = 0
+prev_flow = []
+oo = 1000000000
+
+
+def _init_data():
+    global head, point, nex, flow, capa
+    global dist, work, dsave
+    global src, dest, node, nedge, prev_flow, oo
+
+    head = []
+
+    point = []
+    nex = []
+    flow = []
+    capa = []
+
+    dist = []
+    work = []
+    dsave = []
+
+    src = 0
+    dest = 0
+    node = 0
+    nedge = 0
+    prev_flow = []
+    oo = 1000000000
+
+
+def dinic_bfs():
+    """
+    using BFS to find augmenting basic.
+
+    Returns
+    -------
+    A bool, whether found a augmenting basic or not.
+    """
+    global dist, dest, src, node
+    dist.clear()
+    for i in range(node):
+        dist.append(-1)
+    dist[src] = 0
+    Q = []
+    Q.append(src)
+    cl = 0
+    while cl < len(Q):
+        k_ = Q[cl]
+        i = head[k_]
+        while i >= 0:
+            if flow[i] < capa[i] and dsave[point[i]] == True and dist[point[i]] < 0:
+                dist[point[i]] = dist[k_] + 1
+                Q.append(point[i])
+            i = nex[i]
+        cl += 1
+    return dist[dest] >= 0
+
+
+def dinic_dfs(x, exp):
+    """
+    using DFS to calc the augmenting basic and refresh network.
+    Parameters
+    ----------
+    x : current node.
+    exp : current flow.
+
+    Returns
+    -------
+    current flow.
+    """
+    if x == dest:
+        return exp
+    res = 0
+    i = work[x]
+    global flow
+    while i >= 0:
+        v = point[i]
+        tmp = 0
+        if flow[i] < capa[i] and dist[v] == dist[x] + 1:
+            tmp = dinic_dfs(v, min(exp, capa[i] - flow[i]))
+            if tmp > 0:
+                flow[i] += tmp
+                flow[i ^ 1] -= tmp
+                res += tmp
+                exp -= tmp
+                if exp == 0:
+                    break
+        i = nex[i]
+    return res
+
+
+def dinic_flow():
+    """
+    Dinic algorithm to calc max_flow.
+
+    Returns
+    -------
+    max_flow.
+    """
+    result = 0
+    global work
+    while dinic_bfs():
+        work.clear()
+        for i in range(node):
+            work.append(head[i])
+        result += dinic_dfs(src, oo)
+    return result
+
+
+def max_flow(n, kernels, save, prev_flow=None):
+    """
+    Calculate max_flow.
+    Parameters
+    ----------
+    n : #nodes
+    kernels : A list of kernels.
+    save : A bool list of visited nodes.
+    prev_flow : A list of previous flows.
+
+    Returns
+    -------
+    max_flow
+    """
+    global dsave, node
+    dsave.clear()
+    for i in range(node):
+        dsave.append(True)
+
+    if prev_flow != None:
+        for i in range(nedge):
+            flow.append(prev_flow[i])
+    else:
+        for i in range(nedge):
+            flow.append(0)
+
+    c = len(kernels)
+    for i in range(n):
+        for k_ in range(c - 1):
+            dsave[k_ * n + i] = save[i]
+    ret = dinic_flow()
+    return ret
+
+
+def init_MaxD(_node, _src, _dest):
+    """
+    Initialize a network.
+    Parameters
+    ----------
+    _node : #nodes
+    _src : the source node
+    _dest : the destiny node
+
+    Returns
+    -------
+    void
+    """
+    global node, src, dest
+    node = _node
+    src = _src
+    dest = _dest
+    global point, capa, flow, nex, head
+    head.clear()
+    for i in range(node):
+        head.append(-1)
+    nedge = 0
+    point.clear()
+    capa.clear()
+    flow.clear()
+    nex.clear()
+
+    return
+
+
+def addedge(u, v, c1, c2):
+    """
+    Add an edge(u,v) with capacity c1 and inverse capacity c2.
+    Parameters
+    ----------
+    u : node u
+    v : node v
+    c1 : capacity c1
+    c2 : capacity c2
+
+    Returns
+    -------
+    void
+    """
+    global nedge
+    global point, capa, flow, nex, head
+    point.append(v)
+    capa.append(c1)
+    flow.append(0)
+    nex.append(head[u])
+    head[u] = nedge
+    nedge += 1
+
+    point.append(u)
+    capa.append(c2)
+    flow.append(0)
+    nex.append(head[v])
+    head[v] = nedge
+    nedge += 1
+    return
+
+
+def build_network(kernels, c, G):
+    """
+    build a network.
+    Parameters
+    ----------
+    kernels : A list of kernels.
+    c : #communities.
+    G : graph
+        An undirected graph.
+
+    Returns
+    -------
+    void
+    """
+    n = len(G)
+    init_MaxD(n * (c - 1) + 2, n * (c - 1), n * (c - 1) + 1)
+
+    base = 0
+    for k_iter in range(c):
+        S1 = set()
+        S2 = set()
+        for i in range(c):
+            for j in range(len(kernels[i])):
+                if i == k_iter:
+                    S1.add(kernels[i][j])
+                elif i < k_iter:
+                    S2.add(kernels[i][j])
+        if len(S1) == 0 or len(S2) == 0:
+            continue
+
+        for edges in G.edges:
+            addedge(base + edges[0] - 1, base + edges[1] - 1, 1, 1)
+            addedge(base + edges[1] - 1, base + edges[0] - 1, 1, 1)
+
+        for i in S1:
+            if i not in S2:
+                addedge(src, base + i - 1, n, 0)
+        for i in S2:
+            if i not in S1:
+                addedge(base + i - 1, dest, n, 0)
+        base += n
+    return
```

## easygraph/functions/structural_holes/maxBlock.py

 * *Ordering differences only*

```diff
@@ -1,611 +1,611 @@
-import math
-import random
-import sys
-
-import easygraph as eg
-
-from easygraph.functions.components.strongly_connected import condensation
-from easygraph.functions.components.strongly_connected import (
-    number_strongly_connected_components,
-)
-from easygraph.utils import *
-
-
-__all__ = [
-    "maxBlock",
-    "maxBlockFast",
-]
-tim = 0
-sys.setrecursionlimit(9000000)
-
-
-class dom_g:
-    def __init__(self, N, M):
-        self.tot = 0
-        self.h = []
-        self.ne = []
-        self.to = []
-        for i in range(N + 1):
-            self.h.append(0)
-        for i in range(max(N + 1, M + 1)):
-            self.ne.append(0)
-            self.to.append(0)
-
-    def add(self, x, y):
-        self.tot += 1
-        self.to[self.tot] = y
-        self.ne[self.tot] = self.h[x]
-        self.h[x] = self.tot
-
-
-def _tarjan(x, dfn, repos, g, fa):
-    global tim
-    tim += 1
-    dfn[x] = tim
-    repos[tim] = x
-    i = g.h[x]
-    while i:
-        if dfn[g.to[i]] == 0:
-            fa[g.to[i]] = x
-            _tarjan(g.to[i], dfn, repos, g, fa)
-        i = g.ne[i]
-
-
-def _find(x, f, dfn, semi, mi):
-    if x == f[x]:
-        return x
-    tmp = f[x]
-    f[x] = _find(f[x], f, dfn, semi, mi)
-    if dfn[semi[mi[tmp]]] < dfn[semi[mi[x]]]:
-        mi[x] = mi[tmp]
-    return f[x]
-
-
-def _dfs(x, tr, ans, desc_set):
-    ans[x] += 1
-    i = tr.h[x]
-
-    while i:
-        y = tr.to[i]
-        desc_set[x].add(y)
-        _dfs(y, tr, ans, desc_set)
-        ans[x] += ans[y]
-        for n in desc_set[y]:
-            desc_set[x].add(n)
-        i = tr.ne[i]
-
-
-def _get_idom(G, G_tr, node_s, ans_real, desc_set_real):
-    """Find the immediate dominator of each node and construct an s-rooted dominator tree.
-
-    Parameters
-    ----------
-    G: easygraph.DiGraph
-
-    G_tr: easygraph.DiGraph
-        an s-rooted dominator tree to be constructed.
-
-    node_s: int
-        the node s
-
-    ans_real: dict
-        denotes the number of proper descendants nu of each node u in the dominator tree.
-        a result to be calculated
-
-    desc_set_real: dict
-        denotes the set of proper descendants of node u in the dominator tree.
-        a result to be calculated
-
-    Examples
-    --------
-    # >>> G_tr = eg.DiGraph()
-    # >>> n_set = {}
-    # >>> desc_set = {}
-    # >>> _get_idom(G, G_tr, node_s, n_set, desc_set)
-
-    References
-    ----------
-    .. [1] http://keyblog.cn/article-173.html
-
-    """
-
-    global tim
-    tim = 0
-    n_dom = G.number_of_nodes()
-    m_dom = G.number_of_edges()
-    g = dom_g(n_dom + 1, m_dom + 1)
-    rg = dom_g(n_dom + 1, m_dom + 1)
-    ng = dom_g(n_dom + 1, m_dom + 1)
-    tr = dom_g(n_dom + 1, m_dom + 1)
-
-    dfn = [0 for i in range(n_dom + 1)]
-    repos = [0 for i in range(n_dom + 1)]
-    mi = [i for i in range(n_dom + 1)]
-    fa = [0 for i in range(n_dom + 1)]
-    f = [i for i in range(n_dom + 1)]
-    semi = [i for i in range(n_dom + 1)]
-    idom = [0 for i in range(n_dom + 1)]
-
-    # init
-    j = 0
-    node_map = {}
-    index_map = {}
-    for node in G.nodes:
-        j += 1
-        node_map[node] = j
-        index_map[j] = node
-
-    for edge in G.edges:
-        g.add(node_map[edge[0]], node_map[edge[1]])
-        rg.add(node_map[edge[1]], node_map[edge[0]])
-
-    # tarjan
-    _tarjan(node_map[node_s], dfn, repos, g, fa)
-    # work
-    i = n_dom
-    while i >= 2:
-        x = repos[i]
-        tmp = n_dom
-        j = rg.h[x]
-        while j:
-            if dfn[rg.to[j]] == 0:
-                j = rg.ne[j]
-                continue
-            if dfn[rg.to[j]] < dfn[x]:
-                tmp = min(tmp, dfn[rg.to[j]])
-            else:
-                _find(rg.to[j], f, dfn, semi, mi)
-                tmp = min(tmp, dfn[semi[mi[rg.to[j]]]])
-            j = rg.ne[j]
-
-        semi[x] = repos[tmp]
-        f[x] = fa[x]
-        ng.add(semi[x], x)
-        x = repos[i - 1]
-        j = ng.h[x]
-        while j:
-            y = ng.to[j]
-            _find(y, f, dfn, semi, mi)
-            if semi[mi[y]] == semi[y]:
-                idom[y] = semi[y]
-            else:
-                idom[y] = mi[y]
-            j = ng.ne[j]
-        i -= 1
-
-    i = 2
-    while i <= n_dom:
-        x = repos[i]
-        if x != 0:
-            if idom[x] != semi[x]:
-                idom[x] = idom[idom[x]]
-            tr.add(idom[x], x)
-            if x != node_map[node_s]:
-                G_tr.add_edge(index_map[idom[x]], index_map[x])
-        i += 1
-    G_tr.add_node(node_s)
-    ans = {}
-    desc_set = {}
-    for node in G_tr.nodes:
-        ans[node_map[node]] = 0
-        desc_set[node_map[node]] = set()
-
-    _dfs(node_map[node_s], tr, ans, desc_set)
-    for key in ans.keys():
-        ans[key] -= 1
-        ans_real[index_map[key]] = ans[key]
-    for key in desc_set.keys():
-        desc_set_real[index_map[key]] = set()
-        for value in desc_set[key]:
-            desc_set_real[index_map[key]].add(index_map[value])
-
-
-def _find_topk_shs_under_l(G, f_set, k, L):
-    """Find the top-k structural hole spanners under L simulations.
-
-    Parameters
-    ----------
-    G: easygraph.DiGraph
-
-    f_set: dict
-        user vi shares his/her information on network G at a rate fi.
-
-    k: int
-        top - k structural hole spanners.
-
-    L: int
-        the number of simulations.
-
-    Returns
-    -------
-    S_list : list
-        A set S of k nodes that block the maximum number of information propagations within L simulations.
-
-    ave_H_Lt_S: float
-        the average number of blocked information propagations by the nodes in set S with L t simulations.
-
-    """
-    h_set = {}
-    n = G.number_of_nodes()
-    for node in G.nodes:
-        h_set[node] = 0
-    for l in range(L):
-        if l % 100000 == 0:
-            print("[", l, "/", L, "] find topk shs under L")
-        # Choose a node s from the n nodes in G randomly
-        node_s = random.choice(list(G.nodes))
-        # Generate a graph G & = (V, E & ) from G under the live-edge graph model
-        G_live = G.copy()
-        for edge in G_live.edges:
-            wij = G_live[edge[0]][edge[1]]["weight"]
-            toss = random.random() + 0.1
-            if toss >= wij:
-                G_live.remove_edge(edge[0], edge[1])
-        # Obtain the induced subgraph by the set R G & (s ) of reachable nodes from s
-        R_set = eg.connected_component_of_node(G_live, node_s)
-        G_subgraph = eg.DiGraph()
-        for node in R_set:
-            G_subgraph.add_node(node)
-        for edge in G_live.edges:
-            if edge[0] in G_subgraph.nodes and edge[1] in G_subgraph.nodes:
-                G_subgraph.add_edge(edge[0], edge[1])
-        # Find the immediate dominator idom (v ) of each node v $ V && \ { s } in G
-        # Construct an s -rooted dominator tree
-        # Calculate the number of proper descendants n u of each node u $ V &&
-        G_tr = eg.DiGraph()
-        n_set = {}
-        desc_set = {}
-        _get_idom(G_subgraph, G_tr, node_s, n_set, desc_set)
-        for node_u in G_tr.nodes:
-            if node_u != node_s:
-                # the number of blocked information propagations by node u
-                h_set[node_u] += n_set[node_u] * f_set[node_s]
-    ave_H_set = {}
-    for node in G.nodes:
-        ave_H_set[node] = h_set[node] * n / L
-    ordered_set = sorted(ave_H_set.items(), key=lambda x: x[1], reverse=True)
-    S_list = []
-    ave_H_Lt_S = 0
-    for i in range(k):
-        S_list.append((ordered_set[i])[0])
-        ave_H_Lt_S += (ordered_set[i])[1]
-    return S_list, ave_H_Lt_S
-
-
-def _get_estimated_opt(G, f_set, k, c, delta):
-    """Estimation of the optimal value OPT.
-
-    Parameters
-    ----------
-    G: easygraph.DiGraph
-
-    f_set: dict
-        user vi shares his/her information on network G at a rate fi.
-
-    k: int
-        top - k structural hole spanners.
-
-    c: int
-        Success probability 1-n^-c of maxBlock.
-
-    delta: float
-        a small value delta > 0.
-
-    Returns
-    -------
-    res_opt : float
-        An approximate value OPT.
-
-    """
-    print("Estimating the optimal value OPT...")
-    n = G.number_of_nodes()
-    opt_ub = 0
-    for f_key in f_set.keys():
-        opt_ub = opt_ub + f_set[f_key]
-    opt_ub = opt_ub * k * (n - 1)
-    T = math.log((opt_ub / (delta / 2)), 2)
-    T = math.ceil(T)
-    lamda = 4 * (c * math.log(n, 2) + math.log(k * T, 2)) * (2 * k + 1) * k * n * n
-    for t in range(T):
-        opt_g = opt_ub / math.pow(2, t + 1)
-        L_t = math.ceil(lamda / opt_g)
-        print("[", t, "/", T, "] Estimating OPT: L=", L_t)
-        S_list, ave_H_Lt_S = _find_topk_shs_under_l(G, f_set, k, L_t)
-        if ave_H_Lt_S >= opt_g:
-            res_opt = opt_g / 2
-            return res_opt
-    print("[Warning] OPT is not greater that delta")
-    return -1
-
-
-def _find_separation_nodes(G):
-    G_s = condensation(G)
-    SCC_mapping = {}
-    incoming_info = G_s.graph["incoming_info"]
-    G_s_undirected = eg.Graph()
-    sep_nodes = set()
-    for node in (G_s.nodes).keys():
-        SCC_mapping[node] = G_s.nodes[node]["member"]
-        if len(G_s.nodes[node]["member"]) == 1:
-            sep_nodes.add(node)
-        G_s_undirected.add_node(node, member=G_s.nodes[node]["member"])
-    for edge in G_s.edges:
-        G_s_undirected.add_edge(edge[0], edge[1])
-    cut_nodes = eg.generator_articulation_points(G_s_undirected)
-    out_degree = G_s.out_degree()
-    in_degree = G_s.in_degree()
-    separations = set()
-    for cut_node in cut_nodes:
-        if cut_node in sep_nodes:
-            if out_degree[cut_node] >= 1 and in_degree[cut_node] >= 1:
-                CC_u = eg.connected_component_of_node(G_s_undirected, node=cut_node)
-                G_CC = G_s_undirected.nodes_subgraph(list(CC_u))
-                G_CC.remove_node(cut_node)
-                successors = G_s.neighbors(node=cut_node)
-                predecessors = G_s.predecessors(node=cut_node)
-                CC_removal = eg.connected_components(G_CC)
-                flag = True
-                for group in CC_removal:
-                    flag_succ = False
-                    flag_pred = False
-                    for node in group:
-                        if node in successors:
-                            flag_succ = True
-                            if flag_pred:
-                                flag = False
-                                break
-                        elif node in predecessors:
-                            flag_pred = True
-                            if flag_succ:
-                                flag = False
-                                break
-                    if not flag:
-                        break
-                if flag:
-                    separations.add(list(SCC_mapping[cut_node])[0])
-    return separations, SCC_mapping, incoming_info
-
-
-def _find_ancestors_of_node(G, node_t):
-    G_reverse = eg.DiGraph()
-    for node in G.nodes:
-        G_reverse.add_node(node)
-    for edge in G.edges:
-        G_reverse.add_edge(edge[1], edge[0])
-    node_dict = eg.Dijkstra(G_reverse, node=node_t)
-    ancestors = []
-    for node in G.nodes:
-        if node_dict[node] < float("inf") and node != node_t:
-            ancestors.append(node)
-    return ancestors
-
-
-@not_implemented_for("multigraph")
-def maxBlock(G, k, f_set=None, delta=1, eps=0.5, c=1, flag_weight=False):
-    """Structural hole spanners detection via maxBlock method.
-
-    Parameters
-    ----------
-    G: easygraph.DiGraph
-
-    k: int
-        top - k structural hole spanners.
-
-    f_set: dict, optional
-        user vi shares his/her information on network G at a rate fi.
-        default is a random [0,1) integer for each node
-
-    delta: float, optional (default: 1)
-        a small value delta > 0.
-
-    eps: float, optional (default: 0.5)
-        an error ratio eps with 0 < eps < 1.
-
-    c: int, optional (default: 1)
-        Success probability 1-n^-c of maxBlock.
-
-    flag_weight: bool, optional (default: False)
-        Denotes whether each edge has attribute 'weight'
-
-    Returns
-    -------
-    S_list : list
-        The list of each top-k structural hole spanners.
-
-    See Also
-    -------
-    maxBlockFast
-
-    Examples
-    --------
-    # >>> maxBlock(G, 100)
-
-    References
-    ----------
-    .. [1] https://doi.org/10.1016/j.ins.2019.07.072
-
-    """
-    if f_set is None:
-        f_set = {}
-        for node in G.nodes:
-            f_set[node] = random.random()
-    if not flag_weight:
-        for edge in G.edges:
-            G[edge[0]][edge[1]]["weight"] = random.random()
-    n = G.number_of_nodes()
-    approximate_opt = _get_estimated_opt(G, f_set, k, c, delta)
-    print("approximate_opt:", approximate_opt)
-    L_min = (k + c) * math.log(n, 2) + math.log(4, 2)
-    L_min = L_min * k * n * n * math.pow(eps, -2) * (8 * k + 2 * eps)
-    L_min = L_min / approximate_opt
-    L_min = math.ceil(L_min)
-    print("L_min:", L_min)
-    S_list, ave_H_Lt_S = _find_topk_shs_under_l(G, f_set, k, L_min)
-    return S_list
-
-
-@not_implemented_for("multigraph")
-def maxBlockFast(G, k, f_set=None, L=None, flag_weight=False):
-    """Structural hole spanners detection via maxBlockFast method.
-
-    Parameters
-    ----------
-    G: easygraph.DiGraph
-
-    G: easygraph.DiGraph
-
-    k: int
-        top - k structural hole spanners.
-
-    f_set: dict, optional
-        user vi shares his/her information on network G at a rate fi.
-        default is a random [0,1) integer for each node
-
-    L: int, optional (default: log2n)
-        Simulation time L for maxBlockFast.
-
-    flag_weight: bool, optional (default: False)
-        Denotes whether each edge has attribute 'weight'
-
-    See Also
-    -------
-    maxBlock
-
-    Examples
-    --------
-    # >>> maxBlockFast(G, 100)
-
-    References
-    ----------
-    .. [1] https://doi.org/10.1016/j.ins.2019.07.072
-
-    """
-    h_set = {}
-    n = G.number_of_nodes()
-    if L is None:
-        L = math.ceil(math.log(n, 2))
-    # print("L:", L)
-    if f_set is None:
-        f_set = {}
-        for node in G.nodes:
-            f_set[node] = random.random()
-    for node in G.nodes:
-        h_set[node] = 0
-    if not flag_weight:
-        for edge in G.edges:
-            G[edge[0]][edge[1]]["weight"] = random.random()
-    for l in range(L):
-        if l % 10000 == 0:
-            print(l, "/", L, "...")
-        # Generate a graph G & = (V, E & ) from G under the live-edge graph model
-        G_live = G.copy()
-        for edge in G_live.edges:
-            wij = G_live[edge[0]][edge[1]]["weight"]
-            toss = random.random() + 0.1
-            if toss >= wij:
-                G_live.remove_edge(edge[0], edge[1])
-
-        G0 = G_live.copy()
-        d_dict = {}
-        ns = number_strongly_connected_components(G0)
-        non_considered_nodes = set()
-        for node in G0.nodes:
-            d_dict[node] = 1
-            non_considered_nodes.add(node)
-        G_p_1 = G0.copy()
-        for i in range(ns):
-            separation_nodes, SCC_mapping, incoming_info = _find_separation_nodes(G_p_1)
-            # print("separation_nodes:", separation_nodes)
-            if len(separation_nodes) > 0:
-                chosen_node = -1
-                for node in separation_nodes:
-                    node_dict = eg.Dijkstra(G_p_1, node=node)
-                    flag = True
-                    for other_sep in separation_nodes:
-                        if other_sep != node:
-                            if node_dict[other_sep] < float("inf"):
-                                flag = False
-                                break
-                    if flag:
-                        chosen_node = node
-                        break
-                # print("chosen_node:", chosen_node)
-                G_tr = eg.DiGraph()
-                n_set = {}
-                desc_set = {}
-                _get_idom(G_p_1, G_tr, chosen_node, n_set, desc_set)
-                ancestors = _find_ancestors_of_node(G_p_1, chosen_node)
-                sum_fi = 0
-                for node_av in ancestors:
-                    sum_fi += f_set[node_av]
-                for node_u in G_tr.nodes:
-                    D_u = 0
-                    for desc in desc_set[node_u]:
-                        if desc not in d_dict.keys():
-                            print(
-                                "Error: desc:",
-                                desc,
-                                "node_u",
-                                node_u,
-                                "d_dict:",
-                                d_dict,
-                            )
-                            print(desc_set[node_u])
-                        D_u += d_dict[desc]
-                    if node_u != chosen_node:
-                        h_set[node_u] += (f_set[chosen_node] + sum_fi) * D_u
-                    elif node_u == chosen_node:
-                        h_set[node_u] += sum_fi * D_u
-                d_dict[chosen_node] = 0
-                for node_vj in G_tr.nodes:
-                    d_dict[chosen_node] += d_dict[node_vj]
-                G_p = G_p_1.copy()
-                for neighbor in G_p_1.neighbors(node=chosen_node):
-                    G_p.remove_edge(chosen_node, neighbor)
-                G_p_1 = G_p.copy()
-                non_considered_nodes.remove(chosen_node)
-            else:
-                V_set = set()
-                for key in SCC_mapping.keys():
-                    for node in SCC_mapping[key]:
-                        if (node in non_considered_nodes) and (
-                            node not in incoming_info.keys()
-                        ):
-                            V_set.add(node)
-                    if len(V_set) > 0:
-                        break
-                # print("V_set:", V_set)
-                for node_v in V_set:
-                    G_tr = eg.DiGraph()
-                    n_set = {}
-                    desc_set = {}
-                    _get_idom(G_p_1, G_tr, node_v, n_set, desc_set)
-                    for node_u in G_tr.nodes:
-                        D_u = 0
-                        for desc in desc_set[node_u]:
-                            if desc not in d_dict.keys():
-                                print(
-                                    "Error: desc:",
-                                    desc,
-                                    "node_u",
-                                    node_u,
-                                    "d_dict:",
-                                    d_dict,
-                                )
-                                print(desc_set[node_u])
-                            D_u += d_dict[desc]
-                        h_set[node_u] += f_set[node_v] * D_u
-                G_p = G_p_1.copy()
-                for node_v in V_set:
-                    non_considered_nodes.remove(node_v)
-                    for neighbor in G_p_1.neighbors(node=node_v):
-                        G_p.remove_edge(node_v, neighbor)
-                G_p_1 = G_p.copy()
-    ave_H_set = {}
-    for node in G.nodes:
-        ave_H_set[node] = h_set[node] * n / L
-    ordered_set = sorted(ave_H_set.items(), key=lambda x: x[1], reverse=True)
-    S_list = []
-    for i in range(k):
-        S_list.append((ordered_set[i])[0])
-    return S_list
+import math
+import random
+import sys
+
+import easygraph as eg
+
+from easygraph.functions.components.strongly_connected import condensation
+from easygraph.functions.components.strongly_connected import (
+    number_strongly_connected_components,
+)
+from easygraph.utils import *
+
+
+__all__ = [
+    "maxBlock",
+    "maxBlockFast",
+]
+tim = 0
+sys.setrecursionlimit(9000000)
+
+
+class dom_g:
+    def __init__(self, N, M):
+        self.tot = 0
+        self.h = []
+        self.ne = []
+        self.to = []
+        for i in range(N + 1):
+            self.h.append(0)
+        for i in range(max(N + 1, M + 1)):
+            self.ne.append(0)
+            self.to.append(0)
+
+    def add(self, x, y):
+        self.tot += 1
+        self.to[self.tot] = y
+        self.ne[self.tot] = self.h[x]
+        self.h[x] = self.tot
+
+
+def _tarjan(x, dfn, repos, g, fa):
+    global tim
+    tim += 1
+    dfn[x] = tim
+    repos[tim] = x
+    i = g.h[x]
+    while i:
+        if dfn[g.to[i]] == 0:
+            fa[g.to[i]] = x
+            _tarjan(g.to[i], dfn, repos, g, fa)
+        i = g.ne[i]
+
+
+def _find(x, f, dfn, semi, mi):
+    if x == f[x]:
+        return x
+    tmp = f[x]
+    f[x] = _find(f[x], f, dfn, semi, mi)
+    if dfn[semi[mi[tmp]]] < dfn[semi[mi[x]]]:
+        mi[x] = mi[tmp]
+    return f[x]
+
+
+def _dfs(x, tr, ans, desc_set):
+    ans[x] += 1
+    i = tr.h[x]
+
+    while i:
+        y = tr.to[i]
+        desc_set[x].add(y)
+        _dfs(y, tr, ans, desc_set)
+        ans[x] += ans[y]
+        for n in desc_set[y]:
+            desc_set[x].add(n)
+        i = tr.ne[i]
+
+
+def _get_idom(G, G_tr, node_s, ans_real, desc_set_real):
+    """Find the immediate dominator of each node and construct an s-rooted dominator tree.
+
+    Parameters
+    ----------
+    G: easygraph.DiGraph
+
+    G_tr: easygraph.DiGraph
+        an s-rooted dominator tree to be constructed.
+
+    node_s: int
+        the node s
+
+    ans_real: dict
+        denotes the number of proper descendants nu of each node u in the dominator tree.
+        a result to be calculated
+
+    desc_set_real: dict
+        denotes the set of proper descendants of node u in the dominator tree.
+        a result to be calculated
+
+    Examples
+    --------
+    # >>> G_tr = eg.DiGraph()
+    # >>> n_set = {}
+    # >>> desc_set = {}
+    # >>> _get_idom(G, G_tr, node_s, n_set, desc_set)
+
+    References
+    ----------
+    .. [1] http://keyblog.cn/article-173.html
+
+    """
+
+    global tim
+    tim = 0
+    n_dom = G.number_of_nodes()
+    m_dom = G.number_of_edges()
+    g = dom_g(n_dom + 1, m_dom + 1)
+    rg = dom_g(n_dom + 1, m_dom + 1)
+    ng = dom_g(n_dom + 1, m_dom + 1)
+    tr = dom_g(n_dom + 1, m_dom + 1)
+
+    dfn = [0 for i in range(n_dom + 1)]
+    repos = [0 for i in range(n_dom + 1)]
+    mi = [i for i in range(n_dom + 1)]
+    fa = [0 for i in range(n_dom + 1)]
+    f = [i for i in range(n_dom + 1)]
+    semi = [i for i in range(n_dom + 1)]
+    idom = [0 for i in range(n_dom + 1)]
+
+    # init
+    j = 0
+    node_map = {}
+    index_map = {}
+    for node in G.nodes:
+        j += 1
+        node_map[node] = j
+        index_map[j] = node
+
+    for edge in G.edges:
+        g.add(node_map[edge[0]], node_map[edge[1]])
+        rg.add(node_map[edge[1]], node_map[edge[0]])
+
+    # tarjan
+    _tarjan(node_map[node_s], dfn, repos, g, fa)
+    # work
+    i = n_dom
+    while i >= 2:
+        x = repos[i]
+        tmp = n_dom
+        j = rg.h[x]
+        while j:
+            if dfn[rg.to[j]] == 0:
+                j = rg.ne[j]
+                continue
+            if dfn[rg.to[j]] < dfn[x]:
+                tmp = min(tmp, dfn[rg.to[j]])
+            else:
+                _find(rg.to[j], f, dfn, semi, mi)
+                tmp = min(tmp, dfn[semi[mi[rg.to[j]]]])
+            j = rg.ne[j]
+
+        semi[x] = repos[tmp]
+        f[x] = fa[x]
+        ng.add(semi[x], x)
+        x = repos[i - 1]
+        j = ng.h[x]
+        while j:
+            y = ng.to[j]
+            _find(y, f, dfn, semi, mi)
+            if semi[mi[y]] == semi[y]:
+                idom[y] = semi[y]
+            else:
+                idom[y] = mi[y]
+            j = ng.ne[j]
+        i -= 1
+
+    i = 2
+    while i <= n_dom:
+        x = repos[i]
+        if x != 0:
+            if idom[x] != semi[x]:
+                idom[x] = idom[idom[x]]
+            tr.add(idom[x], x)
+            if x != node_map[node_s]:
+                G_tr.add_edge(index_map[idom[x]], index_map[x])
+        i += 1
+    G_tr.add_node(node_s)
+    ans = {}
+    desc_set = {}
+    for node in G_tr.nodes:
+        ans[node_map[node]] = 0
+        desc_set[node_map[node]] = set()
+
+    _dfs(node_map[node_s], tr, ans, desc_set)
+    for key in ans.keys():
+        ans[key] -= 1
+        ans_real[index_map[key]] = ans[key]
+    for key in desc_set.keys():
+        desc_set_real[index_map[key]] = set()
+        for value in desc_set[key]:
+            desc_set_real[index_map[key]].add(index_map[value])
+
+
+def _find_topk_shs_under_l(G, f_set, k, L):
+    """Find the top-k structural hole spanners under L simulations.
+
+    Parameters
+    ----------
+    G: easygraph.DiGraph
+
+    f_set: dict
+        user vi shares his/her information on network G at a rate fi.
+
+    k: int
+        top - k structural hole spanners.
+
+    L: int
+        the number of simulations.
+
+    Returns
+    -------
+    S_list : list
+        A set S of k nodes that block the maximum number of information propagations within L simulations.
+
+    ave_H_Lt_S: float
+        the average number of blocked information propagations by the nodes in set S with L t simulations.
+
+    """
+    h_set = {}
+    n = G.number_of_nodes()
+    for node in G.nodes:
+        h_set[node] = 0
+    for l in range(L):
+        if l % 100000 == 0:
+            print("[", l, "/", L, "] find topk shs under L")
+        # Choose a node s from the n nodes in G randomly
+        node_s = random.choice(list(G.nodes))
+        # Generate a graph G & = (V, E & ) from G under the live-edge graph model
+        G_live = G.copy()
+        for edge in G_live.edges:
+            wij = G_live[edge[0]][edge[1]]["weight"]
+            toss = random.random() + 0.1
+            if toss >= wij:
+                G_live.remove_edge(edge[0], edge[1])
+        # Obtain the induced subgraph by the set R G & (s ) of reachable nodes from s
+        R_set = eg.connected_component_of_node(G_live, node_s)
+        G_subgraph = eg.DiGraph()
+        for node in R_set:
+            G_subgraph.add_node(node)
+        for edge in G_live.edges:
+            if edge[0] in G_subgraph.nodes and edge[1] in G_subgraph.nodes:
+                G_subgraph.add_edge(edge[0], edge[1])
+        # Find the immediate dominator idom (v ) of each node v $ V && \ { s } in G
+        # Construct an s -rooted dominator tree
+        # Calculate the number of proper descendants n u of each node u $ V &&
+        G_tr = eg.DiGraph()
+        n_set = {}
+        desc_set = {}
+        _get_idom(G_subgraph, G_tr, node_s, n_set, desc_set)
+        for node_u in G_tr.nodes:
+            if node_u != node_s:
+                # the number of blocked information propagations by node u
+                h_set[node_u] += n_set[node_u] * f_set[node_s]
+    ave_H_set = {}
+    for node in G.nodes:
+        ave_H_set[node] = h_set[node] * n / L
+    ordered_set = sorted(ave_H_set.items(), key=lambda x: x[1], reverse=True)
+    S_list = []
+    ave_H_Lt_S = 0
+    for i in range(k):
+        S_list.append((ordered_set[i])[0])
+        ave_H_Lt_S += (ordered_set[i])[1]
+    return S_list, ave_H_Lt_S
+
+
+def _get_estimated_opt(G, f_set, k, c, delta):
+    """Estimation of the optimal value OPT.
+
+    Parameters
+    ----------
+    G: easygraph.DiGraph
+
+    f_set: dict
+        user vi shares his/her information on network G at a rate fi.
+
+    k: int
+        top - k structural hole spanners.
+
+    c: int
+        Success probability 1-n^-c of maxBlock.
+
+    delta: float
+        a small value delta > 0.
+
+    Returns
+    -------
+    res_opt : float
+        An approximate value OPT.
+
+    """
+    print("Estimating the optimal value OPT...")
+    n = G.number_of_nodes()
+    opt_ub = 0
+    for f_key in f_set.keys():
+        opt_ub = opt_ub + f_set[f_key]
+    opt_ub = opt_ub * k * (n - 1)
+    T = math.log((opt_ub / (delta / 2)), 2)
+    T = math.ceil(T)
+    lamda = 4 * (c * math.log(n, 2) + math.log(k * T, 2)) * (2 * k + 1) * k * n * n
+    for t in range(T):
+        opt_g = opt_ub / math.pow(2, t + 1)
+        L_t = math.ceil(lamda / opt_g)
+        print("[", t, "/", T, "] Estimating OPT: L=", L_t)
+        S_list, ave_H_Lt_S = _find_topk_shs_under_l(G, f_set, k, L_t)
+        if ave_H_Lt_S >= opt_g:
+            res_opt = opt_g / 2
+            return res_opt
+    print("[Warning] OPT is not greater that delta")
+    return -1
+
+
+def _find_separation_nodes(G):
+    G_s = condensation(G)
+    SCC_mapping = {}
+    incoming_info = G_s.graph["incoming_info"]
+    G_s_undirected = eg.Graph()
+    sep_nodes = set()
+    for node in (G_s.nodes).keys():
+        SCC_mapping[node] = G_s.nodes[node]["member"]
+        if len(G_s.nodes[node]["member"]) == 1:
+            sep_nodes.add(node)
+        G_s_undirected.add_node(node, member=G_s.nodes[node]["member"])
+    for edge in G_s.edges:
+        G_s_undirected.add_edge(edge[0], edge[1])
+    cut_nodes = eg.generator_articulation_points(G_s_undirected)
+    out_degree = G_s.out_degree()
+    in_degree = G_s.in_degree()
+    separations = set()
+    for cut_node in cut_nodes:
+        if cut_node in sep_nodes:
+            if out_degree[cut_node] >= 1 and in_degree[cut_node] >= 1:
+                CC_u = eg.connected_component_of_node(G_s_undirected, node=cut_node)
+                G_CC = G_s_undirected.nodes_subgraph(list(CC_u))
+                G_CC.remove_node(cut_node)
+                successors = G_s.neighbors(node=cut_node)
+                predecessors = G_s.predecessors(node=cut_node)
+                CC_removal = eg.connected_components(G_CC)
+                flag = True
+                for group in CC_removal:
+                    flag_succ = False
+                    flag_pred = False
+                    for node in group:
+                        if node in successors:
+                            flag_succ = True
+                            if flag_pred:
+                                flag = False
+                                break
+                        elif node in predecessors:
+                            flag_pred = True
+                            if flag_succ:
+                                flag = False
+                                break
+                    if not flag:
+                        break
+                if flag:
+                    separations.add(list(SCC_mapping[cut_node])[0])
+    return separations, SCC_mapping, incoming_info
+
+
+def _find_ancestors_of_node(G, node_t):
+    G_reverse = eg.DiGraph()
+    for node in G.nodes:
+        G_reverse.add_node(node)
+    for edge in G.edges:
+        G_reverse.add_edge(edge[1], edge[0])
+    node_dict = eg.Dijkstra(G_reverse, node=node_t)
+    ancestors = []
+    for node in G.nodes:
+        if node_dict[node] < float("inf") and node != node_t:
+            ancestors.append(node)
+    return ancestors
+
+
+@not_implemented_for("multigraph")
+def maxBlock(G, k, f_set=None, delta=1, eps=0.5, c=1, flag_weight=False):
+    """Structural hole spanners detection via maxBlock method.
+
+    Parameters
+    ----------
+    G: easygraph.DiGraph
+
+    k: int
+        top - k structural hole spanners.
+
+    f_set: dict, optional
+        user vi shares his/her information on network G at a rate fi.
+        default is a random [0,1) integer for each node
+
+    delta: float, optional (default: 1)
+        a small value delta > 0.
+
+    eps: float, optional (default: 0.5)
+        an error ratio eps with 0 < eps < 1.
+
+    c: int, optional (default: 1)
+        Success probability 1-n^-c of maxBlock.
+
+    flag_weight: bool, optional (default: False)
+        Denotes whether each edge has attribute 'weight'
+
+    Returns
+    -------
+    S_list : list
+        The list of each top-k structural hole spanners.
+
+    See Also
+    -------
+    maxBlockFast
+
+    Examples
+    --------
+    # >>> maxBlock(G, 100)
+
+    References
+    ----------
+    .. [1] https://doi.org/10.1016/j.ins.2019.07.072
+
+    """
+    if f_set is None:
+        f_set = {}
+        for node in G.nodes:
+            f_set[node] = random.random()
+    if not flag_weight:
+        for edge in G.edges:
+            G[edge[0]][edge[1]]["weight"] = random.random()
+    n = G.number_of_nodes()
+    approximate_opt = _get_estimated_opt(G, f_set, k, c, delta)
+    print("approximate_opt:", approximate_opt)
+    L_min = (k + c) * math.log(n, 2) + math.log(4, 2)
+    L_min = L_min * k * n * n * math.pow(eps, -2) * (8 * k + 2 * eps)
+    L_min = L_min / approximate_opt
+    L_min = math.ceil(L_min)
+    print("L_min:", L_min)
+    S_list, ave_H_Lt_S = _find_topk_shs_under_l(G, f_set, k, L_min)
+    return S_list
+
+
+@not_implemented_for("multigraph")
+def maxBlockFast(G, k, f_set=None, L=None, flag_weight=False):
+    """Structural hole spanners detection via maxBlockFast method.
+
+    Parameters
+    ----------
+    G: easygraph.DiGraph
+
+    G: easygraph.DiGraph
+
+    k: int
+        top - k structural hole spanners.
+
+    f_set: dict, optional
+        user vi shares his/her information on network G at a rate fi.
+        default is a random [0,1) integer for each node
+
+    L: int, optional (default: log2n)
+        Simulation time L for maxBlockFast.
+
+    flag_weight: bool, optional (default: False)
+        Denotes whether each edge has attribute 'weight'
+
+    See Also
+    -------
+    maxBlock
+
+    Examples
+    --------
+    # >>> maxBlockFast(G, 100)
+
+    References
+    ----------
+    .. [1] https://doi.org/10.1016/j.ins.2019.07.072
+
+    """
+    h_set = {}
+    n = G.number_of_nodes()
+    if L is None:
+        L = math.ceil(math.log(n, 2))
+    # print("L:", L)
+    if f_set is None:
+        f_set = {}
+        for node in G.nodes:
+            f_set[node] = random.random()
+    for node in G.nodes:
+        h_set[node] = 0
+    if not flag_weight:
+        for edge in G.edges:
+            G[edge[0]][edge[1]]["weight"] = random.random()
+    for l in range(L):
+        if l % 10000 == 0:
+            print(l, "/", L, "...")
+        # Generate a graph G & = (V, E & ) from G under the live-edge graph model
+        G_live = G.copy()
+        for edge in G_live.edges:
+            wij = G_live[edge[0]][edge[1]]["weight"]
+            toss = random.random() + 0.1
+            if toss >= wij:
+                G_live.remove_edge(edge[0], edge[1])
+
+        G0 = G_live.copy()
+        d_dict = {}
+        ns = number_strongly_connected_components(G0)
+        non_considered_nodes = set()
+        for node in G0.nodes:
+            d_dict[node] = 1
+            non_considered_nodes.add(node)
+        G_p_1 = G0.copy()
+        for i in range(ns):
+            separation_nodes, SCC_mapping, incoming_info = _find_separation_nodes(G_p_1)
+            # print("separation_nodes:", separation_nodes)
+            if len(separation_nodes) > 0:
+                chosen_node = -1
+                for node in separation_nodes:
+                    node_dict = eg.Dijkstra(G_p_1, node=node)
+                    flag = True
+                    for other_sep in separation_nodes:
+                        if other_sep != node:
+                            if node_dict[other_sep] < float("inf"):
+                                flag = False
+                                break
+                    if flag:
+                        chosen_node = node
+                        break
+                # print("chosen_node:", chosen_node)
+                G_tr = eg.DiGraph()
+                n_set = {}
+                desc_set = {}
+                _get_idom(G_p_1, G_tr, chosen_node, n_set, desc_set)
+                ancestors = _find_ancestors_of_node(G_p_1, chosen_node)
+                sum_fi = 0
+                for node_av in ancestors:
+                    sum_fi += f_set[node_av]
+                for node_u in G_tr.nodes:
+                    D_u = 0
+                    for desc in desc_set[node_u]:
+                        if desc not in d_dict.keys():
+                            print(
+                                "Error: desc:",
+                                desc,
+                                "node_u",
+                                node_u,
+                                "d_dict:",
+                                d_dict,
+                            )
+                            print(desc_set[node_u])
+                        D_u += d_dict[desc]
+                    if node_u != chosen_node:
+                        h_set[node_u] += (f_set[chosen_node] + sum_fi) * D_u
+                    elif node_u == chosen_node:
+                        h_set[node_u] += sum_fi * D_u
+                d_dict[chosen_node] = 0
+                for node_vj in G_tr.nodes:
+                    d_dict[chosen_node] += d_dict[node_vj]
+                G_p = G_p_1.copy()
+                for neighbor in G_p_1.neighbors(node=chosen_node):
+                    G_p.remove_edge(chosen_node, neighbor)
+                G_p_1 = G_p.copy()
+                non_considered_nodes.remove(chosen_node)
+            else:
+                V_set = set()
+                for key in SCC_mapping.keys():
+                    for node in SCC_mapping[key]:
+                        if (node in non_considered_nodes) and (
+                            node not in incoming_info.keys()
+                        ):
+                            V_set.add(node)
+                    if len(V_set) > 0:
+                        break
+                # print("V_set:", V_set)
+                for node_v in V_set:
+                    G_tr = eg.DiGraph()
+                    n_set = {}
+                    desc_set = {}
+                    _get_idom(G_p_1, G_tr, node_v, n_set, desc_set)
+                    for node_u in G_tr.nodes:
+                        D_u = 0
+                        for desc in desc_set[node_u]:
+                            if desc not in d_dict.keys():
+                                print(
+                                    "Error: desc:",
+                                    desc,
+                                    "node_u",
+                                    node_u,
+                                    "d_dict:",
+                                    d_dict,
+                                )
+                                print(desc_set[node_u])
+                            D_u += d_dict[desc]
+                        h_set[node_u] += f_set[node_v] * D_u
+                G_p = G_p_1.copy()
+                for node_v in V_set:
+                    non_considered_nodes.remove(node_v)
+                    for neighbor in G_p_1.neighbors(node=node_v):
+                        G_p.remove_edge(node_v, neighbor)
+                G_p_1 = G_p.copy()
+    ave_H_set = {}
+    for node in G.nodes:
+        ave_H_set[node] = h_set[node] * n / L
+    ordered_set = sorted(ave_H_set.items(), key=lambda x: x[1], reverse=True)
+    S_list = []
+    for i in range(k):
+        S_list.append((ordered_set[i])[0])
+    return S_list
```

## easygraph/functions/structural_holes/ICC.py

 * *Ordering differences only*

```diff
@@ -1,298 +1,298 @@
-import easygraph as eg
-
-from easygraph.utils import *
-
-
-__all__ = ["ICC", "BICC", "AP_BICC"]
-
-
-def inverse_closeness_centrality(G, v):
-    c_v = sum(eg.Dijkstra(G, v).values()) / (len(G) - 1)
-    return c_v
-
-
-def bounded_inverse_closeness_centrality(G, v, l):
-    queue = []
-    queue.append(v)
-    seen = set()
-    seen.add(v)
-    shortest_path = eg.Floyd(G)
-    result = 0
-    while len(queue) > 0:
-        vertex = queue.pop(0)
-        if shortest_path[v][vertex] == l + 1:
-            break
-        nodes = G.neighbors(node=vertex)
-        for w in nodes:
-            if w not in seen:
-                queue.append(w)
-                seen.add(w)
-                result += shortest_path[v][w]
-    return result / (len(G) - 1)
-
-
-def Modified_DFS(G, u, V, time, n):
-    V[u]["color"] = "black"
-    time += 1
-    n -= 1
-    V[u]["discovered"] = time
-    V[u]["lowest"] = time
-    cc0 = n
-    V[u]["descendant"] = 0
-    root = u
-    for edge in G.edges:
-        u, v = edge[:2]
-        if V[u]["color"] == "white":
-            V[u]["color"] = "grey"
-            V[v]["parent"] = u
-            V[u]["child"] += 1
-            V, time, n = Modified_DFS(G, v, V, time, n)
-            V[u]["descendant"] = V[u]["descendant"] + V[v]["descendant"]
-            V[u]["lowest"] = min(V[u]["lowest"], V[v]["lowest"])
-            if V[v]["lowest"] >= V[u]["discovered"] or root == u and V[u]["child"] > 1:
-                V[u]["c"] += V[v]["descendant"] * (n - V[v]["descendant"] - 1)
-                cc0 -= V[v]["descendant"]
-        elif v != V[u]["parent"]:
-            V[u]["lowest"] = min(V[u]["lowest"], V[v]["discovered"])
-    V[u]["c"] += cc0 * (n - cc0 - 1)
-    return V, time, n
-
-
-def approximate_inverse_closeness_centrality(G):
-    V = {}
-    for i in G.nodes:
-        V[i] = {}
-        V[i]["child"] = 0
-        V[i]["color"] = "white"
-        V[i]["c"] = 0
-        V[i]["parent"] = None
-        V[i]["discovered"] = 0
-        V[i]["lowest"] = 0
-        V[i]["descendant"] = 0
-    time = 0
-    n = len(G)
-    for u in G.nodes:
-        if V[u]["color"] == "white":
-            V, time, n = Modified_DFS(G, u, V, time, n)
-    return V
-
-
-@not_implemented_for("multigraph")
-def ICC(G, k):
-    """an efficient algorithm for structural hole spanners detection.
-
-    Returns top k nodes as structural hole spanners,
-    Algorithm 1 of [1]_
-
-    Parameters
-    ----------
-    G : easygraph.Graph
-        An unweighted and undirected graph.
-
-    k : int
-        top - k structural hole spanners
-
-    Returns
-    -------
-    V : list
-        The list of top-k structural hole spanners.
-
-    Examples
-    --------
-    Returns the top k nodes as structural hole spanners, using **ICC**.
-
-    >>> ICC(G,k=3)
-
-    References
-    ----------
-    .. [1] https://dl.acm.org/doi/10.1145/2806416.2806431
-
-    """
-    Q = []
-    V = []
-    for v in G.nodes:
-        i_c = inverse_closeness_centrality(G, v)
-        if len(Q) < k:
-            Q.append([v, i_c])
-            continue
-        MAX = 0
-        t = v
-        for i in Q:
-            if MAX < i[1]:
-                MAX = i[1]
-                t = i[0]
-        if i_c < MAX:
-            Q.remove([t, MAX])
-            Q.append([v, i_c])
-    for i in Q:
-        V.append(i[0])
-    return V
-
-
-@not_implemented_for("multigraph")
-def BICC(G, k, K, l):
-    """an efficient algorithm for structural hole spanners detection.
-
-    Returns top k nodes as structural hole spanners,
-    Algorithm 2 of [1]_
-
-    Parameters
-    ----------
-    G : easygraph.Graph
-        An unweighted and undirected graph.
-
-    k : int
-        top - k structural hole spanners
-
-    K : int
-        the number of candidates K for the top-k hole spanners
-
-    l : int
-        level-l neighbors of nodes
-
-    Returns
-    -------
-    V : list
-        The list of top-k structural hole spanners.
-
-    Examples
-    --------
-    Returns the top k nodes as structural hole spanners, using **BICC**.
-
-    >>> BICC(G,k=3,K=5,l=4)
-
-    References
-    ----------
-    .. [1] https://dl.acm.org/doi/10.1145/2806416.2806431
-
-    """
-    H = []
-    V = []
-    for v in G.nodes:
-        b_i_c = bounded_inverse_closeness_centrality(G, v, l)
-        if len(H) < K:
-            H.append([v, b_i_c])
-            continue
-        MIN = 10000000
-        t = v
-        for i in H:
-            if MIN > i[1]:
-                MIN = i[1]
-                t = i[0]
-        if b_i_c > MIN:
-            H.remove([t, MIN])
-            H.append([v, b_i_c])
-    for i in H:
-        v = i[0]
-        i_c = inverse_closeness_centrality(G, v)
-        if len(V) < k:
-            V.append([v, i_c])
-            continue
-        MAX = 0
-        t = v
-        for i in V:
-            if MAX < i[1]:
-                MAX = i[1]
-                t = i[0]
-        if i_c < MAX:
-            V.remove([t, MAX])
-            V.append([v, i_c])
-    VS = []
-    for i in V:
-        VS.append(i[0])
-    return VS
-
-
-@not_implemented_for("multigraph")
-def AP_BICC(G, k, K, l):
-    """an efficient algorithm for structural hole spanners detection.
-
-    Returns top k nodes as structural hole spanners,
-    Algorithm 3 of [1]_
-
-    Parameters
-    ----------
-    G : easygraph.Graph
-        An unweighted and undirected graph.
-
-    k : int
-        top - k structural hole spanners
-
-    K : int
-        the number of candidates K for the top-k hole spanners
-
-    l : int
-        level-l neighbors of nodes
-
-    Returns
-    -------
-    V : list
-        The list of top-k structural hole spanners.
-
-    Examples
-    --------
-    Returns the top k nodes as structural hole spanners, using **AP_BICC**.
-
-    >>> AP_BICC(G,k=3,K=5,l=4)
-
-    References
-    ----------
-    .. [1] https://dl.acm.org/doi/10.1145/2806416.2806431
-
-    """
-    V = []
-    T = []
-    A = {}
-    A = approximate_inverse_closeness_centrality(G)
-    for v in A:
-        if len(T) < k:
-            T.append([v, A[v]["c"]])
-            continue
-        MIN = 10000000
-        t = v
-        for i in T:
-            if MIN > i[1]:
-                MIN = i[1]
-                t = i[0]
-        if A[v]["c"] > MIN:
-            T.remove([t, MIN])
-            T.append([v, A[v]["c"]])
-    if len(T) < k:
-        U = {}
-        for i in G.nodes:
-            if i not in A:
-                U.append(i)
-        kk = k - len(T)
-        Q = []
-        for v in U:
-            b_i_c = bounded_inverse_closeness_centrality(G, v, l)
-            if len(Q) < K:
-                Q.append([v, b_i_c])
-            else:
-                MIN = 10000000
-                t = v
-                for i in Q:
-                    if MIN > i[1]:
-                        MIN = i[1]
-                        t = i[0]
-                if b_i_c > MIN:
-                    Q.remove([t, MIN])
-                    Q.append([v, b_i_c])
-    while len(T) != k:
-        MAX = 0
-        t = None
-        for i in Q:
-            if MAX < i[1]:
-                MAX = i[1]
-                t = i[0]
-        T.append([t, A[t]["c"]])
-    for i in T:
-        V.append(i[0])
-    return V
-
-
-if __name__ == "__main__":
-    G = eg.datasets.get_graph_karateclub()
-    print(ICC(G, 3))
-    print(BICC(G, 3, 5, 3))
-    print(AP_BICC(G, 3, 5, 3))
+import easygraph as eg
+
+from easygraph.utils import *
+
+
+__all__ = ["ICC", "BICC", "AP_BICC"]
+
+
+def inverse_closeness_centrality(G, v):
+    c_v = sum(eg.Dijkstra(G, v).values()) / (len(G) - 1)
+    return c_v
+
+
+def bounded_inverse_closeness_centrality(G, v, l):
+    queue = []
+    queue.append(v)
+    seen = set()
+    seen.add(v)
+    shortest_path = eg.Floyd(G)
+    result = 0
+    while len(queue) > 0:
+        vertex = queue.pop(0)
+        if shortest_path[v][vertex] == l + 1:
+            break
+        nodes = G.neighbors(node=vertex)
+        for w in nodes:
+            if w not in seen:
+                queue.append(w)
+                seen.add(w)
+                result += shortest_path[v][w]
+    return result / (len(G) - 1)
+
+
+def Modified_DFS(G, u, V, time, n):
+    V[u]["color"] = "black"
+    time += 1
+    n -= 1
+    V[u]["discovered"] = time
+    V[u]["lowest"] = time
+    cc0 = n
+    V[u]["descendant"] = 0
+    root = u
+    for edge in G.edges:
+        u, v = edge[:2]
+        if V[u]["color"] == "white":
+            V[u]["color"] = "grey"
+            V[v]["parent"] = u
+            V[u]["child"] += 1
+            V, time, n = Modified_DFS(G, v, V, time, n)
+            V[u]["descendant"] = V[u]["descendant"] + V[v]["descendant"]
+            V[u]["lowest"] = min(V[u]["lowest"], V[v]["lowest"])
+            if V[v]["lowest"] >= V[u]["discovered"] or root == u and V[u]["child"] > 1:
+                V[u]["c"] += V[v]["descendant"] * (n - V[v]["descendant"] - 1)
+                cc0 -= V[v]["descendant"]
+        elif v != V[u]["parent"]:
+            V[u]["lowest"] = min(V[u]["lowest"], V[v]["discovered"])
+    V[u]["c"] += cc0 * (n - cc0 - 1)
+    return V, time, n
+
+
+def approximate_inverse_closeness_centrality(G):
+    V = {}
+    for i in G.nodes:
+        V[i] = {}
+        V[i]["child"] = 0
+        V[i]["color"] = "white"
+        V[i]["c"] = 0
+        V[i]["parent"] = None
+        V[i]["discovered"] = 0
+        V[i]["lowest"] = 0
+        V[i]["descendant"] = 0
+    time = 0
+    n = len(G)
+    for u in G.nodes:
+        if V[u]["color"] == "white":
+            V, time, n = Modified_DFS(G, u, V, time, n)
+    return V
+
+
+@not_implemented_for("multigraph")
+def ICC(G, k):
+    """an efficient algorithm for structural hole spanners detection.
+
+    Returns top k nodes as structural hole spanners,
+    Algorithm 1 of [1]_
+
+    Parameters
+    ----------
+    G : easygraph.Graph
+        An unweighted and undirected graph.
+
+    k : int
+        top - k structural hole spanners
+
+    Returns
+    -------
+    V : list
+        The list of top-k structural hole spanners.
+
+    Examples
+    --------
+    Returns the top k nodes as structural hole spanners, using **ICC**.
+
+    >>> ICC(G,k=3)
+
+    References
+    ----------
+    .. [1] https://dl.acm.org/doi/10.1145/2806416.2806431
+
+    """
+    Q = []
+    V = []
+    for v in G.nodes:
+        i_c = inverse_closeness_centrality(G, v)
+        if len(Q) < k:
+            Q.append([v, i_c])
+            continue
+        MAX = 0
+        t = v
+        for i in Q:
+            if MAX < i[1]:
+                MAX = i[1]
+                t = i[0]
+        if i_c < MAX:
+            Q.remove([t, MAX])
+            Q.append([v, i_c])
+    for i in Q:
+        V.append(i[0])
+    return V
+
+
+@not_implemented_for("multigraph")
+def BICC(G, k, K, l):
+    """an efficient algorithm for structural hole spanners detection.
+
+    Returns top k nodes as structural hole spanners,
+    Algorithm 2 of [1]_
+
+    Parameters
+    ----------
+    G : easygraph.Graph
+        An unweighted and undirected graph.
+
+    k : int
+        top - k structural hole spanners
+
+    K : int
+        the number of candidates K for the top-k hole spanners
+
+    l : int
+        level-l neighbors of nodes
+
+    Returns
+    -------
+    V : list
+        The list of top-k structural hole spanners.
+
+    Examples
+    --------
+    Returns the top k nodes as structural hole spanners, using **BICC**.
+
+    >>> BICC(G,k=3,K=5,l=4)
+
+    References
+    ----------
+    .. [1] https://dl.acm.org/doi/10.1145/2806416.2806431
+
+    """
+    H = []
+    V = []
+    for v in G.nodes:
+        b_i_c = bounded_inverse_closeness_centrality(G, v, l)
+        if len(H) < K:
+            H.append([v, b_i_c])
+            continue
+        MIN = 10000000
+        t = v
+        for i in H:
+            if MIN > i[1]:
+                MIN = i[1]
+                t = i[0]
+        if b_i_c > MIN:
+            H.remove([t, MIN])
+            H.append([v, b_i_c])
+    for i in H:
+        v = i[0]
+        i_c = inverse_closeness_centrality(G, v)
+        if len(V) < k:
+            V.append([v, i_c])
+            continue
+        MAX = 0
+        t = v
+        for i in V:
+            if MAX < i[1]:
+                MAX = i[1]
+                t = i[0]
+        if i_c < MAX:
+            V.remove([t, MAX])
+            V.append([v, i_c])
+    VS = []
+    for i in V:
+        VS.append(i[0])
+    return VS
+
+
+@not_implemented_for("multigraph")
+def AP_BICC(G, k, K, l):
+    """an efficient algorithm for structural hole spanners detection.
+
+    Returns top k nodes as structural hole spanners,
+    Algorithm 3 of [1]_
+
+    Parameters
+    ----------
+    G : easygraph.Graph
+        An unweighted and undirected graph.
+
+    k : int
+        top - k structural hole spanners
+
+    K : int
+        the number of candidates K for the top-k hole spanners
+
+    l : int
+        level-l neighbors of nodes
+
+    Returns
+    -------
+    V : list
+        The list of top-k structural hole spanners.
+
+    Examples
+    --------
+    Returns the top k nodes as structural hole spanners, using **AP_BICC**.
+
+    >>> AP_BICC(G,k=3,K=5,l=4)
+
+    References
+    ----------
+    .. [1] https://dl.acm.org/doi/10.1145/2806416.2806431
+
+    """
+    V = []
+    T = []
+    A = {}
+    A = approximate_inverse_closeness_centrality(G)
+    for v in A:
+        if len(T) < k:
+            T.append([v, A[v]["c"]])
+            continue
+        MIN = 10000000
+        t = v
+        for i in T:
+            if MIN > i[1]:
+                MIN = i[1]
+                t = i[0]
+        if A[v]["c"] > MIN:
+            T.remove([t, MIN])
+            T.append([v, A[v]["c"]])
+    if len(T) < k:
+        U = {}
+        for i in G.nodes:
+            if i not in A:
+                U.append(i)
+        kk = k - len(T)
+        Q = []
+        for v in U:
+            b_i_c = bounded_inverse_closeness_centrality(G, v, l)
+            if len(Q) < K:
+                Q.append([v, b_i_c])
+            else:
+                MIN = 10000000
+                t = v
+                for i in Q:
+                    if MIN > i[1]:
+                        MIN = i[1]
+                        t = i[0]
+                if b_i_c > MIN:
+                    Q.remove([t, MIN])
+                    Q.append([v, b_i_c])
+    while len(T) != k:
+        MAX = 0
+        t = None
+        for i in Q:
+            if MAX < i[1]:
+                MAX = i[1]
+                t = i[0]
+        T.append([t, A[t]["c"]])
+    for i in T:
+        V.append(i[0])
+    return V
+
+
+if __name__ == "__main__":
+    G = eg.datasets.get_graph_karateclub()
+    print(ICC(G, 3))
+    print(BICC(G, 3, 5, 3))
+    print(AP_BICC(G, 3, 5, 3))
```

## easygraph/functions/structural_holes/metrics.py

 * *Ordering differences only*

```diff
@@ -1,394 +1,394 @@
-import math
-import random
-
-import easygraph as eg
-import numpy as np
-
-from easygraph.utils import *
-
-
-__all__ = [
-    "sum_of_shortest_paths",
-    "nodes_of_max_cc_without_shs",
-    "structural_hole_influence_index",
-]
-
-
-@not_implemented_for("multigraph")
-def sum_of_shortest_paths(G, S):
-    r"""Returns the difference between the sum of lengths of all pairs shortest paths in G and the one in G\S.
-    The experiment ml_metrics in [1]_
-
-    Parameters
-    ----------
-    G: easygraph.Graph or easygraph.DiGraph
-
-    S: list of int
-        A list of nodes witch are structural hole spanners.
-
-    Returns
-    -------
-    differ_between_sum : int
-        The difference between the sum of lengths of all pairs shortest paths in G and the one in G\S.
-        C(G/S)-C(G)
-
-    Examples
-    --------
-    >>> G_t=eg.datasets.get_graph_blogcatalog()
-    >>> S_t=eg.AP_Greedy(G_t, 10000)
-    >>> diff = sum_of_shortest_paths(G_t, S_t)
-    >>> print(diff)
-
-    References
-    ----------
-    .. [1] https://dl.acm.org/profile/81484650642
-
-    """
-    mat_G = eg.Floyd(G)
-    sum_G = 0
-    inf_const_G = math.ceil((G.number_of_nodes() ** 3) / 3)
-    for i in mat_G.values():
-        for j in i.values():
-            if math.isinf(j):
-                j = inf_const_G
-            sum_G += j
-    G_S = G.copy()
-    G_S.remove_nodes(S)
-    mat_G_S = eg.Floyd(G_S)
-    sum_G_S = 0
-    inf_const_G_S = math.ceil((G_S.number_of_nodes() ** 3) / 3)
-    for i in mat_G_S.values():
-        for j in i.values():
-            if math.isinf(j):
-                j = inf_const_G_S
-            sum_G_S += j
-    return sum_G_S - sum_G
-
-
-@not_implemented_for("multigraph")
-def nodes_of_max_cc_without_shs(G, S):
-    r"""Returns the number of nodes in the maximum connected component in graph G\S.
-    The experiment ml_metrics in [1]_
-
-    Parameters
-    ----------
-    G: easygraph.Graph or easygraph.DiGraph
-
-    S: list of int
-        A list of nodes witch are structural hole spanners.
-
-    Returns
-    -------
-    G_S_nodes_of_max_CC: int
-        The number of nodes in the maximum connected component in graph G\S.
-
-    Examples
-    --------
-    >>> G_t=eg.datasets.get_graph_blogcatalog()
-    >>> S_t=eg.AP_Greedy(G_t, 10000)
-    >>> maxx = nodes_of_max_cc_without_shs(G_t, S_t)
-    >>> print(maxx)
-
-    References
-    ----------
-    .. [1] https://dl.acm.org/profile/81484650642
-
-    """
-    G_S = G.copy()
-    G_S.remove_nodes(S)
-    ccs = eg.connected_components(G_S)
-    max_num = 0
-    for cc in ccs:
-        if len(cc) > max_num:
-            max_num = len(cc)
-    return max_num
-
-
-class NodeParams:
-    def __init__(self, active, inWeight, threshold):
-        self.active = active
-        self.inWeight = inWeight
-        self.threshold = threshold
-
-
-@not_implemented_for("multigraph")
-def structural_hole_influence_index(
-    G_original,
-    S,
-    C,
-    model,
-    variant=False,
-    seedRatio=0.05,
-    randSeedIter=10,
-    countIterations=100,
-    Directed=True,
-):
-    """Returns the SHII metric of each seed.
-
-    Parameters
-    ----------
-    G_original: easygraph.Graph or easygraph.DiGraph
-
-    S: list of int
-        A list of nodes which are structural hole spanners.
-
-    C: list of list
-        Each list includes the nodes in one community.
-
-    model: string
-        Propagation Model. Should be IC or LT.
-
-    variant: bool, default is False
-        Whether returns variant SHII ml_metrics or not.
-        variant SHII = # of the influenced outsider / # of the influenced insiders
-        SHII = # of the influenced outsiders / # of the total influenced nodes
-
-    seedRatio: float, default is 0.05
-        # of sampled seeds / # of nodes of the community that the given SHS belongs to.
-
-    randSeedIter: int, default is 10
-        How many iterations to sample seeds.
-
-    countIterations: int default is 100
-        Number of monte carlo simulations to be used.
-
-    Directed: bool, default is True
-        Whether the graph is directed or not.
-
-    Returns
-    -------
-    seed_shii_pair : dict
-        the SHII metric of each seed
-
-    Examples
-    --------
-    # >>> structural_hole_influence_index(G, [3, 20, 9], Com, 'LT', seedRatio=0.1, Directed=False)
-
-    References
-    ----------
-    .. [1] https://dl.acm.org/doi/pdf/10.1145/2939672.2939807
-    .. [2] https://github.com/LifangHe/KDD16_HAM/tree/master/SHII_metric
-
-    """
-    if not Directed:
-        G = eg.DiGraph()
-        for edge in G_original.edges:
-            G.add_edge(edge[0], edge[1])
-            G.add_edge(edge[1], edge[0])
-    else:
-        G = G_original.copy()
-    # form pair like {node_1:community_label_1,node_2:community_label_2}
-    node_label_pair = {}
-    for community_label in range(len(C)):
-        for node_i in range(len(C[community_label])):
-            node_label_pair[C[community_label][node_i]] = community_label
-    # print(node_label_pair)
-    seed_shii_pair = {}
-    for community_label in range(len(C)):
-        nodesInCommunity = []
-        seedSetInCommunity = []
-        for node in node_label_pair.keys():
-            if node_label_pair[node] == community_label:
-                nodesInCommunity.append(node)
-                if node in S:
-                    seedSetInCommunity.append(node)
-
-        seedSetSize = int(math.ceil(len(nodesInCommunity) * seedRatio))
-
-        if len(seedSetInCommunity) == 0:
-            continue
-
-        for seed in seedSetInCommunity:
-            print(">>>>>> processing seed ", seed, " now.")
-            oneSeedSet = []
-            if node not in oneSeedSet:
-                oneSeedSet.append(seed)
-            seedNeighborSet = []
-            # using BFS to add neighbors of the SH spanner to the seedNeighborSet as seed candidates
-            queue = []
-            queue.append(seed)
-            while len(queue) > 0:
-                cur_node = queue[0]
-                count_neighbor = 0
-                for neighbor in G.neighbors(node=cur_node):
-                    if neighbor not in seedNeighborSet:
-                        seedNeighborSet.append(neighbor)
-                    count_neighbor = count_neighbor + 1
-                if count_neighbor > 0:
-                    if (
-                        len(queue) == 1
-                        and len(oneSeedSet) + len(seedNeighborSet) < seedSetSize
-                    ):
-                        for node in seedNeighborSet:
-                            if node not in oneSeedSet:
-                                oneSeedSet.append(node)
-                            queue.append(node)
-                        seedNeighborSet.clear()
-                queue.pop(0)
-
-            avg_censor_score_1 = 0.0
-            avg_censor_score_2 = 0.0
-
-            for randIter in range(randSeedIter):
-                if randIter % 5 == 0:
-                    print("seed ", seed, ": ", randIter, " in ", randSeedIter)
-                randSeedSet = []
-                for node in oneSeedSet:
-                    randSeedSet.append(node)
-                seedNeighbors = []
-                for node in seedNeighborSet:
-                    seedNeighbors.append(node)
-                while len(seedNeighbors) > 0 and len(randSeedSet) < seedSetSize:
-                    r = random.randint(0, len(seedNeighbors) - 1)
-                    if seedNeighbors[r] not in randSeedSet:
-                        randSeedSet.append(seedNeighbors[r])
-                    seedNeighbors.pop(r)
-
-                if model == "IC":
-                    censor_score_1, censor_score_2 = _independent_cascade(
-                        G,
-                        randSeedSet,
-                        community_label,
-                        countIterations,
-                        node_label_pair,
-                    )
-                elif model == "LT":
-                    censor_score_1, censor_score_2 = _linear_threshold(
-                        G,
-                        randSeedSet,
-                        community_label,
-                        countIterations,
-                        node_label_pair,
-                    )
-                avg_censor_score_1 += censor_score_1 / randSeedIter
-                avg_censor_score_2 += censor_score_2 / randSeedIter
-                # print("seed ", seed, " avg_censor_score in ", randIter, "is ", censor_score_1 / randSeedIter)
-            if variant:
-                seed_shii_pair[seed] = avg_censor_score_2
-            else:
-                seed_shii_pair[seed] = avg_censor_score_1
-    return seed_shii_pair
-
-
-def _independent_cascade(G, S, community_label, countIterations, node_label_pair):
-    avg_result_1 = 0
-    avg_result_2 = 0
-    N = G.number_of_nodes()
-    for b in range(countIterations):
-        # print(b, " in ", countIterations)
-        p_vw = np.zeros((N, N))  # 节点被激活时，激活其它节点的概率,a对b的影响等于b对a的影响
-        for random_i in range(N):
-            for random_j in range(random_i + 1, N):
-                num = random.random()
-                p_vw[random_i][random_j] = num
-                p_vw[random_j][random_i] = num
-        Q = []
-        activeNodes = []
-        for v in S:
-            Q.append(v)
-            activeNodes.append(v)
-        while len(Q) > 0:
-            v = Q[0]
-            for neighbor in G.neighbors(node=v):
-                if neighbor not in activeNodes:
-                    toss = random.random() + 0.1
-                    if v <= 0 or neighbor <= 0:
-                        print(v, neighbor)
-                    # if toss>0.5:
-                    #     activeNodes.append(neighbor)
-                    #     Q.append(neighbor)
-                    if toss >= p_vw[v - 1][neighbor - 1]:
-                        activeNodes.append(neighbor)
-                        Q.append(neighbor)
-            Q.pop(0)
-        self_cov = 0
-        total_cov = 0
-        uniqueActiveNodes = []
-        for i in activeNodes:
-            if i not in uniqueActiveNodes:
-                uniqueActiveNodes.append(i)
-        for v in uniqueActiveNodes:
-            total_cov += 1
-            if node_label_pair[v] == community_label:
-                self_cov += 1
-        censor_score_1 = (total_cov - self_cov) / total_cov
-        censor_score_2 = (total_cov - self_cov) / self_cov
-        avg_result_1 += censor_score_1 / countIterations
-        avg_result_2 += censor_score_2 / countIterations
-    return avg_result_1, avg_result_2
-
-
-def _linear_threshold(G, S, community_label, countIterations, node_label_pair):
-    tol = 0.00001
-    avg_result_1 = 0
-    avg_result_2 = 0
-    for b in range(countIterations):
-        activeNodes = []
-        # T is the set of nodes that are to be processed
-        T = []
-        Q = {}
-        for v in S:
-            activeNodes.append(v)
-            for neighbor in G.neighbors(node=v):
-                if neighbor not in S:
-                    weight_degree = 1.0 / float(G.in_degree()[neighbor])
-                    if neighbor not in Q.keys():
-                        np = NodeParams(False, weight_degree, random.random())
-                        Q[neighbor] = np
-                        T.append(neighbor)
-                    else:
-                        Q[neighbor].inWeight += weight_degree
-
-        while len(T) > 0:
-            u = T[0]
-            if Q[u].inWeight >= Q[u].threshold + tol and not Q[u].active:
-                activeNodes.append(u)
-                Q[u].active = True
-                for neighbor in G.neighbors(node=u):
-                    if neighbor in S:
-                        continue
-                    weight_degree = 1.0 / float(G.in_degree()[neighbor])
-                    if neighbor not in Q.keys():
-                        np = NodeParams(False, weight_degree, random.random())
-                        Q[neighbor] = np
-                        T.append(neighbor)
-                    else:
-                        if not Q[neighbor].active:
-                            T.append(neighbor)
-                            Q[neighbor].inWeight += weight_degree
-                            if Q[neighbor].inWeight - 1 > tol:
-                                print("Error: the inweight for a node is > 1.")
-            T.pop(0)
-
-        T.clear()
-        Q.clear()
-
-        self_cov = 0
-        total_cov = 0
-        uniqueActiveNodes = []
-        for i in activeNodes:
-            if i not in uniqueActiveNodes:
-                uniqueActiveNodes.append(i)
-        for v in uniqueActiveNodes:
-            total_cov += 1
-            if node_label_pair[v] == community_label:
-                self_cov += 1
-        censor_score_1 = (total_cov - self_cov) / total_cov  # ==> SHII
-        censor_score_2 = (total_cov - self_cov) / self_cov
-        avg_result_1 += censor_score_1 / countIterations
-        avg_result_2 += censor_score_2 / countIterations
-    return avg_result_1, avg_result_2
-
-
-if __name__ == "__main__":
-    G = eg.datasets.get_graph_karateclub()
-    Com = []
-    t1 = [1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 17, 18, 20, 22]
-    Com.append(t1)
-    t2 = [9, 10, 15, 16, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
-    Com.append(t2)
-    print("community_label:", Com)
-    result = structural_hole_influence_index(
-        G, [3, 20, 9], Com, "IC", seedRatio=0.1, Directed=False
-    )
-    print(result)
+import math
+import random
+
+import easygraph as eg
+import numpy as np
+
+from easygraph.utils import *
+
+
+__all__ = [
+    "sum_of_shortest_paths",
+    "nodes_of_max_cc_without_shs",
+    "structural_hole_influence_index",
+]
+
+
+@not_implemented_for("multigraph")
+def sum_of_shortest_paths(G, S):
+    r"""Returns the difference between the sum of lengths of all pairs shortest paths in G and the one in G\S.
+    The experiment ml_metrics in [1]_
+
+    Parameters
+    ----------
+    G: easygraph.Graph or easygraph.DiGraph
+
+    S: list of int
+        A list of nodes witch are structural hole spanners.
+
+    Returns
+    -------
+    differ_between_sum : int
+        The difference between the sum of lengths of all pairs shortest paths in G and the one in G\S.
+        C(G/S)-C(G)
+
+    Examples
+    --------
+    >>> G_t=eg.datasets.get_graph_blogcatalog()
+    >>> S_t=eg.AP_Greedy(G_t, 10000)
+    >>> diff = sum_of_shortest_paths(G_t, S_t)
+    >>> print(diff)
+
+    References
+    ----------
+    .. [1] https://dl.acm.org/profile/81484650642
+
+    """
+    mat_G = eg.Floyd(G)
+    sum_G = 0
+    inf_const_G = math.ceil((G.number_of_nodes() ** 3) / 3)
+    for i in mat_G.values():
+        for j in i.values():
+            if math.isinf(j):
+                j = inf_const_G
+            sum_G += j
+    G_S = G.copy()
+    G_S.remove_nodes(S)
+    mat_G_S = eg.Floyd(G_S)
+    sum_G_S = 0
+    inf_const_G_S = math.ceil((G_S.number_of_nodes() ** 3) / 3)
+    for i in mat_G_S.values():
+        for j in i.values():
+            if math.isinf(j):
+                j = inf_const_G_S
+            sum_G_S += j
+    return sum_G_S - sum_G
+
+
+@not_implemented_for("multigraph")
+def nodes_of_max_cc_without_shs(G, S):
+    r"""Returns the number of nodes in the maximum connected component in graph G\S.
+    The experiment ml_metrics in [1]_
+
+    Parameters
+    ----------
+    G: easygraph.Graph or easygraph.DiGraph
+
+    S: list of int
+        A list of nodes witch are structural hole spanners.
+
+    Returns
+    -------
+    G_S_nodes_of_max_CC: int
+        The number of nodes in the maximum connected component in graph G\S.
+
+    Examples
+    --------
+    >>> G_t=eg.datasets.get_graph_blogcatalog()
+    >>> S_t=eg.AP_Greedy(G_t, 10000)
+    >>> maxx = nodes_of_max_cc_without_shs(G_t, S_t)
+    >>> print(maxx)
+
+    References
+    ----------
+    .. [1] https://dl.acm.org/profile/81484650642
+
+    """
+    G_S = G.copy()
+    G_S.remove_nodes(S)
+    ccs = eg.connected_components(G_S)
+    max_num = 0
+    for cc in ccs:
+        if len(cc) > max_num:
+            max_num = len(cc)
+    return max_num
+
+
+class NodeParams:
+    def __init__(self, active, inWeight, threshold):
+        self.active = active
+        self.inWeight = inWeight
+        self.threshold = threshold
+
+
+@not_implemented_for("multigraph")
+def structural_hole_influence_index(
+    G_original,
+    S,
+    C,
+    model,
+    variant=False,
+    seedRatio=0.05,
+    randSeedIter=10,
+    countIterations=100,
+    Directed=True,
+):
+    """Returns the SHII metric of each seed.
+
+    Parameters
+    ----------
+    G_original: easygraph.Graph or easygraph.DiGraph
+
+    S: list of int
+        A list of nodes which are structural hole spanners.
+
+    C: list of list
+        Each list includes the nodes in one community.
+
+    model: string
+        Propagation Model. Should be IC or LT.
+
+    variant: bool, default is False
+        Whether returns variant SHII ml_metrics or not.
+        variant SHII = # of the influenced outsider / # of the influenced insiders
+        SHII = # of the influenced outsiders / # of the total influenced nodes
+
+    seedRatio: float, default is 0.05
+        # of sampled seeds / # of nodes of the community that the given SHS belongs to.
+
+    randSeedIter: int, default is 10
+        How many iterations to sample seeds.
+
+    countIterations: int default is 100
+        Number of monte carlo simulations to be used.
+
+    Directed: bool, default is True
+        Whether the graph is directed or not.
+
+    Returns
+    -------
+    seed_shii_pair : dict
+        the SHII metric of each seed
+
+    Examples
+    --------
+    # >>> structural_hole_influence_index(G, [3, 20, 9], Com, 'LT', seedRatio=0.1, Directed=False)
+
+    References
+    ----------
+    .. [1] https://dl.acm.org/doi/pdf/10.1145/2939672.2939807
+    .. [2] https://github.com/LifangHe/KDD16_HAM/tree/master/SHII_metric
+
+    """
+    if not Directed:
+        G = eg.DiGraph()
+        for edge in G_original.edges:
+            G.add_edge(edge[0], edge[1])
+            G.add_edge(edge[1], edge[0])
+    else:
+        G = G_original.copy()
+    # form pair like {node_1:community_label_1,node_2:community_label_2}
+    node_label_pair = {}
+    for community_label in range(len(C)):
+        for node_i in range(len(C[community_label])):
+            node_label_pair[C[community_label][node_i]] = community_label
+    # print(node_label_pair)
+    seed_shii_pair = {}
+    for community_label in range(len(C)):
+        nodesInCommunity = []
+        seedSetInCommunity = []
+        for node in node_label_pair.keys():
+            if node_label_pair[node] == community_label:
+                nodesInCommunity.append(node)
+                if node in S:
+                    seedSetInCommunity.append(node)
+
+        seedSetSize = int(math.ceil(len(nodesInCommunity) * seedRatio))
+
+        if len(seedSetInCommunity) == 0:
+            continue
+
+        for seed in seedSetInCommunity:
+            print(">>>>>> processing seed ", seed, " now.")
+            oneSeedSet = []
+            if node not in oneSeedSet:
+                oneSeedSet.append(seed)
+            seedNeighborSet = []
+            # using BFS to add neighbors of the SH spanner to the seedNeighborSet as seed candidates
+            queue = []
+            queue.append(seed)
+            while len(queue) > 0:
+                cur_node = queue[0]
+                count_neighbor = 0
+                for neighbor in G.neighbors(node=cur_node):
+                    if neighbor not in seedNeighborSet:
+                        seedNeighborSet.append(neighbor)
+                    count_neighbor = count_neighbor + 1
+                if count_neighbor > 0:
+                    if (
+                        len(queue) == 1
+                        and len(oneSeedSet) + len(seedNeighborSet) < seedSetSize
+                    ):
+                        for node in seedNeighborSet:
+                            if node not in oneSeedSet:
+                                oneSeedSet.append(node)
+                            queue.append(node)
+                        seedNeighborSet.clear()
+                queue.pop(0)
+
+            avg_censor_score_1 = 0.0
+            avg_censor_score_2 = 0.0
+
+            for randIter in range(randSeedIter):
+                if randIter % 5 == 0:
+                    print("seed ", seed, ": ", randIter, " in ", randSeedIter)
+                randSeedSet = []
+                for node in oneSeedSet:
+                    randSeedSet.append(node)
+                seedNeighbors = []
+                for node in seedNeighborSet:
+                    seedNeighbors.append(node)
+                while len(seedNeighbors) > 0 and len(randSeedSet) < seedSetSize:
+                    r = random.randint(0, len(seedNeighbors) - 1)
+                    if seedNeighbors[r] not in randSeedSet:
+                        randSeedSet.append(seedNeighbors[r])
+                    seedNeighbors.pop(r)
+
+                if model == "IC":
+                    censor_score_1, censor_score_2 = _independent_cascade(
+                        G,
+                        randSeedSet,
+                        community_label,
+                        countIterations,
+                        node_label_pair,
+                    )
+                elif model == "LT":
+                    censor_score_1, censor_score_2 = _linear_threshold(
+                        G,
+                        randSeedSet,
+                        community_label,
+                        countIterations,
+                        node_label_pair,
+                    )
+                avg_censor_score_1 += censor_score_1 / randSeedIter
+                avg_censor_score_2 += censor_score_2 / randSeedIter
+                # print("seed ", seed, " avg_censor_score in ", randIter, "is ", censor_score_1 / randSeedIter)
+            if variant:
+                seed_shii_pair[seed] = avg_censor_score_2
+            else:
+                seed_shii_pair[seed] = avg_censor_score_1
+    return seed_shii_pair
+
+
+def _independent_cascade(G, S, community_label, countIterations, node_label_pair):
+    avg_result_1 = 0
+    avg_result_2 = 0
+    N = G.number_of_nodes()
+    for b in range(countIterations):
+        # print(b, " in ", countIterations)
+        p_vw = np.zeros((N, N))  # 节点被激活时，激活其它节点的概率,a对b的影响等于b对a的影响
+        for random_i in range(N):
+            for random_j in range(random_i + 1, N):
+                num = random.random()
+                p_vw[random_i][random_j] = num
+                p_vw[random_j][random_i] = num
+        Q = []
+        activeNodes = []
+        for v in S:
+            Q.append(v)
+            activeNodes.append(v)
+        while len(Q) > 0:
+            v = Q[0]
+            for neighbor in G.neighbors(node=v):
+                if neighbor not in activeNodes:
+                    toss = random.random() + 0.1
+                    if v <= 0 or neighbor <= 0:
+                        print(v, neighbor)
+                    # if toss>0.5:
+                    #     activeNodes.append(neighbor)
+                    #     Q.append(neighbor)
+                    if toss >= p_vw[v - 1][neighbor - 1]:
+                        activeNodes.append(neighbor)
+                        Q.append(neighbor)
+            Q.pop(0)
+        self_cov = 0
+        total_cov = 0
+        uniqueActiveNodes = []
+        for i in activeNodes:
+            if i not in uniqueActiveNodes:
+                uniqueActiveNodes.append(i)
+        for v in uniqueActiveNodes:
+            total_cov += 1
+            if node_label_pair[v] == community_label:
+                self_cov += 1
+        censor_score_1 = (total_cov - self_cov) / total_cov
+        censor_score_2 = (total_cov - self_cov) / self_cov
+        avg_result_1 += censor_score_1 / countIterations
+        avg_result_2 += censor_score_2 / countIterations
+    return avg_result_1, avg_result_2
+
+
+def _linear_threshold(G, S, community_label, countIterations, node_label_pair):
+    tol = 0.00001
+    avg_result_1 = 0
+    avg_result_2 = 0
+    for b in range(countIterations):
+        activeNodes = []
+        # T is the set of nodes that are to be processed
+        T = []
+        Q = {}
+        for v in S:
+            activeNodes.append(v)
+            for neighbor in G.neighbors(node=v):
+                if neighbor not in S:
+                    weight_degree = 1.0 / float(G.in_degree()[neighbor])
+                    if neighbor not in Q.keys():
+                        np = NodeParams(False, weight_degree, random.random())
+                        Q[neighbor] = np
+                        T.append(neighbor)
+                    else:
+                        Q[neighbor].inWeight += weight_degree
+
+        while len(T) > 0:
+            u = T[0]
+            if Q[u].inWeight >= Q[u].threshold + tol and not Q[u].active:
+                activeNodes.append(u)
+                Q[u].active = True
+                for neighbor in G.neighbors(node=u):
+                    if neighbor in S:
+                        continue
+                    weight_degree = 1.0 / float(G.in_degree()[neighbor])
+                    if neighbor not in Q.keys():
+                        np = NodeParams(False, weight_degree, random.random())
+                        Q[neighbor] = np
+                        T.append(neighbor)
+                    else:
+                        if not Q[neighbor].active:
+                            T.append(neighbor)
+                            Q[neighbor].inWeight += weight_degree
+                            if Q[neighbor].inWeight - 1 > tol:
+                                print("Error: the inweight for a node is > 1.")
+            T.pop(0)
+
+        T.clear()
+        Q.clear()
+
+        self_cov = 0
+        total_cov = 0
+        uniqueActiveNodes = []
+        for i in activeNodes:
+            if i not in uniqueActiveNodes:
+                uniqueActiveNodes.append(i)
+        for v in uniqueActiveNodes:
+            total_cov += 1
+            if node_label_pair[v] == community_label:
+                self_cov += 1
+        censor_score_1 = (total_cov - self_cov) / total_cov  # ==> SHII
+        censor_score_2 = (total_cov - self_cov) / self_cov
+        avg_result_1 += censor_score_1 / countIterations
+        avg_result_2 += censor_score_2 / countIterations
+    return avg_result_1, avg_result_2
+
+
+if __name__ == "__main__":
+    G = eg.datasets.get_graph_karateclub()
+    Com = []
+    t1 = [1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 17, 18, 20, 22]
+    Com.append(t1)
+    t2 = [9, 10, 15, 16, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
+    Com.append(t2)
+    print("community_label:", Com)
+    result = structural_hole_influence_index(
+        G, [3, 20, 9], Com, "IC", seedRatio=0.1, Directed=False
+    )
+    print(result)
```

## easygraph/functions/structural_holes/HIS.py

 * *Ordering differences only*

```diff
@@ -1,147 +1,147 @@
-import math
-
-from itertools import combinations
-from typing import List
-
-from easygraph.utils import *
-
-
-__all__ = ["get_structural_holes_HIS"]
-
-
-@not_implemented_for("multigraph")
-def get_structural_holes_HIS(G, C: List[frozenset], epsilon=1e-4, weight="weight"):
-    """Structural hole spanners detection via HIS method.
-
-    Both **HIS** and **MaxD** are methods in [1]_.
-    The authors developed these two methods to find the structural holes spanners,
-    based on theory of information diffusion.
-
-    Returns the value of `S`, `I`, `H` ,defined in **HIS** of [1], of each node in the graph.
-    Note that `H` quantifies the possibility that a node is a structural hole spanner.
-    To use `HIS` method, you should provide the community detection result as parameter.
-
-    Parameters
-    ----------
-    C : list of frozenset
-        Each frozenset denotes a community of nodes.
-
-    epsilon : float
-        The threshold value.
-
-    weight : string, optional (default : 'weight')
-        The key for edge weight.
-
-    Returns
-    -------
-    S : list of tuple
-        The `S` value in [1]_.
-
-    I : float
-        The `I` value in [1]_.
-
-    H : float
-        The `H` value in [1]_.
-
-    See Also
-    --------
-    MaxD
-
-    Examples
-    --------
-
-    >>> get_structural_holes_HIS(G,
-    ...                          C = [frozenset([1,2,3]), frozenset([4,5,6])], # Two communities
-    ...                          epsilon = 0.01,
-    ...                          weight = 'weight'
-    ...                          )
-
-
-    References
-    ----------
-    .. [1] https://www.aminer.cn/structural-hole
-
-    """
-    # S: List[subset_index]
-    S = []
-    for community_subset_size in range(2, len(C) + 1):
-        S.extend(list(combinations(range(len(C)), community_subset_size)))
-    # I: dict[node][cmnt_index]
-    # H: dict[node][subset_index]
-    I, H = initialize(G, C, S, weight=weight)
-
-    alphas = [0.3 for i in range(len(C))]  # list[cmnt_index]
-    betas = [(0.5 - math.pow(0.5, len(subset))) for subset in S]  # list[subset_index]
-
-    while True:
-        P = update_P(G, C, alphas, betas, S, I, H)  # dict[node][cmnt_index]
-        I_new, H_new = update_I_H(G, C, S, P, I)
-        if is_convergence(G, C, I, I_new, epsilon):
-            break
-        else:
-            I, H = I_new, H_new
-    return S, I, H
-
-
-def initialize(G, C: List[frozenset], S: [tuple], weight="weight"):
-    I, H = dict(), dict()
-    for node in G.nodes:
-        I[node] = dict()
-        H[node] = dict()
-
-    for node in G.nodes:
-        for index, community in enumerate(C):
-            if node in community:
-                # TODO: add PageRank or HITS to initialize I
-                I[node][index] = G.degree(weight=weight)[node]
-            else:
-                I[node][index] = 0
-
-    for node in G.nodes:
-        for index, subset in enumerate(S):
-            H[node][index] = min(I[node][i] for i in subset)
-
-    return I, H
-
-
-def update_P(G, C, alphas, betas, S, I, H):
-    P = dict()
-    for node in G.nodes:
-        P[node] = dict()
-
-    for node in G.nodes:
-        for cmnt_index in range(len(C)):
-            subsets_including_current_cmnt = []
-            for subset_index in range(len(S)):
-                if cmnt_index in S[subset_index]:
-                    subsets_including_current_cmnt.append(
-                        alphas[cmnt_index] * I[node][cmnt_index]
-                        + betas[subset_index] * H[node][subset_index]
-                    )
-            P[node][cmnt_index] = max(subsets_including_current_cmnt)
-    return P
-
-
-def update_I_H(G, C, S, P, I):
-    I_new, H_new = dict(), dict()
-    for node in G.nodes:
-        I_new[node] = dict()
-        H_new[node] = dict()
-
-    for node in G.nodes:
-        for cmnt_index in range(len(C)):
-            P_max = max(P[neighbour][cmnt_index] for neighbour in G.adj[node])
-            I_new[node][cmnt_index] = (
-                P_max if (P_max > I[node][cmnt_index]) else I[node][cmnt_index]
-            )
-        for subset_index, subset in enumerate(S):
-            H_new[node][subset_index] = min(I_new[node][i] for i in subset)
-    return I_new, H_new
-
-
-def is_convergence(G, C, I, I_new, epsilon):
-    deltas = []
-    for node in G.nodes:
-        for cmnt_index in range(len(C)):
-            deltas.append(abs(I[node][cmnt_index] - I_new[node][cmnt_index]))
-    return max(deltas) < epsilon
+import math
+
+from itertools import combinations
+from typing import List
+
+from easygraph.utils import *
+
+
+__all__ = ["get_structural_holes_HIS"]
+
+
+@not_implemented_for("multigraph")
+def get_structural_holes_HIS(G, C: List[frozenset], epsilon=1e-4, weight="weight"):
+    """Structural hole spanners detection via HIS method.
+
+    Both **HIS** and **MaxD** are methods in [1]_.
+    The authors developed these two methods to find the structural holes spanners,
+    based on theory of information diffusion.
+
+    Returns the value of `S`, `I`, `H` ,defined in **HIS** of [1], of each node in the graph.
+    Note that `H` quantifies the possibility that a node is a structural hole spanner.
+    To use `HIS` method, you should provide the community detection result as parameter.
+
+    Parameters
+    ----------
+    C : list of frozenset
+        Each frozenset denotes a community of nodes.
+
+    epsilon : float
+        The threshold value.
+
+    weight : string, optional (default : 'weight')
+        The key for edge weight.
+
+    Returns
+    -------
+    S : list of tuple
+        The `S` value in [1]_.
+
+    I : float
+        The `I` value in [1]_.
+
+    H : float
+        The `H` value in [1]_.
+
+    See Also
+    --------
+    MaxD
+
+    Examples
+    --------
+
+    >>> get_structural_holes_HIS(G,
+    ...                          C = [frozenset([1,2,3]), frozenset([4,5,6])], # Two communities
+    ...                          epsilon = 0.01,
+    ...                          weight = 'weight'
+    ...                          )
+
+
+    References
+    ----------
+    .. [1] https://www.aminer.cn/structural-hole
+
+    """
+    # S: List[subset_index]
+    S = []
+    for community_subset_size in range(2, len(C) + 1):
+        S.extend(list(combinations(range(len(C)), community_subset_size)))
+    # I: dict[node][cmnt_index]
+    # H: dict[node][subset_index]
+    I, H = initialize(G, C, S, weight=weight)
+
+    alphas = [0.3 for i in range(len(C))]  # list[cmnt_index]
+    betas = [(0.5 - math.pow(0.5, len(subset))) for subset in S]  # list[subset_index]
+
+    while True:
+        P = update_P(G, C, alphas, betas, S, I, H)  # dict[node][cmnt_index]
+        I_new, H_new = update_I_H(G, C, S, P, I)
+        if is_convergence(G, C, I, I_new, epsilon):
+            break
+        else:
+            I, H = I_new, H_new
+    return S, I, H
+
+
+def initialize(G, C: List[frozenset], S: [tuple], weight="weight"):
+    I, H = dict(), dict()
+    for node in G.nodes:
+        I[node] = dict()
+        H[node] = dict()
+
+    for node in G.nodes:
+        for index, community in enumerate(C):
+            if node in community:
+                # TODO: add PageRank or HITS to initialize I
+                I[node][index] = G.degree(weight=weight)[node]
+            else:
+                I[node][index] = 0
+
+    for node in G.nodes:
+        for index, subset in enumerate(S):
+            H[node][index] = min(I[node][i] for i in subset)
+
+    return I, H
+
+
+def update_P(G, C, alphas, betas, S, I, H):
+    P = dict()
+    for node in G.nodes:
+        P[node] = dict()
+
+    for node in G.nodes:
+        for cmnt_index in range(len(C)):
+            subsets_including_current_cmnt = []
+            for subset_index in range(len(S)):
+                if cmnt_index in S[subset_index]:
+                    subsets_including_current_cmnt.append(
+                        alphas[cmnt_index] * I[node][cmnt_index]
+                        + betas[subset_index] * H[node][subset_index]
+                    )
+            P[node][cmnt_index] = max(subsets_including_current_cmnt)
+    return P
+
+
+def update_I_H(G, C, S, P, I):
+    I_new, H_new = dict(), dict()
+    for node in G.nodes:
+        I_new[node] = dict()
+        H_new[node] = dict()
+
+    for node in G.nodes:
+        for cmnt_index in range(len(C)):
+            P_max = max(P[neighbour][cmnt_index] for neighbour in G.adj[node])
+            I_new[node][cmnt_index] = (
+                P_max if (P_max > I[node][cmnt_index]) else I[node][cmnt_index]
+            )
+        for subset_index, subset in enumerate(S):
+            H_new[node][subset_index] = min(I_new[node][i] for i in subset)
+    return I_new, H_new
+
+
+def is_convergence(G, C, I, I_new, epsilon):
+    deltas = []
+    for node in G.nodes:
+        for cmnt_index in range(len(C)):
+            deltas.append(abs(I[node][cmnt_index] - I_new[node][cmnt_index]))
+    return max(deltas) < epsilon
```

## easygraph/functions/structural_holes/weakTie.py

 * *Ordering differences only*

```diff
@@ -1,338 +1,338 @@
-import easygraph as eg
-
-from easygraph.utils import *
-
-
-__all__ = [
-    "weakTie",
-    "weakTieLocal",
-]
-
-
-def _computeTieStrength(G, node_u, node_v):
-    F_u = set(G.neighbors(node=node_u))
-    F_u.add(node_u)
-    F_v = set(G.neighbors(node=node_v))
-    F_v.add(node_v)
-    uni = len(F_u.union(F_v))
-    inter = len(F_u.intersection(F_v))
-    S_uv = inter / uni
-    G[node_u][node_v]["strength"] = S_uv
-
-
-def _computeAllTieStrength(G):
-    for edge in G.edges:
-        node_u = edge[0]
-        node_v = edge[1]
-        _computeTieStrength(G, node_u, node_v)
-    # print(G.edges)
-
-
-def _strongly_connected_components(G, threshold):
-    """Generate nodes in strongly connected components of graph with constraint threshold.
-
-    Parameters
-    ----------
-    G : easygraph.DiGraph
-        A directed graph.
-
-    threshold: float
-        the edge whose tie strength is smaller than threshold will be ignored.
-
-    Returns
-    -------
-    comp : generator of sets
-        A generator of sets of nodes, one for each strongly connected
-        component of G.
-
-    Examples
-    --------
-    # >>> _strongly_connected_components(G, 0.2)
-
-    Notes
-    -----
-    Uses Tarjan's algorithm[1]_ with Nuutila's modifications[2]_.
-    Nonrecursive version of algorithm.
-
-    References
-    ----------
-    .. [1] Depth-first search and linear graph algorithms, R. Tarjan
-       SIAM Journal of Computing 1(2):146-160, (1972).
-
-    .. [2] On finding the strongly connected components in a directed graph.
-       E. Nuutila and E. Soisalon-Soinen
-       Information Processing Letters 49(1): 9-14, (1994)..
-
-    """
-    preorder = {}
-    lowlink = {}
-    scc_found = set()
-    scc_queue = []
-    i = 0  # Preorder counter
-    for source in G:
-        if source not in scc_found:
-            queue = [source]
-            while queue:
-                v = queue[-1]
-                if v not in preorder:
-                    i = i + 1
-                    preorder[v] = i
-                done = True
-                for w in G[v]:
-                    if G[v][w]["strength"] >= threshold:
-                        if w not in preorder:
-                            queue.append(w)
-                            done = False
-                            break
-                if done:
-                    lowlink[v] = preorder[v]
-                    for w in G[v]:
-                        if G[v][w]["strength"] >= threshold:
-                            if w not in scc_found:
-                                if preorder[w] > preorder[v]:
-                                    lowlink[v] = min([lowlink[v], lowlink[w]])
-                                else:
-                                    lowlink[v] = min([lowlink[v], preorder[w]])
-                    queue.pop()
-                    if lowlink[v] == preorder[v]:
-                        scc = {v}
-                        while scc_queue and preorder[scc_queue[-1]] > preorder[v]:
-                            k = scc_queue.pop()
-                            scc.add(k)
-                        scc_found.update(scc)
-                        yield scc
-                    else:
-                        scc_queue.append(v)
-
-
-def _computeCloseness(G, c, u, threshold, length):
-    n = 0
-    strength_sum_u = 0
-    for v in c:
-        if u in G[v] and v != u:
-            if G[v][u]["strength"] != 0:
-                n += 1
-                strength_sum_u += G[v][u]["strength"]
-    closeness_c_u = (strength_sum_u - n * threshold) / length
-    return closeness_c_u
-
-
-def _computeScore(G, threshold):
-    score_dict = {}
-    for node in G.nodes:
-        score_dict[node] = 0
-    for c in _strongly_connected_components(G, threshold):
-        length = len(c)
-        for u in G.nodes:
-            closeness_c_u = _computeCloseness(G, c, u, threshold, length)
-            if closeness_c_u < 0:
-                score_dict[u] += (-1) * closeness_c_u
-    return score_dict
-
-
-@not_implemented_for("multigraph")
-def weakTie(G, threshold, k):
-    """Return top-k nodes with highest scores which were computed by WeakTie method.
-
-    Parameters
-    ----------
-    G: easygraph.DiGraph
-
-    k: int
-        top - k nodes with highest scores.
-
-    threshold: float
-        tie strength threshold.
-
-    Returns
-    -------
-    SHS_list : list
-        The list of each nodes with highest scores.
-
-    score_dict: dict
-        The score of each node, can be used for WeakTie-Local and WeakTie-Bi.
-
-    See Also
-    -------
-    weakTieLocal
-
-    Examples
-    --------
-    # >>> SHS_list,score_dict=weakTie(G, 0.2, 3)
-
-    References
-    ----------
-    .. [1] Mining Brokers in Dynamic Social Networks. Chonggang Song, Wynne Hsu, Mong Li Lee. Proc. of ACM CIKM, 2015.
-
-    """
-    _computeAllTieStrength(G)
-    score_dict = _computeScore(G, threshold)
-    ordered_set = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)
-    SHS_list = []
-    for i in range(k):
-        SHS_list.append((ordered_set[i])[0])
-    print("score dict:", score_dict)
-    print("top-k nodes:", SHS_list)
-    return SHS_list, score_dict
-
-
-@not_implemented_for("multigraph")
-def _updateScore(u, G, threshold):
-    score_u = 0
-    for c in _strongly_connected_components(G, threshold):
-        length = len(c)
-        closeness_c_u = _computeCloseness(G, c, u, threshold, length)
-        if closeness_c_u < 0:
-            score_u -= closeness_c_u
-    return score_u
-
-
-def _get2hop(G, node):
-    neighbors = []
-    firstlevel = {node: 1}
-    seen = {}  # level (number of hops) when seen in BFS
-    level = 0  # the current level
-    nextlevel = set(firstlevel)  # set of nodes to check at next level
-    n = len(G.adj)
-    while nextlevel and level <= 2:
-        thislevel = nextlevel  # advance to next level
-        nextlevel = set()  # and start a new set (fringe)
-        found = []
-        for v in thislevel:
-            if v not in seen:
-                seen[v] = level  # set the level of vertex v
-                found.append(v)
-                # yield (v, level)
-                neighbors.append(v)
-        if len(seen) == n:
-            return
-        for v in found:
-            nextlevel.update(G.adj[v])
-        level += 1
-    del seen
-    return neighbors
-
-
-def _commonUpdate(G, node_u, node_v, threshold, score_dict):
-    for node_w in G.neighbors(node=node_u):
-        _computeTieStrength(G, node_u, node_w)
-    for node_w in G.predecessors(node=node_u):
-        _computeTieStrength(G, node_w, node_u)
-    G_un = eg.Graph()
-    for node in G.nodes:
-        G_un.add_node(node)
-    for edge in G.edges:
-        if not G_un.has_edge(edge[0], edge[1]):
-            G_un.add_edge(edge[0], edge[1])
-    u_2hop = _get2hop(G_un, node_u)
-    G_u = G.nodes_subgraph(from_nodes=u_2hop)
-    v_2hop = _get2hop(G_un, node_v)
-    G_v = G.nodes_subgraph(from_nodes=v_2hop)
-    score_u = _updateScore(node_u, G_u, threshold)
-    score_v = _updateScore(node_v, G_v, threshold)
-    score_dict[node_u] = score_u
-    score_dict[node_v] = score_v
-    all_neigh_u = list(set(G.all_neighbors(node=node_u)))
-    # print("all_neigh:",all_neigh_u)
-    all_neigh_v = list(set(G.all_neighbors(node=node_v)))
-    for node_w in all_neigh_u:
-        if node_w in all_neigh_v:
-            w_2hop = _get2hop(G_un, node_w)
-            G_w = G.nodes_subgraph(from_nodes=w_2hop)
-            score_w = _updateScore(node_w, G_w, threshold)
-        else:
-            score_w = 0
-            w_2hop = _get2hop(G_un, node_w)
-            G_w = G.nodes_subgraph(from_nodes=w_2hop)
-            for c in _strongly_connected_components(G_w, threshold):
-                if node_u in c:
-                    length = len(c)
-                    closeness_c_w = _computeCloseness(G, c, node_w, threshold, length)
-                    if closeness_c_w < 0:
-                        score_w -= closeness_c_w
-        score_dict[node_w] = score_w
-
-
-def weakTieLocal(G, edges_plus, edges_delete, threshold, score_dict, k):
-    """Find brokers in evolving social networks, utilize the 2-hop neighborhood of an affected node to identify brokers.
-
-    Parameters
-    ----------
-    G: easygraph.DiGraph
-
-    edges_plus: list of list
-        set of edges to be added
-
-    edges_delete: list of list
-        set of edges to be removed
-
-    threshold: float
-        tie strength threshold.
-
-    score_dict: dict
-        The score of each node computed before.
-
-    k: int
-        top - k nodes with highest scores.
-
-    Returns
-    -------
-    SHS_list : list
-        The list of each nodes with highest scores.
-
-    See Also
-    -------
-    weakTie
-
-    Examples
-    --------
-    # >>> SHS_list=weakTieLocal(G, [[2, 7]], [[1,3]], 0.2, score_dict, 3)
-
-    References
-    ----------
-    .. [1] Mining Brokers in Dynamic Social Networks. Chonggang Song, Wynne Hsu, Mong Li Lee. Proc. of ACM CIKM, 2015.
-
-    """
-    for edge in edges_plus:
-        G.add_edge(edge[0], edge[1])
-        _computeTieStrength(G, edge[0], edge[1])
-        _commonUpdate(G, edge[0], edge[1], threshold, score_dict)
-    for edge in edges_delete:
-        G.remove_edge(edge[0], edge[1])
-        _commonUpdate(G, edge[0], edge[1], threshold, score_dict)
-    ordered_set = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)
-    SHS_list = []
-    for i in range(k):
-        SHS_list.append((ordered_set[i])[0])
-    print("updated score:", score_dict)
-    print("top-k nodes:", SHS_list)
-    return SHS_list
-
-
-if __name__ == "__main__":
-    G = eg.DiGraph()
-    G.add_edge(1, 5)
-    G.add_edge(1, 4)
-    G.add_edge(2, 1)
-    G.add_edge(2, 6)
-    G.add_edge(2, 9)
-    G.add_edge(3, 4)
-    G.add_edge(3, 1)
-    G.add_edge(4, 3)
-    G.add_edge(4, 1)
-    G.add_edge(4, 5)
-    G.add_edge(5, 4)
-    G.add_edge(5, 8)
-    G.add_edge(6, 1)
-    G.add_edge(6, 2)
-    G.add_edge(7, 2)
-    G.add_edge(7, 3)
-    G.add_edge(7, 10)
-    G.add_edge(8, 4)
-    G.add_edge(8, 5)
-    G.add_edge(9, 6)
-    G.add_edge(9, 10)
-    G.add_edge(10, 7)
-    G.add_edge(10, 9)
-    SHS_list, score_dict = weakTie(G, 0.2, 3)
-    SHS_list = weakTieLocal(G, [[2, 7]], [[2, 7]], 0.2, score_dict, 3)
+import easygraph as eg
+
+from easygraph.utils import *
+
+
+__all__ = [
+    "weakTie",
+    "weakTieLocal",
+]
+
+
+def _computeTieStrength(G, node_u, node_v):
+    F_u = set(G.neighbors(node=node_u))
+    F_u.add(node_u)
+    F_v = set(G.neighbors(node=node_v))
+    F_v.add(node_v)
+    uni = len(F_u.union(F_v))
+    inter = len(F_u.intersection(F_v))
+    S_uv = inter / uni
+    G[node_u][node_v]["strength"] = S_uv
+
+
+def _computeAllTieStrength(G):
+    for edge in G.edges:
+        node_u = edge[0]
+        node_v = edge[1]
+        _computeTieStrength(G, node_u, node_v)
+    # print(G.edges)
+
+
+def _strongly_connected_components(G, threshold):
+    """Generate nodes in strongly connected components of graph with constraint threshold.
+
+    Parameters
+    ----------
+    G : easygraph.DiGraph
+        A directed graph.
+
+    threshold: float
+        the edge whose tie strength is smaller than threshold will be ignored.
+
+    Returns
+    -------
+    comp : generator of sets
+        A generator of sets of nodes, one for each strongly connected
+        component of G.
+
+    Examples
+    --------
+    # >>> _strongly_connected_components(G, 0.2)
+
+    Notes
+    -----
+    Uses Tarjan's algorithm[1]_ with Nuutila's modifications[2]_.
+    Nonrecursive version of algorithm.
+
+    References
+    ----------
+    .. [1] Depth-first search and linear graph algorithms, R. Tarjan
+       SIAM Journal of Computing 1(2):146-160, (1972).
+
+    .. [2] On finding the strongly connected components in a directed graph.
+       E. Nuutila and E. Soisalon-Soinen
+       Information Processing Letters 49(1): 9-14, (1994)..
+
+    """
+    preorder = {}
+    lowlink = {}
+    scc_found = set()
+    scc_queue = []
+    i = 0  # Preorder counter
+    for source in G:
+        if source not in scc_found:
+            queue = [source]
+            while queue:
+                v = queue[-1]
+                if v not in preorder:
+                    i = i + 1
+                    preorder[v] = i
+                done = True
+                for w in G[v]:
+                    if G[v][w]["strength"] >= threshold:
+                        if w not in preorder:
+                            queue.append(w)
+                            done = False
+                            break
+                if done:
+                    lowlink[v] = preorder[v]
+                    for w in G[v]:
+                        if G[v][w]["strength"] >= threshold:
+                            if w not in scc_found:
+                                if preorder[w] > preorder[v]:
+                                    lowlink[v] = min([lowlink[v], lowlink[w]])
+                                else:
+                                    lowlink[v] = min([lowlink[v], preorder[w]])
+                    queue.pop()
+                    if lowlink[v] == preorder[v]:
+                        scc = {v}
+                        while scc_queue and preorder[scc_queue[-1]] > preorder[v]:
+                            k = scc_queue.pop()
+                            scc.add(k)
+                        scc_found.update(scc)
+                        yield scc
+                    else:
+                        scc_queue.append(v)
+
+
+def _computeCloseness(G, c, u, threshold, length):
+    n = 0
+    strength_sum_u = 0
+    for v in c:
+        if u in G[v] and v != u:
+            if G[v][u]["strength"] != 0:
+                n += 1
+                strength_sum_u += G[v][u]["strength"]
+    closeness_c_u = (strength_sum_u - n * threshold) / length
+    return closeness_c_u
+
+
+def _computeScore(G, threshold):
+    score_dict = {}
+    for node in G.nodes:
+        score_dict[node] = 0
+    for c in _strongly_connected_components(G, threshold):
+        length = len(c)
+        for u in G.nodes:
+            closeness_c_u = _computeCloseness(G, c, u, threshold, length)
+            if closeness_c_u < 0:
+                score_dict[u] += (-1) * closeness_c_u
+    return score_dict
+
+
+@not_implemented_for("multigraph")
+def weakTie(G, threshold, k):
+    """Return top-k nodes with highest scores which were computed by WeakTie method.
+
+    Parameters
+    ----------
+    G: easygraph.DiGraph
+
+    k: int
+        top - k nodes with highest scores.
+
+    threshold: float
+        tie strength threshold.
+
+    Returns
+    -------
+    SHS_list : list
+        The list of each nodes with highest scores.
+
+    score_dict: dict
+        The score of each node, can be used for WeakTie-Local and WeakTie-Bi.
+
+    See Also
+    -------
+    weakTieLocal
+
+    Examples
+    --------
+    # >>> SHS_list,score_dict=weakTie(G, 0.2, 3)
+
+    References
+    ----------
+    .. [1] Mining Brokers in Dynamic Social Networks. Chonggang Song, Wynne Hsu, Mong Li Lee. Proc. of ACM CIKM, 2015.
+
+    """
+    _computeAllTieStrength(G)
+    score_dict = _computeScore(G, threshold)
+    ordered_set = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)
+    SHS_list = []
+    for i in range(k):
+        SHS_list.append((ordered_set[i])[0])
+    print("score dict:", score_dict)
+    print("top-k nodes:", SHS_list)
+    return SHS_list, score_dict
+
+
+@not_implemented_for("multigraph")
+def _updateScore(u, G, threshold):
+    score_u = 0
+    for c in _strongly_connected_components(G, threshold):
+        length = len(c)
+        closeness_c_u = _computeCloseness(G, c, u, threshold, length)
+        if closeness_c_u < 0:
+            score_u -= closeness_c_u
+    return score_u
+
+
+def _get2hop(G, node):
+    neighbors = []
+    firstlevel = {node: 1}
+    seen = {}  # level (number of hops) when seen in BFS
+    level = 0  # the current level
+    nextlevel = set(firstlevel)  # set of nodes to check at next level
+    n = len(G.adj)
+    while nextlevel and level <= 2:
+        thislevel = nextlevel  # advance to next level
+        nextlevel = set()  # and start a new set (fringe)
+        found = []
+        for v in thislevel:
+            if v not in seen:
+                seen[v] = level  # set the level of vertex v
+                found.append(v)
+                # yield (v, level)
+                neighbors.append(v)
+        if len(seen) == n:
+            return
+        for v in found:
+            nextlevel.update(G.adj[v])
+        level += 1
+    del seen
+    return neighbors
+
+
+def _commonUpdate(G, node_u, node_v, threshold, score_dict):
+    for node_w in G.neighbors(node=node_u):
+        _computeTieStrength(G, node_u, node_w)
+    for node_w in G.predecessors(node=node_u):
+        _computeTieStrength(G, node_w, node_u)
+    G_un = eg.Graph()
+    for node in G.nodes:
+        G_un.add_node(node)
+    for edge in G.edges:
+        if not G_un.has_edge(edge[0], edge[1]):
+            G_un.add_edge(edge[0], edge[1])
+    u_2hop = _get2hop(G_un, node_u)
+    G_u = G.nodes_subgraph(from_nodes=u_2hop)
+    v_2hop = _get2hop(G_un, node_v)
+    G_v = G.nodes_subgraph(from_nodes=v_2hop)
+    score_u = _updateScore(node_u, G_u, threshold)
+    score_v = _updateScore(node_v, G_v, threshold)
+    score_dict[node_u] = score_u
+    score_dict[node_v] = score_v
+    all_neigh_u = list(set(G.all_neighbors(node=node_u)))
+    # print("all_neigh:",all_neigh_u)
+    all_neigh_v = list(set(G.all_neighbors(node=node_v)))
+    for node_w in all_neigh_u:
+        if node_w in all_neigh_v:
+            w_2hop = _get2hop(G_un, node_w)
+            G_w = G.nodes_subgraph(from_nodes=w_2hop)
+            score_w = _updateScore(node_w, G_w, threshold)
+        else:
+            score_w = 0
+            w_2hop = _get2hop(G_un, node_w)
+            G_w = G.nodes_subgraph(from_nodes=w_2hop)
+            for c in _strongly_connected_components(G_w, threshold):
+                if node_u in c:
+                    length = len(c)
+                    closeness_c_w = _computeCloseness(G, c, node_w, threshold, length)
+                    if closeness_c_w < 0:
+                        score_w -= closeness_c_w
+        score_dict[node_w] = score_w
+
+
+def weakTieLocal(G, edges_plus, edges_delete, threshold, score_dict, k):
+    """Find brokers in evolving social networks, utilize the 2-hop neighborhood of an affected node to identify brokers.
+
+    Parameters
+    ----------
+    G: easygraph.DiGraph
+
+    edges_plus: list of list
+        set of edges to be added
+
+    edges_delete: list of list
+        set of edges to be removed
+
+    threshold: float
+        tie strength threshold.
+
+    score_dict: dict
+        The score of each node computed before.
+
+    k: int
+        top - k nodes with highest scores.
+
+    Returns
+    -------
+    SHS_list : list
+        The list of each nodes with highest scores.
+
+    See Also
+    -------
+    weakTie
+
+    Examples
+    --------
+    # >>> SHS_list=weakTieLocal(G, [[2, 7]], [[1,3]], 0.2, score_dict, 3)
+
+    References
+    ----------
+    .. [1] Mining Brokers in Dynamic Social Networks. Chonggang Song, Wynne Hsu, Mong Li Lee. Proc. of ACM CIKM, 2015.
+
+    """
+    for edge in edges_plus:
+        G.add_edge(edge[0], edge[1])
+        _computeTieStrength(G, edge[0], edge[1])
+        _commonUpdate(G, edge[0], edge[1], threshold, score_dict)
+    for edge in edges_delete:
+        G.remove_edge(edge[0], edge[1])
+        _commonUpdate(G, edge[0], edge[1], threshold, score_dict)
+    ordered_set = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)
+    SHS_list = []
+    for i in range(k):
+        SHS_list.append((ordered_set[i])[0])
+    print("updated score:", score_dict)
+    print("top-k nodes:", SHS_list)
+    return SHS_list
+
+
+if __name__ == "__main__":
+    G = eg.DiGraph()
+    G.add_edge(1, 5)
+    G.add_edge(1, 4)
+    G.add_edge(2, 1)
+    G.add_edge(2, 6)
+    G.add_edge(2, 9)
+    G.add_edge(3, 4)
+    G.add_edge(3, 1)
+    G.add_edge(4, 3)
+    G.add_edge(4, 1)
+    G.add_edge(4, 5)
+    G.add_edge(5, 4)
+    G.add_edge(5, 8)
+    G.add_edge(6, 1)
+    G.add_edge(6, 2)
+    G.add_edge(7, 2)
+    G.add_edge(7, 3)
+    G.add_edge(7, 10)
+    G.add_edge(8, 4)
+    G.add_edge(8, 5)
+    G.add_edge(9, 6)
+    G.add_edge(9, 10)
+    G.add_edge(10, 7)
+    G.add_edge(10, 9)
+    SHS_list, score_dict = weakTie(G, 0.2, 3)
+    SHS_list = weakTieLocal(G, [[2, 7]], [[2, 7]], 0.2, score_dict, 3)
```

## easygraph/functions/structural_holes/evaluation.py

```diff
@@ -1,396 +1,397 @@
-import math
-
-from easygraph.utils import *
-
-
-__all__ = ["effective_size", "efficiency", "constraint", "hierarchy"]
-
-
-def mutual_weight(G, u, v, weight=None):
-    try:
-        a_uv = G[u][v].get(weight, 1)
-    except KeyError:
-        a_uv = 0
-    try:
-        a_vu = G[v][u].get(weight, 1)
-    except KeyError:
-        a_vu = 0
-    return a_uv + a_vu
-
-
-sum_nmw_rec = {}
-max_nmw_rec = {}
-
-
-def normalized_mutual_weight(G, u, v, norm=sum, weight=None):
-    if norm == sum:
-        try:
-            return sum_nmw_rec[(u, v)]
-        except KeyError:
-            scale = norm(
-                mutual_weight(G, u, w, weight) for w in set(G.all_neighbors(u))
-            )
-            nmw = 0 if scale == 0 else mutual_weight(G, u, v, weight) / scale
-            sum_nmw_rec[(u, v)] = nmw
-            return nmw
-    elif norm == max:
-        try:
-            return max_nmw_rec[(u, v)]
-        except KeyError:
-            scale = norm(
-                mutual_weight(G, u, w, weight) for w in set(G.all_neighbors(u))
-            )
-            nmw = 0 if scale == 0 else mutual_weight(G, u, v, weight) / scale
-            max_nmw_rec[(u, v)] = nmw
-            return nmw
-
-
-def effective_size_parallel(nodes, G, weight):
-    ret = []
-    for node in nodes:
-        neighbors_of_node = set(G.all_neighbors(node))
-        if len(neighbors_of_node) == 0:
-            ret.append([node, float("nan")])
-            continue
-        ret.append(
-            [node, sum(redundancy(G, node, u, weight) for u in neighbors_of_node)]
-        )
-    return ret
-
-
-def effective_size_borgatti_parallel(nodes, G, weight):
-    ret = []
-    for node in nodes:
-        # Effective size is not defined for isolated nodes
-        if len(G[node]) == 0:
-            ret.append([node, float("nan")])
-            continue
-        E = G.ego_subgraph(node)
-        E.remove_node(node)
-        ret.append([node, len(E) - (2 * E.size()) / len(E)])
-    return ret
-
-
-def redundancy(G, u, v, weight=None):
-    nmw = normalized_mutual_weight
-    r = sum(
-        nmw(G, u, w, weight=weight) * nmw(G, v, w, norm=max, weight=weight)
-        for w in set(G.all_neighbors(u))
-    )
-    return 1 - r
-
-
-@not_implemented_for("multigraph")
-@hybrid("cpp_effective_size")
-def effective_size(G, nodes=None, weight=None, n_workers=None):
-    """Burt's metric - Effective Size.
-    Parameters
-    ----------
-    G : easygraph.Graph or easygraph.DiGraph
-    nodes : list of nodes or None, optional (default : None)
-        The nodes you want to calculate. If *None*, all nodes in `G` will be calculated.
-    weight : string or None, optional (default : None)
-        The key for edge weight. If *None*, `G` will be regarded as unweighted graph.
-    Returns
-    -------
-    effective_size : dict
-        The Effective Size of node in `nodes`.
-    Examples
-    --------
-    >>> effective_size(G,
-    ...                nodes=[1,2,3], # Compute the Effective Size of some nodes. The default is None for all nodes in G.
-    ...                weight='weight' # The weight key of the graph. The default is None for unweighted graph.
-    ...                )
-    References
-    ----------
-    .. [1] Burt R S. Structural holes: The social structure of competition[M].
-       Harvard university press, 2009.
-    """
-    sum_nmw_rec.clear()
-    max_nmw_rec.clear()
-    effective_size = {}
-    if nodes is None:
-        nodes = G
-    # Use Borgatti's simplified formula for unweighted and undirected graphs
-    if not G.is_directed() and weight is None:
-        if n_workers is not None:
-            import random
-
-            from functools import partial
-            from multiprocessing import Pool
-
-            local_function = partial(
-                effective_size_borgatti_parallel, G=G, weight=weight
-            )
-            nodes = list(nodes)
-            random.shuffle(nodes)
-            if len(nodes) > n_workers * 50000:
-                nodes = split_len(nodes, step=50000)
-            else:
-                nodes = split(nodes, n_workers)
-            with Pool(n_workers) as p:
-                ret = p.imap(local_function, nodes)
-                res = [x for i in ret for x in i]
-            effective_size = dict(res)
-        else:
-            for v in nodes:
-                # Effective size is not defined for isolated nodes
-                if len(G[v]) == 0:
-                    effective_size[v] = float("nan")
-                    continue
-                E = G.ego_subgraph(v)
-                E.remove_node(v)
-                effective_size[v] = len(E) - (2 * E.size()) / len(E)
-    else:
-        if n_workers is not None:
-            import random
-
-            from functools import partial
-            from multiprocessing import Pool
-
-            local_function = partial(effective_size_parallel, G=G, weight=weight)
-            nodes = list(nodes)
-            random.shuffle(nodes)
-            if len(nodes) > n_workers * 30000:
-                nodes = split_len(nodes, step=30000)
-            else:
-                nodes = split(nodes, n_workers)
-            with Pool(n_workers) as p:
-                ret = p.imap(local_function, nodes)
-                res = [x for i in ret for x in i]
-            effective_size = dict(res)
-        else:
-            for v in nodes:
-                # Effective size is not defined for isolated nodes
-                if len(G[v]) == 0:
-                    effective_size[v] = float("nan")
-                    continue
-                effective_size[v] = sum(
-                    redundancy(G, v, u, weight) for u in set(G.all_neighbors(v))
-                )
-    return effective_size
-
-
-@not_implemented_for("multigraph")
-def efficiency(G, nodes=None, weight=None):
-    """Burt's metric - Efficiency.
-    Parameters
-    ----------
-    G : easygraph.Graph
-    nodes : list of nodes or None, optional (default : None)
-        The nodes you want to calculate. If *None*, all nodes in `G` will be calculated.
-    weight : string or None, optional (default : None)
-        The key for edge weight. If *None*, `G` will be regarded as unweighted graph.
-    Returns
-    -------
-    efficiency : dict
-        The Efficiency of node in `nodes`.
-    Examples
-    --------
-    >>> efficiency(G,
-    ...            nodes=[1,2,3], # Compute the Efficiency of some nodes. The default is None for all nodes in G.
-    ...            weight='weight' # The weight key of the graph. The default is None for unweighted graph.
-    ...            )
-    References
-    ----------
-    .. [1] Burt R S. Structural holes: The social structure of competition[M].
-       Harvard university press, 2009.
-    """
-    e_size = effective_size(G, nodes=nodes, weight=weight)
-    degree = G.degree(weight=weight)
-    efficiency = {n: v / degree[n] for n, v in e_size.items()}
-    return efficiency
-
-
-def compute_constraint_of_nodes(nodes, G, weight):
-    ret = []
-    for node in nodes:
-        neighbors_of_node = set(G.all_neighbors(node))
-        if len(neighbors_of_node) == 0:
-            ret.append([node, float("nan")])
-            continue
-        ret.append(
-            [node, sum(local_constraint(G, node, u, weight) for u in neighbors_of_node)]
-        )
-    return ret
-
-
-@not_implemented_for("multigraph")
-@hybrid("cpp_constraint")
-def constraint(G, nodes=None, weight=None, n_workers=None):
-    """Burt's metric - Constraint.
-    Parameters
-    ----------
-    G : easygraph.Graph
-    nodes : list of nodes or None, optional (default : None)
-        The nodes you want to calculate. If *None*, all nodes in `G` will be calculated.
-    weight : string or None, optional (default : None)
-        The key for edge weight. If *None*, `G` will be regarded as unweighted graph.
-    workers : int or None, optional (default : None)
-        The number of workers calculating (default: None).
-        None if not using only one worker.
-    Returns
-    -------
-    constraint : dict
-        The Constraint of node in `nodes`.
-    Examples
-    --------
-    >>> constraint(G,
-    ...            nodes=[1,2,3], # Compute the Constraint of some nodes. The default is None for all nodes in G.
-    ...            weight='weight', # The weight key of the graph. The default is None for unweighted graph.
-    ...            n_workers=4 # Parallel computing on four workers. The default is None for serial computing.
-    ...            )
-    References
-    ----------
-    .. [1] Burt R S. Structural holes: The social structure of competition[M].
-       Harvard university press, 2009.
-    """
-    sum_nmw_rec.clear()
-    max_nmw_rec.clear()
-    local_constraint_rec.clear()
-    if nodes is None:
-        nodes = G.nodes
-    constraint = {}
-
-    def compute_constraint_of_v(v):
-        neighbors_of_v = set(G.all_neighbors(v))
-        if len(neighbors_of_v) == 0:
-            constraint_of_v = float("nan")
-        else:
-            constraint_of_v = sum(
-                local_constraint(G, v, n, weight) for n in neighbors_of_v
-            )
-        return v, constraint_of_v
-
-    if n_workers is not None:
-        import random
-
-        from functools import partial
-        from multiprocessing import Pool
-
-        local_function = partial(compute_constraint_of_nodes, G=G, weight=weight)
-        nodes = list(nodes)
-        random.shuffle(nodes)
-        if len(nodes) > n_workers * 30000:
-            nodes = split_len(nodes, step=30000)
-        else:
-            nodes = split(nodes, n_workers)
-        with Pool(n_workers) as p:
-            ret = p.imap(local_function, nodes)
-            constraint_results = [x for i in ret for x in i]
-    else:
-        constraint_results = []
-        for v in nodes:
-            constraint_results.append(compute_constraint_of_v(v))
-
-    constraint = dict(constraint_results)
-    return constraint
-
-
-local_constraint_rec = {}
-
-
-def local_constraint(G, u, v, weight=None):
-    try:
-        return local_constraint_rec[(u, v)]
-    except KeyError:
-        nmw = normalized_mutual_weight
-        direct = nmw(G, u, v, weight=weight)
-        indirect = sum(
-            nmw(G, u, w, weight=weight) * nmw(G, w, v, weight=weight)
-            for w in set(G.all_neighbors(u))
-        )
-        result = (direct + indirect) ** 2
-        local_constraint_rec[(u, v)] = result
-        return result
-
-
-def hierarchy_parallel(nodes, G, weight):
-    ret = []
-    for v in nodes:
-        E = G.ego_subgraph(v)
-        n = len(E) - 1
-        C = 0
-        c = {}
-        neighbors_of_v = set(G.all_neighbors(v))
-        for w in neighbors_of_v:
-            C += local_constraint(G, v, w, weight)
-            c[w] = local_constraint(G, v, w, weight)
-        if n > 1:
-            ret.append(
-                [
-                    v,
-                    sum(
-                        c[w] / C * n * math.log(c[w] / C * n) / (n * math.log(n))
-                        for w in neighbors_of_v
-                    ),
-                ]
-            )
-        else:
-            ret.append([v, 0])
-
-    return ret
-
-
-@not_implemented_for("multigraph")
-@hybrid("cpp_hierarchy")
-def hierarchy(G, nodes=None, weight=None, n_workers=None):
-    """Returns the hierarchy of nodes in the graph
-    Parameters
-    ----------
-    G : graph
-    nodes :  dict, optional (default: None)
-    weight : dict, optional (default: None)
-    Returns
-    -------
-    hierarchy : dict
-        the hierarchy of nodes in the graph
-    Examples
-    --------
-    Returns the hierarchy of nodes in the graph G
-    >>> hierarchy(G)
-    Reference
-    ---------
-    https://m.book118.com/html/2019/0318/5320024122002021.shtm
-    """
-    sum_nmw_rec.clear()
-    max_nmw_rec.clear()
-    local_constraint_rec.clear()
-    if nodes is None:
-        nodes = G.nodes
-    hierarchy = {}
-    if n_workers is not None:
-        import random
-
-        from functools import partial
-        from multiprocessing import Pool
-
-        local_function = partial(hierarchy_parallel, G=G, weight=weight)
-        nodes = list(nodes)
-        random.shuffle(nodes)
-        if len(nodes) > n_workers * 30000:
-            nodes = split_len(nodes, step=30000)
-        else:
-            nodes = split(nodes, n_workers)
-        with Pool(n_workers) as p:
-            ret = p.imap(local_function, nodes)
-            res = [x for i in ret for x in i]
-        hierarchy = dict(res)
-    else:
-        for v in nodes:
-            E = G.ego_subgraph(v)
-            n = len(E) - 1
-            C = 0
-            c = {}
-            neighbors_of_v = set(G.all_neighbors(v))
-            for w in neighbors_of_v:
-                C += local_constraint(G, v, w, weight)
-                c[w] = local_constraint(G, v, w, weight)
-            if n > 1:
-                hierarchy[v] = sum(
-                    c[w] / C * n * math.log(c[w] / C * n) / (n * math.log(n))
-                    for w in neighbors_of_v
-                )
-            if v not in hierarchy:
-                hierarchy[v] = 0
-    return hierarchy
+import math
+
+from easygraph.utils import *
+
+
+__all__ = ["effective_size", "efficiency", "constraint", "hierarchy"]
+
+
+def mutual_weight(G, u, v, weight=None):
+    try:
+        a_uv = G[u][v].get(weight, 1)
+    except KeyError:
+        a_uv = 0
+    try:
+        a_vu = G[v][u].get(weight, 1)
+    except KeyError:
+        a_vu = 0
+    return a_uv + a_vu
+
+
+sum_nmw_rec = {}
+max_nmw_rec = {}
+
+
+def normalized_mutual_weight(G, u, v, norm=sum, weight=None):
+    if norm == sum:
+        try:
+            return sum_nmw_rec[(u, v)]
+        except KeyError:
+            scale = norm(
+                mutual_weight(G, u, w, weight) for w in set(G.all_neighbors(u))
+            )
+            nmw = 0 if scale == 0 else mutual_weight(G, u, v, weight) / scale
+            sum_nmw_rec[(u, v)] = nmw
+            return nmw
+    elif norm == max:
+        try:
+            return max_nmw_rec[(u, v)]
+        except KeyError:
+            scale = norm(
+                mutual_weight(G, u, w, weight) for w in set(G.all_neighbors(u))
+            )
+            nmw = 0 if scale == 0 else mutual_weight(G, u, v, weight) / scale
+            max_nmw_rec[(u, v)] = nmw
+            return nmw
+
+
+def effective_size_parallel(nodes, G, weight):
+    ret = []
+    for node in nodes:
+        neighbors_of_node = set(G.all_neighbors(node))
+        if len(neighbors_of_node) == 0:
+            ret.append([node, float("nan")])
+            continue
+        ret.append(
+            [node, sum(redundancy(G, node, u, weight) for u in neighbors_of_node)]
+        )
+    return ret
+
+
+def effective_size_borgatti_parallel(nodes, G, weight):
+    ret = []
+    for node in nodes:
+        # Effective size is not defined for isolated nodes
+        if len(G[node]) == 0:
+            ret.append([node, float("nan")])
+            continue
+        E = G.ego_subgraph(node)
+        E.remove_node(node)
+        ret.append([node, len(E) - (2 * E.size()) / len(E)])
+    return ret
+
+
+def redundancy(G, u, v, weight=None):
+    nmw = normalized_mutual_weight
+    r = sum(
+        nmw(G, u, w, weight=weight) * nmw(G, v, w, norm=max, weight=weight)
+        for w in set(G.all_neighbors(u))
+    )
+    return 1 - r
+
+
+@not_implemented_for("multigraph")
+@hybrid("cpp_effective_size")
+def effective_size(G, nodes=None, weight=None, n_workers=None):
+    """Burt's metric - Effective Size.
+    Parameters
+    ----------
+    G : easygraph.Graph or easygraph.DiGraph
+    nodes : list of nodes or None, optional (default : None)
+        The nodes you want to calculate. If *None*, all nodes in `G` will be calculated.
+    weight : string or None, optional (default : None)
+        The key for edge weight. If *None*, `G` will be regarded as unweighted graph.
+    Returns
+    -------
+    effective_size : dict
+        The Effective Size of node in `nodes`.
+    Examples
+    --------
+    >>> effective_size(G,
+    ...                nodes=[1,2,3], # Compute the Effective Size of some nodes. The default is None for all nodes in G.
+    ...                weight='weight' # The weight key of the graph. The default is None for unweighted graph.
+    ...                )
+    References
+    ----------
+    .. [1] Burt R S. Structural holes: The social structure of competition[M].
+       Harvard university press, 2009.
+    """
+    sum_nmw_rec.clear()
+    max_nmw_rec.clear()
+    effective_size = {}
+    if nodes is None:
+        nodes = G
+    # Use Borgatti's simplified formula for unweighted and undirected graphs
+    if not G.is_directed() and weight is None:
+        if n_workers is not None:
+            import random
+
+            from functools import partial
+            from multiprocessing import Pool
+
+            local_function = partial(
+                effective_size_borgatti_parallel, G=G, weight=weight
+            )
+            nodes = list(nodes)
+            random.shuffle(nodes)
+            if len(nodes) > n_workers * 50000:
+                nodes = split_len(nodes, step=50000)
+            else:
+                nodes = split(nodes, n_workers)
+            with Pool(n_workers) as p:
+                ret = p.imap(local_function, nodes)
+                res = [x for i in ret for x in i]
+            effective_size = dict(res)
+        else:
+            for v in nodes:
+                # Effective size is not defined for isolated nodes
+                if len(G[v]) == 0:
+                    effective_size[v] = float("nan")
+                    continue
+                E = G.ego_subgraph(v)
+                E.remove_node(v)
+                effective_size[v] = len(E) - (2 * E.size()) / len(E)
+    else:
+        if n_workers is not None:
+            import random
+
+            from functools import partial
+            from multiprocessing import Pool
+
+            local_function = partial(effective_size_parallel, G=G, weight=weight)
+            nodes = list(nodes)
+            random.shuffle(nodes)
+            if len(nodes) > n_workers * 30000:
+                nodes = split_len(nodes, step=30000)
+            else:
+                nodes = split(nodes, n_workers)
+            with Pool(n_workers) as p:
+                ret = p.imap(local_function, nodes)
+                res = [x for i in ret for x in i]
+            effective_size = dict(res)
+        else:
+            for v in nodes:
+                # Effective size is not defined for isolated nodes
+                # Check both in-degree and out-degree for directed graphs
+                if G.in_degree(v) == 0 and G.out_degree(v) == 0:
+                    effective_size[v] = float("nan")
+                    continue
+                effective_size[v] = sum(
+                    redundancy(G, v, u, weight) for u in set(G.all_neighbors(v))
+                )
+    return effective_size
+
+
+@not_implemented_for("multigraph")
+def efficiency(G, nodes=None, weight=None):
+    """Burt's metric - Efficiency.
+    Parameters
+    ----------
+    G : easygraph.Graph
+    nodes : list of nodes or None, optional (default : None)
+        The nodes you want to calculate. If *None*, all nodes in `G` will be calculated.
+    weight : string or None, optional (default : None)
+        The key for edge weight. If *None*, `G` will be regarded as unweighted graph.
+    Returns
+    -------
+    efficiency : dict
+        The Efficiency of node in `nodes`.
+    Examples
+    --------
+    >>> efficiency(G,
+    ...            nodes=[1,2,3], # Compute the Efficiency of some nodes. The default is None for all nodes in G.
+    ...            weight='weight' # The weight key of the graph. The default is None for unweighted graph.
+    ...            )
+    References
+    ----------
+    .. [1] Burt R S. Structural holes: The social structure of competition[M].
+       Harvard university press, 2009.
+    """
+    e_size = effective_size(G, nodes=nodes, weight=weight)
+    degree = G.degree(weight=weight)
+    efficiency = {n: v / degree[n] for n, v in e_size.items()}
+    return efficiency
+
+
+def compute_constraint_of_nodes(nodes, G, weight):
+    ret = []
+    for node in nodes:
+        neighbors_of_node = set(G.all_neighbors(node))
+        if len(neighbors_of_node) == 0:
+            ret.append([node, float("nan")])
+            continue
+        ret.append(
+            [node, sum(local_constraint(G, node, u, weight) for u in neighbors_of_node)]
+        )
+    return ret
+
+
+@not_implemented_for("multigraph")
+@hybrid("cpp_constraint")
+def constraint(G, nodes=None, weight=None, n_workers=None):
+    """Burt's metric - Constraint.
+    Parameters
+    ----------
+    G : easygraph.Graph
+    nodes : list of nodes or None, optional (default : None)
+        The nodes you want to calculate. If *None*, all nodes in `G` will be calculated.
+    weight : string or None, optional (default : None)
+        The key for edge weight. If *None*, `G` will be regarded as unweighted graph.
+    workers : int or None, optional (default : None)
+        The number of workers calculating (default: None).
+        None if not using only one worker.
+    Returns
+    -------
+    constraint : dict
+        The Constraint of node in `nodes`.
+    Examples
+    --------
+    >>> constraint(G,
+    ...            nodes=[1,2,3], # Compute the Constraint of some nodes. The default is None for all nodes in G.
+    ...            weight='weight', # The weight key of the graph. The default is None for unweighted graph.
+    ...            n_workers=4 # Parallel computing on four workers. The default is None for serial computing.
+    ...            )
+    References
+    ----------
+    .. [1] Burt R S. Structural holes: The social structure of competition[M].
+       Harvard university press, 2009.
+    """
+    sum_nmw_rec.clear()
+    max_nmw_rec.clear()
+    local_constraint_rec.clear()
+    if nodes is None:
+        nodes = G.nodes
+    constraint = {}
+
+    def compute_constraint_of_v(v):
+        neighbors_of_v = set(G.all_neighbors(v))
+        if len(neighbors_of_v) == 0:
+            constraint_of_v = float("nan")
+        else:
+            constraint_of_v = sum(
+                local_constraint(G, v, n, weight) for n in neighbors_of_v
+            )
+        return v, constraint_of_v
+
+    if n_workers is not None:
+        import random
+
+        from functools import partial
+        from multiprocessing import Pool
+
+        local_function = partial(compute_constraint_of_nodes, G=G, weight=weight)
+        nodes = list(nodes)
+        random.shuffle(nodes)
+        if len(nodes) > n_workers * 30000:
+            nodes = split_len(nodes, step=30000)
+        else:
+            nodes = split(nodes, n_workers)
+        with Pool(n_workers) as p:
+            ret = p.imap(local_function, nodes)
+            constraint_results = [x for i in ret for x in i]
+    else:
+        constraint_results = []
+        for v in nodes:
+            constraint_results.append(compute_constraint_of_v(v))
+
+    constraint = dict(constraint_results)
+    return constraint
+
+
+local_constraint_rec = {}
+
+
+def local_constraint(G, u, v, weight=None):
+    try:
+        return local_constraint_rec[(u, v)]
+    except KeyError:
+        nmw = normalized_mutual_weight
+        direct = nmw(G, u, v, weight=weight)
+        indirect = sum(
+            nmw(G, u, w, weight=weight) * nmw(G, w, v, weight=weight)
+            for w in set(G.all_neighbors(u))
+        )
+        result = (direct + indirect) ** 2
+        local_constraint_rec[(u, v)] = result
+        return result
+
+
+def hierarchy_parallel(nodes, G, weight):
+    ret = []
+    for v in nodes:
+        E = G.ego_subgraph(v)
+        n = len(E) - 1
+        C = 0
+        c = {}
+        neighbors_of_v = set(G.all_neighbors(v))
+        for w in neighbors_of_v:
+            C += local_constraint(G, v, w, weight)
+            c[w] = local_constraint(G, v, w, weight)
+        if n > 1:
+            ret.append(
+                [
+                    v,
+                    sum(
+                        c[w] / C * n * math.log(c[w] / C * n) / (n * math.log(n))
+                        for w in neighbors_of_v
+                    ),
+                ]
+            )
+        else:
+            ret.append([v, 0])
+
+    return ret
+
+
+@not_implemented_for("multigraph")
+@hybrid("cpp_hierarchy")
+def hierarchy(G, nodes=None, weight=None, n_workers=None):
+    """Returns the hierarchy of nodes in the graph
+    Parameters
+    ----------
+    G : graph
+    nodes :  dict, optional (default: None)
+    weight : dict, optional (default: None)
+    Returns
+    -------
+    hierarchy : dict
+        the hierarchy of nodes in the graph
+    Examples
+    --------
+    Returns the hierarchy of nodes in the graph G
+    >>> hierarchy(G)
+    Reference
+    ---------
+    https://m.book118.com/html/2019/0318/5320024122002021.shtm
+    """
+    sum_nmw_rec.clear()
+    max_nmw_rec.clear()
+    local_constraint_rec.clear()
+    if nodes is None:
+        nodes = G.nodes
+    hierarchy = {}
+    if n_workers is not None:
+        import random
+
+        from functools import partial
+        from multiprocessing import Pool
+
+        local_function = partial(hierarchy_parallel, G=G, weight=weight)
+        nodes = list(nodes)
+        random.shuffle(nodes)
+        if len(nodes) > n_workers * 30000:
+            nodes = split_len(nodes, step=30000)
+        else:
+            nodes = split(nodes, n_workers)
+        with Pool(n_workers) as p:
+            ret = p.imap(local_function, nodes)
+            res = [x for i in ret for x in i]
+        hierarchy = dict(res)
+    else:
+        for v in nodes:
+            E = G.ego_subgraph(v)
+            n = len(E) - 1
+            C = 0
+            c = {}
+            neighbors_of_v = set(G.all_neighbors(v))
+            for w in neighbors_of_v:
+                C += local_constraint(G, v, w, weight)
+                c[w] = local_constraint(G, v, w, weight)
+            if n > 1:
+                hierarchy[v] = sum(
+                    c[w] / C * n * math.log(c[w] / C * n) / (n * math.log(n))
+                    for w in neighbors_of_v
+                )
+            if v not in hierarchy:
+                hierarchy[v] = 0
+    return hierarchy
```

## easygraph/functions/structural_holes/SHII_metric.py

 * *Ordering differences only*

```diff
@@ -1,294 +1,294 @@
-import math
-import random
-
-import easygraph as eg
-import numpy as np
-
-
-class NodeParams:
-    def __init__(self, active, inWeight, threshold):
-        self.active = active
-        self.inWeight = inWeight
-        self.threshold = threshold
-
-
-def structural_hole_influence_index(
-    G_original,
-    S,
-    C,
-    model,
-    variant=False,
-    seedRatio=0.05,
-    randSeedIter=10,
-    countIterations=100,
-    Directed=True,
-):
-    """Returns the SHII metric of each seed.
-
-    Parameters
-    ----------
-    G_original: easygraph.Graph or easygraph.DiGraph
-
-    S: list of int
-        A list of nodes which are structural hole spanners.
-
-    C: list of list
-        Each list includes the nodes in one community.
-
-    model: string
-        Propagation Model. Should be IC or LT.
-
-    variant: bool, default is False
-        Whether returns variant SHII ml_metrics or not.
-        variant SHII = # of the influenced outsider / # of the influenced insiders
-        SHII = # of the influenced outsiders / # of the total influenced nodes
-
-    seedRatio: float, default is 0.05
-        # of sampled seeds / # of nodes of the community that the given SHS belongs to.
-
-    randSeedIter: int, default is 10
-        How many iterations to sample seeds.
-
-    countIterations: int default is 100
-        Number of monte carlo simulations to be used.
-
-    Directed: bool, default is True
-        Whether the graph is directed or not.
-
-    Returns
-    -------
-    seed_shii_pair : dict
-        the SHII metric of each seed
-
-    Examples
-    --------
-    # >>> structural_hole_influence_index(G, [3, 20, 9], Com, 'LT', seedRatio=0.1, Directed=False)
-
-    References
-    ----------
-    .. [1] https://dl.acm.org/doi/pdf/10.1145/2939672.2939807
-    .. [2] https://github.com/LifangHe/KDD16_HAM/tree/master/SHII_metric
-
-    """
-    if not Directed:
-        G = eg.DiGraph()
-        for edge in G_original.edges:
-            G.add_edge(edge[0], edge[1])
-            G.add_edge(edge[1], edge[0])
-    else:
-        G = G_original.copy()
-    # form pair like {node_1:community_label_1,node_2:community_label_2}
-    node_label_pair = {}
-    for community_label in range(len(C)):
-        for node_i in range(len(C[community_label])):
-            node_label_pair[C[community_label][node_i]] = community_label
-    # print(node_label_pair)
-    seed_shii_pair = {}
-    for community_label in range(len(C)):
-        nodesInCommunity = []
-        seedSetInCommunity = []
-        for node in node_label_pair.keys():
-            if node_label_pair[node] == community_label:
-                nodesInCommunity.append(node)
-                if node in S:
-                    seedSetInCommunity.append(node)
-
-        seedSetSize = int(math.ceil(len(nodesInCommunity) * seedRatio))
-
-        if len(seedSetInCommunity) == 0:
-            continue
-
-        for seed in seedSetInCommunity:
-            print(">>>>>> processing seed ", seed, " now.")
-            oneSeedSet = []
-            if node not in oneSeedSet:
-                oneSeedSet.append(seed)
-            seedNeighborSet = []
-            # using BFS to add neighbors of the SH spanner to the seedNeighborSet as seed candidates
-            queue = []
-            queue.append(seed)
-            while len(queue) > 0:
-                cur_node = queue[0]
-                count_neighbor = 0
-                for neighbor in G.neighbors(node=cur_node):
-                    if neighbor not in seedNeighborSet:
-                        seedNeighborSet.append(neighbor)
-                    count_neighbor = count_neighbor + 1
-                if count_neighbor > 0:
-                    if (
-                        len(queue) == 1
-                        and len(oneSeedSet) + len(seedNeighborSet) < seedSetSize
-                    ):
-                        for node in seedNeighborSet:
-                            if node not in oneSeedSet:
-                                oneSeedSet.append(node)
-                            queue.append(node)
-                        seedNeighborSet.clear()
-                queue.pop(0)
-
-            avg_censor_score_1 = 0.0
-            avg_censor_score_2 = 0.0
-
-            for randIter in range(randSeedIter):
-                if randIter % 5 == 0:
-                    print("seed ", seed, ": ", randIter, " in ", randSeedIter)
-                randSeedSet = []
-                for node in oneSeedSet:
-                    randSeedSet.append(node)
-                seedNeighbors = []
-                for node in seedNeighborSet:
-                    seedNeighbors.append(node)
-                while len(seedNeighbors) > 0 and len(randSeedSet) < seedSetSize:
-                    r = random.randint(0, len(seedNeighbors) - 1)
-                    if seedNeighbors[r] not in randSeedSet:
-                        randSeedSet.append(seedNeighbors[r])
-                    seedNeighbors.pop(r)
-
-                if model == "IC":
-                    censor_score_1, censor_score_2 = _independent_cascade(
-                        G,
-                        randSeedSet,
-                        community_label,
-                        countIterations,
-                        node_label_pair,
-                    )
-                elif model == "LT":
-                    censor_score_1, censor_score_2 = _linear_threshold(
-                        G,
-                        randSeedSet,
-                        community_label,
-                        countIterations,
-                        node_label_pair,
-                    )
-                avg_censor_score_1 += censor_score_1 / randSeedIter
-                avg_censor_score_2 += censor_score_2 / randSeedIter
-                # print("seed ", seed, " avg_censor_score in ", randIter, "is ", censor_score_1 / randSeedIter)
-            if variant:
-                seed_shii_pair[seed] = avg_censor_score_2
-            else:
-                seed_shii_pair[seed] = avg_censor_score_1
-    return seed_shii_pair
-
-
-def _independent_cascade(G, S, community_label, countIterations, node_label_pair):
-    avg_result_1 = 0
-    avg_result_2 = 0
-    N = G.number_of_nodes()
-    for b in range(countIterations):
-        # print(b, " in ", countIterations)
-        p_vw = np.zeros((N, N))  # 节点被激活时，激活其它节点的概率,a对b的影响等于b对a的影响
-        for random_i in range(N):
-            for random_j in range(random_i + 1, N):
-                num = random.random()
-                p_vw[random_i][random_j] = num
-                p_vw[random_j][random_i] = num
-        Q = []
-        activeNodes = []
-        for v in S:
-            Q.append(v)
-            activeNodes.append(v)
-        while len(Q) > 0:
-            v = Q[0]
-            for neighbor in G.neighbors(node=v):
-                if neighbor not in activeNodes:
-                    toss = random.random() + 0.1
-                    if v <= 0 or neighbor <= 0:
-                        print(v, neighbor)
-                    # if toss>0.5:
-                    #     activeNodes.append(neighbor)
-                    #     Q.append(neighbor)
-                    if toss >= p_vw[v - 1][neighbor - 1]:
-                        activeNodes.append(neighbor)
-                        Q.append(neighbor)
-            Q.pop(0)
-        self_cov = 0
-        total_cov = 0
-        uniqueActiveNodes = []
-        for i in activeNodes:
-            if i not in uniqueActiveNodes:
-                uniqueActiveNodes.append(i)
-        for v in uniqueActiveNodes:
-            total_cov += 1
-            if node_label_pair[v] == community_label:
-                self_cov += 1
-        censor_score_1 = (total_cov - self_cov) / total_cov
-        censor_score_2 = (total_cov - self_cov) / self_cov
-        avg_result_1 += censor_score_1 / countIterations
-        avg_result_2 += censor_score_2 / countIterations
-    return avg_result_1, avg_result_2
-
-
-def _linear_threshold(G, S, community_label, countIterations, node_label_pair):
-    tol = 0.00001
-    avg_result_1 = 0
-    avg_result_2 = 0
-    for b in range(countIterations):
-        activeNodes = []
-        # T is the set of nodes that are to be processed
-        T = []
-        Q = {}
-        for v in S:
-            activeNodes.append(v)
-            for neighbor in G.neighbors(node=v):
-                if neighbor not in S:
-                    weight_degree = 1.0 / float(G.in_degree()[neighbor])
-                    if neighbor not in Q.keys():
-                        np = NodeParams(False, weight_degree, random.random())
-                        Q[neighbor] = np
-                        T.append(neighbor)
-                    else:
-                        Q[neighbor].inWeight += weight_degree
-
-        while len(T) > 0:
-            u = T[0]
-            if Q[u].inWeight >= Q[u].threshold + tol and not Q[u].active:
-                activeNodes.append(u)
-                Q[u].active = True
-                for neighbor in G.neighbors(node=u):
-                    if neighbor in S:
-                        continue
-                    weight_degree = 1.0 / float(G.in_degree()[neighbor])
-                    if neighbor not in Q.keys():
-                        np = NodeParams(False, weight_degree, random.random())
-                        Q[neighbor] = np
-                        T.append(neighbor)
-                    else:
-                        if not Q[neighbor].active:
-                            T.append(neighbor)
-                            Q[neighbor].inWeight += weight_degree
-                            if Q[neighbor].inWeight - 1 > tol:
-                                print("Error: the inweight for a node is > 1.")
-            T.pop(0)
-
-        T.clear()
-        Q.clear()
-
-        self_cov = 0
-        total_cov = 0
-        uniqueActiveNodes = []
-        for i in activeNodes:
-            if i not in uniqueActiveNodes:
-                uniqueActiveNodes.append(i)
-        for v in uniqueActiveNodes:
-            total_cov += 1
-            if node_label_pair[v] == community_label:
-                self_cov += 1
-        censor_score_1 = (total_cov - self_cov) / total_cov  # ==> SHII
-        censor_score_2 = (total_cov - self_cov) / self_cov
-        avg_result_1 += censor_score_1 / countIterations
-        avg_result_2 += censor_score_2 / countIterations
-    return avg_result_1, avg_result_2
-
-
-if __name__ == "__main__":
-    G = eg.datasets.get_graph_karateclub()
-    Com = []
-    t1 = [1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 17, 18, 20, 22]
-    Com.append(t1)
-    t2 = [9, 10, 15, 16, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
-    Com.append(t2)
-    print("community_label:", Com)
-    result = structural_hole_influence_index(
-        G, [3, 20, 9], Com, "IC", seedRatio=0.1, Directed=False
-    )
-    print(result)
+import math
+import random
+
+import easygraph as eg
+import numpy as np
+
+
+class NodeParams:
+    def __init__(self, active, inWeight, threshold):
+        self.active = active
+        self.inWeight = inWeight
+        self.threshold = threshold
+
+
+def structural_hole_influence_index(
+    G_original,
+    S,
+    C,
+    model,
+    variant=False,
+    seedRatio=0.05,
+    randSeedIter=10,
+    countIterations=100,
+    Directed=True,
+):
+    """Returns the SHII metric of each seed.
+
+    Parameters
+    ----------
+    G_original: easygraph.Graph or easygraph.DiGraph
+
+    S: list of int
+        A list of nodes which are structural hole spanners.
+
+    C: list of list
+        Each list includes the nodes in one community.
+
+    model: string
+        Propagation Model. Should be IC or LT.
+
+    variant: bool, default is False
+        Whether returns variant SHII ml_metrics or not.
+        variant SHII = # of the influenced outsider / # of the influenced insiders
+        SHII = # of the influenced outsiders / # of the total influenced nodes
+
+    seedRatio: float, default is 0.05
+        # of sampled seeds / # of nodes of the community that the given SHS belongs to.
+
+    randSeedIter: int, default is 10
+        How many iterations to sample seeds.
+
+    countIterations: int default is 100
+        Number of monte carlo simulations to be used.
+
+    Directed: bool, default is True
+        Whether the graph is directed or not.
+
+    Returns
+    -------
+    seed_shii_pair : dict
+        the SHII metric of each seed
+
+    Examples
+    --------
+    # >>> structural_hole_influence_index(G, [3, 20, 9], Com, 'LT', seedRatio=0.1, Directed=False)
+
+    References
+    ----------
+    .. [1] https://dl.acm.org/doi/pdf/10.1145/2939672.2939807
+    .. [2] https://github.com/LifangHe/KDD16_HAM/tree/master/SHII_metric
+
+    """
+    if not Directed:
+        G = eg.DiGraph()
+        for edge in G_original.edges:
+            G.add_edge(edge[0], edge[1])
+            G.add_edge(edge[1], edge[0])
+    else:
+        G = G_original.copy()
+    # form pair like {node_1:community_label_1,node_2:community_label_2}
+    node_label_pair = {}
+    for community_label in range(len(C)):
+        for node_i in range(len(C[community_label])):
+            node_label_pair[C[community_label][node_i]] = community_label
+    # print(node_label_pair)
+    seed_shii_pair = {}
+    for community_label in range(len(C)):
+        nodesInCommunity = []
+        seedSetInCommunity = []
+        for node in node_label_pair.keys():
+            if node_label_pair[node] == community_label:
+                nodesInCommunity.append(node)
+                if node in S:
+                    seedSetInCommunity.append(node)
+
+        seedSetSize = int(math.ceil(len(nodesInCommunity) * seedRatio))
+
+        if len(seedSetInCommunity) == 0:
+            continue
+
+        for seed in seedSetInCommunity:
+            print(">>>>>> processing seed ", seed, " now.")
+            oneSeedSet = []
+            if node not in oneSeedSet:
+                oneSeedSet.append(seed)
+            seedNeighborSet = []
+            # using BFS to add neighbors of the SH spanner to the seedNeighborSet as seed candidates
+            queue = []
+            queue.append(seed)
+            while len(queue) > 0:
+                cur_node = queue[0]
+                count_neighbor = 0
+                for neighbor in G.neighbors(node=cur_node):
+                    if neighbor not in seedNeighborSet:
+                        seedNeighborSet.append(neighbor)
+                    count_neighbor = count_neighbor + 1
+                if count_neighbor > 0:
+                    if (
+                        len(queue) == 1
+                        and len(oneSeedSet) + len(seedNeighborSet) < seedSetSize
+                    ):
+                        for node in seedNeighborSet:
+                            if node not in oneSeedSet:
+                                oneSeedSet.append(node)
+                            queue.append(node)
+                        seedNeighborSet.clear()
+                queue.pop(0)
+
+            avg_censor_score_1 = 0.0
+            avg_censor_score_2 = 0.0
+
+            for randIter in range(randSeedIter):
+                if randIter % 5 == 0:
+                    print("seed ", seed, ": ", randIter, " in ", randSeedIter)
+                randSeedSet = []
+                for node in oneSeedSet:
+                    randSeedSet.append(node)
+                seedNeighbors = []
+                for node in seedNeighborSet:
+                    seedNeighbors.append(node)
+                while len(seedNeighbors) > 0 and len(randSeedSet) < seedSetSize:
+                    r = random.randint(0, len(seedNeighbors) - 1)
+                    if seedNeighbors[r] not in randSeedSet:
+                        randSeedSet.append(seedNeighbors[r])
+                    seedNeighbors.pop(r)
+
+                if model == "IC":
+                    censor_score_1, censor_score_2 = _independent_cascade(
+                        G,
+                        randSeedSet,
+                        community_label,
+                        countIterations,
+                        node_label_pair,
+                    )
+                elif model == "LT":
+                    censor_score_1, censor_score_2 = _linear_threshold(
+                        G,
+                        randSeedSet,
+                        community_label,
+                        countIterations,
+                        node_label_pair,
+                    )
+                avg_censor_score_1 += censor_score_1 / randSeedIter
+                avg_censor_score_2 += censor_score_2 / randSeedIter
+                # print("seed ", seed, " avg_censor_score in ", randIter, "is ", censor_score_1 / randSeedIter)
+            if variant:
+                seed_shii_pair[seed] = avg_censor_score_2
+            else:
+                seed_shii_pair[seed] = avg_censor_score_1
+    return seed_shii_pair
+
+
+def _independent_cascade(G, S, community_label, countIterations, node_label_pair):
+    avg_result_1 = 0
+    avg_result_2 = 0
+    N = G.number_of_nodes()
+    for b in range(countIterations):
+        # print(b, " in ", countIterations)
+        p_vw = np.zeros((N, N))  # 节点被激活时，激活其它节点的概率,a对b的影响等于b对a的影响
+        for random_i in range(N):
+            for random_j in range(random_i + 1, N):
+                num = random.random()
+                p_vw[random_i][random_j] = num
+                p_vw[random_j][random_i] = num
+        Q = []
+        activeNodes = []
+        for v in S:
+            Q.append(v)
+            activeNodes.append(v)
+        while len(Q) > 0:
+            v = Q[0]
+            for neighbor in G.neighbors(node=v):
+                if neighbor not in activeNodes:
+                    toss = random.random() + 0.1
+                    if v <= 0 or neighbor <= 0:
+                        print(v, neighbor)
+                    # if toss>0.5:
+                    #     activeNodes.append(neighbor)
+                    #     Q.append(neighbor)
+                    if toss >= p_vw[v - 1][neighbor - 1]:
+                        activeNodes.append(neighbor)
+                        Q.append(neighbor)
+            Q.pop(0)
+        self_cov = 0
+        total_cov = 0
+        uniqueActiveNodes = []
+        for i in activeNodes:
+            if i not in uniqueActiveNodes:
+                uniqueActiveNodes.append(i)
+        for v in uniqueActiveNodes:
+            total_cov += 1
+            if node_label_pair[v] == community_label:
+                self_cov += 1
+        censor_score_1 = (total_cov - self_cov) / total_cov
+        censor_score_2 = (total_cov - self_cov) / self_cov
+        avg_result_1 += censor_score_1 / countIterations
+        avg_result_2 += censor_score_2 / countIterations
+    return avg_result_1, avg_result_2
+
+
+def _linear_threshold(G, S, community_label, countIterations, node_label_pair):
+    tol = 0.00001
+    avg_result_1 = 0
+    avg_result_2 = 0
+    for b in range(countIterations):
+        activeNodes = []
+        # T is the set of nodes that are to be processed
+        T = []
+        Q = {}
+        for v in S:
+            activeNodes.append(v)
+            for neighbor in G.neighbors(node=v):
+                if neighbor not in S:
+                    weight_degree = 1.0 / float(G.in_degree()[neighbor])
+                    if neighbor not in Q.keys():
+                        np = NodeParams(False, weight_degree, random.random())
+                        Q[neighbor] = np
+                        T.append(neighbor)
+                    else:
+                        Q[neighbor].inWeight += weight_degree
+
+        while len(T) > 0:
+            u = T[0]
+            if Q[u].inWeight >= Q[u].threshold + tol and not Q[u].active:
+                activeNodes.append(u)
+                Q[u].active = True
+                for neighbor in G.neighbors(node=u):
+                    if neighbor in S:
+                        continue
+                    weight_degree = 1.0 / float(G.in_degree()[neighbor])
+                    if neighbor not in Q.keys():
+                        np = NodeParams(False, weight_degree, random.random())
+                        Q[neighbor] = np
+                        T.append(neighbor)
+                    else:
+                        if not Q[neighbor].active:
+                            T.append(neighbor)
+                            Q[neighbor].inWeight += weight_degree
+                            if Q[neighbor].inWeight - 1 > tol:
+                                print("Error: the inweight for a node is > 1.")
+            T.pop(0)
+
+        T.clear()
+        Q.clear()
+
+        self_cov = 0
+        total_cov = 0
+        uniqueActiveNodes = []
+        for i in activeNodes:
+            if i not in uniqueActiveNodes:
+                uniqueActiveNodes.append(i)
+        for v in uniqueActiveNodes:
+            total_cov += 1
+            if node_label_pair[v] == community_label:
+                self_cov += 1
+        censor_score_1 = (total_cov - self_cov) / total_cov  # ==> SHII
+        censor_score_2 = (total_cov - self_cov) / self_cov
+        avg_result_1 += censor_score_1 / countIterations
+        avg_result_2 += censor_score_2 / countIterations
+    return avg_result_1, avg_result_2
+
+
+if __name__ == "__main__":
+    G = eg.datasets.get_graph_karateclub()
+    Com = []
+    t1 = [1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 17, 18, 20, 22]
+    Com.append(t1)
+    t2 = [9, 10, 15, 16, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
+    Com.append(t2)
+    print("community_label:", Com)
+    result = structural_hole_influence_index(
+        G, [3, 20, 9], Com, "IC", seedRatio=0.1, Directed=False
+    )
+    print(result)
```

## easygraph/functions/structural_holes/AP_Greedy.py

 * *Ordering differences only*

```diff
@@ -1,402 +1,402 @@
-import math
-import random
-
-import easygraph as eg
-
-from easygraph.functions.components.biconnected import generator_articulation_points
-from easygraph.functions.components.connected import connected_components
-from easygraph.utils.decorators import *
-
-
-__all__ = ["common_greedy", "AP_Greedy"]
-
-
-@not_implemented_for("multigraph")
-@only_implemented_for_UnDirected_graph
-def common_greedy(G, k, c=1.0, weight="weight"):
-    """Common greedy method for structural hole spanners detection.
-
-    Returns top k nodes as structural hole spanners,
-    Algorithm 1 of [1]_
-
-    Parameters
-    ----------
-    G : easygraph.Graph
-        An undirected graph.
-
-    k : int
-        top - k structural hole spanners
-
-    c : float, optional (default : 1.0)
-        To define zeta: zeta = c * (n*n*n), and zeta is the large
-        value assigned as the shortest distance of two unreachable
-        vertices.
-        Default is 1.
-
-    weight : String or None, optional (default : 'weight')
-        Key for edge weight. None if not concerning about edge weight.
-
-    Returns
-    -------
-    common_greedy : list
-        The list of each top-k structural hole spanners.
-
-    See Also
-    --------
-    AP_Greedy
-
-    Examples
-    --------
-    Returns the top k nodes as structural hole spanners, using **common_greedy**.
-
-    >>> common_greedy(G,
-    ...               k = 3, # To find top three structural holes spanners.
-    ...               c = 1.0, # To define zeta: zeta = c * (n*n*n), and zeta is the large value assigned as the shortest distance of two unreachable vertices.
-    ...               weight = 'weight')
-
-    References
-    ----------
-    .. [1] https://dl.acm.org/profile/81484650642
-
-    """
-    v_sns = []
-    G_i = G.copy()
-    N = len(G)
-    for i in range(k):
-        sorted_nodes = sort_nodes_by_degree(G_i, weight)
-        C_max = 0
-
-        for j in range(N - i):
-            G_i_j = G_i.copy()
-            G_i_j.remove_node(sorted_nodes[j])
-            upper_bound = procedure1(G_i_j, c)
-            if upper_bound < C_max:
-                pass
-            else:
-                sum_all_shortest_paths = procedure2(G_i_j, c)
-                if sum_all_shortest_paths >= C_max:
-                    v_i = sorted_nodes[j]
-                    C_max = sum_all_shortest_paths
-                else:
-                    pass
-            del G_i_j
-
-        v_sns.append(v_i)
-        G_i.remove_node(v_i)
-
-    del G_i
-    return v_sns
-
-
-def sort_nodes_by_degree(G, weight="weight"):
-    sorted_nodes = []
-    for node, degree in sorted(
-        G.degree(weight=weight).items(), key=lambda x: x[1], reverse=True
-    ):
-        sorted_nodes.append(node)
-    return sorted_nodes
-
-
-def procedure1(G, c=1.0):
-    """
-    Procedure 1 of https://dl.acm.org/profile/81484650642
-
-    Parameters
-    -----------
-    G : graph
-
-    c : float
-        To define zeta: zeta = c * (n*n*n)
-        Default is 1.
-
-    """
-    components = connected_components(G)
-    upper_bound = 0
-    for component in components:
-        component_subgraph = G.nodes_subgraph(from_nodes=list(component))
-        spanning_tree = _get_spanning_tree_of_component(component_subgraph)
-
-        random_root = list(spanning_tree.nodes)[
-            random.randint(0, len(spanning_tree) - 1)
-        ]
-        num_subtree_nodes = _get_num_subtree_nodes(spanning_tree, random_root)
-
-        N_tree = num_subtree_nodes[random_root]
-        for node, num in num_subtree_nodes.items():
-            upper_bound += 2 * num * (N_tree - num)
-
-        del component_subgraph, spanning_tree
-
-    N_G = len(G)
-    zeta = c * math.pow(N_G, 3)
-    for component in components:
-        N_c = len(component)
-        upper_bound += N_c * (N_G - N_c) * zeta
-
-    return upper_bound
-
-
-def _get_spanning_tree_of_component(G):
-    spanning_tree = eg.Graph()
-    seen = set()
-
-    def _plain_dfs(u):
-        for v, edge_data in G.adj[u].items():
-            if v not in seen:
-                seen.add(v)
-                spanning_tree.add_edge(u, v)
-                _plain_dfs(v)
-
-    random_node = list(G.nodes)[0]
-    seen.add(random_node)
-    spanning_tree.add_node(random_node)
-
-    _plain_dfs(random_node)
-
-    return spanning_tree
-
-
-def _get_num_subtree_nodes(G, root):
-    num_subtree_nodes = dict()
-    seen = set()
-
-    def _plain_dfs(u):
-        num_nodes = 1
-        for v, edge_data in G.adj[u].items():
-            if v not in seen:
-                seen.add(v)
-                num_nodes += _plain_dfs(v)
-
-        num_subtree_nodes[u] = num_nodes
-        return num_nodes
-
-    seen.add(root)
-    _plain_dfs(root)
-
-    return num_subtree_nodes
-
-
-def procedure2(G, c=1.0):
-    """
-    Procedure 2 of https://dl.acm.org/profile/81484650642
-
-    Parameters
-    -----------
-    G : graph
-
-    c : float
-        To define zeta: zeta = c * (n*n*n)
-        Default is 1.
-    """
-    components = connected_components(G)
-    C = 0
-    N_G = len(G)
-    zeta = c * math.pow(N_G, 3)
-    for component in components:
-        component_subgraph = G.nodes_subgraph(from_nodes=list(component))
-        C_l = _get_sum_all_shortest_paths_of_component(component_subgraph)
-        N_c = len(component)
-        C += C_l + N_c * (N_G - N_c) * zeta
-
-        del component_subgraph
-
-    return C
-
-
-def _get_sum_all_shortest_paths_of_component(G):
-    # TODO: Using randomized algorithm in http://de.arxiv.org/pdf/1503.08528
-    #       instead of bfs method.
-    def _plain_bfs(G, source):
-        seen = {source}
-        nextlevel = {source}
-        level = 1
-        sum_paths_of_G = 0
-
-        while nextlevel:
-            thislevel = nextlevel
-            nextlevel = set()
-            for u in thislevel:
-                for v in G.adj[u]:
-                    if v not in seen:
-                        seen.add(v)
-                        nextlevel.add(v)
-                        sum_paths_of_G += level
-            level += 1
-        return sum_paths_of_G
-
-    sum_paths = 0
-    for node in G.nodes:
-        sum_paths += _plain_bfs(G, node)
-
-    return sum_paths
-
-
-@not_implemented_for("multigraph")
-@only_implemented_for_UnDirected_graph
-def AP_Greedy(G, k, c=1.0, weight="weight"):
-    """AP greedy method for structural hole spanners detection.
-
-    Returns top k nodes as structural hole spanners,
-    Algorithm 2 of [1]_
-
-    Parameters
-    ----------
-    G : easygraph.Graph
-        An undirected graph.
-
-    k : int
-        top - k structural hole spanners
-
-    c : float, optional (default : 1.0)
-        To define zeta: zeta = c * (n*n*n), and zeta is the large
-        value assigned as the shortest distance of two unreachable
-        vertices.
-        Default is 1.
-
-    weight : String or None, optional (default : 'weight')
-        Key for edge weight. None if not concerning about edge weight.
-
-    Returns
-    -------
-    AP_greedy : list
-        The list of each top-k structural hole spanners.
-
-    Examples
-    --------
-    Returns the top k nodes as structural hole spanners, using **AP_greedy**.
-
-    >>> AP_greedy(G,
-    ...           k = 3, # To find top three structural holes spanners.
-    ...           c = 1.0, # To define zeta: zeta = c * (n*n*n), and zeta is the large value assigned as the shortest distance of two unreachable vertices.
-    ...           weight = 'weight')
-
-    References
-    ----------
-    .. [1] https://dl.acm.org/profile/81484650642
-    """
-    v_sns = []
-    G_i = G.copy()
-    N = len(G)
-    for i in range(k):
-        v_ap, lower_bound = _get_lower_bound_of_ap_nodes(G_i, c)
-        upper_bound = _get_upper_bound_of_non_ap_nodes(G_i, v_ap, c)
-        lower_bound = sorted(lower_bound.items(), key=lambda x: x[1], reverse=True)
-
-        # print(upper_bound)
-        # print(lower_bound)
-        if len(lower_bound) != 0 and lower_bound[0][1] > max(upper_bound):
-            v_i = lower_bound[0][0]
-        else:  # If acticulation points not chosen, use common_greedy instead.
-            sorted_nodes = sort_nodes_by_degree(G_i, weight)
-            C_max = 0
-
-            for j in range(N - i):
-                G_i_j = G_i.copy()
-                G_i_j.remove_node(sorted_nodes[j])
-                upper_bound = procedure1(G_i_j, c)
-                if upper_bound < C_max:
-                    pass
-                else:
-                    sum_all_shortest_paths = procedure2(G_i_j, c)
-                    if sum_all_shortest_paths >= C_max:
-                        v_i = sorted_nodes[j]
-                        C_max = sum_all_shortest_paths
-                    else:
-                        pass
-                del G_i_j
-
-        v_sns.append(v_i)
-        G_i.remove_node(v_i)
-
-    del G_i
-    return v_sns
-
-
-def _get_lower_bound_of_ap_nodes(G, c=1.0):
-    """
-    Returns the articulation points and lower bound for each of them.
-    Procedure 3 of https://dl.acm.org/profile/81484650642
-
-    Parameters
-    ----------
-    G : graph
-        An undirected graph.
-
-    c : float
-        To define zeta: zeta = c * (n*n*n), and zeta is the large
-        value assigned as the shortest distance of two unreachable
-        vertices.
-        Default is 1.
-    """
-    v_ap = []
-    lower_bound = dict()
-
-    N_G = len(G)
-    zeta = c * math.pow(N_G, 3)
-    components = connected_components(G)
-    for component in components:
-        component_subgraph = G.nodes_subgraph(from_nodes=list(component))
-        articulation_points = list(generator_articulation_points(component_subgraph))
-        N_component = len(component_subgraph)
-        for articulation in articulation_points:
-            component_subgraph_after_remove = component_subgraph.copy()
-            component_subgraph_after_remove.remove_node(articulation)
-
-            lower_bound_value = 0
-            lower_bound_value += sum(
-                (len(temp) * (N_G - len(temp))) for temp in components
-            )
-            lower_bound_value += sum(
-                (len(temp) * (N_component - 1 - len(temp)))
-                for temp in connected_components(component_subgraph_after_remove)
-            )
-            lower_bound_value += 2 * N_component - 2 * N_G
-            lower_bound_value *= zeta
-
-            v_ap.append(articulation)
-            lower_bound[articulation] = lower_bound_value
-
-            del component_subgraph_after_remove
-
-        del component_subgraph
-
-    return v_ap, lower_bound
-
-
-def _get_upper_bound_of_non_ap_nodes(G, ap: list, c=1.0):
-    """
-    Returns the upper bound value for each non-articulation points.
-    Eq.(14) of https://dl.acm.org/profile/81484650642
-
-    Parameters
-    ----------
-    G : graph
-        An undirected graph.
-
-    ap : list
-        Articulation points of G.
-
-    c : float
-        To define zeta: zeta = c * (n*n*n), and zeta is the large
-        value assigned as the shortest distance of two unreachable
-        vertices.
-        Default is 1.
-    """
-    upper_bound = []
-
-    N_G = len(G)
-    zeta = c * math.pow(N_G, 3)
-    components = connected_components(G)
-    for component in components:
-        non_articulation_points = component - set(ap)
-        for node in non_articulation_points:
-            upper_bound_value = 0
-            upper_bound_value += sum(
-                (len(temp) * (N_G - len(temp))) for temp in components
-            )
-            upper_bound_value += 2 * len(component) + 1 - 2 * N_G
-            upper_bound_value *= zeta
-
-            upper_bound.append(upper_bound_value)
-
-    return upper_bound
+import math
+import random
+
+import easygraph as eg
+
+from easygraph.functions.components.biconnected import generator_articulation_points
+from easygraph.functions.components.connected import connected_components
+from easygraph.utils.decorators import *
+
+
+__all__ = ["common_greedy", "AP_Greedy"]
+
+
+@not_implemented_for("multigraph")
+@only_implemented_for_UnDirected_graph
+def common_greedy(G, k, c=1.0, weight="weight"):
+    """Common greedy method for structural hole spanners detection.
+
+    Returns top k nodes as structural hole spanners,
+    Algorithm 1 of [1]_
+
+    Parameters
+    ----------
+    G : easygraph.Graph
+        An undirected graph.
+
+    k : int
+        top - k structural hole spanners
+
+    c : float, optional (default : 1.0)
+        To define zeta: zeta = c * (n*n*n), and zeta is the large
+        value assigned as the shortest distance of two unreachable
+        vertices.
+        Default is 1.
+
+    weight : String or None, optional (default : 'weight')
+        Key for edge weight. None if not concerning about edge weight.
+
+    Returns
+    -------
+    common_greedy : list
+        The list of each top-k structural hole spanners.
+
+    See Also
+    --------
+    AP_Greedy
+
+    Examples
+    --------
+    Returns the top k nodes as structural hole spanners, using **common_greedy**.
+
+    >>> common_greedy(G,
+    ...               k = 3, # To find top three structural holes spanners.
+    ...               c = 1.0, # To define zeta: zeta = c * (n*n*n), and zeta is the large value assigned as the shortest distance of two unreachable vertices.
+    ...               weight = 'weight')
+
+    References
+    ----------
+    .. [1] https://dl.acm.org/profile/81484650642
+
+    """
+    v_sns = []
+    G_i = G.copy()
+    N = len(G)
+    for i in range(k):
+        sorted_nodes = sort_nodes_by_degree(G_i, weight)
+        C_max = 0
+
+        for j in range(N - i):
+            G_i_j = G_i.copy()
+            G_i_j.remove_node(sorted_nodes[j])
+            upper_bound = procedure1(G_i_j, c)
+            if upper_bound < C_max:
+                pass
+            else:
+                sum_all_shortest_paths = procedure2(G_i_j, c)
+                if sum_all_shortest_paths >= C_max:
+                    v_i = sorted_nodes[j]
+                    C_max = sum_all_shortest_paths
+                else:
+                    pass
+            del G_i_j
+
+        v_sns.append(v_i)
+        G_i.remove_node(v_i)
+
+    del G_i
+    return v_sns
+
+
+def sort_nodes_by_degree(G, weight="weight"):
+    sorted_nodes = []
+    for node, degree in sorted(
+        G.degree(weight=weight).items(), key=lambda x: x[1], reverse=True
+    ):
+        sorted_nodes.append(node)
+    return sorted_nodes
+
+
+def procedure1(G, c=1.0):
+    """
+    Procedure 1 of https://dl.acm.org/profile/81484650642
+
+    Parameters
+    -----------
+    G : graph
+
+    c : float
+        To define zeta: zeta = c * (n*n*n)
+        Default is 1.
+
+    """
+    components = connected_components(G)
+    upper_bound = 0
+    for component in components:
+        component_subgraph = G.nodes_subgraph(from_nodes=list(component))
+        spanning_tree = _get_spanning_tree_of_component(component_subgraph)
+
+        random_root = list(spanning_tree.nodes)[
+            random.randint(0, len(spanning_tree) - 1)
+        ]
+        num_subtree_nodes = _get_num_subtree_nodes(spanning_tree, random_root)
+
+        N_tree = num_subtree_nodes[random_root]
+        for node, num in num_subtree_nodes.items():
+            upper_bound += 2 * num * (N_tree - num)
+
+        del component_subgraph, spanning_tree
+
+    N_G = len(G)
+    zeta = c * math.pow(N_G, 3)
+    for component in components:
+        N_c = len(component)
+        upper_bound += N_c * (N_G - N_c) * zeta
+
+    return upper_bound
+
+
+def _get_spanning_tree_of_component(G):
+    spanning_tree = eg.Graph()
+    seen = set()
+
+    def _plain_dfs(u):
+        for v, edge_data in G.adj[u].items():
+            if v not in seen:
+                seen.add(v)
+                spanning_tree.add_edge(u, v)
+                _plain_dfs(v)
+
+    random_node = list(G.nodes)[0]
+    seen.add(random_node)
+    spanning_tree.add_node(random_node)
+
+    _plain_dfs(random_node)
+
+    return spanning_tree
+
+
+def _get_num_subtree_nodes(G, root):
+    num_subtree_nodes = dict()
+    seen = set()
+
+    def _plain_dfs(u):
+        num_nodes = 1
+        for v, edge_data in G.adj[u].items():
+            if v not in seen:
+                seen.add(v)
+                num_nodes += _plain_dfs(v)
+
+        num_subtree_nodes[u] = num_nodes
+        return num_nodes
+
+    seen.add(root)
+    _plain_dfs(root)
+
+    return num_subtree_nodes
+
+
+def procedure2(G, c=1.0):
+    """
+    Procedure 2 of https://dl.acm.org/profile/81484650642
+
+    Parameters
+    -----------
+    G : graph
+
+    c : float
+        To define zeta: zeta = c * (n*n*n)
+        Default is 1.
+    """
+    components = connected_components(G)
+    C = 0
+    N_G = len(G)
+    zeta = c * math.pow(N_G, 3)
+    for component in components:
+        component_subgraph = G.nodes_subgraph(from_nodes=list(component))
+        C_l = _get_sum_all_shortest_paths_of_component(component_subgraph)
+        N_c = len(component)
+        C += C_l + N_c * (N_G - N_c) * zeta
+
+        del component_subgraph
+
+    return C
+
+
+def _get_sum_all_shortest_paths_of_component(G):
+    # TODO: Using randomized algorithm in http://de.arxiv.org/pdf/1503.08528
+    #       instead of bfs method.
+    def _plain_bfs(G, source):
+        seen = {source}
+        nextlevel = {source}
+        level = 1
+        sum_paths_of_G = 0
+
+        while nextlevel:
+            thislevel = nextlevel
+            nextlevel = set()
+            for u in thislevel:
+                for v in G.adj[u]:
+                    if v not in seen:
+                        seen.add(v)
+                        nextlevel.add(v)
+                        sum_paths_of_G += level
+            level += 1
+        return sum_paths_of_G
+
+    sum_paths = 0
+    for node in G.nodes:
+        sum_paths += _plain_bfs(G, node)
+
+    return sum_paths
+
+
+@not_implemented_for("multigraph")
+@only_implemented_for_UnDirected_graph
+def AP_Greedy(G, k, c=1.0, weight="weight"):
+    """AP greedy method for structural hole spanners detection.
+
+    Returns top k nodes as structural hole spanners,
+    Algorithm 2 of [1]_
+
+    Parameters
+    ----------
+    G : easygraph.Graph
+        An undirected graph.
+
+    k : int
+        top - k structural hole spanners
+
+    c : float, optional (default : 1.0)
+        To define zeta: zeta = c * (n*n*n), and zeta is the large
+        value assigned as the shortest distance of two unreachable
+        vertices.
+        Default is 1.
+
+    weight : String or None, optional (default : 'weight')
+        Key for edge weight. None if not concerning about edge weight.
+
+    Returns
+    -------
+    AP_greedy : list
+        The list of each top-k structural hole spanners.
+
+    Examples
+    --------
+    Returns the top k nodes as structural hole spanners, using **AP_greedy**.
+
+    >>> AP_greedy(G,
+    ...           k = 3, # To find top three structural holes spanners.
+    ...           c = 1.0, # To define zeta: zeta = c * (n*n*n), and zeta is the large value assigned as the shortest distance of two unreachable vertices.
+    ...           weight = 'weight')
+
+    References
+    ----------
+    .. [1] https://dl.acm.org/profile/81484650642
+    """
+    v_sns = []
+    G_i = G.copy()
+    N = len(G)
+    for i in range(k):
+        v_ap, lower_bound = _get_lower_bound_of_ap_nodes(G_i, c)
+        upper_bound = _get_upper_bound_of_non_ap_nodes(G_i, v_ap, c)
+        lower_bound = sorted(lower_bound.items(), key=lambda x: x[1], reverse=True)
+
+        # print(upper_bound)
+        # print(lower_bound)
+        if len(lower_bound) != 0 and lower_bound[0][1] > max(upper_bound):
+            v_i = lower_bound[0][0]
+        else:  # If acticulation points not chosen, use common_greedy instead.
+            sorted_nodes = sort_nodes_by_degree(G_i, weight)
+            C_max = 0
+
+            for j in range(N - i):
+                G_i_j = G_i.copy()
+                G_i_j.remove_node(sorted_nodes[j])
+                upper_bound = procedure1(G_i_j, c)
+                if upper_bound < C_max:
+                    pass
+                else:
+                    sum_all_shortest_paths = procedure2(G_i_j, c)
+                    if sum_all_shortest_paths >= C_max:
+                        v_i = sorted_nodes[j]
+                        C_max = sum_all_shortest_paths
+                    else:
+                        pass
+                del G_i_j
+
+        v_sns.append(v_i)
+        G_i.remove_node(v_i)
+
+    del G_i
+    return v_sns
+
+
+def _get_lower_bound_of_ap_nodes(G, c=1.0):
+    """
+    Returns the articulation points and lower bound for each of them.
+    Procedure 3 of https://dl.acm.org/profile/81484650642
+
+    Parameters
+    ----------
+    G : graph
+        An undirected graph.
+
+    c : float
+        To define zeta: zeta = c * (n*n*n), and zeta is the large
+        value assigned as the shortest distance of two unreachable
+        vertices.
+        Default is 1.
+    """
+    v_ap = []
+    lower_bound = dict()
+
+    N_G = len(G)
+    zeta = c * math.pow(N_G, 3)
+    components = connected_components(G)
+    for component in components:
+        component_subgraph = G.nodes_subgraph(from_nodes=list(component))
+        articulation_points = list(generator_articulation_points(component_subgraph))
+        N_component = len(component_subgraph)
+        for articulation in articulation_points:
+            component_subgraph_after_remove = component_subgraph.copy()
+            component_subgraph_after_remove.remove_node(articulation)
+
+            lower_bound_value = 0
+            lower_bound_value += sum(
+                (len(temp) * (N_G - len(temp))) for temp in components
+            )
+            lower_bound_value += sum(
+                (len(temp) * (N_component - 1 - len(temp)))
+                for temp in connected_components(component_subgraph_after_remove)
+            )
+            lower_bound_value += 2 * N_component - 2 * N_G
+            lower_bound_value *= zeta
+
+            v_ap.append(articulation)
+            lower_bound[articulation] = lower_bound_value
+
+            del component_subgraph_after_remove
+
+        del component_subgraph
+
+    return v_ap, lower_bound
+
+
+def _get_upper_bound_of_non_ap_nodes(G, ap: list, c=1.0):
+    """
+    Returns the upper bound value for each non-articulation points.
+    Eq.(14) of https://dl.acm.org/profile/81484650642
+
+    Parameters
+    ----------
+    G : graph
+        An undirected graph.
+
+    ap : list
+        Articulation points of G.
+
+    c : float
+        To define zeta: zeta = c * (n*n*n), and zeta is the large
+        value assigned as the shortest distance of two unreachable
+        vertices.
+        Default is 1.
+    """
+    upper_bound = []
+
+    N_G = len(G)
+    zeta = c * math.pow(N_G, 3)
+    components = connected_components(G)
+    for component in components:
+        non_articulation_points = component - set(ap)
+        for node in non_articulation_points:
+            upper_bound_value = 0
+            upper_bound_value += sum(
+                (len(temp) * (N_G - len(temp))) for temp in components
+            )
+            upper_bound_value += 2 * len(component) + 1 - 2 * N_G
+            upper_bound_value *= zeta
+
+            upper_bound.append(upper_bound_value)
+
+    return upper_bound
```

## easygraph/functions/structural_holes/NOBE.py

 * *Ordering differences only*

```diff
@@ -1,167 +1,167 @@
-import easygraph as eg
-import numpy as np
-
-from easygraph.utils import *
-
-
-__all__ = ["NOBE_SH", "NOBE_GA_SH"]
-
-
-@not_implemented_for("multigraph")
-def NOBE_SH(G, K, topk):
-    """detect SH spanners via NOBE[1].
-
-    Parameters
-    ----------
-    G : easygraph.Graph
-        An unweighted and undirected graph.
-
-    K : int
-        Embedding dimension k
-
-    topk : int
-        top - k structural hole spanners
-
-    Returns
-    -------
-    SHS : list
-        The top-k structural hole spanners.
-
-    Examples
-    --------
-    >>> NOBE_SH(G,K=8,topk=5)
-
-    References
-    ----------
-    .. [1] https://www.researchgate.net/publication/325004496_On_Spectral_Graph_Embedding_A_Non-Backtracking_Perspective_and_Graph_Approximation
-
-    """
-    from sklearn.cluster import KMeans
-
-    Y = eg.graph_embedding.NOBE(G, K)
-    dict = {}
-    a = 0
-    for i in G.nodes:
-        dict[i] = a
-        a += 1
-    if isinstance(Y[0, 0], complex):
-        Y = abs(Y)
-    kmeans = KMeans(n_clusters=K, random_state=0).fit(Y)
-    com = {}
-    cluster = {}
-    for i in dict:
-        com[i] = kmeans.labels_[dict[i]]
-    for i in com:
-        if com[i] in cluster:
-            cluster[com[i]].append(i)
-        else:
-            cluster[com[i]] = []
-            cluster[com[i]].append(i)
-    vector = {}
-    for i in dict:
-        vector[i] = Y[dict[i]]
-    rds = RDS(com, cluster, vector, K)
-    rds_sort = sorted(rds.items(), key=lambda d: d[1], reverse=True)
-    SHS = list()
-    a = 0
-    for i in rds_sort:
-        SHS.append(i[0])
-        a += 1
-        if a == topk:
-            break
-    return SHS
-
-
-@not_implemented_for("multigraph")
-def NOBE_GA_SH(G, K, topk):
-    """detect SH spanners via NOBE-GA[1].
-
-    Parameters
-    ----------
-    G : easygraph.Graph
-        An unweighted and undirected graph.
-
-    K : int
-        Embedding dimension k
-
-    topk : int
-        top - k structural hole spanners
-
-    Returns
-    -------
-    SHS : list
-        The top-k structural hole spanners.
-
-    Examples
-    --------
-    >>> NOBE_GA_SH(G,K=8,topk=5)
-
-    References
-    ----------
-    .. [1] https://www.researchgate.net/publication/325004496_On_Spectral_Graph_Embedding_A_Non-Backtracking_Perspective_and_Graph_Approximation
-
-    """
-    from sklearn.cluster import KMeans
-
-    Y = eg.NOBE_GA(G, K)
-    if isinstance(Y[0, 0], complex):
-        Y = abs(Y)
-    kmeans = KMeans(n_clusters=K, random_state=0).fit(Y)
-    com = {}
-    cluster = {}
-    a = 0
-    for i in G.nodes:
-        com[i] = kmeans.labels_[a]
-        a += 1
-    for i in com:
-        if com[i] in cluster:
-            cluster[com[i]].append(i)
-        else:
-            cluster[com[i]] = []
-            cluster[com[i]].append(i)
-    vector = {}
-    a = 0
-    for i in G.nodes:
-        vector[i] = Y[a]
-        a += 1
-    rds = RDS(com, cluster, vector, K)
-    rds_sort = sorted(rds.items(), key=lambda d: d[1], reverse=True)
-    SHS = list()
-    a = 0
-    for i in rds_sort:
-        SHS.append(i[0])
-        a += 1
-        if a == topk:
-            break
-    return SHS
-
-
-def RDS(com, cluster, vector, K):
-    rds = {}
-    Uc = {}
-    Rc = {}
-    for i in cluster:
-        sum_vec = np.zeros(K)
-        for j in cluster[i]:
-            sum_vec += vector[j]
-        Uc[i] = sum_vec / len(cluster[i])
-    for i in cluster:
-        sum_dist = 0
-        for j in cluster[i]:
-            sum_dist += np.linalg.norm(vector[j] - Uc[i])
-        Rc[i] = sum_dist
-    for i in com:
-        maxx = 0
-        fenzi = np.linalg.norm(vector[i] - Uc[com[i]]) / Rc[com[i]]
-        for j in cluster:
-            fenmu = np.linalg.norm(vector[i] - Uc[j]) / Rc[j]
-            if maxx < fenzi / fenmu:
-                maxx = fenzi / fenmu
-        rds[i] = maxx
-    return rds
-
-
-if __name__ == "__main__":
-    G = eg.datasets.get_graph_karateclub()
-    print(NOBE_SH(G, K=2, topk=3))
-    print(NOBE_GA_SH(G, K=2, topk=3))
+import easygraph as eg
+import numpy as np
+
+from easygraph.utils import *
+
+
+__all__ = ["NOBE_SH", "NOBE_GA_SH"]
+
+
+@not_implemented_for("multigraph")
+def NOBE_SH(G, K, topk):
+    """detect SH spanners via NOBE[1].
+
+    Parameters
+    ----------
+    G : easygraph.Graph
+        An unweighted and undirected graph.
+
+    K : int
+        Embedding dimension k
+
+    topk : int
+        top - k structural hole spanners
+
+    Returns
+    -------
+    SHS : list
+        The top-k structural hole spanners.
+
+    Examples
+    --------
+    >>> NOBE_SH(G,K=8,topk=5)
+
+    References
+    ----------
+    .. [1] https://www.researchgate.net/publication/325004496_On_Spectral_Graph_Embedding_A_Non-Backtracking_Perspective_and_Graph_Approximation
+
+    """
+    from sklearn.cluster import KMeans
+
+    Y = eg.graph_embedding.NOBE(G, K)
+    dict = {}
+    a = 0
+    for i in G.nodes:
+        dict[i] = a
+        a += 1
+    if isinstance(Y[0, 0], complex):
+        Y = abs(Y)
+    kmeans = KMeans(n_clusters=K, random_state=0).fit(Y)
+    com = {}
+    cluster = {}
+    for i in dict:
+        com[i] = kmeans.labels_[dict[i]]
+    for i in com:
+        if com[i] in cluster:
+            cluster[com[i]].append(i)
+        else:
+            cluster[com[i]] = []
+            cluster[com[i]].append(i)
+    vector = {}
+    for i in dict:
+        vector[i] = Y[dict[i]]
+    rds = RDS(com, cluster, vector, K)
+    rds_sort = sorted(rds.items(), key=lambda d: d[1], reverse=True)
+    SHS = list()
+    a = 0
+    for i in rds_sort:
+        SHS.append(i[0])
+        a += 1
+        if a == topk:
+            break
+    return SHS
+
+
+@not_implemented_for("multigraph")
+def NOBE_GA_SH(G, K, topk):
+    """detect SH spanners via NOBE-GA[1].
+
+    Parameters
+    ----------
+    G : easygraph.Graph
+        An unweighted and undirected graph.
+
+    K : int
+        Embedding dimension k
+
+    topk : int
+        top - k structural hole spanners
+
+    Returns
+    -------
+    SHS : list
+        The top-k structural hole spanners.
+
+    Examples
+    --------
+    >>> NOBE_GA_SH(G,K=8,topk=5)
+
+    References
+    ----------
+    .. [1] https://www.researchgate.net/publication/325004496_On_Spectral_Graph_Embedding_A_Non-Backtracking_Perspective_and_Graph_Approximation
+
+    """
+    from sklearn.cluster import KMeans
+
+    Y = eg.NOBE_GA(G, K)
+    if isinstance(Y[0, 0], complex):
+        Y = abs(Y)
+    kmeans = KMeans(n_clusters=K, random_state=0).fit(Y)
+    com = {}
+    cluster = {}
+    a = 0
+    for i in G.nodes:
+        com[i] = kmeans.labels_[a]
+        a += 1
+    for i in com:
+        if com[i] in cluster:
+            cluster[com[i]].append(i)
+        else:
+            cluster[com[i]] = []
+            cluster[com[i]].append(i)
+    vector = {}
+    a = 0
+    for i in G.nodes:
+        vector[i] = Y[a]
+        a += 1
+    rds = RDS(com, cluster, vector, K)
+    rds_sort = sorted(rds.items(), key=lambda d: d[1], reverse=True)
+    SHS = list()
+    a = 0
+    for i in rds_sort:
+        SHS.append(i[0])
+        a += 1
+        if a == topk:
+            break
+    return SHS
+
+
+def RDS(com, cluster, vector, K):
+    rds = {}
+    Uc = {}
+    Rc = {}
+    for i in cluster:
+        sum_vec = np.zeros(K)
+        for j in cluster[i]:
+            sum_vec += vector[j]
+        Uc[i] = sum_vec / len(cluster[i])
+    for i in cluster:
+        sum_dist = 0
+        for j in cluster[i]:
+            sum_dist += np.linalg.norm(vector[j] - Uc[i])
+        Rc[i] = sum_dist
+    for i in com:
+        maxx = 0
+        fenzi = np.linalg.norm(vector[i] - Uc[com[i]]) / Rc[com[i]]
+        for j in cluster:
+            fenmu = np.linalg.norm(vector[i] - Uc[j]) / Rc[j]
+            if maxx < fenzi / fenmu:
+                maxx = fenzi / fenmu
+        rds[i] = maxx
+    return rds
+
+
+if __name__ == "__main__":
+    G = eg.datasets.get_graph_karateclub()
+    print(NOBE_SH(G, K=2, topk=3))
+    print(NOBE_GA_SH(G, K=2, topk=3))
```

## easygraph/functions/structural_holes/HAM.py

 * *Ordering differences only*

```diff
@@ -1,300 +1,300 @@
-__all__ = ["get_structural_holes_HAM"]
-from collections import Counter
-
-import numpy as np
-
-from easygraph.utils import *
-
-
-eps = 2.220446049250313e-16
-
-
-def sym(w):
-    import scipy.linalg as spl
-
-    """
-    Initialize a random orthogonal matrix F = w * (wT * w)^ (-1/2)
-    Parameters
-    ----------
-    w : A random matrix.
-
-    Returns
-    -------
-    F : a random orthogonal matrix.
-    """
-    return w.dot(spl.inv(spl.sqrtm(w.T.dot(w))))
-
-
-def avg_entropy(predicted_labels, actual_labels):
-    """
-    Calculate the average entropy between predicted_labels and actual_labels.
-
-    Parameters
-    ----------
-    predicted_labels : a Ndarray of predicted_labels.
-    actual_labels : a Ndarray of actual_labels.
-
-    Returns
-    -------
-    A float of average entropy.
-    """
-    import scipy.stats as stat
-
-    actual_labels_dict = {}
-    predicted_labels_dict = {}
-    for label in np.unique(actual_labels):
-        actual_labels_dict[label] = np.nonzero(actual_labels == label)[0]
-    for label in np.unique(predicted_labels):
-        predicted_labels_dict[label] = np.nonzero(predicted_labels == label)[0]
-    avg_value = 0
-    N = len(predicted_labels)
-    # store entropy for each community
-    for label, items in predicted_labels_dict.items():
-        N_i = float(len(items))
-        p_i = []
-        for label2, items2 in actual_labels_dict.items():
-            common = set(items.tolist()).intersection(set(items2.tolist()))
-            p_ij = float(len(common)) / N_i
-            p_i.append(p_ij)
-        entropy_i = stat.entropy(p_i)
-        avg_value += entropy_i * (N_i / float(N))
-    return avg_value
-
-
-def load_adj_matrix(G):
-    """
-    Transfer the graph into sparse matrix.
-    Parameters
-    ----------
-    G : graph
-        An undirected graph.
-
-    Returns
-    -------
-    A : A sparse matrix A
-    """
-    import scipy.sparse as sps
-
-    listE = []
-    for edge in G.edges:
-        listE.append(edge[0] - 1)
-        listE.append(edge[1] - 1)
-    adj_tuples = np.array(listE).reshape(-1, 2)
-    n = len(np.unique(adj_tuples))
-    vals = np.array([1] * len(G.edges))
-    max_id = max(max(adj_tuples[:, 0]), max(adj_tuples[:, 1])) + 1
-    A = sps.csr_matrix(
-        (vals, (adj_tuples[:, 0], adj_tuples[:, 1])), shape=(max_id, max_id)
-    )
-    A = A + A.T
-    return sps.csr_matrix(A)
-
-
-def majority_voting(votes):
-    """
-    majority voting.
-
-    Parameters
-    ----------
-    votes : a Ndarray of votes
-
-    Returns
-    -------
-    the most common label.
-    """
-    C = Counter(votes)
-    pairs = C.most_common(2)
-    if len(pairs) == 0:
-        return 0
-    if pairs[0][0] > 0:
-        return pairs[0][0]
-    elif len(pairs) > 1:
-        return pairs[1][0]
-    else:
-        return 0
-
-
-def label_by_neighbors(AdjMat, labels):
-    """
-    classifify SHS using majority voting.
-
-    Parameters
-    ----------
-    AdjMat : adjacency matrix
-    labels : a Ndarray of labeled communities of the nodes.
-
-    Returns
-    -------
-    labels : a Ndarray of labeled communities of the nodes.
-    """
-    assert AdjMat.shape[0] == len(labels), "dimensions are not equal"
-    unlabeled_idx = labels == 0
-    num_unlabeled = sum(unlabeled_idx)
-    count = 0
-    while num_unlabeled > 0:
-        idxs = np.array(np.nonzero(unlabeled_idx)[0])
-        next_labels = np.zeros(len(labels))
-        for idx in idxs:
-            neighbors = np.nonzero(AdjMat[idx, :] > 0)[1]
-            if len(neighbors) == 0:
-                next_labels[idx] = majority_voting(labels)
-            else:
-                neighbor_labels = labels[neighbors]
-                next_labels[idx] = majority_voting(neighbor_labels)
-        labels[idxs] = next_labels[idxs]
-        unlabeled_idx = labels == 0
-        num_unlabeled = sum(unlabeled_idx)
-    return labels
-
-
-@not_implemented_for("multigraph")
-def get_structural_holes_HAM(G, k, c, ground_truth_labels):
-    """Structural hole spanners detection via HAM method.
-
-    Using HAM [1]_ to jointly detect SHS and communities.
-
-    Parameters
-    ----------
-    G : easygraph.Graph
-        An undirected graph.
-
-    k : int
-        top - k structural hole spanners
-
-    c : int
-        the number of communities
-
-    ground_truth_labels : list of lists
-        The label of each node's community.
-
-    Returns
-    -------
-    top_k_nodes : list
-        The top-k structural hole spanners.
-
-    SH_score : dict
-        The structural hole spanners score for each node, given by HAM.
-
-    cmnt_labels : dict
-        The communities label of each node.
-
-
-    Examples
-    --------
-
-    >>> get_structural_holes_HAM(G,
-    ...                         k = 2, # To find top two structural holes spanners.
-    ...                          c = 2,
-    ...                          ground_truth_labels = [[0], [0], [1], [0], [1]] # The ground truth labels for each node - community detection result, for example.
-    ...                         )
-
-    References
-    ----------
-    .. [1] https://dl.acm.org/doi/10.1145/2939672.2939807
-
-    """
-    import scipy.linalg as spl
-    import scipy.sparse as sps
-
-    from scipy.cluster.vq import kmeans
-    from scipy.cluster.vq import vq
-    from sklearn import metrics
-
-    G_index, _, node_of_index = G.to_index_node_graph(begin_index=1)
-
-    A_mat = load_adj_matrix(G_index)
-    A = A_mat  # adjacency matrix
-    n = A.shape[0]  # the number of nodes
-
-    epsilon = 1e-4  # smoothing value: epsilon
-    max_iter = 50  # maximum iteration value
-    seeeed = 5433
-    np.random.seed(seeeed)
-    topk = k
-
-    # Inv of degree matrix D^-1
-    invD = sps.diags((np.array(A.sum(axis=0))[0, :] + eps) ** (-1.0), 0)
-    # Laplacian matrix L = I - D^-1 * A
-    L = (sps.identity(n) - invD.dot(A)).tocsr()
-    # Initialize a random orthogonal matrix F
-    F = sym(np.random.random((n, c)))
-
-    # Algorithm 1
-    for step in range(max_iter):
-        Q = sps.identity(n).tocsr()
-        P = L.dot(F)
-        for i in range(n):
-            Q[i, i] = 0.5 / (spl.norm(P[i, :]) + epsilon)
-
-        R = L.T.dot(Q).dot(L)
-
-        W, V = np.linalg.eigh(R.todense())
-        Wsort = np.argsort(W)  # sort from smallest to largest
-        F = V[:, Wsort[0:c]]  # select the smallest eigenvectors
-
-    # find SH spanner
-    SH = np.zeros((n,))
-    for i in range(n):
-        SH[i] = np.linalg.norm(F[i, :])
-    SHrank = np.argsort(SH)  # index of SH
-
-    # METRICS BEGIN
-
-    to_keep_index = np.sort(SHrank[topk:])
-    A_temp = A[to_keep_index, :]
-    A_temp = A_temp[:, to_keep_index]
-    HAM_labels_keep = np.asarray(ground_truth_labels)[to_keep_index]
-    allLabels = np.asarray(ground_truth_labels)
-
-    cluster_matrix = F
-    labelbook, distortion = kmeans(cluster_matrix[to_keep_index, :], c)
-    HAM_labels, dist = vq(cluster_matrix[to_keep_index, :], labelbook)
-
-    print("AMI")
-    print(
-        "HAM: "
-        + str(metrics.adjusted_mutual_info_score(HAM_labels, HAM_labels_keep.T[0]))
-    )
-
-    # classifify SHS using majority voting
-    predLabels = np.zeros(len(ground_truth_labels))
-    predLabels[to_keep_index] = HAM_labels + 1
-
-    HAM_predLabels = label_by_neighbors(A, predLabels)
-    print(
-        "HAM_all: "
-        + str(metrics.adjusted_mutual_info_score(HAM_predLabels, allLabels.T[0]))
-    )
-
-    print("NMI")
-    print(
-        "HAM: "
-        + str(metrics.normalized_mutual_info_score(HAM_labels, HAM_labels_keep.T[0]))
-    )
-    print(
-        "HAM_all: "
-        + str(metrics.normalized_mutual_info_score(HAM_predLabels, allLabels.T[0]))
-    )
-
-    print("Entropy")
-    print("HAM: " + str(avg_entropy(HAM_labels, HAM_labels_keep.T[0])))
-    print("HAM_all: " + str(avg_entropy(HAM_predLabels, allLabels.T[0])))
-
-    # METRICS END
-
-    SH_score = dict()
-    for index, rank in enumerate(SHrank):
-        SH_score[node_of_index[index + 1]] = int(rank)
-
-    cmnt_labels = dict()
-    for index, label in enumerate(HAM_predLabels):
-        cmnt_labels[node_of_index[index + 1]] = int(label)
-
-    # top-k SHS
-    top_k_ind = np.argpartition(SHrank, -k)[-k:]
-    top_k_ind = top_k_ind[np.argsort(SHrank[top_k_ind])[::-1][:k]]
-    top_k_nodes = []
-    for ind in top_k_ind:
-        top_k_nodes.append(node_of_index[ind + 1])
-
-    return top_k_nodes, SH_score, cmnt_labels
+__all__ = ["get_structural_holes_HAM"]
+from collections import Counter
+
+import numpy as np
+
+from easygraph.utils import *
+
+
+eps = 2.220446049250313e-16
+
+
+def sym(w):
+    import scipy.linalg as spl
+
+    """
+    Initialize a random orthogonal matrix F = w * (wT * w)^ (-1/2)
+    Parameters
+    ----------
+    w : A random matrix.
+
+    Returns
+    -------
+    F : a random orthogonal matrix.
+    """
+    return w.dot(spl.inv(spl.sqrtm(w.T.dot(w))))
+
+
+def avg_entropy(predicted_labels, actual_labels):
+    """
+    Calculate the average entropy between predicted_labels and actual_labels.
+
+    Parameters
+    ----------
+    predicted_labels : a Ndarray of predicted_labels.
+    actual_labels : a Ndarray of actual_labels.
+
+    Returns
+    -------
+    A float of average entropy.
+    """
+    import scipy.stats as stat
+
+    actual_labels_dict = {}
+    predicted_labels_dict = {}
+    for label in np.unique(actual_labels):
+        actual_labels_dict[label] = np.nonzero(actual_labels == label)[0]
+    for label in np.unique(predicted_labels):
+        predicted_labels_dict[label] = np.nonzero(predicted_labels == label)[0]
+    avg_value = 0
+    N = len(predicted_labels)
+    # store entropy for each community
+    for label, items in predicted_labels_dict.items():
+        N_i = float(len(items))
+        p_i = []
+        for label2, items2 in actual_labels_dict.items():
+            common = set(items.tolist()).intersection(set(items2.tolist()))
+            p_ij = float(len(common)) / N_i
+            p_i.append(p_ij)
+        entropy_i = stat.entropy(p_i)
+        avg_value += entropy_i * (N_i / float(N))
+    return avg_value
+
+
+def load_adj_matrix(G):
+    """
+    Transfer the graph into sparse matrix.
+    Parameters
+    ----------
+    G : graph
+        An undirected graph.
+
+    Returns
+    -------
+    A : A sparse matrix A
+    """
+    import scipy.sparse as sps
+
+    listE = []
+    for edge in G.edges:
+        listE.append(edge[0] - 1)
+        listE.append(edge[1] - 1)
+    adj_tuples = np.array(listE).reshape(-1, 2)
+    n = len(np.unique(adj_tuples))
+    vals = np.array([1] * len(G.edges))
+    max_id = max(max(adj_tuples[:, 0]), max(adj_tuples[:, 1])) + 1
+    A = sps.csr_matrix(
+        (vals, (adj_tuples[:, 0], adj_tuples[:, 1])), shape=(max_id, max_id)
+    )
+    A = A + A.T
+    return sps.csr_matrix(A)
+
+
+def majority_voting(votes):
+    """
+    majority voting.
+
+    Parameters
+    ----------
+    votes : a Ndarray of votes
+
+    Returns
+    -------
+    the most common label.
+    """
+    C = Counter(votes)
+    pairs = C.most_common(2)
+    if len(pairs) == 0:
+        return 0
+    if pairs[0][0] > 0:
+        return pairs[0][0]
+    elif len(pairs) > 1:
+        return pairs[1][0]
+    else:
+        return 0
+
+
+def label_by_neighbors(AdjMat, labels):
+    """
+    classifify SHS using majority voting.
+
+    Parameters
+    ----------
+    AdjMat : adjacency matrix
+    labels : a Ndarray of labeled communities of the nodes.
+
+    Returns
+    -------
+    labels : a Ndarray of labeled communities of the nodes.
+    """
+    assert AdjMat.shape[0] == len(labels), "dimensions are not equal"
+    unlabeled_idx = labels == 0
+    num_unlabeled = sum(unlabeled_idx)
+    count = 0
+    while num_unlabeled > 0:
+        idxs = np.array(np.nonzero(unlabeled_idx)[0])
+        next_labels = np.zeros(len(labels))
+        for idx in idxs:
+            neighbors = np.nonzero(AdjMat[idx, :] > 0)[1]
+            if len(neighbors) == 0:
+                next_labels[idx] = majority_voting(labels)
+            else:
+                neighbor_labels = labels[neighbors]
+                next_labels[idx] = majority_voting(neighbor_labels)
+        labels[idxs] = next_labels[idxs]
+        unlabeled_idx = labels == 0
+        num_unlabeled = sum(unlabeled_idx)
+    return labels
+
+
+@not_implemented_for("multigraph")
+def get_structural_holes_HAM(G, k, c, ground_truth_labels):
+    """Structural hole spanners detection via HAM method.
+
+    Using HAM [1]_ to jointly detect SHS and communities.
+
+    Parameters
+    ----------
+    G : easygraph.Graph
+        An undirected graph.
+
+    k : int
+        top - k structural hole spanners
+
+    c : int
+        the number of communities
+
+    ground_truth_labels : list of lists
+        The label of each node's community.
+
+    Returns
+    -------
+    top_k_nodes : list
+        The top-k structural hole spanners.
+
+    SH_score : dict
+        The structural hole spanners score for each node, given by HAM.
+
+    cmnt_labels : dict
+        The communities label of each node.
+
+
+    Examples
+    --------
+
+    >>> get_structural_holes_HAM(G,
+    ...                         k = 2, # To find top two structural holes spanners.
+    ...                          c = 2,
+    ...                          ground_truth_labels = [[0], [0], [1], [0], [1]] # The ground truth labels for each node - community detection result, for example.
+    ...                         )
+
+    References
+    ----------
+    .. [1] https://dl.acm.org/doi/10.1145/2939672.2939807
+
+    """
+    import scipy.linalg as spl
+    import scipy.sparse as sps
+
+    from scipy.cluster.vq import kmeans
+    from scipy.cluster.vq import vq
+    from sklearn import metrics
+
+    G_index, _, node_of_index = G.to_index_node_graph(begin_index=1)
+
+    A_mat = load_adj_matrix(G_index)
+    A = A_mat  # adjacency matrix
+    n = A.shape[0]  # the number of nodes
+
+    epsilon = 1e-4  # smoothing value: epsilon
+    max_iter = 50  # maximum iteration value
+    seeeed = 5433
+    np.random.seed(seeeed)
+    topk = k
+
+    # Inv of degree matrix D^-1
+    invD = sps.diags((np.array(A.sum(axis=0))[0, :] + eps) ** (-1.0), 0)
+    # Laplacian matrix L = I - D^-1 * A
+    L = (sps.identity(n) - invD.dot(A)).tocsr()
+    # Initialize a random orthogonal matrix F
+    F = sym(np.random.random((n, c)))
+
+    # Algorithm 1
+    for step in range(max_iter):
+        Q = sps.identity(n).tocsr()
+        P = L.dot(F)
+        for i in range(n):
+            Q[i, i] = 0.5 / (spl.norm(P[i, :]) + epsilon)
+
+        R = L.T.dot(Q).dot(L)
+
+        W, V = np.linalg.eigh(R.todense())
+        Wsort = np.argsort(W)  # sort from smallest to largest
+        F = V[:, Wsort[0:c]]  # select the smallest eigenvectors
+
+    # find SH spanner
+    SH = np.zeros((n,))
+    for i in range(n):
+        SH[i] = np.linalg.norm(F[i, :])
+    SHrank = np.argsort(SH)  # index of SH
+
+    # METRICS BEGIN
+
+    to_keep_index = np.sort(SHrank[topk:])
+    A_temp = A[to_keep_index, :]
+    A_temp = A_temp[:, to_keep_index]
+    HAM_labels_keep = np.asarray(ground_truth_labels)[to_keep_index]
+    allLabels = np.asarray(ground_truth_labels)
+
+    cluster_matrix = F
+    labelbook, distortion = kmeans(cluster_matrix[to_keep_index, :], c)
+    HAM_labels, dist = vq(cluster_matrix[to_keep_index, :], labelbook)
+
+    print("AMI")
+    print(
+        "HAM: "
+        + str(metrics.adjusted_mutual_info_score(HAM_labels, HAM_labels_keep.T[0]))
+    )
+
+    # classifify SHS using majority voting
+    predLabels = np.zeros(len(ground_truth_labels))
+    predLabels[to_keep_index] = HAM_labels + 1
+
+    HAM_predLabels = label_by_neighbors(A, predLabels)
+    print(
+        "HAM_all: "
+        + str(metrics.adjusted_mutual_info_score(HAM_predLabels, allLabels.T[0]))
+    )
+
+    print("NMI")
+    print(
+        "HAM: "
+        + str(metrics.normalized_mutual_info_score(HAM_labels, HAM_labels_keep.T[0]))
+    )
+    print(
+        "HAM_all: "
+        + str(metrics.normalized_mutual_info_score(HAM_predLabels, allLabels.T[0]))
+    )
+
+    print("Entropy")
+    print("HAM: " + str(avg_entropy(HAM_labels, HAM_labels_keep.T[0])))
+    print("HAM_all: " + str(avg_entropy(HAM_predLabels, allLabels.T[0])))
+
+    # METRICS END
+
+    SH_score = dict()
+    for index, rank in enumerate(SHrank):
+        SH_score[node_of_index[index + 1]] = int(rank)
+
+    cmnt_labels = dict()
+    for index, label in enumerate(HAM_predLabels):
+        cmnt_labels[node_of_index[index + 1]] = int(label)
+
+    # top-k SHS
+    top_k_ind = np.argpartition(SHrank, -k)[-k:]
+    top_k_ind = top_k_ind[np.argsort(SHrank[top_k_ind])[::-1][:k]]
+    top_k_nodes = []
+    for ind in top_k_ind:
+        top_k_nodes.append(node_of_index[ind + 1])
+
+    return top_k_nodes, SH_score, cmnt_labels
```

## easygraph/functions/structural_holes/__init__.py

 * *Ordering differences only*

```diff
@@ -1,8 +1,8 @@
-from .AP_Greedy import *
-from .evaluation import *
-from .HAM import *
-from .HIS import *
-from .ICC import *
-from .MaxD import *
-from .metrics import *
-from .NOBE import *
+from .AP_Greedy import *
+from .evaluation import *
+from .HAM import *
+from .HIS import *
+from .ICC import *
+from .MaxD import *
+from .metrics import *
+from .NOBE import *
```

## easygraph/functions/graph_generator/classic.py

 * *Ordering differences only*

```diff
@@ -1,73 +1,73 @@
-import itertools
-
-from easygraph.classes import Graph
-from easygraph.utils import nodes_or_number
-from easygraph.utils import pairwise
-
-
-__all__ = ["empty_graph", "path_graph", "complete_graph"]
-
-
-@nodes_or_number(0)
-def empty_graph(n=0, create_using=None, default=Graph):
-    if create_using is None:
-        G = default()
-    elif hasattr(create_using, "_adj"):
-        # create_using is a EasyGraph style Graph
-        G = create_using
-    else:
-        # try create_using as constructor
-        G = create_using()
-
-    n_name, nodes = n
-    G.add_nodes_from(nodes)
-    return G
-
-
-@nodes_or_number(0)
-def path_graph(n, create_using=None):
-    n_name, nodes = n
-    G = empty_graph(nodes, create_using)
-    G.add_edges_from(pairwise(nodes))
-    return G
-
-
-@nodes_or_number(0)
-def complete_graph(n, create_using=None):
-    """Return the complete graph `K_n` with n nodes.
-
-    A complete graph on `n` nodes means that all pairs
-    of distinct nodes have an edge connecting them.
-
-    Parameters
-    ----------
-    n : int or iterable container of nodes
-        If n is an integer, nodes are from range(n).
-        If n is a container of nodes, those nodes appear in the graph.
-    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
-       Graph type to create. If graph instance, then cleared before populated.
-
-    Examples
-    --------
-    >>> G = eg.complete_graph(9)
-    >>> len(G)
-    9
-    >>> G.size()
-    36
-    >>> G = eg.complete_graph(range(11, 14))
-    >>> list(G.nodes())
-    [11, 12, 13]
-    >>> G = eg.complete_graph(4, eg.DiGraph())
-    >>> G.is_directed()
-    True
-
-    """
-    n_name, nodes = n
-    G = empty_graph(n_name, create_using)
-    if len(nodes) > 1:
-        if G.is_directed():
-            edges = itertools.permutations(nodes, 2)
-        else:
-            edges = itertools.combinations(nodes, 2)
-        G.add_edges_from(edges)
-    return G
+import itertools
+
+from easygraph.classes import Graph
+from easygraph.utils import nodes_or_number
+from easygraph.utils import pairwise
+
+
+__all__ = ["empty_graph", "path_graph", "complete_graph"]
+
+
+@nodes_or_number(0)
+def empty_graph(n=0, create_using=None, default=Graph):
+    if create_using is None:
+        G = default()
+    elif hasattr(create_using, "_adj"):
+        # create_using is a EasyGraph style Graph
+        G = create_using
+    else:
+        # try create_using as constructor
+        G = create_using()
+
+    n_name, nodes = n
+    G.add_nodes_from(nodes)
+    return G
+
+
+@nodes_or_number(0)
+def path_graph(n, create_using=None):
+    n_name, nodes = n
+    G = empty_graph(nodes, create_using)
+    G.add_edges_from(pairwise(nodes))
+    return G
+
+
+@nodes_or_number(0)
+def complete_graph(n, create_using=None):
+    """Return the complete graph `K_n` with n nodes.
+
+    A complete graph on `n` nodes means that all pairs
+    of distinct nodes have an edge connecting them.
+
+    Parameters
+    ----------
+    n : int or iterable container of nodes
+        If n is an integer, nodes are from range(n).
+        If n is a container of nodes, those nodes appear in the graph.
+    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
+       Graph type to create. If graph instance, then cleared before populated.
+
+    Examples
+    --------
+    >>> G = eg.complete_graph(9)
+    >>> len(G)
+    9
+    >>> G.size()
+    36
+    >>> G = eg.complete_graph(range(11, 14))
+    >>> list(G.nodes())
+    [11, 12, 13]
+    >>> G = eg.complete_graph(4, eg.DiGraph())
+    >>> G.is_directed()
+    True
+
+    """
+    n_name, nodes = n
+    G = empty_graph(n_name, create_using)
+    if len(nodes) > 1:
+        if G.is_directed():
+            edges = itertools.permutations(nodes, 2)
+        else:
+            edges = itertools.combinations(nodes, 2)
+        G.add_edges_from(edges)
+    return G
```

## easygraph/functions/graph_generator/RandomNetwork.py

 * *Ordering differences only*

```diff
@@ -1,411 +1,411 @@
-import math
-import random
-
-import easygraph as eg
-
-from easygraph.classes.graph import Graph
-
-
-__all__ = [
-    "erdos_renyi_M",
-    "erdos_renyi_P",
-    "fast_erdos_renyi_P",
-    "WS_Random",
-    "graph_Gnm",
-]
-
-
-def erdos_renyi_M(n, edge, directed=False, FilePath=None):
-    """Given the number of nodes and the number of edges, return an Erdős-Rényi random graph, and store the graph in a document.
-
-    Parameters
-    ----------
-    n : int
-        The number of nodes.
-    edge : int
-        The number of edges.
-    directed : bool, optional (default=False)
-        If True, this function returns a directed graph.
-    FilePath : string
-        The file for storing the output graph G.
-
-    Returns
-    -------
-    G : graph
-        an Erdős-Rényi random graph.
-
-    Examples
-    --------
-    Returns an Erdős-Rényi random graph G.
-
-    >>> erdos_renyi_M(100,180,directed=False,FilePath="/users/fudanmsn/downloads/RandomNetwork.txt")
-
-    References
-    ----------
-    .. [1] P. Erdős and A. Rényi, On Random Graphs, Publ. Math. 6, 290 (1959).
-    .. [2] E. N. Gilbert, Random Graphs, Ann. Math. Stat., 30, 1141 (1959).
-    """
-    if directed:
-        G = eg.DiGraph()
-        adjacent = {}
-        mmax = n * (n - 1)
-        if edge >= mmax:
-            for i in range(n):
-                for j in range(n):
-                    if i != j:
-                        G.add_edge(i, j)
-                        if i not in adjacent:
-                            adjacent[i] = []
-                            adjacent[i].append(j)
-                        else:
-                            adjacent[i].append(j)
-            return G
-        count = 0
-        while count < edge:
-            i = random.randint(0, n - 1)
-            j = random.randint(0, n - 1)
-            if i == j or G.has_edge(i, j):
-                continue
-            else:
-                count = count + 1
-                if i not in adjacent:
-                    adjacent[i] = []
-                    adjacent[i].append(j)
-                else:
-                    adjacent[i].append(j)
-                G.add_edge(i, j)
-    else:
-        G = eg.Graph()
-        adjacent = {}
-        mmax = n * (n - 1) / 2
-        if edge >= mmax:
-            for i in range(n):
-                for j in range(n):
-                    if i != j:
-                        G.add_edge(i, j)
-                        if i not in adjacent:
-                            adjacent[i] = []
-                            adjacent[i].append(j)
-                        else:
-                            adjacent[i].append(j)
-                        if j not in adjacent:
-                            adjacent[j] = []
-                            adjacent[j].append(i)
-                        else:
-                            adjacent[j].append(i)
-            return G
-        count = 0
-        while count < edge:
-            i = random.randint(0, n - 1)
-            j = random.randint(0, n - 1)
-            if i == j or G.has_edge(i, j):
-                continue
-            else:
-                count = count + 1
-                if i not in adjacent:
-                    adjacent[i] = []
-                    adjacent[i].append(j)
-                else:
-                    adjacent[i].append(j)
-                if j not in adjacent:
-                    adjacent[j] = []
-                    adjacent[j].append(i)
-                else:
-                    adjacent[j].append(i)
-                G.add_edge(i, j)
-
-    writeRandomNetworkToFile(n, adjacent, FilePath)
-    return G
-
-
-def erdos_renyi_P(n, p, directed=False, FilePath=None):
-    """Given the number of nodes and the probability of edge creation, return an Erdős-Rényi random graph, and store the graph in a document.
-
-    Parameters
-    ----------
-    n : int
-        The number of nodes.
-    p : float
-        Probability for edge creation.
-    directed : bool, optional (default=False)
-        If True, this function returns a directed graph.
-    FilePath : string
-        The file for storing the output graph G.
-
-    Returns
-    -------
-    G : graph
-        an Erdős-Rényi random graph.
-
-    Examples
-    --------
-    Returns an Erdős-Rényi random graph G
-
-    >>> erdos_renyi_P(100,0.5,directed=False,FilePath="/users/fudanmsn/downloads/RandomNetwork.txt")
-
-    References
-    ----------
-    .. [1] P. Erdős and A. Rényi, On Random Graphs, Publ. Math. 6, 290 (1959).
-    .. [2] E. N. Gilbert, Random Graphs, Ann. Math. Stat., 30, 1141 (1959).
-    """
-    if directed:
-        G = eg.DiGraph()
-        adjacent = {}
-        probability = 0.0
-        for i in range(n):
-            for j in range(i + 1, n):
-                probability = random.random()
-                if probability < p:
-                    if i not in adjacent:
-                        adjacent[i] = []
-                        adjacent[i].append(j)
-                    else:
-                        adjacent[i].append(j)
-                    G.add_edge(i, j)
-    else:
-        G = eg.Graph()
-        adjacent = {}
-        probability = 0.0
-        for i in range(n):
-            for j in range(i + 1, n):
-                probability = random.random()
-                if probability < p:
-                    if i not in adjacent:
-                        adjacent[i] = []
-                        adjacent[i].append(j)
-                    else:
-                        adjacent[i].append(j)
-                    if j not in adjacent:
-                        adjacent[j] = []
-                        adjacent[j].append(i)
-                    else:
-                        adjacent[j].append(i)
-                    G.add_edge(i, j)
-
-    writeRandomNetworkToFile(n, adjacent, FilePath)
-    return G
-
-
-def fast_erdos_renyi_P(n, p, directed=False, FilePath=None):
-    """Given the number of nodes and the probability of edge creation, return an Erdős-Rényi random graph, and store the graph in a document. Use this function for generating a huge scale graph.
-
-    Parameters
-    ----------
-    n : int
-        The number of nodes.
-    p : float
-        Probability for edge creation.
-    directed : bool, optional (default=False)
-        If True, this function returns a directed graph.
-    FilePath : string
-        The file for storing the output graph G.
-
-    Returns
-    -------
-    G : graph
-        an Erdős-Rényi random graph.
-
-    Examples
-    --------
-    Returns an Erdős-Rényi random graph G
-
-    >>> erdos_renyi_P(100,0.5,directed=False,FilePath="/users/fudanmsn/downloads/RandomNetwork.txt")
-
-    References
-    ----------
-    .. [1] P. Erdős and A. Rényi, On Random Graphs, Publ. Math. 6, 290 (1959).
-    .. [2] E. N. Gilbert, Random Graphs, Ann. Math. Stat., 30, 1141 (1959).
-    """
-    if directed:
-        G = eg.DiGraph()
-        w = -1
-        lp = math.log(1.0 - p)
-        v = 0
-        adjacent = {}
-        while v < n:
-            lr = math.log(1.0 - random.random())
-            w = w + 1 + int(lr / lp)
-            if v == w:  # avoid self loops
-                w = w + 1
-            while v < n <= w:
-                w = w - n
-                v = v + 1
-                if v == w:  # avoid self loops
-                    w = w + 1
-            if v < n:
-                G.add_edge(v, w)
-                if v not in adjacent:
-                    adjacent[v] = []
-                    adjacent[v].append(w)
-                else:
-                    adjacent[v].append(w)
-    else:
-        G = eg.Graph()
-        w = -1
-        lp = math.log(1.0 - p)
-        v = 1
-        adjacent = {}
-        while v < n:
-            lr = math.log(1.0 - random.random())
-            w = w + 1 + int(lr / lp)
-            while w >= v and v < n:
-                w = w - v
-                v = v + 1
-            if v < n:
-                G.add_edge(v, w)
-                if v not in adjacent:
-                    adjacent[v] = []
-                    adjacent[v].append(w)
-                else:
-                    adjacent[v].append(w)
-                if w not in adjacent:
-                    adjacent[w] = []
-                    adjacent[w].append(v)
-                else:
-                    adjacent[w].append(v)
-
-    writeRandomNetworkToFile(n, adjacent, FilePath)
-    return G
-
-
-def WS_Random(n, k, p, FilePath=None):
-    """Returns a small-world graph.
-
-    Parameters
-    ----------
-    n : int
-        The number of nodes
-    k : int
-        Each node is joined with its `k` nearest neighbors in a ring
-        topology.
-    p : float
-        The probability of rewiring each edge
-    FilePath : string
-        The file for storing the output graph G
-
-    Returns
-    -------
-    G : graph
-        a small-world graph
-
-    Examples
-    --------
-    Returns a small-world graph G
-
-    >>> WS_Random(100,10,0.3,"/users/fudanmsn/downloads/RandomNetwork.txt")
-
-    """
-    if k >= n:
-        print("k>=n, choose smaller k or larger n")
-        return
-    adjacent = {}
-    G = eg.Graph()
-    NUM1 = n
-    NUM2 = NUM1 - 1
-    K = k
-    K1 = K + 1
-    N = list(range(NUM1))
-    G.add_nodes(N)
-
-    for i in range(NUM1):
-        for j in range(1, K1):
-            K_add = NUM1 - K
-            i_add_j = i + j + 1
-            if i >= K_add and i_add_j > NUM1:
-                i_add = i + j - NUM1
-                G.add_edge(i, i_add)
-            else:
-                i_add = i + j
-                G.add_edge(i, i_add)
-            if i not in adjacent:
-                adjacent[i] = []
-                adjacent[i].append(i_add)
-            else:
-                adjacent[i].append(i_add)
-            if i_add not in adjacent:
-                adjacent[i_add] = []
-                adjacent[i_add].append(i)
-            else:
-                adjacent[i_add].append(i)
-    for i in range(NUM1):
-        for e_del in range(i + 1, i + K1):
-            if e_del >= NUM1:
-                e_del = e_del - NUM1
-            P_random = random.random()
-            if P_random < p:
-                G.remove_edge(i, e_del)
-                adjacent[i].remove(e_del)
-                if adjacent[i] == []:
-                    adjacent.pop(i)
-                adjacent[e_del].remove(i)
-                if adjacent[e_del] == []:
-                    adjacent.pop(e_del)
-                e_add = random.randint(0, NUM2)
-                while e_add == i or G.has_edge(i, e_add) == True:
-                    e_add = random.randint(0, NUM2)
-                G.add_edge(i, e_add)
-                if i not in adjacent:
-                    adjacent[i] = []
-                    adjacent[i].append(e_add)
-                else:
-                    adjacent[i].append(e_add)
-                if e_add not in adjacent:
-                    adjacent[e_add] = []
-                    adjacent[e_add].append(i)
-                else:
-                    adjacent[e_add].append(i)
-    writeRandomNetworkToFile(n, adjacent, FilePath)
-    return G
-
-
-def writeRandomNetworkToFile(n, adjacent, FilePath):
-    if FilePath != None:
-        f = open(FilePath, "w+")
-    else:
-        f = open("RandomNetwork.txt", "w+")
-    adjacent = sorted(adjacent.items(), key=lambda d: d[0])
-    for i in adjacent:
-        i[1].sort()
-        for j in i[1]:
-            f.write(str(i[0]))
-            f.write(" ")
-            f.write(str(j))
-            f.write("\n")
-    f.close()
-
-
-def graph_Gnm(num_v: int, num_e: int):
-    r"""Return a random graph with ``num_v`` vertices and ``num_e`` edges. Edges are drawn uniformly from the set of possible edges.
-
-    Args:
-        ``num_v`` (``int``): The Number of vertices.
-        ``num_e`` (``int``): The Number of edges.
-
-    Examples:
-        >>> import easygraph.randomhypergraph as rh
-        >>> g = rh.graph_Gnm(4, 5)
-        >>> g.e
-        ([(1, 2), (0, 3), (2, 3), (0, 2), (1, 3)], [1.0, 1.0, 1.0, 1.0, 1.0])
-    """
-    assert num_v > 1, "num_v must be greater than 1"
-    assert (
-        num_e < num_v * (num_v - 1) // 2
-    ), "the specified num_e is larger than the possible number of edges"
-
-    v_list = list(range(num_v))
-    cur_num_e, e_set = 0, set()
-    while cur_num_e < num_e:
-        v = random.choice(v_list)
-        w = random.choice(v_list)
-        if v > w:
-            v, w = w, v
-        if v == w or (v, w) in e_set:
-            continue
-        e_set.add((v, w))
-        cur_num_e += 1
-    g = Graph()
-    g.add_nodes(list(range(0, num_v)))
-    for ee in list(e_set):
-        g.add_edge(ee[0], ee[1], weight=1.0)
-
-    return g
+import math
+import random
+
+import easygraph as eg
+
+from easygraph.classes.graph import Graph
+
+
+__all__ = [
+    "erdos_renyi_M",
+    "erdos_renyi_P",
+    "fast_erdos_renyi_P",
+    "WS_Random",
+    "graph_Gnm",
+]
+
+
+def erdos_renyi_M(n, edge, directed=False, FilePath=None):
+    """Given the number of nodes and the number of edges, return an Erdős-Rényi random graph, and store the graph in a document.
+
+    Parameters
+    ----------
+    n : int
+        The number of nodes.
+    edge : int
+        The number of edges.
+    directed : bool, optional (default=False)
+        If True, this function returns a directed graph.
+    FilePath : string
+        The file for storing the output graph G.
+
+    Returns
+    -------
+    G : graph
+        an Erdős-Rényi random graph.
+
+    Examples
+    --------
+    Returns an Erdős-Rényi random graph G.
+
+    >>> erdos_renyi_M(100,180,directed=False,FilePath="/users/fudanmsn/downloads/RandomNetwork.txt")
+
+    References
+    ----------
+    .. [1] P. Erdős and A. Rényi, On Random Graphs, Publ. Math. 6, 290 (1959).
+    .. [2] E. N. Gilbert, Random Graphs, Ann. Math. Stat., 30, 1141 (1959).
+    """
+    if directed:
+        G = eg.DiGraph()
+        adjacent = {}
+        mmax = n * (n - 1)
+        if edge >= mmax:
+            for i in range(n):
+                for j in range(n):
+                    if i != j:
+                        G.add_edge(i, j)
+                        if i not in adjacent:
+                            adjacent[i] = []
+                            adjacent[i].append(j)
+                        else:
+                            adjacent[i].append(j)
+            return G
+        count = 0
+        while count < edge:
+            i = random.randint(0, n - 1)
+            j = random.randint(0, n - 1)
+            if i == j or G.has_edge(i, j):
+                continue
+            else:
+                count = count + 1
+                if i not in adjacent:
+                    adjacent[i] = []
+                    adjacent[i].append(j)
+                else:
+                    adjacent[i].append(j)
+                G.add_edge(i, j)
+    else:
+        G = eg.Graph()
+        adjacent = {}
+        mmax = n * (n - 1) / 2
+        if edge >= mmax:
+            for i in range(n):
+                for j in range(n):
+                    if i != j:
+                        G.add_edge(i, j)
+                        if i not in adjacent:
+                            adjacent[i] = []
+                            adjacent[i].append(j)
+                        else:
+                            adjacent[i].append(j)
+                        if j not in adjacent:
+                            adjacent[j] = []
+                            adjacent[j].append(i)
+                        else:
+                            adjacent[j].append(i)
+            return G
+        count = 0
+        while count < edge:
+            i = random.randint(0, n - 1)
+            j = random.randint(0, n - 1)
+            if i == j or G.has_edge(i, j):
+                continue
+            else:
+                count = count + 1
+                if i not in adjacent:
+                    adjacent[i] = []
+                    adjacent[i].append(j)
+                else:
+                    adjacent[i].append(j)
+                if j not in adjacent:
+                    adjacent[j] = []
+                    adjacent[j].append(i)
+                else:
+                    adjacent[j].append(i)
+                G.add_edge(i, j)
+
+    writeRandomNetworkToFile(n, adjacent, FilePath)
+    return G
+
+
+def erdos_renyi_P(n, p, directed=False, FilePath=None):
+    """Given the number of nodes and the probability of edge creation, return an Erdős-Rényi random graph, and store the graph in a document.
+
+    Parameters
+    ----------
+    n : int
+        The number of nodes.
+    p : float
+        Probability for edge creation.
+    directed : bool, optional (default=False)
+        If True, this function returns a directed graph.
+    FilePath : string
+        The file for storing the output graph G.
+
+    Returns
+    -------
+    G : graph
+        an Erdős-Rényi random graph.
+
+    Examples
+    --------
+    Returns an Erdős-Rényi random graph G
+
+    >>> erdos_renyi_P(100,0.5,directed=False,FilePath="/users/fudanmsn/downloads/RandomNetwork.txt")
+
+    References
+    ----------
+    .. [1] P. Erdős and A. Rényi, On Random Graphs, Publ. Math. 6, 290 (1959).
+    .. [2] E. N. Gilbert, Random Graphs, Ann. Math. Stat., 30, 1141 (1959).
+    """
+    if directed:
+        G = eg.DiGraph()
+        adjacent = {}
+        probability = 0.0
+        for i in range(n):
+            for j in range(i + 1, n):
+                probability = random.random()
+                if probability < p:
+                    if i not in adjacent:
+                        adjacent[i] = []
+                        adjacent[i].append(j)
+                    else:
+                        adjacent[i].append(j)
+                    G.add_edge(i, j)
+    else:
+        G = eg.Graph()
+        adjacent = {}
+        probability = 0.0
+        for i in range(n):
+            for j in range(i + 1, n):
+                probability = random.random()
+                if probability < p:
+                    if i not in adjacent:
+                        adjacent[i] = []
+                        adjacent[i].append(j)
+                    else:
+                        adjacent[i].append(j)
+                    if j not in adjacent:
+                        adjacent[j] = []
+                        adjacent[j].append(i)
+                    else:
+                        adjacent[j].append(i)
+                    G.add_edge(i, j)
+
+    writeRandomNetworkToFile(n, adjacent, FilePath)
+    return G
+
+
+def fast_erdos_renyi_P(n, p, directed=False, FilePath=None):
+    """Given the number of nodes and the probability of edge creation, return an Erdős-Rényi random graph, and store the graph in a document. Use this function for generating a huge scale graph.
+
+    Parameters
+    ----------
+    n : int
+        The number of nodes.
+    p : float
+        Probability for edge creation.
+    directed : bool, optional (default=False)
+        If True, this function returns a directed graph.
+    FilePath : string
+        The file for storing the output graph G.
+
+    Returns
+    -------
+    G : graph
+        an Erdős-Rényi random graph.
+
+    Examples
+    --------
+    Returns an Erdős-Rényi random graph G
+
+    >>> erdos_renyi_P(100,0.5,directed=False,FilePath="/users/fudanmsn/downloads/RandomNetwork.txt")
+
+    References
+    ----------
+    .. [1] P. Erdős and A. Rényi, On Random Graphs, Publ. Math. 6, 290 (1959).
+    .. [2] E. N. Gilbert, Random Graphs, Ann. Math. Stat., 30, 1141 (1959).
+    """
+    if directed:
+        G = eg.DiGraph()
+        w = -1
+        lp = math.log(1.0 - p)
+        v = 0
+        adjacent = {}
+        while v < n:
+            lr = math.log(1.0 - random.random())
+            w = w + 1 + int(lr / lp)
+            if v == w:  # avoid self loops
+                w = w + 1
+            while v < n <= w:
+                w = w - n
+                v = v + 1
+                if v == w:  # avoid self loops
+                    w = w + 1
+            if v < n:
+                G.add_edge(v, w)
+                if v not in adjacent:
+                    adjacent[v] = []
+                    adjacent[v].append(w)
+                else:
+                    adjacent[v].append(w)
+    else:
+        G = eg.Graph()
+        w = -1
+        lp = math.log(1.0 - p)
+        v = 1
+        adjacent = {}
+        while v < n:
+            lr = math.log(1.0 - random.random())
+            w = w + 1 + int(lr / lp)
+            while w >= v and v < n:
+                w = w - v
+                v = v + 1
+            if v < n:
+                G.add_edge(v, w)
+                if v not in adjacent:
+                    adjacent[v] = []
+                    adjacent[v].append(w)
+                else:
+                    adjacent[v].append(w)
+                if w not in adjacent:
+                    adjacent[w] = []
+                    adjacent[w].append(v)
+                else:
+                    adjacent[w].append(v)
+
+    writeRandomNetworkToFile(n, adjacent, FilePath)
+    return G
+
+
+def WS_Random(n, k, p, FilePath=None):
+    """Returns a small-world graph.
+
+    Parameters
+    ----------
+    n : int
+        The number of nodes
+    k : int
+        Each node is joined with its `k` nearest neighbors in a ring
+        topology.
+    p : float
+        The probability of rewiring each edge
+    FilePath : string
+        The file for storing the output graph G
+
+    Returns
+    -------
+    G : graph
+        a small-world graph
+
+    Examples
+    --------
+    Returns a small-world graph G
+
+    >>> WS_Random(100,10,0.3,"/users/fudanmsn/downloads/RandomNetwork.txt")
+
+    """
+    if k >= n:
+        print("k>=n, choose smaller k or larger n")
+        return
+    adjacent = {}
+    G = eg.Graph()
+    NUM1 = n
+    NUM2 = NUM1 - 1
+    K = k
+    K1 = K + 1
+    N = list(range(NUM1))
+    G.add_nodes(N)
+
+    for i in range(NUM1):
+        for j in range(1, K1):
+            K_add = NUM1 - K
+            i_add_j = i + j + 1
+            if i >= K_add and i_add_j > NUM1:
+                i_add = i + j - NUM1
+                G.add_edge(i, i_add)
+            else:
+                i_add = i + j
+                G.add_edge(i, i_add)
+            if i not in adjacent:
+                adjacent[i] = []
+                adjacent[i].append(i_add)
+            else:
+                adjacent[i].append(i_add)
+            if i_add not in adjacent:
+                adjacent[i_add] = []
+                adjacent[i_add].append(i)
+            else:
+                adjacent[i_add].append(i)
+    for i in range(NUM1):
+        for e_del in range(i + 1, i + K1):
+            if e_del >= NUM1:
+                e_del = e_del - NUM1
+            P_random = random.random()
+            if P_random < p:
+                G.remove_edge(i, e_del)
+                adjacent[i].remove(e_del)
+                if adjacent[i] == []:
+                    adjacent.pop(i)
+                adjacent[e_del].remove(i)
+                if adjacent[e_del] == []:
+                    adjacent.pop(e_del)
+                e_add = random.randint(0, NUM2)
+                while e_add == i or G.has_edge(i, e_add) == True:
+                    e_add = random.randint(0, NUM2)
+                G.add_edge(i, e_add)
+                if i not in adjacent:
+                    adjacent[i] = []
+                    adjacent[i].append(e_add)
+                else:
+                    adjacent[i].append(e_add)
+                if e_add not in adjacent:
+                    adjacent[e_add] = []
+                    adjacent[e_add].append(i)
+                else:
+                    adjacent[e_add].append(i)
+    writeRandomNetworkToFile(n, adjacent, FilePath)
+    return G
+
+
+def writeRandomNetworkToFile(n, adjacent, FilePath):
+    if FilePath != None:
+        f = open(FilePath, "w+")
+    else:
+        f = open("RandomNetwork.txt", "w+")
+    adjacent = sorted(adjacent.items(), key=lambda d: d[0])
+    for i in adjacent:
+        i[1].sort()
+        for j in i[1]:
+            f.write(str(i[0]))
+            f.write(" ")
+            f.write(str(j))
+            f.write("\n")
+    f.close()
+
+
+def graph_Gnm(num_v: int, num_e: int):
+    r"""Return a random graph with ``num_v`` vertices and ``num_e`` edges. Edges are drawn uniformly from the set of possible edges.
+
+    Args:
+        ``num_v`` (``int``): The Number of vertices.
+        ``num_e`` (``int``): The Number of edges.
+
+    Examples:
+        >>> import easygraph.randomhypergraph as rh
+        >>> g = rh.graph_Gnm(4, 5)
+        >>> g.e
+        ([(1, 2), (0, 3), (2, 3), (0, 2), (1, 3)], [1.0, 1.0, 1.0, 1.0, 1.0])
+    """
+    assert num_v > 1, "num_v must be greater than 1"
+    assert (
+        num_e < num_v * (num_v - 1) // 2
+    ), "the specified num_e is larger than the possible number of edges"
+
+    v_list = list(range(num_v))
+    cur_num_e, e_set = 0, set()
+    while cur_num_e < num_e:
+        v = random.choice(v_list)
+        w = random.choice(v_list)
+        if v > w:
+            v, w = w, v
+        if v == w or (v, w) in e_set:
+            continue
+        e_set.add((v, w))
+        cur_num_e += 1
+    g = Graph()
+    g.add_nodes(list(range(0, num_v)))
+    for ee in list(e_set):
+        g.add_edge(ee[0], ee[1], weight=1.0)
+
+    return g
```

## easygraph/functions/graph_generator/__init__.py

 * *Ordering differences only*

```diff
@@ -1,2 +1,2 @@
-from .classic import *
-from .RandomNetwork import *
+from .classic import *
+from .RandomNetwork import *
```

## easygraph/functions/core/k_core.py

 * *Ordering differences only*

```diff
@@ -1,69 +1,69 @@
-import easygraph as eg
-
-from easygraph.utils import *
-
-
-__all__ = [
-    "k_core",
-]
-
-
-from typing import TYPE_CHECKING
-from typing import List
-from typing import Union
-
-
-if TYPE_CHECKING:
-    from easygraph import Graph
-
-
-@hybrid("cpp_k_core")
-def k_core(G: "Graph") -> Union["Graph", List]:
-    """
-    Returns the k-core of G.
-
-    A k-core is a maximal subgraph that contains nodes of degree k or more.
-
-    Parameters
-    ----------
-    G : EasyGraph graph
-      A graph or directed graph
-    k : int, optional
-      The order of the core.  If not specified return the main core.
-    return_graph : bool, optional
-        If True, return the k-core as a graph.  If False, return a list of nodes.
-
-    Returns
-    -------
-    G : EasyGraph graph, if return_graph is True, else a list of nodes
-      The k-core subgraph
-    """
-    # Create a shallow copy of the input graph
-    H = G.copy()
-
-    # Initialize a dictionary to store the degrees of the nodes
-    degrees = dict(G.degree())
-    # Sort nodes by degree.
-    nodes = sorted(degrees, key=degrees.get)
-    bin_boundaries = [0]
-    curr_degree = 0
-    for i, v in enumerate(nodes):
-        if degrees[v] > curr_degree:
-            bin_boundaries.extend([i] * (degrees[v] - curr_degree))
-            curr_degree = degrees[v]
-    node_pos = {v: pos for pos, v in enumerate(nodes)}
-    # The initial guess for the core number of a node is its degree.
-    core = degrees
-    nbrs = {v: list(G.neighbors(v)) for v in G}
-    for v in nodes:
-        for u in nbrs[v]:
-            if core[u] > core[v]:
-                nbrs[u].remove(v)
-                pos = node_pos[u]
-                bin_start = bin_boundaries[core[u]]
-                node_pos[u] = bin_start
-                node_pos[nodes[bin_start]] = pos
-                nodes[bin_start], nodes[pos] = nodes[pos], nodes[bin_start]
-                bin_boundaries[core[u]] += 1
-                core[u] -= 1
-    return list(core.values())
+import easygraph as eg
+
+from easygraph.utils import *
+
+
+__all__ = [
+    "k_core",
+]
+
+
+from typing import TYPE_CHECKING
+from typing import List
+from typing import Union
+
+
+if TYPE_CHECKING:
+    from easygraph import Graph
+
+
+@hybrid("cpp_k_core")
+def k_core(G: "Graph") -> Union["Graph", List]:
+    """
+    Returns the k-core of G.
+
+    A k-core is a maximal subgraph that contains nodes of degree k or more.
+
+    Parameters
+    ----------
+    G : EasyGraph graph
+      A graph or directed graph
+    k : int, optional
+      The order of the core.  If not specified return the main core.
+    return_graph : bool, optional
+        If True, return the k-core as a graph.  If False, return a list of nodes.
+
+    Returns
+    -------
+    G : EasyGraph graph, if return_graph is True, else a list of nodes
+      The k-core subgraph
+    """
+    # Create a shallow copy of the input graph
+    H = G.copy()
+
+    # Initialize a dictionary to store the degrees of the nodes
+    degrees = dict(G.degree())
+    # Sort nodes by degree.
+    nodes = sorted(degrees, key=degrees.get)
+    bin_boundaries = [0]
+    curr_degree = 0
+    for i, v in enumerate(nodes):
+        if degrees[v] > curr_degree:
+            bin_boundaries.extend([i] * (degrees[v] - curr_degree))
+            curr_degree = degrees[v]
+    node_pos = {v: pos for pos, v in enumerate(nodes)}
+    # The initial guess for the core number of a node is its degree.
+    core = degrees
+    nbrs = {v: list(G.neighbors(v)) for v in G}
+    for v in nodes:
+        for u in nbrs[v]:
+            if core[u] > core[v]:
+                nbrs[u].remove(v)
+                pos = node_pos[u]
+                bin_start = bin_boundaries[core[u]]
+                node_pos[u] = bin_start
+                node_pos[nodes[bin_start]] = pos
+                nodes[bin_start], nodes[pos] = nodes[pos], nodes[bin_start]
+                bin_boundaries[core[u]] += 1
+                core[u] -= 1
+    return list(core.values())
```

## easygraph/functions/core/__init__.py

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-from .k_core import *
+from .k_core import *
```

## easygraph/utils/convert_to_matrix.py

 * *Ordering differences only*

```diff
@@ -1,1002 +1,1002 @@
-import itertools
-
-import easygraph as eg
-
-
-__all__ = [
-    "to_numpy_matrix",
-    "from_numpy_array",
-    "to_numpy_array",
-    "from_pandas_adjacency",
-    "from_pandas_edgelist",
-    "from_scipy_sparse_matrix",
-    "to_scipy_sparse_matrix",
-    "to_scipy_sparse_array",
-]
-
-
-def to_scipy_sparse_array(G, nodelist=None, dtype=None, weight="weight", format="csr"):
-    """Returns the graph adjacency matrix as a SciPy sparse array.
-
-    Parameters
-    ----------
-    G : graph
-        The EasyGraph graph used to construct the sparse matrix.
-
-    nodelist : list, optional
-       The rows and columns are ordered according to the nodes in `nodelist`.
-       If `nodelist` is None, then the ordering is produced by G.nodes().
-
-    dtype : NumPy data-type, optional
-        A valid NumPy dtype used to initialize the array. If None, then the
-        NumPy default is used.
-
-    weight : string or None   optional (default='weight')
-        The edge attribute that holds the numerical value used for
-        the edge weight.  If None then all edge weights are 1.
-
-    format : str in {'bsr', 'csr', 'csc', 'coo', 'lil', 'dia', 'dok'}
-        The type of the matrix to be returned (default 'csr').  For
-        some algorithms different implementations of sparse matrices
-        can perform better.  See [1]_ for details.
-
-    Returns
-    -------
-    A : SciPy sparse array
-       Graph adjacency matrix.
-
-    Notes
-    -----
-    For directed graphs, matrix entry i,j corresponds to an edge from i to j.
-
-    The matrix entries are populated using the edge attribute held in
-    parameter weight. When an edge does not have that attribute, the
-    value of the entry is 1.
-
-    For multiple edges the matrix values are the sums of the edge weights.
-
-    When `nodelist` does not contain every node in `G`, the adjacency matrix
-    is built from the subgraph of `G` that is induced by the nodes in
-    `nodelist`.
-
-    The convention used for self-loop edges in graphs is to assign the
-    diagonal matrix entry value to the weight attribute of the edge
-    (or the number 1 if the edge has no weight attribute).  If the
-    alternate convention of doubling the edge weight is desired the
-    resulting Scipy sparse matrix can be modified as follows:
-
-    >>> G = eg.Graph([(1, 1)])
-    >>> A = eg.to_scipy_sparse_array(G)
-    >>> print(A.todense())
-    [[1]]
-    >>> A.setdiag(A.diagonal() * 2)
-    >>> print(A.toarray())
-    [[2]]
-
-    Examples
-    --------
-    >>> S = eg.to_scipy_sparse_array(G, nodelist=[0, 1, 2])
-    >>> print(S.toarray())
-    [[0 2 0]
-     [1 0 0]
-     [0 0 4]]
-
-    References
-    ----------
-    .. [1] Scipy Dev. References, "Sparse Matrices",
-       https://docs.scipy.org/doc/scipy/reference/sparse.html
-    """
-    import scipy as sp
-    import scipy.sparse  # call as sp.sparse
-
-    if len(G) == 0:
-        raise eg.EasyGraphError("Graph has no nodes or edges")
-
-    if nodelist is None:
-        nodelist = list(G)
-        nlen = len(G)
-    else:
-        nlen = len(nodelist)
-        if nlen == 0:
-            raise eg.EasyGraphError("nodelist has no nodes")
-        nodeset = set(G.nbunch_iter(nodelist))
-        if nlen != len(nodeset):
-            for n in nodelist:
-                if n not in G:
-                    raise eg.EasyGraphError(f"Node {n} in nodelist is not in G")
-            raise eg.EasyGraphError("nodelist contains duplicates.")
-        if nlen < len(G):
-            G = G.subgraph(nodelist)
-
-    index = dict(zip(nodelist, range(nlen)))
-
-    # G.edges(data=weight, default=1)
-
-    coefficients = zip(
-        *((index[u], index[v], wt.get("weight", 1)) for u, v, wt in G.edges)
-    )
-    try:
-        row, col, data = coefficients
-    except ValueError:
-        # there is no edge in the subgraph
-        row, col, data = [], [], []
-
-    if G.is_directed():
-        A = sp.sparse.coo_array((data, (row, col)), shape=(nlen, nlen), dtype=dtype)
-    else:
-        # symmetrize matrix
-        d = data + data
-        r = row + col
-        c = col + row
-        # selfloop entries get double counted when symmetrizing
-        # so we subtract the data on the diagonal
-        selfloops = list(eg.selfloop_edges(G, data=weight, default=1))
-        if selfloops:
-            diag_index, diag_data = zip(*((index[u], -wt) for u, v, wt in selfloops))
-            d += diag_data
-            r += diag_index
-            c += diag_index
-        A = sp.sparse.coo_array((d, (r, c)), shape=(nlen, nlen), dtype=dtype)
-    try:
-        return A.asformat(format)
-    except ValueError as err:
-        raise eg.EasyGraphError(f"Unknown sparse matrix format: {format}") from err
-
-
-def to_scipy_sparse_matrix(G, nodelist=None, dtype=None, weight="weight", format="csr"):
-    """Returns the graph adjacency matrix as a SciPy sparse matrix.
-
-    Parameters
-    ----------
-    G : graph
-        The EasyGraph graph used to construct the sparse matrix.
-
-    nodelist : list, optional
-       The rows and columns are ordered according to the nodes in `nodelist`.
-       If `nodelist` is None, then the ordering is produced by G.nodes().
-
-    dtype : NumPy data-type, optional
-        A valid NumPy dtype used to initialize the array. If None, then the
-        NumPy default is used.
-
-    weight : string or None   optional (default='weight')
-        The edge attribute that holds the numerical value used for
-        the edge weight.  If None then all edge weights are 1.
-
-    format : str in {'bsr', 'csr', 'csc', 'coo', 'lil', 'dia', 'dok'}
-        The type of the matrix to be returned (default 'csr').  For
-        some algorithms different implementations of sparse matrices
-        can perform better.  See [1]_ for details.
-
-    Returns
-    -------
-    A : SciPy sparse matrix
-       Graph adjacency matrix.
-
-    Notes
-    -----
-    For directed graphs, matrix entry i,j corresponds to an edge from i to j.
-
-    The matrix entries are populated using the edge attribute held in
-    parameter weight. When an edge does not have that attribute, the
-    value of the entry is 1.
-
-    For multiple edges the matrix values are the sums of the edge weights.
-
-    When `nodelist` does not contain every node in `G`, the adjacency matrix
-    is built from the subgraph of `G` that is induced by the nodes in
-    `nodelist`.
-
-    The convention used for self-loop edges in graphs is to assign the
-    diagonal matrix entry value to the weight attribute of the edge
-    (or the number 1 if the edge has no weight attribute).  If the
-    alternate convention of doubling the edge weight is desired the
-    resulting Scipy sparse matrix can be modified as follows:
-
-    >>> G = eg.Graph([(1, 1)])
-    >>> A = eg.to_scipy_sparse_matrix(G)
-    >>> print(A.todense())
-    [[1]]
-    >>> A.setdiag(A.diagonal() * 2)
-    >>> print(A.todense())
-    [[2]]
-
-    Examples
-    --------
-
-    >>> G.add_edge(1, 0)
-    0
-    >>> G.add_edge(2, 2, weight=3)
-    0
-    >>> G.add_edge(2, 2)
-    1
-    >>> S = eg.to_scipy_sparse_matrix(G, nodelist=[0, 1, 2])
-    >>> print(S.todense())
-    [[0 2 0]
-     [1 0 0]
-     [0 0 4]]
-
-    References
-    ----------
-    .. [1] Scipy Dev. References, "Sparse Matrices",
-       https://docs.scipy.org/doc/scipy/reference/sparse.html
-    """
-    import scipy as sp
-    import scipy.sparse
-
-    A = to_scipy_sparse_array(
-        G, nodelist=nodelist, dtype=dtype, weight=weight, format=format
-    )
-    return sp.sparse.csr_matrix(A).asformat(format)
-
-
-def to_numpy_matrix(G, edge_sign=1.0, not_edge_sign=0.0):
-    """
-    Returns the graph adjacency matrix as a NumPy matrix.
-
-    Parameters
-    ----------
-    edge_sign : float
-        Sign for the position of matrix where there is an edge
-
-    not_edge_sign : float
-        Sign for the position of matrix where there is no edge
-
-    """
-    import numpy as np
-
-    index_of_node = dict(zip(G.nodes, range(len(G))))
-    N = len(G)
-    M = np.full((N, N), not_edge_sign)
-
-    for u, udict in G.adj.items():
-        for v, data in udict.items():
-            M[index_of_node[u], index_of_node[v]] = edge_sign
-
-    M = np.asmatrix(M)
-    return M
-
-
-def from_numpy_array(A, parallel_edges=False, create_using=None):
-    """Returns a graph from a 2D NumPy array.
-
-    The 2D NumPy array is interpreted as an adjacency matrix for the graph.
-
-    Parameters
-    ----------
-    A : a 2D numpy.ndarray
-        An adjacency matrix representation of a graph
-
-    parallel_edges : Boolean
-        If this is True, `create_using` is a multigraph, and `A` is an
-        integer array, then entry *(i, j)* in the array is interpreted as the
-        number of parallel edges joining vertices *i* and *j* in the graph.
-        If it is False, then the entries in the array are interpreted as
-        the weight of a single edge joining the vertices.
-
-    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
-       Graph type to create. If graph instance, then cleared before populated.
-
-    Notes
-    -----
-    For directed graphs, explicitly mention create_using=eg.DiGraph,
-    and entry i,j of A corresponds to an edge from i to j.
-
-    If `create_using` is :class:`easygraph.MultiGraph` or
-    :class:`easygraph.MultiDiGraph`, `parallel_edges` is True, and the
-    entries of `A` are of type :class:`int`, then this function returns a
-    multigraph (of the same type as `create_using`) with parallel edges.
-
-    If `create_using` indicates an undirected multigraph, then only the edges
-    indicated by the upper triangle of the array `A` will be added to the
-    graph.
-
-    If the NumPy array has a single data type for each array entry it
-    will be converted to an appropriate Python data type.
-
-    If the NumPy array has a user-specified compound data type the names
-    of the data fields will be used as attribute keys in the resulting
-    EasyGraph graph.
-
-    See Also
-    --------
-    to_numpy_array
-
-    Examples
-    --------
-    Simple integer weights on edges:
-
-    >>> import numpy as np
-    >>> A = np.array([[1, 1], [2, 1]])
-    >>> G = eg.from_numpy_array(A)
-    >>> G.edges(data=True)
-    EdgeDataView([(0, 0, {'weight': 1}), (0, 1, {'weight': 2}), (1, 1, {'weight': 1})])
-
-    If `create_using` indicates a multigraph and the array has only integer
-    entries and `parallel_edges` is False, then the entries will be treated
-    as weights for edges joining the nodes (without creating parallel edges):
-
-    >>> A = np.array([[1, 1], [1, 2]])
-    >>> G = eg.from_numpy_array(A, create_using=eg.MultiGraph)
-    >>> G[1][1]
-    AtlasView({0: {'weight': 2}})
-
-    If `create_using` indicates a multigraph and the array has only integer
-    entries and `parallel_edges` is True, then the entries will be treated
-    as the number of parallel edges joining those two vertices:
-
-    >>> A = np.array([[1, 1], [1, 2]])
-    >>> temp = eg.MultiGraph()
-    >>> G = eg.from_numpy_array(A, parallel_edges=True, create_using=temp)
-    >>> G[1][1]
-    AtlasView({0: {'weight': 1}, 1: {'weight': 1}})
-
-    User defined compound data type on edges:
-
-    >>> dt = [("weight", float), ("cost", int)]
-    >>> A = np.array([[(1.0, 2)]], dtype=dt)
-    >>> G = eg.from_numpy_array(A)
-    >>> G.edges()
-    EdgeView([(0, 0)])
-    >>> G[0][0]["cost"]
-    2
-    >>> G[0][0]["weight"]
-    1.0
-
-    """
-    kind_to_python_type = {
-        "f": float,
-        "i": int,
-        "u": int,
-        "b": bool,
-        "c": complex,
-        "S": str,
-        "U": str,
-        "V": "void",
-    }
-    G = eg.empty_graph(0, create_using)
-    if A.ndim != 2:
-        raise eg.EasyGraphError(f"Input array must be 2D, not {A.ndim}")
-    n, m = A.shape
-    if n != m:
-        raise eg.EasyGraphError(f"Adjacency matrix not square: eg,ny={A.shape}")
-    dt = A.dtype
-    try:
-        python_type = kind_to_python_type[dt.kind]
-    except Exception as err:
-        raise TypeError(f"Unknown numpy data type: {dt}") from err
-
-    # Make sure we get even the isolated nodes of the graph.
-    G.add_nodes_from(range(n))
-    # Get a list of all the entries in the array with nonzero entries. These
-    # coordinates become edges in the graph. (convert to int from np.int64)
-    edges = ((int(e[0]), int(e[1])) for e in zip(*A.nonzero()))
-    # handle numpy constructed data type
-    if python_type == "void":
-        # Sort the fields by their offset, then by dtype, then by name.
-        fields = sorted(
-            (offset, dtype, name) for name, (dtype, offset) in A.dtype.fields.items()
-        )
-        triples = (
-            (
-                u,
-                v,
-                {
-                    name: kind_to_python_type[dtype.kind](val)
-                    for (_, dtype, name), val in zip(fields, A[u, v])
-                },
-            )
-            for u, v in edges
-        )
-    # If the entries in the adjacency matrix are integers, the graph is a
-    # multigraph, and parallel_edges is True, then create parallel edges, each
-    # with weight 1, for each entry in the adjacency matrix. Otherwise, create
-    # one edge for each positive entry in the adjacency matrix and set the
-    # weight of that edge to be the entry in the matrix.
-    elif python_type is int and G.is_multigraph() and parallel_edges:
-        chain = itertools.chain.from_iterable
-        # The following line is equivalent to:
-        #
-        #     for (u, v) in edges:
-        #         for d in range(A[u, v]):
-        #             G.add_edge(u, v, weight=1)
-        #
-        triples = chain(
-            ((u, v, {"weight": 1}) for d in range(A[u, v])) for (u, v) in edges
-        )
-    else:  # basic data type
-        triples = ((u, v, dict(weight=python_type(A[u, v]))) for u, v in edges)
-    # If we are creating an undirected multigraph, only add the edges from the
-    # upper triangle of the matrix. Otherwise, add all the edges. This relies
-    # on the fact that the vertices created in the
-    # `_generated_weighted_edges()` function are actually the row/column
-    # indices for the matrix `A`.
-    #
-    # Without this check, we run into a problem where each edge is added twice
-    # when `G.add_edges_from()` is invoked below.
-    if G.is_multigraph() and not G.is_directed():
-        triples = ((u, v, d) for u, v, d in triples if u <= v)
-    G.add_edges_from(triples)
-    return G
-
-
-def to_numpy_array(
-    G,
-    nodelist=None,
-    dtype=None,
-    order=None,
-    multigraph_weight=sum,
-    weight="weight",
-    nonedge=0.0,
-):
-    """Returns the graph adjacency matrix as a NumPy array.
-
-    Parameters
-    ----------
-    G : graph
-        The EasyGraph graph used to construct the NumPy array.
-
-    nodelist : list, optional
-        The rows and columns are ordered according to the nodes in `nodelist`.
-        If `nodelist` is None, then the ordering is produced by G.nodes().
-
-    dtype : NumPy data type, optional
-        A valid single NumPy data type used to initialize the array.
-        This must be a simple type such as int or numpy.float64 and
-        not a compound data type (see to_numpy_recarray)
-        If None, then the NumPy default is used.
-
-    order : {'C', 'F'}, optional
-        Whether to store multidimensional data in C- or Fortran-contiguous
-        (row- or column-wise) order in memory. If None, then the NumPy default
-        is used.
-
-    multigraph_weight : {sum, min, max}, optional
-        An operator that determines how weights in multigraphs are handled.
-        The default is to sum the weights of the multiple edges.
-
-    weight : string or None optional (default = 'weight')
-        The edge attribute that holds the numerical value used for
-        the edge weight. If an edge does not have that attribute, then the
-        value 1 is used instead.
-
-    nonedge : float (default = 0.0)
-        The array values corresponding to nonedges are typically set to zero.
-        However, this could be undesirable if there are array values
-        corresponding to actual edges that also have the value zero. If so,
-        one might prefer nonedges to have some other value, such as nan.
-
-    Returns
-    -------
-    A : NumPy ndarray
-        Graph adjacency matrix
-
-    See Also
-    --------
-    from_numpy_array
-
-    Notes
-    -----
-    For directed graphs, entry i,j corresponds to an edge from i to j.
-
-    Entries in the adjacency matrix are assigned to the weight edge attribute.
-    When an edge does not have a weight attribute, the value of the entry is
-    set to the number 1.  For multiple (parallel) edges, the values of the
-    entries are determined by the `multigraph_weight` parameter. The default is
-    to sum the weight attributes for each of the parallel edges.
-
-    When `nodelist` does not contain every node in `G`, the adjacency matrix is
-    built from the subgraph of `G` that is induced by the nodes in `nodelist`.
-
-    The convention used for self-loop edges in graphs is to assign the
-    diagonal array entry value to the weight attribute of the edge
-    (or the number 1 if the edge has no weight attribute). If the
-    alternate convention of doubling the edge weight is desired the
-    resulting NumPy array can be modified as follows:
-
-    >>> import numpy as np
-    >>> G = eg.Graph([(1, 1)])
-    >>> A = eg.to_numpy_array(G)
-    >>> A
-    array([[1.]])
-    >>> A[np.diag_indices_from(A)] *= 2
-    >>> A
-    array([[2.]])
-
-    Examples
-    --------
-    >>> G = eg.MultiDiGraph()
-    >>> G.add_edge(0, 1, weight=2)
-    0
-    >>> G.add_edge(1, 0)
-    0
-    >>> G.add_edge(2, 2, weight=3)
-    0
-    >>> G.add_edge(2, 2)
-    1
-    >>> eg.to_numpy_array(G, nodelist=[0, 1, 2])
-    array([[0., 2., 0.],
-           [1., 0., 0.],
-           [0., 0., 4.]])
-
-    """
-    import numpy as np
-
-    if nodelist is None:
-        nodelist = list(G)
-        nodeset = G
-        nlen = len(G)
-    else:
-        nlen = len(nodelist)
-        nodeset = set(G.nodes)
-        if nlen != len(nodeset):
-            for n in nodelist:
-                if n not in G:
-                    raise eg.EasyGraphError(f"Node {n} in nodelist is not in G")
-            raise eg.EasyGraphError("nodelist contains duplicates.")
-
-    undirected = not G.is_directed()
-    index = dict(zip(nodelist, range(nlen)))
-
-    # Initially, we start with an array of nans.  Then we populate the array
-    # using data from the graph.  Afterwards, any leftover nans will be
-    # converted to the value of `nonedge`.  Note, we use nans initially,
-    # instead of zero, for two reasons:
-    #
-    #   1) It can be important to distinguish a real edge with the value 0
-    #      from a nonedge with the value 0.
-    #
-    #   2) When working with multi(di)graphs, we must combine the values of all
-    #      edges between any two nodes in some manner.  This often takes the
-    #      form of a sum, min, or max.  Using the value 0 for a nonedge would
-    #      have undesirable effects with min and max, but using nanmin and
-    #      nanmax with initially nan values is not problematic at all.
-    #
-    # That said, there are still some drawbacks to this approach. Namely, if
-    # a real edge is nan, then that value is a) not distinguishable from
-    # nonedges and b) is ignored by the default combinator (nansum, nanmin,
-    # nanmax) functions used for multi(di)graphs. If this becomes an issue,
-    # an alternative approach is to use masked arrays.  Initially, every
-    # element is masked and set to some `initial` value. As we populate the
-    # graph, elements are unmasked (automatically) when we combine the initial
-    # value with the values given by real edges.  At the end, we convert all
-    # masked values to `nonedge`. Using masked arrays fully addresses reason 1,
-    # but for reason 2, we would still have the issue with min and max if the
-    # initial values were 0.0.  Note: an initial value of +inf is appropriate
-    # for min, while an initial value of -inf is appropriate for max. When
-    # working with sum, an initial value of zero is appropriate. Ideally then,
-    # we'd want to allow users to specify both a value for nonedges and also
-    # an initial value.  For multi(di)graphs, the choice of the initial value
-    # will, in general, depend on the combinator function---sensible defaults
-    # can be provided.
-
-    if G.is_multigraph():
-        # Handle MultiGraphs and MultiDiGraphs
-        A = np.full((nlen, nlen), np.nan, order=order)
-        # use numpy nan-aware operations
-        operator = {sum: np.nansum, min: np.nanmin, max: np.nanmax}
-        try:
-            op = operator[multigraph_weight]
-        except Exception as err:
-            raise ValueError("multigraph_weight must be sum, min, or max") from err
-
-        for u, v, _, attrs in G.edges:
-            if (u in nodeset) and (v in nodeset):
-                i, j = index[u], index[v]
-                e_weight = attrs.get(weight, 1)
-                A[i, j] = op([e_weight, A[i, j]])
-                if undirected:
-                    A[j, i] = A[i, j]
-    else:
-        # Graph or DiGraph, this is much faster than above
-        A = np.full((nlen, nlen), np.nan, order=order)
-        for u, nbrdict in G.adj.items():
-            for v, d in nbrdict.items():
-                try:
-                    A[index[u], index[v]] = d.get(weight, 1)
-                except KeyError:
-                    # This occurs when there are fewer desired nodes than
-                    # there are nodes in the graph: len(nodelist) < len(G)
-                    pass
-
-    A[np.isnan(A)] = nonedge
-    A = np.asarray(A, dtype=dtype)
-    return A
-
-
-def from_pandas_adjacency(df, create_using=None):
-    r"""Returns a graph from Pandas DataFrame.
-
-    The Pandas DataFrame is interpreted as an adjacency matrix for the graph.
-
-    Parameters
-    ----------
-    df : Pandas DataFrame
-      An adjacency matrix representation of a graph
-
-    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
-       Graph type to create. If graph instance, then cleared before populated.
-
-    Notes
-    -----
-    For directed graphs, explicitly mention create_using=eg.DiGraph,
-    and entry i,j of df corresponds to an edge from i to j.
-
-    If `df` has a single data type for each entry it will be converted to an
-    appropriate Python data type.
-
-    If `df` has a user-specified compound data type the names
-    of the data fields will be used as attribute keys in the resulting
-    EasyGraph graph.
-
-    See Also
-    --------
-    to_pandas_adjacency
-
-    Examples
-    --------
-    Simple integer weights on edges:
-
-    >>> import pandas as pd
-    >>> pd.options.display.max_columns = 20
-    >>> df = pd.DataFrame([[1, 1], [2, 1]])
-    >>> df
-       0  1
-    0  1  1
-    1  2  1
-    >>> G = eg.from_pandas_adjacency(df)
-    >>> G.name = "Graph from pandas adjacency matrix"
-    """
-
-    try:
-        df = df[df.index]
-    except Exception as err:
-        missing = list(set(df.index).difference(set(df.columns)))
-        msg = f"{missing} not in columns"
-        raise eg.EasyGraphError("Columns must match Indices.", msg) from err
-
-    A = df.values
-    G = from_numpy_array(A, create_using=create_using)
-
-    G = eg.relabel_nodes(G, dict(enumerate(df.columns)))
-    return G
-
-
-def from_pandas_edgelist(
-    df,
-    source="source",
-    target="target",
-    edge_attr=None,
-    create_using=None,
-    edge_key=None,
-):
-    """Returns a graph from Pandas DataFrame containing an edge list.
-
-    The Pandas DataFrame should contain at least two columns of node names and
-    zero or more columns of edge attributes. Each row will be processed as one
-    edge instance.
-
-    Note: This function iterates over DataFrame.values, which is not
-    guaranteed to retain the data type across columns in the row. This is only
-    a problem if your row is entirely numeric and a mix of ints and floats. In
-    that case, all values will be returned as floats. See the
-    DataFrame.iterrows documentation for an example.
-
-    Parameters
-    ----------
-    df : Pandas DataFrame
-        An edge list representation of a graph
-
-    source : str or int
-        A valid column name (string or integer) for the source nodes (for the
-        directed case).
-
-    target : str or int
-        A valid column name (string or integer) for the target nodes (for the
-        directed case).
-
-    edge_attr : str or int, iterable, True, or None
-        A valid column name (str or int) or iterable of column names that are
-        used to retrieve items and add them to the graph as edge attributes.
-        If `True`, all of the remaining columns will be added.
-        If `None`, no edge attributes are added to the graph.
-
-    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
-        Graph type to create. If graph instance, then cleared before populated.
-
-    edge_key : str or None, optional (default=None)
-        A valid column name for the edge keys (for a MultiGraph). The values in
-        this column are used for the edge keys when adding edges if create_using
-        is a multigraph.
-
-    See Also
-    --------
-    to_pandas_edgelist
-
-    Examples
-    --------
-    Simple integer weights on edges:
-
-    >>> import pandas as pd
-    >>> pd.options.display.max_columns = 20
-    >>> import numpy as np
-    >>> rng = np.random.RandomState(seed=5)
-    >>> ints = rng.randint(1, 11, size=(3, 2))
-    >>> a = ["A", "B", "C"]
-    >>> b = ["D", "A", "E"]
-    >>> df = pd.DataFrame(ints, columns=["weight", "cost"])
-    >>> df[0] = a
-    >>> df["b"] = b
-    >>> df[["weight", "cost", 0, "b"]]
-       weight  cost  0  b
-    0       4     7  A  D
-    1       7     1  B  A
-    2      10     9  C  E
-    >>> G = eg.from_pandas_edgelist(df, 0, "b", ["weight", "cost"])
-    >>> G["E"]["C"]["weight"]
-    10
-    >>> G["E"]["C"]["cost"]
-    9
-    >>> edges = pd.DataFrame(
-    ...     {
-    ...         "source": [0, 1, 2],
-    ...         "target": [2, 2, 3],
-    ...         "weight": [3, 4, 5],
-    ...         "color": ["red", "blue", "blue"],
-    ...     }
-    ... )
-    >>> G = eg.from_pandas_edgelist(edges, edge_attr=True)
-    >>> G[0][2]["color"]
-    'red'
-
-    Build multigraph with custom keys:
-
-    >>> edges = pd.DataFrame(
-    ...     {
-    ...         "source": [0, 1, 2, 0],
-    ...         "target": [2, 2, 3, 2],
-    ...         "my_edge_key": ["A", "B", "C", "D"],
-    ...         "weight": [3, 4, 5, 6],
-    ...         "color": ["red", "blue", "blue", "blue"],
-    ...     }
-    ... )
-    >>> G = eg.from_pandas_edgelist(
-    ...     edges,
-    ...     edge_key="my_edge_key",
-    ...     edge_attr=["weight", "color"],
-    ...     create_using=eg.MultiGraph(),
-    ... )
-    >>> G[0][2]
-    AtlasView({'A': {'weight': 3, 'color': 'red'}, 'D': {'weight': 6, 'color': 'blue'}})
-
-
-    """
-    g = eg.empty_graph(0, create_using)
-
-    if edge_attr is None:
-        g.add_edges_from(zip(df[source], df[target]))
-        return g
-
-    reserved_columns = [source, target]
-
-    # Additional columns requested
-    attr_col_headings = []
-    attribute_data = []
-    if edge_attr is True:
-        attr_col_headings = [c for c in df.columns if c not in reserved_columns]
-    elif isinstance(edge_attr, (list, tuple)):
-        attr_col_headings = edge_attr
-    else:
-        attr_col_headings = [edge_attr]
-    if len(attr_col_headings) == 0:
-        raise eg.EasyGraphError(
-            "Invalid edge_attr argument: No columns found with name:"
-            f" {attr_col_headings}"
-        )
-
-    try:
-        attribute_data = zip(*[df[col] for col in attr_col_headings])
-    except (KeyError, TypeError) as err:
-        msg = f"Invalid edge_attr argument: {edge_attr}"
-        raise eg.EasyGraphError(msg) from err
-
-    if g.is_multigraph():
-        # => append the edge keys from the df to the bundled data
-        if edge_key is not None:
-            try:
-                multigraph_edge_keys = df[edge_key]
-                attribute_data = zip(attribute_data, multigraph_edge_keys)
-            except (KeyError, TypeError) as err:
-                msg = f"Invalid edge_key argument: {edge_key}"
-                raise eg.EasyGraphError(msg) from err
-
-        for s, t, attrs in zip(df[source], df[target], attribute_data):
-            if edge_key is not None:
-                attrs, multigraph_edge_key = attrs
-                key = g.add_edge(s, t, key=multigraph_edge_key)
-            else:
-                key = g.add_edge(s, t)
-
-            g[s][t][key].update(zip(attr_col_headings, attrs))
-    else:
-        for s, t, attrs in zip(df[source], df[target], attribute_data):
-            g.add_edge(s, t)
-            g[s][t].update(zip(attr_col_headings, attrs))
-
-    return g
-
-
-def from_scipy_sparse_matrix(
-    A, parallel_edges=False, create_using=None, edge_attribute="weight"
-):
-    """Creates a new graph from an adjacency matrix given as a SciPy sparse
-    matrix.
-
-    Parameters
-    ----------
-    A: scipy sparse matrix
-      An adjacency matrix representation of a graph
-
-    parallel_edges : Boolean
-      If this is True, `create_using` is a multigraph, and `A` is an
-      integer matrix, then entry *(i, j)* in the matrix is interpreted as the
-      number of parallel edges joining vertices *i* and *j* in the graph.
-      If it is False, then the entries in the matrix are interpreted as
-      the weight of a single edge joining the vertices.
-
-    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
-       Graph type to create. If graph instance, then cleared before populated.
-
-    edge_attribute: string
-       Name of edge attribute to store matrix numeric value. The data will
-       have the same type as the matrix entry (int, float, (real,imag)).
-
-    Notes
-    -----
-    For directed graphs, explicitly mention create_using=eg.DiGraph,
-    and entry i,j of A corresponds to an edge from i to j.
-
-    If `create_using` is :class:`easygraph.MultiGraph` or
-    :class:`easygraph.MultiDiGraph`, `parallel_edges` is True, and the
-    entries of `A` are of type :class:`int`, then this function returns a
-    multigraph (constructed from `create_using`) with parallel edges.
-    In this case, `edge_attribute` will be ignored.
-
-    If `create_using` indicates an undirected multigraph, then only the edges
-    indicated by the upper triangle of the matrix `A` will be added to the
-    graph.
-
-    Examples
-    --------
-    >>> import scipy as sp
-    >>> import scipy.sparse  # call as sp.sparse
-    >>> A = sp.sparse.eye(2, 2, 1)
-    >>> G = eg.from_scipy_sparse_matrix(A)
-
-    If `create_using` indicates a multigraph and the matrix has only integer
-    entries and `parallel_edges` is Falnxse, then the entries will be treated
-    as weights for edges joining the nodes (without creating parallel edges):
-
-    >>> A = sp.sparse.csr_matrix([[1, 1], [1, 2]])
-    >>> G = eg.from_scipy_sparse_matrix(A, create_using=eg.MultiGraph)
-    >>> G[1][1]
-    AtlasView({0: {'weight': 2}})
-
-    If `create_using` indicates a multigraph and the matrix has only integer
-    entries and `parallel_edges` is True, then the entries will be treated
-    as the number of parallel edges joining those two vertices:
-
-    >>> A = sp.sparse.csr_matrix([[1, 1], [1, 2]])
-    >>> G = eg.from_scipy_sparse_matrix(
-    ...     A, parallel_edges=True, create_using=eg.MultiGraph
-    ... )
-    >>> G[1][1]
-    AtlasView({0: {'weight': 1}, 1: {'weight': 1}})
-
-    """
-
-    return from_scipy_sparse_array(
-        A,
-        parallel_edges=parallel_edges,
-        create_using=create_using,
-        edge_attribute=edge_attribute,
-    )
-
-
-def from_scipy_sparse_array(
-    A, parallel_edges=False, create_using=None, edge_attribute="weight"
-):
-    G = eg.empty_graph(0, create_using)
-    n, m = A.shape
-    if n != m:
-        raise eg.EasyGraphError(f"Adjacency matrix not square: nx,ny={A.shape}")
-    # Make sure we get even the isolated nodes of the graph.
-    G.add_nodes_from(range(n))
-    # Create an iterable over (u, v, w) triples and for each triple, add an
-    # edge from u to v with weight w.
-    triples = _generate_weighted_edges(A)
-    # If the entries in the adjacency matrix are integers, the graph is a
-    # multigraph, and parallel_edges is True, then create parallel edges, each
-    # with weight 1, for each entry in the adjacency matrix. Otherwise, create
-    # one edge for each positive entry in the adjacency matrix and set the
-    # weight of that edge to be the entry in the matrix.
-    if A.dtype.kind in ("i", "u") and G.is_multigraph() and parallel_edges:
-        chain = itertools.chain.from_iterable
-        # The following line is equivalent to:
-        #
-        #     for (u, v) in edges:
-        #         for d in range(A[u, v]):
-        #             G.add_edge(u, v, weight=1)
-        #
-        triples = chain(((u, v, 1) for d in range(w)) for (u, v, w) in triples)
-    # If we are creating an undirected multigraph, only add the edges from the
-    # upper triangle of the matrix. Otherwise, add all the edges. This relies
-    # on the fact that the vertices created in the
-    # `_generated_weighted_edges()` function are actually the row/column
-    # indices for the matrix `A`.
-    #
-    # Without this check, we run into a problem where each edge is added twice
-    # when `G.add_weighted_edges_from()` is invoked below.
-    if G.is_multigraph() and not G.is_directed():
-        triples = ((u, v, d) for u, v, d in triples if u <= v)
-    G.add_edges_from(((u, v, {"weight": d}) for u, v, d in triples))
-    return G
-
-
-def _generate_weighted_edges(A):
-    """Returns an iterable over (u, v, w) triples, where u and v are adjacent
-    vertices and w is the weight of the edge joining u and v.
-
-    `A` is a SciPy sparse matrix (in any format).
-
-    """
-    if A.format == "csr":
-        return _csr_gen_triples(A)
-    if A.format == "csc":
-        return _csc_gen_triples(A)
-    if A.format == "dok":
-        return _dok_gen_triples(A)
-    # If A is in any other format (including COO), convert it to COO format.
-    return _coo_gen_triples(A.tocoo())
-
-
-def _csr_gen_triples(A):
-    """Converts a SciPy sparse matrix in **Compressed Sparse Row** format to
-    an iterable of weighted edge triples.
-
-    """
-    nrows = A.shape[0]
-    data, indices, indptr = A.data, A.indices, A.indptr
-    for i in range(nrows):
-        for j in range(indptr[i], indptr[i + 1]):
-            yield i, indices[j], data[j]
-
-
-def _csc_gen_triples(A):
-    """Converts a SciPy sparse matrix in **Compressed Sparse Column** format to
-    an iterable of weighted edge triples.
-
-    """
-    ncols = A.shape[1]
-    data, indices, indptr = A.data, A.indices, A.indptr
-    for i in range(ncols):
-        for j in range(indptr[i], indptr[i + 1]):
-            yield indices[j], i, data[j]
-
-
-def _coo_gen_triples(A):
-    """Converts a SciPy sparse matrix in **Coordinate** format to an iterable
-    of weighted edge triples.
-
-    """
-    row, col, data = A.row, A.col, A.data
-    return zip(row, col, data)
-
-
-def _dok_gen_triples(A):
-    """Converts a SciPy sparse matrix in **Dictionary of Keys** format to an
-    iterable of weighted edge triples.
-
-    """
-    for (r, c), v in A.items():
-        yield r, c, v
+import itertools
+
+import easygraph as eg
+
+
+__all__ = [
+    "to_numpy_matrix",
+    "from_numpy_array",
+    "to_numpy_array",
+    "from_pandas_adjacency",
+    "from_pandas_edgelist",
+    "from_scipy_sparse_matrix",
+    "to_scipy_sparse_matrix",
+    "to_scipy_sparse_array",
+]
+
+
+def to_scipy_sparse_array(G, nodelist=None, dtype=None, weight="weight", format="csr"):
+    """Returns the graph adjacency matrix as a SciPy sparse array.
+
+    Parameters
+    ----------
+    G : graph
+        The EasyGraph graph used to construct the sparse matrix.
+
+    nodelist : list, optional
+       The rows and columns are ordered according to the nodes in `nodelist`.
+       If `nodelist` is None, then the ordering is produced by G.nodes().
+
+    dtype : NumPy data-type, optional
+        A valid NumPy dtype used to initialize the array. If None, then the
+        NumPy default is used.
+
+    weight : string or None   optional (default='weight')
+        The edge attribute that holds the numerical value used for
+        the edge weight.  If None then all edge weights are 1.
+
+    format : str in {'bsr', 'csr', 'csc', 'coo', 'lil', 'dia', 'dok'}
+        The type of the matrix to be returned (default 'csr').  For
+        some algorithms different implementations of sparse matrices
+        can perform better.  See [1]_ for details.
+
+    Returns
+    -------
+    A : SciPy sparse array
+       Graph adjacency matrix.
+
+    Notes
+    -----
+    For directed graphs, matrix entry i,j corresponds to an edge from i to j.
+
+    The matrix entries are populated using the edge attribute held in
+    parameter weight. When an edge does not have that attribute, the
+    value of the entry is 1.
+
+    For multiple edges the matrix values are the sums of the edge weights.
+
+    When `nodelist` does not contain every node in `G`, the adjacency matrix
+    is built from the subgraph of `G` that is induced by the nodes in
+    `nodelist`.
+
+    The convention used for self-loop edges in graphs is to assign the
+    diagonal matrix entry value to the weight attribute of the edge
+    (or the number 1 if the edge has no weight attribute).  If the
+    alternate convention of doubling the edge weight is desired the
+    resulting Scipy sparse matrix can be modified as follows:
+
+    >>> G = eg.Graph([(1, 1)])
+    >>> A = eg.to_scipy_sparse_array(G)
+    >>> print(A.todense())
+    [[1]]
+    >>> A.setdiag(A.diagonal() * 2)
+    >>> print(A.toarray())
+    [[2]]
+
+    Examples
+    --------
+    >>> S = eg.to_scipy_sparse_array(G, nodelist=[0, 1, 2])
+    >>> print(S.toarray())
+    [[0 2 0]
+     [1 0 0]
+     [0 0 4]]
+
+    References
+    ----------
+    .. [1] Scipy Dev. References, "Sparse Matrices",
+       https://docs.scipy.org/doc/scipy/reference/sparse.html
+    """
+    import scipy as sp
+    import scipy.sparse  # call as sp.sparse
+
+    if len(G) == 0:
+        raise eg.EasyGraphError("Graph has no nodes or edges")
+
+    if nodelist is None:
+        nodelist = list(G)
+        nlen = len(G)
+    else:
+        nlen = len(nodelist)
+        if nlen == 0:
+            raise eg.EasyGraphError("nodelist has no nodes")
+        nodeset = set(G.nbunch_iter(nodelist))
+        if nlen != len(nodeset):
+            for n in nodelist:
+                if n not in G:
+                    raise eg.EasyGraphError(f"Node {n} in nodelist is not in G")
+            raise eg.EasyGraphError("nodelist contains duplicates.")
+        if nlen < len(G):
+            G = G.subgraph(nodelist)
+
+    index = dict(zip(nodelist, range(nlen)))
+
+    # G.edges(data=weight, default=1)
+
+    coefficients = zip(
+        *((index[u], index[v], wt.get("weight", 1)) for u, v, wt in G.edges)
+    )
+    try:
+        row, col, data = coefficients
+    except ValueError:
+        # there is no edge in the subgraph
+        row, col, data = [], [], []
+
+    if G.is_directed():
+        A = sp.sparse.coo_array((data, (row, col)), shape=(nlen, nlen), dtype=dtype)
+    else:
+        # symmetrize matrix
+        d = data + data
+        r = row + col
+        c = col + row
+        # selfloop entries get double counted when symmetrizing
+        # so we subtract the data on the diagonal
+        selfloops = list(eg.selfloop_edges(G, data=weight, default=1))
+        if selfloops:
+            diag_index, diag_data = zip(*((index[u], -wt) for u, v, wt in selfloops))
+            d += diag_data
+            r += diag_index
+            c += diag_index
+        A = sp.sparse.coo_array((d, (r, c)), shape=(nlen, nlen), dtype=dtype)
+    try:
+        return A.asformat(format)
+    except ValueError as err:
+        raise eg.EasyGraphError(f"Unknown sparse matrix format: {format}") from err
+
+
+def to_scipy_sparse_matrix(G, nodelist=None, dtype=None, weight="weight", format="csr"):
+    """Returns the graph adjacency matrix as a SciPy sparse matrix.
+
+    Parameters
+    ----------
+    G : graph
+        The EasyGraph graph used to construct the sparse matrix.
+
+    nodelist : list, optional
+       The rows and columns are ordered according to the nodes in `nodelist`.
+       If `nodelist` is None, then the ordering is produced by G.nodes().
+
+    dtype : NumPy data-type, optional
+        A valid NumPy dtype used to initialize the array. If None, then the
+        NumPy default is used.
+
+    weight : string or None   optional (default='weight')
+        The edge attribute that holds the numerical value used for
+        the edge weight.  If None then all edge weights are 1.
+
+    format : str in {'bsr', 'csr', 'csc', 'coo', 'lil', 'dia', 'dok'}
+        The type of the matrix to be returned (default 'csr').  For
+        some algorithms different implementations of sparse matrices
+        can perform better.  See [1]_ for details.
+
+    Returns
+    -------
+    A : SciPy sparse matrix
+       Graph adjacency matrix.
+
+    Notes
+    -----
+    For directed graphs, matrix entry i,j corresponds to an edge from i to j.
+
+    The matrix entries are populated using the edge attribute held in
+    parameter weight. When an edge does not have that attribute, the
+    value of the entry is 1.
+
+    For multiple edges the matrix values are the sums of the edge weights.
+
+    When `nodelist` does not contain every node in `G`, the adjacency matrix
+    is built from the subgraph of `G` that is induced by the nodes in
+    `nodelist`.
+
+    The convention used for self-loop edges in graphs is to assign the
+    diagonal matrix entry value to the weight attribute of the edge
+    (or the number 1 if the edge has no weight attribute).  If the
+    alternate convention of doubling the edge weight is desired the
+    resulting Scipy sparse matrix can be modified as follows:
+
+    >>> G = eg.Graph([(1, 1)])
+    >>> A = eg.to_scipy_sparse_matrix(G)
+    >>> print(A.todense())
+    [[1]]
+    >>> A.setdiag(A.diagonal() * 2)
+    >>> print(A.todense())
+    [[2]]
+
+    Examples
+    --------
+
+    >>> G.add_edge(1, 0)
+    0
+    >>> G.add_edge(2, 2, weight=3)
+    0
+    >>> G.add_edge(2, 2)
+    1
+    >>> S = eg.to_scipy_sparse_matrix(G, nodelist=[0, 1, 2])
+    >>> print(S.todense())
+    [[0 2 0]
+     [1 0 0]
+     [0 0 4]]
+
+    References
+    ----------
+    .. [1] Scipy Dev. References, "Sparse Matrices",
+       https://docs.scipy.org/doc/scipy/reference/sparse.html
+    """
+    import scipy as sp
+    import scipy.sparse
+
+    A = to_scipy_sparse_array(
+        G, nodelist=nodelist, dtype=dtype, weight=weight, format=format
+    )
+    return sp.sparse.csr_matrix(A).asformat(format)
+
+
+def to_numpy_matrix(G, edge_sign=1.0, not_edge_sign=0.0):
+    """
+    Returns the graph adjacency matrix as a NumPy matrix.
+
+    Parameters
+    ----------
+    edge_sign : float
+        Sign for the position of matrix where there is an edge
+
+    not_edge_sign : float
+        Sign for the position of matrix where there is no edge
+
+    """
+    import numpy as np
+
+    index_of_node = dict(zip(G.nodes, range(len(G))))
+    N = len(G)
+    M = np.full((N, N), not_edge_sign)
+
+    for u, udict in G.adj.items():
+        for v, data in udict.items():
+            M[index_of_node[u], index_of_node[v]] = edge_sign
+
+    M = np.asmatrix(M)
+    return M
+
+
+def from_numpy_array(A, parallel_edges=False, create_using=None):
+    """Returns a graph from a 2D NumPy array.
+
+    The 2D NumPy array is interpreted as an adjacency matrix for the graph.
+
+    Parameters
+    ----------
+    A : a 2D numpy.ndarray
+        An adjacency matrix representation of a graph
+
+    parallel_edges : Boolean
+        If this is True, `create_using` is a multigraph, and `A` is an
+        integer array, then entry *(i, j)* in the array is interpreted as the
+        number of parallel edges joining vertices *i* and *j* in the graph.
+        If it is False, then the entries in the array are interpreted as
+        the weight of a single edge joining the vertices.
+
+    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
+       Graph type to create. If graph instance, then cleared before populated.
+
+    Notes
+    -----
+    For directed graphs, explicitly mention create_using=eg.DiGraph,
+    and entry i,j of A corresponds to an edge from i to j.
+
+    If `create_using` is :class:`easygraph.MultiGraph` or
+    :class:`easygraph.MultiDiGraph`, `parallel_edges` is True, and the
+    entries of `A` are of type :class:`int`, then this function returns a
+    multigraph (of the same type as `create_using`) with parallel edges.
+
+    If `create_using` indicates an undirected multigraph, then only the edges
+    indicated by the upper triangle of the array `A` will be added to the
+    graph.
+
+    If the NumPy array has a single data type for each array entry it
+    will be converted to an appropriate Python data type.
+
+    If the NumPy array has a user-specified compound data type the names
+    of the data fields will be used as attribute keys in the resulting
+    EasyGraph graph.
+
+    See Also
+    --------
+    to_numpy_array
+
+    Examples
+    --------
+    Simple integer weights on edges:
+
+    >>> import numpy as np
+    >>> A = np.array([[1, 1], [2, 1]])
+    >>> G = eg.from_numpy_array(A)
+    >>> G.edges(data=True)
+    EdgeDataView([(0, 0, {'weight': 1}), (0, 1, {'weight': 2}), (1, 1, {'weight': 1})])
+
+    If `create_using` indicates a multigraph and the array has only integer
+    entries and `parallel_edges` is False, then the entries will be treated
+    as weights for edges joining the nodes (without creating parallel edges):
+
+    >>> A = np.array([[1, 1], [1, 2]])
+    >>> G = eg.from_numpy_array(A, create_using=eg.MultiGraph)
+    >>> G[1][1]
+    AtlasView({0: {'weight': 2}})
+
+    If `create_using` indicates a multigraph and the array has only integer
+    entries and `parallel_edges` is True, then the entries will be treated
+    as the number of parallel edges joining those two vertices:
+
+    >>> A = np.array([[1, 1], [1, 2]])
+    >>> temp = eg.MultiGraph()
+    >>> G = eg.from_numpy_array(A, parallel_edges=True, create_using=temp)
+    >>> G[1][1]
+    AtlasView({0: {'weight': 1}, 1: {'weight': 1}})
+
+    User defined compound data type on edges:
+
+    >>> dt = [("weight", float), ("cost", int)]
+    >>> A = np.array([[(1.0, 2)]], dtype=dt)
+    >>> G = eg.from_numpy_array(A)
+    >>> G.edges()
+    EdgeView([(0, 0)])
+    >>> G[0][0]["cost"]
+    2
+    >>> G[0][0]["weight"]
+    1.0
+
+    """
+    kind_to_python_type = {
+        "f": float,
+        "i": int,
+        "u": int,
+        "b": bool,
+        "c": complex,
+        "S": str,
+        "U": str,
+        "V": "void",
+    }
+    G = eg.empty_graph(0, create_using)
+    if A.ndim != 2:
+        raise eg.EasyGraphError(f"Input array must be 2D, not {A.ndim}")
+    n, m = A.shape
+    if n != m:
+        raise eg.EasyGraphError(f"Adjacency matrix not square: eg,ny={A.shape}")
+    dt = A.dtype
+    try:
+        python_type = kind_to_python_type[dt.kind]
+    except Exception as err:
+        raise TypeError(f"Unknown numpy data type: {dt}") from err
+
+    # Make sure we get even the isolated nodes of the graph.
+    G.add_nodes_from(range(n))
+    # Get a list of all the entries in the array with nonzero entries. These
+    # coordinates become edges in the graph. (convert to int from np.int64)
+    edges = ((int(e[0]), int(e[1])) for e in zip(*A.nonzero()))
+    # handle numpy constructed data type
+    if python_type == "void":
+        # Sort the fields by their offset, then by dtype, then by name.
+        fields = sorted(
+            (offset, dtype, name) for name, (dtype, offset) in A.dtype.fields.items()
+        )
+        triples = (
+            (
+                u,
+                v,
+                {
+                    name: kind_to_python_type[dtype.kind](val)
+                    for (_, dtype, name), val in zip(fields, A[u, v])
+                },
+            )
+            for u, v in edges
+        )
+    # If the entries in the adjacency matrix are integers, the graph is a
+    # multigraph, and parallel_edges is True, then create parallel edges, each
+    # with weight 1, for each entry in the adjacency matrix. Otherwise, create
+    # one edge for each positive entry in the adjacency matrix and set the
+    # weight of that edge to be the entry in the matrix.
+    elif python_type is int and G.is_multigraph() and parallel_edges:
+        chain = itertools.chain.from_iterable
+        # The following line is equivalent to:
+        #
+        #     for (u, v) in edges:
+        #         for d in range(A[u, v]):
+        #             G.add_edge(u, v, weight=1)
+        #
+        triples = chain(
+            ((u, v, {"weight": 1}) for d in range(A[u, v])) for (u, v) in edges
+        )
+    else:  # basic data type
+        triples = ((u, v, dict(weight=python_type(A[u, v]))) for u, v in edges)
+    # If we are creating an undirected multigraph, only add the edges from the
+    # upper triangle of the matrix. Otherwise, add all the edges. This relies
+    # on the fact that the vertices created in the
+    # `_generated_weighted_edges()` function are actually the row/column
+    # indices for the matrix `A`.
+    #
+    # Without this check, we run into a problem where each edge is added twice
+    # when `G.add_edges_from()` is invoked below.
+    if G.is_multigraph() and not G.is_directed():
+        triples = ((u, v, d) for u, v, d in triples if u <= v)
+    G.add_edges_from(triples)
+    return G
+
+
+def to_numpy_array(
+    G,
+    nodelist=None,
+    dtype=None,
+    order=None,
+    multigraph_weight=sum,
+    weight="weight",
+    nonedge=0.0,
+):
+    """Returns the graph adjacency matrix as a NumPy array.
+
+    Parameters
+    ----------
+    G : graph
+        The EasyGraph graph used to construct the NumPy array.
+
+    nodelist : list, optional
+        The rows and columns are ordered according to the nodes in `nodelist`.
+        If `nodelist` is None, then the ordering is produced by G.nodes().
+
+    dtype : NumPy data type, optional
+        A valid single NumPy data type used to initialize the array.
+        This must be a simple type such as int or numpy.float64 and
+        not a compound data type (see to_numpy_recarray)
+        If None, then the NumPy default is used.
+
+    order : {'C', 'F'}, optional
+        Whether to store multidimensional data in C- or Fortran-contiguous
+        (row- or column-wise) order in memory. If None, then the NumPy default
+        is used.
+
+    multigraph_weight : {sum, min, max}, optional
+        An operator that determines how weights in multigraphs are handled.
+        The default is to sum the weights of the multiple edges.
+
+    weight : string or None optional (default = 'weight')
+        The edge attribute that holds the numerical value used for
+        the edge weight. If an edge does not have that attribute, then the
+        value 1 is used instead.
+
+    nonedge : float (default = 0.0)
+        The array values corresponding to nonedges are typically set to zero.
+        However, this could be undesirable if there are array values
+        corresponding to actual edges that also have the value zero. If so,
+        one might prefer nonedges to have some other value, such as nan.
+
+    Returns
+    -------
+    A : NumPy ndarray
+        Graph adjacency matrix
+
+    See Also
+    --------
+    from_numpy_array
+
+    Notes
+    -----
+    For directed graphs, entry i,j corresponds to an edge from i to j.
+
+    Entries in the adjacency matrix are assigned to the weight edge attribute.
+    When an edge does not have a weight attribute, the value of the entry is
+    set to the number 1.  For multiple (parallel) edges, the values of the
+    entries are determined by the `multigraph_weight` parameter. The default is
+    to sum the weight attributes for each of the parallel edges.
+
+    When `nodelist` does not contain every node in `G`, the adjacency matrix is
+    built from the subgraph of `G` that is induced by the nodes in `nodelist`.
+
+    The convention used for self-loop edges in graphs is to assign the
+    diagonal array entry value to the weight attribute of the edge
+    (or the number 1 if the edge has no weight attribute). If the
+    alternate convention of doubling the edge weight is desired the
+    resulting NumPy array can be modified as follows:
+
+    >>> import numpy as np
+    >>> G = eg.Graph([(1, 1)])
+    >>> A = eg.to_numpy_array(G)
+    >>> A
+    array([[1.]])
+    >>> A[np.diag_indices_from(A)] *= 2
+    >>> A
+    array([[2.]])
+
+    Examples
+    --------
+    >>> G = eg.MultiDiGraph()
+    >>> G.add_edge(0, 1, weight=2)
+    0
+    >>> G.add_edge(1, 0)
+    0
+    >>> G.add_edge(2, 2, weight=3)
+    0
+    >>> G.add_edge(2, 2)
+    1
+    >>> eg.to_numpy_array(G, nodelist=[0, 1, 2])
+    array([[0., 2., 0.],
+           [1., 0., 0.],
+           [0., 0., 4.]])
+
+    """
+    import numpy as np
+
+    if nodelist is None:
+        nodelist = list(G)
+        nodeset = G
+        nlen = len(G)
+    else:
+        nlen = len(nodelist)
+        nodeset = set(G.nodes)
+        if nlen != len(nodeset):
+            for n in nodelist:
+                if n not in G:
+                    raise eg.EasyGraphError(f"Node {n} in nodelist is not in G")
+            raise eg.EasyGraphError("nodelist contains duplicates.")
+
+    undirected = not G.is_directed()
+    index = dict(zip(nodelist, range(nlen)))
+
+    # Initially, we start with an array of nans.  Then we populate the array
+    # using data from the graph.  Afterwards, any leftover nans will be
+    # converted to the value of `nonedge`.  Note, we use nans initially,
+    # instead of zero, for two reasons:
+    #
+    #   1) It can be important to distinguish a real edge with the value 0
+    #      from a nonedge with the value 0.
+    #
+    #   2) When working with multi(di)graphs, we must combine the values of all
+    #      edges between any two nodes in some manner.  This often takes the
+    #      form of a sum, min, or max.  Using the value 0 for a nonedge would
+    #      have undesirable effects with min and max, but using nanmin and
+    #      nanmax with initially nan values is not problematic at all.
+    #
+    # That said, there are still some drawbacks to this approach. Namely, if
+    # a real edge is nan, then that value is a) not distinguishable from
+    # nonedges and b) is ignored by the default combinator (nansum, nanmin,
+    # nanmax) functions used for multi(di)graphs. If this becomes an issue,
+    # an alternative approach is to use masked arrays.  Initially, every
+    # element is masked and set to some `initial` value. As we populate the
+    # graph, elements are unmasked (automatically) when we combine the initial
+    # value with the values given by real edges.  At the end, we convert all
+    # masked values to `nonedge`. Using masked arrays fully addresses reason 1,
+    # but for reason 2, we would still have the issue with min and max if the
+    # initial values were 0.0.  Note: an initial value of +inf is appropriate
+    # for min, while an initial value of -inf is appropriate for max. When
+    # working with sum, an initial value of zero is appropriate. Ideally then,
+    # we'd want to allow users to specify both a value for nonedges and also
+    # an initial value.  For multi(di)graphs, the choice of the initial value
+    # will, in general, depend on the combinator function---sensible defaults
+    # can be provided.
+
+    if G.is_multigraph():
+        # Handle MultiGraphs and MultiDiGraphs
+        A = np.full((nlen, nlen), np.nan, order=order)
+        # use numpy nan-aware operations
+        operator = {sum: np.nansum, min: np.nanmin, max: np.nanmax}
+        try:
+            op = operator[multigraph_weight]
+        except Exception as err:
+            raise ValueError("multigraph_weight must be sum, min, or max") from err
+
+        for u, v, _, attrs in G.edges:
+            if (u in nodeset) and (v in nodeset):
+                i, j = index[u], index[v]
+                e_weight = attrs.get(weight, 1)
+                A[i, j] = op([e_weight, A[i, j]])
+                if undirected:
+                    A[j, i] = A[i, j]
+    else:
+        # Graph or DiGraph, this is much faster than above
+        A = np.full((nlen, nlen), np.nan, order=order)
+        for u, nbrdict in G.adj.items():
+            for v, d in nbrdict.items():
+                try:
+                    A[index[u], index[v]] = d.get(weight, 1)
+                except KeyError:
+                    # This occurs when there are fewer desired nodes than
+                    # there are nodes in the graph: len(nodelist) < len(G)
+                    pass
+
+    A[np.isnan(A)] = nonedge
+    A = np.asarray(A, dtype=dtype)
+    return A
+
+
+def from_pandas_adjacency(df, create_using=None):
+    r"""Returns a graph from Pandas DataFrame.
+
+    The Pandas DataFrame is interpreted as an adjacency matrix for the graph.
+
+    Parameters
+    ----------
+    df : Pandas DataFrame
+      An adjacency matrix representation of a graph
+
+    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
+       Graph type to create. If graph instance, then cleared before populated.
+
+    Notes
+    -----
+    For directed graphs, explicitly mention create_using=eg.DiGraph,
+    and entry i,j of df corresponds to an edge from i to j.
+
+    If `df` has a single data type for each entry it will be converted to an
+    appropriate Python data type.
+
+    If `df` has a user-specified compound data type the names
+    of the data fields will be used as attribute keys in the resulting
+    EasyGraph graph.
+
+    See Also
+    --------
+    to_pandas_adjacency
+
+    Examples
+    --------
+    Simple integer weights on edges:
+
+    >>> import pandas as pd
+    >>> pd.options.display.max_columns = 20
+    >>> df = pd.DataFrame([[1, 1], [2, 1]])
+    >>> df
+       0  1
+    0  1  1
+    1  2  1
+    >>> G = eg.from_pandas_adjacency(df)
+    >>> G.name = "Graph from pandas adjacency matrix"
+    """
+
+    try:
+        df = df[df.index]
+    except Exception as err:
+        missing = list(set(df.index).difference(set(df.columns)))
+        msg = f"{missing} not in columns"
+        raise eg.EasyGraphError("Columns must match Indices.", msg) from err
+
+    A = df.values
+    G = from_numpy_array(A, create_using=create_using)
+
+    G = eg.relabel_nodes(G, dict(enumerate(df.columns)))
+    return G
+
+
+def from_pandas_edgelist(
+    df,
+    source="source",
+    target="target",
+    edge_attr=None,
+    create_using=None,
+    edge_key=None,
+):
+    """Returns a graph from Pandas DataFrame containing an edge list.
+
+    The Pandas DataFrame should contain at least two columns of node names and
+    zero or more columns of edge attributes. Each row will be processed as one
+    edge instance.
+
+    Note: This function iterates over DataFrame.values, which is not
+    guaranteed to retain the data type across columns in the row. This is only
+    a problem if your row is entirely numeric and a mix of ints and floats. In
+    that case, all values will be returned as floats. See the
+    DataFrame.iterrows documentation for an example.
+
+    Parameters
+    ----------
+    df : Pandas DataFrame
+        An edge list representation of a graph
+
+    source : str or int
+        A valid column name (string or integer) for the source nodes (for the
+        directed case).
+
+    target : str or int
+        A valid column name (string or integer) for the target nodes (for the
+        directed case).
+
+    edge_attr : str or int, iterable, True, or None
+        A valid column name (str or int) or iterable of column names that are
+        used to retrieve items and add them to the graph as edge attributes.
+        If `True`, all of the remaining columns will be added.
+        If `None`, no edge attributes are added to the graph.
+
+    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
+        Graph type to create. If graph instance, then cleared before populated.
+
+    edge_key : str or None, optional (default=None)
+        A valid column name for the edge keys (for a MultiGraph). The values in
+        this column are used for the edge keys when adding edges if create_using
+        is a multigraph.
+
+    See Also
+    --------
+    to_pandas_edgelist
+
+    Examples
+    --------
+    Simple integer weights on edges:
+
+    >>> import pandas as pd
+    >>> pd.options.display.max_columns = 20
+    >>> import numpy as np
+    >>> rng = np.random.RandomState(seed=5)
+    >>> ints = rng.randint(1, 11, size=(3, 2))
+    >>> a = ["A", "B", "C"]
+    >>> b = ["D", "A", "E"]
+    >>> df = pd.DataFrame(ints, columns=["weight", "cost"])
+    >>> df[0] = a
+    >>> df["b"] = b
+    >>> df[["weight", "cost", 0, "b"]]
+       weight  cost  0  b
+    0       4     7  A  D
+    1       7     1  B  A
+    2      10     9  C  E
+    >>> G = eg.from_pandas_edgelist(df, 0, "b", ["weight", "cost"])
+    >>> G["E"]["C"]["weight"]
+    10
+    >>> G["E"]["C"]["cost"]
+    9
+    >>> edges = pd.DataFrame(
+    ...     {
+    ...         "source": [0, 1, 2],
+    ...         "target": [2, 2, 3],
+    ...         "weight": [3, 4, 5],
+    ...         "color": ["red", "blue", "blue"],
+    ...     }
+    ... )
+    >>> G = eg.from_pandas_edgelist(edges, edge_attr=True)
+    >>> G[0][2]["color"]
+    'red'
+
+    Build multigraph with custom keys:
+
+    >>> edges = pd.DataFrame(
+    ...     {
+    ...         "source": [0, 1, 2, 0],
+    ...         "target": [2, 2, 3, 2],
+    ...         "my_edge_key": ["A", "B", "C", "D"],
+    ...         "weight": [3, 4, 5, 6],
+    ...         "color": ["red", "blue", "blue", "blue"],
+    ...     }
+    ... )
+    >>> G = eg.from_pandas_edgelist(
+    ...     edges,
+    ...     edge_key="my_edge_key",
+    ...     edge_attr=["weight", "color"],
+    ...     create_using=eg.MultiGraph(),
+    ... )
+    >>> G[0][2]
+    AtlasView({'A': {'weight': 3, 'color': 'red'}, 'D': {'weight': 6, 'color': 'blue'}})
+
+
+    """
+    g = eg.empty_graph(0, create_using)
+
+    if edge_attr is None:
+        g.add_edges_from(zip(df[source], df[target]))
+        return g
+
+    reserved_columns = [source, target]
+
+    # Additional columns requested
+    attr_col_headings = []
+    attribute_data = []
+    if edge_attr is True:
+        attr_col_headings = [c for c in df.columns if c not in reserved_columns]
+    elif isinstance(edge_attr, (list, tuple)):
+        attr_col_headings = edge_attr
+    else:
+        attr_col_headings = [edge_attr]
+    if len(attr_col_headings) == 0:
+        raise eg.EasyGraphError(
+            "Invalid edge_attr argument: No columns found with name:"
+            f" {attr_col_headings}"
+        )
+
+    try:
+        attribute_data = zip(*[df[col] for col in attr_col_headings])
+    except (KeyError, TypeError) as err:
+        msg = f"Invalid edge_attr argument: {edge_attr}"
+        raise eg.EasyGraphError(msg) from err
+
+    if g.is_multigraph():
+        # => append the edge keys from the df to the bundled data
+        if edge_key is not None:
+            try:
+                multigraph_edge_keys = df[edge_key]
+                attribute_data = zip(attribute_data, multigraph_edge_keys)
+            except (KeyError, TypeError) as err:
+                msg = f"Invalid edge_key argument: {edge_key}"
+                raise eg.EasyGraphError(msg) from err
+
+        for s, t, attrs in zip(df[source], df[target], attribute_data):
+            if edge_key is not None:
+                attrs, multigraph_edge_key = attrs
+                key = g.add_edge(s, t, key=multigraph_edge_key)
+            else:
+                key = g.add_edge(s, t)
+
+            g[s][t][key].update(zip(attr_col_headings, attrs))
+    else:
+        for s, t, attrs in zip(df[source], df[target], attribute_data):
+            g.add_edge(s, t)
+            g[s][t].update(zip(attr_col_headings, attrs))
+
+    return g
+
+
+def from_scipy_sparse_matrix(
+    A, parallel_edges=False, create_using=None, edge_attribute="weight"
+):
+    """Creates a new graph from an adjacency matrix given as a SciPy sparse
+    matrix.
+
+    Parameters
+    ----------
+    A: scipy sparse matrix
+      An adjacency matrix representation of a graph
+
+    parallel_edges : Boolean
+      If this is True, `create_using` is a multigraph, and `A` is an
+      integer matrix, then entry *(i, j)* in the matrix is interpreted as the
+      number of parallel edges joining vertices *i* and *j* in the graph.
+      If it is False, then the entries in the matrix are interpreted as
+      the weight of a single edge joining the vertices.
+
+    create_using : EasyGraph graph constructor, optional (default=eg.Graph)
+       Graph type to create. If graph instance, then cleared before populated.
+
+    edge_attribute: string
+       Name of edge attribute to store matrix numeric value. The data will
+       have the same type as the matrix entry (int, float, (real,imag)).
+
+    Notes
+    -----
+    For directed graphs, explicitly mention create_using=eg.DiGraph,
+    and entry i,j of A corresponds to an edge from i to j.
+
+    If `create_using` is :class:`easygraph.MultiGraph` or
+    :class:`easygraph.MultiDiGraph`, `parallel_edges` is True, and the
+    entries of `A` are of type :class:`int`, then this function returns a
+    multigraph (constructed from `create_using`) with parallel edges.
+    In this case, `edge_attribute` will be ignored.
+
+    If `create_using` indicates an undirected multigraph, then only the edges
+    indicated by the upper triangle of the matrix `A` will be added to the
+    graph.
+
+    Examples
+    --------
+    >>> import scipy as sp
+    >>> import scipy.sparse  # call as sp.sparse
+    >>> A = sp.sparse.eye(2, 2, 1)
+    >>> G = eg.from_scipy_sparse_matrix(A)
+
+    If `create_using` indicates a multigraph and the matrix has only integer
+    entries and `parallel_edges` is Falnxse, then the entries will be treated
+    as weights for edges joining the nodes (without creating parallel edges):
+
+    >>> A = sp.sparse.csr_matrix([[1, 1], [1, 2]])
+    >>> G = eg.from_scipy_sparse_matrix(A, create_using=eg.MultiGraph)
+    >>> G[1][1]
+    AtlasView({0: {'weight': 2}})
+
+    If `create_using` indicates a multigraph and the matrix has only integer
+    entries and `parallel_edges` is True, then the entries will be treated
+    as the number of parallel edges joining those two vertices:
+
+    >>> A = sp.sparse.csr_matrix([[1, 1], [1, 2]])
+    >>> G = eg.from_scipy_sparse_matrix(
+    ...     A, parallel_edges=True, create_using=eg.MultiGraph
+    ... )
+    >>> G[1][1]
+    AtlasView({0: {'weight': 1}, 1: {'weight': 1}})
+
+    """
+
+    return from_scipy_sparse_array(
+        A,
+        parallel_edges=parallel_edges,
+        create_using=create_using,
+        edge_attribute=edge_attribute,
+    )
+
+
+def from_scipy_sparse_array(
+    A, parallel_edges=False, create_using=None, edge_attribute="weight"
+):
+    G = eg.empty_graph(0, create_using)
+    n, m = A.shape
+    if n != m:
+        raise eg.EasyGraphError(f"Adjacency matrix not square: nx,ny={A.shape}")
+    # Make sure we get even the isolated nodes of the graph.
+    G.add_nodes_from(range(n))
+    # Create an iterable over (u, v, w) triples and for each triple, add an
+    # edge from u to v with weight w.
+    triples = _generate_weighted_edges(A)
+    # If the entries in the adjacency matrix are integers, the graph is a
+    # multigraph, and parallel_edges is True, then create parallel edges, each
+    # with weight 1, for each entry in the adjacency matrix. Otherwise, create
+    # one edge for each positive entry in the adjacency matrix and set the
+    # weight of that edge to be the entry in the matrix.
+    if A.dtype.kind in ("i", "u") and G.is_multigraph() and parallel_edges:
+        chain = itertools.chain.from_iterable
+        # The following line is equivalent to:
+        #
+        #     for (u, v) in edges:
+        #         for d in range(A[u, v]):
+        #             G.add_edge(u, v, weight=1)
+        #
+        triples = chain(((u, v, 1) for d in range(w)) for (u, v, w) in triples)
+    # If we are creating an undirected multigraph, only add the edges from the
+    # upper triangle of the matrix. Otherwise, add all the edges. This relies
+    # on the fact that the vertices created in the
+    # `_generated_weighted_edges()` function are actually the row/column
+    # indices for the matrix `A`.
+    #
+    # Without this check, we run into a problem where each edge is added twice
+    # when `G.add_weighted_edges_from()` is invoked below.
+    if G.is_multigraph() and not G.is_directed():
+        triples = ((u, v, d) for u, v, d in triples if u <= v)
+    G.add_edges_from(((u, v, {"weight": d}) for u, v, d in triples))
+    return G
+
+
+def _generate_weighted_edges(A):
+    """Returns an iterable over (u, v, w) triples, where u and v are adjacent
+    vertices and w is the weight of the edge joining u and v.
+
+    `A` is a SciPy sparse matrix (in any format).
+
+    """
+    if A.format == "csr":
+        return _csr_gen_triples(A)
+    if A.format == "csc":
+        return _csc_gen_triples(A)
+    if A.format == "dok":
+        return _dok_gen_triples(A)
+    # If A is in any other format (including COO), convert it to COO format.
+    return _coo_gen_triples(A.tocoo())
+
+
+def _csr_gen_triples(A):
+    """Converts a SciPy sparse matrix in **Compressed Sparse Row** format to
+    an iterable of weighted edge triples.
+
+    """
+    nrows = A.shape[0]
+    data, indices, indptr = A.data, A.indices, A.indptr
+    for i in range(nrows):
+        for j in range(indptr[i], indptr[i + 1]):
+            yield i, indices[j], data[j]
+
+
+def _csc_gen_triples(A):
+    """Converts a SciPy sparse matrix in **Compressed Sparse Column** format to
+    an iterable of weighted edge triples.
+
+    """
+    ncols = A.shape[1]
+    data, indices, indptr = A.data, A.indices, A.indptr
+    for i in range(ncols):
+        for j in range(indptr[i], indptr[i + 1]):
+            yield indices[j], i, data[j]
+
+
+def _coo_gen_triples(A):
+    """Converts a SciPy sparse matrix in **Coordinate** format to an iterable
+    of weighted edge triples.
+
+    """
+    row, col, data = A.row, A.col, A.data
+    return zip(row, col, data)
+
+
+def _dok_gen_triples(A):
+    """Converts a SciPy sparse matrix in **Dictionary of Keys** format to an
+    iterable of weighted edge triples.
+
+    """
+    for (r, c), v in A.items():
+        yield r, c, v
```

## easygraph/utils/exception.py

 * *Ordering differences only*

```diff
@@ -1,43 +1,43 @@
-"""
-**********
-Exceptions
-**********
-
-Base exceptions and errors for EasyGraph.
-"""
-
-
-__all__ = [
-    "EasyGraphException",
-    "EasyGraphError",
-    "EasyGraphNotImplemented",
-    "EasyGraphPointlessConcept",
-]
-
-
-class EasyGraphException(Exception):
-    """Base class for exceptions in EasyGraph."""
-
-
-class EasyGraphError(EasyGraphException):
-    """Exception for a serious error in EasyGraph"""
-
-
-class EasyGraphNotImplemented(EasyGraphException):
-    """Exception raised by algorithms not implemented for a type of graph."""
-
-
-class EasyGraphPointlessConcept(EasyGraphException):
-    """Raised when a null graph is provided as input to an algorithm
-    that cannot use it.
-
-    The null graph is sometimes considered a pointless concept [1]_,
-    thus the name of the exception.
-
-    References
-    ----------
-    .. [1] Harary, F. and Read, R. "Is the Null Graph a Pointless
-       Concept?"  In Graphs and Combinatorics Conference, George
-       Washington University.  New York: Springer-Verlag, 1973.
-
-    """
+"""
+**********
+Exceptions
+**********
+
+Base exceptions and errors for EasyGraph.
+"""
+
+
+__all__ = [
+    "EasyGraphException",
+    "EasyGraphError",
+    "EasyGraphNotImplemented",
+    "EasyGraphPointlessConcept",
+]
+
+
+class EasyGraphException(Exception):
+    """Base class for exceptions in EasyGraph."""
+
+
+class EasyGraphError(EasyGraphException):
+    """Exception for a serious error in EasyGraph"""
+
+
+class EasyGraphNotImplemented(EasyGraphException):
+    """Exception raised by algorithms not implemented for a type of graph."""
+
+
+class EasyGraphPointlessConcept(EasyGraphException):
+    """Raised when a null graph is provided as input to an algorithm
+    that cannot use it.
+
+    The null graph is sometimes considered a pointless concept [1]_,
+    thus the name of the exception.
+
+    References
+    ----------
+    .. [1] Harary, F. and Read, R. "Is the Null Graph a Pointless
+       Concept?"  In Graphs and Combinatorics Conference, George
+       Washington University.  New York: Springer-Verlag, 1973.
+
+    """
```

## easygraph/utils/index_of_node.py

 * *Ordering differences only*

```diff
@@ -1,12 +1,12 @@
-__all__ = ["get_relation_of_index_and_node"]
-
-
-def get_relation_of_index_and_node(graph):
-    node2idx = {}
-    idx2node = []
-    node_size = 0
-    for node in graph.nodes:
-        node2idx[node] = node_size
-        idx2node.append(node)
-        node_size += 1
-    return idx2node, node2idx
+__all__ = ["get_relation_of_index_and_node"]
+
+
+def get_relation_of_index_and_node(graph):
+    node2idx = {}
+    idx2node = []
+    node_size = 0
+    for node in graph.nodes:
+        node2idx[node] = node_size
+        idx2node.append(node)
+        node_size += 1
+    return idx2node, node2idx
```

## easygraph/utils/relabel.py

 * *Ordering differences only*

```diff
@@ -1,106 +1,106 @@
-import easygraph as eg
-
-
-__all__ = ["relabel_nodes", "convert_node_labels_to_integers"]
-
-
-def relabel_nodes(G, mapping):
-    if not hasattr(mapping, "__getitem__"):
-        m = {n: mapping(n) for n in G}
-    else:
-        m = mapping
-    return _relabel_copy(G, m)
-
-
-def _relabel_copy(G, mapping):
-    H = G.__class__()
-    H.add_nodes_from(mapping.get(n, n) for n in G)
-    H._node.update((mapping.get(n, n), d.copy()) for n, d in G.nodes.items())
-    if G.is_multigraph():
-        new_edges = [
-            (mapping.get(n1, n1), mapping.get(n2, n2), k, d.copy())
-            for (n1, n2, k, d) in G.edges
-        ]
-
-        # check for conflicting edge-keys
-        undirected = not G.is_directed()
-        seen_edges = set()
-        for i, (source, target, key, data) in enumerate(new_edges):
-            while (source, target, key) in seen_edges:
-                if not isinstance(key, (int, float)):
-                    key = 0
-                key += 1
-            seen_edges.add((source, target, key))
-            if undirected:
-                seen_edges.add((target, source, key))
-            new_edges[i] = (source, target, key, data)
-
-        H.add_edges_from(new_edges)
-    else:
-        H.add_edges_from(
-            (mapping.get(n1, n1), mapping.get(n2, n2), d.copy())
-            for (n1, n2, d) in G.edges
-        )
-    H.graph.update(G.graph)
-    return H
-
-
-def convert_node_labels_to_integers(
-    G, first_label=0, ordering="default", label_attribute=None
-):
-    """Returns a copy of the graph G with the nodes relabeled using
-    consecutive integers.
-
-    Parameters
-    ----------
-    G : graph
-       A easygraph graph
-
-    first_label : int, optional (default=0)
-       An integer specifying the starting offset in numbering nodes.
-       The new integer labels are numbered first_label, ..., n-1+first_label.
-
-    ordering : string
-       "default" : inherit node ordering from G.nodes
-       "sorted"  : inherit node ordering from sorted(G.nodes)
-       "increasing degree" : nodes are sorted by increasing degree
-       "decreasing degree" : nodes are sorted by decreasing degree
-
-    label_attribute : string, optional (default=None)
-       Name of node attribute to store old label.  If None no attribute
-       is created.
-
-    Notes
-    -----
-    Node and edge attribute data are copied to the new (relabeled) graph.
-
-    There is no guarantee that the relabeling of nodes to integers will
-    give the same two integers for two (even identical graphs).
-    Use the `ordering` argument to try to preserve the order.
-
-    See Also
-    --------
-    relabel_nodes
-    """
-    N = G.number_of_nodes() + first_label
-    if ordering == "default":
-        mapping = dict(zip(G.nodes, range(first_label, N)))
-    elif ordering == "sorted":
-        nlist = sorted(G.nodes)
-        mapping = dict(zip(nlist, range(first_label, N)))
-    elif ordering == "increasing degree":
-        dv_pairs = [(d, n) for (n, d) in G.degree()]
-        dv_pairs.sort()  # in-place sort from lowest to highest degree
-        mapping = dict(zip([n for d, n in dv_pairs], range(first_label, N)))
-    elif ordering == "decreasing degree":
-        dv_pairs = [(d, n) for (n, d) in G.degree()]
-        dv_pairs.sort()  # in-place sort from lowest to highest degree
-        dv_pairs.reverse()
-        mapping = dict(zip([n for d, n in dv_pairs], range(first_label, N)))
-    else:
-        raise eg.EasyGraphError(f"Unknown node ordering: {ordering}")
-    H = relabel_nodes(G, mapping)
-    # create node attribute with the old label
-    if label_attribute is not None:
-        eg.set_node_attributes(H, {v: k for k, v in mapping.items()}, label_attribute)
-    return H
+import easygraph as eg
+
+
+__all__ = ["relabel_nodes", "convert_node_labels_to_integers"]
+
+
+def relabel_nodes(G, mapping):
+    if not hasattr(mapping, "__getitem__"):
+        m = {n: mapping(n) for n in G}
+    else:
+        m = mapping
+    return _relabel_copy(G, m)
+
+
+def _relabel_copy(G, mapping):
+    H = G.__class__()
+    H.add_nodes_from(mapping.get(n, n) for n in G)
+    H._node.update((mapping.get(n, n), d.copy()) for n, d in G.nodes.items())
+    if G.is_multigraph():
+        new_edges = [
+            (mapping.get(n1, n1), mapping.get(n2, n2), k, d.copy())
+            for (n1, n2, k, d) in G.edges
+        ]
+
+        # check for conflicting edge-keys
+        undirected = not G.is_directed()
+        seen_edges = set()
+        for i, (source, target, key, data) in enumerate(new_edges):
+            while (source, target, key) in seen_edges:
+                if not isinstance(key, (int, float)):
+                    key = 0
+                key += 1
+            seen_edges.add((source, target, key))
+            if undirected:
+                seen_edges.add((target, source, key))
+            new_edges[i] = (source, target, key, data)
+
+        H.add_edges_from(new_edges)
+    else:
+        H.add_edges_from(
+            (mapping.get(n1, n1), mapping.get(n2, n2), d.copy())
+            for (n1, n2, d) in G.edges
+        )
+    H.graph.update(G.graph)
+    return H
+
+
+def convert_node_labels_to_integers(
+    G, first_label=0, ordering="default", label_attribute=None
+):
+    """Returns a copy of the graph G with the nodes relabeled using
+    consecutive integers.
+
+    Parameters
+    ----------
+    G : graph
+       A easygraph graph
+
+    first_label : int, optional (default=0)
+       An integer specifying the starting offset in numbering nodes.
+       The new integer labels are numbered first_label, ..., n-1+first_label.
+
+    ordering : string
+       "default" : inherit node ordering from G.nodes
+       "sorted"  : inherit node ordering from sorted(G.nodes)
+       "increasing degree" : nodes are sorted by increasing degree
+       "decreasing degree" : nodes are sorted by decreasing degree
+
+    label_attribute : string, optional (default=None)
+       Name of node attribute to store old label.  If None no attribute
+       is created.
+
+    Notes
+    -----
+    Node and edge attribute data are copied to the new (relabeled) graph.
+
+    There is no guarantee that the relabeling of nodes to integers will
+    give the same two integers for two (even identical graphs).
+    Use the `ordering` argument to try to preserve the order.
+
+    See Also
+    --------
+    relabel_nodes
+    """
+    N = G.number_of_nodes() + first_label
+    if ordering == "default":
+        mapping = dict(zip(G.nodes, range(first_label, N)))
+    elif ordering == "sorted":
+        nlist = sorted(G.nodes)
+        mapping = dict(zip(nlist, range(first_label, N)))
+    elif ordering == "increasing degree":
+        dv_pairs = [(d, n) for (n, d) in G.degree()]
+        dv_pairs.sort()  # in-place sort from lowest to highest degree
+        mapping = dict(zip([n for d, n in dv_pairs], range(first_label, N)))
+    elif ordering == "decreasing degree":
+        dv_pairs = [(d, n) for (n, d) in G.degree()]
+        dv_pairs.sort()  # in-place sort from lowest to highest degree
+        dv_pairs.reverse()
+        mapping = dict(zip([n for d, n in dv_pairs], range(first_label, N)))
+    else:
+        raise eg.EasyGraphError(f"Unknown node ordering: {ordering}")
+    H = relabel_nodes(G, mapping)
+    # create node attribute with the old label
+    if label_attribute is not None:
+        eg.set_node_attributes(H, {v: k for k, v in mapping.items()}, label_attribute)
+    return H
```

## easygraph/utils/mapped_queue.py

 * *Ordering differences only*

```diff
@@ -1,186 +1,186 @@
-"""
-Priority queue class with updatable priorities.
-Codes from NetworkX - http://networkx.github.io/
-"""
-
-
-import heapq
-
-
-__all__ = ["MappedQueue"]
-
-
-class MappedQueue:
-    """
-    The MappedQueue class implements an efficient minimum heap. The
-    smallest element can be popped in O(1) time, new elements can be pushed
-    in O(log n) time, and any element can be removed or updated in O(log n)
-    time. The queue cannot contain duplicate elements and an attempt to push an
-    element already in the queue will have no effect.
-
-    MappedQueue complements the heapq package from the python standard
-    library. While MappedQueue is designed for maximum compatibility with
-    heapq, it has slightly different functionality.
-
-    Examples
-    --------
-
-    A `MappedQueue` can be created empty or optionally given an array of
-    initial elements. Calling `push()` will add an element and calling `pop()`
-    will remove and return the smallest element.
-
-    >>> q = MappedQueue([916, 50, 4609, 493, 237])
-    >>> q.push(1310)
-    True
-    >>> x = [q.pop() for i in range(len(q.h))]
-    >>> x
-    [50, 237, 493, 916, 1310, 4609]
-
-    Elements can also be updated or removed from anywhere in the queue.
-
-    >>> q = MappedQueue([916, 50, 4609, 493, 237])
-    >>> q.remove(493)
-    >>> q.update(237, 1117)
-    >>> x = [q.pop() for i in range(len(q.h))]
-    >>> x
-    [50, 916, 1117, 4609]
-
-    References
-    ----------
-    .. [1] Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001).
-       Introduction to algorithms second edition.
-    .. [2] Knuth, D. E. (1997). The art of computer programming (Vol. 3).
-       Pearson Education.
-    """
-
-    def __init__(self, data=[]):
-        """Priority queue class with updatable priorities."""
-        self.h = list(data)
-        self.d = dict()
-        self._heapify()
-
-    def __len__(self):
-        return len(self.h)
-
-    def _heapify(self):
-        """Restore heap invariant and recalculate map."""
-        heapq.heapify(self.h)
-        self.d = {elt: pos for pos, elt in enumerate(self.h)}
-        if len(self.h) != len(self.d):
-            raise AssertionError("Heap contains duplicate elements")
-
-    def push(self, elt):
-        """Add an element to the queue."""
-        # If element is already in queue, do nothing
-        if elt in self.d:
-            return False
-        # Add element to heap and dict
-        pos = len(self.h)
-        self.h.append(elt)
-        self.d[elt] = pos
-        # Restore invariant by sifting down
-        self._siftdown(pos)
-        return True
-
-    def pop(self):
-        """Remove and return the smallest element in the queue."""
-        # Remove smallest element
-        elt = self.h[0]
-        del self.d[elt]
-        # If elt is last item, remove and return
-        if len(self.h) == 1:
-            self.h.pop()
-            return elt
-        # Replace root with last element
-        last = self.h.pop()
-        self.h[0] = last
-        self.d[last] = 0
-        # Restore invariant by sifting up, then down
-        pos = self._siftup(0)
-        self._siftdown(pos)
-        # Return smallest element
-        return elt
-
-    def update(self, elt, new):
-        """Replace an element in the queue with a new one."""
-        # Replace
-        pos = self.d[elt]
-        self.h[pos] = new
-        del self.d[elt]
-        self.d[new] = pos
-        # Restore invariant by sifting up, then down
-        pos = self._siftup(pos)
-        self._siftdown(pos)
-
-    def remove(self, elt):
-        """Remove an element from the queue."""
-        # Find and remove element
-        try:
-            pos = self.d[elt]
-            del self.d[elt]
-        except KeyError:
-            # Not in queue
-            raise
-        # If elt is last item, remove and return
-        if pos == len(self.h) - 1:
-            self.h.pop()
-            return
-        # Replace elt with last element
-        last = self.h.pop()
-        self.h[pos] = last
-        self.d[last] = pos
-        # Restore invariant by sifting up, then down
-        pos = self._siftup(pos)
-        self._siftdown(pos)
-
-    def _siftup(self, pos):
-        """Move element at pos down to a leaf by repeatedly moving the smaller
-        child up."""
-        h, d = self.h, self.d
-        elt = h[pos]
-        # Continue until element is in a leaf
-        end_pos = len(h)
-        left_pos = (pos << 1) + 1
-        while left_pos < end_pos:
-            # Left child is guaranteed to exist by loop predicate
-            left = h[left_pos]
-            try:
-                right_pos = left_pos + 1
-                right = h[right_pos]
-                # Out-of-place, swap with left unless right is smaller
-                if right < left:
-                    h[pos], h[right_pos] = right, elt
-                    pos, right_pos = right_pos, pos
-                    d[elt], d[right] = pos, right_pos
-                else:
-                    h[pos], h[left_pos] = left, elt
-                    pos, left_pos = left_pos, pos
-                    d[elt], d[left] = pos, left_pos
-            except IndexError:
-                # Left leaf is the end of the heap, swap
-                h[pos], h[left_pos] = left, elt
-                pos, left_pos = left_pos, pos
-                d[elt], d[left] = pos, left_pos
-            # Update left_pos
-            left_pos = (pos << 1) + 1
-        return pos
-
-    def _siftdown(self, pos):
-        """Restore invariant by repeatedly replacing out-of-place element with
-        its parent."""
-        h, d = self.h, self.d
-        elt = h[pos]
-        # Continue until element is at root
-        while pos > 0:
-            parent_pos = (pos - 1) >> 1
-            parent = h[parent_pos]
-            if parent > elt:
-                # Swap out-of-place element with parent
-                h[parent_pos], h[pos] = elt, parent
-                parent_pos, pos = pos, parent_pos
-                d[elt] = pos
-                d[parent] = parent_pos
-            else:
-                # Invariant is satisfied
-                break
-        return pos
+"""
+Priority queue class with updatable priorities.
+Codes from NetworkX - http://networkx.github.io/
+"""
+
+
+import heapq
+
+
+__all__ = ["MappedQueue"]
+
+
+class MappedQueue:
+    """
+    The MappedQueue class implements an efficient minimum heap. The
+    smallest element can be popped in O(1) time, new elements can be pushed
+    in O(log n) time, and any element can be removed or updated in O(log n)
+    time. The queue cannot contain duplicate elements and an attempt to push an
+    element already in the queue will have no effect.
+
+    MappedQueue complements the heapq package from the python standard
+    library. While MappedQueue is designed for maximum compatibility with
+    heapq, it has slightly different functionality.
+
+    Examples
+    --------
+
+    A `MappedQueue` can be created empty or optionally given an array of
+    initial elements. Calling `push()` will add an element and calling `pop()`
+    will remove and return the smallest element.
+
+    >>> q = MappedQueue([916, 50, 4609, 493, 237])
+    >>> q.push(1310)
+    True
+    >>> x = [q.pop() for i in range(len(q.h))]
+    >>> x
+    [50, 237, 493, 916, 1310, 4609]
+
+    Elements can also be updated or removed from anywhere in the queue.
+
+    >>> q = MappedQueue([916, 50, 4609, 493, 237])
+    >>> q.remove(493)
+    >>> q.update(237, 1117)
+    >>> x = [q.pop() for i in range(len(q.h))]
+    >>> x
+    [50, 916, 1117, 4609]
+
+    References
+    ----------
+    .. [1] Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001).
+       Introduction to algorithms second edition.
+    .. [2] Knuth, D. E. (1997). The art of computer programming (Vol. 3).
+       Pearson Education.
+    """
+
+    def __init__(self, data=[]):
+        """Priority queue class with updatable priorities."""
+        self.h = list(data)
+        self.d = dict()
+        self._heapify()
+
+    def __len__(self):
+        return len(self.h)
+
+    def _heapify(self):
+        """Restore heap invariant and recalculate map."""
+        heapq.heapify(self.h)
+        self.d = {elt: pos for pos, elt in enumerate(self.h)}
+        if len(self.h) != len(self.d):
+            raise AssertionError("Heap contains duplicate elements")
+
+    def push(self, elt):
+        """Add an element to the queue."""
+        # If element is already in queue, do nothing
+        if elt in self.d:
+            return False
+        # Add element to heap and dict
+        pos = len(self.h)
+        self.h.append(elt)
+        self.d[elt] = pos
+        # Restore invariant by sifting down
+        self._siftdown(pos)
+        return True
+
+    def pop(self):
+        """Remove and return the smallest element in the queue."""
+        # Remove smallest element
+        elt = self.h[0]
+        del self.d[elt]
+        # If elt is last item, remove and return
+        if len(self.h) == 1:
+            self.h.pop()
+            return elt
+        # Replace root with last element
+        last = self.h.pop()
+        self.h[0] = last
+        self.d[last] = 0
+        # Restore invariant by sifting up, then down
+        pos = self._siftup(0)
+        self._siftdown(pos)
+        # Return smallest element
+        return elt
+
+    def update(self, elt, new):
+        """Replace an element in the queue with a new one."""
+        # Replace
+        pos = self.d[elt]
+        self.h[pos] = new
+        del self.d[elt]
+        self.d[new] = pos
+        # Restore invariant by sifting up, then down
+        pos = self._siftup(pos)
+        self._siftdown(pos)
+
+    def remove(self, elt):
+        """Remove an element from the queue."""
+        # Find and remove element
+        try:
+            pos = self.d[elt]
+            del self.d[elt]
+        except KeyError:
+            # Not in queue
+            raise
+        # If elt is last item, remove and return
+        if pos == len(self.h) - 1:
+            self.h.pop()
+            return
+        # Replace elt with last element
+        last = self.h.pop()
+        self.h[pos] = last
+        self.d[last] = pos
+        # Restore invariant by sifting up, then down
+        pos = self._siftup(pos)
+        self._siftdown(pos)
+
+    def _siftup(self, pos):
+        """Move element at pos down to a leaf by repeatedly moving the smaller
+        child up."""
+        h, d = self.h, self.d
+        elt = h[pos]
+        # Continue until element is in a leaf
+        end_pos = len(h)
+        left_pos = (pos << 1) + 1
+        while left_pos < end_pos:
+            # Left child is guaranteed to exist by loop predicate
+            left = h[left_pos]
+            try:
+                right_pos = left_pos + 1
+                right = h[right_pos]
+                # Out-of-place, swap with left unless right is smaller
+                if right < left:
+                    h[pos], h[right_pos] = right, elt
+                    pos, right_pos = right_pos, pos
+                    d[elt], d[right] = pos, right_pos
+                else:
+                    h[pos], h[left_pos] = left, elt
+                    pos, left_pos = left_pos, pos
+                    d[elt], d[left] = pos, left_pos
+            except IndexError:
+                # Left leaf is the end of the heap, swap
+                h[pos], h[left_pos] = left, elt
+                pos, left_pos = left_pos, pos
+                d[elt], d[left] = pos, left_pos
+            # Update left_pos
+            left_pos = (pos << 1) + 1
+        return pos
+
+    def _siftdown(self, pos):
+        """Restore invariant by repeatedly replacing out-of-place element with
+        its parent."""
+        h, d = self.h, self.d
+        elt = h[pos]
+        # Continue until element is at root
+        while pos > 0:
+            parent_pos = (pos - 1) >> 1
+            parent = h[parent_pos]
+            if parent > elt:
+                # Swap out-of-place element with parent
+                h[parent_pos], h[pos] = elt, parent
+                parent_pos, pos = pos, parent_pos
+                d[elt] = pos
+                d[parent] = parent_pos
+            else:
+                # Invariant is satisfied
+                break
+        return pos
```

## easygraph/utils/decorators.py

 * *Ordering differences only*

```diff
@@ -1,1145 +1,1145 @@
-import bz2
-import collections
-import gzip
-import inspect
-import re
-
-from collections import defaultdict
-from functools import wraps
-from os.path import splitext
-from pathlib import Path
-
-import easygraph as eg
-
-from easygraph.utils.exception import EasyGraphError
-
-
-__all__ = [
-    "only_implemented_for_UnDirected_graph",
-    "only_implemented_for_Directed_graph",
-    "open_file",
-    "nodes_or_number",
-    "not_implemented_for",
-    "hybrid",
-    "retry_method_with_fix",
-]
-
-
-def retry_method_with_fix(fix_method):
-    """Decorator that executes a fix method before retrying again when the decorated method
-    fails once with any exception.
-
-    If the decorated method fails again, the execution fails with that exception.
-
-    Notes
-    -----
-    This decorator only works on class methods, and the fix function must also be a class method.
-    It would not work on functions.
-
-    Parameters
-    ----------
-    fix_func : callable
-        The fix method to execute.  It should not accept any arguments.  Its return values are
-        ignored.
-    """
-
-    def _creator(func):
-        @wraps(func)
-        def wrapper(self, *args, **kwargs):
-            # pylint: disable=W0703,bare-except
-            try:
-                return func(self, *args, **kwargs)
-            except:
-                fix_method(self)
-                return func(self, *args, **kwargs)
-
-        return wrapper
-
-    return _creator
-
-
-def only_implemented_for_UnDirected_graph(func):
-    # print("--------{:<40}: Only Implemented For UnDirected Graph--------".format(func.__name__))
-    return func
-
-
-def only_implemented_for_Directed_graph(func):
-    # print("--------{:<40}: Only Implemented For Directed Graph--------".format(func.__name__))
-    return func
-
-
-def not_implemented_for(*graph_types):
-    """Decorator to mark algorithms as not implemented
-
-    Parameters
-    ----------
-    graph_types : container of strings
-        Entries must be one of "directed", "undirected", "multigraph", or "graph".
-
-    Returns
-    -------
-    _require : function
-        The decorated function.
-
-    Raises
-    ------
-    EasyGraphNotImplemented
-    If any of the packages cannot be imported
-
-    Notes
-    -----
-    Multiple types are joined logically with "and".
-    For "or" use multiple @not_implemented_for() lines.
-
-    Examples
-    --------
-    Decorate functions like this::
-
-       @not_implemented_for("directed")
-       def sp_function(G):
-           pass
-
-       # rule out MultiDiGraph
-       @not_implemented_for("directed","multigraph")
-       def sp_np_function(G):
-           pass
-
-       # rule out all except DiGraph
-       @not_implemented_for("undirected")
-       @not_implemented_for("multigraph")
-       def sp_np_function(G):
-           pass
-    """
-    if ("directed" in graph_types) and ("undirected" in graph_types):
-        raise ValueError("Function not implemented on directed AND undirected graphs?")
-    if ("multigraph" in graph_types) and ("graph" in graph_types):
-        raise ValueError("Function not implemented on graph AND multigraphs?")
-    if not set(graph_types) < {"directed", "undirected", "multigraph", "graph"}:
-        raise KeyError(
-            "use one or more of directed, undirected, multigraph, graph.  "
-            f"You used {graph_types}"
-        )
-
-    # 3-way logic: True if "directed" input, False if "undirected" input, else None
-    dval = ("directed" in graph_types) or not ("undirected" in graph_types) and None
-    mval = ("multigraph" in graph_types) or not ("graph" in graph_types) and None
-    errmsg = f"not implemented for {' '.join(graph_types)} type"
-
-    def _not_implemented_for(g):
-        if (mval is None or mval == g.is_multigraph()) and (
-            dval is None or dval == g.is_directed()
-        ):
-            raise eg.EasyGraphNotImplemented(errmsg)
-
-        return g
-
-    return argmap(_not_implemented_for, 0)
-
-
-import functools
-
-import cpp_easygraph
-
-
-def hybrid(cpp_method_name):
-    def _hybrid(py_method):
-        @functools.wraps(py_method)
-        def method(*args, **kwargs):
-            G = args[0]
-            if G.cflag and cpp_method_name is not None:
-                try:
-                    cpp_method = getattr(cpp_easygraph, cpp_method_name)
-                    return cpp_method(*args, **kwargs)
-                except AttributeError as e:
-                    print(f"Warning: {e}. Use python method instead.")
-            return py_method(*args, **kwargs)
-
-        return method
-
-    return _hybrid
-
-
-# To handle new extensions, define a function accepting a `path` and `mode`.
-# Then add the extension to _dispatch_dict.
-fopeners = {
-    ".gz": gzip.open,
-    ".gzip": gzip.open,
-    ".bz2": bz2.BZ2File,
-}
-_dispatch_dict = defaultdict(lambda: open, **fopeners)  # type: ignore
-
-
-def open_file(path_arg, mode="r"):
-    """Decorator to ensure clean opening and closing of files.
-
-    Parameters
-    ----------
-    path_arg : string or int
-        Name or index of the argument that is a path.
-
-    mode : str
-        String for opening mode.
-
-    Returns
-    -------
-    _open_file : function
-        Function which cleanly executes the io.
-
-    Examples
-    --------
-    Decorate functions like this::
-
-       @open_file(0,"r")
-       def read_function(pathname):
-           pass
-
-       @open_file(1,"w")
-       def write_function(G, pathname):
-           pass
-
-       @open_file(1,"w")
-       def write_function(G, pathname="graph.dot"):
-           pass
-
-       @open_file("pathname","w")
-       def write_function(G, pathname="graph.dot"):
-           pass
-
-       @open_file("path", "w+")
-       def another_function(arg, **kwargs):
-           path = kwargs["path"]
-           pass
-
-    Notes
-    -----
-    Note that this decorator solves the problem when a path argument is
-    specified as a string, but it does not handle the situation when the
-    function wants to accept a default of None (and then handle it).
-
-    Here is an example of how to handle this case::
-
-      @open_file("path")
-      def some_function(arg1, arg2, path=None):
-         if path is None:
-             fobj = tempfile.NamedTemporaryFile(delete=False)
-         else:
-             # `path` could have been a string or file object or something
-             # similar. In any event, the decorator has given us a file object
-             # and it will close it for us, if it should.
-             fobj = path
-
-         try:
-             fobj.write("blah")
-         finally:
-             if path is None:
-                 fobj.close()
-
-    Normally, we'd want to use "with" to ensure that fobj gets closed.
-    However, the decorator will make `path` a file object for us,
-    and using "with" would undesirably close that file object.
-    Instead, we use a try block, as shown above.
-    When we exit the function, fobj will be closed, if it should be, by the decorator.
-    """
-
-    def _open_file(path):
-        # Now we have the path_arg. There are two types of input to consider:
-        #   1) string representing a path that should be opened
-        #   2) an already opened file object
-        if isinstance(path, str):
-            ext = splitext(path)[1]
-        elif isinstance(path, Path):
-            # path is a pathlib reference to a filename
-            ext = path.suffix
-            path = str(path)
-        else:
-            # could be None, or a file handle, in which case the algorithm will deal with it
-            return path, lambda: None
-
-        fobj = _dispatch_dict[ext](path, mode=mode)
-        return fobj, lambda: fobj.close()
-
-    return argmap(_open_file, path_arg, try_finally=True)
-
-
-class argmap:
-    """A decorator to apply a map to arguments before calling the function
-
-    This class provides a decorator that maps (transforms) arguments of the function
-    before the function is called. Thus for example, we have similar code
-    in many functions to determine whether an argument is the number of nodes
-    to be created, or a list of nodes to be handled. The decorator provides
-    the code to accept either -- transforming the indicated argument into a
-    list of nodes before the actual function is called.
-
-    This decorator class allows us to process single or multiple arguments.
-    The arguments to be processed can be specified by string, naming the argument,
-    or by index, specifying the item in the args list.
-
-    Parameters
-    ----------
-    func : callable
-        The function to apply to arguments
-
-    *args : iterable of (int, str or tuple)
-        A list of parameters, specified either as strings (their names), ints
-        (numerical indices) or tuples, which may contain ints, strings, and
-        (recursively) tuples. Each indicates which parameters the decorator
-        should map. Tuples indicate that the map function takes (and returns)
-        multiple parameters in the same order and nested structure as indicated
-        here.
-
-    try_finally : bool (default: False)
-        When True, wrap the function call in a try-finally block with code
-        for the finally block created by `func`. This is used when the map
-        function constructs an object (like a file handle) that requires
-        post-processing (like closing).
-
-    Examples
-    --------
-    Most of these examples use `@argmap(...)` to apply the decorator to
-    the function defined on the next line.
-    In the EasyGraph codebase however, `argmap` is used within a function to
-    construct a decorator. That is, the decorator defines a mapping function
-    and then uses `argmap` to build and return a decorated function.
-    A simple example is a decorator that specifies which currency to report money.
-    The decorator (named `convert_to`) would be used like::
-
-        @convert_to("US_Dollars", "income")
-        def show_me_the_money(name, income):
-            print(f"{name} : {income}")
-
-    And the code to create the decorator might be::
-
-        def convert_to(currency, which_arg):
-            def _convert(amount):
-                if amount.currency != currency:
-                    amount = amount.to_currency(currency)
-                return amount
-            return argmap(_convert, which_arg)
-
-    Despite this common idiom for argmap, most of the following examples
-    use the `@argmap(...)` idiom to save space.
-
-    Here's an example use of argmap to sum the elements of two of the functions
-    arguments. The decorated function::
-
-        @argmap(sum, "xlist", "zlist")
-        def foo(xlist, y, zlist):
-            return xlist - y + zlist
-
-    is syntactic sugar for::
-
-        def foo(xlist, y, zlist):
-            x = sum(xlist)
-            z = sum(zlist)
-            return x - y + z
-
-    and is equivalent to (using argument indexes)::
-
-        @argmap(sum, "xlist", 2)
-        def foo(xlist, y, zlist):
-            return xlist - y + zlist
-
-    or::
-
-        @argmap(sum, "zlist", 0)
-        def foo(xlist, y, zlist):
-            return xlist - y + zlist
-
-    Transforming functions can be applied to multiple arguments, such as::
-
-        def swap(x, y):
-            return y, x
-
-        # the 2-tuple tells argmap that the map `swap` has 2 inputs/outputs.
-        @argmap(swap, ("a", "b")):
-        def foo(a, b, c):
-            return a / b * c
-
-    is equivalent to::
-
-        def foo(a, b, c):
-            a, b = swap(a, b)
-            return a / b * c
-
-    More generally, the applied arguments can be nested tuples of strings or ints.
-    The syntax `@argmap(some_func, ("a", ("b", "c")))` would expect `some_func` to
-    accept 2 inputs with the second expected to be a 2-tuple. It should then return
-    2 outputs with the second a 2-tuple. The returns values would replace input "a"
-    "b" and "c" respectively. Similarly for `@argmap(some_func, (0, ("b", 2)))`.
-
-    Also, note that an index larger than the number of named parameters is allowed
-    for variadic functions. For example::
-
-        def double(a):
-            return 2 * a
-
-        @argmap(double, 3)
-        def overflow(a, *args):
-            return a, args
-
-        print(overflow(1, 2, 3, 4, 5, 6))  # output is 1, (2, 3, 8, 5, 6)
-
-    **Try Finally**
-
-    Additionally, this `argmap` class can be used to create a decorator that
-    initiates a try...finally block. The decorator must be written to return
-    both the transformed argument and a closing function.
-    This feature was included to enable the `open_file` decorator which might
-    need to close the file or not depending on whether it had to open that file.
-    This feature uses the keyword-only `try_finally` argument to `@argmap`.
-
-    For example this map opens a file and then makes sure it is closed::
-
-        def open_file(fn):
-            f = open(fn)
-            return f, lambda: f.close()
-
-    The decorator applies that to the function `foo`::
-
-        @argmap(open_file, "file", try_finally=True)
-        def foo(file):
-            print(file.read())
-
-    is syntactic sugar for::
-
-        def foo(file):
-            file, close_file = open_file(file)
-            try:
-                print(file.read())
-            finally:
-                close_file()
-
-    and is equivalent to (using indexes)::
-
-        @argmap(open_file, 0, try_finally=True)
-        def foo(file):
-            print(file.read())
-
-    Here's an example of the try_finally feature used to create a decorator::
-
-        def my_closing_decorator(which_arg):
-            def _opener(path):
-                if path is None:
-                    path = open(path)
-                    fclose = path.close
-                else:
-                    # assume `path` handles the closing
-                    fclose = lambda: None
-                return path, fclose
-            return argmap(_opener, which_arg, try_finally=True)
-
-    which can then be used as::
-
-        @my_closing_decorator("file")
-        def fancy_reader(file=None):
-            # this code doesn't need to worry about closing the file
-            print(file.read())
-
-    Notes
-    -----
-    An object of this class is callable and intended to be used when
-    defining a decorator. Generally, a decorator takes a function as input
-    and constructs a function as output. Specifically, an `argmap` object
-    returns the input function decorated/wrapped so that specified arguments
-    are mapped (transformed) to new values before the decorated function is called.
-
-    As an overview, the argmap object returns a new function with all the
-    dunder values of the original function (like `__doc__`, `__name__`, etc).
-    Code for this decorated function is built based on the original function's
-    signature. It starts by mapping the input arguments to potentially new
-    values. Then it calls the decorated function with these new values in place
-    of the indicated arguments that have been mapped. The return value of the
-    original function is then returned. This new function is the function that
-    is actually called by the user.
-
-    Three additional features are provided.
-        1) The code is lazily compiled. That is, the new function is returned
-        as an object without the code compiled, but with all information
-        needed so it can be compiled upon it's first invocation. This saves
-        time on import at the cost of additional time on the first call of
-        the function. Subsequent calls are then just as fast as normal.
-
-        2) If the "try_finally" keyword-only argument is True, a try block
-        follows each mapped argument, matched on the other side of the wrapped
-        call, by a finally block closing that mapping.  We expect func to return
-        a 2-tuple: the mapped value and a function to be called in the finally
-        clause.  This feature was included so the `open_file` decorator could
-        provide a file handle to the decorated function and close the file handle
-        after the function call. It even keeps track of whether to close the file
-        handle or not based on whether it had to open the file or the input was
-        already open. So, the decorated function does not need to include any
-        code to open or close files.
-
-        3) The maps applied can process multiple arguments. For example,
-        you could swap two arguments using a mapping, or transform
-        them to their sum and their difference. This was included to allow
-        a decorator in the `quality.py` module that checks that an input
-        `partition` is a valid partition of the nodes of the input graph `G`.
-        In this example, the map has inputs `(G, partition)`. After checking
-        for a valid partition, the map either raises an exception or leaves
-        the inputs unchanged. Thus many functions that make this check can
-        use the decorator rather than copy the checking code into each function.
-        More complicated nested argument structures are described below.
-
-    The remaining notes describe the code structure and methods for this
-    class in broad terms to aid in understanding how to use it.
-
-    Instantiating an `argmap` object simply stores the mapping function and
-    the input identifiers of which arguments to map. The resulting decorator
-    is ready to use this map to decorate any function. Calling that object
-    (`argmap.__call__`, but usually done via `@my_decorator`) a lazily
-    compiled thin wrapper of the decorated function is constructed,
-    wrapped with the necessary function dunder attributes like `__doc__`
-    and `__name__`. That thinly wrapped function is returned as the
-    decorated function. When that decorated function is called, the thin
-    wrapper of code calls `argmap._lazy_compile` which compiles the decorated
-    function (using `argmap.compile`) and replaces the code of the thin
-    wrapper with the newly compiled code. This saves the compilation step
-    every import of easygraph, at the cost of compiling upon the first call
-    to the decorated function.
-
-    When the decorated function is compiled, the code is recursively assembled
-    using the `argmap.assemble` method. The recursive nature is needed in
-    case of nested decorators. The result of the assembly is a number of
-    useful objects.
-
-      sig : the function signature of the original decorated function as
-          constructed by :func:`argmap.signature`. This is constructed
-          using `inspect.signature` but enhanced with attribute
-          strings `sig_def` and `sig_call`, and other information
-          specific to mapping arguments of this function.
-          This information is used to construct a string of code defining
-          the new decorated function.
-
-      wrapped_name : a unique internally used name constructed by argmap
-          for the decorated function.
-
-      functions : a dict of the functions used inside the code of this
-          decorated function, to be used as `globals` in `exec`.
-          This dict is recursively updated to allow for nested decorating.
-
-      mapblock : code (as a list of strings) to map the incoming argument
-          values to their mapped values.
-
-      finallys : code (as a list of strings) to provide the possibly nested
-          set of finally clauses if needed.
-
-      mutable_args : a bool indicating whether the `sig.args` tuple should be
-          converted to a list so mutation can occur.
-
-    After this recursive assembly process, the `argmap.compile` method
-    constructs code (as strings) to convert the tuple `sig.args` to a list
-    if needed. It joins the defining code with appropriate indents and
-    compiles the result.  Finally, this code is evaluated and the original
-    wrapper's implementation is replaced with the compiled version (see
-    `argmap._lazy_compile` for more details).
-
-    Other `argmap` methods include `_name` and `_count` which allow internally
-    generated names to be unique within a python session.
-    The methods `_flatten` and `_indent` process the nested lists of strings
-    into properly indented python code ready to be compiled.
-
-    More complicated nested tuples of arguments also allowed though
-    usually not used. For the simple 2 argument case, the argmap
-    input ("a", "b") implies the mapping function will take 2 arguments
-    and return a 2-tuple of mapped values. A more complicated example
-    with argmap input `("a", ("b", "c"))` requires the mapping function
-    take 2 inputs, with the second being a 2-tuple. It then must output
-    the 3 mapped values in the same nested structure `(newa, (newb, newc))`.
-    This level of generality is not often needed, but was convenient
-    to implement when handling the multiple arguments.
-
-    See Also
-    --------
-    not_implemented_for
-    open_file
-    nodes_or_number
-    random_state
-    py_random_state
-    easygraph.community.quality.require_partition
-    require_partition
-
-    """
-
-    def __init__(self, func, *args, try_finally=False):
-        self._func = func
-        self._args = args
-        self._finally = try_finally
-
-    @staticmethod
-    def _lazy_compile(func):
-        """Compile the source of a wrapped function
-
-        Assemble and compile the decorated function, and intrusively replace its
-        code with the compiled version's.  The thinly wrapped function becomes
-        the decorated function.
-
-        Parameters
-        ----------
-        func : callable
-            A function returned by argmap.__call__ which is in the process
-            of being called for the first time.
-
-        Returns
-        -------
-        func : callable
-            The same function, with a new __code__ object.
-
-        Notes
-        -----
-        It was observed in easygraph issue #4732 [1] that the import time of
-        easygraph was significantly bloated by the use of decorators: over half
-        of the import time was being spent decorating functions.  This was
-        somewhat improved by a change made to the `decorator` library, at the
-        cost of a relatively heavy-weight call to `inspect.Signature.bind`
-        for each call to the decorated function.
-
-        The workaround we arrived at is to do minimal work at the time of
-        decoration.  When the decorated function is called for the first time,
-        we compile a function with the same function signature as the wrapped
-        function.  The resulting decorated function is faster than one made by
-        the `decorator` library, so that the overhead of the first call is
-        'paid off' after a small number of calls.
-        """
-        real_func = func.__argmap__.compile(func.__wrapped__)
-        func.__code__ = real_func.__code__
-        func.__globals__.update(real_func.__globals__)
-        func.__dict__.update(real_func.__dict__)
-        return func
-
-    def __call__(self, f):
-        """Construct a lazily decorated wrapper of f.
-
-        The decorated function will be compiled when it is called for the first time,
-        and it will replace its own __code__ object so subsequent calls are fast.
-
-        Parameters
-        ----------
-        f : callable
-            A function to be decorated.
-
-        Returns
-        -------
-        func : callable
-            The decorated function.
-
-        See Also
-        --------
-        argmap._lazy_compile
-        """
-
-        if inspect.isgeneratorfunction(f):
-
-            def func(*args, __wrapper=None, **kwargs):
-                yield from argmap._lazy_compile(__wrapper)(*args, **kwargs)
-
-        else:
-
-            def func(*args, __wrapper=None, **kwargs):
-                return argmap._lazy_compile(__wrapper)(*args, **kwargs)
-
-        # standard function-wrapping stuff
-        func.__name__ = f.__name__
-        func.__doc__ = f.__doc__
-        func.__defaults__ = f.__defaults__
-        func.__kwdefaults__.update(f.__kwdefaults__ or {})
-        func.__module__ = f.__module__
-        func.__qualname__ = f.__qualname__
-        func.__dict__.update(f.__dict__)
-        func.__wrapped__ = f
-
-        # now that we've wrapped f, we may have picked up some __dict__ or
-        # __kwdefaults__ items that were set by a previous argmap.  Thus, we set
-        # these values after those update() calls.
-
-        # If we attempt to access func from within itself, that happens through
-        # a closure -- which trips an error when we replace func.__code__.  The
-        # standard workaround for functions which can't see themselves is to use
-        # a Y-combinator, as we do here.
-        func.__kwdefaults__["_argmap__wrapper"] = func
-
-        # this self-reference is here because functools.wraps preserves
-        # everything in __dict__, and we don't want to mistake a non-argmap
-        # wrapper for an argmap wrapper
-        func.__self__ = func
-
-        # this is used to variously call self.assemble and self.compile
-        func.__argmap__ = self
-
-        return func
-
-    __count = 0
-
-    @classmethod
-    def _count(cls):
-        """Maintain a globally-unique identifier for function names and "file" names
-
-        Note that this counter is a class method reporting a class variable
-        so the count is unique within a Python session. It could differ from
-        session to session for a specific decorator depending on the order
-        that the decorators are created. But that doesn't disrupt `argmap`.
-
-        This is used in two places: to construct unique variable names
-        in the `_name` method and to construct unique fictitious filenames
-        in the `_compile` method.
-
-        Returns
-        -------
-        count : int
-            An integer unique to this Python session (simply counts from zero)
-        """
-        cls.__count += 1
-        return cls.__count
-
-    _bad_chars = re.compile("[^a-zA-Z0-9_]")
-
-    @classmethod
-    def _name(cls, f):
-        """Mangle the name of a function to be unique but somewhat human-readable
-
-        The names are unique within a Python session and set using `_count`.
-
-        Parameters
-        ----------
-        f : str or object
-
-        Returns
-        -------
-        name : str
-            The mangled version of `f.__name__` (if `f.__name__` exists) or `f`
-
-        """
-        f = f.__name__ if hasattr(f, "__name__") else f
-        fname = re.sub(cls._bad_chars, "_", f)
-        return f"argmap_{fname}_{cls._count()}"
-
-    def compile(self, f):
-        """Compile the decorated function.
-
-        Called once for a given decorated function -- collects the code from all
-        argmap decorators in the stack, and compiles the decorated function.
-
-        Much of the work done here uses the `assemble` method to allow recursive
-        treatment of multiple argmap decorators on a single decorated function.
-        That flattens the argmap decorators, collects the source code to construct
-        a single decorated function, then compiles/executes/returns that function.
-
-        The source code for the decorated function is stored as an attribute
-        `_code` on the function object itself.
-
-        Note that Python's `compile` function requires a filename, but this
-        code is constructed without a file, so a fictitious filename is used
-        to describe where the function comes from. The name is something like:
-        "argmap compilation 4".
-
-        Parameters
-        ----------
-        f : callable
-            The function to be decorated
-
-        Returns
-        -------
-        func : callable
-            The decorated file
-
-        """
-        sig, wrapped_name, functions, mapblock, finallys, mutable_args = self.assemble(
-            f
-        )
-
-        call = f"{sig.call_sig.format(wrapped_name)}#"
-        mut_args = f"{sig.args} = list({sig.args})" if mutable_args else ""
-        body = argmap._indent(sig.def_sig, mut_args, mapblock, call, finallys)
-        code = "\n".join(body)
-
-        locl = {}
-        globl = dict(functions.values())
-        filename = f"{self.__class__} compilation {self._count()}"
-        compiled = compile(code, filename, "exec")
-        exec(compiled, globl, locl)
-        func = locl[sig.name]
-        func._code = code
-        return func
-
-    def assemble(self, f):
-        """Collects components of the source for the decorated function wrapping f.
-
-        If `f` has multiple argmap decorators, we recursively assemble the stack of
-        decorators into a single flattened function.
-
-        This method is part of the `compile` method's process yet separated
-        from that method to allow recursive processing. The outputs are
-        strings, dictionaries and lists that collect needed info to
-        flatten any nested argmap-decoration.
-
-        Parameters
-        ----------
-        f : callable
-            The function to be decorated.  If f is argmapped, we assemble it.
-
-        Returns
-        -------
-        sig : argmap.Signature
-            The function signature as an `argmap.Signature` object.
-        wrapped_name : str
-            The mangled name used to represent the wrapped function in the code
-            being assembled.
-        functions : dict
-            A dictionary mapping id(g) -> (mangled_name(g), g) for functions g
-            referred to in the code being assembled. These need to be present
-            in the ``globals`` scope of ``exec`` when defining the decorated
-            function.
-        mapblock : list of lists and/or strings
-            Code that implements mapping of parameters including any try blocks
-            if needed. This code will precede the decorated function call.
-        finallys : list of lists and/or strings
-            Code that implements the finally blocks to post-process the
-            arguments (usually close any files if needed) after the
-            decorated function is called.
-        mutable_args : bool
-            True if the decorator needs to modify positional arguments
-            via their indices. The compile method then turns the argument
-            tuple into a list so that the arguments can be modified.
-        """
-
-        # first, we check if f is already argmapped -- if that's the case,
-        # build up the function recursively.
-        # > mapblock is generally a list of function calls of the sort
-        #     arg = func(arg)
-        # in addition to some try-blocks if needed.
-        # > finallys is a recursive list of finally blocks of the sort
-        #         finally:
-        #             close_func_1()
-        #     finally:
-        #         close_func_2()
-        # > functions is a dict of functions used in the scope of our decorated
-        # function. It will be used to construct globals used in compilation.
-        # We make functions[id(f)] = name_of_f, f to ensure that a given
-        # function is stored and named exactly once even if called by
-        # nested decorators.
-        if hasattr(f, "__argmap__") and f.__self__ is f:
-            (
-                sig,
-                wrapped_name,
-                functions,
-                mapblock,
-                finallys,
-                mutable_args,
-            ) = f.__argmap__.assemble(f.__wrapped__)
-            functions = dict(functions)  # shallow-copy just in case
-        else:
-            sig = self.signature(f)
-            wrapped_name = self._name(f)
-            mapblock, finallys = [], []
-            functions = {id(f): (wrapped_name, f)}
-            mutable_args = False
-
-        if id(self._func) in functions:
-            fname, _ = functions[id(self._func)]
-        else:
-            fname, _ = functions[id(self._func)] = self._name(self._func), self._func
-
-        # this is a bit complicated -- we can call functions with a variety of
-        # nested arguments, so long as their input and output are tuples with
-        # the same nested structure. e.g. ("a", "b") maps arguments a and b.
-        # A more complicated nesting like (0, (3, 4)) maps arguments 0, 3, 4
-        # expecting the mapping to output new values in the same nested shape.
-        # while we're not taking full advantage of the ability to handle
-        # multiply-nested tuples, it was convenient to implement this in
-        # generality because the recursive call to `get_name` is necessary in
-        # any case.
-        applied = set()
-
-        def get_name(arg, first=True):
-            nonlocal mutable_args
-            if isinstance(arg, tuple):
-                name = ", ".join(get_name(x, False) for x in arg)
-                return name if first else f"({name})"
-            if arg in applied:
-                raise EasyGraphError(f"argument {arg} is specified multiple times")
-            applied.add(arg)
-            if arg in sig.names:
-                return sig.names[arg]
-            elif isinstance(arg, str):
-                if sig.kwargs is None:
-                    raise EasyGraphError(
-                        f"name {arg} is not a named parameter and this function doesn't"
-                        " have kwargs"
-                    )
-                return f"{sig.kwargs}[{arg!r}]"
-            else:
-                if sig.args is None:
-                    raise EasyGraphError(
-                        f"index {arg} not a parameter index and this function doesn't"
-                        " have args"
-                    )
-                mutable_args = True
-                return f"{sig.args}[{arg - sig.n_positional}]"
-
-        if self._finally:
-            # here's where we handle try_finally decorators.  Such a decorator
-            # returns a mapped argument and a function to be called in a
-            # finally block.  This feature was required by the open_file
-            # decorator.  The below generates the code
-            #
-            # name, final = func(name)                   #<--append to mapblock
-            # try:                                       #<--append to mapblock
-            #     ... more argmapping and try blocks
-            #     return WRAPPED_FUNCTION(...)
-            #     ... more finally blocks
-            # finally:                                   #<--prepend to finallys
-            #     final()                                #<--prepend to finallys
-            #
-            for a in self._args:
-                name = get_name(a)
-                final = self._name(name)
-                mapblock.append(f"{name}, {final} = {fname}({name})")
-                mapblock.append("try:")
-                finallys = ["finally:", f"{final}()#", "#", finallys]
-        else:
-            mapblock.extend(
-                f"{name} = {fname}({name})" for name in map(get_name, self._args)
-            )
-
-        return sig, wrapped_name, functions, mapblock, finallys, mutable_args
-
-    @classmethod
-    def signature(cls, f):
-        r"""Construct a Signature object describing `f`
-
-        Compute a Signature so that we can write a function wrapping f with
-        the same signature and call-type.
-
-        Parameters
-        ----------
-        f : callable
-            A function to be decorated
-
-        Returns
-        -------
-        sig : argmap.Signature
-            The Signature of f
-
-        Notes
-        -----
-        The Signature is a namedtuple with names:
-
-            name : a unique version of the name of the decorated function
-            signature : the inspect.signature of the decorated function
-            def_sig : a string used as code to define the new function
-            call_sig : a string used as code to call the decorated function
-            names : a dict keyed by argument name and index to the argument's name
-            n_positional : the number of positional arguments in the signature
-            args : the name of the VAR_POSITIONAL argument if any, i.e. \*theseargs
-            kwargs : the name of the VAR_KEYWORDS argument if any, i.e. \*\*kwargs
-
-        These named attributes of the signature are used in `assemble` and `compile`
-        to construct a string of source code for the decorated function.
-
-        """
-        sig = inspect.signature(f, follow_wrapped=False)
-        def_sig = []
-        call_sig = []
-        names = {}
-
-        kind = None
-        args = None
-        kwargs = None
-        npos = 0
-        for i, param in enumerate(sig.parameters.values()):
-            # parameters can be position-only, keyword-or-position, keyword-only
-            # in any combination, but only in the order as above.  we do edge
-            # detection to add the appropriate punctuation
-            prev = kind
-            kind = param.kind
-            if prev == param.POSITIONAL_ONLY != kind:
-                # the last token was position-only, but this one isn't
-                def_sig.append("/")
-            if prev != param.KEYWORD_ONLY == kind != param.VAR_POSITIONAL:
-                # param is the first keyword-only arg and isn't starred
-                def_sig.append("*")
-
-            # star arguments as appropriate
-            if kind == param.VAR_POSITIONAL:
-                name = "*" + param.name
-                args = param.name
-                count = 0
-            elif kind == param.VAR_KEYWORD:
-                name = "**" + param.name
-                kwargs = param.name
-                count = 0
-            else:
-                names[i] = names[param.name] = param.name
-                name = param.name
-                count = 1
-
-            # assign to keyword-only args in the function call
-            if kind == param.KEYWORD_ONLY:
-                call_sig.append(f"{name} = {name}")
-            else:
-                npos += count
-                call_sig.append(name)
-
-            def_sig.append(name)
-
-        fname = cls._name(f)
-        def_sig = f'def {fname}({", ".join(def_sig)}):'
-
-        if inspect.isgeneratorfunction(f):
-            _return = "yield from"
-        else:
-            _return = "return"
-
-        call_sig = f"{_return} {{}}({', '.join(call_sig)})"
-
-        return cls.Signature(fname, sig, def_sig, call_sig, names, npos, args, kwargs)
-
-    Signature = collections.namedtuple(
-        "Signature",
-        [
-            "name",
-            "signature",
-            "def_sig",
-            "call_sig",
-            "names",
-            "n_positional",
-            "args",
-            "kwargs",
-        ],
-    )
-
-    @staticmethod
-    def _flatten(nestlist, visited):
-        """flattens a recursive list of lists that doesn't have cyclic references
-
-        Parameters
-        ----------
-        nestlist : iterable
-            A recursive list of objects to be flattened into a single iterable
-
-        visited : set
-            A set of object ids which have been walked -- initialize with an
-            empty set
-
-        Yields
-        ------
-        Non-list objects contained in nestlist
-
-        """
-        for thing in nestlist:
-            if isinstance(thing, list):
-                if id(thing) in visited:
-                    raise ValueError("A cycle was found in nestlist.  Be a tree.")
-                else:
-                    visited.add(id(thing))
-                yield from argmap._flatten(thing, visited)
-            else:
-                yield thing
-
-    _tabs = " " * 64
-
-    @staticmethod
-    def _indent(*lines):
-        """Indent list of code lines to make executable Python code
-
-        Indents a tree-recursive list of strings, following the rule that one
-        space is added to the tab after a line that ends in a colon, and one is
-        removed after a line that ends in an hashmark.
-
-        Parameters
-        ----------
-        *lines : lists and/or strings
-            A recursive list of strings to be assembled into properly indented
-            code.
-
-        Returns
-        -------
-        code : str
-
-        Examples
-        --------
-
-            argmap._indent(*["try:", "try:", "pass#", "finally:", "pass#", "#",
-                             "finally:", "pass#"])
-
-        renders to
-
-            '''try:
-             try:
-              pass#
-             finally:
-              pass#
-             #
-            finally:
-             pass#'''
-        """
-        depth = 0
-        for line in argmap._flatten(lines, set()):
-            yield f"{argmap._tabs[:depth]}{line}"
-            depth += (line[-1:] == ":") - (line[-1:] == "#")
-
-
-def nodes_or_number(which_args):
-    """Decorator to allow number of nodes or container of nodes.
-
-    With this decorator, the specified argument can be either a number or a container
-    of nodes. If it is a number, the nodes used are `range(n)`.
-    This allows `eg.complete_graph(50)` in place of `eg.complete_graph(list(range(50)))`.
-    And it also allows `eg.complete_graph(any_list_of_nodes)`.
-
-    Parameters
-    ----------
-    which_args : string or int or sequence of strings or ints
-        If string, the name of the argument to be treated.
-        If int, the index of the argument to be treated.
-        If more than one node argument is allowed, can be a list of locations.
-
-    Returns
-    -------
-    _nodes_or_numbers : function
-        Function which replaces int args with ranges.
-
-    Examples
-    --------
-    Decorate functions like this::
-
-       @nodes_or_number("nodes")
-       def empty_graph(nodes):
-           # nodes is converted to a list of nodes
-
-       @nodes_or_number(0)
-       def empty_graph(nodes):
-           # nodes is converted to a list of nodes
-
-       @nodes_or_number(["m1", "m2"])
-       def grid_2d_graph(m1, m2, periodic=False):
-           # m1 and m2 are each converted to a list of nodes
-
-       @nodes_or_number([0, 1])
-       def grid_2d_graph(m1, m2, periodic=False):
-           # m1 and m2 are each converted to a list of nodes
-
-       @nodes_or_number(1)
-       def full_rary_tree(r, n)
-           # presumably r is a number. It is not handled by this decorator.
-           # n is converted to a list of nodes
-    """
-
-    def _nodes_or_number(n):
-        try:
-            nodes = list(range(n))
-        except TypeError:
-            nodes = tuple(n)
-        else:
-            if n < 0:
-                msg = "Negative number of nodes not valid: {n}"
-                raise EasyGraphError(msg)
-        return (n, nodes)
-
-    try:
-        iter_wa = iter(which_args)
-    except TypeError:
-        iter_wa = (which_args,)
-
-    return argmap(_nodes_or_number, *iter_wa)
+import bz2
+import collections
+import gzip
+import inspect
+import re
+
+from collections import defaultdict
+from functools import wraps
+from os.path import splitext
+from pathlib import Path
+
+import easygraph as eg
+
+from easygraph.utils.exception import EasyGraphError
+
+
+__all__ = [
+    "only_implemented_for_UnDirected_graph",
+    "only_implemented_for_Directed_graph",
+    "open_file",
+    "nodes_or_number",
+    "not_implemented_for",
+    "hybrid",
+    "retry_method_with_fix",
+]
+
+
+def retry_method_with_fix(fix_method):
+    """Decorator that executes a fix method before retrying again when the decorated method
+    fails once with any exception.
+
+    If the decorated method fails again, the execution fails with that exception.
+
+    Notes
+    -----
+    This decorator only works on class methods, and the fix function must also be a class method.
+    It would not work on functions.
+
+    Parameters
+    ----------
+    fix_func : callable
+        The fix method to execute.  It should not accept any arguments.  Its return values are
+        ignored.
+    """
+
+    def _creator(func):
+        @wraps(func)
+        def wrapper(self, *args, **kwargs):
+            # pylint: disable=W0703,bare-except
+            try:
+                return func(self, *args, **kwargs)
+            except:
+                fix_method(self)
+                return func(self, *args, **kwargs)
+
+        return wrapper
+
+    return _creator
+
+
+def only_implemented_for_UnDirected_graph(func):
+    # print("--------{:<40}: Only Implemented For UnDirected Graph--------".format(func.__name__))
+    return func
+
+
+def only_implemented_for_Directed_graph(func):
+    # print("--------{:<40}: Only Implemented For Directed Graph--------".format(func.__name__))
+    return func
+
+
+def not_implemented_for(*graph_types):
+    """Decorator to mark algorithms as not implemented
+
+    Parameters
+    ----------
+    graph_types : container of strings
+        Entries must be one of "directed", "undirected", "multigraph", or "graph".
+
+    Returns
+    -------
+    _require : function
+        The decorated function.
+
+    Raises
+    ------
+    EasyGraphNotImplemented
+    If any of the packages cannot be imported
+
+    Notes
+    -----
+    Multiple types are joined logically with "and".
+    For "or" use multiple @not_implemented_for() lines.
+
+    Examples
+    --------
+    Decorate functions like this::
+
+       @not_implemented_for("directed")
+       def sp_function(G):
+           pass
+
+       # rule out MultiDiGraph
+       @not_implemented_for("directed","multigraph")
+       def sp_np_function(G):
+           pass
+
+       # rule out all except DiGraph
+       @not_implemented_for("undirected")
+       @not_implemented_for("multigraph")
+       def sp_np_function(G):
+           pass
+    """
+    if ("directed" in graph_types) and ("undirected" in graph_types):
+        raise ValueError("Function not implemented on directed AND undirected graphs?")
+    if ("multigraph" in graph_types) and ("graph" in graph_types):
+        raise ValueError("Function not implemented on graph AND multigraphs?")
+    if not set(graph_types) < {"directed", "undirected", "multigraph", "graph"}:
+        raise KeyError(
+            "use one or more of directed, undirected, multigraph, graph.  "
+            f"You used {graph_types}"
+        )
+
+    # 3-way logic: True if "directed" input, False if "undirected" input, else None
+    dval = ("directed" in graph_types) or not ("undirected" in graph_types) and None
+    mval = ("multigraph" in graph_types) or not ("graph" in graph_types) and None
+    errmsg = f"not implemented for {' '.join(graph_types)} type"
+
+    def _not_implemented_for(g):
+        if (mval is None or mval == g.is_multigraph()) and (
+            dval is None or dval == g.is_directed()
+        ):
+            raise eg.EasyGraphNotImplemented(errmsg)
+
+        return g
+
+    return argmap(_not_implemented_for, 0)
+
+
+import functools
+
+import cpp_easygraph
+
+
+def hybrid(cpp_method_name):
+    def _hybrid(py_method):
+        @functools.wraps(py_method)
+        def method(*args, **kwargs):
+            G = args[0]
+            if G.cflag and cpp_method_name is not None:
+                try:
+                    cpp_method = getattr(cpp_easygraph, cpp_method_name)
+                    return cpp_method(*args, **kwargs)
+                except AttributeError as e:
+                    print(f"Warning: {e}. Use python method instead.")
+            return py_method(*args, **kwargs)
+
+        return method
+
+    return _hybrid
+
+
+# To handle new extensions, define a function accepting a `path` and `mode`.
+# Then add the extension to _dispatch_dict.
+fopeners = {
+    ".gz": gzip.open,
+    ".gzip": gzip.open,
+    ".bz2": bz2.BZ2File,
+}
+_dispatch_dict = defaultdict(lambda: open, **fopeners)  # type: ignore
+
+
+def open_file(path_arg, mode="r"):
+    """Decorator to ensure clean opening and closing of files.
+
+    Parameters
+    ----------
+    path_arg : string or int
+        Name or index of the argument that is a path.
+
+    mode : str
+        String for opening mode.
+
+    Returns
+    -------
+    _open_file : function
+        Function which cleanly executes the io.
+
+    Examples
+    --------
+    Decorate functions like this::
+
+       @open_file(0,"r")
+       def read_function(pathname):
+           pass
+
+       @open_file(1,"w")
+       def write_function(G, pathname):
+           pass
+
+       @open_file(1,"w")
+       def write_function(G, pathname="graph.dot"):
+           pass
+
+       @open_file("pathname","w")
+       def write_function(G, pathname="graph.dot"):
+           pass
+
+       @open_file("path", "w+")
+       def another_function(arg, **kwargs):
+           path = kwargs["path"]
+           pass
+
+    Notes
+    -----
+    Note that this decorator solves the problem when a path argument is
+    specified as a string, but it does not handle the situation when the
+    function wants to accept a default of None (and then handle it).
+
+    Here is an example of how to handle this case::
+
+      @open_file("path")
+      def some_function(arg1, arg2, path=None):
+         if path is None:
+             fobj = tempfile.NamedTemporaryFile(delete=False)
+         else:
+             # `path` could have been a string or file object or something
+             # similar. In any event, the decorator has given us a file object
+             # and it will close it for us, if it should.
+             fobj = path
+
+         try:
+             fobj.write("blah")
+         finally:
+             if path is None:
+                 fobj.close()
+
+    Normally, we'd want to use "with" to ensure that fobj gets closed.
+    However, the decorator will make `path` a file object for us,
+    and using "with" would undesirably close that file object.
+    Instead, we use a try block, as shown above.
+    When we exit the function, fobj will be closed, if it should be, by the decorator.
+    """
+
+    def _open_file(path):
+        # Now we have the path_arg. There are two types of input to consider:
+        #   1) string representing a path that should be opened
+        #   2) an already opened file object
+        if isinstance(path, str):
+            ext = splitext(path)[1]
+        elif isinstance(path, Path):
+            # path is a pathlib reference to a filename
+            ext = path.suffix
+            path = str(path)
+        else:
+            # could be None, or a file handle, in which case the algorithm will deal with it
+            return path, lambda: None
+
+        fobj = _dispatch_dict[ext](path, mode=mode)
+        return fobj, lambda: fobj.close()
+
+    return argmap(_open_file, path_arg, try_finally=True)
+
+
+class argmap:
+    """A decorator to apply a map to arguments before calling the function
+
+    This class provides a decorator that maps (transforms) arguments of the function
+    before the function is called. Thus for example, we have similar code
+    in many functions to determine whether an argument is the number of nodes
+    to be created, or a list of nodes to be handled. The decorator provides
+    the code to accept either -- transforming the indicated argument into a
+    list of nodes before the actual function is called.
+
+    This decorator class allows us to process single or multiple arguments.
+    The arguments to be processed can be specified by string, naming the argument,
+    or by index, specifying the item in the args list.
+
+    Parameters
+    ----------
+    func : callable
+        The function to apply to arguments
+
+    *args : iterable of (int, str or tuple)
+        A list of parameters, specified either as strings (their names), ints
+        (numerical indices) or tuples, which may contain ints, strings, and
+        (recursively) tuples. Each indicates which parameters the decorator
+        should map. Tuples indicate that the map function takes (and returns)
+        multiple parameters in the same order and nested structure as indicated
+        here.
+
+    try_finally : bool (default: False)
+        When True, wrap the function call in a try-finally block with code
+        for the finally block created by `func`. This is used when the map
+        function constructs an object (like a file handle) that requires
+        post-processing (like closing).
+
+    Examples
+    --------
+    Most of these examples use `@argmap(...)` to apply the decorator to
+    the function defined on the next line.
+    In the EasyGraph codebase however, `argmap` is used within a function to
+    construct a decorator. That is, the decorator defines a mapping function
+    and then uses `argmap` to build and return a decorated function.
+    A simple example is a decorator that specifies which currency to report money.
+    The decorator (named `convert_to`) would be used like::
+
+        @convert_to("US_Dollars", "income")
+        def show_me_the_money(name, income):
+            print(f"{name} : {income}")
+
+    And the code to create the decorator might be::
+
+        def convert_to(currency, which_arg):
+            def _convert(amount):
+                if amount.currency != currency:
+                    amount = amount.to_currency(currency)
+                return amount
+            return argmap(_convert, which_arg)
+
+    Despite this common idiom for argmap, most of the following examples
+    use the `@argmap(...)` idiom to save space.
+
+    Here's an example use of argmap to sum the elements of two of the functions
+    arguments. The decorated function::
+
+        @argmap(sum, "xlist", "zlist")
+        def foo(xlist, y, zlist):
+            return xlist - y + zlist
+
+    is syntactic sugar for::
+
+        def foo(xlist, y, zlist):
+            x = sum(xlist)
+            z = sum(zlist)
+            return x - y + z
+
+    and is equivalent to (using argument indexes)::
+
+        @argmap(sum, "xlist", 2)
+        def foo(xlist, y, zlist):
+            return xlist - y + zlist
+
+    or::
+
+        @argmap(sum, "zlist", 0)
+        def foo(xlist, y, zlist):
+            return xlist - y + zlist
+
+    Transforming functions can be applied to multiple arguments, such as::
+
+        def swap(x, y):
+            return y, x
+
+        # the 2-tuple tells argmap that the map `swap` has 2 inputs/outputs.
+        @argmap(swap, ("a", "b")):
+        def foo(a, b, c):
+            return a / b * c
+
+    is equivalent to::
+
+        def foo(a, b, c):
+            a, b = swap(a, b)
+            return a / b * c
+
+    More generally, the applied arguments can be nested tuples of strings or ints.
+    The syntax `@argmap(some_func, ("a", ("b", "c")))` would expect `some_func` to
+    accept 2 inputs with the second expected to be a 2-tuple. It should then return
+    2 outputs with the second a 2-tuple. The returns values would replace input "a"
+    "b" and "c" respectively. Similarly for `@argmap(some_func, (0, ("b", 2)))`.
+
+    Also, note that an index larger than the number of named parameters is allowed
+    for variadic functions. For example::
+
+        def double(a):
+            return 2 * a
+
+        @argmap(double, 3)
+        def overflow(a, *args):
+            return a, args
+
+        print(overflow(1, 2, 3, 4, 5, 6))  # output is 1, (2, 3, 8, 5, 6)
+
+    **Try Finally**
+
+    Additionally, this `argmap` class can be used to create a decorator that
+    initiates a try...finally block. The decorator must be written to return
+    both the transformed argument and a closing function.
+    This feature was included to enable the `open_file` decorator which might
+    need to close the file or not depending on whether it had to open that file.
+    This feature uses the keyword-only `try_finally` argument to `@argmap`.
+
+    For example this map opens a file and then makes sure it is closed::
+
+        def open_file(fn):
+            f = open(fn)
+            return f, lambda: f.close()
+
+    The decorator applies that to the function `foo`::
+
+        @argmap(open_file, "file", try_finally=True)
+        def foo(file):
+            print(file.read())
+
+    is syntactic sugar for::
+
+        def foo(file):
+            file, close_file = open_file(file)
+            try:
+                print(file.read())
+            finally:
+                close_file()
+
+    and is equivalent to (using indexes)::
+
+        @argmap(open_file, 0, try_finally=True)
+        def foo(file):
+            print(file.read())
+
+    Here's an example of the try_finally feature used to create a decorator::
+
+        def my_closing_decorator(which_arg):
+            def _opener(path):
+                if path is None:
+                    path = open(path)
+                    fclose = path.close
+                else:
+                    # assume `path` handles the closing
+                    fclose = lambda: None
+                return path, fclose
+            return argmap(_opener, which_arg, try_finally=True)
+
+    which can then be used as::
+
+        @my_closing_decorator("file")
+        def fancy_reader(file=None):
+            # this code doesn't need to worry about closing the file
+            print(file.read())
+
+    Notes
+    -----
+    An object of this class is callable and intended to be used when
+    defining a decorator. Generally, a decorator takes a function as input
+    and constructs a function as output. Specifically, an `argmap` object
+    returns the input function decorated/wrapped so that specified arguments
+    are mapped (transformed) to new values before the decorated function is called.
+
+    As an overview, the argmap object returns a new function with all the
+    dunder values of the original function (like `__doc__`, `__name__`, etc).
+    Code for this decorated function is built based on the original function's
+    signature. It starts by mapping the input arguments to potentially new
+    values. Then it calls the decorated function with these new values in place
+    of the indicated arguments that have been mapped. The return value of the
+    original function is then returned. This new function is the function that
+    is actually called by the user.
+
+    Three additional features are provided.
+        1) The code is lazily compiled. That is, the new function is returned
+        as an object without the code compiled, but with all information
+        needed so it can be compiled upon it's first invocation. This saves
+        time on import at the cost of additional time on the first call of
+        the function. Subsequent calls are then just as fast as normal.
+
+        2) If the "try_finally" keyword-only argument is True, a try block
+        follows each mapped argument, matched on the other side of the wrapped
+        call, by a finally block closing that mapping.  We expect func to return
+        a 2-tuple: the mapped value and a function to be called in the finally
+        clause.  This feature was included so the `open_file` decorator could
+        provide a file handle to the decorated function and close the file handle
+        after the function call. It even keeps track of whether to close the file
+        handle or not based on whether it had to open the file or the input was
+        already open. So, the decorated function does not need to include any
+        code to open or close files.
+
+        3) The maps applied can process multiple arguments. For example,
+        you could swap two arguments using a mapping, or transform
+        them to their sum and their difference. This was included to allow
+        a decorator in the `quality.py` module that checks that an input
+        `partition` is a valid partition of the nodes of the input graph `G`.
+        In this example, the map has inputs `(G, partition)`. After checking
+        for a valid partition, the map either raises an exception or leaves
+        the inputs unchanged. Thus many functions that make this check can
+        use the decorator rather than copy the checking code into each function.
+        More complicated nested argument structures are described below.
+
+    The remaining notes describe the code structure and methods for this
+    class in broad terms to aid in understanding how to use it.
+
+    Instantiating an `argmap` object simply stores the mapping function and
+    the input identifiers of which arguments to map. The resulting decorator
+    is ready to use this map to decorate any function. Calling that object
+    (`argmap.__call__`, but usually done via `@my_decorator`) a lazily
+    compiled thin wrapper of the decorated function is constructed,
+    wrapped with the necessary function dunder attributes like `__doc__`
+    and `__name__`. That thinly wrapped function is returned as the
+    decorated function. When that decorated function is called, the thin
+    wrapper of code calls `argmap._lazy_compile` which compiles the decorated
+    function (using `argmap.compile`) and replaces the code of the thin
+    wrapper with the newly compiled code. This saves the compilation step
+    every import of easygraph, at the cost of compiling upon the first call
+    to the decorated function.
+
+    When the decorated function is compiled, the code is recursively assembled
+    using the `argmap.assemble` method. The recursive nature is needed in
+    case of nested decorators. The result of the assembly is a number of
+    useful objects.
+
+      sig : the function signature of the original decorated function as
+          constructed by :func:`argmap.signature`. This is constructed
+          using `inspect.signature` but enhanced with attribute
+          strings `sig_def` and `sig_call`, and other information
+          specific to mapping arguments of this function.
+          This information is used to construct a string of code defining
+          the new decorated function.
+
+      wrapped_name : a unique internally used name constructed by argmap
+          for the decorated function.
+
+      functions : a dict of the functions used inside the code of this
+          decorated function, to be used as `globals` in `exec`.
+          This dict is recursively updated to allow for nested decorating.
+
+      mapblock : code (as a list of strings) to map the incoming argument
+          values to their mapped values.
+
+      finallys : code (as a list of strings) to provide the possibly nested
+          set of finally clauses if needed.
+
+      mutable_args : a bool indicating whether the `sig.args` tuple should be
+          converted to a list so mutation can occur.
+
+    After this recursive assembly process, the `argmap.compile` method
+    constructs code (as strings) to convert the tuple `sig.args` to a list
+    if needed. It joins the defining code with appropriate indents and
+    compiles the result.  Finally, this code is evaluated and the original
+    wrapper's implementation is replaced with the compiled version (see
+    `argmap._lazy_compile` for more details).
+
+    Other `argmap` methods include `_name` and `_count` which allow internally
+    generated names to be unique within a python session.
+    The methods `_flatten` and `_indent` process the nested lists of strings
+    into properly indented python code ready to be compiled.
+
+    More complicated nested tuples of arguments also allowed though
+    usually not used. For the simple 2 argument case, the argmap
+    input ("a", "b") implies the mapping function will take 2 arguments
+    and return a 2-tuple of mapped values. A more complicated example
+    with argmap input `("a", ("b", "c"))` requires the mapping function
+    take 2 inputs, with the second being a 2-tuple. It then must output
+    the 3 mapped values in the same nested structure `(newa, (newb, newc))`.
+    This level of generality is not often needed, but was convenient
+    to implement when handling the multiple arguments.
+
+    See Also
+    --------
+    not_implemented_for
+    open_file
+    nodes_or_number
+    random_state
+    py_random_state
+    easygraph.community.quality.require_partition
+    require_partition
+
+    """
+
+    def __init__(self, func, *args, try_finally=False):
+        self._func = func
+        self._args = args
+        self._finally = try_finally
+
+    @staticmethod
+    def _lazy_compile(func):
+        """Compile the source of a wrapped function
+
+        Assemble and compile the decorated function, and intrusively replace its
+        code with the compiled version's.  The thinly wrapped function becomes
+        the decorated function.
+
+        Parameters
+        ----------
+        func : callable
+            A function returned by argmap.__call__ which is in the process
+            of being called for the first time.
+
+        Returns
+        -------
+        func : callable
+            The same function, with a new __code__ object.
+
+        Notes
+        -----
+        It was observed in easygraph issue #4732 [1] that the import time of
+        easygraph was significantly bloated by the use of decorators: over half
+        of the import time was being spent decorating functions.  This was
+        somewhat improved by a change made to the `decorator` library, at the
+        cost of a relatively heavy-weight call to `inspect.Signature.bind`
+        for each call to the decorated function.
+
+        The workaround we arrived at is to do minimal work at the time of
+        decoration.  When the decorated function is called for the first time,
+        we compile a function with the same function signature as the wrapped
+        function.  The resulting decorated function is faster than one made by
+        the `decorator` library, so that the overhead of the first call is
+        'paid off' after a small number of calls.
+        """
+        real_func = func.__argmap__.compile(func.__wrapped__)
+        func.__code__ = real_func.__code__
+        func.__globals__.update(real_func.__globals__)
+        func.__dict__.update(real_func.__dict__)
+        return func
+
+    def __call__(self, f):
+        """Construct a lazily decorated wrapper of f.
+
+        The decorated function will be compiled when it is called for the first time,
+        and it will replace its own __code__ object so subsequent calls are fast.
+
+        Parameters
+        ----------
+        f : callable
+            A function to be decorated.
+
+        Returns
+        -------
+        func : callable
+            The decorated function.
+
+        See Also
+        --------
+        argmap._lazy_compile
+        """
+
+        if inspect.isgeneratorfunction(f):
+
+            def func(*args, __wrapper=None, **kwargs):
+                yield from argmap._lazy_compile(__wrapper)(*args, **kwargs)
+
+        else:
+
+            def func(*args, __wrapper=None, **kwargs):
+                return argmap._lazy_compile(__wrapper)(*args, **kwargs)
+
+        # standard function-wrapping stuff
+        func.__name__ = f.__name__
+        func.__doc__ = f.__doc__
+        func.__defaults__ = f.__defaults__
+        func.__kwdefaults__.update(f.__kwdefaults__ or {})
+        func.__module__ = f.__module__
+        func.__qualname__ = f.__qualname__
+        func.__dict__.update(f.__dict__)
+        func.__wrapped__ = f
+
+        # now that we've wrapped f, we may have picked up some __dict__ or
+        # __kwdefaults__ items that were set by a previous argmap.  Thus, we set
+        # these values after those update() calls.
+
+        # If we attempt to access func from within itself, that happens through
+        # a closure -- which trips an error when we replace func.__code__.  The
+        # standard workaround for functions which can't see themselves is to use
+        # a Y-combinator, as we do here.
+        func.__kwdefaults__["_argmap__wrapper"] = func
+
+        # this self-reference is here because functools.wraps preserves
+        # everything in __dict__, and we don't want to mistake a non-argmap
+        # wrapper for an argmap wrapper
+        func.__self__ = func
+
+        # this is used to variously call self.assemble and self.compile
+        func.__argmap__ = self
+
+        return func
+
+    __count = 0
+
+    @classmethod
+    def _count(cls):
+        """Maintain a globally-unique identifier for function names and "file" names
+
+        Note that this counter is a class method reporting a class variable
+        so the count is unique within a Python session. It could differ from
+        session to session for a specific decorator depending on the order
+        that the decorators are created. But that doesn't disrupt `argmap`.
+
+        This is used in two places: to construct unique variable names
+        in the `_name` method and to construct unique fictitious filenames
+        in the `_compile` method.
+
+        Returns
+        -------
+        count : int
+            An integer unique to this Python session (simply counts from zero)
+        """
+        cls.__count += 1
+        return cls.__count
+
+    _bad_chars = re.compile("[^a-zA-Z0-9_]")
+
+    @classmethod
+    def _name(cls, f):
+        """Mangle the name of a function to be unique but somewhat human-readable
+
+        The names are unique within a Python session and set using `_count`.
+
+        Parameters
+        ----------
+        f : str or object
+
+        Returns
+        -------
+        name : str
+            The mangled version of `f.__name__` (if `f.__name__` exists) or `f`
+
+        """
+        f = f.__name__ if hasattr(f, "__name__") else f
+        fname = re.sub(cls._bad_chars, "_", f)
+        return f"argmap_{fname}_{cls._count()}"
+
+    def compile(self, f):
+        """Compile the decorated function.
+
+        Called once for a given decorated function -- collects the code from all
+        argmap decorators in the stack, and compiles the decorated function.
+
+        Much of the work done here uses the `assemble` method to allow recursive
+        treatment of multiple argmap decorators on a single decorated function.
+        That flattens the argmap decorators, collects the source code to construct
+        a single decorated function, then compiles/executes/returns that function.
+
+        The source code for the decorated function is stored as an attribute
+        `_code` on the function object itself.
+
+        Note that Python's `compile` function requires a filename, but this
+        code is constructed without a file, so a fictitious filename is used
+        to describe where the function comes from. The name is something like:
+        "argmap compilation 4".
+
+        Parameters
+        ----------
+        f : callable
+            The function to be decorated
+
+        Returns
+        -------
+        func : callable
+            The decorated file
+
+        """
+        sig, wrapped_name, functions, mapblock, finallys, mutable_args = self.assemble(
+            f
+        )
+
+        call = f"{sig.call_sig.format(wrapped_name)}#"
+        mut_args = f"{sig.args} = list({sig.args})" if mutable_args else ""
+        body = argmap._indent(sig.def_sig, mut_args, mapblock, call, finallys)
+        code = "\n".join(body)
+
+        locl = {}
+        globl = dict(functions.values())
+        filename = f"{self.__class__} compilation {self._count()}"
+        compiled = compile(code, filename, "exec")
+        exec(compiled, globl, locl)
+        func = locl[sig.name]
+        func._code = code
+        return func
+
+    def assemble(self, f):
+        """Collects components of the source for the decorated function wrapping f.
+
+        If `f` has multiple argmap decorators, we recursively assemble the stack of
+        decorators into a single flattened function.
+
+        This method is part of the `compile` method's process yet separated
+        from that method to allow recursive processing. The outputs are
+        strings, dictionaries and lists that collect needed info to
+        flatten any nested argmap-decoration.
+
+        Parameters
+        ----------
+        f : callable
+            The function to be decorated.  If f is argmapped, we assemble it.
+
+        Returns
+        -------
+        sig : argmap.Signature
+            The function signature as an `argmap.Signature` object.
+        wrapped_name : str
+            The mangled name used to represent the wrapped function in the code
+            being assembled.
+        functions : dict
+            A dictionary mapping id(g) -> (mangled_name(g), g) for functions g
+            referred to in the code being assembled. These need to be present
+            in the ``globals`` scope of ``exec`` when defining the decorated
+            function.
+        mapblock : list of lists and/or strings
+            Code that implements mapping of parameters including any try blocks
+            if needed. This code will precede the decorated function call.
+        finallys : list of lists and/or strings
+            Code that implements the finally blocks to post-process the
+            arguments (usually close any files if needed) after the
+            decorated function is called.
+        mutable_args : bool
+            True if the decorator needs to modify positional arguments
+            via their indices. The compile method then turns the argument
+            tuple into a list so that the arguments can be modified.
+        """
+
+        # first, we check if f is already argmapped -- if that's the case,
+        # build up the function recursively.
+        # > mapblock is generally a list of function calls of the sort
+        #     arg = func(arg)
+        # in addition to some try-blocks if needed.
+        # > finallys is a recursive list of finally blocks of the sort
+        #         finally:
+        #             close_func_1()
+        #     finally:
+        #         close_func_2()
+        # > functions is a dict of functions used in the scope of our decorated
+        # function. It will be used to construct globals used in compilation.
+        # We make functions[id(f)] = name_of_f, f to ensure that a given
+        # function is stored and named exactly once even if called by
+        # nested decorators.
+        if hasattr(f, "__argmap__") and f.__self__ is f:
+            (
+                sig,
+                wrapped_name,
+                functions,
+                mapblock,
+                finallys,
+                mutable_args,
+            ) = f.__argmap__.assemble(f.__wrapped__)
+            functions = dict(functions)  # shallow-copy just in case
+        else:
+            sig = self.signature(f)
+            wrapped_name = self._name(f)
+            mapblock, finallys = [], []
+            functions = {id(f): (wrapped_name, f)}
+            mutable_args = False
+
+        if id(self._func) in functions:
+            fname, _ = functions[id(self._func)]
+        else:
+            fname, _ = functions[id(self._func)] = self._name(self._func), self._func
+
+        # this is a bit complicated -- we can call functions with a variety of
+        # nested arguments, so long as their input and output are tuples with
+        # the same nested structure. e.g. ("a", "b") maps arguments a and b.
+        # A more complicated nesting like (0, (3, 4)) maps arguments 0, 3, 4
+        # expecting the mapping to output new values in the same nested shape.
+        # while we're not taking full advantage of the ability to handle
+        # multiply-nested tuples, it was convenient to implement this in
+        # generality because the recursive call to `get_name` is necessary in
+        # any case.
+        applied = set()
+
+        def get_name(arg, first=True):
+            nonlocal mutable_args
+            if isinstance(arg, tuple):
+                name = ", ".join(get_name(x, False) for x in arg)
+                return name if first else f"({name})"
+            if arg in applied:
+                raise EasyGraphError(f"argument {arg} is specified multiple times")
+            applied.add(arg)
+            if arg in sig.names:
+                return sig.names[arg]
+            elif isinstance(arg, str):
+                if sig.kwargs is None:
+                    raise EasyGraphError(
+                        f"name {arg} is not a named parameter and this function doesn't"
+                        " have kwargs"
+                    )
+                return f"{sig.kwargs}[{arg!r}]"
+            else:
+                if sig.args is None:
+                    raise EasyGraphError(
+                        f"index {arg} not a parameter index and this function doesn't"
+                        " have args"
+                    )
+                mutable_args = True
+                return f"{sig.args}[{arg - sig.n_positional}]"
+
+        if self._finally:
+            # here's where we handle try_finally decorators.  Such a decorator
+            # returns a mapped argument and a function to be called in a
+            # finally block.  This feature was required by the open_file
+            # decorator.  The below generates the code
+            #
+            # name, final = func(name)                   #<--append to mapblock
+            # try:                                       #<--append to mapblock
+            #     ... more argmapping and try blocks
+            #     return WRAPPED_FUNCTION(...)
+            #     ... more finally blocks
+            # finally:                                   #<--prepend to finallys
+            #     final()                                #<--prepend to finallys
+            #
+            for a in self._args:
+                name = get_name(a)
+                final = self._name(name)
+                mapblock.append(f"{name}, {final} = {fname}({name})")
+                mapblock.append("try:")
+                finallys = ["finally:", f"{final}()#", "#", finallys]
+        else:
+            mapblock.extend(
+                f"{name} = {fname}({name})" for name in map(get_name, self._args)
+            )
+
+        return sig, wrapped_name, functions, mapblock, finallys, mutable_args
+
+    @classmethod
+    def signature(cls, f):
+        r"""Construct a Signature object describing `f`
+
+        Compute a Signature so that we can write a function wrapping f with
+        the same signature and call-type.
+
+        Parameters
+        ----------
+        f : callable
+            A function to be decorated
+
+        Returns
+        -------
+        sig : argmap.Signature
+            The Signature of f
+
+        Notes
+        -----
+        The Signature is a namedtuple with names:
+
+            name : a unique version of the name of the decorated function
+            signature : the inspect.signature of the decorated function
+            def_sig : a string used as code to define the new function
+            call_sig : a string used as code to call the decorated function
+            names : a dict keyed by argument name and index to the argument's name
+            n_positional : the number of positional arguments in the signature
+            args : the name of the VAR_POSITIONAL argument if any, i.e. \*theseargs
+            kwargs : the name of the VAR_KEYWORDS argument if any, i.e. \*\*kwargs
+
+        These named attributes of the signature are used in `assemble` and `compile`
+        to construct a string of source code for the decorated function.
+
+        """
+        sig = inspect.signature(f, follow_wrapped=False)
+        def_sig = []
+        call_sig = []
+        names = {}
+
+        kind = None
+        args = None
+        kwargs = None
+        npos = 0
+        for i, param in enumerate(sig.parameters.values()):
+            # parameters can be position-only, keyword-or-position, keyword-only
+            # in any combination, but only in the order as above.  we do edge
+            # detection to add the appropriate punctuation
+            prev = kind
+            kind = param.kind
+            if prev == param.POSITIONAL_ONLY != kind:
+                # the last token was position-only, but this one isn't
+                def_sig.append("/")
+            if prev != param.KEYWORD_ONLY == kind != param.VAR_POSITIONAL:
+                # param is the first keyword-only arg and isn't starred
+                def_sig.append("*")
+
+            # star arguments as appropriate
+            if kind == param.VAR_POSITIONAL:
+                name = "*" + param.name
+                args = param.name
+                count = 0
+            elif kind == param.VAR_KEYWORD:
+                name = "**" + param.name
+                kwargs = param.name
+                count = 0
+            else:
+                names[i] = names[param.name] = param.name
+                name = param.name
+                count = 1
+
+            # assign to keyword-only args in the function call
+            if kind == param.KEYWORD_ONLY:
+                call_sig.append(f"{name} = {name}")
+            else:
+                npos += count
+                call_sig.append(name)
+
+            def_sig.append(name)
+
+        fname = cls._name(f)
+        def_sig = f'def {fname}({", ".join(def_sig)}):'
+
+        if inspect.isgeneratorfunction(f):
+            _return = "yield from"
+        else:
+            _return = "return"
+
+        call_sig = f"{_return} {{}}({', '.join(call_sig)})"
+
+        return cls.Signature(fname, sig, def_sig, call_sig, names, npos, args, kwargs)
+
+    Signature = collections.namedtuple(
+        "Signature",
+        [
+            "name",
+            "signature",
+            "def_sig",
+            "call_sig",
+            "names",
+            "n_positional",
+            "args",
+            "kwargs",
+        ],
+    )
+
+    @staticmethod
+    def _flatten(nestlist, visited):
+        """flattens a recursive list of lists that doesn't have cyclic references
+
+        Parameters
+        ----------
+        nestlist : iterable
+            A recursive list of objects to be flattened into a single iterable
+
+        visited : set
+            A set of object ids which have been walked -- initialize with an
+            empty set
+
+        Yields
+        ------
+        Non-list objects contained in nestlist
+
+        """
+        for thing in nestlist:
+            if isinstance(thing, list):
+                if id(thing) in visited:
+                    raise ValueError("A cycle was found in nestlist.  Be a tree.")
+                else:
+                    visited.add(id(thing))
+                yield from argmap._flatten(thing, visited)
+            else:
+                yield thing
+
+    _tabs = " " * 64
+
+    @staticmethod
+    def _indent(*lines):
+        """Indent list of code lines to make executable Python code
+
+        Indents a tree-recursive list of strings, following the rule that one
+        space is added to the tab after a line that ends in a colon, and one is
+        removed after a line that ends in an hashmark.
+
+        Parameters
+        ----------
+        *lines : lists and/or strings
+            A recursive list of strings to be assembled into properly indented
+            code.
+
+        Returns
+        -------
+        code : str
+
+        Examples
+        --------
+
+            argmap._indent(*["try:", "try:", "pass#", "finally:", "pass#", "#",
+                             "finally:", "pass#"])
+
+        renders to
+
+            '''try:
+             try:
+              pass#
+             finally:
+              pass#
+             #
+            finally:
+             pass#'''
+        """
+        depth = 0
+        for line in argmap._flatten(lines, set()):
+            yield f"{argmap._tabs[:depth]}{line}"
+            depth += (line[-1:] == ":") - (line[-1:] == "#")
+
+
+def nodes_or_number(which_args):
+    """Decorator to allow number of nodes or container of nodes.
+
+    With this decorator, the specified argument can be either a number or a container
+    of nodes. If it is a number, the nodes used are `range(n)`.
+    This allows `eg.complete_graph(50)` in place of `eg.complete_graph(list(range(50)))`.
+    And it also allows `eg.complete_graph(any_list_of_nodes)`.
+
+    Parameters
+    ----------
+    which_args : string or int or sequence of strings or ints
+        If string, the name of the argument to be treated.
+        If int, the index of the argument to be treated.
+        If more than one node argument is allowed, can be a list of locations.
+
+    Returns
+    -------
+    _nodes_or_numbers : function
+        Function which replaces int args with ranges.
+
+    Examples
+    --------
+    Decorate functions like this::
+
+       @nodes_or_number("nodes")
+       def empty_graph(nodes):
+           # nodes is converted to a list of nodes
+
+       @nodes_or_number(0)
+       def empty_graph(nodes):
+           # nodes is converted to a list of nodes
+
+       @nodes_or_number(["m1", "m2"])
+       def grid_2d_graph(m1, m2, periodic=False):
+           # m1 and m2 are each converted to a list of nodes
+
+       @nodes_or_number([0, 1])
+       def grid_2d_graph(m1, m2, periodic=False):
+           # m1 and m2 are each converted to a list of nodes
+
+       @nodes_or_number(1)
+       def full_rary_tree(r, n)
+           # presumably r is a number. It is not handled by this decorator.
+           # n is converted to a list of nodes
+    """
+
+    def _nodes_or_number(n):
+        try:
+            nodes = list(range(n))
+        except TypeError:
+            nodes = tuple(n)
+        else:
+            if n < 0:
+                msg = "Negative number of nodes not valid: {n}"
+                raise EasyGraphError(msg)
+        return (n, nodes)
+
+    try:
+        iter_wa = iter(which_args)
+    except TypeError:
+        iter_wa = (which_args,)
+
+    return argmap(_nodes_or_number, *iter_wa)
```

## easygraph/utils/alias.py

 * *Ordering differences only*

```diff
@@ -1,119 +1,119 @@
-__all__ = ["create_alias_table", "alias_sample", "alias_setup", "alias_draw"]
-
-
-def create_alias_table(area_ratio):
-    """
-    Parameters
-    ---------
-    area_ratio :
-        sum(area_ratio)=1
-
-    Returns
-    ----------
-    1. accept
-    2. alias
-
-    """
-    import numpy as np
-
-    l = len(area_ratio)
-    accept, alias = [0] * l, [0] * l
-    small, large = [], []
-    area_ratio_ = np.array(area_ratio) * l
-    for i, prob in enumerate(area_ratio_):
-        if prob < 1.0:
-            small.append(i)
-        else:
-            large.append(i)
-
-    while small and large:
-        small_idx, large_idx = small.pop(), large.pop()
-        accept[small_idx] = area_ratio_[small_idx]
-        alias[small_idx] = large_idx
-        area_ratio_[large_idx] = area_ratio_[large_idx] - (1 - area_ratio_[small_idx])
-        if area_ratio_[large_idx] < 1.0:
-            small.append(large_idx)
-        else:
-            large.append(large_idx)
-
-    while large:
-        large_idx = large.pop()
-        accept[large_idx] = 1
-    while small:
-        small_idx = small.pop()
-        accept[small_idx] = 1
-
-    return accept, alias
-
-
-def alias_sample(accept, alias):
-    """
-    Parameters
-    ----------
-    accept :
-
-    alias :
-
-    Returns
-    ----------
-    sample index
-    """
-    import numpy as np
-
-    N = len(accept)
-    i = int(np.random.random() * N)
-    r = np.random.random()
-    if r < accept[i]:
-        return i
-    else:
-        return alias[i]
-
-
-def alias_draw(J, q):
-    import numpy as np
-
-    """
-    Draw sample from a non-uniform discrete distribution using alias sampling.
-    """
-    K = len(J)
-
-    kk = int(np.floor(np.random.rand() * K))
-    if np.random.rand() < q[kk]:
-        return kk
-    else:
-        return J[kk]
-
-
-def alias_setup(probs):
-    import numpy as np
-
-    """
-    Compute utility lists for non-uniform sampling from discrete distributions.
-    Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/
-    for details
-    """
-    K = len(probs)
-    q = np.zeros(K)
-    J = np.zeros(K, dtype=int)
-
-    smaller = []
-    larger = []
-    for kk, prob in enumerate(probs):
-        q[kk] = K * prob
-        if q[kk] < 1.0:
-            smaller.append(kk)
-        else:
-            larger.append(kk)
-
-    while len(smaller) > 0 and len(larger) > 0:
-        small = smaller.pop()
-        large = larger.pop()
-
-        J[small] = large
-        q[large] = q[large] + q[small] - 1.0
-        if q[large] < 1.0:
-            smaller.append(large)
-        else:
-            larger.append(large)
-
-    return J, q
+__all__ = ["create_alias_table", "alias_sample", "alias_setup", "alias_draw"]
+
+
+def create_alias_table(area_ratio):
+    """
+    Parameters
+    ---------
+    area_ratio :
+        sum(area_ratio)=1
+
+    Returns
+    ----------
+    1. accept
+    2. alias
+
+    """
+    import numpy as np
+
+    l = len(area_ratio)
+    accept, alias = [0] * l, [0] * l
+    small, large = [], []
+    area_ratio_ = np.array(area_ratio) * l
+    for i, prob in enumerate(area_ratio_):
+        if prob < 1.0:
+            small.append(i)
+        else:
+            large.append(i)
+
+    while small and large:
+        small_idx, large_idx = small.pop(), large.pop()
+        accept[small_idx] = area_ratio_[small_idx]
+        alias[small_idx] = large_idx
+        area_ratio_[large_idx] = area_ratio_[large_idx] - (1 - area_ratio_[small_idx])
+        if area_ratio_[large_idx] < 1.0:
+            small.append(large_idx)
+        else:
+            large.append(large_idx)
+
+    while large:
+        large_idx = large.pop()
+        accept[large_idx] = 1
+    while small:
+        small_idx = small.pop()
+        accept[small_idx] = 1
+
+    return accept, alias
+
+
+def alias_sample(accept, alias):
+    """
+    Parameters
+    ----------
+    accept :
+
+    alias :
+
+    Returns
+    ----------
+    sample index
+    """
+    import numpy as np
+
+    N = len(accept)
+    i = int(np.random.random() * N)
+    r = np.random.random()
+    if r < accept[i]:
+        return i
+    else:
+        return alias[i]
+
+
+def alias_draw(J, q):
+    import numpy as np
+
+    """
+    Draw sample from a non-uniform discrete distribution using alias sampling.
+    """
+    K = len(J)
+
+    kk = int(np.floor(np.random.rand() * K))
+    if np.random.rand() < q[kk]:
+        return kk
+    else:
+        return J[kk]
+
+
+def alias_setup(probs):
+    import numpy as np
+
+    """
+    Compute utility lists for non-uniform sampling from discrete distributions.
+    Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/
+    for details
+    """
+    K = len(probs)
+    q = np.zeros(K)
+    J = np.zeros(K, dtype=int)
+
+    smaller = []
+    larger = []
+    for kk, prob in enumerate(probs):
+        q[kk] = K * prob
+        if q[kk] < 1.0:
+            smaller.append(kk)
+        else:
+            larger.append(kk)
+
+    while len(smaller) > 0 and len(larger) > 0:
+        small = smaller.pop()
+        large = larger.pop()
+
+        J[small] = large
+        q[large] = q[large] + q[small] - 1.0
+        if q[large] < 1.0:
+            smaller.append(large)
+        else:
+            larger.append(large)
+
+    return J, q
```

## easygraph/utils/sparse.py

 * *Ordering differences only*

```diff
@@ -1,39 +1,39 @@
-__all__ = ["sparse_dropout"]
-
-
-# if not type checking
-from typing import TYPE_CHECKING
-
-
-if TYPE_CHECKING:
-    import torch
-
-
-def sparse_dropout(
-    sp_mat: "torch.Tensor", p: float, fill_value: float = 0.0
-) -> "torch.Tensor":
-    import torch
-
-    r"""Dropout function for sparse matrix. This function will return a new sparse matrix with the same shape as the input sparse matrix, but with some elements dropped out.
-
-    Args:
-        ``sp_mat`` (``torch.Tensor``): The sparse matrix with format ``torch.sparse_coo_tensor``.
-        ``p`` (``float``): Probability of an element to be dropped.
-        ``fill_value`` (``float``): The fill value for dropped elements. Defaults to ``0.0``.
-    """
-    device = sp_mat.device
-    sp_mat = sp_mat.coalesce()
-    assert 0 <= p <= 1
-    if p == 0:
-        return sp_mat
-    p = torch.ones(sp_mat._nnz(), device=device) * p
-    keep_mask = torch.bernoulli(1 - p).to(device)
-    fill_values = torch.logical_not(keep_mask) * fill_value
-    new_sp_mat = torch.sparse_coo_tensor(
-        sp_mat._indices(),
-        sp_mat._values() * keep_mask + fill_values,
-        size=sp_mat.size(),
-        device=sp_mat.device,
-        dtype=sp_mat.dtype,
-    )
-    return new_sp_mat
+__all__ = ["sparse_dropout"]
+
+
+# if not type checking
+from typing import TYPE_CHECKING
+
+
+if TYPE_CHECKING:
+    import torch
+
+
+def sparse_dropout(
+    sp_mat: "torch.Tensor", p: float, fill_value: float = 0.0
+) -> "torch.Tensor":
+    import torch
+
+    r"""Dropout function for sparse matrix. This function will return a new sparse matrix with the same shape as the input sparse matrix, but with some elements dropped out.
+
+    Args:
+        ``sp_mat`` (``torch.Tensor``): The sparse matrix with format ``torch.sparse_coo_tensor``.
+        ``p`` (``float``): Probability of an element to be dropped.
+        ``fill_value`` (``float``): The fill value for dropped elements. Defaults to ``0.0``.
+    """
+    device = sp_mat.device
+    sp_mat = sp_mat.coalesce()
+    assert 0 <= p <= 1
+    if p == 0:
+        return sp_mat
+    p = torch.ones(sp_mat._nnz(), device=device) * p
+    keep_mask = torch.bernoulli(1 - p).to(device)
+    fill_values = torch.logical_not(keep_mask) * fill_value
+    new_sp_mat = torch.sparse_coo_tensor(
+        sp_mat._indices(),
+        sp_mat._values() * keep_mask + fill_values,
+        size=sp_mat.size(),
+        device=sp_mat.device,
+        dtype=sp_mat.dtype,
+    )
+    return new_sp_mat
```

## easygraph/utils/type_change.py

 * *Ordering differences only*

```diff
@@ -1,147 +1,147 @@
-import easygraph as eg
-
-
-__all__ = [
-    "from_pyGraphviz_agraph",
-    "to_pyGraphviz_agraph",
-]
-
-
-def from_pyGraphviz_agraph(A, create_using=None):
-    """Returns a EasyGraph Graph or DiGraph from a PyGraphviz graph.
-
-    Parameters
-    ----------
-    A : PyGraphviz AGraph
-      A graph created with PyGraphviz
-
-    create_using : EasyGraph graph constructor, optional (default=None)
-       Graph type to create. If graph instance, then cleared before populated.
-       If `None`, then the appropriate Graph type is inferred from `A`.
-
-    Examples
-    --------
-    >>> K5 = eg.complete_graph(5)
-    >>> A = eg.to_pyGraphviz_agraph(K5)
-    >>> G = eg.from_pyGraphviz_agraph(A)
-
-    Notes
-    -----
-    The Graph G will have a dictionary G.graph_attr containing
-    the default graphviz attributes for graphs, nodes and edges.
-
-    Default node attributes will be in the dictionary G.node_attr
-    which is keyed by node.
-
-    Edge attributes will be returned as edge data in G.  With
-    edge_attr=False the edge data will be the Graphviz edge weight
-    attribute or the value 1 if no edge weight attribute is found.
-
-    """
-    if create_using is None:
-        if A.is_directed():
-            if A.is_strict():
-                create_using = eg.DiGraph
-            else:
-                create_using = eg.MultiDiGraph
-        else:
-            if A.is_strict():
-                create_using = eg.Graph
-            else:
-                create_using = eg.MultiGraph
-
-    # assign defaults
-    N = eg.empty_graph(0, create_using)
-    if A.name is not None:
-        N.name = A.name
-
-    # add graph attributes
-    N.graph.update(A.graph_attr)
-
-    # add nodes, attributes to N.node_attr
-    for n in A.nodes():
-        str_attr = {str(k): v for k, v in n.attr.items()}
-        N.add_node(str(n), **str_attr)
-
-    # add edges, assign edge data as dictionary of attributes
-    for e in A.edges():
-        u, v = str(e[0]), str(e[1])
-        attr = dict(e.attr)
-        str_attr = {str(k): v for k, v in attr.items()}
-        if not N.is_multigraph():
-            if e.name is not None:
-                str_attr["key"] = e.name
-            N.add_edge(u, v, **str_attr)
-        else:
-            N.add_edge(u, v, key=e.name, **str_attr)
-
-    # add default attributes for graph, nodes, and edges
-    # hang them on N.graph_attr
-    N.graph["graph"] = dict(A.graph_attr)
-    N.graph["node"] = dict(A.node_attr)
-    N.graph["edge"] = dict(A.edge_attr)
-    return N
-
-
-def to_pyGraphviz_agraph(N):
-    """Returns a pygraphviz graph from a EasyGraph graph N.
-
-    Parameters
-    ----------
-    N : EasyGraph graph
-      A graph created with EasyGraph
-
-    Examples
-    --------
-    >>> K5 = eg.complete_graph(5)
-    >>> A = eg.to_pyGraphviz_agraph(K5)
-
-    Notes
-    -----
-    If N has an dict N.graph_attr an attempt will be made first
-    to copy properties attached to the graph (see from_agraph)
-    and then updated with the calling arguments if any.
-
-    """
-    try:
-        import pygraphviz
-    except ImportError as err:
-        raise ImportError("requires pygraphviz http://pygraphviz.github.io/") from err
-    directed = N.is_directed()
-    strict = eg.number_of_selfloops(N) == 0 and not N.is_multigraph()
-    A = pygraphviz.AGraph(name=N.name, strict=strict, directed=directed)
-
-    # default graph attributes
-    A.graph_attr.update(N.graph.get("graph", {}))
-    A.node_attr.update(N.graph.get("node", {}))
-    A.edge_attr.update(N.graph.get("edge", {}))
-
-    A.graph_attr.update(
-        (k, v) for k, v in N.graph.items() if k not in ("graph", "node", "edge")
-    )
-
-    # add nodes
-    for n, nodedata in N.nodes(data=True):
-        A.add_node(n)
-        # Add node data
-        a = A.get_node(n)
-        a.attr.update({k: str(v) for k, v in nodedata.items()})
-
-    # loop over edges
-    if N.is_multigraph():
-        for u, v, key, edgedata in N.edges(data=True, keys=True):
-            str_edgedata = {k: str(v) for k, v in edgedata.items() if k != "key"}
-            A.add_edge(u, v, key=str(key))
-            # Add edge data
-            a = A.get_edge(u, v)
-            a.attr.update(str_edgedata)
-
-    else:
-        for u, v, edgedata in N.edges(data=True):
-            str_edgedata = {k: str(v) for k, v in edgedata.items()}
-            A.add_edge(u, v)
-            # Add edge data
-            a = A.get_edge(u, v)
-            a.attr.update(str_edgedata)
-
-    return A
+import easygraph as eg
+
+
+__all__ = [
+    "from_pyGraphviz_agraph",
+    "to_pyGraphviz_agraph",
+]
+
+
+def from_pyGraphviz_agraph(A, create_using=None):
+    """Returns a EasyGraph Graph or DiGraph from a PyGraphviz graph.
+
+    Parameters
+    ----------
+    A : PyGraphviz AGraph
+      A graph created with PyGraphviz
+
+    create_using : EasyGraph graph constructor, optional (default=None)
+       Graph type to create. If graph instance, then cleared before populated.
+       If `None`, then the appropriate Graph type is inferred from `A`.
+
+    Examples
+    --------
+    >>> K5 = eg.complete_graph(5)
+    >>> A = eg.to_pyGraphviz_agraph(K5)
+    >>> G = eg.from_pyGraphviz_agraph(A)
+
+    Notes
+    -----
+    The Graph G will have a dictionary G.graph_attr containing
+    the default graphviz attributes for graphs, nodes and edges.
+
+    Default node attributes will be in the dictionary G.node_attr
+    which is keyed by node.
+
+    Edge attributes will be returned as edge data in G.  With
+    edge_attr=False the edge data will be the Graphviz edge weight
+    attribute or the value 1 if no edge weight attribute is found.
+
+    """
+    if create_using is None:
+        if A.is_directed():
+            if A.is_strict():
+                create_using = eg.DiGraph
+            else:
+                create_using = eg.MultiDiGraph
+        else:
+            if A.is_strict():
+                create_using = eg.Graph
+            else:
+                create_using = eg.MultiGraph
+
+    # assign defaults
+    N = eg.empty_graph(0, create_using)
+    if A.name is not None:
+        N.name = A.name
+
+    # add graph attributes
+    N.graph.update(A.graph_attr)
+
+    # add nodes, attributes to N.node_attr
+    for n in A.nodes():
+        str_attr = {str(k): v for k, v in n.attr.items()}
+        N.add_node(str(n), **str_attr)
+
+    # add edges, assign edge data as dictionary of attributes
+    for e in A.edges():
+        u, v = str(e[0]), str(e[1])
+        attr = dict(e.attr)
+        str_attr = {str(k): v for k, v in attr.items()}
+        if not N.is_multigraph():
+            if e.name is not None:
+                str_attr["key"] = e.name
+            N.add_edge(u, v, **str_attr)
+        else:
+            N.add_edge(u, v, key=e.name, **str_attr)
+
+    # add default attributes for graph, nodes, and edges
+    # hang them on N.graph_attr
+    N.graph["graph"] = dict(A.graph_attr)
+    N.graph["node"] = dict(A.node_attr)
+    N.graph["edge"] = dict(A.edge_attr)
+    return N
+
+
+def to_pyGraphviz_agraph(N):
+    """Returns a pygraphviz graph from a EasyGraph graph N.
+
+    Parameters
+    ----------
+    N : EasyGraph graph
+      A graph created with EasyGraph
+
+    Examples
+    --------
+    >>> K5 = eg.complete_graph(5)
+    >>> A = eg.to_pyGraphviz_agraph(K5)
+
+    Notes
+    -----
+    If N has an dict N.graph_attr an attempt will be made first
+    to copy properties attached to the graph (see from_agraph)
+    and then updated with the calling arguments if any.
+
+    """
+    try:
+        import pygraphviz
+    except ImportError as err:
+        raise ImportError("requires pygraphviz http://pygraphviz.github.io/") from err
+    directed = N.is_directed()
+    strict = eg.number_of_selfloops(N) == 0 and not N.is_multigraph()
+    A = pygraphviz.AGraph(name=N.name, strict=strict, directed=directed)
+
+    # default graph attributes
+    A.graph_attr.update(N.graph.get("graph", {}))
+    A.node_attr.update(N.graph.get("node", {}))
+    A.edge_attr.update(N.graph.get("edge", {}))
+
+    A.graph_attr.update(
+        (k, v) for k, v in N.graph.items() if k not in ("graph", "node", "edge")
+    )
+
+    # add nodes
+    for n, nodedata in N.nodes(data=True):
+        A.add_node(n)
+        # Add node data
+        a = A.get_node(n)
+        a.attr.update({k: str(v) for k, v in nodedata.items()})
+
+    # loop over edges
+    if N.is_multigraph():
+        for u, v, key, edgedata in N.edges(data=True, keys=True):
+            str_edgedata = {k: str(v) for k, v in edgedata.items() if k != "key"}
+            A.add_edge(u, v, key=str(key))
+            # Add edge data
+            a = A.get_edge(u, v)
+            a.attr.update(str_edgedata)
+
+    else:
+        for u, v, edgedata in N.edges(data=True):
+            str_edgedata = {k: str(v) for k, v in edgedata.items()}
+            A.add_edge(u, v)
+            # Add edge data
+            a = A.get_edge(u, v)
+            a.attr.update(str_edgedata)
+
+    return A
```

## easygraph/utils/logging.py

 * *Ordering differences only*

```diff
@@ -1,44 +1,44 @@
-import logging
-import sys
-
-from pathlib import Path
-from typing import Union
-
-
-__all__ = ["default_log_formatter", "simple_stdout2file"]
-
-
-def default_log_formatter() -> logging.Formatter:
-    r"""Create a default formatter of log messages for logging."""
-
-    return logging.Formatter("[%(levelname)s %(asctime)s]-> %(message)s")
-
-
-def simple_stdout2file(file_path: Union[str, Path]) -> None:
-    r"""This function simply wraps the ``sys.stdout`` stream, and outputs messages to the ``sys.stdout`` and a specified file, simultaneously.
-
-    Parameters:
-        ``file_path`` (``file_path: Union[str, Path]``): The path of the file to output the messages.
-    """
-
-    class SimpleLogger:
-        def __init__(self, file_path: Path):
-            file_path = Path(file_path).absolute()
-            assert (
-                file_path.parent.exists()
-            ), f"The parent directory of {file_path} does not exist."
-            self.file_path = file_path
-            self.terminal = sys.stdout
-            self.file = open(file_path, "a")
-
-        def write(self, message):
-            self.terminal.write(message)
-            self.file.write(message)
-            self.flush()
-
-        def flush(self):
-            self.terminal.flush()
-            self.file.flush()
-
-    file_path = Path(file_path)
-    sys.stdout = SimpleLogger(file_path)
+import logging
+import sys
+
+from pathlib import Path
+from typing import Union
+
+
+__all__ = ["default_log_formatter", "simple_stdout2file"]
+
+
+def default_log_formatter() -> logging.Formatter:
+    r"""Create a default formatter of log messages for logging."""
+
+    return logging.Formatter("[%(levelname)s %(asctime)s]-> %(message)s")
+
+
+def simple_stdout2file(file_path: Union[str, Path]) -> None:
+    r"""This function simply wraps the ``sys.stdout`` stream, and outputs messages to the ``sys.stdout`` and a specified file, simultaneously.
+
+    Parameters:
+        ``file_path`` (``file_path: Union[str, Path]``): The path of the file to output the messages.
+    """
+
+    class SimpleLogger:
+        def __init__(self, file_path: Path):
+            file_path = Path(file_path).absolute()
+            assert (
+                file_path.parent.exists()
+            ), f"The parent directory of {file_path} does not exist."
+            self.file_path = file_path
+            self.terminal = sys.stdout
+            self.file = open(file_path, "a")
+
+        def write(self, message):
+            self.terminal.write(message)
+            self.file.write(message)
+            self.flush()
+
+        def flush(self):
+            self.terminal.flush()
+            self.file.flush()
+
+    file_path = Path(file_path)
+    sys.stdout = SimpleLogger(file_path)
```

## easygraph/utils/convert_class.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-__all__ = [
-    "convert_graph_class",
-]
-
-
-def convert_graph_class(G, graph_class):
-    _G = graph_class()
-    _G.graph.update(G.graph)
-    for node, node_attrs in G.nodes.items():
-        dict_attrs = {}
-        for key, value in node_attrs:
-            dict_attrs[key] = value
-        _G.add_node(node, **dict_attrs)
-    for u, v, edge_attrs in G.edges:
-        dict_attrs = {}
-        for key, value in edge_attrs.items():
-            dict_attrs[key] = value
-        _G.add_edge(u, v, **dict_attrs)
-    return _G
+__all__ = [
+    "convert_graph_class",
+]
+
+
+def convert_graph_class(G, graph_class):
+    _G = graph_class()
+    _G.graph.update(G.graph)
+    for node, node_attrs in G.nodes.items():
+        dict_attrs = {}
+        for key, value in node_attrs:
+            dict_attrs[key] = value
+        _G.add_node(node, **dict_attrs)
+    for u, v, edge_attrs in G.edges:
+        dict_attrs = {}
+        for key, value in edge_attrs.items():
+            dict_attrs[key] = value
+        _G.add_edge(u, v, **dict_attrs)
+    return _G
```

## easygraph/utils/misc.py

 * *Ordering differences only*

```diff
@@ -1,222 +1,222 @@
-from collections.abc import Iterable
-from collections.abc import Iterator
-from itertools import chain
-from itertools import tee
-
-
-__all__ = [
-    "split_len",
-    "split",
-    "nodes_equal",
-    "edges_equal",
-    "pairwise",
-    "graphs_equal",
-    # "arbitrary_element"
-]
-
-
-def split_len(nodes, step=30000):
-    ret = []
-    length = len(nodes)
-    for i in range(0, length, step):
-        ret.append(nodes[i : i + step])
-    if len(ret[-1]) * 3 < step:
-        ret[-2] = ret[-2] + ret[-1]
-        ret = ret[:-1]
-    return ret
-
-
-def split(nodes, n):
-    ret = []
-    length = len(nodes)  # 总长
-    step = int(length / n) + 1  # 每份的长度
-    for i in range(0, length, step):
-        ret.append(nodes[i : i + step])
-    return ret
-
-
-def nodes_equal(nodes1, nodes2):
-    """Check if nodes are equal.
-
-    Equality here means equal as Python objects.
-    Node data must match if included.
-    The order of nodes is not relevant.
-
-    Parameters
-    ----------
-    nodes1, nodes2 : iterables of nodes, or (node, datadict) tuples
-
-    Returns
-    -------
-    bool
-        True if nodes are equal, False otherwise.
-    """
-    nlist1 = list(nodes1)
-    nlist2 = list(nodes2)
-    try:
-        d1 = dict(nlist1)
-        d2 = dict(nlist2)
-    except (ValueError, TypeError):
-        d1 = dict.fromkeys(nlist1)
-        d2 = dict.fromkeys(nlist2)
-    return d1 == d2
-
-
-def edges_equal(edges1, edges2, need_data=True):
-    """Check if edges are equal.
-
-    Equality here means equal as Python objects.
-    Edge data must match if included.
-    The order of the edges is not relevant.
-
-    Parameters
-    ----------
-    edges1, edges2 : iterables of with u, v nodes as
-        edge tuples (u, v), or
-        edge tuples with data dicts (u, v, d), or
-        edge tuples with keys and data dicts (u, v, k, d)
-
-    Returns
-    -------
-    bool
-        True if edges are equal, False otherwise.
-    """
-    from collections import defaultdict
-
-    d1 = defaultdict(dict)
-    d2 = defaultdict(dict)
-    c1 = 0
-    for c1, e in enumerate(edges1):
-        u, v = e[0], e[1]
-        data = []
-        if need_data == True:
-            data = [e[2:]]
-            if v in d1[u]:
-                data = d1[u][v] + data
-        d1[u][v] = data
-        d1[v][u] = data
-    c2 = 0
-    for c2, e in enumerate(edges2):
-        u, v = e[0], e[1]
-        data = []
-        if need_data == True:
-            data = [e[2:]]
-            if v in d2[u]:
-                data = d2[u][v] + data
-        d2[u][v] = data
-        d2[v][u] = data
-    if c1 != c2:
-        return False
-    # can check one direction because lengths are the same.
-    for n, nbrdict in d1.items():
-        for nbr, datalist in nbrdict.items():
-            if n not in d2:
-                return False
-            if nbr not in d2[n]:
-                return False
-            d2datalist = d2[n][nbr]
-            for data in datalist:
-                if datalist.count(data) != d2datalist.count(data):
-                    return False
-    return True
-
-
-# Recipe from the itertools documentation.
-def pairwise(iterable, cyclic=False):
-    "s -> (s0, s1), (s1, s2), (s2, s3), ..."
-    a, b = tee(iterable)
-    first = next(b, None)
-    if cyclic is True:
-        return zip(a, chain(b, (first,)))
-    return zip(a, b)
-
-
-def graphs_equal(graph1, graph2):
-    """Check if graphs are equal.
-
-    Equality here means equal as Python objects (not isomorphism).
-    Node, edge and graph data must match.
-
-    Parameters
-    ----------
-    graph1, graph2 : graph
-
-    Returns
-    -------
-    bool
-        True if graphs are equal, False otherwise.
-    """
-    return (
-        graph1.adj == graph2.adj
-        and graph1.nodes == graph2.nodes
-        and graph1.graph == graph2.graph
-    )
-
-
-# def arbitrary_element(iterable):
-#     """Returns an arbitrary element of `iterable` without removing it.
-
-#     This is most useful for "peeking" at an arbitrary element of a set,
-#     but can be used for any list, dictionary, etc., as well.
-
-#     Parameters
-#     ----------
-#     iterable : `abc.collections.Iterable` instance
-#         Any object that implements ``__iter__``, e.g. set, dict, list, tuple,
-#         etc.
-
-#     Returns
-#     -------
-#     The object that results from ``next(iter(iterable))``
-
-#     Raises
-#     ------
-#     ValueError
-#         If `iterable` is an iterator (because the current implementation of
-#         this function would consume an element from the iterator).
-
-#     Examples
-#     --------
-#     Arbitrary elements from common Iterable objects:
-
-#     >>> eg.utils.arbitrary_element([1, 2, 3])  # list
-#     1
-#     >>> eg.utils.arbitrary_element((1, 2, 3))  # tuple
-#     1
-#     >>> eg.utils.arbitrary_element({1, 2, 3})  # set
-#     1
-#     >>> d = {k: v for k, v in zip([1, 2, 3], [3, 2, 1])}
-#     >>> eg.utils.arbitrary_element(d)  # dict_keys
-#     1
-#     >>> eg.utils.arbitrary_element(d.values())   # dict values
-#     3
-
-#     `str` is also an Iterable:
-
-#     >>> eg.utils.arbitrary_element("hello")
-#     'h'
-
-#     :exc:`ValueError` is raised if `iterable` is an iterator:
-
-#     >>> iterator = iter([1, 2, 3])  # Iterator, *not* Iterable
-#     >>> eg.utils.arbitrary_element(iterator)
-#     Traceback (most recent call last):
-#         ...
-#     ValueError: cannot return an arbitrary item from an iterator
-
-#     Notes
-#     -----
-#     This function does not return a *random* element. If `iterable` is
-#     ordered, sequential calls will return the same value::
-
-#         >>> l = [1, 2, 3]
-#         >>> eg.utils.arbitrary_element(l)
-#         1
-#         >>> eg.utils.arbitrary_element(l)
-#         1
-
-#     """
-#     if isinstance(iterable, Iterator):
-#         raise ValueError("cannot return an arbitrary item from an iterator")
-#     # Another possible implementation is ``for x in iterable: return x``.
-#     return next(iter(iterable))
+from collections.abc import Iterable
+from collections.abc import Iterator
+from itertools import chain
+from itertools import tee
+
+
+__all__ = [
+    "split_len",
+    "split",
+    "nodes_equal",
+    "edges_equal",
+    "pairwise",
+    "graphs_equal",
+    # "arbitrary_element"
+]
+
+
+def split_len(nodes, step=30000):
+    ret = []
+    length = len(nodes)
+    for i in range(0, length, step):
+        ret.append(nodes[i : i + step])
+    if len(ret[-1]) * 3 < step:
+        ret[-2] = ret[-2] + ret[-1]
+        ret = ret[:-1]
+    return ret
+
+
+def split(nodes, n):
+    ret = []
+    length = len(nodes)  # 总长
+    step = int(length / n) + 1  # 每份的长度
+    for i in range(0, length, step):
+        ret.append(nodes[i : i + step])
+    return ret
+
+
+def nodes_equal(nodes1, nodes2):
+    """Check if nodes are equal.
+
+    Equality here means equal as Python objects.
+    Node data must match if included.
+    The order of nodes is not relevant.
+
+    Parameters
+    ----------
+    nodes1, nodes2 : iterables of nodes, or (node, datadict) tuples
+
+    Returns
+    -------
+    bool
+        True if nodes are equal, False otherwise.
+    """
+    nlist1 = list(nodes1)
+    nlist2 = list(nodes2)
+    try:
+        d1 = dict(nlist1)
+        d2 = dict(nlist2)
+    except (ValueError, TypeError):
+        d1 = dict.fromkeys(nlist1)
+        d2 = dict.fromkeys(nlist2)
+    return d1 == d2
+
+
+def edges_equal(edges1, edges2, need_data=True):
+    """Check if edges are equal.
+
+    Equality here means equal as Python objects.
+    Edge data must match if included.
+    The order of the edges is not relevant.
+
+    Parameters
+    ----------
+    edges1, edges2 : iterables of with u, v nodes as
+        edge tuples (u, v), or
+        edge tuples with data dicts (u, v, d), or
+        edge tuples with keys and data dicts (u, v, k, d)
+
+    Returns
+    -------
+    bool
+        True if edges are equal, False otherwise.
+    """
+    from collections import defaultdict
+
+    d1 = defaultdict(dict)
+    d2 = defaultdict(dict)
+    c1 = 0
+    for c1, e in enumerate(edges1):
+        u, v = e[0], e[1]
+        data = []
+        if need_data == True:
+            data = [e[2:]]
+            if v in d1[u]:
+                data = d1[u][v] + data
+        d1[u][v] = data
+        d1[v][u] = data
+    c2 = 0
+    for c2, e in enumerate(edges2):
+        u, v = e[0], e[1]
+        data = []
+        if need_data == True:
+            data = [e[2:]]
+            if v in d2[u]:
+                data = d2[u][v] + data
+        d2[u][v] = data
+        d2[v][u] = data
+    if c1 != c2:
+        return False
+    # can check one direction because lengths are the same.
+    for n, nbrdict in d1.items():
+        for nbr, datalist in nbrdict.items():
+            if n not in d2:
+                return False
+            if nbr not in d2[n]:
+                return False
+            d2datalist = d2[n][nbr]
+            for data in datalist:
+                if datalist.count(data) != d2datalist.count(data):
+                    return False
+    return True
+
+
+# Recipe from the itertools documentation.
+def pairwise(iterable, cyclic=False):
+    "s -> (s0, s1), (s1, s2), (s2, s3), ..."
+    a, b = tee(iterable)
+    first = next(b, None)
+    if cyclic is True:
+        return zip(a, chain(b, (first,)))
+    return zip(a, b)
+
+
+def graphs_equal(graph1, graph2):
+    """Check if graphs are equal.
+
+    Equality here means equal as Python objects (not isomorphism).
+    Node, edge and graph data must match.
+
+    Parameters
+    ----------
+    graph1, graph2 : graph
+
+    Returns
+    -------
+    bool
+        True if graphs are equal, False otherwise.
+    """
+    return (
+        graph1.adj == graph2.adj
+        and graph1.nodes == graph2.nodes
+        and graph1.graph == graph2.graph
+    )
+
+
+# def arbitrary_element(iterable):
+#     """Returns an arbitrary element of `iterable` without removing it.
+
+#     This is most useful for "peeking" at an arbitrary element of a set,
+#     but can be used for any list, dictionary, etc., as well.
+
+#     Parameters
+#     ----------
+#     iterable : `abc.collections.Iterable` instance
+#         Any object that implements ``__iter__``, e.g. set, dict, list, tuple,
+#         etc.
+
+#     Returns
+#     -------
+#     The object that results from ``next(iter(iterable))``
+
+#     Raises
+#     ------
+#     ValueError
+#         If `iterable` is an iterator (because the current implementation of
+#         this function would consume an element from the iterator).
+
+#     Examples
+#     --------
+#     Arbitrary elements from common Iterable objects:
+
+#     >>> eg.utils.arbitrary_element([1, 2, 3])  # list
+#     1
+#     >>> eg.utils.arbitrary_element((1, 2, 3))  # tuple
+#     1
+#     >>> eg.utils.arbitrary_element({1, 2, 3})  # set
+#     1
+#     >>> d = {k: v for k, v in zip([1, 2, 3], [3, 2, 1])}
+#     >>> eg.utils.arbitrary_element(d)  # dict_keys
+#     1
+#     >>> eg.utils.arbitrary_element(d.values())   # dict values
+#     3
+
+#     `str` is also an Iterable:
+
+#     >>> eg.utils.arbitrary_element("hello")
+#     'h'
+
+#     :exc:`ValueError` is raised if `iterable` is an iterator:
+
+#     >>> iterator = iter([1, 2, 3])  # Iterator, *not* Iterable
+#     >>> eg.utils.arbitrary_element(iterator)
+#     Traceback (most recent call last):
+#         ...
+#     ValueError: cannot return an arbitrary item from an iterator
+
+#     Notes
+#     -----
+#     This function does not return a *random* element. If `iterable` is
+#     ordered, sequential calls will return the same value::
+
+#         >>> l = [1, 2, 3]
+#         >>> eg.utils.arbitrary_element(l)
+#         1
+#         >>> eg.utils.arbitrary_element(l)
+#         1
+
+#     """
+#     if isinstance(iterable, Iterator):
+#         raise ValueError("cannot return an arbitrary item from an iterator")
+#     # Another possible implementation is ``for x in iterable: return x``.
+#     return next(iter(iterable))
```

## easygraph/utils/download.py

 * *Ordering differences only*

```diff
@@ -1,95 +1,95 @@
-import hashlib
-import warnings
-
-from functools import wraps
-from pathlib import Path
-
-import requests
-
-
-__all__ = [
-    "check_file",
-    "download_file",
-    "download_and_check",
-]
-
-
-def download_file(url: str, file_path: Path):
-    r"""Download a file from a url.
-
-    Args:
-        ``url`` (``str``): the url of the file
-        ``file_path`` (``str``): the path to the file
-    """
-    file_path.parent.mkdir(parents=True, exist_ok=True)
-    r = requests.get(url, stream=True, verify=True)
-    if r.status_code != 200:
-        raise requests.HTTPError(f"{url} is not accessible.")
-    with open(file_path, "wb") as f:
-        for chunk in r.iter_content(chunk_size=1024):
-            if chunk:
-                f.write(chunk)
-
-
-def check_file(file_path: Path, md5: str):
-    r"""Check if a file is valid.
-
-    Args:
-        ``file_path`` (``Path``): The local path of the file.
-        ``md5`` (``str``): The md5 of the file.
-
-    Raises:
-        FileNotFoundError: Not found the file.
-    """
-    if not file_path.exists():
-        raise FileNotFoundError(f"{file_path} does not exist.")
-    else:
-        with open(file_path, "rb") as f:
-            data = f.read()
-        cur_md5 = hashlib.md5(data).hexdigest()
-        return cur_md5 == md5
-
-
-def _retry(n: int, exception_type=requests.HTTPError):
-    r"""A decorator for retrying a function for n times.
-
-    Args:
-        ``n`` (``int``): The number of times to retry.
-    """
-
-    def decorator(fetcher):
-        @wraps(fetcher)
-        def wrapper(*args, **kwargs):
-            for i in range(n - 1):
-                try:
-                    return fetcher(*args, **kwargs)
-                except exception_type as e:
-                    warnings.warn(f"Retry downloading({i + 1}/{n}): {str(e)}")
-                except Exception as e:
-                    raise e
-            return fetcher(*args, **kwargs)
-            # raise FileNotFoundError
-
-        return wrapper
-
-    return decorator
-
-
-@_retry(3)
-def download_and_check(url: str, file_path: Path, md5: str):
-    r"""Download a file from a url and check its integrity.
-
-    Args:
-        ``url`` (``str``): The url of the file.
-        ``file_path`` (``Path``): The path to the file.
-        ``md5`` (``str``): The md5 of the file.
-    """
-    if not file_path.exists():
-        download_file(url, file_path)
-    if not check_file(file_path, md5):
-        file_path.unlink()
-        raise ValueError(
-            f"{file_path} is corrupted. We will delete it, and try to download it"
-            " again."
-        )
-    return True
+import hashlib
+import warnings
+
+from functools import wraps
+from pathlib import Path
+
+import requests
+
+
+__all__ = [
+    "check_file",
+    "download_file",
+    "download_and_check",
+]
+
+
+def download_file(url: str, file_path: Path):
+    r"""Download a file from a url.
+
+    Args:
+        ``url`` (``str``): the url of the file
+        ``file_path`` (``str``): the path to the file
+    """
+    file_path.parent.mkdir(parents=True, exist_ok=True)
+    r = requests.get(url, stream=True, verify=True)
+    if r.status_code != 200:
+        raise requests.HTTPError(f"{url} is not accessible.")
+    with open(file_path, "wb") as f:
+        for chunk in r.iter_content(chunk_size=1024):
+            if chunk:
+                f.write(chunk)
+
+
+def check_file(file_path: Path, md5: str):
+    r"""Check if a file is valid.
+
+    Args:
+        ``file_path`` (``Path``): The local path of the file.
+        ``md5`` (``str``): The md5 of the file.
+
+    Raises:
+        FileNotFoundError: Not found the file.
+    """
+    if not file_path.exists():
+        raise FileNotFoundError(f"{file_path} does not exist.")
+    else:
+        with open(file_path, "rb") as f:
+            data = f.read()
+        cur_md5 = hashlib.md5(data).hexdigest()
+        return cur_md5 == md5
+
+
+def _retry(n: int, exception_type=requests.HTTPError):
+    r"""A decorator for retrying a function for n times.
+
+    Args:
+        ``n`` (``int``): The number of times to retry.
+    """
+
+    def decorator(fetcher):
+        @wraps(fetcher)
+        def wrapper(*args, **kwargs):
+            for i in range(n - 1):
+                try:
+                    return fetcher(*args, **kwargs)
+                except exception_type as e:
+                    warnings.warn(f"Retry downloading({i + 1}/{n}): {str(e)}")
+                except Exception as e:
+                    raise e
+            return fetcher(*args, **kwargs)
+            # raise FileNotFoundError
+
+        return wrapper
+
+    return decorator
+
+
+@_retry(3)
+def download_and_check(url: str, file_path: Path, md5: str):
+    r"""Download a file from a url and check its integrity.
+
+    Args:
+        ``url`` (``str``): The url of the file.
+        ``file_path`` (``Path``): The path to the file.
+        ``md5`` (``str``): The md5 of the file.
+    """
+    if not file_path.exists():
+        download_file(url, file_path)
+    if not check_file(file_path, md5):
+        file_path.unlink()
+        raise ValueError(
+            f"{file_path} is corrupted. We will delete it, and try to download it"
+            " again."
+        )
+    return True
```

## easygraph/utils/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-from easygraph.utils.alias import *
-from easygraph.utils.convert_class import *
-from easygraph.utils.convert_to_matrix import *
-from easygraph.utils.decorators import *
-from easygraph.utils.download import *
-from easygraph.utils.exception import *
-from easygraph.utils.index_of_node import *
-from easygraph.utils.logging import *
-from easygraph.utils.mapped_queue import *
-from easygraph.utils.misc import *
-from easygraph.utils.relabel import *
-from easygraph.utils.sparse import *
-from easygraph.utils.type_change import *
+from easygraph.utils.alias import *
+from easygraph.utils.convert_class import *
+from easygraph.utils.convert_to_matrix import *
+from easygraph.utils.decorators import *
+from easygraph.utils.download import *
+from easygraph.utils.exception import *
+from easygraph.utils.index_of_node import *
+from easygraph.utils.logging import *
+from easygraph.utils.mapped_queue import *
+from easygraph.utils.misc import *
+from easygraph.utils.relabel import *
+from easygraph.utils.sparse import *
+from easygraph.utils.type_change import *
```

## easygraph/experiments/base.py

 * *Ordering differences only*

```diff
@@ -1,204 +1,204 @@
-import abc
-import logging
-import shutil
-import time
-
-from copy import deepcopy
-from pathlib import Path
-from typing import Callable
-from typing import Optional
-from typing import Union
-
-import optuna
-import torch
-import torch.nn as nn
-
-from easygraph.classes.base import load_structure
-from easygraph.ml_metrics import BaseEvaluator
-from easygraph.utils import default_log_formatter
-from optuna.samplers import TPESampler
-
-
-class BaseTask:
-    r"""The base class of Auto-experiment in EasyGraph.
-
-    Args:
-        ``work_root`` (``Optional[Union[str, Path]]``): User's work root to store all studies.
-        ``data`` (``dict``): The dictionary to store input data that used in the experiment.
-        ``model_builder`` (``Callable``): The function to build a model with a fixed parameter ``trial``.
-        ``train_builder`` (``Callable``): The function to build a training configuration with two fixed parameters ``trial`` and ``model``.
-        ``evaluator`` (``eg.ml_metrics.BaseEvaluator``): The EasyGraph evaluator object to evaluate performance of the model in the experiment.
-        ``device`` (``torch.device``): The target device to run the experiment.
-        ``structure_builder`` (``Optional[Callable]``): The function to build a structure with a fixed parameter ``trial``. The structure can be ``eg.Graph``, ``eg.DiGraph``, ``eg.BiGraph``, and ``eg.Hypergraph``.
-        ``study_name`` (``Optional[str]``): The name of this study. If set to ``None``, the study name will be generated automatically according to current time. Defaults to ``None``.
-        ``overwrite`` (``bool``): The flag that whether to overwrite the existing study. Different studies are identified by the ``study_name``. Defaults to ``True``.
-    """
-
-    def __init__(
-        self,
-        work_root: Optional[Union[str, Path]],
-        data: dict,
-        model_builder: Callable,
-        train_builder: Callable,
-        evaluator: BaseEvaluator,
-        device: torch.device,
-        structure_builder: Optional[Callable] = None,
-        study_name: Optional[str] = None,
-        overwrite: bool = True,
-    ):
-        self.data = data
-        self.model_builder = model_builder
-        self.train_builder = train_builder
-        self.structure_builder = structure_builder
-        self.evaluator = evaluator
-        self.device = device
-        self.study = None
-        if study_name is None:
-            self.study_name = time.strftime("%Y-%m-%d--%H-%M-%S", time.localtime())
-        else:
-            self.study_name = study_name
-        work_root = Path(work_root)
-        self.study_root = work_root / self.study_name
-        if overwrite and self.study_root.exists():
-            shutil.rmtree(self.study_root)
-        self.log_file = self.study_root / "log.txt"
-        self.cache_root = self.study_root / "cache"
-        if not work_root.exists():
-            if work_root.parent.exists():
-                work_root.mkdir(exist_ok=True)
-            else:
-                raise ValueError(f"The work_root {work_root} does not exist.")
-        self.study_root.mkdir(exist_ok=True)
-        self.cache_root.mkdir(exist_ok=True)
-        # configure logging
-        self.logger = optuna.logging.get_logger("optuna")
-        self.logger.setLevel(logging.INFO)
-        out_file_handler = logging.FileHandler(self.log_file, mode="a", encoding="utf8")
-        out_file_handler.setFormatter(default_log_formatter())
-        self.logger.addHandler(out_file_handler)
-        self.logger.info(f"Logs will be saved to {self.log_file.absolute()}")
-        self.logger.info(
-            f"Files in training will be saved in {self.study_root.absolute()}"
-        )
-
-    def experiment(self, trial: optuna.Trial):
-        r"""Run the experiment for a given trial.
-
-        Args:
-            ``trial`` (``optuna.Trial``): The ``optuna.Trial`` object.
-        """
-        if self.structure_builder is not None:
-            self.data["structure"] = self.structure_builder(trial).to(self.device)
-        model = self.model_builder(trial).to(self.device)
-        train_configs: dict = self.train_builder(trial, model)
-        assert "optimizer" in train_configs.keys()
-        optimizer = train_configs["optimizer"]
-        assert "criterion" in train_configs.keys()
-        criterion = train_configs["criterion"]
-        scheduler = train_configs.get("scheduler", None)
-
-        best_model = None
-        if self.direction == "maximize":
-            best_score = -float("inf")
-        else:
-            best_score = float("inf")
-        for epoch in range(self.max_epoch):
-            self.train(self.data, model, optimizer, criterion)
-            val_res = self.validate(self.data, model)
-            trial.report(val_res, epoch)
-            if trial.should_prune():
-                raise optuna.exceptions.TrialPruned()
-            if scheduler is not None:
-                scheduler.step()
-            if self.direction == "maximize":
-                if val_res > best_score:
-                    best_score = val_res
-                    best_model = deepcopy(model)
-        with open(self.cache_root / f"{trial.number}_model.pth", "wb") as f:
-            torch.save(best_model.cpu().state_dict(), f)
-        self.data["structure"].save(self.cache_root / f"{trial.number}_structure.dhg")
-        return best_score
-
-    def _remove_cached_data(self):
-        r"""Remove cached models and structures."""
-        if self.study is not None:
-            for filename in self.cache_root.glob("*"):
-                if filename.stem.split("_")[0] != str(self.study.best_trial.number):
-                    filename.unlink()
-
-    def run(self, max_epoch: int, num_trials: int = 1, direction: str = "maximize"):
-        r"""Run experiments with automatically hyper-parameter tuning.
-
-        Args:
-            ``max_epoch`` (``int``): The maximum number of epochs to train for each experiment.
-            ``num_trials`` (``int``): The number of trials to run. Defaults to ``1``.
-            ``direction`` (``str``): The direction to optimize. Defaults to ``"maximize"``.
-        """
-        self.logger.info(f"Random seed is {dhg.random.seed()}")
-        sampler = TPESampler(seed=dhg.random.seed())
-        self.max_epoch, self.direction = max_epoch, direction
-        self.study = optuna.create_study(direction=direction, sampler=sampler)
-        self.study.optimize(self.experiment, n_trials=num_trials, timeout=600)
-
-        self._remove_cached_data()
-        self.best_model = self.model_builder(self.study.best_trial)
-        self.best_model.load_state_dict(
-            torch.load(f"{self.cache_root}/{self.study.best_trial.number}_model.pth")
-        )
-        self.best_structure = load_structure(
-            f"{self.cache_root}/{self.study.best_trial.number}_structure.dhg"
-        )
-        self.best_model = self.best_model.to(self.device)
-        self.best_structure = self.best_structure.to(self.device)
-
-        self.logger.info("Best trial:")
-        self.best_trial = self.study.best_trial
-        self.logger.info(f"\tValue: {self.best_trial.value:.3f}")
-        self.logger.info(f"\tParams:")
-        for key, value in self.best_trial.params.items():
-            self.logger.info(f"\t\t{key} |-> {value}")
-        test_res = self.test()
-        self.logger.info(f"Final test results:")
-        for key, value in test_res.items():
-            self.logger.info(f"\t{key} |-> {value:.3f}")
-
-    @abc.abstractmethod
-    def train(
-        self,
-        data: dict,
-        model: nn.Module,
-        optimizer: torch.optim.Optimizer,
-        criterion: nn.Module,
-    ):
-        r"""Train model for one epoch.
-
-        Args:
-            ``data`` (``dict``): The input data.
-            ``model`` (``nn.Module``): The model.
-            ``optimizer`` (``torch.optim.Optimizer``): The model optimizer.
-            ``criterion`` (``nn.Module``): The loss function.
-        """
-
-    @torch.no_grad()
-    @abc.abstractmethod
-    def validate(
-        self,
-        data: dict,
-        model: nn.Module,
-    ):
-        r"""Validate the model.
-
-        Args:
-            ``data`` (``dict``): The input data.
-            ``model`` (``nn.Module``): The model.
-        """
-
-    @torch.no_grad()
-    @abc.abstractmethod
-    def test(self, data: Optional[dict] = None, model: Optional[nn.Module] = None):
-        r"""Test the model.
-
-        Args:
-            ``data`` (``dict``, optional): The input data if set to ``None``, the specified ``data`` in the initialization of the experiments will be used. Defaults to ``None``.
-            ``model`` (``nn.Module``, optional): The model if set to ``None``, the trained best model will be used. Defaults to ``None``.
-        """
+import abc
+import logging
+import shutil
+import time
+
+from copy import deepcopy
+from pathlib import Path
+from typing import Callable
+from typing import Optional
+from typing import Union
+
+import optuna
+import torch
+import torch.nn as nn
+
+from easygraph.classes.base import load_structure
+from easygraph.ml_metrics import BaseEvaluator
+from easygraph.utils import default_log_formatter
+from optuna.samplers import TPESampler
+
+
+class BaseTask:
+    r"""The base class of Auto-experiment in EasyGraph.
+
+    Args:
+        ``work_root`` (``Optional[Union[str, Path]]``): User's work root to store all studies.
+        ``data`` (``dict``): The dictionary to store input data that used in the experiment.
+        ``model_builder`` (``Callable``): The function to build a model with a fixed parameter ``trial``.
+        ``train_builder`` (``Callable``): The function to build a training configuration with two fixed parameters ``trial`` and ``model``.
+        ``evaluator`` (``eg.ml_metrics.BaseEvaluator``): The EasyGraph evaluator object to evaluate performance of the model in the experiment.
+        ``device`` (``torch.device``): The target device to run the experiment.
+        ``structure_builder`` (``Optional[Callable]``): The function to build a structure with a fixed parameter ``trial``. The structure can be ``eg.Graph``, ``eg.DiGraph``, ``eg.BiGraph``, and ``eg.Hypergraph``.
+        ``study_name`` (``Optional[str]``): The name of this study. If set to ``None``, the study name will be generated automatically according to current time. Defaults to ``None``.
+        ``overwrite`` (``bool``): The flag that whether to overwrite the existing study. Different studies are identified by the ``study_name``. Defaults to ``True``.
+    """
+
+    def __init__(
+        self,
+        work_root: Optional[Union[str, Path]],
+        data: dict,
+        model_builder: Callable,
+        train_builder: Callable,
+        evaluator: BaseEvaluator,
+        device: torch.device,
+        structure_builder: Optional[Callable] = None,
+        study_name: Optional[str] = None,
+        overwrite: bool = True,
+    ):
+        self.data = data
+        self.model_builder = model_builder
+        self.train_builder = train_builder
+        self.structure_builder = structure_builder
+        self.evaluator = evaluator
+        self.device = device
+        self.study = None
+        if study_name is None:
+            self.study_name = time.strftime("%Y-%m-%d--%H-%M-%S", time.localtime())
+        else:
+            self.study_name = study_name
+        work_root = Path(work_root)
+        self.study_root = work_root / self.study_name
+        if overwrite and self.study_root.exists():
+            shutil.rmtree(self.study_root)
+        self.log_file = self.study_root / "log.txt"
+        self.cache_root = self.study_root / "cache"
+        if not work_root.exists():
+            if work_root.parent.exists():
+                work_root.mkdir(exist_ok=True)
+            else:
+                raise ValueError(f"The work_root {work_root} does not exist.")
+        self.study_root.mkdir(exist_ok=True)
+        self.cache_root.mkdir(exist_ok=True)
+        # configure logging
+        self.logger = optuna.logging.get_logger("optuna")
+        self.logger.setLevel(logging.INFO)
+        out_file_handler = logging.FileHandler(self.log_file, mode="a", encoding="utf8")
+        out_file_handler.setFormatter(default_log_formatter())
+        self.logger.addHandler(out_file_handler)
+        self.logger.info(f"Logs will be saved to {self.log_file.absolute()}")
+        self.logger.info(
+            f"Files in training will be saved in {self.study_root.absolute()}"
+        )
+
+    def experiment(self, trial: optuna.Trial):
+        r"""Run the experiment for a given trial.
+
+        Args:
+            ``trial`` (``optuna.Trial``): The ``optuna.Trial`` object.
+        """
+        if self.structure_builder is not None:
+            self.data["structure"] = self.structure_builder(trial).to(self.device)
+        model = self.model_builder(trial).to(self.device)
+        train_configs: dict = self.train_builder(trial, model)
+        assert "optimizer" in train_configs.keys()
+        optimizer = train_configs["optimizer"]
+        assert "criterion" in train_configs.keys()
+        criterion = train_configs["criterion"]
+        scheduler = train_configs.get("scheduler", None)
+
+        best_model = None
+        if self.direction == "maximize":
+            best_score = -float("inf")
+        else:
+            best_score = float("inf")
+        for epoch in range(self.max_epoch):
+            self.train(self.data, model, optimizer, criterion)
+            val_res = self.validate(self.data, model)
+            trial.report(val_res, epoch)
+            if trial.should_prune():
+                raise optuna.exceptions.TrialPruned()
+            if scheduler is not None:
+                scheduler.step()
+            if self.direction == "maximize":
+                if val_res > best_score:
+                    best_score = val_res
+                    best_model = deepcopy(model)
+        with open(self.cache_root / f"{trial.number}_model.pth", "wb") as f:
+            torch.save(best_model.cpu().state_dict(), f)
+        self.data["structure"].save(self.cache_root / f"{trial.number}_structure.dhg")
+        return best_score
+
+    def _remove_cached_data(self):
+        r"""Remove cached models and structures."""
+        if self.study is not None:
+            for filename in self.cache_root.glob("*"):
+                if filename.stem.split("_")[0] != str(self.study.best_trial.number):
+                    filename.unlink()
+
+    def run(self, max_epoch: int, num_trials: int = 1, direction: str = "maximize"):
+        r"""Run experiments with automatically hyper-parameter tuning.
+
+        Args:
+            ``max_epoch`` (``int``): The maximum number of epochs to train for each experiment.
+            ``num_trials`` (``int``): The number of trials to run. Defaults to ``1``.
+            ``direction`` (``str``): The direction to optimize. Defaults to ``"maximize"``.
+        """
+        self.logger.info(f"Random seed is {dhg.random.seed()}")
+        sampler = TPESampler(seed=dhg.random.seed())
+        self.max_epoch, self.direction = max_epoch, direction
+        self.study = optuna.create_study(direction=direction, sampler=sampler)
+        self.study.optimize(self.experiment, n_trials=num_trials, timeout=600)
+
+        self._remove_cached_data()
+        self.best_model = self.model_builder(self.study.best_trial)
+        self.best_model.load_state_dict(
+            torch.load(f"{self.cache_root}/{self.study.best_trial.number}_model.pth")
+        )
+        self.best_structure = load_structure(
+            f"{self.cache_root}/{self.study.best_trial.number}_structure.dhg"
+        )
+        self.best_model = self.best_model.to(self.device)
+        self.best_structure = self.best_structure.to(self.device)
+
+        self.logger.info("Best trial:")
+        self.best_trial = self.study.best_trial
+        self.logger.info(f"\tValue: {self.best_trial.value:.3f}")
+        self.logger.info(f"\tParams:")
+        for key, value in self.best_trial.params.items():
+            self.logger.info(f"\t\t{key} |-> {value}")
+        test_res = self.test()
+        self.logger.info(f"Final test results:")
+        for key, value in test_res.items():
+            self.logger.info(f"\t{key} |-> {value:.3f}")
+
+    @abc.abstractmethod
+    def train(
+        self,
+        data: dict,
+        model: nn.Module,
+        optimizer: torch.optim.Optimizer,
+        criterion: nn.Module,
+    ):
+        r"""Train model for one epoch.
+
+        Args:
+            ``data`` (``dict``): The input data.
+            ``model`` (``nn.Module``): The model.
+            ``optimizer`` (``torch.optim.Optimizer``): The model optimizer.
+            ``criterion`` (``nn.Module``): The loss function.
+        """
+
+    @torch.no_grad()
+    @abc.abstractmethod
+    def validate(
+        self,
+        data: dict,
+        model: nn.Module,
+    ):
+        r"""Validate the model.
+
+        Args:
+            ``data`` (``dict``): The input data.
+            ``model`` (``nn.Module``): The model.
+        """
+
+    @torch.no_grad()
+    @abc.abstractmethod
+    def test(self, data: Optional[dict] = None, model: Optional[nn.Module] = None):
+        r"""Test the model.
+
+        Args:
+            ``data`` (``dict``, optional): The input data if set to ``None``, the specified ``data`` in the initialization of the experiments will be used. Defaults to ``None``.
+            ``model`` (``nn.Module``, optional): The model if set to ``None``, the trained best model will be used. Defaults to ``None``.
+        """
```

## easygraph/experiments/vertex_classification.py

 * *Ordering differences only*

```diff
@@ -1,166 +1,166 @@
-from pathlib import Path
-from typing import Callable
-from typing import Optional
-from typing import Union
-
-import optuna
-import torch
-import torch.nn as nn
-
-from easygraph.ml_metrics import BaseEvaluator
-
-from .base import BaseTask
-
-
-class VertexClassificationTask(BaseTask):
-    r"""The auto-experiment class for the vertex classification task.
-
-    Args:
-        ``work_root`` (``Optional[Union[str, Path]]``): User's work root to store all studies.
-        ``data`` (``dict``): The dictionary to store input data that used in the experiment.
-        ``model_builder`` (``Callable``): The function to build a model with a fixed parameter ``trial``.
-        ``train_builder`` (``Callable``): The function to build a training configuration with two fixed parameters ``trial`` and ``model``.
-        ``evaluator`` (``eg.ml_metrics.BaseEvaluator``): The DHG evaluator object to evaluate performance of the model in the experiment.
-        ``device`` (``torch.device``): The target device to run the experiment.
-        ``structure_builder`` (``Optional[Callable]``): The function to build a structure with a fixed parameter ``trial``. The structure can be ``eg.Hypergraph``.
-        ``study_name`` (``Optional[str]``): The name of this study. If set to ``None``, the study name will be generated automatically according to current time. Defaults to ``None``.
-        ``overwrite`` (``bool``): The flag that whether to overwrite the existing study. Different studies are identified by the ``study_name``. Defaults to ``True``.
-    """
-
-    def __init__(
-        self,
-        work_root: Optional[Union[str, Path]],
-        data: dict,
-        model_builder: Callable,
-        train_builder: Callable,
-        evaluator: BaseEvaluator,
-        device: torch.device,
-        structure_builder: Optional[Callable] = None,
-        study_name: Optional[str] = None,
-        overwrite: bool = True,
-    ):
-        super().__init__(
-            work_root,
-            data,
-            model_builder,
-            train_builder,
-            evaluator,
-            device,
-            structure_builder=structure_builder,
-            study_name=study_name,
-            overwrite=overwrite,
-        )
-        self.to(self.device)
-
-    def to(self, device: torch.device):
-        r"""Move the input data to the target device.
-
-        Args:
-            ``device`` (``torch.device``): The specified target device to store the input data.
-        """
-        self.device = device
-        for name in self.vars_for_DL:
-            if name in self.data.keys():
-                self.data[name] = self.data[name].to(device)
-        return self
-
-    @property
-    def vars_for_DL(self):
-        r"""Return a name list for available variables for deep learning in the vertex classification task. The name list includes ``features``, ``structure``, ``labels``, ``train_mask``, ``val_mask``, and ``test_mask``.
-        """
-        return (
-            "features",
-            "structure",
-            "labels",
-            "train_mask",
-            "val_mask",
-            "test_mask",
-        )
-
-    def experiment(self, trial: optuna.Trial):
-        r"""Run the experiment for a given trial.
-
-        Args:
-            ``trial`` (``optuna.Trial``): The ``optuna.Trial`` object.
-        """
-        return super().experiment(trial)
-
-    def run(self, max_epoch: int, num_trials: int = 1, direction: str = "maximize"):
-        r"""Run experiments with automatically hyper-parameter tuning.
-
-        Args:
-            ``max_epoch`` (``int``): The maximum number of epochs to train for each experiment.
-            ``num_trials`` (``int``): The number of trials to run. Defaults to ``1``.
-            ``direction`` (``str``): The direction to optimize. Defaults to ``"maximize"``.
-        """
-        return super().run(max_epoch, num_trials, direction)
-
-    def train(
-        self,
-        data: dict,
-        model: nn.Module,
-        optimizer: torch.optim.Optimizer,
-        criterion: nn.Module,
-    ):
-        r"""Train model for one epoch.
-
-        Args:
-            ``data`` (``dict``): The input data.
-            ``model`` (``nn.Module``): The model.
-            ``optimizer`` (``torch.optim.Optimizer``): The model optimizer.
-            ``criterion`` (``nn.Module``): The loss function.
-        """
-        features, structure = data["features"], data["structure"]
-        train_mask, labels = data["train_mask"], data["labels"]
-        model.train()
-        optimizer.zero_grad()
-        outputs = model(features, structure)
-        loss = criterion(
-            outputs[train_mask],
-            labels[train_mask],
-        )
-        loss.backward()
-        optimizer.step()
-
-    @torch.no_grad()
-    def validate(self, data: dict, model: nn.Module):
-        r"""Validate the model.
-
-        Args:
-            ``data`` (``dict``): The input data.
-            ``model`` (``nn.Module``): The model.
-        """
-        features, structure = data["features"], data["structure"]
-        val_mask, labels = data["val_mask"], data["labels"]
-        model.eval()
-        outputs = model(features, structure)
-        res = self.evaluator.validate(labels[val_mask], outputs[val_mask])
-        return res
-
-    @torch.no_grad()
-    def test(self, data: Optional[dict] = None, model: Optional[nn.Module] = None):
-        r"""Test the model.
-
-        Args:
-            ``data`` (``dict``, optional): The input data if set to ``None``, the specified ``data`` in the initialization of the experiments will be used. Defaults to ``None``.
-            ``model`` (``nn.Module``, optional): The model if set to ``None``, the trained best model will be used. Defaults to ``None``.
-        """
-        if data is None:
-            features, structure = self.data["features"], self.best_structure
-            test_mask, labels = self.data["test_mask"], self.data["labels"]
-        else:
-            features, structure = (
-                data["features"].to(self.device),
-                data["structure"].to(self.device),
-            )
-            test_mask, labels = (
-                data["test_mask"].to(self.device),
-                data["labels"].to(self.device),
-            )
-        if model is None:
-            model = self.best_model
-        model = model.to(self.device)
-        model.eval()
-        outputs = model(features, structure)
-        res = self.evaluator.test(labels[test_mask], outputs[test_mask])
-        return res
+from pathlib import Path
+from typing import Callable
+from typing import Optional
+from typing import Union
+
+import optuna
+import torch
+import torch.nn as nn
+
+from easygraph.ml_metrics import BaseEvaluator
+
+from .base import BaseTask
+
+
+class VertexClassificationTask(BaseTask):
+    r"""The auto-experiment class for the vertex classification task.
+
+    Args:
+        ``work_root`` (``Optional[Union[str, Path]]``): User's work root to store all studies.
+        ``data`` (``dict``): The dictionary to store input data that used in the experiment.
+        ``model_builder`` (``Callable``): The function to build a model with a fixed parameter ``trial``.
+        ``train_builder`` (``Callable``): The function to build a training configuration with two fixed parameters ``trial`` and ``model``.
+        ``evaluator`` (``eg.ml_metrics.BaseEvaluator``): The DHG evaluator object to evaluate performance of the model in the experiment.
+        ``device`` (``torch.device``): The target device to run the experiment.
+        ``structure_builder`` (``Optional[Callable]``): The function to build a structure with a fixed parameter ``trial``. The structure can be ``eg.Hypergraph``.
+        ``study_name`` (``Optional[str]``): The name of this study. If set to ``None``, the study name will be generated automatically according to current time. Defaults to ``None``.
+        ``overwrite`` (``bool``): The flag that whether to overwrite the existing study. Different studies are identified by the ``study_name``. Defaults to ``True``.
+    """
+
+    def __init__(
+        self,
+        work_root: Optional[Union[str, Path]],
+        data: dict,
+        model_builder: Callable,
+        train_builder: Callable,
+        evaluator: BaseEvaluator,
+        device: torch.device,
+        structure_builder: Optional[Callable] = None,
+        study_name: Optional[str] = None,
+        overwrite: bool = True,
+    ):
+        super().__init__(
+            work_root,
+            data,
+            model_builder,
+            train_builder,
+            evaluator,
+            device,
+            structure_builder=structure_builder,
+            study_name=study_name,
+            overwrite=overwrite,
+        )
+        self.to(self.device)
+
+    def to(self, device: torch.device):
+        r"""Move the input data to the target device.
+
+        Args:
+            ``device`` (``torch.device``): The specified target device to store the input data.
+        """
+        self.device = device
+        for name in self.vars_for_DL:
+            if name in self.data.keys():
+                self.data[name] = self.data[name].to(device)
+        return self
+
+    @property
+    def vars_for_DL(self):
+        r"""Return a name list for available variables for deep learning in the vertex classification task. The name list includes ``features``, ``structure``, ``labels``, ``train_mask``, ``val_mask``, and ``test_mask``.
+        """
+        return (
+            "features",
+            "structure",
+            "labels",
+            "train_mask",
+            "val_mask",
+            "test_mask",
+        )
+
+    def experiment(self, trial: optuna.Trial):
+        r"""Run the experiment for a given trial.
+
+        Args:
+            ``trial`` (``optuna.Trial``): The ``optuna.Trial`` object.
+        """
+        return super().experiment(trial)
+
+    def run(self, max_epoch: int, num_trials: int = 1, direction: str = "maximize"):
+        r"""Run experiments with automatically hyper-parameter tuning.
+
+        Args:
+            ``max_epoch`` (``int``): The maximum number of epochs to train for each experiment.
+            ``num_trials`` (``int``): The number of trials to run. Defaults to ``1``.
+            ``direction`` (``str``): The direction to optimize. Defaults to ``"maximize"``.
+        """
+        return super().run(max_epoch, num_trials, direction)
+
+    def train(
+        self,
+        data: dict,
+        model: nn.Module,
+        optimizer: torch.optim.Optimizer,
+        criterion: nn.Module,
+    ):
+        r"""Train model for one epoch.
+
+        Args:
+            ``data`` (``dict``): The input data.
+            ``model`` (``nn.Module``): The model.
+            ``optimizer`` (``torch.optim.Optimizer``): The model optimizer.
+            ``criterion`` (``nn.Module``): The loss function.
+        """
+        features, structure = data["features"], data["structure"]
+        train_mask, labels = data["train_mask"], data["labels"]
+        model.train()
+        optimizer.zero_grad()
+        outputs = model(features, structure)
+        loss = criterion(
+            outputs[train_mask],
+            labels[train_mask],
+        )
+        loss.backward()
+        optimizer.step()
+
+    @torch.no_grad()
+    def validate(self, data: dict, model: nn.Module):
+        r"""Validate the model.
+
+        Args:
+            ``data`` (``dict``): The input data.
+            ``model`` (``nn.Module``): The model.
+        """
+        features, structure = data["features"], data["structure"]
+        val_mask, labels = data["val_mask"], data["labels"]
+        model.eval()
+        outputs = model(features, structure)
+        res = self.evaluator.validate(labels[val_mask], outputs[val_mask])
+        return res
+
+    @torch.no_grad()
+    def test(self, data: Optional[dict] = None, model: Optional[nn.Module] = None):
+        r"""Test the model.
+
+        Args:
+            ``data`` (``dict``, optional): The input data if set to ``None``, the specified ``data`` in the initialization of the experiments will be used. Defaults to ``None``.
+            ``model`` (``nn.Module``, optional): The model if set to ``None``, the trained best model will be used. Defaults to ``None``.
+        """
+        if data is None:
+            features, structure = self.data["features"], self.best_structure
+            test_mask, labels = self.data["test_mask"], self.data["labels"]
+        else:
+            features, structure = (
+                data["features"].to(self.device),
+                data["structure"].to(self.device),
+            )
+            test_mask, labels = (
+                data["test_mask"].to(self.device),
+                data["labels"].to(self.device),
+            )
+        if model is None:
+            model = self.best_model
+        model = model.to(self.device)
+        model.eval()
+        outputs = model(features, structure)
+        res = self.evaluator.test(labels[test_mask], outputs[test_mask])
+        return res
```

## easygraph/experiments/__init__.py

 * *Ordering differences only*

```diff
@@ -1,10 +1,10 @@
-try:
-    from .base import BaseTask
-    from .hypergraphs import HypergraphVertexClassificationTask
-
-
-except:
-    print(
-        "Warning raise in module: experiments. Please install Pytorch before you use"
-        " functions related to nueral network"
-    )
+try:
+    from .base import BaseTask
+    from .hypergraphs import HypergraphVertexClassificationTask
+
+
+except:
+    print(
+        "Warning raise in module: experiments. Please install Pytorch before you use"
+        " functions related to nueral network"
+    )
```

## easygraph/experiments/hypergraphs/hypergraph.py

 * *Ordering differences only*

```diff
@@ -1,121 +1,121 @@
-from pathlib import Path
-from typing import Callable
-from typing import Optional
-from typing import Union
-
-import optuna
-import torch
-import torch.nn as nn
-
-from easygraph.ml_metrics import BaseEvaluator
-
-from ..vertex_classification import VertexClassificationTask
-
-
-class HypergraphVertexClassificationTask(VertexClassificationTask):
-    r"""The auto-experiment class for the vertex classification task on hypergraph.
-
-    Args:
-        ``work_root`` (``Optional[Union[str, Path]]``): User's work root to store all studies.
-        ``data`` (``dict``): The dictionary to store input data that used in the experiment.
-        ``model_builder`` (``Callable``): The function to build a model with a fixed parameter ``trial``.
-        ``train_builder`` (``Callable``): The function to build a training configuration with two fixed parameters ``trial`` and ``model``.
-        ``evaluator`` (``easygraph.ml_metrics.BaseEvaluator``): The DHG evaluator object to evaluate performance of the model in the experiment.
-        ``device`` (``torch.device``): The target device to run the experiment.
-        ``structure_builder`` (``Optional[Callable]``): The function to build a structure with a fixed parameter ``trial``. The structure should be ``easygraph.Hypergraph``.
-        ``study_name`` (``Optional[str]``): The name of this study. If set to ``None``, the study name will be generated automatically according to current time. Defaults to ``None``.
-        ``overwrite`` (``bool``): The flag that whether to overwrite the existing study. Different studies are identified by the ``study_name``. Defaults to ``True``.
-    """
-
-    def __init__(
-        self,
-        work_root: Optional[Union[str, Path]],
-        data: dict,
-        model_builder: Callable,
-        train_builder: Callable,
-        evaluator: BaseEvaluator,
-        device: torch.device,
-        structure_builder: Optional[Callable] = None,
-        study_name: Optional[str] = None,
-        overwrite: bool = True,
-    ):
-        super().__init__(
-            work_root,
-            data,
-            model_builder,
-            train_builder,
-            evaluator,
-            device,
-            structure_builder=structure_builder,
-            study_name=study_name,
-            overwrite=overwrite,
-        )
-
-    def to(self, device: torch.device):
-        r"""Move the input data to the target device.
-
-        Args:
-            ``device`` (``torch.device``): The specified target device to store the input data.
-        """
-        return super().to(device)
-
-    @property
-    def vars_for_DL(self):
-        r"""Return a name list for available variables for deep learning in the vertex classification on hypergraph. The name list includes ``features``, ``structure``, ``labels``, ``train_mask``, ``val_mask``, and ``test_mask``.
-        """
-        return super().vars_for_DL
-
-    def experiment(self, trial: optuna.Trial):
-        r"""Run the experiment for a given trial.
-
-        Args:
-            ``trial`` (``optuna.Trial``): The ``optuna.Trial`` object.
-        """
-        return super().experiment(trial)
-
-    def run(self, max_epoch: int, num_trials: int = 1, direction: str = "maximize"):
-        r"""Run experiments with automatically hyper-parameter tuning.
-
-        Args:
-            ``max_epoch`` (``int``): The maximum number of epochs to train for each experiment.
-            ``num_trials`` (``int``): The number of trials to run. Defaults to ``1``.
-            ``direction`` (``str``): The direction to optimize. Defaults to ``"maximize"``.
-        """
-        return super().run(max_epoch, num_trials, direction)
-
-    def train(
-        self,
-        data: dict,
-        model: nn.Module,
-        optimizer: torch.optim.Optimizer,
-        criterion: nn.Module,
-    ):
-        r"""Train model for one epoch.
-
-        Args:
-            ``data`` (``dict``): The input data.
-            ``model`` (``nn.Module``): The model.
-            ``optimizer`` (``torch.optim.Optimizer``): The model optimizer.
-            ``criterion`` (``nn.Module``): The loss function.
-        """
-        return super().train(data, model, optimizer, criterion)
-
-    @torch.no_grad()
-    def validate(self, data: dict, model: nn.Module):
-        r"""Validate the model.
-
-        Args:
-            ``data`` (``dict``): The input data.
-            ``model`` (``nn.Module``): The model.
-        """
-        return super().validate(data, model)
-
-    @torch.no_grad()
-    def test(self, data: Optional[dict] = None, model: Optional[nn.Module] = None):
-        r"""Test the model.
-
-        Args:
-            ``data`` (``dict``, optional): The input data if set to ``None``, the specified ``data`` in the intialization of the experiments will be used. Defaults to ``None``.
-            ``model`` (``nn.Module``, optional): The model if set to ``None``, the trained best model will be used. Defaults to ``None``.
-        """
-        return super().test(data, model)
+from pathlib import Path
+from typing import Callable
+from typing import Optional
+from typing import Union
+
+import optuna
+import torch
+import torch.nn as nn
+
+from easygraph.ml_metrics import BaseEvaluator
+
+from ..vertex_classification import VertexClassificationTask
+
+
+class HypergraphVertexClassificationTask(VertexClassificationTask):
+    r"""The auto-experiment class for the vertex classification task on hypergraph.
+
+    Args:
+        ``work_root`` (``Optional[Union[str, Path]]``): User's work root to store all studies.
+        ``data`` (``dict``): The dictionary to store input data that used in the experiment.
+        ``model_builder`` (``Callable``): The function to build a model with a fixed parameter ``trial``.
+        ``train_builder`` (``Callable``): The function to build a training configuration with two fixed parameters ``trial`` and ``model``.
+        ``evaluator`` (``easygraph.ml_metrics.BaseEvaluator``): The DHG evaluator object to evaluate performance of the model in the experiment.
+        ``device`` (``torch.device``): The target device to run the experiment.
+        ``structure_builder`` (``Optional[Callable]``): The function to build a structure with a fixed parameter ``trial``. The structure should be ``easygraph.Hypergraph``.
+        ``study_name`` (``Optional[str]``): The name of this study. If set to ``None``, the study name will be generated automatically according to current time. Defaults to ``None``.
+        ``overwrite`` (``bool``): The flag that whether to overwrite the existing study. Different studies are identified by the ``study_name``. Defaults to ``True``.
+    """
+
+    def __init__(
+        self,
+        work_root: Optional[Union[str, Path]],
+        data: dict,
+        model_builder: Callable,
+        train_builder: Callable,
+        evaluator: BaseEvaluator,
+        device: torch.device,
+        structure_builder: Optional[Callable] = None,
+        study_name: Optional[str] = None,
+        overwrite: bool = True,
+    ):
+        super().__init__(
+            work_root,
+            data,
+            model_builder,
+            train_builder,
+            evaluator,
+            device,
+            structure_builder=structure_builder,
+            study_name=study_name,
+            overwrite=overwrite,
+        )
+
+    def to(self, device: torch.device):
+        r"""Move the input data to the target device.
+
+        Args:
+            ``device`` (``torch.device``): The specified target device to store the input data.
+        """
+        return super().to(device)
+
+    @property
+    def vars_for_DL(self):
+        r"""Return a name list for available variables for deep learning in the vertex classification on hypergraph. The name list includes ``features``, ``structure``, ``labels``, ``train_mask``, ``val_mask``, and ``test_mask``.
+        """
+        return super().vars_for_DL
+
+    def experiment(self, trial: optuna.Trial):
+        r"""Run the experiment for a given trial.
+
+        Args:
+            ``trial`` (``optuna.Trial``): The ``optuna.Trial`` object.
+        """
+        return super().experiment(trial)
+
+    def run(self, max_epoch: int, num_trials: int = 1, direction: str = "maximize"):
+        r"""Run experiments with automatically hyper-parameter tuning.
+
+        Args:
+            ``max_epoch`` (``int``): The maximum number of epochs to train for each experiment.
+            ``num_trials`` (``int``): The number of trials to run. Defaults to ``1``.
+            ``direction`` (``str``): The direction to optimize. Defaults to ``"maximize"``.
+        """
+        return super().run(max_epoch, num_trials, direction)
+
+    def train(
+        self,
+        data: dict,
+        model: nn.Module,
+        optimizer: torch.optim.Optimizer,
+        criterion: nn.Module,
+    ):
+        r"""Train model for one epoch.
+
+        Args:
+            ``data`` (``dict``): The input data.
+            ``model`` (``nn.Module``): The model.
+            ``optimizer`` (``torch.optim.Optimizer``): The model optimizer.
+            ``criterion`` (``nn.Module``): The loss function.
+        """
+        return super().train(data, model, optimizer, criterion)
+
+    @torch.no_grad()
+    def validate(self, data: dict, model: nn.Module):
+        r"""Validate the model.
+
+        Args:
+            ``data`` (``dict``): The input data.
+            ``model`` (``nn.Module``): The model.
+        """
+        return super().validate(data, model)
+
+    @torch.no_grad()
+    def test(self, data: Optional[dict] = None, model: Optional[nn.Module] = None):
+        r"""Test the model.
+
+        Args:
+            ``data`` (``dict``, optional): The input data if set to ``None``, the specified ``data`` in the intialization of the experiments will be used. Defaults to ``None``.
+            ``model`` (``nn.Module``, optional): The model if set to ``None``, the trained best model will be used. Defaults to ``None``.
+        """
+        return super().test(data, model)
```

## easygraph/experiments/hypergraphs/__init__.py

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-from .hypergraph import HypergraphVertexClassificationTask
+from .hypergraph import HypergraphVertexClassificationTask
```

## easygraph/nn/loss.py

 * *Ordering differences only*

```diff
@@ -1,46 +1,46 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-
-__all__ = ["BPRLoss"]
-
-
-class BPRLoss(nn.Module):
-    r"""This criterion computes the Bayesian Personalized Ranking (BPR) loss between the positive scores and the negative scores.
-
-    Parameters:
-        ``alpha`` (``float``, optional): The weight for the positive scores in the BPR loss. Defaults to ``1.0``.
-        ``beta`` (``float``, optional): The weight for the negative scores in the BPR loss. Defaults to ``1.0``.
-        ``activation`` (``str``, optional): The activation function to use can be one of ``"sigmoid_then_log"``, ``"softplus"``. Defaults to ``"sigmoid_then_log"``.
-    """
-
-    def __init__(
-        self,
-        alpha: float = 1.0,
-        beta: float = 1.0,
-        activation: str = "sigmoid_then_log",
-    ):
-        super().__init__()
-        assert activation in (
-            "sigmoid_then_log",
-            "softplus",
-        ), "activation function of BPRLoss must be sigmoid_then_log or softplus."
-        self.activation = activation
-        self.alpha = alpha
-        self.beta = beta
-
-    def forward(self, pos_scores: torch.Tensor, neg_scores: torch.Tensor):
-        r"""The forward function of BPRLoss.
-
-        Parameters:
-            ``pos_scores`` (``torch.Tensor``): The positive scores.
-            ``neg_scores`` (``torch.Tensor``): The negative scores.
-        """
-        if self.activation == "sigmoid_then_log":
-            loss = -(self.alpha * pos_scores - self.beta * neg_scores).sigmoid().log()
-        elif self.activation == "softplus":
-            loss = F.softplus(self.beta * neg_scores - self.alpha * pos_scores)
-        else:
-            raise NotImplementedError
-        return loss.mean()
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+
+__all__ = ["BPRLoss"]
+
+
+class BPRLoss(nn.Module):
+    r"""This criterion computes the Bayesian Personalized Ranking (BPR) loss between the positive scores and the negative scores.
+
+    Parameters:
+        ``alpha`` (``float``, optional): The weight for the positive scores in the BPR loss. Defaults to ``1.0``.
+        ``beta`` (``float``, optional): The weight for the negative scores in the BPR loss. Defaults to ``1.0``.
+        ``activation`` (``str``, optional): The activation function to use can be one of ``"sigmoid_then_log"``, ``"softplus"``. Defaults to ``"sigmoid_then_log"``.
+    """
+
+    def __init__(
+        self,
+        alpha: float = 1.0,
+        beta: float = 1.0,
+        activation: str = "sigmoid_then_log",
+    ):
+        super().__init__()
+        assert activation in (
+            "sigmoid_then_log",
+            "softplus",
+        ), "activation function of BPRLoss must be sigmoid_then_log or softplus."
+        self.activation = activation
+        self.alpha = alpha
+        self.beta = beta
+
+    def forward(self, pos_scores: torch.Tensor, neg_scores: torch.Tensor):
+        r"""The forward function of BPRLoss.
+
+        Parameters:
+            ``pos_scores`` (``torch.Tensor``): The positive scores.
+            ``neg_scores`` (``torch.Tensor``): The negative scores.
+        """
+        if self.activation == "sigmoid_then_log":
+            loss = -(self.alpha * pos_scores - self.beta * neg_scores).sigmoid().log()
+        elif self.activation == "softplus":
+            loss = F.softplus(self.beta * neg_scores - self.alpha * pos_scores)
+        else:
+            raise NotImplementedError
+        return loss.mean()
```

## easygraph/nn/regularization.py

 * *Ordering differences only*

```diff
@@ -1,32 +1,32 @@
-from typing import List
-
-import torch
-import torch.nn as nn
-
-
-__all__ = ["EmbeddingRegularization"]
-
-
-class EmbeddingRegularization(nn.Module):
-    r"""Regularization function for embeddings.
-
-    Parameters:
-        ``p`` (``int``): The power to use in the regularization. Defaults to ``2``.
-        ``weight_decay`` (``float``): The weight of the regularization. Defaults to ``1e-4``.
-    """
-
-    def __init__(self, p: int = 2, weight_decay: float = 1e-4):
-        super().__init__()
-        self.p = p
-        self.weight_decay = weight_decay
-
-    def forward(self, *embs: List[torch.Tensor]):
-        r"""The forward function.
-
-        Parameters:
-            ``embs`` (``List[torch.Tensor]``): The input embeddings.
-        """
-        loss = 0
-        for emb in embs:
-            loss += 1 / self.p * emb.pow(self.p).sum(dim=1).mean()
-        return self.weight_decay * loss
+from typing import List
+
+import torch
+import torch.nn as nn
+
+
+__all__ = ["EmbeddingRegularization"]
+
+
+class EmbeddingRegularization(nn.Module):
+    r"""Regularization function for embeddings.
+
+    Parameters:
+        ``p`` (``int``): The power to use in the regularization. Defaults to ``2``.
+        ``weight_decay`` (``float``): The weight of the regularization. Defaults to ``1e-4``.
+    """
+
+    def __init__(self, p: int = 2, weight_decay: float = 1e-4):
+        super().__init__()
+        self.p = p
+        self.weight_decay = weight_decay
+
+    def forward(self, *embs: List[torch.Tensor]):
+        r"""The forward function.
+
+        Parameters:
+            ``embs`` (``List[torch.Tensor]``): The input embeddings.
+        """
+        loss = 0
+        for emb in embs:
+            loss += 1 / self.p * emb.pow(self.p).sum(dim=1).mean()
+        return self.weight_decay * loss
```

## easygraph/nn/__init__.py

 * *Ordering differences only*

```diff
@@ -1,22 +1,22 @@
-try:
-    from .convs.common import MLP
-    from .convs.common import MultiHeadWrapper
-    from .convs.hypergraphs import HGNNConv
-    from .convs.hypergraphs import HGNNPConv
-    from .convs.hypergraphs import HNHNConv
-    from .convs.hypergraphs import HyperGCNConv
-    from .convs.hypergraphs import JHConv
-    from .convs.hypergraphs import UniGATConv
-    from .convs.hypergraphs import UniGCNConv
-    from .convs.hypergraphs import UniGINConv
-    from .convs.hypergraphs import UniSAGEConv
-    from .convs.hypergraphs.halfnlh_conv import HalfNLHconv
-    from .convs.pma import PMA
-    from .loss import BPRLoss
-    from .regularization import EmbeddingRegularization
-except:
-    print(
-        "Warning raise in module:nn. Please install"
-        " Pytorch,torch_geometric,torch_scatter before you use functions related to"
-        " Hypergraph"
-    )
+try:
+    from .convs.common import MLP
+    from .convs.common import MultiHeadWrapper
+    from .convs.hypergraphs import HGNNConv
+    from .convs.hypergraphs import HGNNPConv
+    from .convs.hypergraphs import HNHNConv
+    from .convs.hypergraphs import HyperGCNConv
+    from .convs.hypergraphs import JHConv
+    from .convs.hypergraphs import UniGATConv
+    from .convs.hypergraphs import UniGCNConv
+    from .convs.hypergraphs import UniGINConv
+    from .convs.hypergraphs import UniSAGEConv
+    from .convs.hypergraphs.halfnlh_conv import HalfNLHconv
+    from .convs.pma import PMA
+    from .loss import BPRLoss
+    from .regularization import EmbeddingRegularization
+except:
+    print(
+        "Warning raise in module:nn. Please install"
+        " Pytorch,torch_geometric,torch_scatter before you use functions related to"
+        " Hypergraph"
+    )
```

## easygraph/nn/tests/test_regularization.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-import easygraph as eg
-import pytest
-import torch
-
-
-def test_embedding_reg():
-    print("EmbeddingRegularization" in eg.__dir__())
-    emb_reg = eg.EmbeddingRegularization(p=2, weight_decay=1e-4)
-    embs = [torch.randn(10, 3), torch.randn(10, 3)]
-    loss = emb_reg(*embs)
-    true_loss = 0
-    for emb in embs:
-        true_loss += 1 / 2 * emb.norm(2).pow(2) / 10
-    assert loss.item() == pytest.approx(1e-4 * true_loss.item())
+import easygraph as eg
+import pytest
+import torch
+
+
+def test_embedding_reg():
+    print("EmbeddingRegularization" in eg.__dir__())
+    emb_reg = eg.EmbeddingRegularization(p=2, weight_decay=1e-4)
+    embs = [torch.randn(10, 3), torch.randn(10, 3)]
+    loss = emb_reg(*embs)
+    true_loss = 0
+    for emb in embs:
+        true_loss += 1 / 2 * emb.norm(2).pow(2) / 10
+    assert loss.item() == pytest.approx(1e-4 * true_loss.item())
```

## easygraph/nn/convs/common.py

 * *Ordering differences only*

```diff
@@ -1,136 +1,136 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-
-class MultiHeadWrapper(nn.Module):
-    r"""A wrapper to apply multiple heads to a given layer.
-
-    Parameters:
-        ``num_heads`` (``int``): The number of heads.
-        ``readout`` (``bool``): The readout method. Can be ``"mean"``, ``"max"``, ``"sum"``, or ``"concat"``.
-        ``layer`` (``nn.Module``): The layer to apply multiple heads.
-        ``**kwargs``: The keyword arguments for the layer.
-
-    """
-
-    def __init__(
-        self, num_heads: int, readout: str, layer: nn.Module, **kwargs
-    ) -> None:
-        super().__init__()
-        self.layers = nn.ModuleList()
-        for _ in range(num_heads):
-            self.layers.append(layer(**kwargs))
-        self.num_heads = num_heads
-        self.readout = readout
-
-    def forward(self, **kwargs) -> torch.Tensor:
-        r"""The forward function.
-
-        .. note::
-            You must explicitly pass the keyword arguments to the layer. For example, if the layer is ``GATConv``, you must pass ``X=X`` and ``g=g``.
-        """
-        if self.readout == "concat":
-            return torch.cat([layer(**kwargs) for layer in self.layers], dim=-1)
-        else:
-            outs = torch.stack([layer(**kwargs) for layer in self.layers])
-            if self.readout == "mean":
-                return outs.mean(dim=0)
-            elif self.readout == "max":
-                return outs.max(dim=0)[0]
-            elif self.readout == "sum":
-                return outs.sum(dim=0)
-            else:
-                raise ValueError("Unknown readout type")
-
-
-class MLP(nn.Module):
-    """adapted from https://github.com/CUAI/CorrectAndSmooth/blob/master/gen_models.py
-    """
-
-    def __init__(
-        self,
-        in_channels,
-        hidden_channels,
-        out_channels,
-        num_layers,
-        dropout=0.5,
-        normalization="bn",
-        InputNorm=False,
-    ):
-        super(MLP, self).__init__()
-        self.lins = nn.ModuleList()
-        self.normalizations = nn.ModuleList()
-        self.InputNorm = InputNorm
-
-        assert normalization in ["bn", "ln", "None"]
-        if normalization == "bn":
-            if num_layers == 1:
-                # just linear layer i.e. logistic regression
-                if InputNorm:
-                    self.normalizations.append(nn.BatchNorm1d(in_channels))
-                else:
-                    self.normalizations.append(nn.Identity())
-                self.lins.append(nn.Linear(in_channels, out_channels))
-            else:
-                if InputNorm:
-                    self.normalizations.append(nn.BatchNorm1d(in_channels))
-                else:
-                    self.normalizations.append(nn.Identity())
-                self.lins.append(nn.Linear(in_channels, hidden_channels))
-                self.normalizations.append(nn.BatchNorm1d(hidden_channels))
-                for _ in range(num_layers - 2):
-                    self.lins.append(nn.Linear(hidden_channels, hidden_channels))
-                    self.normalizations.append(nn.BatchNorm1d(hidden_channels))
-                self.lins.append(nn.Linear(hidden_channels, out_channels))
-        elif normalization == "ln":
-            if num_layers == 1:
-                # just linear layer i.e. logistic regression
-                if InputNorm:
-                    self.normalizations.append(nn.LayerNorm(in_channels))
-                else:
-                    self.normalizations.append(nn.Identity())
-                self.lins.append(nn.Linear(in_channels, out_channels))
-            else:
-                if InputNorm:
-                    self.normalizations.append(nn.LayerNorm(in_channels))
-                else:
-                    self.normalizations.append(nn.Identity())
-                self.lins.append(nn.Linear(in_channels, hidden_channels))
-                self.normalizations.append(nn.LayerNorm(hidden_channels))
-                for _ in range(num_layers - 2):
-                    self.lins.append(nn.Linear(hidden_channels, hidden_channels))
-                    self.normalizations.append(nn.LayerNorm(hidden_channels))
-                self.lins.append(nn.Linear(hidden_channels, out_channels))
-        else:
-            if num_layers == 1:
-                # just linear layer i.e. logistic regression
-                self.normalizations.append(nn.Identity())
-                self.lins.append(nn.Linear(in_channels, out_channels))
-            else:
-                self.normalizations.append(nn.Identity())
-                self.lins.append(nn.Linear(in_channels, hidden_channels))
-                self.normalizations.append(nn.Identity())
-                for _ in range(num_layers - 2):
-                    self.lins.append(nn.Linear(hidden_channels, hidden_channels))
-                    self.normalizations.append(nn.Identity())
-                self.lins.append(nn.Linear(hidden_channels, out_channels))
-
-        self.dropout = dropout
-
-    def reset_parameters(self):
-        for lin in self.lins:
-            lin.reset_parameters()
-        for normalization in self.normalizations:
-            if not (normalization.__class__.__name__ is "Identity"):
-                normalization.reset_parameters()
-
-    def forward(self, x):
-        x = self.normalizations[0](x)
-        for i, lin in enumerate(self.lins[:-1]):
-            x = lin(x)
-            x = F.relu(x, inplace=True)
-            x = self.normalizations[i + 1](x)
-            x = F.dropout(x, p=self.dropout, training=self.training)
-        x = self.lins[-1](x)
-        return x
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+
+class MultiHeadWrapper(nn.Module):
+    r"""A wrapper to apply multiple heads to a given layer.
+
+    Parameters:
+        ``num_heads`` (``int``): The number of heads.
+        ``readout`` (``bool``): The readout method. Can be ``"mean"``, ``"max"``, ``"sum"``, or ``"concat"``.
+        ``layer`` (``nn.Module``): The layer to apply multiple heads.
+        ``**kwargs``: The keyword arguments for the layer.
+
+    """
+
+    def __init__(
+        self, num_heads: int, readout: str, layer: nn.Module, **kwargs
+    ) -> None:
+        super().__init__()
+        self.layers = nn.ModuleList()
+        for _ in range(num_heads):
+            self.layers.append(layer(**kwargs))
+        self.num_heads = num_heads
+        self.readout = readout
+
+    def forward(self, **kwargs) -> torch.Tensor:
+        r"""The forward function.
+
+        .. note::
+            You must explicitly pass the keyword arguments to the layer. For example, if the layer is ``GATConv``, you must pass ``X=X`` and ``g=g``.
+        """
+        if self.readout == "concat":
+            return torch.cat([layer(**kwargs) for layer in self.layers], dim=-1)
+        else:
+            outs = torch.stack([layer(**kwargs) for layer in self.layers])
+            if self.readout == "mean":
+                return outs.mean(dim=0)
+            elif self.readout == "max":
+                return outs.max(dim=0)[0]
+            elif self.readout == "sum":
+                return outs.sum(dim=0)
+            else:
+                raise ValueError("Unknown readout type")
+
+
+class MLP(nn.Module):
+    """adapted from https://github.com/CUAI/CorrectAndSmooth/blob/master/gen_models.py
+    """
+
+    def __init__(
+        self,
+        in_channels,
+        hidden_channels,
+        out_channels,
+        num_layers,
+        dropout=0.5,
+        normalization="bn",
+        InputNorm=False,
+    ):
+        super(MLP, self).__init__()
+        self.lins = nn.ModuleList()
+        self.normalizations = nn.ModuleList()
+        self.InputNorm = InputNorm
+
+        assert normalization in ["bn", "ln", "None"]
+        if normalization == "bn":
+            if num_layers == 1:
+                # just linear layer i.e. logistic regression
+                if InputNorm:
+                    self.normalizations.append(nn.BatchNorm1d(in_channels))
+                else:
+                    self.normalizations.append(nn.Identity())
+                self.lins.append(nn.Linear(in_channels, out_channels))
+            else:
+                if InputNorm:
+                    self.normalizations.append(nn.BatchNorm1d(in_channels))
+                else:
+                    self.normalizations.append(nn.Identity())
+                self.lins.append(nn.Linear(in_channels, hidden_channels))
+                self.normalizations.append(nn.BatchNorm1d(hidden_channels))
+                for _ in range(num_layers - 2):
+                    self.lins.append(nn.Linear(hidden_channels, hidden_channels))
+                    self.normalizations.append(nn.BatchNorm1d(hidden_channels))
+                self.lins.append(nn.Linear(hidden_channels, out_channels))
+        elif normalization == "ln":
+            if num_layers == 1:
+                # just linear layer i.e. logistic regression
+                if InputNorm:
+                    self.normalizations.append(nn.LayerNorm(in_channels))
+                else:
+                    self.normalizations.append(nn.Identity())
+                self.lins.append(nn.Linear(in_channels, out_channels))
+            else:
+                if InputNorm:
+                    self.normalizations.append(nn.LayerNorm(in_channels))
+                else:
+                    self.normalizations.append(nn.Identity())
+                self.lins.append(nn.Linear(in_channels, hidden_channels))
+                self.normalizations.append(nn.LayerNorm(hidden_channels))
+                for _ in range(num_layers - 2):
+                    self.lins.append(nn.Linear(hidden_channels, hidden_channels))
+                    self.normalizations.append(nn.LayerNorm(hidden_channels))
+                self.lins.append(nn.Linear(hidden_channels, out_channels))
+        else:
+            if num_layers == 1:
+                # just linear layer i.e. logistic regression
+                self.normalizations.append(nn.Identity())
+                self.lins.append(nn.Linear(in_channels, out_channels))
+            else:
+                self.normalizations.append(nn.Identity())
+                self.lins.append(nn.Linear(in_channels, hidden_channels))
+                self.normalizations.append(nn.Identity())
+                for _ in range(num_layers - 2):
+                    self.lins.append(nn.Linear(hidden_channels, hidden_channels))
+                    self.normalizations.append(nn.Identity())
+                self.lins.append(nn.Linear(hidden_channels, out_channels))
+
+        self.dropout = dropout
+
+    def reset_parameters(self):
+        for lin in self.lins:
+            lin.reset_parameters()
+        for normalization in self.normalizations:
+            if not (normalization.__class__.__name__ is "Identity"):
+                normalization.reset_parameters()
+
+    def forward(self, x):
+        x = self.normalizations[0](x)
+        for i, lin in enumerate(self.lins[:-1]):
+            x = lin(x)
+            x = F.relu(x, inplace=True)
+            x = self.normalizations[i + 1](x)
+            x = F.dropout(x, p=self.dropout, training=self.training)
+        x = self.lins[-1](x)
+        return x
```

## easygraph/nn/convs/pma.py

 * *Ordering differences only*

```diff
@@ -1,180 +1,180 @@
-import math
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-from easygraph.nn.convs.common import MLP
-from torch import Tensor
-from torch.nn import Linear
-from torch.nn import Parameter
-from torch_geometric.nn.conv import MessagePassing
-from torch_geometric.typing import Adj
-from torch_geometric.typing import OptTensor
-from torch_geometric.typing import Size
-from torch_geometric.typing import SparseTensor
-from torch_geometric.utils import softmax
-from torch_scatter import scatter
-
-
-def glorot(tensor):
-    if tensor is not None:
-        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))
-        tensor.data.uniform_(-stdv, stdv)
-
-
-def zeros(tensor):
-    if tensor is not None:
-        tensor.data.fill_(0)
-
-
-class PMA(MessagePassing):
-    """
-    PMA part:
-    Note that in original PMA, we need to compute the inner product of the seed and neighbor nodes.
-    i.e. e_ij = a(Wh_i,Wh_j), where a should be the inner product, h_i is the seed and h_j are neightbor nodes.
-    In GAT, a(x,y) = a^T[x||y]. We use the same logic.
-    """
-
-    _alpha: OptTensor
-
-    def __init__(
-        self,
-        in_channels,
-        hid_dim,
-        out_channels,
-        num_layers,
-        heads=1,
-        concat=True,
-        negative_slope=0.2,
-        dropout=0.0,
-        bias=False,
-        **kwargs,
-    ):
-        super(PMA, self).__init__(node_dim=0, **kwargs)
-
-        self.in_channels = in_channels
-        self.hidden = hid_dim // heads
-        self.out_channels = out_channels
-        self.heads = heads
-        self.concat = concat
-        self.negative_slope = negative_slope
-        self.dropout = dropout
-        self.aggr = "add"
-        #         self.input_seed = input_seed
-
-        #         This is the encoder part. Where we use 1 layer NN (Theta*x_i in the GATConv description)
-        #         Now, no seed as input. Directly learn the importance weights alpha_ij.
-        #         self.lin_O = Linear(heads*self.hidden, self.hidden) # For heads combining
-        # For neighbor nodes (source side, key)
-        self.lin_K = Linear(in_channels, self.heads * self.hidden)
-        # For neighbor nodes (source side, value)
-        self.lin_V = Linear(in_channels, self.heads * self.hidden)
-        self.att_r = Parameter(torch.Tensor(1, heads, self.hidden))  # Seed vector
-        self.rFF = MLP(
-            in_channels=self.heads * self.hidden,
-            hidden_channels=self.heads * self.hidden,
-            out_channels=out_channels,
-            num_layers=num_layers,
-            dropout=0.0,
-            normalization="None",
-        )
-        self.ln0 = nn.LayerNorm(self.heads * self.hidden)
-        self.ln1 = nn.LayerNorm(self.heads * self.hidden)
-        #         if bias and concat:
-        #             self.bias = Parameter(torch.Tensor(heads * out_channels))
-        #         elif bias and not concat:
-        #             self.bias = Parameter(torch.Tensor(out_channels))
-        #         else:
-
-        #         Always no bias! (For now)
-        self.register_parameter("bias", None)
-
-        self._alpha = None
-
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        glorot(self.lin_K.weight)
-        glorot(self.lin_V.weight)
-        self.rFF.reset_parameters()
-        self.ln0.reset_parameters()
-        self.ln1.reset_parameters()
-        #         glorot(self.att_l)
-        nn.init.xavier_uniform_(self.att_r)
-
-    #         zeros(self.bias)
-
-    def forward(
-        self, x, edge_index: Adj, size: Size = None, return_attention_weights=None
-    ):
-        r"""
-        Args:
-            return_attention_weights (bool, optional): If set to :obj:`True`,
-                will additionally return the tuple
-                :obj:`(edge_index, attention_weights)`, holding the computed
-                attention weights for each edge. (default: :obj:`None`)
-        """
-        H, C = self.heads, self.hidden
-
-        x_l: OptTensor = None
-        x_r: OptTensor = None
-        alpha_l: OptTensor = None
-        alpha_r: OptTensor = None
-        if isinstance(x, Tensor):
-            assert x.dim() == 2, "Static graphs not supported in `GATConv`."
-            x_K = self.lin_K(x).view(-1, H, C)
-            x_V = self.lin_V(x).view(-1, H, C)
-            alpha_r = (x_K * self.att_r).sum(dim=-1)
-
-        out = self.propagate(edge_index, x=x_V, alpha=alpha_r, aggr=self.aggr)
-
-        alpha = self._alpha
-        self._alpha = None
-
-        #         Note that in the original code of GMT paper, they do not use additional W^O to combine heads.
-        #         This is because O = softmax(QK^T)V and V = V_in*W^V. So W^O can be effectively taken care by W^V!!!
-        out += self.att_r  # This is Seed + Multihead
-        # concat heads then LayerNorm. Z (rhs of Eq(7)) in GMT paper.
-        out = self.ln0(out.view(-1, self.heads * self.hidden))
-        # rFF and skip connection. Lhs of eq(7) in GMT paper.
-        out = self.ln1(out + F.relu(self.rFF(out)))
-
-        if isinstance(return_attention_weights, bool):
-            assert alpha is not None
-            if isinstance(edge_index, Tensor):
-                return out, (edge_index, alpha)
-            elif isinstance(edge_index, SparseTensor):
-                return out, edge_index.set_value(alpha, layout="coo")
-        else:
-            return out
-
-    def message(self, x_j, alpha_j, index, ptr, size_j):
-        #         ipdb.set_trace()
-        alpha = alpha_j
-        alpha = F.leaky_relu(alpha, self.negative_slope)
-        alpha = softmax(alpha, index, ptr, index.max() + 1)
-        self._alpha = alpha
-        alpha = F.dropout(alpha, p=self.dropout, training=self.training)
-        return x_j * alpha.unsqueeze(-1)
-
-    def aggregate(self, inputs, index, dim_size=None, aggr="add"):
-        r"""Aggregates messages from neighbors as
-        :math:`\square_{j \in \mathcal{N}(i)}`.
-
-        Takes in the output of message computation as first argument and any
-        argument which was initially passed to :meth:`propagate`.
-
-        By default, this function will delegate its call to scatter functions
-        that support "add", "mean" and "max" operations as specified in
-        :meth:`__init__` by the :obj:`aggr` argument.
-        """
-        #         ipdb.set_trace()
-        if aggr is None:
-            raise ValueError("aggr was not passed!")
-        return scatter(inputs, index, dim=self.node_dim, reduce=aggr)
-
-    def __repr__(self):
-        return "{}({}, {}, heads={})".format(
-            self.__class__.__name__, self.in_channels, self.out_channels, self.heads
-        )
+import math
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+from easygraph.nn.convs.common import MLP
+from torch import Tensor
+from torch.nn import Linear
+from torch.nn import Parameter
+from torch_geometric.nn.conv import MessagePassing
+from torch_geometric.typing import Adj
+from torch_geometric.typing import OptTensor
+from torch_geometric.typing import Size
+from torch_geometric.typing import SparseTensor
+from torch_geometric.utils import softmax
+from torch_scatter import scatter
+
+
+def glorot(tensor):
+    if tensor is not None:
+        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))
+        tensor.data.uniform_(-stdv, stdv)
+
+
+def zeros(tensor):
+    if tensor is not None:
+        tensor.data.fill_(0)
+
+
+class PMA(MessagePassing):
+    """
+    PMA part:
+    Note that in original PMA, we need to compute the inner product of the seed and neighbor nodes.
+    i.e. e_ij = a(Wh_i,Wh_j), where a should be the inner product, h_i is the seed and h_j are neightbor nodes.
+    In GAT, a(x,y) = a^T[x||y]. We use the same logic.
+    """
+
+    _alpha: OptTensor
+
+    def __init__(
+        self,
+        in_channels,
+        hid_dim,
+        out_channels,
+        num_layers,
+        heads=1,
+        concat=True,
+        negative_slope=0.2,
+        dropout=0.0,
+        bias=False,
+        **kwargs,
+    ):
+        super(PMA, self).__init__(node_dim=0, **kwargs)
+
+        self.in_channels = in_channels
+        self.hidden = hid_dim // heads
+        self.out_channels = out_channels
+        self.heads = heads
+        self.concat = concat
+        self.negative_slope = negative_slope
+        self.dropout = dropout
+        self.aggr = "add"
+        #         self.input_seed = input_seed
+
+        #         This is the encoder part. Where we use 1 layer NN (Theta*x_i in the GATConv description)
+        #         Now, no seed as input. Directly learn the importance weights alpha_ij.
+        #         self.lin_O = Linear(heads*self.hidden, self.hidden) # For heads combining
+        # For neighbor nodes (source side, key)
+        self.lin_K = Linear(in_channels, self.heads * self.hidden)
+        # For neighbor nodes (source side, value)
+        self.lin_V = Linear(in_channels, self.heads * self.hidden)
+        self.att_r = Parameter(torch.Tensor(1, heads, self.hidden))  # Seed vector
+        self.rFF = MLP(
+            in_channels=self.heads * self.hidden,
+            hidden_channels=self.heads * self.hidden,
+            out_channels=out_channels,
+            num_layers=num_layers,
+            dropout=0.0,
+            normalization="None",
+        )
+        self.ln0 = nn.LayerNorm(self.heads * self.hidden)
+        self.ln1 = nn.LayerNorm(self.heads * self.hidden)
+        #         if bias and concat:
+        #             self.bias = Parameter(torch.Tensor(heads * out_channels))
+        #         elif bias and not concat:
+        #             self.bias = Parameter(torch.Tensor(out_channels))
+        #         else:
+
+        #         Always no bias! (For now)
+        self.register_parameter("bias", None)
+
+        self._alpha = None
+
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        glorot(self.lin_K.weight)
+        glorot(self.lin_V.weight)
+        self.rFF.reset_parameters()
+        self.ln0.reset_parameters()
+        self.ln1.reset_parameters()
+        #         glorot(self.att_l)
+        nn.init.xavier_uniform_(self.att_r)
+
+    #         zeros(self.bias)
+
+    def forward(
+        self, x, edge_index: Adj, size: Size = None, return_attention_weights=None
+    ):
+        r"""
+        Args:
+            return_attention_weights (bool, optional): If set to :obj:`True`,
+                will additionally return the tuple
+                :obj:`(edge_index, attention_weights)`, holding the computed
+                attention weights for each edge. (default: :obj:`None`)
+        """
+        H, C = self.heads, self.hidden
+
+        x_l: OptTensor = None
+        x_r: OptTensor = None
+        alpha_l: OptTensor = None
+        alpha_r: OptTensor = None
+        if isinstance(x, Tensor):
+            assert x.dim() == 2, "Static graphs not supported in `GATConv`."
+            x_K = self.lin_K(x).view(-1, H, C)
+            x_V = self.lin_V(x).view(-1, H, C)
+            alpha_r = (x_K * self.att_r).sum(dim=-1)
+
+        out = self.propagate(edge_index, x=x_V, alpha=alpha_r, aggr=self.aggr)
+
+        alpha = self._alpha
+        self._alpha = None
+
+        #         Note that in the original code of GMT paper, they do not use additional W^O to combine heads.
+        #         This is because O = softmax(QK^T)V and V = V_in*W^V. So W^O can be effectively taken care by W^V!!!
+        out += self.att_r  # This is Seed + Multihead
+        # concat heads then LayerNorm. Z (rhs of Eq(7)) in GMT paper.
+        out = self.ln0(out.view(-1, self.heads * self.hidden))
+        # rFF and skip connection. Lhs of eq(7) in GMT paper.
+        out = self.ln1(out + F.relu(self.rFF(out)))
+
+        if isinstance(return_attention_weights, bool):
+            assert alpha is not None
+            if isinstance(edge_index, Tensor):
+                return out, (edge_index, alpha)
+            elif isinstance(edge_index, SparseTensor):
+                return out, edge_index.set_value(alpha, layout="coo")
+        else:
+            return out
+
+    def message(self, x_j, alpha_j, index, ptr, size_j):
+        #         ipdb.set_trace()
+        alpha = alpha_j
+        alpha = F.leaky_relu(alpha, self.negative_slope)
+        alpha = softmax(alpha, index, ptr, index.max() + 1)
+        self._alpha = alpha
+        alpha = F.dropout(alpha, p=self.dropout, training=self.training)
+        return x_j * alpha.unsqueeze(-1)
+
+    def aggregate(self, inputs, index, dim_size=None, aggr="add"):
+        r"""Aggregates messages from neighbors as
+        :math:`\square_{j \in \mathcal{N}(i)}`.
+
+        Takes in the output of message computation as first argument and any
+        argument which was initially passed to :meth:`propagate`.
+
+        By default, this function will delegate its call to scatter functions
+        that support "add", "mean" and "max" operations as specified in
+        :meth:`__init__` by the :obj:`aggr` argument.
+        """
+        #         ipdb.set_trace()
+        if aggr is None:
+            raise ValueError("aggr was not passed!")
+        return scatter(inputs, index, dim=self.node_dim, reduce=aggr)
+
+    def __repr__(self):
+        return "{}({}, {}, heads={})".format(
+            self.__class__.__name__, self.in_channels, self.out_channels, self.heads
+        )
```

## easygraph/nn/convs/__init__.py

```diff
@@ -1 +1 @@
-00000000: 0a                                       .
+00000000: 0d0a                                     ..
```

## easygraph/nn/convs/hypergraphs/unignn_conv.py

 * *Ordering differences only*

```diff
@@ -1,290 +1,290 @@
-import torch
-import torch.nn as nn
-
-from easygraph.classes import Hypergraph
-
-
-class UniGCNConv(nn.Module):
-    r"""The UniGCN convolution layer proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
-
-    Sparse Format:
-
-    .. math::
-        \left\{
-            \begin{aligned}
-            h_{e} &= \frac{1}{|e|} \sum_{j \in e} x_{j} \\
-            \tilde{x}_{i} &= \frac{1}{\sqrt{d_{i}}} \sum_{e \in \tilde{E}_{i}} \frac{1}{\sqrt{\tilde{d}_{e}}} W h_{e}
-            \end{aligned}
-        \right. .
-
-    where :math:`\tilde{d}_{e} = \frac{1}{|e|} \sum_{i \in e} d_{i}`.
-
-    Matrix Format:
-
-    .. math::
-        \mathbf{X}^{\prime} = \sigma \left(  \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \tilde{\mathbf{D}}_e^{-\frac{1}{2}} \cdot \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{X} \mathbf{\Theta} \right) .
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
-        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
-        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
-        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        out_channels: int,
-        bias: bool = True,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-        is_last: bool = False,
-    ):
-        super().__init__()
-        self.is_last = is_last
-        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
-        self.act = nn.ReLU(inplace=True)
-        self.drop = nn.Dropout(drop_rate)
-        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
-
-    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\mathcal{V}|, C_{in})`.
-            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`|\mathcal{V}|` vertices.
-        """
-        X = self.theta(X)
-        # import time
-        # start = time.time()
-        Y = hg.v2e(X, aggr="mean")
-        # end = time.time()
-        # print("eg v2e:",end-start)
-        # ===============================================
-        # compute the special degree of hyperedges
-        _De = torch.zeros(hg.num_e, device=hg.device)
-        # scatter_reduce() is relay on the torch 1.12.1, which may be updated in the future
-        # print("hg.v2e_dst:",hg.v2e_dst)
-        # start = time.time()
-        _De = _De.scatter_reduce(
-            0, index=hg.v2e_dst, src=hg.D_v._values()[hg.v2e_src], reduce="mean"
-        )
-        # end = time.time()
-        # print("eg scatter:",end-start)
-        _De = _De.pow(-0.5)
-        _De[_De.isinf()] = 1
-        Y = _De.view(-1, 1) * Y
-        # ===============================================
-        X = hg.e2v(Y, aggr="sum")
-        X = torch.sparse.mm(hg.D_v_neg_1_2, X)
-
-        if not self.is_last:
-            X = self.act(X)
-            if self.bn is not None:
-                X = self.bn(X)
-            X = self.drop(X)
-        return X
-
-
-class UniGATConv(nn.Module):
-    r"""The UniGAT convolution layer proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
-
-    Sparse Format:
-
-    .. math::
-        \left\{
-            \begin{aligned}
-                \alpha_{i e} &=\sigma\left(a^{T}\left[W h_{\{i\}} ; W h_{e}\right]\right) \\
-                \tilde{\alpha}_{i e} &=\frac{\exp \left(\alpha_{i e}\right)}{\sum_{e^{\prime} \in \tilde{E}_{i}} \exp \left(\alpha_{i e^{\prime}}\right)} \\
-                \tilde{x}_{i} &=\sum_{e \in \tilde{E}_{i}} \tilde{\alpha}_{i e} W h_{e}
-            \end{aligned}
-        \right. .
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
-        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
-        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``): The dropout probability. If ``dropout <= 0``, the layer will not drop values. Defaults to ``0.5``.
-        ``atten_neg_slope`` (``float``): Hyper-parameter of the ``LeakyReLU`` activation of edge attention. Defaults to ``0.2``.
-        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        out_channels: int,
-        bias: bool = True,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-        atten_neg_slope: float = 0.2,
-        is_last: bool = False,
-    ):
-        super().__init__()
-        self.is_last = is_last
-        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
-        self.atten_dropout = nn.Dropout(drop_rate)
-        self.atten_act = nn.LeakyReLU(atten_neg_slope)
-        self.act = nn.ELU(inplace=True)
-        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
-        self.atten_e = nn.Linear(out_channels, 1, bias=False)
-        self.atten_dst = nn.Linear(out_channels, 1, bias=False)
-
-    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\mathcal{V}|, C_{in})`.
-            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`|\mathcal{V}|` vertices.
-        """
-        X = self.theta(X)
-        Y = hg.v2e(X, aggr="mean")
-        # ===============================================
-        alpha_e = self.atten_e(Y)
-        e_atten_score = alpha_e[hg.e2v_src]
-        e_atten_score = self.atten_dropout(self.atten_act(e_atten_score).squeeze())
-        # ================================================================================
-        # We suggest to add a clamp on attention weight to avoid Nan error in training.
-        e_atten_score.clamp_(min=0.001, max=5)
-        # e_atten_score = torch.clamp(e_atten_score, min=0.001, max=5)
-        # ================================================================================
-        X = hg.e2v(Y, aggr="softmax_then_sum", e2v_weight=e_atten_score)
-
-        if not self.is_last:
-            X = self.act(X)
-            if self.bn is not None:
-                X = self.bn(X)
-        return X
-
-
-class UniSAGEConv(nn.Module):
-    r"""The UniSAGE convolution layer proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
-
-    Sparse Format:
-
-    .. math::
-        \left\{
-            \begin{aligned}
-            h_{e} &= \frac{1}{|e|} \sum_{j \in e} x_{j} \\
-            \tilde{x}_{i} &= W\left(x_{i}+\text { AGGREGATE }\left(\left\{x_{j}\right\}_{j \in \mathcal{N}_{i}}\right)\right)
-            \end{aligned}
-        \right. .
-
-    Matrix Format:
-
-    .. math::
-        \mathbf{X}^{\prime} = \sigma \left( \left( \mathbf{I} + \mathbf{H} \mathbf{D}_e^{-1} \mathbf{H}^\top \right) \mathbf{X} \mathbf{\Theta} \right) .
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
-        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
-        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
-        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        out_channels: int,
-        bias: bool = True,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-        is_last: bool = False,
-    ):
-        super().__init__()
-        self.is_last = is_last
-        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
-        self.act = nn.ReLU(inplace=True)
-        self.drop = nn.Dropout(drop_rate)
-        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
-
-    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\mathcal{V}|, C_{in})`.
-            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`|\mathcal{V}|` vertices.
-        """
-        X = self.theta(X)
-        Y = hg.v2e(X, aggr="mean")
-        X = hg.e2v(Y, aggr="sum") + X
-        if not self.is_last:
-            X = self.act(X)
-            if self.bn is not None:
-                X = self.bn(X)
-            X = self.drop(X)
-        return X
-
-
-class UniGINConv(nn.Module):
-    r"""The UniGIN convolution layer proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
-
-    Sparse Format:
-
-    .. math::
-
-        \left\{
-            \begin{aligned}
-            h_{e} &= \frac{1}{|e|} \sum_{j \in e} x_{j} \\
-            \tilde{x}_{i} &= W\left((1+\varepsilon) x_{i}+\sum_{e \in E_{i}} h_{e}\right)
-            \end{aligned}
-        \right. .
-
-    Matrix Format:
-
-    .. math::
-        \mathbf{X}^{\prime} = \sigma \left( \left( \left( \mathbf{I} + \varepsilon \right) + \mathbf{H} \mathbf{D}_e^{-1} \mathbf{H}^\top \right) \mathbf{X} \mathbf{\Theta} \right) .
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
-        ``eps`` (``float``): :math:`\varepsilon` is the learnable parameter. Defaults to ``0.0``.
-        ``train_eps`` (``bool``): If set to ``True``, the layer will learn the :math:`\varepsilon` parameter. Defaults to ``False``.
-        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
-        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
-        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        out_channels: int,
-        eps: float = 0.0,
-        train_eps: bool = False,
-        bias: bool = True,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-        is_last: bool = False,
-    ):
-        super().__init__()
-        self.is_last = is_last
-        if train_eps:
-            self.eps = nn.Parameter(torch.tensor([eps]))
-        else:
-            self.eps = eps
-        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
-        self.act = nn.ReLU(inplace=True)
-        self.drop = nn.Dropout(drop_rate)
-        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
-
-    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\mathcal{V}|, C_{in})`.
-            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`|\mathcal{V}|` vertices.
-        """
-        X = self.theta(X)
-        Y = hg.v2e(X, aggr="mean")
-        X = (1 + self.eps) * hg.e2v(Y, aggr="sum") + X
-        if not self.is_last:
-            X = self.act(X)
-            if self.bn is not None:
-                X = self.bn(X)
-            X = self.drop(X)
-        return X
+import torch
+import torch.nn as nn
+
+from easygraph.classes import Hypergraph
+
+
+class UniGCNConv(nn.Module):
+    r"""The UniGCN convolution layer proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
+
+    Sparse Format:
+
+    .. math::
+        \left\{
+            \begin{aligned}
+            h_{e} &= \frac{1}{|e|} \sum_{j \in e} x_{j} \\
+            \tilde{x}_{i} &= \frac{1}{\sqrt{d_{i}}} \sum_{e \in \tilde{E}_{i}} \frac{1}{\sqrt{\tilde{d}_{e}}} W h_{e}
+            \end{aligned}
+        \right. .
+
+    where :math:`\tilde{d}_{e} = \frac{1}{|e|} \sum_{i \in e} d_{i}`.
+
+    Matrix Format:
+
+    .. math::
+        \mathbf{X}^{\prime} = \sigma \left(  \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \tilde{\mathbf{D}}_e^{-\frac{1}{2}} \cdot \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{X} \mathbf{\Theta} \right) .
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
+        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
+        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
+        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        bias: bool = True,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+        is_last: bool = False,
+    ):
+        super().__init__()
+        self.is_last = is_last
+        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
+        self.act = nn.ReLU(inplace=True)
+        self.drop = nn.Dropout(drop_rate)
+        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
+
+    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\mathcal{V}|, C_{in})`.
+            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`|\mathcal{V}|` vertices.
+        """
+        X = self.theta(X)
+        # import time
+        # start = time.time()
+        Y = hg.v2e(X, aggr="mean")
+        # end = time.time()
+        # print("eg v2e:",end-start)
+        # ===============================================
+        # compute the special degree of hyperedges
+        _De = torch.zeros(hg.num_e, device=hg.device)
+        # scatter_reduce() is relay on the torch 1.12.1, which may be updated in the future
+        # print("hg.v2e_dst:",hg.v2e_dst)
+        # start = time.time()
+        _De = _De.scatter_reduce(
+            0, index=hg.v2e_dst, src=hg.D_v._values()[hg.v2e_src], reduce="mean"
+        )
+        # end = time.time()
+        # print("eg scatter:",end-start)
+        _De = _De.pow(-0.5)
+        _De[_De.isinf()] = 1
+        Y = _De.view(-1, 1) * Y
+        # ===============================================
+        X = hg.e2v(Y, aggr="sum")
+        X = torch.sparse.mm(hg.D_v_neg_1_2, X)
+
+        if not self.is_last:
+            X = self.act(X)
+            if self.bn is not None:
+                X = self.bn(X)
+            X = self.drop(X)
+        return X
+
+
+class UniGATConv(nn.Module):
+    r"""The UniGAT convolution layer proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
+
+    Sparse Format:
+
+    .. math::
+        \left\{
+            \begin{aligned}
+                \alpha_{i e} &=\sigma\left(a^{T}\left[W h_{\{i\}} ; W h_{e}\right]\right) \\
+                \tilde{\alpha}_{i e} &=\frac{\exp \left(\alpha_{i e}\right)}{\sum_{e^{\prime} \in \tilde{E}_{i}} \exp \left(\alpha_{i e^{\prime}}\right)} \\
+                \tilde{x}_{i} &=\sum_{e \in \tilde{E}_{i}} \tilde{\alpha}_{i e} W h_{e}
+            \end{aligned}
+        \right. .
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
+        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
+        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``): The dropout probability. If ``dropout <= 0``, the layer will not drop values. Defaults to ``0.5``.
+        ``atten_neg_slope`` (``float``): Hyper-parameter of the ``LeakyReLU`` activation of edge attention. Defaults to ``0.2``.
+        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        bias: bool = True,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+        atten_neg_slope: float = 0.2,
+        is_last: bool = False,
+    ):
+        super().__init__()
+        self.is_last = is_last
+        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
+        self.atten_dropout = nn.Dropout(drop_rate)
+        self.atten_act = nn.LeakyReLU(atten_neg_slope)
+        self.act = nn.ELU(inplace=True)
+        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
+        self.atten_e = nn.Linear(out_channels, 1, bias=False)
+        self.atten_dst = nn.Linear(out_channels, 1, bias=False)
+
+    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\mathcal{V}|, C_{in})`.
+            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`|\mathcal{V}|` vertices.
+        """
+        X = self.theta(X)
+        Y = hg.v2e(X, aggr="mean")
+        # ===============================================
+        alpha_e = self.atten_e(Y)
+        e_atten_score = alpha_e[hg.e2v_src]
+        e_atten_score = self.atten_dropout(self.atten_act(e_atten_score).squeeze())
+        # ================================================================================
+        # We suggest to add a clamp on attention weight to avoid Nan error in training.
+        e_atten_score.clamp_(min=0.001, max=5)
+        # e_atten_score = torch.clamp(e_atten_score, min=0.001, max=5)
+        # ================================================================================
+        X = hg.e2v(Y, aggr="softmax_then_sum", e2v_weight=e_atten_score)
+
+        if not self.is_last:
+            X = self.act(X)
+            if self.bn is not None:
+                X = self.bn(X)
+        return X
+
+
+class UniSAGEConv(nn.Module):
+    r"""The UniSAGE convolution layer proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
+
+    Sparse Format:
+
+    .. math::
+        \left\{
+            \begin{aligned}
+            h_{e} &= \frac{1}{|e|} \sum_{j \in e} x_{j} \\
+            \tilde{x}_{i} &= W\left(x_{i}+\text { AGGREGATE }\left(\left\{x_{j}\right\}_{j \in \mathcal{N}_{i}}\right)\right)
+            \end{aligned}
+        \right. .
+
+    Matrix Format:
+
+    .. math::
+        \mathbf{X}^{\prime} = \sigma \left( \left( \mathbf{I} + \mathbf{H} \mathbf{D}_e^{-1} \mathbf{H}^\top \right) \mathbf{X} \mathbf{\Theta} \right) .
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
+        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
+        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
+        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        bias: bool = True,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+        is_last: bool = False,
+    ):
+        super().__init__()
+        self.is_last = is_last
+        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
+        self.act = nn.ReLU(inplace=True)
+        self.drop = nn.Dropout(drop_rate)
+        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
+
+    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\mathcal{V}|, C_{in})`.
+            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`|\mathcal{V}|` vertices.
+        """
+        X = self.theta(X)
+        Y = hg.v2e(X, aggr="mean")
+        X = hg.e2v(Y, aggr="sum") + X
+        if not self.is_last:
+            X = self.act(X)
+            if self.bn is not None:
+                X = self.bn(X)
+            X = self.drop(X)
+        return X
+
+
+class UniGINConv(nn.Module):
+    r"""The UniGIN convolution layer proposed in `UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks <https://arxiv.org/pdf/2105.00956.pdf>`_ paper (IJCAI 2021).
+
+    Sparse Format:
+
+    .. math::
+
+        \left\{
+            \begin{aligned}
+            h_{e} &= \frac{1}{|e|} \sum_{j \in e} x_{j} \\
+            \tilde{x}_{i} &= W\left((1+\varepsilon) x_{i}+\sum_{e \in E_{i}} h_{e}\right)
+            \end{aligned}
+        \right. .
+
+    Matrix Format:
+
+    .. math::
+        \mathbf{X}^{\prime} = \sigma \left( \left( \left( \mathbf{I} + \varepsilon \right) + \mathbf{H} \mathbf{D}_e^{-1} \mathbf{H}^\top \right) \mathbf{X} \mathbf{\Theta} \right) .
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
+        ``eps`` (``float``): :math:`\varepsilon` is the learnable parameter. Defaults to ``0.0``.
+        ``train_eps`` (``bool``): If set to ``True``, the layer will learn the :math:`\varepsilon` parameter. Defaults to ``False``.
+        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
+        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
+        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        eps: float = 0.0,
+        train_eps: bool = False,
+        bias: bool = True,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+        is_last: bool = False,
+    ):
+        super().__init__()
+        self.is_last = is_last
+        if train_eps:
+            self.eps = nn.Parameter(torch.tensor([eps]))
+        else:
+            self.eps = eps
+        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
+        self.act = nn.ReLU(inplace=True)
+        self.drop = nn.Dropout(drop_rate)
+        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
+
+    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\mathcal{V}|, C_{in})`.
+            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`|\mathcal{V}|` vertices.
+        """
+        X = self.theta(X)
+        Y = hg.v2e(X, aggr="mean")
+        X = (1 + self.eps) * hg.e2v(Y, aggr="sum") + X
+        if not self.is_last:
+            X = self.act(X)
+            if self.bn is not None:
+                X = self.bn(X)
+            X = self.drop(X)
+        return X
```

## easygraph/nn/convs/hypergraphs/hgnn_conv.py

 * *Ordering differences only*

```diff
@@ -1,72 +1,72 @@
-import torch
-import torch.nn as nn
-
-from easygraph.classes import Hypergraph
-
-
-class HGNNConv(nn.Module):
-    r"""The HGNN convolution layer proposed in `Hypergraph Neural Networks <https://arxiv.org/pdf/1809.09401>`_ paper (AAAI 2019).
-    Matrix Format:
-
-    .. math::
-        \mathbf{X}^{\prime} = \sigma \left( \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1}
-        \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}} \mathbf{X} \mathbf{\Theta} \right).
-
-    where :math:`\mathbf{X}` is the input vertex feature matrix, :math:`\mathbf{H}` is the hypergraph incidence matrix,
-    :math:`\mathbf{W}_e` is a diagonal hyperedge weight matrix, :math:`\mathbf{D}_v` is a diagonal vertex degree matrix,
-    :math:`\mathbf{D}_e` is a diagonal hyperedge degree matrix, :math:`\mathbf{\Theta}` is the learnable parameters.
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
-        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
-        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
-        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        out_channels: int,
-        bias: bool = True,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-        is_last: bool = False,
-    ):
-        super().__init__()
-        self.is_last = is_last
-        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
-        self.act = nn.ReLU(inplace=True)
-        self.drop = nn.Dropout(drop_rate)
-        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
-
-        # self.Theta1 = nn.Linear(in_size, hidden_dims)
-        # self.Theta2 = nn.Linear(hidden_dims, out_size)
-        # self.dropout = nn.Dropout(0.5)
-        #
-        # ###########################################################
-        # # (HIGHLIGHT) Compute the Laplacian with Sparse Matrix API
-        # ###########################################################
-        # d_V = H.sum(1)  # node degree
-        # d_E = H.sum(0)  # edge degree
-        # n_edges = d_E.shape[0]
-        # D_V_invsqrt = dglsp.diag(d_V ** -0.5)  # D_V ** (-1/2)
-        # D_E_inv = dglsp.diag(d_E ** -1)  # D_E ** (-1)
-        # W = dglsp.identity((n_edges, n_edges))
-        # self.laplacian = D_V_invsqrt @ H @ W @ D_E_inv @ H.T @ D_V_invsqrt
-
-    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
-            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
-        """
-        X = self.theta(X)
-        if self.bn is not None:
-            X = self.bn(X)
-        X = hg.smoothing_with_HGNN(X)
-        if not self.is_last:
-            X = self.drop(self.act(X))
-        return X
+import torch
+import torch.nn as nn
+
+from easygraph.classes import Hypergraph
+
+
+class HGNNConv(nn.Module):
+    r"""The HGNN convolution layer proposed in `Hypergraph Neural Networks <https://arxiv.org/pdf/1809.09401>`_ paper (AAAI 2019).
+    Matrix Format:
+
+    .. math::
+        \mathbf{X}^{\prime} = \sigma \left( \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1}
+        \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}} \mathbf{X} \mathbf{\Theta} \right).
+
+    where :math:`\mathbf{X}` is the input vertex feature matrix, :math:`\mathbf{H}` is the hypergraph incidence matrix,
+    :math:`\mathbf{W}_e` is a diagonal hyperedge weight matrix, :math:`\mathbf{D}_v` is a diagonal vertex degree matrix,
+    :math:`\mathbf{D}_e` is a diagonal hyperedge degree matrix, :math:`\mathbf{\Theta}` is the learnable parameters.
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
+        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
+        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
+        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        bias: bool = True,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+        is_last: bool = False,
+    ):
+        super().__init__()
+        self.is_last = is_last
+        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
+        self.act = nn.ReLU(inplace=True)
+        self.drop = nn.Dropout(drop_rate)
+        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
+
+        # self.Theta1 = nn.Linear(in_size, hidden_dims)
+        # self.Theta2 = nn.Linear(hidden_dims, out_size)
+        # self.dropout = nn.Dropout(0.5)
+        #
+        # ###########################################################
+        # # (HIGHLIGHT) Compute the Laplacian with Sparse Matrix API
+        # ###########################################################
+        # d_V = H.sum(1)  # node degree
+        # d_E = H.sum(0)  # edge degree
+        # n_edges = d_E.shape[0]
+        # D_V_invsqrt = dglsp.diag(d_V ** -0.5)  # D_V ** (-1/2)
+        # D_E_inv = dglsp.diag(d_E ** -1)  # D_E ** (-1)
+        # W = dglsp.identity((n_edges, n_edges))
+        # self.laplacian = D_V_invsqrt @ H @ W @ D_E_inv @ H.T @ D_V_invsqrt
+
+    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
+            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
+        """
+        X = self.theta(X)
+        if self.bn is not None:
+            X = self.bn(X)
+        X = hg.smoothing_with_HGNN(X)
+        if not self.is_last:
+            X = self.drop(self.act(X))
+        return X
```

## easygraph/nn/convs/hypergraphs/hnhn_conv.py

 * *Ordering differences only*

```diff
@@ -1,53 +1,53 @@
-import torch
-import torch.nn as nn
-
-from easygraph.classes import Hypergraph
-
-
-class HNHNConv(nn.Module):
-    r"""The HNHN convolution layer proposed in `HNHN: Hypergraph Networks with Hyperedge Neurons <https://arxiv.org/pdf/2006.12278.pdf>`_ paper (ICML 2020).
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
-        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
-        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
-        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        out_channels: int,
-        bias: bool = True,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-        is_last: bool = False,
-    ):
-        super().__init__()
-        self.is_last = is_last
-        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
-        self.act = nn.ReLU(inplace=True)
-        self.drop = nn.Dropout(drop_rate)
-        self.theta_v2e = nn.Linear(in_channels, out_channels, bias=bias)
-        self.theta_e2v = nn.Linear(out_channels, out_channels, bias=bias)
-
-    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\mathcal{V}|, C_{in})`.
-            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`|\mathcal{V}|` vertices.
-        """
-        # v -> e
-        X = self.theta_v2e(X)
-        if self.bn is not None:
-            X = self.bn(X)
-        Y = self.act(hg.v2e(X, aggr="mean"))
-        # e -> v
-        Y = self.theta_e2v(Y)
-        X = hg.e2v(Y, aggr="mean")
-        if not self.is_last:
-            X = self.drop(self.act(X))
-        return X
+import torch
+import torch.nn as nn
+
+from easygraph.classes import Hypergraph
+
+
+class HNHNConv(nn.Module):
+    r"""The HNHN convolution layer proposed in `HNHN: Hypergraph Networks with Hyperedge Neurons <https://arxiv.org/pdf/2006.12278.pdf>`_ paper (ICML 2020).
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
+        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
+        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
+        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        bias: bool = True,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+        is_last: bool = False,
+    ):
+        super().__init__()
+        self.is_last = is_last
+        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
+        self.act = nn.ReLU(inplace=True)
+        self.drop = nn.Dropout(drop_rate)
+        self.theta_v2e = nn.Linear(in_channels, out_channels, bias=bias)
+        self.theta_e2v = nn.Linear(out_channels, out_channels, bias=bias)
+
+    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\mathcal{V}|, C_{in})`.
+            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`|\mathcal{V}|` vertices.
+        """
+        # v -> e
+        X = self.theta_v2e(X)
+        if self.bn is not None:
+            X = self.bn(X)
+        Y = self.act(hg.v2e(X, aggr="mean"))
+        # e -> v
+        Y = self.theta_e2v(Y)
+        X = hg.e2v(Y, aggr="mean")
+        if not self.is_last:
+            X = self.drop(self.act(X))
+        return X
```

## easygraph/nn/convs/hypergraphs/hgnnp_conv.py

 * *Ordering differences only*

```diff
@@ -1,67 +1,67 @@
-import torch
-import torch.nn as nn
-
-from easygraph.classes import Hypergraph
-
-
-class HGNNPConv(nn.Module):
-    r"""The HGNN :sup:`+` convolution layer proposed in `HGNN+: General Hypergraph Neural Networks <https://ieeexplore.ieee.org/document/9795251>`_ paper (IEEE T-PAMI 2022).
-
-    Sparse Format:
-    
-    .. math::
-
-        \left\{
-            \begin{aligned}
-                m_{\beta}^{t} &=\sum_{\alpha \in \mathcal{N}_{v}(\beta)} M_{v}^{t}\left(x_{\alpha}^{t}\right) \\
-                y_{\beta}^{t} &=U_{e}^{t}\left(w_{\beta}, m_{\beta}^{t}\right) \\
-                m_{\alpha}^{t+1} &=\sum_{\beta \in \mathcal{N}_{e}(\alpha)} M_{e}^{t}\left(x_{\alpha}^{t}, y_{\beta}^{t}\right) \\
-                x_{\alpha}^{t+1} &=U_{v}^{t}\left(x_{\alpha}^{t}, m_{\alpha}^{t+1}\right) \\
-            \end{aligned}
-        \right.
-
-    Matrix Format:
-
-    .. math::
-        \mathbf{X}^{\prime} = \sigma \left( \mathbf{D}_v^{-1} \mathbf{H} \mathbf{W}_e 
-        \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{X} \mathbf{\Theta} \right).
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
-        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
-        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
-        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        out_channels: int,
-        bias: bool = True,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-        is_last: bool = False,
-    ):
-        super().__init__()
-        self.is_last = is_last
-        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
-        self.act = nn.ReLU(inplace=True)
-        self.drop = nn.Dropout(drop_rate)
-        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
-
-    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\mathcal{V}|, C_{in})`.
-            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`|\mathcal{V}|` vertices.
-        """
-        X = self.theta(X)
-        if self.bn is not None:
-            X = self.bn(X)
-        X = hg.v2v(X, aggr="mean")
-        if not self.is_last:
-            X = self.drop(self.act(X))
-        return X
+import torch
+import torch.nn as nn
+
+from easygraph.classes import Hypergraph
+
+
+class HGNNPConv(nn.Module):
+    r"""The HGNN :sup:`+` convolution layer proposed in `HGNN+: General Hypergraph Neural Networks <https://ieeexplore.ieee.org/document/9795251>`_ paper (IEEE T-PAMI 2022).
+
+    Sparse Format:
+    
+    .. math::
+
+        \left\{
+            \begin{aligned}
+                m_{\beta}^{t} &=\sum_{\alpha \in \mathcal{N}_{v}(\beta)} M_{v}^{t}\left(x_{\alpha}^{t}\right) \\
+                y_{\beta}^{t} &=U_{e}^{t}\left(w_{\beta}, m_{\beta}^{t}\right) \\
+                m_{\alpha}^{t+1} &=\sum_{\beta \in \mathcal{N}_{e}(\alpha)} M_{e}^{t}\left(x_{\alpha}^{t}, y_{\beta}^{t}\right) \\
+                x_{\alpha}^{t+1} &=U_{v}^{t}\left(x_{\alpha}^{t}, m_{\alpha}^{t+1}\right) \\
+            \end{aligned}
+        \right.
+
+    Matrix Format:
+
+    .. math::
+        \mathbf{X}^{\prime} = \sigma \left( \mathbf{D}_v^{-1} \mathbf{H} \mathbf{W}_e 
+        \mathbf{D}_e^{-1} \mathbf{H}^\top \mathbf{X} \mathbf{\Theta} \right).
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
+        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
+        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
+        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        bias: bool = True,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+        is_last: bool = False,
+    ):
+        super().__init__()
+        self.is_last = is_last
+        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
+        self.act = nn.ReLU(inplace=True)
+        self.drop = nn.Dropout(drop_rate)
+        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
+
+    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\mathcal{V}|, C_{in})`.
+            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`|\mathcal{V}|` vertices.
+        """
+        X = self.theta(X)
+        if self.bn is not None:
+            X = self.bn(X)
+        X = hg.v2v(X, aggr="mean")
+        if not self.is_last:
+            X = self.drop(self.act(X))
+        return X
```

## easygraph/nn/convs/hypergraphs/hypergcn_conv.py

 * *Ordering differences only*

```diff
@@ -1,61 +1,61 @@
-from typing import Optional
-
-import torch
-import torch.nn as nn
-
-from easygraph.classes import Graph
-from easygraph.classes import Hypergraph
-
-
-class HyperGCNConv(nn.Module):
-    r"""The HyperGCN convolution layer proposed in `HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs <https://papers.nips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf>`_ paper (NeurIPS 2019).
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
-        ``use_mediator`` (``str``): Whether to use mediator to transform the hyperedges to edges in the graph. Defaults to ``False``.
-        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
-        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
-        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        out_channels: int,
-        use_mediator: bool = False,
-        bias: bool = True,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-        is_last: bool = False,
-    ):
-        super().__init__()
-        self.is_last = is_last
-        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
-        self.use_mediator = use_mediator
-        self.act = nn.ReLU(inplace=True)
-        self.drop = nn.Dropout(drop_rate)
-        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
-
-    def forward(
-        self, X: torch.Tensor, hg: Hypergraph, cached_g: Optional[Graph] = None
-    ) -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
-            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
-            ``cached_g`` (``eg.Graph``): The pre-transformed graph structure from the hypergraph structure that contains :math:`N` vertices. If not provided, the graph structure will be transformed for each forward time. Defaults to ``None``.
-        """
-        X = self.theta(X)
-        if self.bn is not None:
-            X = self.bn(X)
-        if cached_g is None:
-            g = Graph.from_hypergraph_hypergcn(hg, X, self.use_mediator)
-            X = g.smoothing_with_GCN(X)
-        else:
-            X = cached_g.smoothing_with_GCN(X)
-        if not self.is_last:
-            X = self.drop(self.act(X))
-        return X
+from typing import Optional
+
+import torch
+import torch.nn as nn
+
+from easygraph.classes import Graph
+from easygraph.classes import Hypergraph
+
+
+class HyperGCNConv(nn.Module):
+    r"""The HyperGCN convolution layer proposed in `HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs <https://papers.nips.cc/paper/2019/file/1efa39bcaec6f3900149160693694536-Paper.pdf>`_ paper (NeurIPS 2019).
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
+        ``use_mediator`` (``str``): Whether to use mediator to transform the hyperedges to edges in the graph. Defaults to ``False``.
+        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
+        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
+        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        use_mediator: bool = False,
+        bias: bool = True,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+        is_last: bool = False,
+    ):
+        super().__init__()
+        self.is_last = is_last
+        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
+        self.use_mediator = use_mediator
+        self.act = nn.ReLU(inplace=True)
+        self.drop = nn.Dropout(drop_rate)
+        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
+
+    def forward(
+        self, X: torch.Tensor, hg: Hypergraph, cached_g: Optional[Graph] = None
+    ) -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
+            ``hg`` (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
+            ``cached_g`` (``eg.Graph``): The pre-transformed graph structure from the hypergraph structure that contains :math:`N` vertices. If not provided, the graph structure will be transformed for each forward time. Defaults to ``None``.
+        """
+        X = self.theta(X)
+        if self.bn is not None:
+            X = self.bn(X)
+        if cached_g is None:
+            g = Graph.from_hypergraph_hypergcn(hg, X, self.use_mediator)
+            X = g.smoothing_with_GCN(X)
+        else:
+            X = cached_g.smoothing_with_GCN(X)
+        if not self.is_last:
+            X = self.drop(self.act(X))
+        return X
```

## easygraph/nn/convs/hypergraphs/dhcf_conv.py

 * *Ordering differences only*

```diff
@@ -1,54 +1,54 @@
-import torch
-import torch.nn as nn
-
-from easygraph.classes import Hypergraph
-
-
-class JHConv(nn.Module):
-    r"""The Jump Hypergraph Convolution layer proposed in `Dual Channel Hypergraph Collaborative Filtering <https://dl.acm.org/doi/10.1145/3394486.3403253>`_ paper (KDD 2020).
-
-    Matrix Format:
-
-    .. math::
-        \mathbf{X}^{\prime} = \sigma \left( \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1}
-        \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}} \mathbf{X} \mathbf{\Theta} + \mathbf{X} \right).
-
-    Parameters:
-        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
-        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
-        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
-        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
-        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
-        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
-    """
-
-    def __init__(
-        self,
-        in_channels: int,
-        out_channels: int,
-        bias: bool = True,
-        use_bn: bool = False,
-        drop_rate: float = 0.5,
-        is_last: bool = False,
-    ):
-        super().__init__()
-        self.is_last = is_last
-        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
-        self.act = nn.ReLU(inplace=True)
-        self.drop = nn.Dropout(drop_rate)
-        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
-
-    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
-        r"""The forward function.
-
-        Parameters:
-            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
-            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
-        """
-        X = self.theta(X)
-        if self.bn is not None:
-            X = self.bn(X)
-        X = hg.smoothing_with_HGNN(X) + X
-        if not self.is_last:
-            X = self.drop(self.act(X))
-        return X
+import torch
+import torch.nn as nn
+
+from easygraph.classes import Hypergraph
+
+
+class JHConv(nn.Module):
+    r"""The Jump Hypergraph Convolution layer proposed in `Dual Channel Hypergraph Collaborative Filtering <https://dl.acm.org/doi/10.1145/3394486.3403253>`_ paper (KDD 2020).
+
+    Matrix Format:
+
+    .. math::
+        \mathbf{X}^{\prime} = \sigma \left( \mathbf{D}_v^{-\frac{1}{2}} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-1}
+        \mathbf{H}^\top \mathbf{D}_v^{-\frac{1}{2}} \mathbf{X} \mathbf{\Theta} + \mathbf{X} \right).
+
+    Parameters:
+        ``in_channels`` (``int``): :math:`C_{in}` is the number of input channels.
+        ``out_channels`` (int): :math:`C_{out}` is the number of output channels.
+        ``bias`` (``bool``): If set to ``False``, the layer will not learn the bias parameter. Defaults to ``True``.
+        ``use_bn`` (``bool``): If set to ``True``, the layer will use batch normalization. Defaults to ``False``.
+        ``drop_rate`` (``float``): If set to a positive number, the layer will use dropout. Defaults to ``0.5``.
+        ``is_last`` (``bool``): If set to ``True``, the layer will not apply the final activation and dropout functions. Defaults to ``False``.
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        bias: bool = True,
+        use_bn: bool = False,
+        drop_rate: float = 0.5,
+        is_last: bool = False,
+    ):
+        super().__init__()
+        self.is_last = is_last
+        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None
+        self.act = nn.ReLU(inplace=True)
+        self.drop = nn.Dropout(drop_rate)
+        self.theta = nn.Linear(in_channels, out_channels, bias=bias)
+
+    def forward(self, X: torch.Tensor, hg: Hypergraph) -> torch.Tensor:
+        r"""The forward function.
+
+        Parameters:
+            X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.
+            hg (``eg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.
+        """
+        X = self.theta(X)
+        if self.bn is not None:
+            X = self.bn(X)
+        X = hg.smoothing_with_HGNN(X) + X
+        if not self.is_last:
+            X = self.drop(self.act(X))
+        return X
```

## easygraph/nn/convs/hypergraphs/halfnlh_conv.py

 * *Ordering differences only*

```diff
@@ -1,112 +1,112 @@
-import torch.nn as nn
-import torch.nn.functional as F
-
-from easygraph.nn.convs.common import MLP
-from easygraph.nn.convs.pma import PMA
-from torch_geometric.nn.conv import MessagePassing
-from torch_scatter import scatter
-
-
-class HalfNLHconv(MessagePassing):
-    r"""The HalfNLHconv model proposed in `YOU ARE ALLSET: A MULTISET LEARNING FRAMEWORK FOR HYPERGRAPH NEURAL NETWORKS <https://openreview.net/pdf?id=hpBTIv2uy_E>`_ paper (ICLR 2022).
-
-    Parameters:
-        ``in_dim`` (``int``): : The dimension of input.
-        ``hid_dim`` (``int``): : The dimension of hidden.
-        ``out_dim`` (``int``): : The dimension of output.
-        ``num_layers`` (``int``): : The number of layers.
-        ``dropout`` (``float``): Dropout ratio. Defaults to 0.5.
-        ``normalization`` (``str``): The normalization method. Defaults to ``bn``
-        ``InputNorm`` (``bool``): Defaults to False.
-        ``heads`` (``int``):  Defaults to 1
-        `attention`` (``bool``):  Defaults to True
-
-    """
-
-    def __init__(
-        self,
-        in_dim,
-        hid_dim,
-        out_dim,
-        num_layers,
-        dropout,
-        normalization="bn",
-        InputNorm=False,
-        heads=1,
-        attention=True,
-    ):
-        super(HalfNLHconv, self).__init__()
-
-        self.attention = attention
-        self.dropout = dropout
-
-        if self.attention:
-            self.prop = PMA(in_dim, hid_dim, out_dim, num_layers, heads=heads)
-        else:
-            if num_layers > 0:
-                self.f_enc = MLP(
-                    in_dim,
-                    hid_dim,
-                    hid_dim,
-                    num_layers,
-                    dropout,
-                    normalization,
-                    InputNorm,
-                )
-                self.f_dec = MLP(
-                    hid_dim,
-                    hid_dim,
-                    out_dim,
-                    num_layers,
-                    dropout,
-                    normalization,
-                    InputNorm,
-                )
-            else:
-                self.f_enc = nn.Identity()
-                self.f_dec = nn.Identity()
-
-    def reset_parameters(self):
-        if self.attention:
-            self.prop.reset_parameters()
-        else:
-            if not (self.f_enc.__class__.__name__ is "Identity"):
-                self.f_enc.reset_parameters()
-            if not (self.f_dec.__class__.__name__ is "Identity"):
-                self.f_dec.reset_parameters()
-
-    #         self.bn.reset_parameters()
-
-    def forward(self, x, edge_index, norm, aggr="add"):
-        """
-        input -> MLP -> Prop
-        """
-
-        if self.attention:
-            x = self.prop(x, edge_index)
-        else:
-            x = F.relu(self.f_enc(x))
-            x = F.dropout(x, p=self.dropout, training=self.training)
-            x = self.propagate(edge_index, x=x, norm=norm, aggr=aggr)
-            x = F.relu(self.f_dec(x))
-
-        return x
-
-    def message(self, x_j, norm):
-        return norm.view(-1, 1) * x_j
-
-    def aggregate(self, inputs, index, dim_size=None, aggr=None):
-        r"""Aggregates messages from neighbors as
-        :math:`\square_{j \in \mathcal{N}(i)}`.
-
-        Takes in the output of message computation as first argument and any
-        argument which was initially passed to :meth:`propagate`.
-
-        By default, this function will delegate its call to scatter functions
-        that support "add", "mean" and "max" operations as specified in
-        :meth:`__init__` by the :obj:`aggr` argument.
-        """
-        #         ipdb.set_trace()
-        if aggr is None:
-            raise ValueError("aggr was not passed!")
-        return scatter(inputs, index, dim=self.node_dim, reduce=aggr)
+import torch.nn as nn
+import torch.nn.functional as F
+
+from easygraph.nn.convs.common import MLP
+from easygraph.nn.convs.pma import PMA
+from torch_geometric.nn.conv import MessagePassing
+from torch_scatter import scatter
+
+
+class HalfNLHconv(MessagePassing):
+    r"""The HalfNLHconv model proposed in `YOU ARE ALLSET: A MULTISET LEARNING FRAMEWORK FOR HYPERGRAPH NEURAL NETWORKS <https://openreview.net/pdf?id=hpBTIv2uy_E>`_ paper (ICLR 2022).
+
+    Parameters:
+        ``in_dim`` (``int``): : The dimension of input.
+        ``hid_dim`` (``int``): : The dimension of hidden.
+        ``out_dim`` (``int``): : The dimension of output.
+        ``num_layers`` (``int``): : The number of layers.
+        ``dropout`` (``float``): Dropout ratio. Defaults to 0.5.
+        ``normalization`` (``str``): The normalization method. Defaults to ``bn``
+        ``InputNorm`` (``bool``): Defaults to False.
+        ``heads`` (``int``):  Defaults to 1
+        `attention`` (``bool``):  Defaults to True
+
+    """
+
+    def __init__(
+        self,
+        in_dim,
+        hid_dim,
+        out_dim,
+        num_layers,
+        dropout,
+        normalization="bn",
+        InputNorm=False,
+        heads=1,
+        attention=True,
+    ):
+        super(HalfNLHconv, self).__init__()
+
+        self.attention = attention
+        self.dropout = dropout
+
+        if self.attention:
+            self.prop = PMA(in_dim, hid_dim, out_dim, num_layers, heads=heads)
+        else:
+            if num_layers > 0:
+                self.f_enc = MLP(
+                    in_dim,
+                    hid_dim,
+                    hid_dim,
+                    num_layers,
+                    dropout,
+                    normalization,
+                    InputNorm,
+                )
+                self.f_dec = MLP(
+                    hid_dim,
+                    hid_dim,
+                    out_dim,
+                    num_layers,
+                    dropout,
+                    normalization,
+                    InputNorm,
+                )
+            else:
+                self.f_enc = nn.Identity()
+                self.f_dec = nn.Identity()
+
+    def reset_parameters(self):
+        if self.attention:
+            self.prop.reset_parameters()
+        else:
+            if not (self.f_enc.__class__.__name__ is "Identity"):
+                self.f_enc.reset_parameters()
+            if not (self.f_dec.__class__.__name__ is "Identity"):
+                self.f_dec.reset_parameters()
+
+    #         self.bn.reset_parameters()
+
+    def forward(self, x, edge_index, norm, aggr="add"):
+        """
+        input -> MLP -> Prop
+        """
+
+        if self.attention:
+            x = self.prop(x, edge_index)
+        else:
+            x = F.relu(self.f_enc(x))
+            x = F.dropout(x, p=self.dropout, training=self.training)
+            x = self.propagate(edge_index, x=x, norm=norm, aggr=aggr)
+            x = F.relu(self.f_dec(x))
+
+        return x
+
+    def message(self, x_j, norm):
+        return norm.view(-1, 1) * x_j
+
+    def aggregate(self, inputs, index, dim_size=None, aggr=None):
+        r"""Aggregates messages from neighbors as
+        :math:`\square_{j \in \mathcal{N}(i)}`.
+
+        Takes in the output of message computation as first argument and any
+        argument which was initially passed to :meth:`propagate`.
+
+        By default, this function will delegate its call to scatter functions
+        that support "add", "mean" and "max" operations as specified in
+        :meth:`__init__` by the :obj:`aggr` argument.
+        """
+        #         ipdb.set_trace()
+        if aggr is None:
+            raise ValueError("aggr was not passed!")
+        return scatter(inputs, index, dim=self.node_dim, reduce=aggr)
```

## easygraph/nn/convs/hypergraphs/__init__.py

 * *Ordering differences only*

```diff
@@ -1,9 +1,9 @@
-from .dhcf_conv import JHConv
-from .hgnn_conv import HGNNConv
-from .hgnnp_conv import HGNNPConv
-from .hnhn_conv import HNHNConv
-from .hypergcn_conv import HyperGCNConv
-from .unignn_conv import UniGATConv
-from .unignn_conv import UniGCNConv
-from .unignn_conv import UniGINConv
-from .unignn_conv import UniSAGEConv
+from .dhcf_conv import JHConv
+from .hgnn_conv import HGNNConv
+from .hgnnp_conv import HGNNPConv
+from .hnhn_conv import HNHNConv
+from .hypergcn_conv import HyperGCNConv
+from .unignn_conv import UniGATConv
+from .unignn_conv import UniGCNConv
+from .unignn_conv import UniGINConv
+from .unignn_conv import UniSAGEConv
```

## Comparing `Python_EasyGraph-1.1.dist-info/RECORD` & `Python_EasyGraph-1.2.dist-info/RECORD`

 * *Files 11% similar despite different names*

```diff
@@ -1,225 +1,225 @@
-cpp_easygraph.pypy39-pp73-x86_64-linux-gnu.so,sha256=wWyMCWMJkx90OCF3hXiA-Y-_-bVQNgVvwef_SRMUdSM,970360
-Python_EasyGraph-1.1.dist-info/top_level.txt,sha256=sTleEAisCc-R1iwDM-6kfIHIHnEW71eyDRSAyePlumM,24
-Python_EasyGraph-1.1.dist-info/WHEEL,sha256=Cbg0nsX8XUWK0XXyuFqImveu21GlM0i0SUNuS4t0nDw,162
-Python_EasyGraph-1.1.dist-info/RECORD,,
-Python_EasyGraph-1.1.dist-info/LICENSE,sha256=vWdw-flKKdoiITDoVRm17vUAvXWMnsgNcf7KdRjWW60,1561
-Python_EasyGraph-1.1.dist-info/LICENSE.NetworkX,sha256=VmLJV4hya-pmp-XpPBWp50zBEvBk-BX7lj7bYtx-N9c,1619
-Python_EasyGraph-1.1.dist-info/METADATA,sha256=RqY2KhAjeX_O6zag9Looimmy_3pscFkbr6WJjS4jyd4,6633
-easygraph/_global.py,sha256=S_QzUf4ky5EC4oGzmnRUUgfsvRB1roowjT04uZH_oVk,389
-easygraph/exception.py,sha256=lprnaOhhjoS8rxMefOiXhOWx4Yb_eI1R3PgmNoz2w_k,2246
-easygraph/convert.py,sha256=ypC-i9xPWy6Q-RPkINS6sXZ1LfEPEGlDlWWEAdBgavY,19169
-easygraph/__init__.py,sha256=hr0SLE627qs1FwAO1u9q5pQWi_LV9btaWPgRLifJSe4,702
-easygraph/readwrite/ucinet.py,sha256=v0sv7i-AIC2tVo5dUR6LKLLCDloexiGJJoLf1lGnvPw,10202
-easygraph/readwrite/gexf.py,sha256=PQzt_7NGiVLOpyvp3iJPeo1jcAK0wSe6FQkqh-H1yDc,38050
-easygraph/readwrite/gml.py,sha256=edj_gGyyYtejkogayuDiFbDKRD6a5_omifTwi_4AWkc,28326
-easygraph/readwrite/graphviz.py,sha256=hFbI2j7b7-8DnuGaO-Ce49GnRp-Ib6gZgk_fOH8k9_8,5038
-easygraph/readwrite/pickle.py,sha256=jvdofOCz6O3E_mTnOCRpy5jdfxHmbhtV9FiMDgqPkFc,259
-easygraph/readwrite/pajek.py,sha256=bBjB9ut-q2DkXEtED-vYWnOSnluBg6VKz3ebJlNvHlE,10793
-easygraph/readwrite/graphml.py,sha256=4GKgktTrpuRnhvFuAEuSYU5Fabc0kBXuRxgxNI7F-1g,39166
-easygraph/readwrite/edgelist.py,sha256=d1nGSzCCnc3H3IC5G0q9ybDovtka5prCPEBL2aIcW-s,13552
-easygraph/readwrite/__init__.py,sha256=LA8ExCDHxX61WF-giH6MDSt3nK83BPDvnTxTC9uynkA,372
-easygraph/readwrite/tests/test_gml.py,sha256=-QRXih1nk3XkgDwwTnGvCJuHQ9uemYeyFJG9nBvZCGE,17416
-easygraph/readwrite/tests/test_graphviz.py,sha256=MVV3EsvnAc_9g7KmoRWJDp4i0DogXvDQdk5gkzhbpxs,1446
-easygraph/readwrite/tests/test_gexf.py,sha256=54Y_jXzh4bpoEZSeGFkbQoz_KGglU2pkPULzIhP2l-8,15231
-easygraph/readwrite/tests/test_ucinet.py,sha256=SVqxUZGJ-aJ05mkLrCvXTGuJYDI5X7_T6HHo8pH1a5k,7995
-easygraph/readwrite/tests/test_pickle.py,sha256=3Xpp_gFv4NF8MHv_qJsRzpWaw6LzDvoRjK102hYe7mM,2177
-easygraph/readwrite/tests/test_edgelist.py,sha256=w2POWr94AhmAhGuqDE2qPEWMN8_im9_xVV7ryDo6Q5k,10062
-easygraph/readwrite/tests/test_pajek.py,sha256=DU1iS5obtTBKMo4tyAYo2W_hagnx14NRs0LZX9xH6LE,9253
-easygraph/readwrite/tests/test_graphml.py,sha256=oUOUTxOWCLdt9cfOYUEdh7YSlzsxk4ZXUxNLe2C5ZkE,65180
-easygraph/readwrite/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-easygraph/readwrite/json_graph/node_link.py,sha256=MkSjFiEfPNo6C_NzaIiIFhMjWWPPzIWOVi7cqyysxwg,3331
-easygraph/readwrite/json_graph/__init__.py,sha256=8t_80fNe5NpPAdm82hv2qJz4fIKGx0QoJlhkdiBbDq8,521
-easygraph/ml_metrics/base.py,sha256=bkYHj9dHzG5gAh-7AzkfXDR5BUpD4HZXTV1kfq_rTJ0,8298
-easygraph/ml_metrics/classification.py,sha256=WYBSO3hIrr9G_VS2rGY6iObXtxr3ImakKA2Olq5amRg,6393
-easygraph/ml_metrics/__init__.py,sha256=hVc0qx86Y-77JU3ze3TfHqf_PwuUeUJ4Nl38qSjiidc,1574
-easygraph/ml_metrics/hypergraphs/hypergraph.py,sha256=iGm_E_s7AfeaxIvPPRxhOpmrowd12GRNObnl7RwdIN0,1730
-easygraph/ml_metrics/hypergraphs/__init__.py,sha256=Xoga-19etjw-WrY7tZAtTA_aAJg1lJ93Q98mClQ4HrY,64
-easygraph/model/__init__.py,sha256=oZ_ZmoTbtxKSnx_MktpfLqOR7G0_RybHbw1Rg-BGI_w,507
-easygraph/model/hypergraphs/setgnn.py,sha256=HrVMImll7mdUFeGDYAxOdhTmTHm4gQE_hKu-3xHRj9g,36302
-easygraph/model/hypergraphs/hgnn.py,sha256=DEhCDsBi6oanmxPt-LpMQ7WkfKC4bQy_UGSF-M0v8hc,1840
-easygraph/model/hypergraphs/hypergcn.py,sha256=by6q4vM-_TIHC1GRanlrrV7QpJQa3MAhqM2OuI0utZ4,2577
-easygraph/model/hypergraphs/dhne.py,sha256=Iu1gnS9OSypkULRafBbCtZLXAkStH6QZcLU6zyMOqYo,2556
-easygraph/model/hypergraphs/dhcf.py,sha256=Vx20yzcdh9aCtFb3Vg1PmdaW8mWEkjdxH46N7NDNa9M,3736
-easygraph/model/hypergraphs/unignn.py,sha256=nz3x0XkEb5JMubd60HKFkycJkJMZSMuO8Wg3o-bRSQs,7888
-easygraph/model/hypergraphs/__init__.py,sha256=6t7P-eN8sjIiTeXeNyP92oxCa-LNzf3qeXLYub1nXyY,261
-easygraph/model/hypergraphs/hgnnp.py,sha256=eSTxJtSCSEzw78DHNnUQCvEMVmrhpzGfKAKPJXvVvFw,1626
-easygraph/model/hypergraphs/hnhn.py,sha256=a_MuUkzlGlKjbQKKC1wxksZkpQxyDR_16f-EWbOHOko,1605
-easygraph/tests/test_convert.py,sha256=ih8FAmxqclYYy0F_BQislmm2FPXJu19vPci3gUm7MVc,3043
-easygraph/tests/teddy_test_cpp_easygraph_sanity_check.py,sha256=55D0M_dfSxMYHDDTo_DR02-TjOI1HvJH6L0COf9ZDwY,5871
-easygraph/tests/test_cpp_easygraph.py,sha256=vV2nNgGnBj0UDpudjaS06K0PdlHkZ9OqqDZAzPnJYS4,302
-easygraph/tests/script_test_cpp_easygraph.py,sha256=YmsGh0v0_FXBfvO6Ot4Qlwrb2ROhZml5uAxTQ6--dzI,7380
-easygraph/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-easygraph/classes/hypergraph.py,sha256=1eenoZRQXepj6-hQMgDWHtd6a6ZVOTANwfwj3cOBE74,106226
-easygraph/classes/graphviews.py,sha256=WKxa5mAMG6AUikiQWebs8B4EAnZXNxp2YruPoedhlwQ,337
-easygraph/classes/base.py,sha256=b23PtoETfF30ZJs2imQD19z_4LS-7utpj9f4ot-H7iA,44973
-easygraph/classes/graph.py,sha256=aIZR6k4RcSZJZ2ZfJt7GG-mo6o_NWNL3_T8UgdBpnZs,50454
-easygraph/classes/multigraph.py,sha256=fB4GFh08eij7YFGWmmAV0Ml2nOam3FP73tWEctTqT8s,24639
-easygraph/classes/directed_multigraph.py,sha256=sQyw_Sy0ImHBhlQ1TMA--E-dPrBmKtmUR4Tc2ENaDks,14760
-easygraph/classes/directed_graph.py,sha256=LKF9PlLr1PvR4IG89Nxj3AIGna5LKUMtJ6OsWqfd18E,36196
-easygraph/classes/operation.py,sha256=MKcRak0PmyYsQznjqskbvC_N4-1gPxDj775XurFs1BM,13931
-easygraph/classes/__init__.py,sha256=akHhfuQKILZnLsKJBL-PrWoMWo73feSZh35n9VYwE6g,525
-easygraph/classes/tests/test_multidigraph.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-easygraph/classes/tests/test_multigraph.py,sha256=FrLr03RMuQTQGwjwSaOeurYib79rR-p3qkQduLuhGBo,2011
-easygraph/classes/tests/test_hypergraph.py,sha256=4064AyoUL41_KTJ_A53xnF37d1auWqH6t5pqYABc-jE,30053
-easygraph/classes/tests/test_operation.py,sha256=yClv0DhPJFguTtnWLm3ecfjNRX3aTRp6OxbXoREGOj8,455
-easygraph/classes/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-easygraph/datapipe/common.py,sha256=DI2ClhEmgRo-hLzLl_a_5twGRt2UHNc9roD1jDVIXqw,2909
-easygraph/datapipe/normalize.py,sha256=o6lYtISx4z7G1AFqCIuALhRy1JzZVn578eZkApRBKJ0,2734
-easygraph/datapipe/loader.py,sha256=U7zWvRwkvDSzD_sHb0ieAkbFaoMO7z9Guhq43wK6rqM,2897
-easygraph/datapipe/__init__.py,sha256=C31KUH6sDlBaAL4Q9M9kQUCOrNUE7N6iqvRb3JSEl50,717
-easygraph/datasets/ppi.py,sha256=A1uC2oWzLpQPw0UIvS7BK9WMPMILqlbkBnWzvRxe9YI,6662
-easygraph/datasets/karate.py,sha256=Y7m1hn-_Gk6hmlMAA8VZVSHaLbfSNunkufSILTbqW_E,2554
-easygraph/datasets/utils.py,sha256=ZTw26TgPuxg3oKdJpQBwio0p5x1WJEpY7s5jrZXZoB0,10334
-easygraph/datasets/citation_graph.py,sha256=4oPEx7qdiCiNq3IbFImCg67CxhDXPsEyKDgdpaai0V8,28329
-easygraph/datasets/gnn_benchmark.py,sha256=gGjFw-1qUQrtOf3_6MtEBW39D3p6Q1FI95VTLuUnF1c,6948
-easygraph/datasets/get_sample_graph.py,sha256=tyyZfM71YK1YU10GiA1Q1RdDkoOMNKJqLRzPZPEiyPU,6876
-easygraph/datasets/graph_dataset_base.py,sha256=oVpy1d6WOka5cB0-G8tKMSLs-jk9e3zvUmwAG-e0VEo,9514
-easygraph/datasets/__init__.py,sha256=FgFfyEo1e3RyoN1oFoxwG_aKoAZ4PHGB1TLK_6hzC-o,744
-easygraph/datasets/hypergraph/House_Committees.py,sha256=n8IXr33p2kG8BOAbPYPi22VnPrNymaNVUehOtcdP-LI,4060
-easygraph/datasets/hypergraph/_global.py,sha256=Mbd77q2d1MuVG7HeKkrnykviCPeDk89VYq-EbOk0X1c,336
-easygraph/datasets/hypergraph/senate_committees.py,sha256=GT28c5LZvGBTHnPud94RiscXRT-8sbJIq5pMBjrkbLQ,3998
-easygraph/datasets/hypergraph/cocitation.py,sha256=vi78jfWhtid6HekXXb6dorqSC809snqSsxru-T6McrQ,11351
-easygraph/datasets/hypergraph/mathoverflow_answers.py,sha256=KxaugrUqNZc_ItmMsgt__T2Xch4bUHqe_aiq14GWSBg,4258
-easygraph/datasets/hypergraph/cat_edge_Cooking.py,sha256=9mKlFlZi4N33nl32mzdD57CpfJfeAsWqcNGGrWoGOIU,4106
-easygraph/datasets/hypergraph/Yelp.py,sha256=h3IZc88XL5RA4TeBcoq9jtjreIm5mTLM2_luVTgAl9I,3361
-easygraph/datasets/hypergraph/walmart_trips.py,sha256=xPemdLiXfm_3mqYj-L1i6cwvMDdxIgNFDjSlr0IqpYI,3854
-easygraph/datasets/hypergraph/contact_primary_school.py,sha256=yX44y1pIwxAYfagxtSbUHO5AxhhT6AZQ-kaThC02nRs,4005
-easygraph/datasets/hypergraph/hypergraph_dataset_base.py,sha256=O1NZCLd8tDI1W6uSzNnmrcKjH3BicsA1tj490vv175k,4265
-easygraph/datasets/hypergraph/cooking_200.py,sha256=PtUjYvNaHecxSklFTNh-HotJtr3bBLUduDNCtAqjjDw,3460
-easygraph/datasets/hypergraph/trivago_clicks.py,sha256=ViqiTXhRf8Nsls9iDgTfzgQrbC57RghBX9NUgFXJzWs,3734
-easygraph/datasets/hypergraph/__init__.py,sha256=diU5R7LID_ava4-ZWTc2Vo8d_cxcjyGBaVjvgw_D-F8,276
-easygraph/datasets/hypergraph/coauthorship.py,sha256=MIaKPzmpWyhUsKVM9VnfRV61ISt9MWtdZlK7XtoPLNk,7690
-easygraph/datasets/dynamic/hospital_lyon.py,sha256=8mvzkmanKO1ddW5tLP_1wKBIyyqfjzOQTsKYDyIHuc8,4202
-easygraph/datasets/dynamic/load_dataset.py,sha256=mhcWQUCdlaKdAujqfApQePjXXIyLfGxWMYJDiEgi9pw,2471
-easygraph/datasets/dynamic/email_enron.py,sha256=nCrw6j3N07sK8wQa-dqWbfDuDM7oeWECN2JCg3Q24hY,2499
-easygraph/datasets/dynamic/__init__.py,sha256=cYHcidjWvX1w4xJnLuHueueVqFYq4LtgyuerIUlUgck,108
-easygraph/datasets/dynamic/email_eu.py,sha256=8xCmElMMORJw3Yr0JDW8O1VjeKU01lq_6e_wL21mbcE,2370
-easygraph/functions/isolate.py,sha256=EEKBME4G8rGVpMA1sHthztD44us1ydEwNeiUtN-VQ9Y,2274
-easygraph/functions/__init__.py,sha256=GgAE7nv8SVNT8fiNRnZSe1uTS7wlrEU1Nqx4KDgKtCk,566
-easygraph/functions/centrality/closeness.py,sha256=S4eGhpnWsQtarEw7LtGOwVzIooYhyv-Pk2fkxK8ZCmI,2939
-easygraph/functions/centrality/pagerank.py,sha256=VllCxVK6IoufA93zwC6jKEwluMTH0Tuu82kjewa-Vds,1322
-easygraph/functions/centrality/laplacian.py,sha256=Ip7iCdiybN_ZX889t3xLy1ANo2qdwH3ehTWdt5Df5OY,3772
-easygraph/functions/centrality/betweenness.py,sha256=_jvMIyw-3MyWpAOVEGgKvAhZxp57_PQjGsr32UpyZPc,6563
-easygraph/functions/centrality/flowbetweenness.py,sha256=rqhFbYwD3tm80StXoIbuI4wTykuOwzzweIbVVX_Dtx0,4111
-easygraph/functions/centrality/degree.py,sha256=tCCAVEwQOGw-8rkISuQsFtCr2wRFl_BZ8DMgysKoUOY,3026
-easygraph/functions/centrality/ego_betweenness.py,sha256=CNzqWCdT07MhXMvYAE4rYgzKFtIkcegW1ZJXm2FcOkI,1500
-easygraph/functions/centrality/__init__.py,sha256=eh6NW_mEUueT9rFgljhqNYsOwieBGNc63HSMIOsBoRo,185
-easygraph/functions/basic/avg_degree.py,sha256=evDSw9NU3pR2yHGSucTrofIRIfVrSfgljWhAv2eysS8,629
-easygraph/functions/basic/localassort.py,sha256=O2bFPTH_en9pUnEZg8CVFZVIiuXMMMhezrccO5mqjOs,7064
-easygraph/functions/basic/predecessor_path_based.py,sha256=fnAqFOM8urO4ZfoRB7P9Vz_5aEt_mXZLPNiJgBooNQ4,2934
-easygraph/functions/basic/cluster.py,sha256=qAufNgS5O7_hr8Fsk2ToU0qk0ugh8irKztxxdeXJywU,19309
-easygraph/functions/basic/__init__.py,sha256=bBwqHcfbCIMPFEMVG2GncJyJZOQO0cE9JVFBB1bqNB4,114
-easygraph/functions/basic/tests/test_avg_degree.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-easygraph/functions/basic/tests/test_predecessor.py,sha256=TDmc2sDC6H2V7EtB6Z7_e8VHvb2exDRiyNIlJacPXQM,494
-easygraph/functions/basic/tests/test_cluster.py,sha256=12sTjFxXjPfbLhZYMdENyCctp23A0jRSNk2pNRUAwLE,10992
-easygraph/functions/basic/tests/test_localassort.py,sha256=WlFyO2gTl5ueudrV4snD1kSV8B8G9RGgpQuyn5mEJpY,1064
-easygraph/functions/basic/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-easygraph/functions/graph_embedding/line.py,sha256=SFCTV498a8TNzcl6lSbNgj3GKZ8q7rrONPPgT1AH8A0,9777
-easygraph/functions/graph_embedding/sdne.py,sha256=SkvBRQzuHt1EfzGW5QJSEGi8L57ZvLOkmDrd-Loaa6Q,9288
-easygraph/functions/graph_embedding/deepwalk.py,sha256=ZkLqmeceqyrtLI4a97g-Alt3SsIENf0_-Pzs9KXqjog,2909
-easygraph/functions/graph_embedding/node2vec.py,sha256=N0PvDyOOlr6yLum3oiiKMxnFHzfD7peuJ1m05z34fUw,8992
-easygraph/functions/graph_embedding/NOBE.py,sha256=xUu8hwTFEPjrBHohdihTDf9i_khipmkLCse8hlMyLro,3933
-easygraph/functions/graph_embedding/__init__.py,sha256=9cG8lF8FpBd3GB_ka462ndAnryAt6McUXFgF8UQ94pQ,293
-easygraph/functions/graph_embedding/net_emb_example_citeseer.py,sha256=iMS1SCWVor2RhHrpDxWcEj-mwo2-S-sNNsP7VraTDbI,5095
-easygraph/functions/hypergraph/hypergraph_operation.py,sha256=PBgBSprrFLQ2_9fUtqkZ96e-BucMPEflEmm-XNDgNyo,2268
-easygraph/functions/hypergraph/hypergraph_clustering.py,sha256=fVBfEgWw1qHMumtDk1yyhRFRV9HKkBol8cx2EcDQUsE,8101
-easygraph/functions/hypergraph/assortativity.py,sha256=6O_D3SWXcT7rFJUl8pNvkYo6Ux7R4nSBmrHH-VeD4Kg,4645
-easygraph/functions/hypergraph/__init__.py,sha256=AY_AbgG5jX_0foLLdcPAaXKDxRsUU9BACCjEPXCYjJQ,164
-easygraph/functions/hypergraph/centrality/cycle_ratio.py,sha256=JvuJnHbDcjxlEDDJ-rBEqGENkwpI9_aFdBdNV0QFqy4,6282
-easygraph/functions/hypergraph/centrality/hypercoreness.py,sha256=UKNnbyef69bU9OsH3bbjfVJcounDnC9Za35WAWhGtmM,11579
-easygraph/functions/hypergraph/centrality/vector_centrality.py,sha256=CVTG-wK69pb4xgMHdV7H1loBLsrf3yu_8pDrjllz6W4,1786
-easygraph/functions/hypergraph/centrality/s_centrality.py,sha256=zh-y1Xh-70yA5PEe_1AP0UFYknVqysJWnB26NUT8tG0,2273
-easygraph/functions/hypergraph/centrality/degree.py,sha256=TF46PEpBT1f8wZOuV-OcSpbIz_iCGu_7MTqajfxN5yU,474
-easygraph/functions/hypergraph/centrality/__init__.py,sha256=wQZMdPEuBKAMGlPHbHsRUJ3VvxfY1MovSO5yQPTzfMs,139
-easygraph/functions/hypergraph/hypergraph_generator/simple.py,sha256=yApwBC2ALvZJ63TKo1k8phQ20GA3xxjNCARhb_mHjnM,1874
-easygraph/functions/hypergraph/hypergraph_generator/uniform.py,sha256=bnVwMPW4zahyLiXo3vdyOFcNnuTTJTCcYxzIsxDjLDM,12592
-easygraph/functions/hypergraph/hypergraph_generator/lattice.py,sha256=JOiWCeFok2JdZtkB7dYzeeSA-Q2IXtT1GR4ES5TiJ1Q,1462
-easygraph/functions/hypergraph/hypergraph_generator/random.py,sha256=_0C9GlzCpPnkRp3Ks3xVtAnq-pJLaLeQdAxfCFWXXCw,13577
-easygraph/functions/hypergraph/hypergraph_generator/hypergraph_classic.py,sha256=3Cw5G2lo4LdPvbxWUzbbdiiC-GTo8ZkcHlXOzSUhOxE,940
-easygraph/functions/hypergraph/hypergraph_generator/__init__.py,sha256=31io_r6-W49Ib72ZW8MAUgRCIgfqdvAJOQBa53p5GtQ,124
-easygraph/functions/hypergraph/hypergraph_generator/tests/test_classic.py,sha256=rWJhoM_rz95VDikgiBVaIv-doFt9437AjLh7lo7rPwQ,1458
-easygraph/functions/hypergraph/hypergraph_generator/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-easygraph/functions/path/diameter.py,sha256=kTg9QTc2h913WMh2DfNKXr7iTBEec1IDzbIvelxi4HA,2584
-easygraph/functions/path/mst.py,sha256=FPz2NwH6Zht6V9T2x2z-2KNjiiKFMMvbAIzRSFi1Awo,22611
-easygraph/functions/path/bridges.py,sha256=KUveY5sIdhHj9IAjHp7vqiwW_Z_su79fWGb_eA8ZUXo,5979
-easygraph/functions/path/average_shortest_path_length.py,sha256=w_ean6-fRHeE5ot-3LF-7dn5gGiF8uQKYMuRUPY2P_o,4060
-easygraph/functions/path/path.py,sha256=EwlzPwATHgXDViFrMaPOoCe1JZtd9v_quS29PlsgqJ8,6448
-easygraph/functions/path/__init__.py,sha256=Ual82rdElsEc0rBLm4_w8s_sNgzAGbTVYYktY8E8gsI,130
-easygraph/functions/community/motif.py,sha256=t6oWx5RX-uVfZUFHEEeGy1E_XtlfPf44OpOajZpv3I4,3328
-easygraph/functions/community/louvain.py,sha256=I119w_YYjvWIFD2DoA63_fydKJ4iMamTWWRwZG5UmG0,12340
-easygraph/functions/community/ego_graph.py,sha256=vFokgqtLhqfWoR8crH9vJyV47dFg_VRmVW11idK2Kz0,1918
-easygraph/functions/community/LPA.py,sha256=Fs-PeijK-t9xu5GW_dUygyCqMZ1NVEc5JFpKEtB_Z1A,23852
-easygraph/functions/community/modularity.py,sha256=4VNdDaC86beAeyvi1SGDCuSIqMuGtbXJKWlYPDsMQd4,1964
-easygraph/functions/community/modularity_max_detection.py,sha256=2nTjAwkdmZqQYuxw1ZfarDDqDWOkFyfu_5zazSnYTME,7371
-easygraph/functions/community/__init__.py,sha256=BNWlZZlAVL7AIwfHuXNZOmT_eUqiV3drnLl5Yh73dpo,154
-easygraph/functions/community/tests/test_motif.py,sha256=-Vi5HmaCw3PlHk-vrVM2nh5IP8VShUS9lelxQ_ifWmo,518
-easygraph/functions/community/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-easygraph/functions/components/connected.py,sha256=xTxukB_VSzDAP6WBCsM87ZIkwLpvvm5dV5qyWkdfIic,3572
-easygraph/functions/components/weakly_connected.py,sha256=1ZDL6wI-cixpUTx5pB4Ccj4TqDqSIBrRIG6T7DMFgo0,4084
-easygraph/functions/components/strongly_connected.py,sha256=khALp3pzz_w-QRUmulXhBmHKOvuzVmtQfoagX3gFDTc,6999
-easygraph/functions/components/biconnected.py,sha256=1MnRejdWKdfjJOx9slZOZb0xM4S1LXqOWwLH1fYACxA,7790
-easygraph/functions/components/__init__.py,sha256=skV0HEe9YGYNCjhlB46cQGfCpyOUZfZJochkEGGdgTg,118
-easygraph/functions/drawing/defaults.py,sha256=TYpzJ5YbPlsoJNvXfDpgRZOtI62rN4XjU6_Ch3VWgqU,7891
-easygraph/functions/drawing/positioning.py,sha256=vG4AUM6Ykl0ePu0QYiiSA5LPrjirFtfVPwEDB956Dxw,20431
-easygraph/functions/drawing/utils.py,sha256=4ovPPBPVeXIz-DtKOf8bTO0_deIehHpt_3gkKoRneEg,19115
-easygraph/functions/drawing/plot.py,sha256=XcFFgaJebjBTftXvfART_WB41kdY4uKsSLwT68t8ZUk,6229
-easygraph/functions/drawing/simulator.py,sha256=O0-JplJmzXYBcJVx1JC6WRPN4OORQdV7txbB384h3y8,7115
-easygraph/functions/drawing/drawing.py,sha256=uNAqfR83nKhezJe4iYtomNt5IJBu_zcclNSPKV0VBCg,45419
-easygraph/functions/drawing/layout.py,sha256=28DXSw8HalXz92iIxbklHD9UrWXi8Q1-zui_uvo3F6k,975
-easygraph/functions/drawing/geometry.py,sha256=yW7RRpPVrkSYUn01FfPG87NeRVUiNhZNzaHtoqEKLU0,803
-easygraph/functions/drawing/__init__.py,sha256=0C8zizV0rcAmY_I5u0hkNKwp1hC7mYX9Mzc9z5Chfn8,70
-easygraph/functions/structural_holes/MaxD.py,sha256=WyJJ3O-mC1kwTmB4Gi_sZ3zvYXrQ5Die49NHqkBC2rQ,9822
-easygraph/functions/structural_holes/maxBlock.py,sha256=f8n_fmGQmCgw3NDo4CPZW0yvjI5y0-GQNFYyfXaqX-o,18955
-easygraph/functions/structural_holes/ICC.py,sha256=ilJ3NaOa8610iXTbnJYjBhH25K9U-OCl653-HyFTeBI,7104
-easygraph/functions/structural_holes/metrics.py,sha256=NYQFhsy-3wupEVd7oW5pd5RmRUCimNPuVNu6BDudiG0,13179
-easygraph/functions/structural_holes/HIS.py,sha256=iJQvCPsIAlqgalTK5WsC4Dqf9Gv_x_RZuMrQ3ZsBWzM,4364
-easygraph/functions/structural_holes/weakTie.py,sha256=gW-T81TMrpA1TPCetkKyYGfaHmHy7LTRsD1fgemHtLw,9978
-easygraph/functions/structural_holes/evaluation.py,sha256=z5YcXNeNxg5n15X5Se0oHKLLnBgSJaz6LSpkmrnbeKk,12818
-easygraph/functions/structural_holes/SHII_metric.py,sha256=whFBaM31f4xAISVBkDu1sH7lkK2Q_NBxwkC3-uOejPE,10650
-easygraph/functions/structural_holes/AP_Greedy.py,sha256=MwPbbU0WYVj40suX8zRw8n040CXZgpC7l5D53bN0OL0,11359
-easygraph/functions/structural_holes/NOBE.py,sha256=HI0rzEvw2tGsZmD3Ceu2i0Y_GE6DhNZ4ejautmE3VHI,3857
-easygraph/functions/structural_holes/HAM.py,sha256=skwV_zHTii6tOogY4ZKZP8jH5YJp0VWdEiO93djJyn0,8168
-easygraph/functions/structural_holes/__init__.py,sha256=60lOb_NVftcPgqiKXvtGOccx3VoMcL0CqJTDBlEbTt0,171
-easygraph/functions/graph_generator/classic.py,sha256=XnB57pMpY8cfnqfdpgQwGtmKsNsZ9nBeiqFZ7zs_BE8,1894
-easygraph/functions/graph_generator/RandomNetwork.py,sha256=WIJkuv2e911gBbYuXCfHE2RGVhknEg_YGQEJpHPyzCM,12333
-easygraph/functions/graph_generator/__init__.py,sha256=TYV5JxCt7eBvWb75YR8XsPrtOvCgPVvNPTl0NShk88Q,52
-easygraph/functions/core/k_core.py,sha256=WQ7_To-4D419SR2hsKD1sDtvnvcfAVpZRt0Zl-Ojf5s,1945
-easygraph/functions/core/__init__.py,sha256=Y2PRaV1U9bKG4ABXHs8NGqX4AvSMw4b-g3u38Y4y51Q,22
-easygraph/utils/convert_to_matrix.py,sha256=xcR9YrSwNp-G1IR35jZgkwzImsmwloAV0gQr7r2ALlg,34870
-easygraph/utils/exception.py,sha256=0lNZ0qMnWyeZCg14MWa8csF7K-A0iDHY--WYX5Gnl8E,1018
-easygraph/utils/index_of_node.py,sha256=dQTKA9lQTB4J8-XNbLKQTHiFfFH9mNSli8V2mM0ufXU,291
-easygraph/utils/relabel.py,sha256=j9901sfjWiwc7kpK65cimmCXPdLJp75frkXhNMhpXoU,3662
-easygraph/utils/mapped_queue.py,sha256=2DlG251Wq1vjG-iCb0pnFFqMJ28NTmuKyPnEBx6XfcQ,6017
-easygraph/utils/decorators.py,sha256=xsYyagJvr5Kcja66wSya9a4h1KZahUpJX77CyWBbLC4,42004
-easygraph/utils/alias.py,sha256=-gZfB7F_oZA5yg0ERufqlos6D5g65QToMYouZpbCBkU,2595
-easygraph/utils/sparse.py,sha256=YyU7QPPnwOPpkOuqvPb5exp-7Bj294Lq1-2nLH_26iA,1216
-easygraph/utils/type_change.py,sha256=JiQHxjwR7mBotTTY1bnAte-zybwtfQgGkrWzqeZGSpg,4385
-easygraph/utils/logging.py,sha256=thQAX-5kwFoosuZf7jNcWqkqiG0Z3Hll-p0JcYzYTek,1343
-easygraph/utils/convert_class.py,sha256=55Ll__XWSDqlebh6-9uGC-y7VhKpdtUIUEsNTpXABxw,516
-easygraph/utils/misc.py,sha256=JTl1_AWZ0BM0TTfrrBKGSlG5Sae7K3WCCIJnI4f0rtk,5846
-easygraph/utils/download.py,sha256=tH6SKF304jtFQcz3-AbkDU0rgbb43IAy0mjfpjW9jis,2552
-easygraph/utils/__init__.py,sha256=034eumTmwiu5yChHlVGPw-VKk-L5z4PG0K6rUdzrXso,525
-easygraph/experiments/base.py,sha256=gzHLh2WkIIGnSE_TMHclPsG-BJKHwHG06ll7dR-Y5hQ,8648
-easygraph/experiments/vertex_classification.py,sha256=DKE7wXd_nrYh2KGxF4OF1Gq0nhmYBxrk08nZFJHgPBk,6408
-easygraph/experiments/__init__.py,sha256=aJpy5Wte0u2KXJCa_iX1IaKZ-4WF9Cj63Vne4hP4yOM,260
-easygraph/experiments/hypergraphs/hypergraph.py,sha256=Gt8bti9sRLNUvLe-iluhLvBXNRS1ftF4-htV4jCKfTE,4924
-easygraph/experiments/hypergraphs/__init__.py,sha256=JKczgu3ybCi6-PvvTFnJoPMviJjEYDj6fCm7pPESwo4,59
-easygraph/nn/loss.py,sha256=5Uq5HXqXyPARM1-Z0xKL6Yu1pEj5156gUfhddOW9I3E,1722
-easygraph/nn/regularization.py,sha256=QY5LIJBtnEFOlnpvI7ync3L58doGOp0bgXq3pYrXsPI,883
-easygraph/nn/__init__.py,sha256=wGpDYE_CdiQqtFEQKAI-mpod0Wvp-epi9MXivp0zbao,856
-easygraph/nn/tests/test_regularization.py,sha256=DuMXG4398mZ89CeOevi117SW51ToAu8HduLNRBgsSMw,431
-easygraph/nn/tests/test_gcnconv.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-easygraph/nn/tests/test_gatconv.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-easygraph/nn/tests/test_graphsageconv.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-easygraph/nn/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-easygraph/nn/convs/common.py,sha256=SyVZ1y_KxuFTL3ahNcFT4TRF3Yid-LxsIoLoFtoWj9M,5520
-easygraph/nn/convs/pma.py,sha256=cFZO2R_r5W1WySvr_vvW_DXNEyBpa8OxF_CdQpjZiYQ,6561
-easygraph/nn/convs/__init__.py,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-easygraph/nn/convs/hypergraphs/unignn_conv.py,sha256=-Qw5nDS--OBjLFuyvwlImtPXRcfj-rdNlFpAgmFLzFk,12363
-easygraph/nn/convs/hypergraphs/hgnn_conv.py,sha256=13ur0YSzZfTI2dGnNI0mnwYfj7AmuqCzgYHMKO_J4Y4,3267
-easygraph/nn/convs/hypergraphs/hnhn_conv.py,sha256=Esmw1jVKyHZC_qEzn2JKnDchW7ubMt1flZbiauRp7YQ,2175
-easygraph/nn/convs/hypergraphs/hgnnp_conv.py,sha256=WfbID56H8haK2GKrka-duu3N9rZ5BlApZxwYDmbxKcU,2747
-easygraph/nn/convs/hypergraphs/hypergcn_conv.py,sha256=fJdurRwKo-7LKJD_YCRcLeJnys4v9Vyqs8VmyHTGlwk,2789
-easygraph/nn/convs/hypergraphs/dhcf_conv.py,sha256=g_G5f4DkcM_Lv15hh6WHKov6JNsNIwtxO5cF58v_H64,2234
-easygraph/nn/convs/hypergraphs/halfnlh_conv.py,sha256=4qSKPKTTAmWqgIBrX5G_UphvUsmsBeFrtfz2GQPJr6s,3702
-easygraph/nn/convs/hypergraphs/__init__.py,sha256=yznEkQWJ3MV3VF45xW0n1gG_qwIYtLORkcxSeCm87_M,313
+cpp_easygraph.cp39-win_amd64.pyd,sha256=NmRIdvYGsOuLew1xC94S8xL8HFutwMXEHRcL_ul-EaI,492544
+easygraph/__init__.py,sha256=_tYVwlaEEaOTJci7mTSZ6zSNU8sFINk1sDU2-mxZs2A,729
+easygraph/_global.py,sha256=aDpFtXbF3c_JRAThlt8NY0hEnJK1BMj1FS3THxZZzLI,404
+easygraph/convert.py,sha256=Lu5DBb5chn-Il1VCLHe7rDZW7_gqDdieKby2IN0zXWA,19760
+easygraph/exception.py,sha256=g5VoeHf9YPxIXEqzS1GiAnjfOWRVBcA0aIkcnAEwCYg,2330
+easygraph/classes/__init__.py,sha256=UQTjBgw7iH-SOLfrJsfTMCdQh5tMphFMExo26MCYlkI,544
+easygraph/classes/base.py,sha256=wYduHzOKJjrpy0cTD9XYch-QkBNWgRLsxEe8zBgjynI,45970
+easygraph/classes/directed_graph.py,sha256=og82O0luYrjOIneO60Xhi4oEm6MWRVqmw4uuOu6r_T4,38487
+easygraph/classes/directed_multigraph.py,sha256=B8iq4mTBZ6hJKbqJebjHV7H0-MNaPpyUAZFJXdcqOnQ,15177
+easygraph/classes/graph.py,sha256=vdnQOu9zoDaW3sOlMbFmWxbJMU8synKgQ0HdfLtd6Jc,52087
+easygraph/classes/graphviews.py,sha256=sgDzSZiUtzVXaSfu74K6TzbeM7-QDsTxvuagI-YKEv8,352
+easygraph/classes/hypergraph.py,sha256=ImxY0_pF0QxJqMSmmVmfH5LwMTMK56sYPVMCDlrPW2M,108689
+easygraph/classes/multigraph.py,sha256=q6jrYaSB6t3zwz1W8f-5PDF3i9EoIEXdIiKBGKOf8xQ,25368
+easygraph/classes/operation.py,sha256=h5o6LFjXkyv2WkvqDSp8DUQ-cqIhR6TimuRor37WcI0,14378
+easygraph/classes/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+easygraph/classes/tests/test_hypergraph.py,sha256=jFdUDmvaEYfW1CjY3iJY5IErt3CSK_reS-l5r4qR8gY,31054
+easygraph/classes/tests/test_multidigraph.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+easygraph/classes/tests/test_multigraph.py,sha256=-upu4qaaoFwShr3dNuEQIm3zBsZJaHavvV4sQ1Cpl9Y,2072
+easygraph/classes/tests/test_operation.py,sha256=6QZmTgPueY9CDemrbDVOwFUbXbLBAA5BXqDyjochaR4,470
+easygraph/datapipe/__init__.py,sha256=JxxL0EgvE0Px6db-6lPMJP79IfZ-ZoUYWbY9N_ppj3M,746
+easygraph/datapipe/common.py,sha256=Rja0i2GnxN4_qsrQ9099hLJ1FBuDnYYn14JQwgnxM_4,3015
+easygraph/datapipe/loader.py,sha256=J351tm0luHEDyDVnQbglJBsD5c6TNPQI1Wn5K3O6ewY,2987
+easygraph/datapipe/normalize.py,sha256=erHG9k6vjVOnrR3FK_qkfrx48EdXK6dk6Tntu3q8Q9M,2808
+easygraph/datasets/__init__.py,sha256=FgFfyEo1e3RyoN1oFoxwG_aKoAZ4PHGB1TLK_6hzC-o,744
+easygraph/datasets/citation_graph.py,sha256=Vb3gJmgT1IKCSFfgHhpbOV-EqX0dD5-iohFOYKxl6Ao,29204
+easygraph/datasets/get_sample_graph.py,sha256=2FXgg8MTBaiezvdjHjCAvXB3FCznl6ct2Gj0JqB64Bc,7086
+easygraph/datasets/gnn_benchmark.py,sha256=TEl3msKQ_ur7jvytNxmRlZ-LGbPV08cbIavrhtsBUdA,7164
+easygraph/datasets/graph_dataset_base.py,sha256=58AAEiVNaTq33xqV8L7tgk5mPvPPDPiflRebTgCkNXE,9833
+easygraph/datasets/karate.py,sha256=urq0jEEJEBsOlAduviyAWMdfvJ2SvlJpd4uFizlaswM,2647
+easygraph/datasets/ppi.py,sha256=DDpWPSnaW4DY3lmlvcSTddRgko2Sxc_dMNLaPBld-DQ,6878
+easygraph/datasets/utils.py,sha256=VUiIlSxcOjfTUVMe6XpCuj5pK7s0h7zc2c-DQZ0kmpI,10692
+easygraph/datasets/dynamic/__init__.py,sha256=awvijxaL6qMykc4ZWMIyCgZmsnrqMCUcCP_03WIq82E,112
+easygraph/datasets/dynamic/email_enron.py,sha256=bWrB1GruAlSoc1yl8xW8Fukt9SN3sb82Cqg282DplEc,2586
+easygraph/datasets/dynamic/email_eu.py,sha256=RJaIXXYIc0hMl7tMBx4-K8C7lCeF0B1YDO2us-JmqaQ,2452
+easygraph/datasets/dynamic/hospital_lyon.py,sha256=SJYOMbD7w22VADqcaekbfjcn4Gk66x5DjH-kMR_B248,4334
+easygraph/datasets/dynamic/load_dataset.py,sha256=EZdvqJSdHT0SWNYlRFeTkoiz99Z0lGYlV6_9RdIm-rU,2565
+easygraph/datasets/hypergraph/House_Committees.py,sha256=0L_ebE8DrExS3YmZdkGTvNNhE8XzPB4DZHekBnNpEiY,4172
+easygraph/datasets/hypergraph/Yelp.py,sha256=RYADrojqbtxUyrm8i7SyL3xqb8GISs62DMiJOorxpUY,3443
+easygraph/datasets/hypergraph/__init__.py,sha256=EORj6Ucp1toz4A0z9l7yF63CZI_yT0rU1aLWcQTX4CE,285
+easygraph/datasets/hypergraph/_global.py,sha256=meW-Wya4QXQgZ4q1Hk836XrW3xeauoNakdVOta85rp8,350
+easygraph/datasets/hypergraph/cat_edge_Cooking.py,sha256=S7cg5hVpKlsSR-VL9dc8CA9SDQWVvcuMxyXxP9mO1zY,4221
+easygraph/datasets/hypergraph/coauthorship.py,sha256=W0y4F44h3zATpyCWNfIIiSpgRZgJbLIXvV9i4tyWSh4,7879
+easygraph/datasets/hypergraph/cocitation.py,sha256=VvvUVLNac0jmg7v-YIU4T5r6USKd4GfDEyStEmJmEEU,11630
+easygraph/datasets/hypergraph/contact_primary_school.py,sha256=kFTUZed3TEbao6t6GnvNA1YGQOG4zlIg5UetMcl2gP0,4117
+easygraph/datasets/hypergraph/cooking_200.py,sha256=dt8Ug59OFR3GLbiSXBx3cmssV_SuuybVXglMwGqDEPg,3545
+easygraph/datasets/hypergraph/hypergraph_dataset_base.py,sha256=xjGKWeP5GSTgEPHN89-j5Fz3QRaOYqnAOTxx9b-v1Eg,4384
+easygraph/datasets/hypergraph/mathoverflow_answers.py,sha256=MDCnNOgMlQPXzhN9akAAWzVpCS4zks3Or9IyX4Mkdq4,4371
+easygraph/datasets/hypergraph/senate_committees.py,sha256=9uL4XkgOjMRXY7ofF6tie0L2q18HWN4rHhDRdXmHVxY,4104
+easygraph/datasets/hypergraph/trivago_clicks.py,sha256=TccyoDG1LiQvalzxSENzsbhFWYw8-HDyZ_j2KXO4s5Q,3838
+easygraph/datasets/hypergraph/walmart_trips.py,sha256=COTq8rBuY0fvLp_YVa3GHT8mqxj8Z-mCuJgfRgrdTV0,3963
+easygraph/experiments/__init__.py,sha256=w6WDMe1CjBRQoTa6Gc1njN_LBkhxIAhxM2XsbnGui_A,270
+easygraph/experiments/base.py,sha256=pivlx2Y5mYfSZ1BdZ4RJ45_RrbT6GzkC1CS_wLz17qg,8852
+easygraph/experiments/vertex_classification.py,sha256=JHYid3zx2eEjb9s8hFaJxsGlWFqFquLVQ8hRD5l2F0Y,6574
+easygraph/experiments/hypergraphs/__init__.py,sha256=nzNLGgdnbPbvXp0LVCpS3dAP4frFt-ilApE1u8TS7Zc,60
+easygraph/experiments/hypergraphs/hypergraph.py,sha256=cbgMCRVoXkUyw6ZQXjTYKMTLyxc0I5eeH4o-U3CH6wI,5045
+easygraph/functions/__init__.py,sha256=a2OfVLgGWKWHN2F2iIouB6JbAqtEVRGnbRDiKBQzL3M,579
+easygraph/functions/isolate.py,sha256=naFBthkapZd_t04U0mOF5ok9SDpxVp1JE1dhQeZyVLM,2377
+easygraph/functions/basic/__init__.py,sha256=MKum3Pb2Z0tRHw5hl7Z7PQ38Fefn9DT8GUS6ER0DzdA,118
+easygraph/functions/basic/avg_degree.py,sha256=QmiJBLv5cL84zxgdDQWh-431bq3RAXjWr1ou_IwZIIY,660
+easygraph/functions/basic/cluster.py,sha256=JwFyKGnqX92JuvzDr33gnSk5o43WCm8XEz0lGf7KbNA,19868
+easygraph/functions/basic/localassort.py,sha256=DyTv8CjD3caJbk1g5Pwti0AfwZInjzh97DdDln48MH4,7290
+easygraph/functions/basic/predecessor_path_based.py,sha256=4nYER65F1FJb7ySL0hmNQIMy5HtP1Dh89Rg5U-ZCcd0,3035
+easygraph/functions/basic/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+easygraph/functions/basic/tests/test_avg_degree.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+easygraph/functions/basic/tests/test_cluster.py,sha256=F5rD3xffF8ngexa5A4Ue6GtbsQeol4stXrnq0P2CgY8,11371
+easygraph/functions/basic/tests/test_localassort.py,sha256=g4cFBE8am3DHqg3mUFLlVgRRD1iG-VMDPr0thudBXxU,1100
+easygraph/functions/basic/tests/test_predecessor.py,sha256=-RudwsRfJZX-PXJzIlG0rvjxGDChXE-L34Mvk_8Ujkg,512
+easygraph/functions/centrality/__init__.py,sha256=9xbH1h0EMGQT0fgNDwQKS1fqvzZD5oS0VDOZ4w-nYxk,192
+easygraph/functions/centrality/betweenness.py,sha256=5U67WFET_IzT9PctDxqyWj5JsT58TDqD3H4tdnAbcSQ,6805
+easygraph/functions/centrality/closeness.py,sha256=0bdf2s1yvnx_HKhqOla_z0rpvwgABXIJ6HFGUrOlBSQ,3041
+easygraph/functions/centrality/degree.py,sha256=O9LVEcCKnT0cAsx6aGiTGEDmM-aT6hOmRSuVmKoHuRs,3164
+easygraph/functions/centrality/ego_betweenness.py,sha256=-UdP9hHZDes1FP9OWY663uRS4pWx9w4HwtAu_ri07C4,1552
+easygraph/functions/centrality/flowbetweenness.py,sha256=G0Q741elRmgOH7huxxNo9KxA_29JkxhQAf34huyCLc8,4257
+easygraph/functions/centrality/laplacian.py,sha256=j0lfaXLOBfwXYqnRmQ3507PRUNlOX-ebsI_cojO8CXM,3906
+easygraph/functions/centrality/pagerank.py,sha256=qlubdUeWqFseWVb522Oo8C-KaUka8ijyByCJ0jFgfJI,1377
+easygraph/functions/community/LPA.py,sha256=RxB_av22T4omnr80LEaRPaSD3Fk5vHtqpLE0YkwMy9U,24613
+easygraph/functions/community/__init__.py,sha256=mCG6ew5rV6NBF8bofwAhwCr0bRHMjH5uU-r3U9-M6FY,160
+easygraph/functions/community/ego_graph.py,sha256=zmsg1PpWMDkDqSAnSg1sYSwhm4GXmKOw2V_9YeuobXA,1999
+easygraph/functions/community/louvain.py,sha256=xnFOhVJdN98bL1phvWW73AGaIQ4fLsCVdCVYf73yPyg,12690
+easygraph/functions/community/modularity.py,sha256=RlaMgMljbqpXajhMknA_ds2dwtXwOxxbA-zhYEINaMc,2039
+easygraph/functions/community/modularity_max_detection.py,sha256=X2u3tcRHWUOzDT1TofayC8qg3nkQPtg0OxjnBtM6XWw,7571
+easygraph/functions/community/motif.py,sha256=_bSgJ5fKj36VLYounpVW4lCHeD3YRMeOjgcQg6HpQmI,3448
+easygraph/functions/community/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+easygraph/functions/community/tests/test_motif.py,sha256=i8qZEqHi8rsEFmmkGALRvez9v2ELfUy72JxiQXGcTW4,534
+easygraph/functions/components/__init__.py,sha256=H1wE9JkKM9uN0YV_wLyZzTf1U-0AdqYUTPlsinQUt2g,122
+easygraph/functions/components/biconnected.py,sha256=g8WHtYlBHifxDTWWuoJere6j82SsYtEId6AwMGr70_g,8037
+easygraph/functions/components/connected.py,sha256=g0RgF-l12iYS9qmWbnAavdp-Fdo6EsEVbV93XDahDXA,3732
+easygraph/functions/components/strongly_connected.py,sha256=KANiYDtStJkQnxpsAEkVGxdvWfdqDCBme7ZEZjfF4mM,7243
+easygraph/functions/components/weakly_connected.py,sha256=p8Dqe1O0LNFyfxzuKGyHBCdtO4NGij9-C7ev8hh8LAM,4270
+easygraph/functions/core/__init__.py,sha256=SIdZ680qL8zGxqjTR_0nDRuvNtLlJbamwKOn56TKwck,23
+easygraph/functions/core/k_core.py,sha256=rFZt4fDGb8D7tON0nnvlBz7BSEG3psmr56tKo0ZHwd8,2014
+easygraph/functions/drawing/__init__.py,sha256=5nNrF-1NNGQV0Piy1aTwDCRLxHXGmuStOtRbe6MYhfw,73
+easygraph/functions/drawing/defaults.py,sha256=6vPfT5jTn7QunJ57ODDWwakfSUoA-2m_IM9_IiRUaoQ,8142
+easygraph/functions/drawing/drawing.py,sha256=aD2TADnpGutHF3mroF9Q26go0tfFkPFZ6gSwXAnwOQc,55938
+easygraph/functions/drawing/geometry.py,sha256=43nBrwBVmY4QAm8ZdTyGHt0V8plm40IYd-i4HtIvVmc,844
+easygraph/functions/drawing/layout.py,sha256=aMat33v1iPfdpTNcS5AtOJpNaLCE_3CNIq15z1W9YVg,1008
+easygraph/functions/drawing/plot.py,sha256=VVBAvtRHPkbCKJFUn-M3JF1zuIkLXX-svgTd-_jGgXs,6458
+easygraph/functions/drawing/positioning.py,sha256=uKfw9Nr2rH81x2-93VjqiHjMHeqBOk7OmUhY-8NlPPs,21074
+easygraph/functions/drawing/simulator.py,sha256=8syjePfIcaweErL1kHGCk7-0PYUEr8w5hX8uN7YF1qM,7174
+easygraph/functions/drawing/utils.py,sha256=sMlWH-jwGSlo3-Oxlz8P_gn4lQ94-Sy3JTLDVIFaiSk,20175
+easygraph/functions/graph_embedding/NOBE.py,sha256=vf4c1DeklKT-hoAV2k1wxcF1HulW9qDDUdqivSUYsKc,4117
+easygraph/functions/graph_embedding/__init__.py,sha256=BIQ7t2yalOf-3DV2wP4S7tFnuVQRaccUiFo3txUnye0,306
+easygraph/functions/graph_embedding/deepwalk.py,sha256=pcbeVIpHB_ZJuv0mw6bBi6w4xceOLVaywbyJVgPVgmk,3012
+easygraph/functions/graph_embedding/line.py,sha256=0fapELqEsRc-MDXxmX0E__maGmfEkUXjGxFOYZlQrxs,10080
+easygraph/functions/graph_embedding/net_emb_example_citeseer.py,sha256=bRufAk9GVXucWjQBXv0UfTWf-GM8YxFjGs067b406Qo,5270
+easygraph/functions/graph_embedding/node2vec.py,sha256=uk37iNXtvQ-YB39QHshlX1C27wSfUQDjDCK5JrcL8CM,9300
+easygraph/functions/graph_embedding/sdne.py,sha256=9rFdJwx4m7WcPqUKynK-Tm3OKh9tBcvmaSw8UTKd1lg,9569
+easygraph/functions/graph_generator/RandomNetwork.py,sha256=zbDAtqu2eAaLPdY5HsJomWH-8KqsSIT9k_B-2rwAL68,12744
+easygraph/functions/graph_generator/__init__.py,sha256=1wsdOlitFSe0dYyUr-Qmp3dGMej7DDjYkbZ42RcGiQI,54
+easygraph/functions/graph_generator/classic.py,sha256=CKZRvah4dBgQaeSCvJx8scvxfsG4buVifK4-gIiOigA,1967
+easygraph/functions/hypergraph/__init__.py,sha256=zWhsnIuIk4_O20Je389xIvPsdoyfSlOuiBiX4ZRP4Zk,159
+easygraph/functions/hypergraph/assortativity.py,sha256=osTEtb_ODHy4vYUr10W_rmgbmj9V06OmDQCq-WE9eTQ,4830
+easygraph/functions/hypergraph/hypergraph_clustering.py,sha256=n6hFEJRw-5QHGlO2dOYVZWzcOM3FmEKPNNsQxvwSTzM,8396
+easygraph/functions/hypergraph/hypergraph_operation.py,sha256=0olA6aNbRO8oPNleP3ybtttNVTWZHDBYQS_pnoNIId8,2338
+easygraph/functions/hypergraph/centrality/__init__.py,sha256=KYMC2_QPcKPX83pnjK2-tWqv-JSOoApJyMl5a6FlkVU,144
+easygraph/functions/hypergraph/centrality/cycle_ratio.py,sha256=EugRL_ohvxmcDthy7jw7d5GQd0Ia6rAnjTF0cl7VSZI,6483
+easygraph/functions/hypergraph/centrality/degree.py,sha256=wxlWNGJsk4DVFXGmL6EkGzHQAIcelUnQOouq2yQL3Cg,502
+easygraph/functions/hypergraph/centrality/hypercoreness.py,sha256=7gpJwp5xb1xUxu4yBFAZ1_-PVUhirl5dz4_8jzAaUb4,11930
+easygraph/functions/hypergraph/centrality/s_centrality.py,sha256=qImsEb2FW1ArHpgFoNeIXWh0FhaZgLKjvqCeecGEnBg,2362
+easygraph/functions/hypergraph/centrality/vector_centrality.py,sha256=bST77QSxYbCmFkhQEtn4_-vNakxoJD6IN8TY9eqDH34,1852
+easygraph/functions/hypergraph/null_model/__init__.py,sha256=MQZYQTdpz2CHk1srtDJRXoe-0lWRuntvlZ3gUNCsFvc,129
+easygraph/functions/hypergraph/null_model/hypergraph_classic.py,sha256=4VihAB-FxQMSKAzTiuflA-ExKa1x326hrO_nBdudtcE,979
+easygraph/functions/hypergraph/null_model/lattice.py,sha256=0-NDxuycbW4F46xMCy4gAoebuZZCR8Vl7q2-64EKBu0,1531
+easygraph/functions/hypergraph/null_model/random.py,sha256=Bn78EFmK9DjWMIrltfBzMEQ7exewuU1_CEMLts3F74s,14011
+easygraph/functions/hypergraph/null_model/simple.py,sha256=ndENjYwxHkKhnK8wFC6uCMGL8c6HqQuS0_Wkj5CDEuo,1951
+easygraph/functions/hypergraph/null_model/uniform.py,sha256=zFuDZt2AxK49vjfw3SjzBRGaG857xPKzuux7XLNcwa8,13047
+easygraph/functions/hypergraph/null_model/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+easygraph/functions/hypergraph/null_model/tests/test_classic.py,sha256=-9agJismNtsN8gfkqUusRl_lk1qSK2zIat-rP8UFQcU,1506
+easygraph/functions/path/__init__.py,sha256=4n-RRCryFfPkCgwP5IhtXLZCDrCT3nAoxDmfn4vgviA,135
+easygraph/functions/path/average_shortest_path_length.py,sha256=i4a1pBWORNBsZYj7u76AgWeqcAb7KEgOXjDl-80assU,4175
+easygraph/functions/path/bridges.py,sha256=TA_Acau1XcvKxOtyjytpk94h5LTGu6jHnhsksqm8BQM,6178
+easygraph/functions/path/diameter.py,sha256=f8Ej1qs4ACi53KlQVKLurvb9qFYKrkWrihk06J-Aibo,2689
+easygraph/functions/path/mst.py,sha256=-dkGD9bF-f8TcfUAoGJEL4IKaSZ8kfxb79tWCauSJrw,23296
+easygraph/functions/path/path.py,sha256=r74GYC1F8mMlfJ4cGBHJaKUyTPkkwXayxjfsi2gXSbs,6710
+easygraph/functions/structural_holes/AP_Greedy.py,sha256=splWfjsFS_18g8Rvtnfq4VzgyZxGiIW3a5WD88V2XAg,11761
+easygraph/functions/structural_holes/HAM.py,sha256=dIugPRdRcogZ4pnq10_YKJDe6_fUTj36x__MISZS1CM,8468
+easygraph/functions/structural_holes/HIS.py,sha256=427eB3H_hxcux9qEmeC9bY3yW1UQx3tmbFROiylddGE,4511
+easygraph/functions/structural_holes/ICC.py,sha256=ZpE14veInnVE9evQ2JX0o5g7EwBlxFm6Ut6XkRzB8uE,7402
+easygraph/functions/structural_holes/MaxD.py,sha256=UeKBXV8hLGquUCMALmaMcsdCgo4F3BI4aGlmFJc-kKw,10269
+easygraph/functions/structural_holes/NOBE.py,sha256=FI0CFmKuOTVCtd8YRVrF0bO0m2eDH-zUAsB9avuA-y8,4024
+easygraph/functions/structural_holes/SHII_metric.py,sha256=YQ3NkTxyy-ybIfhjGqNl7KLAZOD76BJ0eY45U-IS2pE,10944
+easygraph/functions/structural_holes/__init__.py,sha256=5BukquKjQVH_PiGBxuL_U6vAICCc64nuudks46svcqw,179
+easygraph/functions/structural_holes/evaluation.py,sha256=QAZWq800Pn3ladatEEMKBufe668bfmXg-Y6GmlIQaIk,13319
+easygraph/functions/structural_holes/maxBlock.py,sha256=Aj46GUXsrghKTp9jSQMUSN3C0rVw7xN9nba0_QhQLtI,19566
+easygraph/functions/structural_holes/metrics.py,sha256=66l-JFUrQqP8oHEFN94CvbEuaXSqYf0W8OnAigfd9Oo,13573
+easygraph/functions/structural_holes/weakTie.py,sha256=5J4TWZ_fgczNfqVqMT8GITKolgMfX-pcWDXJNNMY7oQ,10316
+easygraph/ml_metrics/__init__.py,sha256=JHKsr7ad9UKXNpFyCke7jFoHwZLTN08SI3AS5KGdT-s,1619
+easygraph/ml_metrics/base.py,sha256=uus8RMYTVzfTt6oK9_d4-Iw3BZK-0k-2PJJStRm6-UI,8500
+easygraph/ml_metrics/classification.py,sha256=4zJ0CZmkxJ3Reb55oZg_3SAZY4Zjz3FTPOQt03P7bNA,6555
+easygraph/ml_metrics/hypergraphs/__init__.py,sha256=lKc31nerh0F3Oq7fxuWRRxrSN1JdJl2iiDENpHnsXM4,65
+easygraph/ml_metrics/hypergraphs/hypergraph.py,sha256=fKWMeOzwV8BHAUinzPZjp5p4Btp3oegAFycDBVcpjbo,1777
+easygraph/model/__init__.py,sha256=8nbhk3q2wvULC-VEMKRoKFAgNmjA4xKSSs7nSfLPPHU,523
+easygraph/model/hypergraphs/__init__.py,sha256=dGChFGOB_yhvCupWbypK-9jx7xBu8Xui1yoaIRKS5vY,271
+easygraph/model/hypergraphs/dhcf.py,sha256=hc_1xz7_CtQ_j4EGPyX9G9VDDYBNPSDCbTao66RTKC4,3831
+easygraph/model/hypergraphs/dhne.py,sha256=omZ7tiU2eJ6k0N5Eun-tNmKpv5FtQS41KW949FgncKg,2626
+easygraph/model/hypergraphs/hgnn.py,sha256=rQ2l40Npql6N1oq29uzGTz7J7tuppBo5hmMqVkJq4yc,1893
+easygraph/model/hypergraphs/hgnnp.py,sha256=8LClufTxX0O_2oW2F2-HInbOg1geMZpvOoz2D8yfwjA,1670
+easygraph/model/hypergraphs/hnhn.py,sha256=i_1nR18ev_zYqsbtJBOhWms1w7dcQijV087jCa_nh3E,1649
+easygraph/model/hypergraphs/hypergcn.py,sha256=BMFT2aJUoXK7bgqdQH68ZYXOL0BCKnmfr2Ob6Y_4uHE,2644
+easygraph/model/hypergraphs/setgnn.py,sha256=y8el79vRrmDFoP24d-9Dg-K_VOXlAaW7TP08xnMbSjU,37190
+easygraph/model/hypergraphs/unignn.py,sha256=icJmKNKvisCJLXp_wyQE6jcFmlTAAWdUuKQEe4dOyAY,8102
+easygraph/nn/__init__.py,sha256=x8XAnQ8vjBeUP-Y-rtKDVAOjtSkWCTS60ach5C7NAsU,878
+easygraph/nn/loss.py,sha256=SvgSMDXbLR1qsO9mx6BNZ-ZqxHN7aYNDZVmzCD39D1g,1768
+easygraph/nn/regularization.py,sha256=4Y_SQyqZWKSDS0HvBrT7SyKBIGrWIRQwO90UTbL4OLQ,915
+easygraph/nn/convs/__init__.py,sha256=frcCV1k9oG9oKj3dpUqdJg1PxRT2RSN_XKdLCPjaYaY,2
+easygraph/nn/convs/common.py,sha256=zv0ZrYD69puOYf6eZACfq5I3dQ8I2N_1uv6dBRqpqvM,5656
+easygraph/nn/convs/pma.py,sha256=me58eCgwm5aFJC4y6EA0DSnVxayVfb7C5KKbjGZf_Qo,6741
+easygraph/nn/convs/hypergraphs/__init__.py,sha256=9-MFfphzfS49zI6CyH1Obrf3sDrYo3fQYbloiSr0ee8,322
+easygraph/nn/convs/hypergraphs/dhcf_conv.py,sha256=sni58WQu1bEjmebefY4DE0_p4p0WAZY481nktLYnyvM,2288
+easygraph/nn/convs/hypergraphs/halfnlh_conv.py,sha256=EfrcMPuj2rYJoBGjsb4HTTDVNOadQC7nRyCotV0BvNk,3814
+easygraph/nn/convs/hypergraphs/hgnn_conv.py,sha256=i3WtAVmbHb3d0Rr5jKCrBsLqyMRFjSdWPRtmbJV8tww,3339
+easygraph/nn/convs/hypergraphs/hgnnp_conv.py,sha256=g9H2feGG6CckCU_ZaL1ua_aRcZz9VE8vkWnyJmMAuXE,2814
+easygraph/nn/convs/hypergraphs/hnhn_conv.py,sha256=rC8DZ898dRyyBAg_wXitnY69QdJcJPWDbztyQT5YxQU,2228
+easygraph/nn/convs/hypergraphs/hypergcn_conv.py,sha256=YPUQ4ktX7_WlWCD27EPbaccIvmep-JPn99FE9cBGXe0,2850
+easygraph/nn/convs/hypergraphs/unignn_conv.py,sha256=Cokmtwbg3UoozRGjzlnhbF8_E1S8alHuhgJqZNQKRM0,12653
+easygraph/nn/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+easygraph/nn/tests/test_gatconv.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+easygraph/nn/tests/test_gcnconv.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+easygraph/nn/tests/test_graphsageconv.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+easygraph/nn/tests/test_regularization.py,sha256=lEK23orv48bOP1NozP0vY5_EhDG94-4QDFo2cMTz7S8,445
+easygraph/readwrite/__init__.py,sha256=WF7L_UA5if8NGHgdEOlMSHzxIjZ3PHeB99kEeSl9oFw,381
+easygraph/readwrite/edgelist.py,sha256=7_tMzBdF09bt4qOKKtXGqqBW18r0fevvlnK-T89dbbE,14015
+easygraph/readwrite/gexf.py,sha256=Ps6JuLvgci3zs9KbRaWDMaYCCrXJTtx08T4CVlMjVUQ,39067
+easygraph/readwrite/gml.py,sha256=WKQP8xPU5cLApp41ho01QNNYev5n7qbTSPfpaDQmIrc,29129
+easygraph/readwrite/graphml.py,sha256=iFiem19SoaPcAkvAnk73PR2aVFglQ2Pk8908VFUm8Uc,40222
+easygraph/readwrite/graphviz.py,sha256=7uQwvejLe6BPdpIvasSlHFykrQUgG5_qkF_cHOslMlY,5218
+easygraph/readwrite/pajek.py,sha256=AUGj0vHNdNYinC7X19u_lSCEtkTva4lDYwuLQqzm8fc,11130
+easygraph/readwrite/pickle.py,sha256=PtSC19iCmw9lci1kncGjJ9DUzGeLE-qS32Y4DaDWIoY,274
+easygraph/readwrite/ucinet.py,sha256=fG-Mqec3jSW_u8oNnRZPuz11M-gzE8n1-mUWu6LZdL4,10529
+easygraph/readwrite/json_graph/__init__.py,sha256=EPqejcQZlCFVmxp_QHeS-FwN2OqXrzOqiWVZxw1Ekmw,537
+easygraph/readwrite/json_graph/node_link.py,sha256=0ttWozwgTxorlj2QxVEEzQou0Zh3fliOeZIJ6N9mE58,3442
+easygraph/readwrite/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+easygraph/readwrite/tests/test_edgelist.py,sha256=y52p2ir0Xv0DgluG328Fd9SduNWNf2KMAQ_iMOqn5Rs,10380
+easygraph/readwrite/tests/test_gexf.py,sha256=rdLE6h3e5M2o23UWcPf6pFNkS4XiOm-6F1FKv1mF3dw,15678
+easygraph/readwrite/tests/test_gml.py,sha256=U44goSnj0qXhMmF4dWapQIeYUF8E8ajKsLjs8Ebi40c,18005
+easygraph/readwrite/tests/test_graphml.py,sha256=nyKPbpBzwXurbKcLRtc64xSOi-4ozJTqxowlf6cUyMI,66669
+easygraph/readwrite/tests/test_graphviz.py,sha256=W43mgqfb0qkP84vXOm8o-xUhAbEI_nb7qGs3_efdaRY,1504
+easygraph/readwrite/tests/test_pajek.py,sha256=J9XfhNHVJjsAhWEVLUa7VpYLftgSsrhFJrMAY8fUALU,9560
+easygraph/readwrite/tests/test_pickle.py,sha256=7RrdmmAl1agMicbTHJ1C5kINARh9J80h57dD05INrco,2230
+easygraph/readwrite/tests/test_ucinet.py,sha256=jhQKyR4xU-O5j5CyvCWcLPi1diC_UB1U6OkTd450vbk,8384
+easygraph/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+easygraph/tests/script_test_cpp_easygraph.py,sha256=YmsGh0v0_FXBfvO6Ot4Qlwrb2ROhZml5uAxTQ6--dzI,7380
+easygraph/tests/teddy_test_cpp_easygraph_sanity_check.py,sha256=55D0M_dfSxMYHDDTo_DR02-TjOI1HvJH6L0COf9ZDwY,5871
+easygraph/tests/test_convert.py,sha256=80tv2pix4kOMN9gTs-n1WgZouMM-m1pg0F-qoBQQgSg,3142
+easygraph/tests/test_cpp_easygraph.py,sha256=XOIbVnRVdxdmQhbDXt2-dAx2Z0aw5fo357XuhjSPLaw,314
+easygraph/utils/__init__.py,sha256=91Amck-MeTWoQPCMZawrg_eqkypcghO-vqsrtXWU9bg,538
+easygraph/utils/alias.py,sha256=402sCgB6WZ92oiGtGu1bdgbFkZW0Rx8FjcrHXQuXclQ,2714
+easygraph/utils/convert_class.py,sha256=PsgpZANHOC9bnzS5QAC4QL3NWbpCqY4lQ1k5VBh-FCk,535
+easygraph/utils/convert_to_matrix.py,sha256=CoXs2srmidH5w4HMrGDye9Sc-brRRB6FTk6T2475njU,35872
+easygraph/utils/decorators.py,sha256=FEX8ipNXkMJT1GvpPB0kOdodNmQquwF5SGbj5oLYoT0,43149
+easygraph/utils/download.py,sha256=KJ7_gZ3u01xWazm98FHkQ1YRWkUpyZWKOKeKRzdUbBs,2647
+easygraph/utils/exception.py,sha256=Q8dOUlIjeuFfojMfaprLInGOno-8VJxzAqPGl0P_fsI,1061
+easygraph/utils/index_of_node.py,sha256=W8wL3NgvHGquya_cIyJURBEu1uJtrRnYRg8ncUYDX8s,303
+easygraph/utils/logging.py,sha256=SLfSV_JHBJH4vYfeeqInIz-P_OeUYYWRz0nUtJegg4A,1387
+easygraph/utils/mapped_queue.py,sha256=U2BJjrF8wIfSM91VokVXpLJOMyUZ_xTv2Xp9vL7CmCw,6203
+easygraph/utils/misc.py,sha256=PIeupWjVRRoMJpLyMrTdgFLXAjneOvh245qtD3vE8i4,6068
+easygraph/utils/relabel.py,sha256=NNl3Jv3vbfhOCyZC6pi3tPHREW0fdBS2jD92OI6Z9mw,3768
+easygraph/utils/sparse.py,sha256=YZI1itN7pEihZcmlSKHXioR0_rXfkSJ2bncSzWzhJkw,1255
+easygraph/utils/type_change.py,sha256=wvloYmi3Bl9EVA0LbMS_El0lBl-hBsVqEm0NG_E7UFk,4532
+Python_EasyGraph-1.2.dist-info/LICENSE,sha256=Fa3CfcWP950gZFsh7pBedstyySiOM_6lKCwc_9jywlk,1590
+Python_EasyGraph-1.2.dist-info/LICENSE.NetworkX,sha256=TvPIq4oedoOz8VIzgQFO5ZwbUVFyOzk4_uHTZgVchr8,1652
+Python_EasyGraph-1.2.dist-info/METADATA,sha256=NjCWdo6XTzyUbiR3jmP_yTYPePi8tLTGq8iQNk3G5ng,8230
+Python_EasyGraph-1.2.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
+Python_EasyGraph-1.2.dist-info/top_level.txt,sha256=sTleEAisCc-R1iwDM-6kfIHIHnEW71eyDRSAyePlumM,24
+Python_EasyGraph-1.2.dist-info/RECORD,,
```

## Comparing `Python_EasyGraph-1.1.dist-info/LICENSE` & `Python_EasyGraph-1.2.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-BSD 3-Clause License
-
-Copyright (c) 2020, Mobile Systems and Networking Group, Fudan University
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions are met:
-
-1. Redistributions of source code must retain the above copyright notice, this
-   list of conditions and the following disclaimer.
-
-2. Redistributions in binary form must reproduce the above copyright notice,
-   this list of conditions and the following disclaimer in the documentation
-   and/or other materials provided with the distribution.
-
-3. Neither the name of the copyright holder nor the names of its
-   contributors may be used to endorse or promote products derived from
-   this software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
-AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+BSD 3-Clause License
+
+Copyright (c) 2020, Mobile Systems and Networking Group, Fudan University
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+1. Redistributions of source code must retain the above copyright notice, this
+   list of conditions and the following disclaimer.
+
+2. Redistributions in binary form must reproduce the above copyright notice,
+   this list of conditions and the following disclaimer in the documentation
+   and/or other materials provided with the distribution.
+
+3. Neither the name of the copyright holder nor the names of its
+   contributors may be used to endorse or promote products derived from
+   this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
```

## Comparing `Python_EasyGraph-1.1.dist-info/LICENSE.NetworkX` & `Python_EasyGraph-1.2.dist-info/LICENSE.NetworkX`

 * *Ordering differences only*

 * *Files 7% similar despite different names*

```diff
@@ -1,33 +1,33 @@
-Copyright (C) 2004-2022, NetworkX Developers
-Aric Hagberg <hagberg@lanl.gov>
-Dan Schult <dschult@colgate.edu>
-Pieter Swart <swart@lanl.gov>
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions are
-met:
-
-  * Redistributions of source code must retain the above copyright
-    notice, this list of conditions and the following disclaimer.
-
-  * Redistributions in binary form must reproduce the above
-    copyright notice, this list of conditions and the following
-    disclaimer in the documentation and/or other materials provided
-    with the distribution.
-
-  * Neither the name of the NetworkX Developers nor the names of its
-    contributors may be used to endorse or promote products derived
-    from this software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
-"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
-LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
-A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
-OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
-LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
-DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
-THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+Copyright (C) 2004-2022, NetworkX Developers
+Aric Hagberg <hagberg@lanl.gov>
+Dan Schult <dschult@colgate.edu>
+Pieter Swart <swart@lanl.gov>
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+
+  * Redistributions of source code must retain the above copyright
+    notice, this list of conditions and the following disclaimer.
+
+  * Redistributions in binary form must reproduce the above
+    copyright notice, this list of conditions and the following
+    disclaimer in the documentation and/or other materials provided
+    with the distribution.
+
+  * Neither the name of the NetworkX Developers nor the names of its
+    contributors may be used to endorse or promote products derived
+    from this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
```

## Comparing `easygraph/functions/hypergraph/hypergraph_generator/simple.py` & `easygraph/functions/hypergraph/null_model/simple.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,77 +1,77 @@
-from itertools import combinations
-
-import easygraph as eg
-
-from easygraph.utils.exception import EasyGraphError
-
-
-__all__ = [
-    "star_clique",
-]
-
-
-def star_clique(n_star, n_clique, d_max):
-    """Generate a star-clique structure
-
-    That is a star network and a clique network,
-    connected by one pairwise edge connecting the centre of the star to the clique.
-    network, the each clique is promoted to a hyperedge
-    up to order d_max.
-
-    Parameters
-    ----------
-    n_star : int
-        Number of legs of the star
-    n_clique : int
-        Number of nodes in the clique
-    d_max : int
-        Maximum order up to which to promote
-        cliques to hyperedges
-
-    Returns
-    -------
-    H : Hypergraph
-
-    Examples
-    --------
-    >>> import easygraph as eg
-    >>> H = eg.star_clique(6, 7, 2)
-
-    Notes
-    -----
-    The total number of nodes is n_star + n_clique.
-
-    """
-
-    if n_star <= 0:
-        raise ValueError("n_star must be an integer > 0.")
-    if n_clique <= 0:
-        raise ValueError("n_clique must be an integer > 0.")
-    if d_max < 0:
-        raise ValueError("d_max must be an integer >= 0.")
-    elif d_max > n_clique - 1:
-        raise ValueError("d_max must be <= n_clique - 1.")
-
-    nodes_star = range(n_star)
-    nodes_clique = range(n_star, n_star + n_clique)
-    nodes = list(nodes_star) + list(nodes_clique)
-
-    H = eg.Hypergraph(num_v=len(nodes))
-
-    # add star edges (center of the star is 0-th node)
-    H.add_hyperedges([[nodes_star[0], nodes_star[i]] for i in range(1, n_star)])
-
-    # connect clique and star by adding last star leg
-    H.add_hyperedges([nodes_star[0], nodes_clique[0]])
-
-    # add clique hyperedges up to order d_max
-
-    H.add_hyperedges(
-        [
-            e
-            for d in range(1, d_max + 1)
-            for e in list(combinations(nodes_clique, d + 1))
-        ]
-    )
-
-    return H
+from itertools import combinations
+
+import easygraph as eg
+
+from easygraph.utils.exception import EasyGraphError
+
+
+__all__ = [
+    "star_clique",
+]
+
+
+def star_clique(n_star, n_clique, d_max):
+    """Generate a star-clique structure
+
+    That is a star network and a clique network,
+    connected by one pairwise edge connecting the centre of the star to the clique.
+    network, the each clique is promoted to a hyperedge
+    up to order d_max.
+
+    Parameters
+    ----------
+    n_star : int
+        Number of legs of the star
+    n_clique : int
+        Number of nodes in the clique
+    d_max : int
+        Maximum order up to which to promote
+        cliques to hyperedges
+
+    Returns
+    -------
+    H : Hypergraph
+
+    Examples
+    --------
+    >>> import easygraph as eg
+    >>> H = eg.star_clique(6, 7, 2)
+
+    Notes
+    -----
+    The total number of nodes is n_star + n_clique.
+
+    """
+
+    if n_star <= 0:
+        raise ValueError("n_star must be an integer > 0.")
+    if n_clique <= 0:
+        raise ValueError("n_clique must be an integer > 0.")
+    if d_max < 0:
+        raise ValueError("d_max must be an integer >= 0.")
+    elif d_max > n_clique - 1:
+        raise ValueError("d_max must be <= n_clique - 1.")
+
+    nodes_star = range(n_star)
+    nodes_clique = range(n_star, n_star + n_clique)
+    nodes = list(nodes_star) + list(nodes_clique)
+
+    H = eg.Hypergraph(num_v=len(nodes))
+
+    # add star edges (center of the star is 0-th node)
+    H.add_hyperedges([[nodes_star[0], nodes_star[i]] for i in range(1, n_star)])
+
+    # connect clique and star by adding last star leg
+    H.add_hyperedges([nodes_star[0], nodes_clique[0]])
+
+    # add clique hyperedges up to order d_max
+
+    H.add_hyperedges(
+        [
+            e
+            for d in range(1, d_max + 1)
+            for e in list(combinations(nodes_clique, d + 1))
+        ]
+    )
+
+    return H
```

## Comparing `easygraph/functions/hypergraph/hypergraph_generator/uniform.py` & `easygraph/functions/hypergraph/null_model/uniform.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,455 +1,455 @@
-"""Generate random uniform hypergraphs."""
-import itertools
-import operator
-import random
-import warnings
-
-from functools import reduce
-
-import easygraph as eg
-import numpy as np
-
-from easygraph.utils.exception import EasyGraphError
-
-
-__all__ = [
-    "uniform_hypergraph_configuration_model",
-    "uniform_HSBM",
-    "uniform_HPPM",
-    "uniform_erdos_renyi_hypergraph",
-    "uniform_hypergraph_Gnm",
-]
-
-
-def split_num_e(num_e, worker):
-    import math
-
-    res = []
-    group_size = num_e // worker
-    for i in range(worker):
-        res.append(group_size)
-    return res
-
-
-def uniform_hypergraph_Gnm_parallel(num_e, num_v, k):
-    random.seed()
-    edges = set()
-    while len(edges) < num_e:
-        e = random.sample(range(num_v), k)
-        e = tuple(sorted(e))
-        if e not in edges:
-            edges.add(e)
-    return list(edges)
-
-
-def uniform_hypergraph_Gnm(k: int, num_v: int, num_e: int, n_workers=None):
-    r"""Return a random ``k``-uniform hypergraph with ``num_v`` vertices and ``num_e`` hyperedges.
-
-    Args:
-        ``k`` (``int``): The Number of vertices in each hyperedge.
-        ``num_v`` (``int``): The Number of vertices.
-        ``num_e`` (``int``): The Number of hyperedges.
-
-    Examples:
-        >>> import easygraph as eg
-        >>> hg = eg.uniform_hypergraph_Gnm(3, 5, 4)
-        >>> hg.e
-        ([(0, 1, 2), (0, 1, 3), (0, 3, 4), (2, 3, 4)], [1.0, 1.0, 1.0, 1.0])
-    """
-    # similar to UniformRandomUniform in sagemath, https://doc.sagemath.org/html/en/reference/graphs/sage/graphs/hypergraph_generators.html
-
-    assert k > 1, "k must be greater than 1"  # TODO ?
-    assert num_v > 1, "num_v must be greater than 1"
-    assert num_e > 0, "num_e must be greater than 0"
-
-    if n_workers is not None:
-        #  use the parallel version for large graph
-        edges = set()
-        from functools import partial
-        from multiprocessing import Pool
-
-        # res_edges = set()
-        edges_parallel = split_num_e(num_e=num_e, worker=n_workers)
-        local_function = partial(uniform_hypergraph_Gnm_parallel, num_v=num_v, k=k)
-
-        res_edges = set()
-        import time
-
-        with Pool(n_workers) as p:
-            ret = p.imap(local_function, edges_parallel)
-            for res in ret:
-                for r in res:
-                    res_edges.add(r)
-
-            while len(res_edges) < num_e:
-                e = random.sample(range(num_v), k)
-                e = tuple(sorted(e))
-                if e not in res_edges:
-                    res_edges.add(e)
-
-        res_hypergraph = eg.Hypergraph(num_v=num_v, e_list=list(res_edges))
-        return res_hypergraph
-
-    else:
-        edges = set()
-        while len(edges) < num_e:
-            e = random.sample(range(num_v), k)
-            e = tuple(sorted(e))
-            if e not in edges:
-                edges.add(e)
-
-    return eg.Hypergraph(num_v, list(edges))
-
-
-def uniform_hypergraph_configuration_model(k, m, seed=None):
-    """
-    A function to generate an m-uniform configuration model
-
-    Parameters
-    ----------
-    k : dictionary
-        This is a dictionary where the keys are node ids
-        and the values are node degrees.
-    m : int
-        specifies the hyperedge size
-    seed : integer or None (default)
-        The seed for the random number generator
-
-    Returns
-    -------
-    Hypergraph object
-        The generated hypergraph
-
-    Warns
-    -----
-    warnings.warn
-        If the sums of the degrees are not divisible by m, the
-        algorithm still runs, but raises a warning and adds an
-        additional connection to random nodes to satisfy this
-        condition.
-
-    Notes
-    -----
-    This algorithm normally creates multi-edges and loopy hyperedges.
-    We remove the loopy hyperedges.
-
-    References
-    ----------
-    "The effect of heterogeneity on hypergraph contagion models"
-    by Nicholas W. Landry and Juan G. Restrepo
-    https://doi.org/10.1063/5.0020034
-
-
-    Example
-    -------
-    >>> import easygraph as eg
-    >>> import random
-    >>> n = 1000
-    >>> m = 3
-    >>> k = {1: 1, 2: 2, 3: 3, 4: 3}
-    >>> H = eg.uniform_hypergraph_configuration_model(k, m)
-
-    """
-    if seed is not None:
-        random.seed(seed)
-
-    # Making sure we have the right number of stubs
-    remainder = sum(k.values()) % m
-    if remainder != 0:
-        warnings.warn(
-            "This degree sequence is not realizable. "
-            "Increasing the degree of random nodes so that it is."
-        )
-        random_ids = random.sample(list(k.keys()), int(round(m - remainder)))
-        for id in random_ids:
-            k[id] = k[id] + 1
-
-    stubs = []
-    # Creating the list to index through
-    for id in k:
-        stubs.extend([id] * int(k[id]))
-
-    H = eg.Hypergraph(num_v=len(k))
-
-    while len(stubs) != 0:
-        u = random.sample(range(len(stubs)), m)
-        edge = set()
-        for index in u:
-            edge.add(stubs[index])
-        if len(edge) == m:
-            H.add_hyperedges(list(edge))
-
-        for index in sorted(u, reverse=True):
-            del stubs[index]
-
-    return H
-
-
-def uniform_HSBM(n, m, p, sizes, seed=None):
-    """Create a uniform hypergraph stochastic block model (HSBM).
-
-    Parameters
-    ----------
-    n : int
-        The number of nodes
-    m : int
-        The hyperedge size
-    p : m-dimensional numpy array
-        tensor of probabilities between communities
-    sizes : list or 1D numpy array
-        The sizes of the community blocks in order
-    seed : integer or None (default)
-        The seed for the random number generator
-
-    Returns
-    -------
-    Hypergraph
-        The constructed SBM hypergraph
-
-    Raises
-    ------
-    EasyGraphError
-        - If the length of sizes and p do not match.
-        - If p is not a tensor with every dimension equal
-        - If p is not m-dimensional
-        - If the entries of p are not in the range [0, 1]
-        - If the sum of the vector of sizes does not equal the number of nodes.
-    Exception
-        If there is an integer overflow error
-
-    See Also
-    --------
-    uniform_HPPM
-
-    References
-    ----------
-    Nicholas W. Landry and Juan G. Restrepo.
-    "Polarization in hypergraphs with community structure."
-    Preprint, 2023. https://doi.org/10.48550/arXiv.2302.13967
-    """
-    # Check if dimensions match
-    if len(sizes) != np.size(p, axis=0):
-        raise EasyGraphError("'sizes' and 'p' do not match.")
-    if len(np.shape(p)) != m:
-        raise EasyGraphError("The dimension of p does not match m")
-    # Check that p has the same length over every dimension.
-    if len(set(np.shape(p))) != 1:
-        raise EasyGraphError("'p' must be a square tensor.")
-    if np.max(p) > 1 or np.min(p) < 0:
-        raise EasyGraphError("Entries of 'p' not in [0,1].")
-    if np.sum(sizes) != n:
-        raise EasyGraphError("Sum of sizes does not match n")
-
-    if seed is not None:
-        np.random.seed(seed)
-
-    node_labels = range(n)
-    H = eg.Hypergraph(num_v=n)
-
-    block_range = range(len(sizes))
-    # Split node labels in a partition (list of sets).
-    size_cumsum = [sum(sizes[0:x]) for x in range(0, len(sizes) + 1)]
-    partition = [
-        list(node_labels[size_cumsum[x] : size_cumsum[x + 1]])
-        for x in range(0, len(size_cumsum) - 1)
-    ]
-
-    for block in itertools.product(block_range, repeat=m):
-        if p[block] == 1:  # Test edges cases p_ij = 0 or 1
-            edges = itertools.product((partition[i] for i in block_range))
-            for e in edges:
-                H.add_hyperedges(list(e))
-        elif p[block] > 0:
-            partition_sizes = [len(partition[i]) for i in block]
-            max_index = reduce(operator.mul, partition_sizes, 1)
-            if max_index < 0:
-                raise Exception("Index overflow error!")
-            index = np.random.geometric(p[block]) - 1
-
-            while index < max_index:
-                indices = _index_to_edge_partition(index, partition_sizes, m)
-                e = {partition[block[i]][indices[i]] for i in range(m)}
-                if len(e) == m:
-                    H.add_hyperedges(list(e))
-                index += np.random.geometric(p[block])
-    return H
-
-
-def uniform_HPPM(n, m, rho, k, epsilon, seed=None):
-    """Construct the m-uniform hypergraph planted partition model (m-HPPM)
-
-    Parameters
-    ----------
-    n : int > 0
-        Number of nodes
-    m : int > 0
-        Hyperedge size
-    rho : float between 0 and 1
-        The fraction of nodes in community 1
-    k : float > 0
-        Mean degree
-    epsilon : float > 0
-        Imbalance parameter
-    seed : integer or None (default)
-        The seed for the random number generator
-
-    Returns
-    -------
-    Hypergraph
-        The constructed m-HPPM hypergraph.
-
-    Raises
-    ------
-    EasyGraphError
-        - If rho is not between 0 and 1
-        - If the mean degree is negative.
-        - If epsilon is not between 0 and 1
-
-    See Also
-    --------
-    uniform_HSBM
-
-    References
-    ----------
-    Nicholas W. Landry and Juan G. Restrepo.
-    "Polarization in hypergraphs with community structure."
-    Preprint, 2023. https://doi.org/10.48550/arXiv.2302.13967
-    """
-
-    if rho < 0 or rho > 1:
-        raise EasyGraphError("The value of rho must be between 0 and 1")
-    if k < 0:
-        raise EasyGraphError("The mean degree must be non-negative")
-    if epsilon < 0 or epsilon > 1:
-        raise EasyGraphError("epsilon must be between 0 and 1")
-
-    sizes = [int(rho * n), n - int(rho * n)]
-
-    p = k / (m * n ** (m - 1))
-    # ratio of inter- to intra-community edges
-    q = rho**m + (1 - rho) ** m
-    r = 1 / q - 1
-    p_in = (1 + r * epsilon) * p
-    p_out = (1 - epsilon) * p
-
-    p = p_out * np.ones([2] * m)
-    p[tuple([0] * m)] = p_in
-    p[tuple([1] * m)] = p_in
-
-    return uniform_HSBM(n, m, p, sizes, seed=seed)
-
-
-def uniform_erdos_renyi_hypergraph(n, m, p, p_type="degree", seed=None):
-    """Generate an m-uniform Erdős–Rényi hypergraph
-
-    This creates a hypergraph with `n` nodes where
-    hyperedges of size `m` are created at random to
-    obtain a mean degree of `k`.
-
-    Parameters
-    ----------
-    n : int > 0
-        Number of nodes
-    m : int > 0
-        Hyperedge size
-    p : float or int > 0
-        Mean expected degree if p_type="degree" and
-        probability of an m-hyperedge if p_type="prob"
-    p_type : str
-        "degree" or "prob", by default "degree"
-    seed : integer or None (default)
-        The seed for the random number generator
-
-    Returns
-    -------
-    Hypergraph
-        The Erdos Renyi hypergraph
-
-
-    See Also
-    --------
-    random_hypergraph
-    """
-    if seed is not None:
-        np.random.seed(seed)
-
-    H = eg.Hypergraph(num_v=n)
-
-    if p_type == "degree":
-        q = p / (m * n ** (m - 1))  # wiring probability
-    elif p_type == "prob":
-        q = p
-    else:
-        raise EasyGraphError("Invalid p_type!")
-
-    if q > 1 or q < 0:
-        raise EasyGraphError("Probability not in [0,1].")
-
-    index = np.random.geometric(q) - 1  # -1 b/c zero indexing
-    max_index = n**m
-    while index < max_index:
-        e = set(_index_to_edge(index, n, m))
-        if len(e) == m:
-            H.add_hyperedges(list(e))
-        index += np.random.geometric(q)
-    return H
-
-
-def _index_to_edge(index, n, m):
-    """Generate a hyperedge given an index in the list of possible edges.
-
-    Parameters
-    ----------
-    index : int > 0
-        The index of the hyperedge in the list of all possible hyperedges.
-    n : int > 0
-        The number of nodes
-    m : int > 0
-        The hyperedge size.
-
-    Returns
-    -------
-    list
-        The reconstructed hyperedge
-
-    See Also
-    --------
-    _index_to_edge_partition
-
-    References
-    ----------
-    https://stackoverflow.com/questions/53834707/element-at-index-in-itertools-product
-    """
-    return [(index // (n**r) % n) for r in range(m - 1, -1, -1)]
-
-
-def _index_to_edge_partition(index, partition_sizes, m):
-    """Generate a hyperedge given an index in the list of possible edges
-    and a partition of community labels.
-
-    Parameters
-    ----------
-    index : int > 0
-        The index of the hyperedge in the list of all possible hyperedges.
-    n : int > 0
-        The number of nodes
-    m : int > 0
-        The hyperedge size.
-
-    Returns
-    -------
-    list
-        The reconstructed hyperedge
-
-    See Also
-    --------
-    _index_to_edge
-
-    """
-    try:
-        return [
-            int(index // np.prod(partition_sizes[r + 1 :]) % partition_sizes[r])
-            for r in range(m)
-        ]
-    except KeyError:
-        raise Exception("Invalid parameters")
+"""Generate random uniform hypergraphs."""
+import itertools
+import operator
+import random
+import warnings
+
+from functools import reduce
+
+import easygraph as eg
+import numpy as np
+
+from easygraph.utils.exception import EasyGraphError
+
+
+__all__ = [
+    "uniform_hypergraph_configuration_model",
+    "uniform_HSBM",
+    "uniform_HPPM",
+    "uniform_erdos_renyi_hypergraph",
+    "uniform_hypergraph_Gnm",
+]
+
+
+def split_num_e(num_e, worker):
+    import math
+
+    res = []
+    group_size = num_e // worker
+    for i in range(worker):
+        res.append(group_size)
+    return res
+
+
+def uniform_hypergraph_Gnm_parallel(num_e, num_v, k):
+    random.seed()
+    edges = set()
+    while len(edges) < num_e:
+        e = random.sample(range(num_v), k)
+        e = tuple(sorted(e))
+        if e not in edges:
+            edges.add(e)
+    return list(edges)
+
+
+def uniform_hypergraph_Gnm(k: int, num_v: int, num_e: int, n_workers=None):
+    r"""Return a random ``k``-uniform hypergraph with ``num_v`` vertices and ``num_e`` hyperedges.
+
+    Args:
+        ``k`` (``int``): The Number of vertices in each hyperedge.
+        ``num_v`` (``int``): The Number of vertices.
+        ``num_e`` (``int``): The Number of hyperedges.
+
+    Examples:
+        >>> import easygraph as eg
+        >>> hg = eg.uniform_hypergraph_Gnm(3, 5, 4)
+        >>> hg.e
+        ([(0, 1, 2), (0, 1, 3), (0, 3, 4), (2, 3, 4)], [1.0, 1.0, 1.0, 1.0])
+    """
+    # similar to UniformRandomUniform in sagemath, https://doc.sagemath.org/html/en/reference/graphs/sage/graphs/hypergraph_generators.html
+
+    assert k > 1, "k must be greater than 1"  # TODO ?
+    assert num_v > 1, "num_v must be greater than 1"
+    assert num_e > 0, "num_e must be greater than 0"
+
+    if n_workers is not None:
+        #  use the parallel version for large graph
+        edges = set()
+        from functools import partial
+        from multiprocessing import Pool
+
+        # res_edges = set()
+        edges_parallel = split_num_e(num_e=num_e, worker=n_workers)
+        local_function = partial(uniform_hypergraph_Gnm_parallel, num_v=num_v, k=k)
+
+        res_edges = set()
+        import time
+
+        with Pool(n_workers) as p:
+            ret = p.imap(local_function, edges_parallel)
+            for res in ret:
+                for r in res:
+                    res_edges.add(r)
+
+            while len(res_edges) < num_e:
+                e = random.sample(range(num_v), k)
+                e = tuple(sorted(e))
+                if e not in res_edges:
+                    res_edges.add(e)
+
+        res_hypergraph = eg.Hypergraph(num_v=num_v, e_list=list(res_edges))
+        return res_hypergraph
+
+    else:
+        edges = set()
+        while len(edges) < num_e:
+            e = random.sample(range(num_v), k)
+            e = tuple(sorted(e))
+            if e not in edges:
+                edges.add(e)
+
+    return eg.Hypergraph(num_v, list(edges))
+
+
+def uniform_hypergraph_configuration_model(k, m, seed=None):
+    """
+    A function to generate an m-uniform configuration model
+
+    Parameters
+    ----------
+    k : dictionary
+        This is a dictionary where the keys are node ids
+        and the values are node degrees.
+    m : int
+        specifies the hyperedge size
+    seed : integer or None (default)
+        The seed for the random number generator
+
+    Returns
+    -------
+    Hypergraph object
+        The generated hypergraph
+
+    Warns
+    -----
+    warnings.warn
+        If the sums of the degrees are not divisible by m, the
+        algorithm still runs, but raises a warning and adds an
+        additional connection to random nodes to satisfy this
+        condition.
+
+    Notes
+    -----
+    This algorithm normally creates multi-edges and loopy hyperedges.
+    We remove the loopy hyperedges.
+
+    References
+    ----------
+    "The effect of heterogeneity on hypergraph contagion models"
+    by Nicholas W. Landry and Juan G. Restrepo
+    https://doi.org/10.1063/5.0020034
+
+
+    Example
+    -------
+    >>> import easygraph as eg
+    >>> import random
+    >>> n = 1000
+    >>> m = 3
+    >>> k = {1: 1, 2: 2, 3: 3, 4: 3}
+    >>> H = eg.uniform_hypergraph_configuration_model(k, m)
+
+    """
+    if seed is not None:
+        random.seed(seed)
+
+    # Making sure we have the right number of stubs
+    remainder = sum(k.values()) % m
+    if remainder != 0:
+        warnings.warn(
+            "This degree sequence is not realizable. "
+            "Increasing the degree of random nodes so that it is."
+        )
+        random_ids = random.sample(list(k.keys()), int(round(m - remainder)))
+        for id in random_ids:
+            k[id] = k[id] + 1
+
+    stubs = []
+    # Creating the list to index through
+    for id in k:
+        stubs.extend([id] * int(k[id]))
+
+    H = eg.Hypergraph(num_v=len(k))
+
+    while len(stubs) != 0:
+        u = random.sample(range(len(stubs)), m)
+        edge = set()
+        for index in u:
+            edge.add(stubs[index])
+        if len(edge) == m:
+            H.add_hyperedges(list(edge))
+
+        for index in sorted(u, reverse=True):
+            del stubs[index]
+
+    return H
+
+
+def uniform_HSBM(n, m, p, sizes, seed=None):
+    """Create a uniform hypergraph stochastic block model (HSBM).
+
+    Parameters
+    ----------
+    n : int
+        The number of nodes
+    m : int
+        The hyperedge size
+    p : m-dimensional numpy array
+        tensor of probabilities between communities
+    sizes : list or 1D numpy array
+        The sizes of the community blocks in order
+    seed : integer or None (default)
+        The seed for the random number generator
+
+    Returns
+    -------
+    Hypergraph
+        The constructed SBM hypergraph
+
+    Raises
+    ------
+    EasyGraphError
+        - If the length of sizes and p do not match.
+        - If p is not a tensor with every dimension equal
+        - If p is not m-dimensional
+        - If the entries of p are not in the range [0, 1]
+        - If the sum of the vector of sizes does not equal the number of nodes.
+    Exception
+        If there is an integer overflow error
+
+    See Also
+    --------
+    uniform_HPPM
+
+    References
+    ----------
+    Nicholas W. Landry and Juan G. Restrepo.
+    "Polarization in hypergraphs with community structure."
+    Preprint, 2023. https://doi.org/10.48550/arXiv.2302.13967
+    """
+    # Check if dimensions match
+    if len(sizes) != np.size(p, axis=0):
+        raise EasyGraphError("'sizes' and 'p' do not match.")
+    if len(np.shape(p)) != m:
+        raise EasyGraphError("The dimension of p does not match m")
+    # Check that p has the same length over every dimension.
+    if len(set(np.shape(p))) != 1:
+        raise EasyGraphError("'p' must be a square tensor.")
+    if np.max(p) > 1 or np.min(p) < 0:
+        raise EasyGraphError("Entries of 'p' not in [0,1].")
+    if np.sum(sizes) != n:
+        raise EasyGraphError("Sum of sizes does not match n")
+
+    if seed is not None:
+        np.random.seed(seed)
+
+    node_labels = range(n)
+    H = eg.Hypergraph(num_v=n)
+
+    block_range = range(len(sizes))
+    # Split node labels in a partition (list of sets).
+    size_cumsum = [sum(sizes[0:x]) for x in range(0, len(sizes) + 1)]
+    partition = [
+        list(node_labels[size_cumsum[x] : size_cumsum[x + 1]])
+        for x in range(0, len(size_cumsum) - 1)
+    ]
+
+    for block in itertools.product(block_range, repeat=m):
+        if p[block] == 1:  # Test edges cases p_ij = 0 or 1
+            edges = itertools.product((partition[i] for i in block_range))
+            for e in edges:
+                H.add_hyperedges(list(e))
+        elif p[block] > 0:
+            partition_sizes = [len(partition[i]) for i in block]
+            max_index = reduce(operator.mul, partition_sizes, 1)
+            if max_index < 0:
+                raise Exception("Index overflow error!")
+            index = np.random.geometric(p[block]) - 1
+
+            while index < max_index:
+                indices = _index_to_edge_partition(index, partition_sizes, m)
+                e = {partition[block[i]][indices[i]] for i in range(m)}
+                if len(e) == m:
+                    H.add_hyperedges(list(e))
+                index += np.random.geometric(p[block])
+    return H
+
+
+def uniform_HPPM(n, m, rho, k, epsilon, seed=None):
+    """Construct the m-uniform hypergraph planted partition model (m-HPPM)
+
+    Parameters
+    ----------
+    n : int > 0
+        Number of nodes
+    m : int > 0
+        Hyperedge size
+    rho : float between 0 and 1
+        The fraction of nodes in community 1
+    k : float > 0
+        Mean degree
+    epsilon : float > 0
+        Imbalance parameter
+    seed : integer or None (default)
+        The seed for the random number generator
+
+    Returns
+    -------
+    Hypergraph
+        The constructed m-HPPM hypergraph.
+
+    Raises
+    ------
+    EasyGraphError
+        - If rho is not between 0 and 1
+        - If the mean degree is negative.
+        - If epsilon is not between 0 and 1
+
+    See Also
+    --------
+    uniform_HSBM
+
+    References
+    ----------
+    Nicholas W. Landry and Juan G. Restrepo.
+    "Polarization in hypergraphs with community structure."
+    Preprint, 2023. https://doi.org/10.48550/arXiv.2302.13967
+    """
+
+    if rho < 0 or rho > 1:
+        raise EasyGraphError("The value of rho must be between 0 and 1")
+    if k < 0:
+        raise EasyGraphError("The mean degree must be non-negative")
+    if epsilon < 0 or epsilon > 1:
+        raise EasyGraphError("epsilon must be between 0 and 1")
+
+    sizes = [int(rho * n), n - int(rho * n)]
+
+    p = k / (m * n ** (m - 1))
+    # ratio of inter- to intra-community edges
+    q = rho**m + (1 - rho) ** m
+    r = 1 / q - 1
+    p_in = (1 + r * epsilon) * p
+    p_out = (1 - epsilon) * p
+
+    p = p_out * np.ones([2] * m)
+    p[tuple([0] * m)] = p_in
+    p[tuple([1] * m)] = p_in
+
+    return uniform_HSBM(n, m, p, sizes, seed=seed)
+
+
+def uniform_erdos_renyi_hypergraph(n, m, p, p_type="degree", seed=None):
+    """Generate an m-uniform Erdős–Rényi hypergraph
+
+    This creates a hypergraph with `n` nodes where
+    hyperedges of size `m` are created at random to
+    obtain a mean degree of `k`.
+
+    Parameters
+    ----------
+    n : int > 0
+        Number of nodes
+    m : int > 0
+        Hyperedge size
+    p : float or int > 0
+        Mean expected degree if p_type="degree" and
+        probability of an m-hyperedge if p_type="prob"
+    p_type : str
+        "degree" or "prob", by default "degree"
+    seed : integer or None (default)
+        The seed for the random number generator
+
+    Returns
+    -------
+    Hypergraph
+        The Erdos Renyi hypergraph
+
+
+    See Also
+    --------
+    random_hypergraph
+    """
+    if seed is not None:
+        np.random.seed(seed)
+
+    H = eg.Hypergraph(num_v=n)
+
+    if p_type == "degree":
+        q = p / (m * n ** (m - 1))  # wiring probability
+    elif p_type == "prob":
+        q = p
+    else:
+        raise EasyGraphError("Invalid p_type!")
+
+    if q > 1 or q < 0:
+        raise EasyGraphError("Probability not in [0,1].")
+
+    index = np.random.geometric(q) - 1  # -1 b/c zero indexing
+    max_index = n**m
+    while index < max_index:
+        e = set(_index_to_edge(index, n, m))
+        if len(e) == m:
+            H.add_hyperedges(list(e))
+        index += np.random.geometric(q)
+    return H
+
+
+def _index_to_edge(index, n, m):
+    """Generate a hyperedge given an index in the list of possible edges.
+
+    Parameters
+    ----------
+    index : int > 0
+        The index of the hyperedge in the list of all possible hyperedges.
+    n : int > 0
+        The number of nodes
+    m : int > 0
+        The hyperedge size.
+
+    Returns
+    -------
+    list
+        The reconstructed hyperedge
+
+    See Also
+    --------
+    _index_to_edge_partition
+
+    References
+    ----------
+    https://stackoverflow.com/questions/53834707/element-at-index-in-itertools-product
+    """
+    return [(index // (n**r) % n) for r in range(m - 1, -1, -1)]
+
+
+def _index_to_edge_partition(index, partition_sizes, m):
+    """Generate a hyperedge given an index in the list of possible edges
+    and a partition of community labels.
+
+    Parameters
+    ----------
+    index : int > 0
+        The index of the hyperedge in the list of all possible hyperedges.
+    n : int > 0
+        The number of nodes
+    m : int > 0
+        The hyperedge size.
+
+    Returns
+    -------
+    list
+        The reconstructed hyperedge
+
+    See Also
+    --------
+    _index_to_edge
+
+    """
+    try:
+        return [
+            int(index // np.prod(partition_sizes[r + 1 :]) % partition_sizes[r])
+            for r in range(m)
+        ]
+    except KeyError:
+        raise Exception("Invalid parameters")
```

## Comparing `easygraph/functions/hypergraph/hypergraph_generator/random.py` & `easygraph/functions/hypergraph/null_model/random.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,434 +1,434 @@
-import math
-import random
-import warnings
-
-from collections import defaultdict
-from itertools import combinations
-
-import easygraph as eg
-import numpy as np
-
-# from easygraph.classes.hypergraph import Hypergraph
-from easygraph.utils.exception import EasyGraphError
-from scipy.special import comb
-
-from .lattice import *
-
-
-__all__ = [
-    "random_hypergraph",
-    "chung_lu_hypergraph",
-    "dcsbm_hypergraph",
-    "watts_strogatz_hypergraph",
-    "uniform_hypergraph_Gnp",
-]
-
-
-def uniform_hypergraph_Gnp_parallel(edges, prob):
-    remain_edges = [e for e in edges if random.random() < prob]
-    return remain_edges
-
-
-def split_edges(edges, worker):
-    import math
-
-    edges_size = len(edges)
-    group_size = math.ceil(edges_size / worker)
-    group_lst = []
-    for i in range(0, edges_size, group_size):
-        group_lst.append(edges[i : i + group_size])
-
-    return group_lst
-
-
-def uniform_hypergraph_Gnp(k: int, num_v: int, prob: float, n_workers=None):
-    r"""Return a random ``k``-uniform hypergraph with ``num_v`` vertices and probability ``prob`` of choosing a hyperedge.
-
-    Args:
-        ``num_v`` (``int``): The Number of vertices.
-        ``k`` (``int``): The Number of vertices in each hyperedge.
-        ``prob`` (``float``): Probability of choosing a hyperedge.
-
-    Examples:
-        >>> import easygraph as eg
-        >>> hg = eg.random.uniform_hypergraph_Gnp(3, 5, 0.5)
-        >>> hg.e
-        ([(0, 1, 3), (0, 1, 4), (0, 2, 4), (1, 3, 4), (2, 3, 4)], [1.0, 1.0, 1.0, 1.0, 1.0])
-    """
-    # similar to BinomialRandomUniform in sagemath, https://doc.sagemath.org/html/en/reference/graphs/sage/graphs/hypergraph_generators.html
-
-    assert num_v > 1, "num_v must be greater than 1"
-    assert k > 1, "k must be greater than 1"
-    assert 0 <= prob <= 1, "prob must be between 0 and 1"
-    import random
-
-    if n_workers is not None:
-        #  use the parallel version for large graph
-
-        from functools import partial
-        from multiprocessing import Pool
-
-        edges = combinations(range(num_v), k)
-        edges_parallel = split_edges(edges=list(edges), worker=n_workers)
-        local_function = partial(uniform_hypergraph_Gnp_parallel, prob=prob)
-
-        res_edges = []
-
-        with Pool(n_workers) as p:
-            ret = p.imap(local_function, edges_parallel)
-            for res in ret:
-                res_edges.extend(res)
-        res_hypergraph = eg.Hypergraph(num_v=num_v, e_list=res_edges)
-        return res_hypergraph
-
-    else:
-        edges = combinations(range(num_v), k)
-        edges = [e for e in edges if random.random() < prob]
-        return eg.Hypergraph(num_v=num_v, e_list=edges)
-
-
-def dcsbm_hypergraph(k1, k2, g1, g2, omega, seed=None):
-    """A function to generate a Degree-Corrected Stochastic Block Model
-    (DCSBM) hypergraph.
-
-    Parameters
-    ----------
-    k1 : dict
-        This is a dictionary where the keys are node ids
-        and the values are node degrees.
-    k2 : dict
-        This is a dictionary where the keys are edge ids
-        and the values are edge sizes.
-    g1 : dict
-        This a dictionary where the keys are node ids
-        and the values are the group ids to which the node belongs.
-        The keys must match the keys of k1.
-    g2 : dict
-        This a dictionary where the keys are edge ids
-        and the values are the group ids to which the edge belongs.
-        The keys must match the keys of k2.
-    omega : 2D numpy array
-        This is a matrix with entries which specify the number of edges
-        between a given node community and edge community.
-        The number of rows must match the number of node communities
-        and the number of columns must match the number of edge
-        communities.
-    seed : int or None (default)
-        Seed for the random number generator.
-
-    Returns
-    -------
-    Hypergraph
-
-    Warns
-    -----
-    warnings.warn
-        If the sums of the edge sizes and node degrees are not equal, the
-        algorithm still runs, but raises a warning.
-        Also if the sum of the omega matrix does not match the sum of degrees,
-        a warning is raised.
-
-    Notes
-    -----
-    The sums of k1 and k2 should be the same. If they are not the same, this function
-    returns a warning but still runs. The sum of k1 (and k2) and omega should be the
-    same. If they are not the same, this function returns a warning but still runs and
-    the number of entries in the incidence matrix is determined by the omega matrix.
-
-    References
-    ----------
-    Implemented by Mirah Shi in HyperNetX and described for bipartite networks by
-    Larremore et al. in https://doi.org/10.1103/PhysRevE.90.012805
-
-    Examples
-    --------
-    >>> import easygraph as eg; import random; import numpy as np
-    >>> n = 50
-    >>> k1 = {i : random.randint(1, n) for i in range(n)}
-    >>> k2 = {i : sorted(k1.values())[i] for i in range(n)}
-    >>> g1 = {i : random.choice([0, 1]) for i in range(n)}
-    >>> g2 = {i : random.choice([0, 1]) for i in range(n)}
-    >>> omega = np.array([[n//2, 10], [10, n//2]])
-    >>> H = eg.dcsbm_hypergraph(k1, k2, g1, g2, omega)
-
-    """
-    if seed is not None:
-        random.seed(seed)
-
-    # sort dictionary by degree in decreasing order
-    node_labels = [n for n, _ in sorted(k1.items(), key=lambda d: d[1], reverse=True)]
-    edge_labels = [m for m, _ in sorted(k2.items(), key=lambda d: d[1], reverse=True)]
-
-    # Verify that the sum of node and edge degrees and the sum of node degrees and the
-    # sum of community connection matrix differ by less than a single edge.
-    if abs(sum(k1.values()) - sum(k2.values())) > 1:
-        warnings.warn(
-            "The sum of the degree sequence does not match the sum of the size sequence"
-        )
-
-    if abs(sum(k1.values()) - np.sum(omega)) > 1:
-        warnings.warn(
-            "The sum of the degree sequence does not "
-            "match the entries in the omega matrix"
-        )
-
-    # get indices for each community
-    community1_nodes = defaultdict(list)
-    for label in node_labels:
-        group = g1[label]
-        community1_nodes[group].append(label)
-
-    community2_nodes = defaultdict(list)
-    for label in edge_labels:
-        group = g2[label]
-        community2_nodes[group].append(label)
-
-    H = eg.Hypergraph(num_v=len(node_labels))
-
-    kappa1 = defaultdict(lambda: 0)
-    kappa2 = defaultdict(lambda: 0)
-    for id, g in g1.items():
-        kappa1[g] += k1[id]
-    for id, g in g2.items():
-        kappa2[g] += k2[id]
-
-    tmp_hyperedges = []
-    for group1 in community1_nodes.keys():
-        for group2 in community2_nodes.keys():
-            # for each constant probability patch
-            try:
-                group_constant = omega[group1, group2] / (
-                    kappa1[group1] * kappa2[group2]
-                )
-            except ZeroDivisionError:
-                group_constant = 0
-
-            for u in community1_nodes[group1]:
-                j = 0
-                v = community2_nodes[group2][j]  # start from beginning every time
-                # max probability
-                p = min(k1[u] * k2[v] * group_constant, 1)
-                while j < len(community2_nodes[group2]):
-                    if p != 1:
-                        r = random.random()
-                        try:
-                            j = j + math.floor(math.log(r) / math.log(1 - p))
-                        except ZeroDivisionError:
-                            j = np.inf
-                    if j < len(community2_nodes[group2]):
-                        v = community2_nodes[group2][j]
-                        q = min((k1[u] * k2[v]) * group_constant, 1)
-                        r = random.random()
-                        if r < q / p:
-                            # no duplicates
-                            if v < len(tmp_hyperedges):
-                                tmp_hyperedges[v].append(u)
-                            else:
-                                tmp_hyperedges.append([u])
-
-                        p = q
-                        j = j + 1
-
-    H.add_hyperedges(tmp_hyperedges)
-    return H
-
-
-def watts_strogatz_hypergraph(n, d, k, l, p, seed=None):
-    """
-
-    Parameters
-    ----------
-    n : int
-    The number of nodes
-    d : int
-        Edge size
-    k: int
-        Number of edges of which a node is a part. Should be a multiple of 2.
-    l: int
-        Overlap between edges
-    p  : float
-        The probability of rewiring each edge
-    seed
-
-    Returns
-    -------
-
-    """
-    if seed is not None:
-        np.random.seed(seed)
-    H = ring_lattice(n, d, k, l)
-    to_remove = []
-    to_add = []
-    H_edges = H.e[0]
-    for e in H_edges:
-        if np.random.random() < p:
-            to_remove.append(e)
-            node = min(e)
-            neighbors = np.random.choice(H.v, size=d - 1)
-            to_add.append(np.append(neighbors, node))
-    H.remove_hyperedges(to_remove)
-    H.add_hyperedges(to_add)
-    return H
-
-
-def chung_lu_hypergraph(k1, k2, seed=None):
-    """A function to generate a Chung-Lu hypergraph
-
-    Parameters
-    ----------
-    k1 : dict
-        Dict where the keys are node ids
-        and the values are node degrees.
-    k2 : dict
-        dict where the keys are edge ids
-        and the values are edge sizes.
-    seed : integer or None (default)
-            The seed for the random number generator.
-
-    Returns
-    -------
-    Hypergraph object
-        The generated hypergraph
-
-    Warns
-    -----
-    warnings.warn
-        If the sums of the edge sizes and node degrees are not equal, the
-        algorithm still runs, but raises a warning.
-
-    Notes
-    -----
-    The sums of k1 and k2 should be the same. If they are not the same,
-    this function returns a warning but still runs.
-
-    References
-    ----------
-    Implemented by Mirah Shi in HyperNetX and described for
-    bipartite networks by Aksoy et al. in https://doi.org/10.1093/comnet/cnx001
-
-    Example
-    -------
-    >>> import easygraph as eg
-    >>> import random
-    >>> n = 100
-    >>> k1 = {i : random.randint(1, 100) for i in range(n)}
-    >>> k2 = {i : sorted(k1.values())[i] for i in range(n)}
-    >>> H = eg.chung_lu_hypergraph(k1, k2)
-
-    """
-    if seed is not None:
-        random.seed(seed)
-
-    # sort dictionary by degree in decreasing order
-    node_labels = [n for n, _ in sorted(k1.items(), key=lambda d: d[1], reverse=True)]
-    edge_labels = [m for m, _ in sorted(k2.items(), key=lambda d: d[1], reverse=True)]
-
-    m = len(k2)
-
-    if sum(k1.values()) != sum(k2.values()):
-        warnings.warn(
-            "The sum of the degree sequence does not match the sum of the size sequence"
-        )
-
-    S = sum(k1.values())
-
-    H = eg.Hypergraph(len(node_labels))
-
-    tmp_hyperedges = []
-    for u in node_labels:
-        j = 0
-        v = edge_labels[j]  # start from beginning every time
-        p = min((k1[u] * k2[v]) / S, 1)
-
-        while j < m:
-            if p != 1:
-                r = random.random()
-                try:
-                    j = j + math.floor(math.log(r) / math.log(1 - p))
-                except ZeroDivisionError:
-                    j = np.inf
-
-            if j < m:
-                v = edge_labels[j]
-                q = min((k1[u] * k2[v]) / S, 1)
-                r = random.random()
-                if r < q / p:
-                    # no duplicates
-                    if v < len(tmp_hyperedges):
-                        tmp_hyperedges[v].append(u)
-                    else:
-                        tmp_hyperedges.append([u])
-                p = q
-                j = j + 1
-
-    H.add_hyperedges(tmp_hyperedges)
-    return H
-
-
-def random_hypergraph(N, ps, order=None, seed=None):
-    """Generates a random hypergraph
-
-    Generate N nodes, and connect any d+1 nodes
-    by a hyperedge with probability ps[d-1].
-
-    Parameters
-    ----------
-    N : int
-        Number of nodes
-    ps : list of float
-        List of probabilities (between 0 and 1) to create a
-        hyperedge at each order d between any d+1 nodes. For example,
-        ps[0] is the wiring probability of any edge (2 nodes), ps[1]
-        of any triangles (3 nodes).
-    order: int of None (default)
-        If None, ignore. If int, generates a uniform hypergraph with edges
-        of order `order` (ps must have only one element).
-    seed : integer or None (default)
-            Seed for the random number generator.
-
-    Returns
-    -------
-    Hypergraph object
-        The generated hypergraph
-
-    References
-    ----------
-    Described as 'random hypergraph' by M. Dewar et al. in https://arxiv.org/abs/1703.07686
-
-    Example
-    -------
-    >>> import easygraph as eg
-    >>> H = eg.random_hypergraph(50, [0.1, 0.01])
-
-    """
-    if seed is not None:
-        np.random.seed(seed)
-
-    if order is not None:
-        if len(ps) != 1:
-            raise EasyGraphError("ps must contain a single element if order is an int")
-
-    if (np.any(np.array(ps) < 0)) or (np.any(np.array(ps) > 1)):
-        raise EasyGraphError("All elements of ps must be between 0 and 1 included.")
-
-    nodes = range(N)
-    hyperedges = []
-
-    for i, p in enumerate(ps):
-        if order is not None:
-            d = order
-        else:
-            d = i + 1  # order, ps[0] is prob of edges (d=1)
-
-        potential_edges = combinations(nodes, d + 1)
-        n_comb = comb(N, d + 1, exact=True)
-        mask = np.random.random(size=n_comb) <= p  # True if edge to keep
-
-        edges_to_add = [e for e, val in zip(potential_edges, mask) if val]
-
-        hyperedges += edges_to_add
-
-    H = eg.Hypergraph(num_v=N)
-    H.add_hyperedges(hyperedges)
-
-    return H
+import math
+import random
+import warnings
+
+from collections import defaultdict
+from itertools import combinations
+
+import easygraph as eg
+import numpy as np
+
+# from easygraph.classes.hypergraph import Hypergraph
+from easygraph.utils.exception import EasyGraphError
+from scipy.special import comb
+
+from .lattice import *
+
+
+__all__ = [
+    "random_hypergraph",
+    "chung_lu_hypergraph",
+    "dcsbm_hypergraph",
+    "watts_strogatz_hypergraph",
+    "uniform_hypergraph_Gnp",
+]
+
+
+def uniform_hypergraph_Gnp_parallel(edges, prob):
+    remain_edges = [e for e in edges if random.random() < prob]
+    return remain_edges
+
+
+def split_edges(edges, worker):
+    import math
+
+    edges_size = len(edges)
+    group_size = math.ceil(edges_size / worker)
+    group_lst = []
+    for i in range(0, edges_size, group_size):
+        group_lst.append(edges[i : i + group_size])
+
+    return group_lst
+
+
+def uniform_hypergraph_Gnp(k: int, num_v: int, prob: float, n_workers=None):
+    r"""Return a random ``k``-uniform hypergraph with ``num_v`` vertices and probability ``prob`` of choosing a hyperedge.
+
+    Args:
+        ``num_v`` (``int``): The Number of vertices.
+        ``k`` (``int``): The Number of vertices in each hyperedge.
+        ``prob`` (``float``): Probability of choosing a hyperedge.
+
+    Examples:
+        >>> import easygraph as eg
+        >>> hg = eg.random.uniform_hypergraph_Gnp(3, 5, 0.5)
+        >>> hg.e
+        ([(0, 1, 3), (0, 1, 4), (0, 2, 4), (1, 3, 4), (2, 3, 4)], [1.0, 1.0, 1.0, 1.0, 1.0])
+    """
+    # similar to BinomialRandomUniform in sagemath, https://doc.sagemath.org/html/en/reference/graphs/sage/graphs/hypergraph_generators.html
+
+    assert num_v > 1, "num_v must be greater than 1"
+    assert k > 1, "k must be greater than 1"
+    assert 0 <= prob <= 1, "prob must be between 0 and 1"
+    import random
+
+    if n_workers is not None:
+        #  use the parallel version for large graph
+
+        from functools import partial
+        from multiprocessing import Pool
+
+        edges = combinations(range(num_v), k)
+        edges_parallel = split_edges(edges=list(edges), worker=n_workers)
+        local_function = partial(uniform_hypergraph_Gnp_parallel, prob=prob)
+
+        res_edges = []
+
+        with Pool(n_workers) as p:
+            ret = p.imap(local_function, edges_parallel)
+            for res in ret:
+                res_edges.extend(res)
+        res_hypergraph = eg.Hypergraph(num_v=num_v, e_list=res_edges)
+        return res_hypergraph
+
+    else:
+        edges = combinations(range(num_v), k)
+        edges = [e for e in edges if random.random() < prob]
+        return eg.Hypergraph(num_v=num_v, e_list=edges)
+
+
+def dcsbm_hypergraph(k1, k2, g1, g2, omega, seed=None):
+    """A function to generate a Degree-Corrected Stochastic Block Model
+    (DCSBM) hypergraph.
+
+    Parameters
+    ----------
+    k1 : dict
+        This is a dictionary where the keys are node ids
+        and the values are node degrees.
+    k2 : dict
+        This is a dictionary where the keys are edge ids
+        and the values are edge sizes.
+    g1 : dict
+        This a dictionary where the keys are node ids
+        and the values are the group ids to which the node belongs.
+        The keys must match the keys of k1.
+    g2 : dict
+        This a dictionary where the keys are edge ids
+        and the values are the group ids to which the edge belongs.
+        The keys must match the keys of k2.
+    omega : 2D numpy array
+        This is a matrix with entries which specify the number of edges
+        between a given node community and edge community.
+        The number of rows must match the number of node communities
+        and the number of columns must match the number of edge
+        communities.
+    seed : int or None (default)
+        Seed for the random number generator.
+
+    Returns
+    -------
+    Hypergraph
+
+    Warns
+    -----
+    warnings.warn
+        If the sums of the edge sizes and node degrees are not equal, the
+        algorithm still runs, but raises a warning.
+        Also if the sum of the omega matrix does not match the sum of degrees,
+        a warning is raised.
+
+    Notes
+    -----
+    The sums of k1 and k2 should be the same. If they are not the same, this function
+    returns a warning but still runs. The sum of k1 (and k2) and omega should be the
+    same. If they are not the same, this function returns a warning but still runs and
+    the number of entries in the incidence matrix is determined by the omega matrix.
+
+    References
+    ----------
+    Implemented by Mirah Shi in HyperNetX and described for bipartite networks by
+    Larremore et al. in https://doi.org/10.1103/PhysRevE.90.012805
+
+    Examples
+    --------
+    >>> import easygraph as eg; import random; import numpy as np
+    >>> n = 50
+    >>> k1 = {i : random.randint(1, n) for i in range(n)}
+    >>> k2 = {i : sorted(k1.values())[i] for i in range(n)}
+    >>> g1 = {i : random.choice([0, 1]) for i in range(n)}
+    >>> g2 = {i : random.choice([0, 1]) for i in range(n)}
+    >>> omega = np.array([[n//2, 10], [10, n//2]])
+    >>> H = eg.dcsbm_hypergraph(k1, k2, g1, g2, omega)
+
+    """
+    if seed is not None:
+        random.seed(seed)
+
+    # sort dictionary by degree in decreasing order
+    node_labels = [n for n, _ in sorted(k1.items(), key=lambda d: d[1], reverse=True)]
+    edge_labels = [m for m, _ in sorted(k2.items(), key=lambda d: d[1], reverse=True)]
+
+    # Verify that the sum of node and edge degrees and the sum of node degrees and the
+    # sum of community connection matrix differ by less than a single edge.
+    if abs(sum(k1.values()) - sum(k2.values())) > 1:
+        warnings.warn(
+            "The sum of the degree sequence does not match the sum of the size sequence"
+        )
+
+    if abs(sum(k1.values()) - np.sum(omega)) > 1:
+        warnings.warn(
+            "The sum of the degree sequence does not "
+            "match the entries in the omega matrix"
+        )
+
+    # get indices for each community
+    community1_nodes = defaultdict(list)
+    for label in node_labels:
+        group = g1[label]
+        community1_nodes[group].append(label)
+
+    community2_nodes = defaultdict(list)
+    for label in edge_labels:
+        group = g2[label]
+        community2_nodes[group].append(label)
+
+    H = eg.Hypergraph(num_v=len(node_labels))
+
+    kappa1 = defaultdict(lambda: 0)
+    kappa2 = defaultdict(lambda: 0)
+    for id, g in g1.items():
+        kappa1[g] += k1[id]
+    for id, g in g2.items():
+        kappa2[g] += k2[id]
+
+    tmp_hyperedges = []
+    for group1 in community1_nodes.keys():
+        for group2 in community2_nodes.keys():
+            # for each constant probability patch
+            try:
+                group_constant = omega[group1, group2] / (
+                    kappa1[group1] * kappa2[group2]
+                )
+            except ZeroDivisionError:
+                group_constant = 0
+
+            for u in community1_nodes[group1]:
+                j = 0
+                v = community2_nodes[group2][j]  # start from beginning every time
+                # max probability
+                p = min(k1[u] * k2[v] * group_constant, 1)
+                while j < len(community2_nodes[group2]):
+                    if p != 1:
+                        r = random.random()
+                        try:
+                            j = j + math.floor(math.log(r) / math.log(1 - p))
+                        except ZeroDivisionError:
+                            j = np.inf
+                    if j < len(community2_nodes[group2]):
+                        v = community2_nodes[group2][j]
+                        q = min((k1[u] * k2[v]) * group_constant, 1)
+                        r = random.random()
+                        if r < q / p:
+                            # no duplicates
+                            if v < len(tmp_hyperedges):
+                                tmp_hyperedges[v].append(u)
+                            else:
+                                tmp_hyperedges.append([u])
+
+                        p = q
+                        j = j + 1
+
+    H.add_hyperedges(tmp_hyperedges)
+    return H
+
+
+def watts_strogatz_hypergraph(n, d, k, l, p, seed=None):
+    """
+
+    Parameters
+    ----------
+    n : int
+    The number of nodes
+    d : int
+        Edge size
+    k: int
+        Number of edges of which a node is a part. Should be a multiple of 2.
+    l: int
+        Overlap between edges
+    p  : float
+        The probability of rewiring each edge
+    seed
+
+    Returns
+    -------
+
+    """
+    if seed is not None:
+        np.random.seed(seed)
+    H = ring_lattice(n, d, k, l)
+    to_remove = []
+    to_add = []
+    H_edges = H.e[0]
+    for e in H_edges:
+        if np.random.random() < p:
+            to_remove.append(e)
+            node = min(e)
+            neighbors = np.random.choice(H.v, size=d - 1)
+            to_add.append(np.append(neighbors, node))
+    H.remove_hyperedges(to_remove)
+    H.add_hyperedges(to_add)
+    return H
+
+
+def chung_lu_hypergraph(k1, k2, seed=None):
+    """A function to generate a Chung-Lu hypergraph
+
+    Parameters
+    ----------
+    k1 : dict
+        Dict where the keys are node ids
+        and the values are node degrees.
+    k2 : dict
+        dict where the keys are edge ids
+        and the values are edge sizes.
+    seed : integer or None (default)
+            The seed for the random number generator.
+
+    Returns
+    -------
+    Hypergraph object
+        The generated hypergraph
+
+    Warns
+    -----
+    warnings.warn
+        If the sums of the edge sizes and node degrees are not equal, the
+        algorithm still runs, but raises a warning.
+
+    Notes
+    -----
+    The sums of k1 and k2 should be the same. If they are not the same,
+    this function returns a warning but still runs.
+
+    References
+    ----------
+    Implemented by Mirah Shi in HyperNetX and described for
+    bipartite networks by Aksoy et al. in https://doi.org/10.1093/comnet/cnx001
+
+    Example
+    -------
+    >>> import easygraph as eg
+    >>> import random
+    >>> n = 100
+    >>> k1 = {i : random.randint(1, 100) for i in range(n)}
+    >>> k2 = {i : sorted(k1.values())[i] for i in range(n)}
+    >>> H = eg.chung_lu_hypergraph(k1, k2)
+
+    """
+    if seed is not None:
+        random.seed(seed)
+
+    # sort dictionary by degree in decreasing order
+    node_labels = [n for n, _ in sorted(k1.items(), key=lambda d: d[1], reverse=True)]
+    edge_labels = [m for m, _ in sorted(k2.items(), key=lambda d: d[1], reverse=True)]
+
+    m = len(k2)
+
+    if sum(k1.values()) != sum(k2.values()):
+        warnings.warn(
+            "The sum of the degree sequence does not match the sum of the size sequence"
+        )
+
+    S = sum(k1.values())
+
+    H = eg.Hypergraph(len(node_labels))
+
+    tmp_hyperedges = []
+    for u in node_labels:
+        j = 0
+        v = edge_labels[j]  # start from beginning every time
+        p = min((k1[u] * k2[v]) / S, 1)
+
+        while j < m:
+            if p != 1:
+                r = random.random()
+                try:
+                    j = j + math.floor(math.log(r) / math.log(1 - p))
+                except ZeroDivisionError:
+                    j = np.inf
+
+            if j < m:
+                v = edge_labels[j]
+                q = min((k1[u] * k2[v]) / S, 1)
+                r = random.random()
+                if r < q / p:
+                    # no duplicates
+                    if v < len(tmp_hyperedges):
+                        tmp_hyperedges[v].append(u)
+                    else:
+                        tmp_hyperedges.append([u])
+                p = q
+                j = j + 1
+
+    H.add_hyperedges(tmp_hyperedges)
+    return H
+
+
+def random_hypergraph(N, ps, order=None, seed=None):
+    """Generates a random hypergraph
+
+    Generate N nodes, and connect any d+1 nodes
+    by a hyperedge with probability ps[d-1].
+
+    Parameters
+    ----------
+    N : int
+        Number of nodes
+    ps : list of float
+        List of probabilities (between 0 and 1) to create a
+        hyperedge at each order d between any d+1 nodes. For example,
+        ps[0] is the wiring probability of any edge (2 nodes), ps[1]
+        of any triangles (3 nodes).
+    order: int of None (default)
+        If None, ignore. If int, generates a uniform hypergraph with edges
+        of order `order` (ps must have only one element).
+    seed : integer or None (default)
+            Seed for the random number generator.
+
+    Returns
+    -------
+    Hypergraph object
+        The generated hypergraph
+
+    References
+    ----------
+    Described as 'random hypergraph' by M. Dewar et al. in https://arxiv.org/abs/1703.07686
+
+    Example
+    -------
+    >>> import easygraph as eg
+    >>> H = eg.random_hypergraph(50, [0.1, 0.01])
+
+    """
+    if seed is not None:
+        np.random.seed(seed)
+
+    if order is not None:
+        if len(ps) != 1:
+            raise EasyGraphError("ps must contain a single element if order is an int")
+
+    if (np.any(np.array(ps) < 0)) or (np.any(np.array(ps) > 1)):
+        raise EasyGraphError("All elements of ps must be between 0 and 1 included.")
+
+    nodes = range(N)
+    hyperedges = []
+
+    for i, p in enumerate(ps):
+        if order is not None:
+            d = order
+        else:
+            d = i + 1  # order, ps[0] is prob of edges (d=1)
+
+        potential_edges = combinations(nodes, d + 1)
+        n_comb = comb(N, d + 1, exact=True)
+        mask = np.random.random(size=n_comb) <= p  # True if edge to keep
+
+        edges_to_add = [e for e, val in zip(potential_edges, mask) if val]
+
+        hyperedges += edges_to_add
+
+    H = eg.Hypergraph(num_v=N)
+    H.add_hyperedges(hyperedges)
+
+    return H
```

## Comparing `easygraph/functions/hypergraph/hypergraph_generator/hypergraph_classic.py` & `easygraph/functions/hypergraph/null_model/hypergraph_classic.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,39 +1,39 @@
-import itertools
-
-import easygraph as eg
-
-from easygraph.utils.exception import EasyGraphError
-
-
-__all__ = ["empty_hypergraph", "complete_hypergraph"]
-
-
-def empty_hypergraph(N=1):
-    """
-
-    Parameters
-    ----------
-    N number of node in Hypergraph, default 1
-
-    Returns
-    -------
-    A eg.Hypergraph with n_num node, without any hyperedge.
-
-    """
-    return eg.Hypergraph(N)
-
-
-def complete_hypergraph(n, include_singleton=False):
-    if n == 0:
-        raise EasyGraphError("The number of nodes in a Hypergraph can not be zero")
-    # init
-    # print("easygraph:",eg)
-    hypergraph = eg.Hypergraph(n)
-    total_hyperedegs = []
-    if n > 1:
-        start = 1 if include_singleton else 2
-        for size in range(start, n + 1):
-            hyperedges = itertools.combinations(list(range(n)), size)
-            total_hyperedegs.extend(list(hyperedges))
-        hypergraph.add_hyperedges(total_hyperedegs)
-    return hypergraph
+import itertools
+
+import easygraph as eg
+
+from easygraph.utils.exception import EasyGraphError
+
+
+__all__ = ["empty_hypergraph", "complete_hypergraph"]
+
+
+def empty_hypergraph(N=1):
+    """
+
+    Parameters
+    ----------
+    N number of node in Hypergraph, default 1
+
+    Returns
+    -------
+    A eg.Hypergraph with n_num node, without any hyperedge.
+
+    """
+    return eg.Hypergraph(N)
+
+
+def complete_hypergraph(n, include_singleton=False):
+    if n == 0:
+        raise EasyGraphError("The number of nodes in a Hypergraph can not be zero")
+    # init
+    # print("easygraph:",eg)
+    hypergraph = eg.Hypergraph(n)
+    total_hyperedegs = []
+    if n > 1:
+        start = 1 if include_singleton else 2
+        for size in range(start, n + 1):
+            hyperedges = itertools.combinations(list(range(n)), size)
+            total_hyperedegs.extend(list(hyperedges))
+        hypergraph.add_hyperedges(total_hyperedegs)
+    return hypergraph
```

## Comparing `easygraph/functions/hypergraph/hypergraph_generator/tests/test_classic.py` & `easygraph/functions/hypergraph/null_model/tests/test_classic.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,48 +1,48 @@
-import easygraph as eg
-import pytest
-
-
-class TestClassic:
-    def test_complete_hypergraph(self):
-        print(eg.complete_hypergraph(10))
-        assert eg.complete_hypergraph(10) is not None
-
-    def test_random_hypergraph(self):
-        import random
-
-        import numpy as np
-
-        n = 100
-        k1 = {i: random.randint(1, 100) for i in range(n)}
-        k2 = {i: sorted(k1.values())[i] for i in range(n)}
-        H = eg.chung_lu_hypergraph(k1, k2)
-        H2 = eg.watts_strogatz_hypergraph(n=n, d=10, k=16, p=0.5, l=3)
-        k1 = {i: random.randint(1, n) for i in range(n)}
-        k2 = {i: sorted(k1.values())[i] for i in range(n)}
-        g1 = {i: random.choice([0, 1]) for i in range(n)}
-        g2 = {i: random.choice([0, 1]) for i in range(n)}
-        omega = np.array([[n // 2, 10], [10, n // 2]])
-        H3 = eg.dcsbm_hypergraph(k1, k2, g1, g2, omega)
-
-        assert H != None
-        assert H2 != None
-        assert H3 != None
-
-    def test_simple_hypergraph(self):
-        H = eg.star_clique(6, 7, 2)
-        print(H)
-
-    def test_uniform_hypergraph(self):
-        n = 1000
-        m = 3
-        k = {0: 1, 1: 2, 2: 3, 3: 3}
-        H = eg.uniform_hypergraph_configuration_model(k, m)
-        print(H)
-
-        H2 = eg.uniform_erdos_renyi_hypergraph(10, 5, 0.5, "prob")
-        # H2 = eg.uniform_HSBM(n,5,[3,4,5],[0.5,0.5,0.5])
-        print("H2:", H2)
-
-        H3 = eg.uniform_HPPM(10, 6, 0.9, 10, 0.9)
-
-        print("H3:", H3)
+import easygraph as eg
+import pytest
+
+
+class TestClassic:
+    def test_complete_hypergraph(self):
+        print(eg.complete_hypergraph(10))
+        assert eg.complete_hypergraph(10) is not None
+
+    def test_random_hypergraph(self):
+        import random
+
+        import numpy as np
+
+        n = 100
+        k1 = {i: random.randint(1, 100) for i in range(n)}
+        k2 = {i: sorted(k1.values())[i] for i in range(n)}
+        H = eg.chung_lu_hypergraph(k1, k2)
+        H2 = eg.watts_strogatz_hypergraph(n=n, d=10, k=16, p=0.5, l=3)
+        k1 = {i: random.randint(1, n) for i in range(n)}
+        k2 = {i: sorted(k1.values())[i] for i in range(n)}
+        g1 = {i: random.choice([0, 1]) for i in range(n)}
+        g2 = {i: random.choice([0, 1]) for i in range(n)}
+        omega = np.array([[n // 2, 10], [10, n // 2]])
+        H3 = eg.dcsbm_hypergraph(k1, k2, g1, g2, omega)
+
+        assert H != None
+        assert H2 != None
+        assert H3 != None
+
+    def test_simple_hypergraph(self):
+        H = eg.star_clique(6, 7, 2)
+        print(H)
+
+    def test_uniform_hypergraph(self):
+        n = 1000
+        m = 3
+        k = {0: 1, 1: 2, 2: 3, 3: 3}
+        H = eg.uniform_hypergraph_configuration_model(k, m)
+        print(H)
+
+        H2 = eg.uniform_erdos_renyi_hypergraph(10, 5, 0.5, "prob")
+        # H2 = eg.uniform_HSBM(n,5,[3,4,5],[0.5,0.5,0.5])
+        print("H2:", H2)
+
+        H3 = eg.uniform_HPPM(10, 6, 0.9, 10, 0.9)
+
+        print("H3:", H3)
```

