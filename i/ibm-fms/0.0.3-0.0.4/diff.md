# Comparing `tmp/ibm_fms-0.0.3-py3-none-any.whl.zip` & `tmp/ibm_fms-0.0.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,57 +1,58 @@
-Zip file size: 96948 bytes, number of entries: 55
--rw-r--r--  2.0 unx        0 b- defN 23-Aug-25 14:35 fms/__init__.py
--rw-r--r--  2.0 unx     5966 b- defN 24-Jan-29 21:35 fms/datasets/__init__.py
--rw-r--r--  2.0 unx     5328 b- defN 24-Jan-26 15:08 fms/datasets/arrow.py
--rw-r--r--  2.0 unx     3126 b- defN 24-Jan-26 15:08 fms/datasets/instructions.py
--rw-r--r--  2.0 unx     3181 b- defN 24-Jan-26 15:08 fms/datasets/text.py
--rw-r--r--  2.0 unx      488 b- defN 24-Jan-26 15:08 fms/distributed/__init__.py
--rw-r--r--  2.0 unx     4006 b- defN 24-Jan-26 15:08 fms/distributed/strategy.py
--rw-r--r--  2.0 unx     8520 b- defN 23-Nov-22 19:30 fms/distributed/tensorparallel.py
--rw-r--r--  2.0 unx    10295 b- defN 24-Jan-29 21:35 fms/models/__init__.py
--rw-r--r--  2.0 unx    10277 b- defN 24-Jan-29 21:35 fms/models/gpt_bigcode.py
--rw-r--r--  2.0 unx    16742 b- defN 24-Jan-29 21:35 fms/models/llama.py
--rw-r--r--  2.0 unx    10467 b- defN 24-Jan-29 17:43 fms/models/roberta.py
--rw-r--r--  2.0 unx     1426 b- defN 24-Jan-26 15:08 fms/models/hf/__init__.py
--rw-r--r--  2.0 unx    14675 b- defN 24-Jan-26 15:08 fms/models/hf/lm_head_mixins.py
--rw-r--r--  2.0 unx    84324 b- defN 24-Jan-29 21:35 fms/models/hf/modeling_hf_adapter.py
--rw-r--r--  2.0 unx     4775 b- defN 24-Jan-26 15:08 fms/models/hf/utils.py
--rw-r--r--  2.0 unx     7037 b- defN 24-Jan-26 15:08 fms/models/hf/gpt_bigcode/__init__.py
--rw-r--r--  2.0 unx     2620 b- defN 23-Nov-22 19:30 fms/models/hf/gpt_bigcode/configuration_gpt_bigcode_hf.py
--rw-r--r--  2.0 unx     3420 b- defN 24-Jan-29 21:35 fms/models/hf/gpt_bigcode/modeling_gpt_bigcode_hf.py
--rw-r--r--  2.0 unx     1498 b- defN 24-Jan-26 15:08 fms/models/hf/llama/__init__.py
--rw-r--r--  2.0 unx     2509 b- defN 23-Nov-22 19:30 fms/models/hf/llama/configuration_llama_hf.py
--rw-r--r--  2.0 unx     4822 b- defN 24-Jan-29 21:35 fms/models/hf/llama/modeling_llama_hf.py
--rw-r--r--  2.0 unx        0 b- defN 23-Nov-22 19:30 fms/models/hf/roberta/__init__.py
--rw-r--r--  2.0 unx     6187 b- defN 24-Jan-26 15:08 fms/models/hf/roberta/modeling_roberta_hf.py
--rw-r--r--  2.0 unx        0 b- defN 23-Aug-25 14:35 fms/modules/__init__.py
--rw-r--r--  2.0 unx    12949 b- defN 24-Jan-29 21:35 fms/modules/attention.py
--rw-r--r--  2.0 unx     8374 b- defN 24-Jan-26 15:08 fms/modules/embedding.py
--rw-r--r--  2.0 unx     9770 b- defN 24-Jan-26 15:08 fms/modules/feedforward.py
--rw-r--r--  2.0 unx     2467 b- defN 23-Nov-22 19:30 fms/modules/head.py
--rw-r--r--  2.0 unx     2512 b- defN 23-Aug-25 14:35 fms/modules/layernorm.py
--rw-r--r--  2.0 unx     9433 b- defN 24-Jan-29 21:35 fms/modules/positions.py
--rw-r--r--  2.0 unx     2661 b- defN 24-Jan-26 15:08 fms/modules/tp.py
--rw-r--r--  2.0 unx        0 b- defN 23-Sep-29 16:52 fms/testing/__init__.py
--rw-r--r--  2.0 unx     5596 b- defN 23-Nov-10 21:03 fms/testing/comparison.py
--rw-r--r--  2.0 unx        0 b- defN 23-Sep-29 16:10 fms/testing/_internal/__init__.py
--rw-r--r--  2.0 unx     9461 b- defN 24-Jan-26 15:08 fms/testing/_internal/model_test_suite.py
--rw-r--r--  2.0 unx        0 b- defN 23-Oct-03 13:40 fms/testing/_internal/hf/__init__.py
--rw-r--r--  2.0 unx    14836 b- defN 24-Jan-26 15:08 fms/testing/_internal/hf/model_test_suite.py
--rw-r--r--  2.0 unx        0 b- defN 23-Nov-22 19:30 fms/training/__init__.py
--rw-r--r--  2.0 unx     8009 b- defN 24-Jan-26 15:08 fms/training/plugins.py
--rw-r--r--  2.0 unx     2741 b- defN 24-Jan-26 15:08 fms/training/trainer.py
--rw-r--r--  2.0 unx      650 b- defN 23-Nov-22 19:30 fms/utils/__init__.py
--rw-r--r--  2.0 unx     2020 b- defN 23-Oct-12 03:47 fms/utils/activation.py
--rw-r--r--  2.0 unx     1872 b- defN 24-Jan-26 15:08 fms/utils/config.py
--rw-r--r--  2.0 unx     2626 b- defN 24-Jan-29 21:35 fms/utils/evaluation.py
--rw-r--r--  2.0 unx     4377 b- defN 24-Jan-29 21:35 fms/utils/generation.py
--rw-r--r--  2.0 unx    16944 b- defN 24-Jan-29 23:13 fms/utils/serialization.py
--rw-r--r--  2.0 unx     4982 b- defN 24-Jan-29 21:35 fms/utils/tensors.py
--rw-r--r--  2.0 unx     5337 b- defN 23-Nov-22 19:30 fms/utils/tokenizers.py
--rw-r--r--  2.0 unx     1526 b- defN 23-Nov-15 19:16 fms/utils/tp_wrapping.py
--rw-r--r--  2.0 unx    11357 b- defN 24-Jan-29 23:13 ibm_fms-0.0.3.dist-info/LICENSE
--rw-r--r--  2.0 unx      683 b- defN 24-Jan-29 23:13 ibm_fms-0.0.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Jan-29 23:13 ibm_fms-0.0.3.dist-info/WHEEL
--rw-r--r--  2.0 unx        4 b- defN 24-Jan-29 23:13 ibm_fms-0.0.3.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     4611 b- defN 24-Jan-29 23:13 ibm_fms-0.0.3.dist-info/RECORD
-55 files, 357579 bytes uncompressed, 89648 bytes compressed:  74.9%
+Zip file size: 98076 bytes, number of entries: 56
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-09 13:04 fms/__init__.py
+-rw-r--r--  2.0 unx     1786 b- defN 24-Apr-09 13:04 fms/datasets/__init__.py
+-rw-r--r--  2.0 unx     5384 b- defN 24-Apr-09 13:04 fms/datasets/arrow.py
+-rw-r--r--  2.0 unx     3055 b- defN 24-Apr-09 13:04 fms/datasets/instructions.py
+-rw-r--r--  2.0 unx     2968 b- defN 24-Apr-09 13:04 fms/datasets/text.py
+-rw-r--r--  2.0 unx     5159 b- defN 24-Apr-09 13:04 fms/datasets/util.py
+-rw-r--r--  2.0 unx      590 b- defN 24-Apr-09 13:04 fms/distributed/__init__.py
+-rw-r--r--  2.0 unx     4006 b- defN 24-Apr-09 13:04 fms/distributed/strategy.py
+-rw-r--r--  2.0 unx     7423 b- defN 24-Apr-09 13:04 fms/distributed/tensorparallel.py
+-rw-r--r--  2.0 unx    10549 b- defN 24-Apr-09 13:04 fms/models/__init__.py
+-rw-r--r--  2.0 unx    14866 b- defN 24-Apr-09 13:04 fms/models/gpt_bigcode.py
+-rw-r--r--  2.0 unx    18367 b- defN 24-Apr-09 13:04 fms/models/llama.py
+-rw-r--r--  2.0 unx    10683 b- defN 24-Apr-09 13:04 fms/models/roberta.py
+-rw-r--r--  2.0 unx     1426 b- defN 24-Apr-09 13:04 fms/models/hf/__init__.py
+-rw-r--r--  2.0 unx    14675 b- defN 24-Apr-09 13:04 fms/models/hf/lm_head_mixins.py
+-rw-r--r--  2.0 unx    84324 b- defN 24-Apr-09 13:04 fms/models/hf/modeling_hf_adapter.py
+-rw-r--r--  2.0 unx     4775 b- defN 24-Apr-09 13:04 fms/models/hf/utils.py
+-rw-r--r--  2.0 unx     3488 b- defN 24-Apr-09 13:04 fms/models/hf/gpt_bigcode/__init__.py
+-rw-r--r--  2.0 unx     2659 b- defN 24-Apr-09 13:04 fms/models/hf/gpt_bigcode/configuration_gpt_bigcode_hf.py
+-rw-r--r--  2.0 unx     3420 b- defN 24-Apr-09 13:04 fms/models/hf/gpt_bigcode/modeling_gpt_bigcode_hf.py
+-rw-r--r--  2.0 unx     1498 b- defN 24-Apr-09 13:04 fms/models/hf/llama/__init__.py
+-rw-r--r--  2.0 unx     2509 b- defN 24-Apr-09 13:04 fms/models/hf/llama/configuration_llama_hf.py
+-rw-r--r--  2.0 unx     4822 b- defN 24-Apr-09 13:04 fms/models/hf/llama/modeling_llama_hf.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-09 13:04 fms/models/hf/roberta/__init__.py
+-rw-r--r--  2.0 unx     6187 b- defN 24-Apr-09 13:04 fms/models/hf/roberta/modeling_roberta_hf.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-09 13:04 fms/modules/__init__.py
+-rw-r--r--  2.0 unx    12229 b- defN 24-Apr-09 13:04 fms/modules/attention.py
+-rw-r--r--  2.0 unx     8306 b- defN 24-Apr-09 13:04 fms/modules/embedding.py
+-rw-r--r--  2.0 unx     8944 b- defN 24-Apr-09 13:04 fms/modules/feedforward.py
+-rw-r--r--  2.0 unx     2467 b- defN 24-Apr-09 13:04 fms/modules/head.py
+-rw-r--r--  2.0 unx     2488 b- defN 24-Apr-09 13:04 fms/modules/layernorm.py
+-rw-r--r--  2.0 unx     9492 b- defN 24-Apr-09 13:04 fms/modules/positions.py
+-rw-r--r--  2.0 unx     2661 b- defN 24-Apr-09 13:04 fms/modules/tp.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-09 13:04 fms/testing/__init__.py
+-rw-r--r--  2.0 unx     5596 b- defN 24-Apr-09 13:04 fms/testing/comparison.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-09 13:04 fms/testing/_internal/__init__.py
+-rw-r--r--  2.0 unx     9461 b- defN 24-Apr-09 13:04 fms/testing/_internal/model_test_suite.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-09 13:04 fms/testing/_internal/hf/__init__.py
+-rw-r--r--  2.0 unx    14836 b- defN 24-Apr-09 13:04 fms/testing/_internal/hf/model_test_suite.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-09 13:04 fms/training/__init__.py
+-rw-r--r--  2.0 unx    10914 b- defN 24-Apr-09 13:04 fms/training/plugins.py
+-rw-r--r--  2.0 unx     3020 b- defN 24-Apr-09 13:04 fms/training/trainer.py
+-rw-r--r--  2.0 unx      650 b- defN 24-Apr-09 13:04 fms/utils/__init__.py
+-rw-r--r--  2.0 unx     2020 b- defN 24-Apr-09 13:04 fms/utils/activation.py
+-rw-r--r--  2.0 unx     1872 b- defN 24-Apr-09 13:04 fms/utils/config.py
+-rw-r--r--  2.0 unx     2626 b- defN 24-Apr-09 13:04 fms/utils/evaluation.py
+-rw-r--r--  2.0 unx     4377 b- defN 24-Apr-09 13:04 fms/utils/generation.py
+-rw-r--r--  2.0 unx    17283 b- defN 24-Apr-09 13:04 fms/utils/serialization.py
+-rw-r--r--  2.0 unx     4982 b- defN 24-Apr-09 13:04 fms/utils/tensors.py
+-rw-r--r--  2.0 unx     5337 b- defN 24-Apr-09 13:04 fms/utils/tokenizers.py
+-rw-r--r--  2.0 unx     1596 b- defN 24-Apr-09 13:04 fms/utils/tp_wrapping.py
+-rw-r--r--  2.0 unx    11357 b- defN 24-Apr-09 13:05 ibm_fms-0.0.4.dist-info/LICENSE
+-rw-r--r--  2.0 unx      683 b- defN 24-Apr-09 13:05 ibm_fms-0.0.4.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-09 13:05 ibm_fms-0.0.4.dist-info/WHEEL
+-rw-r--r--  2.0 unx        4 b- defN 24-Apr-09 13:05 ibm_fms-0.0.4.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     4689 b- defN 24-Apr-09 13:05 ibm_fms-0.0.4.dist-info/RECORD
+56 files, 362601 bytes uncompressed, 90660 bytes compressed:  75.0%
```

## zipnote {}

```diff
@@ -9,14 +9,17 @@
 
 Filename: fms/datasets/instructions.py
 Comment: 
 
 Filename: fms/datasets/text.py
 Comment: 
 
+Filename: fms/datasets/util.py
+Comment: 
+
 Filename: fms/distributed/__init__.py
 Comment: 
 
 Filename: fms/distributed/strategy.py
 Comment: 
 
 Filename: fms/distributed/tensorparallel.py
@@ -144,23 +147,23 @@
 
 Filename: fms/utils/tokenizers.py
 Comment: 
 
 Filename: fms/utils/tp_wrapping.py
 Comment: 
 
-Filename: ibm_fms-0.0.3.dist-info/LICENSE
+Filename: ibm_fms-0.0.4.dist-info/LICENSE
 Comment: 
 
-Filename: ibm_fms-0.0.3.dist-info/METADATA
+Filename: ibm_fms-0.0.4.dist-info/METADATA
 Comment: 
 
-Filename: ibm_fms-0.0.3.dist-info/WHEEL
+Filename: ibm_fms-0.0.4.dist-info/WHEEL
 Comment: 
 
-Filename: ibm_fms-0.0.3.dist-info/top_level.txt
+Filename: ibm_fms-0.0.4.dist-info/top_level.txt
 Comment: 
 
-Filename: ibm_fms-0.0.3.dist-info/RECORD
+Filename: ibm_fms-0.0.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## fms/datasets/__init__.py

```diff
@@ -1,19 +1,44 @@
 from typing import Any, Callable, List, Mapping, Optional
 
+import torch
 from torch.utils.data import Dataset, IterableDataset
 
-from fms.datasets import text
+from fms.datasets import arrow, text
 from fms.datasets.instructions import JsonInstructions
 from fms.utils.tokenizers import BaseTokenizer
 
 
+def _arrow_ds_generator(data, tokenizer, **kwargs):
+    return arrow.ArrowFilesDataset(data, **kwargs)
+
+
+class MockDataset(IterableDataset):
+    def __init__(self, data, tokenizer: BaseTokenizer, max_seq_len=4096):
+        self.tokenizer = tokenizer
+        self.data = data
+        self.max_seq_len = max_seq_len
+        self.last_val = 0
+
+    def nextval(self):
+        self.last_val += 1
+        self.last_val = self.last_val % self.tokenizer.vocab_size()
+        return self.last_val
+
+    def __iter__(self):
+        while True:
+            t = torch.tensor([self.nextval() for _ in range(self.max_seq_len)])
+            yield t, t
+
+
 __dataset_factory: Mapping[str, Callable[[str, BaseTokenizer], Dataset] | type] = {
     "instruction": JsonInstructions,
     "text": text.causaltext,
+    "arrow": _arrow_ds_generator,
+    "mock": MockDataset,
 }
 
 
 def get_dataset(name: str, tokenizer: BaseTokenizer, data: str = "", **kwargs):
     """
     Get a dataset by type.
 
@@ -24,143 +49,14 @@
     name = name.lower()
     if name not in __dataset_factory:
         raise NameError(f"Dataset name should be one of {__dataset_factory.keys()}")
     ctr = __dataset_factory[name]
     return ctr(data, tokenizer, **kwargs)
 
 
-def _state_dict_save_helper(target):
-    if isinstance(target, dict):
-        dict_attrs = target
-    elif hasattr(target, "__dict__"):
-        dict_attrs = target.__dict__
-    else:
-        return target
-    result = {}
-    for attr in dict_attrs:
-        if isinstance(attr, str) and (not len(attr) or attr[0] == "_"):
-            continue
-
-        if isinstance(dict_attrs[attr], SavableDataset):
-            sub_dict = dict_attrs[attr].state_dict()
-            for sub_attr in sub_dict:
-                result[f"{attr}.{sub_attr}"] = sub_dict[sub_attr]
-        # We don't serialize any dataset that doesn't have this mixin.
-        elif isinstance(dict_attrs[attr], Dataset):
-            raise TypeError(
-                "Attempting to serialize an unsupported dataset",
-                attr,
-                type(dict_attrs[attr]),
-            )
-        elif isinstance(dict_attrs[attr], dict):
-            sub_dict = _state_dict_save_helper(dict_attrs[attr])
-            # format like pytorch nn.Module state dicts instead of nesting
-            for sub_attr in sub_dict:
-                result[f"{attr}.{sub_attr}"] = sub_dict[sub_attr]
-        elif isinstance(dict_attrs[attr], list):
-            result[attr] = [_state_dict_save_helper(x) for x in dict_attrs[attr]]
-        else:
-            result[attr] = dict_attrs[attr]
-    return result
-
-
-def _state_dict_load_helper(target, state_dict):
-    if isinstance(target, dict):
-        dict_attrs = target
-    else:
-        dict_attrs = target.__dict__
-    for attr in state_dict:
-        if "." in attr:
-            attr_name, sub_attr = attr.split(".", 1)
-            sub_dict = {sub_attr: state_dict[attr]}
-            if isinstance(dict_attrs[attr_name], SavableDataset):
-                # Make sure we use any overriden load_state_dict in nested datasets
-                dict_attrs[attr_name].load_state_dict(sub_dict)
-            else:
-                _state_dict_load_helper(dict_attrs[attr_name], sub_dict)
-        elif attr not in dict_attrs:
-            raise KeyError(f"Unexpected key {attr} in state dict")
-        elif isinstance(dict_attrs[attr], SavableDataset):
-            dict_attrs[attr].load_state_dict(state_dict[attr])
-        elif isinstance(dict_attrs[attr], dict):
-            _state_dict_load_helper(dict_attrs[attr], state_dict[attr])
-        else:
-            dict_attrs[attr] = state_dict[attr]
-
-
-class SavableDataset:
-    """
-    In large-scale pre-training, because we typically only train for only a
-    single epoch, we often need to be able to retain the state of the dataset
-    across restarts.
-    This mixin indicates a dataset that can be serialized and deserialized.
-    IterableDatasets that implement this interface are expected to be
-    re-startable (pick up where they left off).
-
-    If you need a restartable MapDataSet, wrap it in a
-    RestartableFromMapDataset.
-
-    The default implementation may not work for your dataset, so override
-    `state_dict` and/or `load_state_dict` as-needed.
-    """
-
-    def state_dict(self):
-        return _state_dict_save_helper(self)
-
-    # In cases where the instance of SavableDataset composes another
-    # DataSet, an explicit implementation of this function will be needed.
-    # This default implementation doesn't know the type of the serialized
-    # dataset, so can't construct it.
-    def load_state_dict(self, state_dict):
-        _state_dict_load_helper(self, state_dict)
-
-
-class RestartableFromMapDataset(SavableDataset, IterableDataset):
-    def __init__(self, map_ds: Dataset):
-        super().__init__()
-        self._map_ds = map_ds
-        self.current_index = 0
-
-    def __iter__(self):
-        for index in range(self.current_index, len(self._map_ds)):
-            self.current_index = index + 1
-            yield self._map_ds[index]
-
-    def __len__(self):
-        return len(self._map_ds)
-
-
-class PackedSequenceDataset(Dataset, SavableDataset):
-    def __init__(self, dataset: SavableDataset, max_seq_len: int):
-        self.dataset = dataset
-        self.max_seq_len = max_seq_len
-        self.buffer: List[Any] = []
-
-    def __iter__(self):
-        for example in self.dataset:
-            self.buffer.extend(example)
-            while len(self.buffer) >= self.max_seq_len:
-                next_val = self.buffer[: self.max_seq_len]
-                self.buffer = self.buffer[self.max_seq_len :]
-                yield next_val
-
-
-class WithSeparatorDataset(Dataset, SavableDataset):
-    def __init__(
-        self,
-        dataset: SavableDataset,
-        bos_token_id: Optional[int] = None,
-        eos_token_id: Optional[int] = None,
-    ):
-        self.dataset = dataset
-        self._bos_token_id = bos_token_id
-        self._eos_token_id = eos_token_id
-
-    def __iter__(self):
-        for example in self.dataset:
-            result = []
-            if self._bos_token_id is not None:
-                result.append(self._bos_token_id)
-            result.extend(example)
-            if self._eos_token_id is not None:
-                result.append(self._eos_token_id)
-            yield result
+# avoid circular dependency
+from fms.datasets.util import (
+    PackedSequenceDataset,
+    RestartableFromMapDataset,
+    SavableDataset,
+    WithSeparatorDataset,
+)
```

## fms/datasets/arrow.py

```diff
@@ -1,17 +1,19 @@
 import sys
 import warnings
 from collections import UserDict
+from typing import Optional
 
 import pyarrow as pa
+import torch
 import urllib3
 from pyarrow.fs import FileSystem, FileType, LocalFileSystem, S3FileSystem
 from torch.utils.data import Dataset, IterableDataset
 
-from fms.datasets import SavableDataset
+from fms.datasets.util import SavableDataset
 
 
 class _ArrowFileData(UserDict):
     def __init__(self, fs: FileSystem, path: str, column_name: str = "tokens"):
         self.fs = fs
         self.path = path
         self.column_name = column_name
@@ -51,15 +53,15 @@
 
     def __init__(
         self,
         uri: str,
         rank: int = 0,
         world_size: int = 1,
         column_name: str = "tokens",
-        max_seq_len: int = 4096,
+        max_seq_len: Optional[int] = None,
     ):
         self.uri = uri
         self._rank = rank
         self.start_idx = 0
         self._step = world_size
         self.column_name = column_name
         self.max_seq_len = max_seq_len
@@ -117,18 +119,17 @@
 
             for i in range(self._file_offset, len(file), self._step):
                 rb = file[i]
                 while self.record_batch_offset < len(rb):
                     current = rb.slice(
                         offset=self.record_batch_offset, length=self.max_seq_len
                     ).to_pylist()
-                    self.record_batch_offset = (
-                        self.record_batch_offset + self.max_seq_len
-                    )
-                    yield current
+                    self.record_batch_offset = self.record_batch_offset + len(current)
+                    t = torch.tensor(current)
+                    yield t[:-1], t[1:]
                 self.start_idx += self._step
                 self.record_batch_offset = 0
 
             self._file_offset = next_file_offset
 
 
 if __name__ == "__main__":
```

## fms/datasets/instructions.py

```diff
@@ -39,22 +39,20 @@
     https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json
     """
 
     def __init__(
         self,
         path: str,
         tokenizer: tokenizers.BaseTokenizer,
-        device="cpu",
         max_len: int = 1024,
         ignore_index=-100,
     ):
         self.tokenizer = tokenizer
         self.ignore_index = ignore_index
         self.max_len = max_len
-        self.device = device
         self.bos_token_id = tokenizer.bos_token_id
         self.eos_token_id = tokenizer.eos_token_id
         if urllib.parse.urlparse(path).scheme == "":
             file = os.path.expanduser(path)
             with open(file, "r", encoding="utf-8") as reader:
                 text = reader.read()
                 self.instructions = json.loads(text)
@@ -86,15 +84,15 @@
 
         if self.bos_token_id is not None:
             example = [self.bos_token_id] + example
 
         if self.eos_token_id is not None:
             example = example + [self.eos_token_id]
 
-        example = torch.tensor(example, dtype=torch.long, device=self.device)
+        example = torch.tensor(example, dtype=torch.long)
 
         input = example[:-1]
 
         label = example[1:].clone()
         label[: len(prompt)] = self.ignore_index
 
         if input.shape[0] > self.max_len:
```

## fms/datasets/text.py

```diff
@@ -18,44 +18,37 @@
 
     def __init__(
         self,
         text: str,
         tokenizer: tokenizers.BaseTokenizer,
         seq_len: int = 1024,
         pad_token: Optional[str] = None,
-        device: torch.device | str = "cpu",
         ignore_index: int = -100,
     ):
         tokens = tokenizer.tokenize(text)
         ids = tokenizer.convert_tokens_to_ids(tokens)
-        self.ids = torch.tensor(ids, dtype=torch.long, device=device)
+        self.ids = torch.tensor(ids, dtype=torch.long)
         self.ignore_index = ignore_index
         if pad_token is not None:
             self.pad_id = tokenizer.convert_tokens_to_ids([pad_token])[0]
         else:
             self.pad_id = None
         self.tokenizer = tokenizer
         self.seq_len = seq_len
 
-    def to(self, device: torch.device):
-        self.ids = self.ids.to(device)
-        return self
-
     def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
         start_idx = idx * self.seq_len
         end_idx = start_idx + self.seq_len + 1
         if end_idx >= self.ids.shape[0]:
             end_idx = self.ids.shape[0]
         input = self.ids[start_idx : end_idx - 1]
         label = self.ids[start_idx + 1 : end_idx]
 
         if self.pad_id is not None and input.shape[0] < self.seq_len:
-            pad = torch.zeros(
-                self.seq_len - input.shape[0], device=self.ids.device, dtype=torch.long
-            )
+            pad = torch.zeros(self.seq_len - input.shape[0], dtype=torch.long)
             pad.fill_(self.pad_id)
             input = torch.cat((pad, input), dim=0)
             label = torch.cat((pad.fill_(self.ignore_index), label), dim=0)
         return input, label
 
     def __len__(self):
         tokens = self.ids.shape[0]
```

## fms/distributed/__init__.py

```diff
@@ -1,7 +1,9 @@
+import os
+
 import torch
 
 
 def rank_and_world(group=None):
     """
     Returns (rank, world_size) from the optionally-specified group, otherwise
     from the default group, or if non-distributed just returns (0, 1)
@@ -13,7 +15,14 @@
         world_size = 1
         rank = 0
     else:
         world_size = group.size()
         rank = group.rank()
 
     return rank, world_size
+
+
+_LOCAL_RANK = int(os.getenv("LOCAL_RANK", 0))
+
+
+def local_rank():
+    return _LOCAL_RANK
```

## fms/distributed/tensorparallel.py

```diff
@@ -1,16 +1,14 @@
 # mypy: disable-error-code="method-assign,misc"
-from typing import Any, List, Tuple
 
 import torch
-import torch._dynamo as dynamo
-import torch._inductor.codegen.wrapper as inductor_wrapper
-import torch._inductor.ir as inductor_ir
-import torch._inductor.lowering as inductor_lowering
-import torch.distributed._functional_collectives as distfunc
+import torch._inductor.ir as ir
+import torch._inductor.lowering as lowering
+import torch.distributed as dist
+import torch.distributed._functional_collectives as funcol
 from torch import nn
 
 
 def apply_colwise_tp(par_mod: nn.Linear, mod: nn.Linear, world_size, rank):
     # Divide the weight matrix along the last dimension.
     output_size_per_partition = mod.out_features // world_size
     with torch.no_grad():
@@ -43,143 +41,110 @@
     with torch.no_grad():
         par_mod.weight.copy_(
             torch.split(mod.weight, output_size_per_partition, dim=1)[rank]
         )
     # print(f"For rank {rank}, we have the following weights: Base weight {mod.weight} bias {mod.bias}; Par weight {par_mod.weight}, bias {par_mod.bias}")
 
 
-def _all_reduce(input_: torch.Tensor) -> torch.Tensor:
-    """All-reduce the input tensor across model parallel group."""
-    world_size = torch.distributed.get_world_size()
-
-    if world_size == 1:
-        return input_
-
-    return distfunc.all_reduce(input_, "sum", list(range(world_size)))
-
-
-# Fix #1 is porting the code changes in https://github.com/pytorch/pytorch/pull/108811
-@classmethod
-def wait_create(cls, collective_op: "inductor_ir.TensorBox"):
-    collective_op.decide_layout()
-    return inductor_ir.Wait(
-        layout=inductor_ir.AliasedLayout(collective_op),  # type: ignore[arg-type]
-        inputs=[collective_op],
-    )
+## Fixes for PT 2.2 collectives until PT 2.3 is released
 
 
-inductor_ir.Wait.create = wait_create  # type: ignore[assignment]
-
-inductor_ir.AllReduce.get_mutation_names = lambda self: [self.inputs[0].get_name()]  # type: ignore[attr-defined]
-
-
-@classmethod
-def all_reduce_create(
-    cls,
-    x: "inductor_ir.TensorBox",
-    reduce_op: str,
-    tag: str,
-    ranks: List[int],
-    group_size: int,
-):
-    inplace_inputs = cls.wrap_inputs_as_inplace([x])
-    layout = inductor_ir.MutationLayout(inplace_inputs[0])
-
-    _ = inductor_ir.AllReduce(
-        layout=layout,
-        inputs=inplace_inputs,
-        constant_args=[tag, ranks, group_size],
-        reduce_op=reduce_op,
+# Fix 1: https://github.com/pytorch/pytorch/issues/121311
+def get_volatile_reads_fixed(self):
+    inp = self.inputs[0]
+    if isinstance(inp, ir._CollectiveKernel):
+        # Out-of-place single-output
+        return [inp.inputs[0]]
+    elif isinstance(inp, ir.MultiOutput):
+        # Out-of-place multi-output
+        coll = inp.inputs[0]
+        if isinstance(coll, ir._CollectiveKernel):
+            _, idx = inp.indices[0]
+            return [coll.inputs[idx]]
+        return []  # e.g. regular FallbackKernel
+    else:
+        # In-place requires no additional deps handling for volatile
+        # reads since the inputs are mutated.
+        return []
+
+
+ir._WaitKernel.get_volatile_reads = get_volatile_reads_fixed
+
+# Fix 2: These are fixed already in nightlies and will be in 2.3
+for overload in torch.ops._c10d_functional.all_reduce.overloads():
+    other_fn = getattr(torch.ops._c10d_functional.all_reduce, overload)
+    if other_fn in lowering.lowerings:
+        del lowering.lowerings[other_fn]
+
+
+@lowering.register_lowering(torch.ops._c10d_functional.all_reduce)
+def _all_reduce_fixed(inp, reduce_op, group_name):
+    inp = torch.clone(inp)
+    ir._CollectiveKernel.create_inplace(
+        torch.ops._c10d_functional.all_reduce_.default,
+        ir.ExternKernel.require_contiguous(inp),
+        reduce_op,
+        group_name,
     )
-    return inplace_inputs[0]
-
-
-inductor_ir.AllReduce.create = all_reduce_create  # type: ignore[assignment]
-
-
-def wcg_codegen_free(self, buffer):
-    name = buffer.get_name()
-
-    # can be freed but not reused
-    # TODO: Port this one-line fix to PyTorch
-    if isinstance(buffer, (inductor_ir.InputBuffer, inductor_ir.OutputBuffer)):
-        self.writeline(self.make_buffer_free(buffer))
-        return
-
-    if not self.can_reuse(buffer):
-        return
-    self.freed.add(name)
+    return inp
 
-    layout = buffer.get_layout()
-    if isinstance(layout, (inductor_ir.AliasedLayout, inductor_ir.MultiOutputLayout)):
-        self.writeline(self.make_buffer_free(buffer))
-        return
 
-    self.writeline(inductor_wrapper.FreeIfNotReusedLine(self, buffer))
-
-
-inductor_wrapper.WrapperCodeGen.codegen_free = wcg_codegen_free
-# End of fix #1
-
-
-# Fix #2: Asserts + dynamic shapes create graph breaks
-# This function is redefined from torch.distributed._functional_collectives.all_gather_tensor
-# to remove an assert that creates an extra graph break
-def _all_gather_tensor(
-    self: torch.Tensor,
-    gather_dim: int,
-    group: distfunc.RANK_TYPES,
-    tag: str = "",
-):
-    tag, rankset, group_size = distfunc._expand_group(group, tag)
-    tensor = torch.ops.c10d_functional.all_gather_into_tensor(self, tag, rankset, group_size)  # type: ignore[attr-defined]
-    res = distfunc._maybe_wrap_tensor(tensor)
-    # TODO this should be done inside AsyncCollectiveTensor to delay the wait() call
-    if gather_dim != 0:
-        res = torch.cat(torch.chunk(res, group_size, dim=0), dim=gather_dim)
-    return res
-
-
-# Fix #3: Avoid recompiles on batch size for embedding + TP (until https://github.com/pytorch/pytorch/pull/109561 lands)
-for overload in torch.ops.c10d_functional.all_gather_into_tensor.overloads():
-    other_fn = getattr(torch.ops.c10d_functional.all_gather_into_tensor, overload)
-    if other_fn in inductor_lowering.lowerings:
-        del inductor_lowering.lowerings[other_fn]
-
-
-@inductor_lowering.register_lowering(torch.ops.c10d_functional.all_gather_into_tensor)
-def all_gather_into_tensor(shard, tag, ranks, group_size):
-    return inductor_ir.TensorBox.create(
-        inductor_ir.AllGatherIntoTensor.create(
-            inductor_ir.ExternKernel.require_contiguous(shard), tag, ranks, group_size
+for overload in torch.ops._c10d_functional.all_gather_into_tensor.overloads():
+    other_fn = getattr(torch.ops._c10d_functional.all_gather_into_tensor, overload)
+    if other_fn in lowering.lowerings:
+        del lowering.lowerings[other_fn]
+
+
+@lowering.register_lowering(torch.ops._c10d_functional.all_gather_into_tensor)
+def _all_gather_into_tensor(inp, group_size, group_name):
+    return ir.TensorBox.create(
+        ir._CollectiveKernel.create_out_of_place(
+            torch.ops._c10d_functional.all_gather_into_tensor.default,
+            ir.ExternKernel.require_contiguous(inp),
+            group_size,
+            group_name,
         )
     )
 
 
 def _all_gather(input_: torch.Tensor) -> torch.Tensor:
     """Gather the input tensor across model parallel group."""
-    world_size = torch.distributed.get_world_size()
+    world_size = dist.get_world_size()
 
     if world_size == 1:
         return input_
 
     # The transposes here are to avoid excessive recompilation due to split()
     # specializing the dimension where the all_gather is happening
     last_dim = input_.dim() - 1
+    # Starting PT 2.3, we can go back to funcol.all_gather_tensor
     return (
-        _all_gather_tensor(
-            input_.transpose(0, last_dim).contiguous(),
-            0,
-            list(range(world_size)),
+        torch.ops._c10d_functional.wait_tensor(
+            torch.ops._c10d_functional.all_gather_into_tensor(
+                input_.transpose(0, last_dim).contiguous(), world_size, "default"
+            )
         )
         .transpose(0, last_dim)
         .contiguous()
     )
 
 
+def _all_reduce(input_: torch.Tensor) -> torch.Tensor:
+    """All-reduce the input tensor across model parallel group."""
+    world_size = dist.get_world_size()
+
+    if world_size == 1:
+        return input_
+
+    # Starting PT 2.3, we can go back to funcol.all_reduce
+    return torch.ops._c10d_functional.wait_tensor(
+        torch.ops._c10d_functional.all_reduce(input_, "sum", "default")
+    )
+
+
 def _split(input_: torch.Tensor, rank, world_size) -> torch.Tensor:
     """Split the tensor along its last dimension and keep the
     corresponding slice."""
 
     if world_size == 1:
         return input_
```

## fms/models/__init__.py

```diff
@@ -1,10 +1,10 @@
 from contextlib import nullcontext
 from functools import partial
-from typing import Callable, MutableMapping, Optional
+from typing import Any, Callable, MutableMapping, Optional
 
 import torch
 from torch import nn
 from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (
     CheckpointImpl,
     apply_activation_checkpointing,
     checkpoint_wrapper,
@@ -182,15 +182,14 @@
 
     model = FSDP(
         model,
         param_init_fn=init_fn,
         sync_module_states=True,
         device_id=device.index,
         limit_all_gathers=True,
-        use_orig_params=True,
         auto_wrap_policy=_fsdp_autowrap_policy,
         mixed_precision=mp_policy,
         sharding_strategy=dp_strategy,
     )
 
     wrapper_fn = partial(
         checkpoint_wrapper, checkpoint_impl=CheckpointImpl.NO_REENTRANT
@@ -233,44 +232,45 @@
     checkpoint_sharding: how the checkpoint files are sharded: None, 'tp',
                 'fsdp', or 'layer'. If None, guess based on files.
     source: If the weights in the state dict didn't come from an FMS model,
                 `source` specifies which conversion function might be needed.
                 See `serialization.list_sources(architecture)`
     group: ProcessGroup The PG to use for any model distribution
     """
-    local_rank, world_size = distributed.rank_and_world(group)
+    rank, world_size = distributed.rank_and_world(group)
+    local_rank = distributed.local_rank()
 
     if distributed_strategy is None or distributed_strategy == "":
         if world_size > 1:
             distributed_strategy = "tp"
 
     if device_type == "cuda":
         device = torch.device(device_type, local_rank)
     else:
         device = torch.device(device_type)
 
-    if (
-        _is_dp(distributed_strategy)
-        and local_rank != 0
-        and checkpoint_sharding != "fsdp"
-    ):
+    hsdp = distributed_strategy == "hsdp"
+    fsdp = distributed_strategy == "fsdp"
+    ddp = distributed_strategy == "ddp"
+    if (hsdp and local_rank != 0) or ((fsdp or ddp) and rank != 0):
         initial_device = torch.device("meta")
     elif distributed_strategy == "mp":
         initial_device = torch.device("cpu")
     else:
         initial_device = device
 
+    lazy_sd: MutableMapping[str, Any] = {}
     if model_path is not None:
         lazy_sd = serialization.load_state_dict(
             model_path,
             source=source,
             distributed_strategy=distributed_strategy,
             checkpoint_sharding=checkpoint_sharding,
             initial_device=initial_device,
-            rank=local_rank,
+            rank=rank,
             world_size=world_size,
         )
 
     extra_args = kwargs
     if "distributed_strategy" not in extra_args:
         if distributed_strategy == "tp":
             print("using tensor parallel")
@@ -291,33 +291,35 @@
     # distribution strategy and checkpoint sharding
     pre_load = (
         distributed_strategy in ["fsdp", "hsdp"] and checkpoint_sharding != "fsdp"
     )
 
     def model_wrap(model):
         if _is_dp(distributed_strategy):
-            return _fsdp_wrap(model, distributed_strategy, device, local_rank == 0)
+            return _fsdp_wrap(model, distributed_strategy, device, rank == 0)
         return model
 
     if not pre_load:
         fms_model = model_wrap(fms_model)
 
     if len(lazy_sd):
         serialization.load_state_dict_into_model(
             fms_model,
             lazy_sd,
             architecture,
             source if source is not None else "fms",
             distributed_strategy,
             checkpoint_sharding,
             initial_device,
-            local_rank,
+            local_rank if distributed_strategy == "hsdp" else rank,
             world_size,
         )
+    elif hasattr(fms_model, "reset_parameters"):
+        fms_model.reset_parameters()
 
     if pre_load:
         fms_model = model_wrap(fms_model)
 
     return fms_model
 
 
-from fms.models import llama, roberta
+from fms.models import gpt_bigcode, llama, roberta
```

## fms/models/gpt_bigcode.py

```diff
@@ -1,30 +1,33 @@
 import math
 from dataclasses import dataclass
-from typing import List, Optional, Tuple
+from typing import List, Mapping, Optional, Tuple
 
 import torch
 import torch.nn as nn
 
+from fms import models
 from fms.modules.attention import MultiHeadAttention
 from fms.modules.feedforward import FeedForwardBlock
+from fms.utils import serialization
 from fms.utils.activation import str_to_activation
 from fms.utils.config import ModelConfig
+from fms.utils.serialization import FusableWeightsMissingError
 
 
 @dataclass
 class GPTBigCodeConfig(ModelConfig):
     # This param default is based on https://huggingface.co/bigcode/gpt_bigcode-santacoder
     src_vocab_size: int = 49157
     # This param default is based on https://huggingface.co/bigcode/gpt_bigcode-santacoder
     emb_dim: int = 2048
     nheads: int = 12
     nlayers: int = 12
     pad_id: int = 0
-    max_pos: int = 512
+    max_expected_seq_len: int = 512
     hidden_grow_factor: float = 4.0
     activation_fn: str = "gelu-tanh"
     p_dropout: float = 0.0
     emb_dropout: float = 0.0
     multiquery_attn: bool = True
     ln_eps: float = 1e-5
 
@@ -117,15 +120,17 @@
         self.config = config
 
         self.layers = nn.ModuleList(
             [GPTBigCodeBlock(self.config) for _ in range(self.config.nlayers)]
         )
 
         self.embedding = nn.Embedding(self.config.src_vocab_size, self.config.emb_dim)
-        self.position_embedding = nn.Embedding(self.config.max_pos, self.config.emb_dim)
+        self.position_embedding = nn.Embedding(
+            self.config.max_expected_seq_len, self.config.emb_dim
+        )
 
         self.dec_norm = nn.LayerNorm(self.config.emb_dim, eps=self.config.ln_eps)
 
         if self.config.emb_dropout:
             self.emb_dropout = nn.Dropout(self.config.emb_dropout)
 
         if self.config.p_dropout:
@@ -186,15 +191,15 @@
         is_causal_mask = False
         if mask is None:
             if x is None:
                 raise ValueError("cannot create a mask when x is None")
             # we are caching and can assume all 1s in the mask
             if use_cache and klen != 1 and qlen == 1:
                 # b x h x qlen x kvlen
-                mask = torch.ones(qlen, klen, device=x.device)
+                mask = torch.ones(qlen, klen, dtype=torch.bool, device=x.device)
             else:
                 pad_id: int = self.config.pad_id
                 is_pad: torch.Tensor = x == pad_id
                 mask = is_pad.unsqueeze(-1) == is_pad.unsqueeze(-2)
                 mask = mask.tril(diagonal=0)
 
         x_emb = self.embedding(x)
@@ -271,30 +276,36 @@
         self.head = nn.Linear(
             self.config.emb_dim, self.config.src_vocab_size, bias=False
         )
 
         # this model ties weights, so we tie here
         self.head.weight = self.base_model.embedding.weight
 
-        self.reset_params()
-
     @classmethod
     def from_config(cls, config: GPTBigCodeConfig) -> "GPTBigCode":
         return cls(config)
 
     def get_config(self) -> GPTBigCodeConfig:
         return self.config
 
-    def reset_params(self):
-        # Modules are self-initializing, we're just going to down-scale the final prediction head to be
-        # mixed-fan (inputs and gradients scale to the same inverse factors) if it isn't tied
-        self.head.weight.data.normal_(
-            0,
-            1 / math.sqrt(math.sqrt(self.config.emb_dim * self.config.src_vocab_size)),
-        )
+    def reset_parameters(self):
+        # Do not re-initialize head, as weights are tied
+        for m in self.modules():
+            if (
+                isinstance(m, MultiHeadAttention)
+                or isinstance(m, FeedForwardBlock)
+                or isinstance(m, nn.LayerNorm)
+            ):
+                m.reset_parameters()
+            elif isinstance(m, nn.Embedding):
+                nn.init.normal_(
+                    m.weight,
+                    mean=0.0,
+                    std=self.config.emb_dim**-0.5,
+                )
 
     def forward(
         self,
         x: torch.LongTensor,
         mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
         past_key_value_states: Optional[Tuple[torch.FloatTensor,]] = None,
@@ -312,7 +323,145 @@
 
         preds = self.head(output)
 
         if use_cache:
             return preds, cache
         else:
             return preds
+
+
+# Register common GPT Bigcode variants with the model registration API
+_santacoder_config = GPTBigCodeConfig(
+    src_vocab_size=49280,
+    emb_dim=2048,
+    nheads=16,
+    nlayers=24,
+    pad_id=-1,
+    max_expected_seq_len=2048,
+    p_dropout=0.1,
+    emb_dropout=0.1,
+)
+
+# https://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=models-granite-13b-instruct-v2-model-card
+_13b_config = GPTBigCodeConfig(
+    src_vocab_size=50304,
+    emb_dim=5632,
+    nheads=44,
+    nlayers=40,
+    pad_id=50280,
+    max_expected_seq_len=8192,
+    hidden_grow_factor=4.0,
+    p_dropout=0.1,
+    emb_dropout=0.1,
+    ln_eps=1e-5,
+)
+
+#  Config verified with IBM internal repo
+_20b_config = GPTBigCodeConfig(
+    src_vocab_size=49152,
+    emb_dim=6144,
+    nheads=48,
+    nlayers=52,
+    pad_id=0,
+    max_expected_seq_len=8192,
+    hidden_grow_factor=4.0,
+    p_dropout=0.1,
+    emb_dropout=0.1,
+    ln_eps=1e-5,
+)
+
+_architecture_name = "gpt_bigcode"
+
+
+def _gpt_bigcode_factory_factory(config):
+    def factory(**kwargs):
+        return GPTBigCode(config, **kwargs)
+
+    return factory
+
+
+models.register_model(
+    _architecture_name, "santacoder", _gpt_bigcode_factory_factory(_santacoder_config)
+)
+models.register_model(
+    _architecture_name, "ibm.13b", _gpt_bigcode_factory_factory(_13b_config)
+)
+models.register_model(
+    _architecture_name, "ibm.20b", _gpt_bigcode_factory_factory(_20b_config)
+)
+
+
+def _hf_sd_to_fms_sd(hf_sd: Mapping) -> Mapping:
+    import re
+
+    replacements = [
+        ("lm_head.weight", "head.weight"),
+        (r"^transformer.wte.weight", "base_model.embedding.weight"),
+        (r"^transformer.wpe.weight", "base_model.position_embedding.weight"),
+        (r"^transformer.ln_f", "base_model.dec_norm"),
+        (r"^transformer.h", "base_model.layers"),
+        # need to do kqv manually
+        (r"attn\.c_proj", "attn.dense"),
+        (r"mlp\.c_fc", "ff_sub_layer.w1"),
+        (r"mlp\.c_proj", "ff_sub_layer.w2"),
+        (r"ln_1", "ln"),
+        (r"ln_2", "ff_ln"),
+    ]
+    qkv_weight_pattern = re.compile("transformer.h.[0-9]+.attn.c_attn.weight")
+    qkv_bias_pattern = re.compile("transformer.h.[0-9]+.attn.c_attn.bias")
+
+    new_sd = {}
+    for name, param in hf_sd.items():
+        new_name = name
+        for pattern, repl in replacements:
+            new_name = re.sub(pattern, repl, new_name)
+
+        new_sd[new_name] = param
+
+        # qkv fused
+        if bool(qkv_weight_pattern.match(name)):
+            bias_name = name.replace("weight", "bias")
+            if bias_name not in hf_sd:
+                raise FusableWeightsMissingError([bias_name])
+            new_sd.pop(new_name)
+
+            emb_dim = param.size(1)
+            num_heads = emb_dim // 128
+            num_key_value_heads = (param.size(0) // 128 - num_heads) // 2
+            attn_splits = [
+                (num_heads * 128) // num_key_value_heads,
+                (num_key_value_heads * 128) // num_key_value_heads,
+                (num_key_value_heads * 128) // num_key_value_heads,
+            ]
+
+            prefix = new_name.replace("c_attn.weight", "")
+            q, k, v = param.split(attn_splits, dim=0)
+
+            new_sd[f"{prefix}query.weight"] = q
+            new_sd[f"{prefix}key.weight"] = k
+            new_sd[f"{prefix}value.weight"] = v
+        elif bool(qkv_bias_pattern.match(name)):
+            weight_name = name.replace("bias", "weight")
+            if weight_name not in hf_sd:
+                raise FusableWeightsMissingError([weight_name])
+            new_sd.pop(new_name)
+
+            emb_dim = hf_sd[weight_name].size(1)
+            num_heads = emb_dim // 128
+            num_key_value_heads = (param.size(0) // 128 - num_heads) // 2
+            attn_splits = [
+                (num_heads * 128) // num_key_value_heads,
+                (num_key_value_heads * 128) // num_key_value_heads,
+                (num_key_value_heads * 128) // num_key_value_heads,
+            ]
+
+            prefix = new_name.replace("c_attn.bias", "")
+            q, k, v = param.split(attn_splits, dim=0)
+
+            new_sd[f"{prefix}query.bias"] = q
+            new_sd[f"{prefix}key.bias"] = k
+            new_sd[f"{prefix}value.bias"] = v
+
+    return new_sd
+
+
+serialization.register_adapter(_architecture_name, "hf", _hf_sd_to_fms_sd)
```

## fms/models/llama.py

```diff
@@ -191,14 +191,24 @@
         self.shared = self.distributed_strategy.distribute_module(shared)
 
         self.rot_emb = RotaryEmbedding(
             dim=self.config.emb_dim // self.config.nheads,
             ntk_scaling=self.config.ntk_scaling,
             max_seq_len=self.config.max_expected_seq_len,
         )
+        # RoPE init
+        if isinstance(self.distributed_strategy, UniformModelParallelStrategy):
+            for dev_idx in set(self.distributed_strategy.layer_to_device):
+                self.rot_emb.compute_freqs_cis(
+                    torch.device("cuda", dev_idx), self.config.max_expected_seq_len
+                )
+        else:
+            self.rot_emb.compute_freqs_cis(
+                self.shared.emb.weight.device, self.config.max_expected_seq_len
+            )
 
         layers = []
         for i in range(self.config.nlayers):
             block: nn.Module = LLaMABlock(self.config, self.rot_emb)
             block = self.distributed_strategy.distribute_layer(block, i)
             layers.append(block)
         self.layers = nn.ModuleList(layers)
@@ -214,39 +224,65 @@
         self.dec_norm = self.distributed_strategy.distribute_module(
             dec_norm, final_layers=True
         )
 
         if self.config.p_dropout:
             self.dropout = nn.Dropout(self.config.p_dropout)
 
-        self.reset_params()
-
     def get_config(self) -> LLaMAConfig:
         return self.config
 
     @classmethod
     def from_config(cls, config: LLaMAConfig) -> "LLaMA":
         return cls(config)
 
-    def reset_params(self):
-        # Modules are self-initializing, we're just going to down-scale the final prediction head to be
-        # mixed-fan (inputs and gradients scale to the same inverse factors) if it isn't tied
-        self.shared.head.weight.data.normal_(
-            0, 1 / math.sqrt(math.sqrt(self.width * self.shared.vocab_size))
-        )
-
-        if isinstance(self.distributed_strategy, UniformModelParallelStrategy):
-            for dev_idx in set(self.distributed_strategy.layer_to_device):
-                self.rot_emb.compute_freqs_cis(
-                    torch.device("cuda", dev_idx), self.config.max_expected_seq_len
-                )
-        else:
-            self.rot_emb.compute_freqs_cis(
-                self.shared.emb.weight.device, self.config.max_expected_seq_len
-            )
+    def reset_parameters(self):
+        # Call reset_parameters for relevant sub-layers
+        for m in self.modules():
+            if (
+                isinstance(m, MultiHeadAttention)
+                or isinstance(m, WordEmbedding)
+                or isinstance(m, GatedLinearUnit)
+                or isinstance(m, LayerNormParameterized)
+            ):
+                m.reset_parameters()
+
+    def validate_reset_parameters(self):
+        # Verifies that the above self.reset_parameters() executed correctly.
+        # This may not always be the case for distributed settings with sharded tensors,
+        # such as FSDP or TP. Note that performing this check may require unsharding /
+        # re-materializing the full model on a single rank to access the underlying tensors.
+        tolerance = 1e-3
+
+        def check_close(x):
+            assert x.mean().abs() < tolerance
+            assert x.std().sub(0.02).abs() < tolerance
+
+        with torch.no_grad():
+            for p in self.parameters():
+                assert p.isnan().int().sum() == 0
+                assert p.isinf().int().sum() == 0
+            for m in self.modules():
+                if isinstance(LayerNormParameterized):
+                    if m.elementwise_scale:
+                        assert m.weight.sum() == m.weight.numel()
+                    if m.elementwise_shift:
+                        assert m.bias.add(1).sum() == m.bias.numel()
+                elif isinstance(WordEmbedding):
+                    check_close(m.emb.weight)
+                    check_close(m.head.weight)
+                elif isinstance(GatedLinearUnit):
+                    check_close(m.w1.weight)
+                    check_close(m.w2.weight)
+                    check_close(m.wg.weight)
+                elif isinstance(MultiHeadAttention):
+                    check_close(m.query.weight)
+                    check_close(m.key.weight)
+                    check_close(m.value.weight)
+                    check_close(m.dense.weight)
 
     def _helper(
         self,
         x_in,
         mask=None,
         position_ids=None,
         past_key_value_states=None,
```

## fms/models/roberta.py

```diff
@@ -134,21 +134,25 @@
             nn.LayerNorm(self.config.emb_dim, eps=self.config.norm_eps),
             final_layers=True,
         )
 
         if self.config.p_dropout:
             self.dropout = nn.Dropout(self.config.p_dropout)
 
-    def reset_params(self):
+    def reset_parameters(self):
         for layer in ["embedding", "position_embedding"]:
             nn.init.normal_(
                 getattr(self, layer).weight,
                 mean=0.0,
                 std=self.config.emb_dim**-0.5,
             )
+        for layer in self.layers:
+            for sublayer in ["ln", "ff_ln", "attn", "ff_sub_layer"]:
+                getattr(layer, sublayer).reset_parameters()
+        self.enc_norm.reset_parameters()
 
     def forward(
         self,
         x: torch.Tensor,
         mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.Tensor] = None,
         attn_algorithm: Optional[str] = None,
@@ -250,16 +254,16 @@
     @classmethod
     def from_config(cls, config: RoBERTaConfig) -> "RoBERTa":
         return cls(config)
 
     def get_config(self) -> RoBERTaConfig:
         return self.config
 
-    def reset_params(self):
-        self.base_model.reset_params()
+    def reset_parameters(self):
+        self.base_model.reset_parameters()
         if self.config.tie_heads:
             self.classification_head.head.bias.data.zero_()
         else:
             self.classification_head.head.weight.data.normal_(
                 0,
                 1
                 / math.sqrt(
```

## fms/models/hf/gpt_bigcode/__init__.py

```diff
@@ -5,110 +5,14 @@
 from transformers import GPTBigCodeConfig, GPTBigCodeForCausalLM, PreTrainedModel
 
 from fms.models.hf.gpt_bigcode.modeling_gpt_bigcode_hf import (
     HFAdaptedGPTBigCodeForCausalLM,
 )
 
 
-def get_model(
-    model_name_or_path: Union[str, os.PathLike, PreTrainedModel],
-) -> HFAdaptedGPTBigCodeForCausalLM:
-    """
-    Get a Huggingface adapted FMS model from an equivalent HF model
-
-    Parameters
-    ----------
-    model_name_or_path: Union[str, os.PathLike, PreTrainedModel]
-        Either the name of the model in huggingface hub, the absolute path to
-        the huggingface model, or the huggingface model itself. If the
-        model_name_or_path is a PreTrainedModel, it will not be deleted from
-        memory
-
-    Returns
-    -------
-    HFAdaptedGPTBigCodeForCausalLM
-        A Huggingface adapted FMS implementation of GPT-BigCode
-    """
-    import torch
-    from transformers import AutoModelForCausalLM
-
-    from fms.models.gpt_bigcode import GPTBigCode
-    from fms.models.hf.utils import register_fms_models
-
-    register_fms_models()
-    hf_model_in_memory = isinstance(model_name_or_path, PreTrainedModel)
-    if hf_model_in_memory:
-        hf_model = model_name_or_path
-    else:
-        hf_model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
-
-    model = GPTBigCode(
-        src_vocab_size=hf_model.config.vocab_size,
-        emb_dim=hf_model.config.n_embd,
-        ln_eps=hf_model.config.layer_norm_epsilon,
-        nheads=hf_model.config.n_head,
-        nlayers=hf_model.config.n_layer,
-        hidden_grow_factor=hf_model.config.n_inner / hf_model.config.hidden_size,
-        pad_id=-1,
-        max_pos=hf_model.config.max_position_embeddings,
-    )
-
-    new_hf_sd = __rename_weights_to_fms(hf_model.transformer.state_dict())
-    model.load_state_dict(new_hf_sd, strict=False)
-    with torch.no_grad():
-        for i, layer in enumerate(hf_model.transformer.h):
-            q, k, v = layer.attn.c_attn.weight.split([2048, 128, 128], dim=0)
-            q_bias, k_bias, v_bias = layer.attn.c_attn.bias.split(
-                [2048, 128, 128], dim=0
-            )
-            model.base_model.layers[i].attn.query.weight.copy_(q)
-            model.base_model.layers[i].attn.query.bias.copy_(q_bias)
-            model.base_model.layers[i].attn.key.weight.copy_(k)
-            model.base_model.layers[i].attn.key.bias.copy_(k_bias)
-            model.base_model.layers[i].attn.value.weight.copy_(v)
-            model.base_model.layers[i].attn.value.bias.copy_(v_bias)
-            # if the model was not provided to us, we can assume we don't want
-            # the layers to persist in memory
-            if not hf_model_in_memory:
-                del layer
-        model.head.weight.copy_(hf_model.lm_head.weight)
-    hf_model_fms = HFAdaptedGPTBigCodeForCausalLM.from_fms_model(
-        model=model,
-        bos_token_id=hf_model.config.bos_token_id,
-        eos_token_id=hf_model.config.eos_token_id,
-        pad_token_id=hf_model.config.pad_token_id,
-    )
-    return hf_model_fms
-
-
-def __rename_weights_to_fms(orig_sd):
-    import re
-
-    replacements = [
-        (r"^wte.weight", "base_model.embedding.weight"),
-        (r"^wpe.weight", "base_model.position_embedding.weight"),
-        (r"^ln_f", "base_model.dec_norm"),
-        (r"^h", "base_model.layers"),
-        # need to do kqv manually
-        (r"attn\.c_proj", "attn.dense"),
-        (r"mlp\.c_fc", "ff_sub_layer.w1"),
-        (r"mlp\.c_proj", "ff_sub_layer.w2"),
-        (r"ln_1", "ln"),
-        (r"ln_2", "ff_ln"),
-    ]
-    new_sd = {}
-    for name, param in orig_sd.items():
-        new_name = name
-        for pattern, repl in replacements:
-            new_name = re.sub(pattern, repl, new_name)
-        new_sd[new_name] = param
-
-    return new_sd
-
-
 def convert_to_hf(
     fms_hf_model: HFAdaptedGPTBigCodeForCausalLM,
 ) -> GPTBigCodeForCausalLM:
     """
     Convert an HF-Adapted FMS GPTBigCode model to and HF model
 
     Parameters
@@ -129,15 +33,15 @@
             layer_norm_epsilon=hf_config.ln_eps,
             n_head=hf_config.nheads,
             n_layer=hf_config.nlayers,
             n_inner=int(hf_config.hidden_size * hf_config.hidden_grow_factor),
             pad_token_id=hf_config.pad_token_id,
             bos_token_id=hf_config.bos_token_id,
             eos_token_id=hf_config.eos_token_id,
-            n_positions=hf_config.max_pos,
+            n_positions=hf_config.max_expected_seq_len,
             scale_attention_softmax_in_fp32=False,
         )
     )
 
     with torch.no_grad():
         oss_hf_model.transformer.wte.weight.copy_(
             fms_hf_model.decoder.model.embedding.weight
```

## fms/models/hf/gpt_bigcode/configuration_gpt_bigcode_hf.py

```diff
@@ -22,15 +22,15 @@
         ] = 49157,  # This param default is based on https://huggingface.co/bigcode/gpt_bigcode-santacoder
         emb_dim: Optional[
             int
         ] = 2048,  # This param default is based on https://huggingface.co/bigcode/gpt_bigcode-santacoder
         nheads: int = 12,
         nlayers: int = 12,
         pad_token_id: int = 0,
-        max_pos: int = 512,
+        max_expected_seq_len: int = 512,
         hidden_grow_factor: float = 4.0,
         activation_fn: str = "gelu-tanh",
         p_dropout: float = 0.0,
         emb_dropout: float = 0.0,
         multiquery_attn: bool = True,
         ln_eps: float = 1e-5,
         use_cache: bool = True,
@@ -40,15 +40,15 @@
         **kwargs,
     ):
         self.src_vocab_size = src_vocab_size
         self.emb_dim = emb_dim
         self.nheads = nheads
         self.multiquery_attn = multiquery_attn
         self.nlayers = nlayers
-        self.max_pos = max_pos
+        self.max_expected_seq_len = max_expected_seq_len
         self.hidden_grow_factor = hidden_grow_factor
         self.activation_fn = activation_fn
         self.p_dropout = p_dropout
         self.emb_dropout = emb_dropout
         self.ln_eps = ln_eps
         self.use_cache = use_cache
         super().__init__(
```

## fms/modules/attention.py

```diff
@@ -44,15 +44,14 @@
         emb_kq,
         emb_v,
         nheads,
         kvheads,
         p_dropout=None,
         use_bias=False,
         position_encoder: Optional[PositionEncoder] = None,
-        gain=1,
     ):
         super(MultiHeadAttention, self).__init__()
         self.nheads = nheads
         self.kvheads = kvheads
         self.emb_dim = emb_dim
         self.emb_kq_per_head = emb_kq
         self.emb_v_per_head = emb_v
@@ -75,34 +74,21 @@
         self.position_encoder = position_encoder
         # Avoiding graph breaks
         self.previous_flash: bool = torch.backends.cuda.flash_sdp_enabled()
         self.previous_mem_efficient: bool = (
             torch.backends.cuda.mem_efficient_sdp_enabled()
         )
         self.previous_math: bool = torch.backends.cuda.math_sdp_enabled()
-        self.reset_params(gain)
 
-    def reset_params(self, gain=1):
-        # Ensure softmax inputs are standard normal
-        for layer in ["query", "key"]:
-            nn.init.trunc_normal_(
-                getattr(self, layer).weight, mean=0.0, std=self.emb_dim**-0.5
-            )
-        # Ensure projection layers have same scale (for normalized-step dataloaders like
-        # AdamW / Sophia), and maintain input norm up to attention remix, in expectation
-        for layer in ["value", "dense"]:
-            nn.init.trunc_normal_(
-                getattr(self, layer).weight,
-                mean=0.0,
-                std=(gain / (self.emb_dim * self.nheads * self.emb_v_per_head) ** 0.5)
-                ** 0.5,
-            )  # Using explicit terms instead of numel to account for eventual MQA addition
-        if self.use_bias:
-            for layer in ["query", "key", "value", "dense"]:
-                getattr(self, layer).bias.data.zero_()
+    def reset_parameters(self):
+        for m in self.modules():
+            if isinstance(m, nn.Linear):
+                nn.init.trunc_normal_(m.weight, mean=0.0, std=0.02)
+                if self.use_bias:
+                    m.bias.data.zero_()
 
     def forward(
         self,
         q,
         k,
         v,
         mask: Optional[Tensor] = None,
@@ -138,40 +124,41 @@
         kv_len = k.size(1)
 
         # split emb_dim as nheads*emb_dim_per_head
         # b x h x qlen x ds
         queries = self.query(q).view(
             batch_size, q_len, self.nheads, self.emb_kq_per_head
         )
-        queries = queries.transpose(2, 1)  # / (self.emb_kq_per_head**(1/4))
 
         # if this is self attention, we always recompute
         # cross attention only gets computed when a cache does not exist
         # if we dont have the cache yet, we need to compute
         # d x (h x ds)
         # b x kvlen x d
         # b x kvlen x h x ds
         # b x h x kvlen x ds
         if is_self or past_key_value_state is None:
             keys = self.key(k).view(
                 batch_size, kv_len, self.kvheads, self.emb_kq_per_head
             )
-            keys = keys.transpose(2, 1)  # / (self.emb_kq_per_head**(1/4))
 
             values = self.value(v).view(
                 batch_size, kv_len, self.kvheads, self.emb_v_per_head
             )
-            values = values.transpose(2, 1)  # compatible with QK.T
 
             # You want to apply rotary embeddings pre-cache
             if self.position_encoder is not None:
                 queries, keys = self.position_encoder.adjusted_qk(
                     queries, keys, position_ids, past_key_value_state, use_cache
                 )
 
+        queries = queries.transpose(2, 1)  # / (self.emb_kq_per_head**(1/4))
+        keys = keys.transpose(2, 1)  # / (self.emb_kq_per_head**(1/4))
+        values = values.transpose(2, 1)  # compatible with QK.T
+
         # if you want to use caching and past_key_value_state is not None meaning you have values in your cache
         if use_cache and past_key_value_state is not None:
             if is_self:
                 keys = torch.cat((past_key_value_state[0], keys), dim=2)
                 values = torch.cat((past_key_value_state[1], values), dim=2)
             else:
                 keys = past_key_value_state[0]
@@ -266,15 +253,14 @@
         emb_kq,
         emb_v,
         nheads,
         kvheads,
         p_dropout=None,
         use_bias=False,
         position_encoder: Optional[PositionEncoder] = None,
-        gain=1,
         group: Optional[ProcessGroup] = None,
     ):
         assert torch.distributed.is_initialized()
 
         rank, world_size = distributed.rank_and_world(group)
         assert (
             nheads % world_size == 0
@@ -285,21 +271,21 @@
             emb_kq,
             emb_v,
             nheads // world_size,
             (kvheads // world_size) if kvheads > 1 else kvheads,
             p_dropout,
             use_bias,
             position_encoder,
-            gain,
         )
+        self.pre_tp_kvheads = kvheads
         self.setup_tp(rank, world_size)
 
     def colwise_param_names(self) -> List[str]:
         colwise_weights = ["query"]
-        if self.kvheads != 1:
+        if self.pre_tp_kvheads != 1:
             colwise_weights.append("key")
             colwise_weights.append("value")
         return colwise_weights
 
     def rowwise_param_names(self) -> List[str]:
         return ["dense"]
```

## fms/modules/embedding.py

```diff
@@ -79,27 +79,24 @@
         if abs_pos:
             self.pos_emb = nn.Embedding(max_pos, self.emb_dim)
             self.register_buffer("pos_id", torch.arange(max_pos).unsqueeze(0))
         if reversible:
             self.head = nn.Linear(self.emb_dim, self.vocab_size, bias=bias)
             if tie_weights:
                 self.head.weight = self.emb.weight
-        self.reset_params()
 
-    def reset_params(self):
+    def reset_parameters(self):
         # Defaults to norm-preserving in reverse op, unit vector in forward op
         layers = ["emb"]
         if self.abs_pos:
             layers.append("pos_emb")
         if self.reversible and not self.tie_weights:
             layers.append("head")
         for layer in layers:
-            nn.init.trunc_normal_(
-                getattr(self, layer).weight, mean=0.0, std=self.emb_dim**-0.5
-            )
+            nn.init.trunc_normal_(getattr(self, layer).weight, mean=0.0, std=0.02)
         if self.reversible and self.bias:
             self.head.bias.data.zero_()
         # Preserve pad index dummy-hood
         if self.padding_idx is not None:
             self.emb.weight.data[self.padding_idx].zero_()
 
     def forward(self, inp, reverse=False):
```

## fms/modules/feedforward.py

```diff
@@ -38,41 +38,35 @@
         self,
         emb_dim,
         hidden_grow_factor=4.0,
         multiple_of=None,
         activation_fn=nn.ReLU(),
         p_dropout=0.1,
         use_bias=True,
-        gain=1,
     ):
         super(FeedForwardBlock, self).__init__()
         self.hidden_dim = int(hidden_grow_factor * emb_dim)
         if multiple_of:
             self.hidden_dim = multiple_of * (
                 (self.hidden_dim + multiple_of - 1) // multiple_of
             )
         self.w1 = nn.Linear(emb_dim, self.hidden_dim, bias=use_bias)
         self.a = activation_fn
         self.p_dropout = p_dropout
         if p_dropout:
             self.d = nn.Dropout(p_dropout)
         self.w2 = nn.Linear(self.hidden_dim, emb_dim, bias=use_bias)
         self.use_bias = use_bias
-        self.reset_params(gain=gain)
 
-    def reset_params(self, gain=1):
-        # Fulfills following constraints in expectation:
-        #  - Norm of w1 and w2 are equal (for step-normalizing optimizers like AdamW / Sophia)
-        #  - Norm of output equals norm of input times gamma
-        # when activation is relu-like
+    def reset_parameters(self):
         for layer in ["w1", "w2"]:
             nn.init.trunc_normal_(
                 getattr(self, layer).weight,
                 mean=0.0,
-                std=(2**0.5 * gain / self.w1.weight.numel() ** 0.5) ** 0.5,
+                std=0.02,
             )
             if self.use_bias:
                 getattr(self, layer).bias.data.zero_()
 
     def forward(self, x):
         out = self.a(self.w1(x))
         if self.p_dropout:
@@ -99,15 +93,14 @@
         self,
         emb_dim,
         hidden_grow_factor: float = 4,
         multiple_of=None,
         activation_fn=nn.ReLU(),
         p_dropout=0.1,
         use_bias=True,
-        gain=1,
         group: Optional[ProcessGroup] = None,
     ):
         assert torch.distributed.is_initialized()
         hidden_dim = int(hidden_grow_factor * emb_dim)
         if multiple_of:
             hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)
         rank, world_size = distributed.rank_and_world(group)
@@ -118,15 +111,14 @@
             self,
             emb_dim,
             hidden_grow_factor / world_size,
             multiple_of,
             activation_fn,
             p_dropout,
             use_bias,
-            gain,
         )
         self.setup_tp(rank, world_size)
 
     def colwise_param_names(self) -> List[str]:
         return ["w1"]
 
     def rowwise_param_names(self) -> List[str]:
@@ -179,15 +171,14 @@
         self,
         emb_dim,
         hidden_grow_factor: float = 4,
         multiple_of=None,
         activation_fn=nn.ReLU(),
         p_dropout=0.1,
         use_bias=True,
-        gain=1,
     ):
         super(GatedLinearUnit, self).__init__()
         self.hidden_dim = int(hidden_grow_factor * emb_dim)
         if multiple_of:
             self.hidden_dim = multiple_of * (
                 (self.hidden_dim + multiple_of - 1) // multiple_of
             )
@@ -197,26 +188,21 @@
         self.p_dropout = p_dropout
         if p_dropout:
             self.d = nn.Dropout(p_dropout)
         self.w2 = nn.Linear(self.hidden_dim, emb_dim, bias=use_bias)
         self.use_bias = use_bias
         self.width = emb_dim
         self.grow_factor = hidden_grow_factor
-        self.reset_params(gain=gain)
 
-    def reset_params(self, gain=1):
-        # Fulfills following constraints in expectation:
-        #  - Norm of w1, wg and w2 are equal (for step-normalizing optimizers like AdamW / Sophia)
-        #  - Norm of output equals norm of input times gamma
-        # when activation is relu-like and input is standard normal
+    def reset_parameters(self):
         for layer in ["w1", "w2", "wg"]:
             nn.init.trunc_normal_(
                 getattr(self, layer).weight,
                 mean=0.0,
-                std=(2 * gain**2 / self.grow_factor) ** (1 / 6) / self.width**0.5,
+                std=0.02,
             )
             if self.use_bias:
                 getattr(self, layer).bias.data.zero_()
 
     def forward(self, x):
         out = self.a(self.wg(x)) * self.w1(x)
         if self.p_dropout:
@@ -244,15 +230,14 @@
         self,
         emb_dim,
         hidden_grow_factor: float = 4,
         multiple_of=None,
         activation_fn=nn.ReLU(),
         p_dropout=0.1,
         use_bias=True,
-        gain=1,
         group: Optional[ProcessGroup] = None,
     ):
         assert torch.distributed.is_initialized()
         rank, world_size = distributed.rank_and_world(group)
 
         hidden_dim = int(hidden_grow_factor * emb_dim)
         if multiple_of:
@@ -264,15 +249,14 @@
             self,
             emb_dim,
             hidden_grow_factor / world_size,
             multiple_of,
             activation_fn,
             p_dropout,
             use_bias,
-            gain,
         )
         self.setup_tp(rank, world_size)
 
     def colwise_param_names(self) -> List[str]:
         return ["w1", "wg"]
 
     def rowwise_param_names(self) -> List[str]:
```

## fms/modules/layernorm.py

```diff
@@ -45,17 +45,16 @@
             self.weight = nn.Parameter(torch.empty(self.normalized_shape))
         # else:
         #     self.register_parameter("weight", None)
         if self.elementwise_shift:
             self.bias = nn.Parameter(torch.empty(self.normalized_shape))
         # else:
         #     self.register_parameter("bias", None)
-        self.reset_params()
 
-    def reset_params(self):
+    def reset_parameters(self):
         if self.elementwise_scale:
             self.weight.data.fill_(1)
         if self.elementwise_shift:
             self.bias.data.zero_()
 
     def forward(self, x):
         if self.use_mean:
```

## fms/modules/positions.py

```diff
@@ -218,45 +218,51 @@
         past_kv_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
         use_cache=False,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         """
         Args
         ----
         q : torch.Tensor
-            Embedded query tensor, expected size is B x H x S x Eh
+            Embedded query tensor, expected size is B x S x H x Eh
         k : torch.Tensor
-            Embedded query tensor, expected size is B x H x S x Eh
+            Embedded query tensor, expected size is B x S x H x Eh
         position_ids : Optional[torch.LongTensor]
             The position of each of the tokens encoded in q and k. This is important in
             kv-caching and left-padding situations, for which the rotation to be applied might
             not always be the pre-cached position 0...S. For kv-caching without dynamic batching
             or variable per-row left padding position_ids is shared for all the batch.
         """
         assert len(q.size()) == 4
         assert len(k.size()) == 4
 
-        seq_len = max(k.size(2), q.size(2))
+        seq_len = max(k.size(1), q.size(1))
         if position_ids is None:
             # Compute position_ids based on cache config
             position_ids = torch.arange(
                 0, seq_len, dtype=torch.long, device=q.device
             ).repeat(k.size(0), 1)
             if use_cache and past_kv_state is not None:
                 position_ids += past_kv_state[0].size(2)
 
-        q_ = q.float().reshape(*q.size()[:-1], -1, 2)  # B H L D/2 2
-        k_ = k.float().reshape(*k.size()[:-1], -1, 2)  # B H L D/2 2
+        q_ = q.float().view(*q.size()[:-1], -1, 2)  # B L H D/2 2
+        k_ = k.float().view(*k.size()[:-1], -1, 2)  # B L H D/2 2
 
         # the max start position should be based on the max first position of each sequence
         max_start_pos = torch.max(position_ids[:, 0])
         alpha = self.compute_freqs_cis(q.device, max_start_pos + seq_len)
-        freqs = self.cached_freqs[q.device.index][alpha][position_ids].unsqueeze(1)
+        freqs = self.cached_freqs[q.device.index][alpha][position_ids]
 
-        freqs = freqs.float()  # 1 1 L D/2 2 2
+        freqs = freqs.float()  # 1 L D/2 2 2
         q_out = (
-            freqs[:, :, -q.size(2) :, :, :, :].mul(q_.unsqueeze(-2)).sum(5).flatten(3)
-        )
+            freqs[:, -q.size(1) :, None, :, :, :]
+            .mul(q_.unsqueeze(-2))
+            .sum(5)
+            .flatten(3)
+        ).type_as(q)
         k_out = (
-            freqs[:, :, -k.size(2) :, :, :, :].mul(k_.unsqueeze(-2)).sum(5).flatten(3)
-        )
+            freqs[:, -k.size(1) :, None, :, :, :]
+            .mul(k_.unsqueeze(-2))
+            .sum(5)
+            .flatten(3)
+        ).type_as(k)
 
-        return q_out.type_as(q).contiguous(), k_out.type_as(k).contiguous()
+        return q_out.view_as(q), k_out.view_as(k)
```

## fms/training/plugins.py

```diff
@@ -3,16 +3,20 @@
 from datetime import datetime
 from pathlib import Path
 from typing import Dict, List, Optional
 
 import torch
 from torch import distributed as dist
 from torch import nn
+from torch.distributed.fsdp import FullStateDictConfig
+from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
+from torch.distributed.fsdp import StateDictType
 
 from fms import utils
+from fms.datasets.util import SavableDataset
 from fms.utils import generation, print0
 
 
 class TrainerPlugin:
     """
     A TrainerPlugin runs once every epoch, and possibly every `steps` steps.
     It is passed relevant objects that can be used for checkpointing, logging,
@@ -29,16 +33,16 @@
         # if step is None, we're at an epoch end not an intermediate step.
         # By default we always run for epoch ends.
         if step is None:
             return True
         # If self.steps is None, we're only recording epoch ends and this isn't one.
         if self.steps is None:
             return False
-        # record every `step` steps
-        if (step + 1) % self.steps == 0:
+        # record every `step` steps, starting from step `step`
+        if step != 0 and (step + 1) % self.steps == 0:
             return True
         return False
 
     @abstractmethod
     def step(
         self,
         model: nn.Module,
@@ -114,22 +118,31 @@
     seconds by calling `writer` with the log message. A custom writer
     should accept `*args` similar to `print`.
     """
 
     # TODO: add optional validation dataloader and validation loss.
     # TODO: add `writer` functions that handles logging metrics to experiment
     # tracking tools such as aimstack/wandb/neptune (or add alternate plugin?)
-    def __init__(self, seconds=10, writer=print0):
+    def __init__(
+        self,
+        seconds=10,
+        group: Optional[dist.ProcessGroup] = None,
+        prev_step: int = -1,
+        device="cpu",
+        writer=print0,
+    ):
         super().__init__(1)
         self.seconds = seconds
         self.last_reported_time = datetime.now()
-        self.tokens_seen = 0
-        self.last_reported_step = -1
-        self.cum_loss = 0
+        self.tokens_seen = torch.tensor(0.0, device=device)
+        self.last_reported_step = prev_step
+        self.cum_loss = torch.tensor(0.0, device=device)
+        self.time_per_step = torch.tensor(1.0, device=device)
         self.last_step = -1
+        self.group = group
         self.writer = writer
 
     def step(
         self,
         model: nn.Module,
         optimizer,
         epoch: int,
@@ -147,97 +160,152 @@
         if step is None:
             step = self.last_step + 1
             self.last_step = 0
             steps = step - self.last_reported_step
             self.last_reported_step = 0
         else:
             self.last_step = step
-            if elapsed < self.seconds or step == self.last_reported_step:
+            if step == self.last_reported_step:
                 return
+            steps_taken = step - self.last_reported_step
+            if steps_taken * self.time_per_step < self.seconds:
+                return
+            time_per_step = elapsed / (step - self.last_reported_step)
+            self.time_per_step.fill_(time_per_step)
             steps = step - self.last_reported_step
             self.last_reported_step = step
 
-        # TODO: aggregate these per-rank statistics when training with
-        # distributed. e.g.: dist.all_reduce(loss, op=dist.ReduceOp.SUM)
-        # for loss, tokens_seen, etc.
+        world = 1 if self.group is None else self.group.size()
+        if world > 1 and self.tokens_seen.device.type == "cuda":
+            dist.all_reduce(self.tokens_seen, op=dist.ReduceOp.SUM)
+            dist.all_reduce(self.cum_loss, op=dist.ReduceOp.SUM)
+            dist.all_reduce(self.time_per_step, op=dist.ReduceOp.SUM)
+            self.time_per_step /= world
+
         to_report = {}
         if "loss" in metrics:
             to_report["loss"] = f"{metrics['loss']:.4f}"
-        to_report["avg_loss"] = f"{self.cum_loss / steps:.4f}"
+        to_report["avg_loss"] = f"{self.cum_loss.item() / steps / world:.4f}"
 
         more_metrics = {
-            "tok/stp": f"{self.tokens_seen / steps:,.1f}",
-            "tok/s": f"{self.tokens_seen / elapsed:,.1f}",
+            "tok/stp": f"{self.tokens_seen.item() / steps:,.1f}",
+            "s/stp": f"{self.time_per_step.item():,.3f}",
+            "tok/gpu/s": f"{self.tokens_seen.item() / elapsed / world:,.1f}",
         }
         if torch.cuda.is_available() and utils.has_package("pynvml"):
             nvidia_metrics = {
                 "gpu_mem_use": f"{torch.cuda.memory_usage()}%",
                 "gpu_utzn": f"{torch.cuda.utilization()}%",
             }
             more_metrics.update(nvidia_metrics)
 
         self.last_reported_time = current_time
-        self.tokens_seen = 0
-        self.cum_loss = 0
+
+        self.tokens_seen.fill_(0)
+        self.cum_loss.fill_(0)
+
         to_report.update(more_metrics)
         self.writer(epoch, step, current_time, to_report)
 
 
 class Checkpointer(TrainerPlugin):
     """
     A training plugin to write checkpoints.
     TODO: This will require changes to handle distributed checkpoints.
+
+    Args:
+
+    group: The group to checkpoint. i.e. if using hsdp, you would want to pass
+            a subgroup for a single hsdp shard group.
+    name: included in the file path to differentiate this particular checkpoint.
+    save_dir: the base directory into which to save checkpoints.
+    dataset: if set, save the state_dict of this dataset.
+    steps: save a checkpoint every `steps` steps.
     """
 
     def __init__(
         self,
         save_dir: str | Path = Path("./checkpoints"),
+        dataset: Optional[SavableDataset] = None,
         steps: Optional[int] = None,
         name: Optional[str] = None,
         group: Optional[dist.ProcessGroup] = None,
     ):
         super().__init__(steps)
         os.makedirs(save_dir, exist_ok=True)
         save_dir = os.path.expanduser(save_dir)
+        self.dataset = dataset
         self.save_dir = Path(save_dir)
         self.group = group
         self.name = name
 
     # TODO: this probably also needs to accept a dataset since we want to
     # support checkpointable datasets.
-    # TODO: end of epoch checkpoints should probably consolidate to one
-    # rank when using FSDP.
     def step(
         self,
         model: nn.Module,
         optimizer,
         epoch: int,
         metrics: Dict = {},
         step: Optional[int] = None,
     ):
         if not self.run(step):
             return
 
         model_name = (
             model.__class__.__name__.lower() if self.name is None else self.name
         )
-        model_dict = model.state_dict()
+
+        # For FSDP, consolidate checkpointable data to rank0.
+        # TODO: may also want to support distcp checkpoints which should be faster
+        # to save and load but are harder to use for inference.
+
+        is_fsdp = isinstance(model, FSDP)
+        # For HSDP, self.group is only set for the first shard group. We only
+        # need to save checkpoints for one shard group.
+        if is_fsdp and self.group is None:
+            return
+
+        if is_fsdp:
+            dict_type = StateDictType.FULL_STATE_DICT
+            cfg = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
+            with dist.fsdp.FullyShardedDataParallel.state_dict_type(
+                model, dict_type, cfg
+            ):
+                print0("Aggregating FSDP model statedict")
+                model_dict = model.state_dict()
+                print0("Aggregating optim statedict")
+                optim_dict = FSDP.optim_state_dict(model, optimizer, group=self.group)
+        else:
+            model_dict = model.state_dict()
+            optim_dict = optimizer.state_dict()
+
         if step is not None:
             file = f"{model_name}_{epoch:03d}_{step+1:05d}"
         else:
             file = f"{model_name}_{epoch:03d}_final"
         save_dir = self.save_dir
 
         if self.group is None:
             path = save_dir / f"{file}.pth"
-            train_file = f"{file}.train"
+            train_file = save_dir / f"{file}.train"
         else:
             path = save_dir / file
             os.makedirs(path, exist_ok=True)
-            train_file = f"rank_{self.group.rank():02d}.train"
+            train_file = path / f"rank_{self.group.rank():02d}.train"
             path = path / f"rank_{self.group.rank():02d}.pth"
-        print0("Writing checkpoint", path)
-        torch.save(model_dict, path)
 
-        optim_dict = optimizer.state_dict()
-        train_dict = {"optimizer": optim_dict, "epoch": epoch}
-        torch.save(train_dict, train_file)
+        print0("Writing model checkpoint", path)
+        if is_fsdp:
+            if self.group is not None and self.group.rank() == 0:
+                torch.save(model_dict, path)
+        else:
+            torch.save(model_dict, path)
+
+        print0("Writing training state", train_file)
+        train_dict = {"optimizer": optim_dict, "epoch": epoch, "step": step}
+        if self.dataset is not None:
+            dataset_sd = self.dataset.state_dict()
+            train_dict |= {"dataset": dataset_sd}
+
+        if self.group is None or self.group.rank() == 0:
+            torch.save(train_dict, train_file)
```

## fms/training/trainer.py

```diff
@@ -23,15 +23,15 @@
         output = model(input)
         loss = loss_fn(output, label)
 
     if grad_scaler is not None:
         grad_scaler.scale(loss).backward()
     else:
         loss.backward()
-    return loss.item()
+    return loss
 
 
 def __optimize(model, optimizer, grad_scaler):
     if grad_scaler is not None:
         grad_scaler.unscale_(optimizer)
         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
         grad_scaler.step(optimizer)
@@ -42,64 +42,79 @@
     optimizer.zero_grad()
 
 
 def __one_epoch(
     model: nn.Module,
     optimizer: Optimizer,
     data: DataLoader,
+    device,
     loss_fn,
     epoch: int,
+    prev_step: int,
     plugins: List[TrainerPlugin],
     accum_iters: int = 1,
 ):
     print0("Epoch", epoch)
     model.train()
 
     grad_scaler = None
     # grad_scaler = torch.cuda.amp.GradScaler()
 
     if data.sampler is not None and isinstance(data.sampler, DistributedSampler):
         data.sampler.set_epoch(epoch)
 
     optimized = False
     optimizer.zero_grad()
+
     for step, (input, label) in enumerate(data):
+        step = prev_step + step + 1
+
+        batch_size = input.shape[0]
+        input_length = input.shape[1]
+
+        input = input.to(device)
+        label = label.to(device)
+
         loss = __one_step(model, input, label, loss_fn, grad_scaler)
         if (step + 1) % accum_iters == 0:
             __optimize(model, optimizer, grad_scaler)
             optimized = True
         else:
             optimized = False
 
         metrics = {
             "loss": loss,
-            "batch_size": input.shape[0],
-            "input_length": input.shape[1],
+            "batch_size": batch_size,
+            "input_length": input_length,
         }
         for plugin in plugins:
             plugin.step(model, optimizer, epoch, metrics, step)
     if not optimized:
         __optimize(model, optimizer, grad_scaler)
     for plugin in plugins:
         plugin.step(model, optimizer, epoch)
 
 
 def train(
     model,
     optimizer,
     dataloader: DataLoader,
+    device,
     loss_fn: nn.Module,
     start_epoch=0,
     epochs: int = 1,
+    prev_step: int = -1,
     trainer_plugins: List[TrainerPlugin] = [],
     grad_accum_iters: int = 1,
 ):
     for epoch in range(start_epoch, start_epoch + epochs):
         __one_epoch(
             model,
             optimizer,
             dataloader,
+            device,
             loss_fn,
             epoch,
+            prev_step,
             trainer_plugins,
             accum_iters=grad_accum_iters,
         )
```

## fms/utils/__init__.py

```diff
@@ -8,15 +8,15 @@
     """
     Print *args to stdout on rank 0 of the default process group, or an
     optionally specified process group.
     """
     if group is not None:
         rank = group.rank()
     else:
-        rank = int(os.environ.get("LOCAL_RANK", os.environ.get("RANK", 0)))
+        rank = int(os.environ.get("RANK", os.environ.get("LOCAL_RANK", 0)))
     if rank == 0:
         print(*args)
 
 
 def has_package(name):
     """
     Checks if a package is installed and available.
```

## fms/utils/serialization.py

```diff
@@ -195,14 +195,22 @@
     if checkpoint_sharding is not None and checkpoint_sharding != "layer":
         assert world_size == len(
             checkpoints
         ), f"Loading a {checkpoint_sharding}-sharded checkpoint with len={len(checkpoints)} but world size is {world_size}"
 
         checkpoints = [checkpoints[rank]]
 
+    # if there's only one checkpoint for fsdp/hsdp, load it only into rank zero
+    # and it will be distributed by the FSDP `sync_module_states` parameter
+    if checkpoint_sharding is None and distributed_strategy in {"hsdp", "fsdp"}:
+        if rank == 0:
+            checkpoints = [checkpoints[0]]
+        else:
+            return {}
+
     checkpoint_sds = []
     if checkpoints[0].suffix == ".safetensors":
         for ckp in checkpoints:
             checkpoint_sds.append(
                 _load_safetensors_state_dict(
                     ckp,
                     initial_device,
```

## fms/utils/tp_wrapping.py

```diff
@@ -11,15 +11,17 @@
 )
 from fms.modules.positions import Alibi
 
 
 # this probably belongs somewhere else but can't go in fms.distribtued b/c
 # circular dependency.
 def _tp_wrapped(module: nn.Module, group: ProcessGroup):
-    if isinstance(module, FeedForwardBlock):
+    if hasattr(module, "to_tp"):
+        return module.to_tp(group)
+    elif isinstance(module, FeedForwardBlock):
         return TPFeedForwardBlock.import_module(module, group)
     elif isinstance(module, GatedLinearUnit):
         return TPGatedLinearUnit.import_module(module, group)
     elif isinstance(module, MultiHeadAttention):
         return TPMultiHeadAttention.import_module(module, group)
     elif isinstance(module, Alibi):
         raise NotImplementedError("TODO: implement TP for Alibi")
```

## Comparing `ibm_fms-0.0.3.dist-info/LICENSE` & `ibm_fms-0.0.4.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `ibm_fms-0.0.3.dist-info/METADATA` & `ibm_fms-0.0.4.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: ibm-fms
-Version: 0.0.3
+Version: 0.0.4
 Summary: IBM Foundation Model Stack
 Home-page: https://github.com/foundation-model-stack/foundation-model-stack
 Author: Brian Vaughan, Joshua Rosenkranz, Antoni Viros i Martin, Davis Wertheimer, Supriyo Chakraborty, Raghu Kiran Ganti
 Author-email: bvaughan@ibm.com, jmrosenk@us.ibm.com, aviros@ibm.com, Davis.Wertheimer@ibm.com, supriyo@us.ibm.com, rganti@us.ibm.com
 License: Apache License 2.0
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: Apache Software License
```

## Comparing `ibm_fms-0.0.3.dist-info/RECORD` & `ibm_fms-0.0.4.dist-info/RECORD`

 * *Files 15% similar despite different names*

```diff
@@ -1,55 +1,56 @@
 fms/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-fms/datasets/__init__.py,sha256=KDct8ox_NQFuCuTVuR4Yr92SeziVCKRAPYskzd2LHew,5966
-fms/datasets/arrow.py,sha256=Jf5wGSPbatO1THwyoVeWpjTysqZwIdqZP8EkU4lhOo8,5328
-fms/datasets/instructions.py,sha256=RKvhFFlBu4z0ZUVi5QMxgS2mA94j8agGx7ulLPU1QW0,3126
-fms/datasets/text.py,sha256=QMwxpRNqS-himvKPqC3rJsRKgvgFT9rUZR4xH6n7K-4,3181
-fms/distributed/__init__.py,sha256=eRa5nI1LOlGldXCkvXzYyDkikDxMsJV5_qIYnktEEvU,488
+fms/datasets/__init__.py,sha256=R8JlXkBSNJ6Nt1M5gsTOf2o5H8iBQu_2gEUiq04B7sw,1786
+fms/datasets/arrow.py,sha256=BCm5Dv-MjJ04dPGAR2CMWvMbtO56_rMmpN1Su3_R35A,5384
+fms/datasets/instructions.py,sha256=Ltnd-rWPrwCLwTDfnrbUoh6e5MGeB26Oclsl4nrh8Bk,3055
+fms/datasets/text.py,sha256=eoXfUzFw8jCz23YhjLZNEHMFEqJREZcuD0wkAkQHnqY,2968
+fms/datasets/util.py,sha256=ehat5sRTqwuLQB3DFD7L5o3Y720x21O1jehN_Sjlqlc,5159
+fms/distributed/__init__.py,sha256=uKysiqWCseHUck_fql2ceCLVpOINfg8TANJVIO3HEcI,590
 fms/distributed/strategy.py,sha256=77atjbmP-3zpKo10tc_SezY0qg5xCa8SVZFldcH9ol0,4006
-fms/distributed/tensorparallel.py,sha256=Dg-txtUMX5jRIsbNqWQMWB4Tt7c4NQinf05nFeRka1w,8520
-fms/models/__init__.py,sha256=aRcGJIpXYG7-fVHpi4AEERgL35DVxg4mfUgiBAZ8gBI,10295
-fms/models/gpt_bigcode.py,sha256=QK-LXYz9iLJ49jIZjK5JOgOwtaTk4HAlZ4aWhgleft4,10277
-fms/models/llama.py,sha256=xn7pjVUUx0qcQWS6frAZ-XVhFOKB3EN0hFtMrh4Rfio,16742
-fms/models/roberta.py,sha256=dt47ZoKc_3m-_jRQChqjymXngOLx-6v6VS-wEIlE9OA,10467
+fms/distributed/tensorparallel.py,sha256=paxYexSysK5cgaajFt_pzlHqA1UyVSXTOaPAzwq7Avs,7423
+fms/models/__init__.py,sha256=d73XqdD0p4znNv-gjQEqm5MLJW56DDsUmOoh6YFK1VU,10549
+fms/models/gpt_bigcode.py,sha256=75-zb0hguHIMiQ_KHUE7U27WfpoQrYazJicsI0bg7UY,14866
+fms/models/llama.py,sha256=pDaOgU8495YRMiJ3SpuHpruiGpto8Aw-PsVhAs9f6pM,18367
+fms/models/roberta.py,sha256=V0j5dLc7CYYbgREbsYi8SL8bfsurnI_gyANqG4ReTwo,10683
 fms/models/hf/__init__.py,sha256=JkuqV77T_K4pn6KSoaBbsVTyGo0r_XyZ_UE62wP6NMc,1426
 fms/models/hf/lm_head_mixins.py,sha256=zW_qansMMm9pGKM3IEKzmxXJfR_6Q4llK5wMAa_Z1Rw,14675
 fms/models/hf/modeling_hf_adapter.py,sha256=JE_IN79AQniQxgW92rP9WKtW8oumSHh5LmaKnagpXn0,84324
 fms/models/hf/utils.py,sha256=lL-gOGbBMpefTAOPUBISn-n3jvRt-cGYCMMsR2-ajWM,4775
-fms/models/hf/gpt_bigcode/__init__.py,sha256=Gigbdbm5c18oD-b9rJOWG32LhCtrq__XCXJioB2fgw8,7037
-fms/models/hf/gpt_bigcode/configuration_gpt_bigcode_hf.py,sha256=SKwoyC8OkT6X9z09kBlnp8xMb-nrTm2Enb3tT7eKzu8,2620
+fms/models/hf/gpt_bigcode/__init__.py,sha256=jKiTvB6iC7UXEHafLea9nw5Cle7rmuOr2Gdjd4FMoZc,3488
+fms/models/hf/gpt_bigcode/configuration_gpt_bigcode_hf.py,sha256=v8dhpmpUT69iOW_CJPiW3DapnRgYTST_Hr77rFC9ZoM,2659
 fms/models/hf/gpt_bigcode/modeling_gpt_bigcode_hf.py,sha256=cy6oEyc2kBbahjo3s1B1RK5RyrMgOn78dkbmeMfzG5Y,3420
 fms/models/hf/llama/__init__.py,sha256=cYzKdD7_H1_DGagC-Ylw2RBLc8LMQvnP7T7cF5zBZl8,1498
 fms/models/hf/llama/configuration_llama_hf.py,sha256=iwDCXWIcjE3ChfmhcvN36s6bfPCo9ls4JP-fGjHmEws,2509
 fms/models/hf/llama/modeling_llama_hf.py,sha256=dVy461_TUGNgx3mChvj5egrVa866MHqLW-Rm41L-hzY,4822
 fms/models/hf/roberta/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 fms/models/hf/roberta/modeling_roberta_hf.py,sha256=AkVycY88b5KfFVSucup4Ixft-Xm77ztSY3kYoNhFBY0,6187
 fms/modules/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-fms/modules/attention.py,sha256=Mescl4EwFhZ2ShArwE-D_OlWpatzO0Sq-5KYM9yHumU,12949
-fms/modules/embedding.py,sha256=TJUUP6dgBrIwneWF9ktG862wGHKE5nyAkSvpdNd0PtU,8374
-fms/modules/feedforward.py,sha256=yZYuYPRnJSHd8_i0luG019itZGgbioDEXeNOqxAIgoE,9770
+fms/modules/attention.py,sha256=6AmKrFWuWom6kq0X68vw5cBlirIagRdNbbrCv7wpo3A,12229
+fms/modules/embedding.py,sha256=Ab7WG1CByrhFCopabUAtVufcok3wnLWbGzDPZEwYYm4,8306
+fms/modules/feedforward.py,sha256=qJ0F5mSMIcpdGaTIS7r4oWUNZbIrnclkCIExvgNiDZo,8944
 fms/modules/head.py,sha256=l5H6hAtX4Dp9D9zMcvtlyEHAxyKSMlvV_upjdJzinYs,2467
-fms/modules/layernorm.py,sha256=0Jnc6o-OWLDZh8RqLVMd23WLyKqr-D_nqYzMVvORB-Y,2512
-fms/modules/positions.py,sha256=eF3IxLocjvgAHwG93CUn8GXCEOft_4uXfotCXgZ2xzQ,9433
+fms/modules/layernorm.py,sha256=Wiu6ulrr0LGYAucv-4RVyAGPZaWpVEUqlCW_69G5lMo,2488
+fms/modules/positions.py,sha256=sCUoJ9xIN7VqL_tlyI-G-w0DF_SISzN_yI0YC3cI9hA,9492
 fms/modules/tp.py,sha256=Aa55yHChChjQYyPFxcrmKEn39zDWkBDzksi36Y6MT3Q,2661
 fms/testing/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 fms/testing/comparison.py,sha256=fX-sUW_sz8hhzIRTZjbkaD5Qr5JXsuERq94oKxmXPlU,5596
 fms/testing/_internal/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 fms/testing/_internal/model_test_suite.py,sha256=IzEZQEJg5fl1LCslqINz_7sr_uVYr555_ox3hW0ytmE,9461
 fms/testing/_internal/hf/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 fms/testing/_internal/hf/model_test_suite.py,sha256=PM3cK7WdAQYX60oIz7Yy6b-tIq9fAeRwV2CIb0ouGj0,14836
 fms/training/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-fms/training/plugins.py,sha256=bhdidTGzW3VkC0iGkxg6A517cU-5qhJHgJXyoe89vKo,8009
-fms/training/trainer.py,sha256=ZXhOkmR9-6LxIn-VurCHTa50_FaMejJfhd2DDSw0Hog,2741
-fms/utils/__init__.py,sha256=4j7xZSEItW78JZLszQbtmderlVi43K-GvmJ7l0hUWKo,650
+fms/training/plugins.py,sha256=Z6ptpK9X9aL_UcsijJSuFP6jTHK2bkuUpEmO8xwvh_M,10914
+fms/training/trainer.py,sha256=E59iuAyI087uC6AWheXH7qvg021m5AijyZD4zLJKlHE,3020
+fms/utils/__init__.py,sha256=D_byaw4RQk-IKnYyxSPUJSwroK_TyA609Ff2yM3j20o,650
 fms/utils/activation.py,sha256=8WQA2DFjtymHuvqamZvb1zbRkDJGUEZtRNqRXJ3U1LQ,2020
 fms/utils/config.py,sha256=x2kL9NBWZH2yJ0Qre_rgeAS7-kTOhDyi_D45bj3w-0M,1872
 fms/utils/evaluation.py,sha256=XOe5jLlzogssZ_oInVG7ls3fR05DXT-69XplrW7kQhE,2626
 fms/utils/generation.py,sha256=vjCh6sEZNwLFT15VHExCCKbm50zl3kUqjUlskvU8UMg,4377
-fms/utils/serialization.py,sha256=M0Rc0jEW8s7QnHm40z4cwzliqLOwxqqEWn0IrwUX-GM,16944
+fms/utils/serialization.py,sha256=hJY2SOInIzFQ90uiTNftOWcSYP7OZ_iOKapDLgVccXM,17283
 fms/utils/tensors.py,sha256=0tsR9o3ov_v1wWxByKTbHBq6xDAoydyoqnWd_fMUo7E,4982
 fms/utils/tokenizers.py,sha256=TovD2WsigU8ErMTBAIHBABEf0mAqLIrgonj6T_5xLNk,5337
-fms/utils/tp_wrapping.py,sha256=nmcpePBcImC4dCIkJ_XlkF0QRva6jgp4b_IX2y7SJWE,1526
-ibm_fms-0.0.3.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-ibm_fms-0.0.3.dist-info/METADATA,sha256=45vVbAiTP-2MIVQCRTI1PD83vJYG9D0M5Wq9z14VJLk,683
-ibm_fms-0.0.3.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-ibm_fms-0.0.3.dist-info/top_level.txt,sha256=4q4zprUhYB424UwinVFiS1FPsd3KHv1_Y1qvef7MtPw,4
-ibm_fms-0.0.3.dist-info/RECORD,,
+fms/utils/tp_wrapping.py,sha256=g2k5ayIaqJeo-hRKT74NGHaO7qNKnREp1eXGkpCmdtk,1596
+ibm_fms-0.0.4.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+ibm_fms-0.0.4.dist-info/METADATA,sha256=Z5xj9QLM6yAc32c2Tca1qCkuhPwS5ffz8pZdBjZfnb8,683
+ibm_fms-0.0.4.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+ibm_fms-0.0.4.dist-info/top_level.txt,sha256=4q4zprUhYB424UwinVFiS1FPsd3KHv1_Y1qvef7MtPw,4
+ibm_fms-0.0.4.dist-info/RECORD,,
```

