# Comparing `tmp/pami-2024.3.9.2.tar.gz` & `tmp/pami-2024.4.9.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "pami-2024.3.9.2.tar", last modified: Sat Mar  9 07:56:15 2024, max compression
+gzip compressed data, was "pami-2024.4.9.1.tar", last modified: Tue Apr  9 02:13:28 2024, max compression
```

## Comparing `pami-2024.3.9.2.tar` & `pami-2024.4.9.1.tar`

### file list

```diff
@@ -1,481 +1,505 @@
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.153631 pami-2024.3.9.2/
--rw-r--r--   0 uday       (501) staff       (20)    35149 2024-03-07 22:55:35.000000 pami-2024.3.9.2/LICENSE
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.012956 pami-2024.3.9.2/PAMI/
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.013189 pami-2024.3.9.2/PAMI/AssociationRules/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/AssociationRules/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.016048 pami-2024.3.9.2/PAMI/AssociationRules/basic/
--rw-r--r--   0 uday       (501) staff       (20)    13299 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/AssociationRules/basic/ARWithConfidence.py
--rw-r--r--   0 uday       (501) staff       (20)    13147 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/AssociationRules/basic/ARWithLeverage.py
--rw-r--r--   0 uday       (501) staff       (20)    13351 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/AssociationRules/basic/ARWithLift.py
--rw-r--r--   0 uday       (501) staff       (20)    18362 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/AssociationRules/basic/RuleMiner.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/AssociationRules/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6594 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/AssociationRules/basic/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)      139 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.016379 pami-2024.3.9.2/PAMI/correlatedPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/correlatedPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.017861 pami-2024.3.9.2/PAMI/correlatedPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    25405 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/correlatedPattern/basic/CoMine.py
--rw-r--r--   0 uday       (501) staff       (20)    27183 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/correlatedPattern/basic/CoMinePlus.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/correlatedPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6208 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/correlatedPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.018301 pami-2024.3.9.2/PAMI/coveragePattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/coveragePattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.019314 pami-2024.3.9.2/PAMI/coveragePattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    14371 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/coveragePattern/basic/CMine.py
--rw-r--r--   0 uday       (501) staff       (20)    16869 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/coveragePattern/basic/CPPG.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/coveragePattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     7155 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/coveragePattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.023089 pami-2024.3.9.2/PAMI/extras/
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.025074 pami-2024.3.9.2/PAMI/extras/DF2DB/
--rw-r--r--   0 uday       (501) staff       (20)     4303 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/DF2DB/DF2DB.py
--rw-r--r--   0 uday       (501) staff       (20)     4226 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/DF2DB/DF2DBPlus.py
--rw-r--r--   0 uday       (501) staff       (20)    10197 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/DF2DB/DenseFormatDF.py
--rw-r--r--   0 uday       (501) staff       (20)     5349 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/DF2DB/SparseFormatDF.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/DF2DB/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     3102 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/DF2DB/createTDB.py
--rw-r--r--   0 uday       (501) staff       (20)     6884 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/DF2DB/denseDF2DBPlus.py
--rw-r--r--   0 uday       (501) staff       (20)    11821 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/DF2DB/denseDF2DB_dump.py
--rw-r--r--   0 uday       (501) staff       (20)     5272 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/DF2DB/sparseDF2DBPlus.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.025551 pami-2024.3.9.2/PAMI/extras/calculateMISValues/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/calculateMISValues/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6439 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/calculateMISValues/usingBeta.py
--rw-r--r--   0 uday       (501) staff       (20)     6356 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/calculateMISValues/usingSD.py
--rw-r--r--   0 uday       (501) staff       (20)     6964 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/convertMultiTSIntoFuzzy.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.028204 pami-2024.3.9.2/PAMI/extras/dbStats/
--rw-r--r--   0 uday       (501) staff       (20)    14542 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/dbStats/FuzzyDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    13484 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py
--rw-r--r--   0 uday       (501) staff       (20)    15624 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/dbStats/SequentialDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    16405 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/dbStats/TemporalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    12415 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/dbStats/TransactionalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    14704 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/dbStats/UncertainTemporalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    11601 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/dbStats/UncertainTransactionalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    12066 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/dbStats/UtilityDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/dbStats/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.029226 pami-2024.3.9.2/PAMI/extras/fuzzyTransformation/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/fuzzyTransformation/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5238 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/fuzzyTransformation/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)     8598 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py
--rw-r--r--   0 uday       (501) staff       (20)     8795 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py
--rw-r--r--   0 uday       (501) staff       (20)     8313 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.030196 pami-2024.3.9.2/PAMI/extras/generateDatabase/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/generateDatabase/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5683 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     9250 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/generateDatabase/generateTemporalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     5167 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/generateDatabase/generateTransactionalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     5156 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/generateLatexGraphFile.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.032760 pami-2024.3.9.2/PAMI/extras/graph/
--rw-r--r--   0 uday       (501) staff       (20)     2913 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/graph/DF2Fig.py
--rw-r--r--   0 uday       (501) staff       (20)     3574 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/graph/DF2Tex.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/graph/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     2725 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/graph/plotLineGraphFromDictionary.py
--rw-r--r--   0 uday       (501) staff       (20)     3598 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/graph/plotLineGraphsFromDataFrame.py
--rw-r--r--   0 uday       (501) staff       (20)     4275 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/graph/visualizeFuzzyPatterns.py
--rw-r--r--   0 uday       (501) staff       (20)     4002 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/graph/visualizePatterns.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.032901 pami-2024.3.9.2/PAMI/extras/image2Database/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/image2Database/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.033150 pami-2024.3.9.2/PAMI/extras/imageProcessing/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/imageProcessing/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6487 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/imageProcessing/imagery2Databases.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.033712 pami-2024.3.9.2/PAMI/extras/messaging/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-09 02:24:23.000000 pami-2024.3.9.2/PAMI/extras/messaging/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)      533 2024-03-09 07:14:07.000000 pami-2024.3.9.2/PAMI/extras/messaging/discord.py
--rw-r--r--   0 uday       (501) staff       (20)     1576 2024-03-09 07:55:30.000000 pami-2024.3.9.2/PAMI/extras/messaging/gmail.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.034450 pami-2024.3.9.2/PAMI/extras/neighbours/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/neighbours/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4788 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py
--rw-r--r--   0 uday       (501) staff       (20)     4415 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py
--rw-r--r--   0 uday       (501) staff       (20)     4310 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py
--rw-r--r--   0 uday       (501) staff       (20)     5011 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/plotPointOnMap.py
--rw-r--r--   0 uday       (501) staff       (20)     5182 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/plotPointOnMap_dump.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.034638 pami-2024.3.9.2/PAMI/extras/sampleDatasets/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/sampleDatasets/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4023 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/scatterPlotSpatialPoints.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.037670 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     2325 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py
--rw-r--r--   0 uday       (501) staff       (20)     2254 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py
--rw-r--r--   0 uday       (501) staff       (20)     2539 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py
--rw-r--r--   0 uday       (501) staff       (20)     1880 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py
--rw-r--r--   0 uday       (501) staff       (20)     1843 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py
--rw-r--r--   0 uday       (501) staff       (20)     2117 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py
--rw-r--r--   0 uday       (501) staff       (20)     2066 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py
--rw-r--r--   0 uday       (501) staff       (20)     2262 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/fuzzyDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     1121 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/generateTemporal.py
--rw-r--r--   0 uday       (501) staff       (20)     1111 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/generateTransactional.py
--rw-r--r--   0 uday       (501) staff       (20)     1625 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py
--rw-r--r--   0 uday       (501) staff       (20)     1610 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py
--rw-r--r--   0 uday       (501) staff       (20)     3613 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py
--rw-r--r--   0 uday       (501) staff       (20)     3603 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/georeferencedTemporalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/georeferencedTransactionalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     1105 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/temporalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     1096 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/transactionalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     4842 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/utilityDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     3238 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/topKPatterns.py
--rw-r--r--   0 uday       (501) staff       (20)     2321 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/extras/uncertaindb_convert.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.037864 pami-2024.3.9.2/PAMI/faultTolerantFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/faultTolerantFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.038824 pami-2024.3.9.2/PAMI/faultTolerantFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    14147 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py
--rw-r--r--   0 uday       (501) staff       (20)    22841 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/faultTolerantFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6856 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/faultTolerantFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.039016 pami-2024.3.9.2/PAMI/frequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.041043 pami-2024.3.9.2/PAMI/frequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    13598 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/basic/Apriori.py
--rw-r--r--   0 uday       (501) staff       (20)    12939 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/basic/ECLAT.py
--rw-r--r--   0 uday       (501) staff       (20)    13500 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/basic/ECLATDiffset.py
--rw-r--r--   0 uday       (501) staff       (20)    13715 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/basic/ECLATbitset.py
--rw-r--r--   0 uday       (501) staff       (20)    20589 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/basic/FPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     7867 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.042040 pami-2024.3.9.2/PAMI/frequentPattern/closed/
--rw-r--r--   0 uday       (501) staff       (20)    19904 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/closed/CHARM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/closed/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6580 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/closed/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.046489 pami-2024.3.9.2/PAMI/frequentPattern/cuda/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/cuda/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5980 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/cuda/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    13280 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/cuda/cuApriori.py
--rw-r--r--   0 uday       (501) staff       (20)    14031 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/cuda/cuAprioriBit.py
--rw-r--r--   0 uday       (501) staff       (20)    12639 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/cuda/cuEclat.py
--rw-r--r--   0 uday       (501) staff       (20)    14187 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/cuda/cuEclatBit.py
--rw-r--r--   0 uday       (501) staff       (20)    14165 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/cuda/cudaAprioriGCT.py
--rw-r--r--   0 uday       (501) staff       (20)    16736 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/cuda/cudaAprioriTID.py
--rw-r--r--   0 uday       (501) staff       (20)    13414 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/cuda/cudaEclatGCT.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.049196 pami-2024.3.9.2/PAMI/frequentPattern/maximal/
--rw-r--r--   0 uday       (501) staff       (20)    24943 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/maximal/MaxFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/maximal/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6566 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/maximal/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.050881 pami-2024.3.9.2/PAMI/frequentPattern/pyspark/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/pyspark/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5573 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/pyspark/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    15033 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/pyspark/parallelApriori.py
--rw-r--r--   0 uday       (501) staff       (20)    12387 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/pyspark/parallelECLAT.py
--rw-r--r--   0 uday       (501) staff       (20)    16498 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/pyspark/parallelFPGrowth.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.051954 pami-2024.3.9.2/PAMI/frequentPattern/topk/
--rw-r--r--   0 uday       (501) staff       (20)    14740 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/topk/FAE.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/topk/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4575 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/frequentPattern/topk/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.052335 pami-2024.3.9.2/PAMI/fuzzyCorrelatedPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyCorrelatedPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.053445 pami-2024.3.9.2/PAMI/fuzzyCorrelatedPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    26528 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyCorrelatedPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6645 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyCorrelatedPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.053710 pami-2024.3.9.2/PAMI/fuzzyFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.054747 pami-2024.3.9.2/PAMI/fuzzyFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    21456 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py
--rw-r--r--   0 uday       (501) staff       (20)    26942 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6428 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.055097 pami-2024.3.9.2/PAMI/fuzzyGeoreferencedFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.055853 pami-2024.3.9.2/PAMI/fuzzyGeoreferencedFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    24755 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py
--rw-r--r--   0 uday       (501) staff       (20)    26784 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyGeoreferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6724 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.056133 pami-2024.3.9.2/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.056845 pami-2024.3.9.2/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    27119 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py
--rw-r--r--   0 uday       (501) staff       (20)    31875 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6618 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.057141 pami-2024.3.9.2/PAMI/fuzzyPartialPeriodicPatterns/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyPartialPeriodicPatterns/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.058063 pami-2024.3.9.2/PAMI/fuzzyPartialPeriodicPatterns/basic/
--rw-r--r--   0 uday       (501) staff       (20)    20810 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyPartialPeriodicPatterns/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6463 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.058353 pami-2024.3.9.2/PAMI/fuzzyPeriodicFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.059946 pami-2024.3.9.2/PAMI/fuzzyPeriodicFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    23967 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py
--rw-r--r--   0 uday       (501) staff       (20)    25690 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6678 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.060227 pami-2024.3.9.2/PAMI/geoReferencedPeriodicFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/geoReferencedPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.060850 pami-2024.3.9.2/PAMI/geoReferencedPeriodicFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    20431 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/geoReferencedPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6782 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.061141 pami-2024.3.9.2/PAMI/georeferencedFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/georeferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.062076 pami-2024.3.9.2/PAMI/georeferencedFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    20448 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    19405 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/georeferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6689 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/georeferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.062677 pami-2024.3.9.2/PAMI/georeferencedFrequentSequencePattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/georeferencedFrequentSequencePattern/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6690 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/georeferencedFrequentSequencePattern/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.062916 pami-2024.3.9.2/PAMI/georeferencedPartialPeriodicPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/georeferencedPartialPeriodicPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.063374 pami-2024.3.9.2/PAMI/georeferencedPartialPeriodicPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    19898 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/georeferencedPartialPeriodicPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6178 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.064285 pami-2024.3.9.2/PAMI/highUtilityFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.064715 pami-2024.3.9.2/PAMI/highUtilityFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    36056 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityFrequentPattern/basic/HUFIM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6179 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.065071 pami-2024.3.9.2/PAMI/highUtilityGeoreferencedFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.065758 pami-2024.3.9.2/PAMI/highUtilityGeoreferencedFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    41149 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityGeoreferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6307 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.065973 pami-2024.3.9.2/PAMI/highUtilityPattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.071402 pami-2024.3.9.2/PAMI/highUtilityPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    33270 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityPattern/basic/EFIM.py
--rw-r--r--   0 uday       (501) staff       (20)    24905 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityPattern/basic/HMiner.py
--rw-r--r--   0 uday       (501) staff       (20)    27041 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityPattern/basic/UPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5166 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityPattern/basic/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    18911 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityPattern/basic/efimParallel.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.073885 pami-2024.3.9.2/PAMI/highUtilityPattern/parallel/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityPattern/parallel/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5166 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityPattern/parallel/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    16900 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityPattern/parallel/efimparallel.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.074878 pami-2024.3.9.2/PAMI/highUtilityPatternsInStreams/
--rw-r--r--   0 uday       (501) staff       (20)    28756 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityPatternsInStreams/HUPMS.py
--rw-r--r--   0 uday       (501) staff       (20)    31488 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityPatternsInStreams/SHUGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityPatternsInStreams/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5193 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilityPatternsInStreams/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.075386 pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6716 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.076452 pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    27875 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py
--rw-r--r--   0 uday       (501) staff       (20)    34889 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/basic/SHUIM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5934 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.077082 pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/topk/
--rw-r--r--   0 uday       (501) staff       (20)    36415 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/topk/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6618 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/topk/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.077541 pami-2024.3.9.2/PAMI/localPeriodicPattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/localPeriodicPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.078909 pami-2024.3.9.2/PAMI/localPeriodicPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    34480 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/localPeriodicPattern/basic/LPPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    23965 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/localPeriodicPattern/basic/LPPMBreadth.py
--rw-r--r--   0 uday       (501) staff       (20)    22949 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/localPeriodicPattern/basic/LPPMDepth.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/localPeriodicPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     8385 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/localPeriodicPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.079194 pami-2024.3.9.2/PAMI/multipleMinimumSupportBasedFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/multipleMinimumSupportBasedFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.080450 pami-2024.3.9.2/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    23074 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    20984 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5921 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.080649 pami-2024.3.9.2/PAMI/partialPeriodicFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.081252 pami-2024.3.9.2/PAMI/partialPeriodicFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    28279 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    22083 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5398 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.081442 pami-2024.3.9.2/PAMI/partialPeriodicPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.082724 pami-2024.3.9.2/PAMI/partialPeriodicPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    26417 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)     4329 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/basic/Gabstract.py
--rw-r--r--   0 uday       (501) staff       (20)    24904 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/basic/PPPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    18497 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5520 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.083339 pami-2024.3.9.2/PAMI/partialPeriodicPattern/closed/
--rw-r--r--   0 uday       (501) staff       (20)    21000 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/closed/PPPClose.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/closed/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5605 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/closed/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.084128 pami-2024.3.9.2/PAMI/partialPeriodicPattern/maximal/
--rw-r--r--   0 uday       (501) staff       (20)    29141 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/maximal/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4278 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/maximal/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.084717 pami-2024.3.9.2/PAMI/partialPeriodicPattern/pyspark/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/pyspark/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5765 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/pyspark/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    28228 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.085296 pami-2024.3.9.2/PAMI/partialPeriodicPattern/topk/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/topk/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6441 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/topk/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    18484 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/partialPeriodicPattern/topk/k3PMiner.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.085844 pami-2024.3.9.2/PAMI/partialPeriodicPatternInMultipleTimeSeries/
--rw-r--r--   0 uday       (501) staff       (20)    25823 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicPatternInMultipleTimeSeries/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5556 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.086048 pami-2024.3.9.2/PAMI/periodicCorrelatedPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicCorrelatedPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.086492 pami-2024.3.9.2/PAMI/periodicCorrelatedPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    28378 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicCorrelatedPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6640 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicCorrelatedPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.086684 pami-2024.3.9.2/PAMI/periodicFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.088689 pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    16812 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/PFECLAT.py
--rw-r--r--   0 uday       (501) staff       (20)    26469 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/PFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    26035 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py
--rw-r--r--   0 uday       (501) staff       (20)    17758 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/PFPMC.py
--rw-r--r--   0 uday       (501) staff       (20)    34538 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/PSGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)      726 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6545 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    24941 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.089196 pami-2024.3.9.2/PAMI/periodicFrequentPattern/closed/
--rw-r--r--   0 uday       (501) staff       (20)    21962 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/closed/CPFPMiner.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/closed/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6539 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/closed/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.090016 pami-2024.3.9.2/PAMI/periodicFrequentPattern/cuda/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/cuda/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6568 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/cuda/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    20366 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py
--rw-r--r--   0 uday       (501) staff       (20)    16905 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.090767 pami-2024.3.9.2/PAMI/periodicFrequentPattern/maximal/
--rw-r--r--   0 uday       (501) staff       (20)    29439 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/maximal/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     7869 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/maximal/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.091292 pami-2024.3.9.2/PAMI/periodicFrequentPattern/pyspark/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/pyspark/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5219 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/pyspark/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    25050 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.091475 pami-2024.3.9.2/PAMI/periodicFrequentPattern/topk/
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.091990 pami-2024.3.9.2/PAMI/periodicFrequentPattern/topk/TopkPFP/
--rw-r--r--   0 uday       (501) staff       (20)    18261 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/topk/TopkPFP/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6862 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/topk/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.094446 pami-2024.3.9.2/PAMI/periodicFrequentPattern/topk/kPFPMiner/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/topk/kPFPMiner/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4589 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    17258 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.094752 pami-2024.3.9.2/PAMI/recurringPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/recurringPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.098623 pami-2024.3.9.2/PAMI/recurringPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    27057 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/recurringPattern/basic/RPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/recurringPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6637 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/recurringPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.098934 pami-2024.3.9.2/PAMI/relativeFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/relativeFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.099675 pami-2024.3.9.2/PAMI/relativeFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    27865 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/relativeFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4261 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/relativeFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.100678 pami-2024.3.9.2/PAMI/relativeHighUtilityPattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/relativeHighUtilityPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.101158 pami-2024.3.9.2/PAMI/relativeHighUtilityPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    34034 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/relativeHighUtilityPattern/basic/RHUIM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/relativeHighUtilityPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6052 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/relativeHighUtilityPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.102924 pami-2024.3.9.2/PAMI/sequence/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/sequence/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.103037 pami-2024.3.9.2/PAMI/sequentialPatternMining/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/sequentialPatternMining/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.112988 pami-2024.3.9.2/PAMI/sequentialPatternMining/basic/
--rw-r--r--   0 uday       (501) staff       (20)    40522 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/sequentialPatternMining/basic/SPADE.py
--rw-r--r--   0 uday       (501) staff       (20)    18979 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/sequentialPatternMining/basic/SPAM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/sequentialPatternMining/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6569 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/sequentialPatternMining/basic/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    23022 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/sequentialPatternMining/basic/prefixSpan.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.113476 pami-2024.3.9.2/PAMI/sequentialPatternMining/closed/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/sequentialPatternMining/closed/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6285 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/sequentialPatternMining/closed/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/sequentialPatternMining/closed/bide.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.113573 pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.133483 pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    16937 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py
--rw-r--r--   0 uday       (501) staff       (20)    26411 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    19344 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     7271 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.134241 pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/topK/
--rw-r--r--   0 uday       (501) staff       (20)    27859 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/topK/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     7173 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/topK/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.134528 pami-2024.3.9.2/PAMI/subgraphMining/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/subgraphMining/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.137123 pami-2024.3.9.2/PAMI/subgraphMining/basic/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/subgraphMining/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     1241 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/subgraphMining/basic/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)     2396 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/subgraphMining/basic/dfsCode.py
--rw-r--r--   0 uday       (501) staff       (20)      772 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/subgraphMining/basic/edge.py
--rw-r--r--   0 uday       (501) staff       (20)     2616 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/subgraphMining/basic/extendedEdge.py
--rw-r--r--   0 uday       (501) staff       (20)      670 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/subgraphMining/basic/frequentSubgraph.py
--rw-r--r--   0 uday       (501) staff       (20)     4943 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/subgraphMining/basic/graph.py
--rw-r--r--   0 uday       (501) staff       (20)    28131 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/subgraphMining/basic/gspan.py
--rw-r--r--   0 uday       (501) staff       (20)     1748 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/subgraphMining/basic/sparseTriangularMatrix.py
--rw-r--r--   0 uday       (501) staff       (20)      826 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/subgraphMining/basic/vertex.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.142733 pami-2024.3.9.2/PAMI/uncertainFaultTolerantFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)    15305 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainFaultTolerantFrequentPattern/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6756 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.143175 pami-2024.3.9.2/PAMI/uncertainFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.144980 pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    27332 2024-03-09 01:54:15.000000 pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/CUFPTree.py
--rw-r--r--   0 uday       (501) staff       (20)    26483 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    19522 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/TUFP.py
--rw-r--r--   0 uday       (501) staff       (20)    27664 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/TubeP.py
--rw-r--r--   0 uday       (501) staff       (20)    27844 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/TubeS.py
--rw-r--r--   0 uday       (501) staff       (20)    25707 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/UFGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    19561 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/UVECLAT.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4953 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.145271 pami-2024.3.9.2/PAMI/uncertainGeoreferencedFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainGeoreferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.145742 pami-2024.3.9.2/PAMI/uncertainGeoreferencedFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    28959 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainGeoreferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4991 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.146104 pami-2024.3.9.2/PAMI/uncertainPeriodicFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.147123 pami-2024.3.9.2/PAMI/uncertainPeriodicFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    30851 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    31232 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6536 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.147338 pami-2024.3.9.2/PAMI/weightedFrequentNeighbourhoodPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedFrequentNeighbourhoodPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.147917 pami-2024.3.9.2/PAMI/weightedFrequentNeighbourhoodPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    27883 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedFrequentNeighbourhoodPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6603 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.148156 pami-2024.3.9.2/PAMI/weightedFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.148763 pami-2024.3.9.2/PAMI/weightedFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    24644 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedFrequentPattern/basic/WFIM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6659 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.148975 pami-2024.3.9.2/PAMI/weightedFrequentRegularPattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedFrequentRegularPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.149413 pami-2024.3.9.2/PAMI/weightedFrequentRegularPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    27943 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedFrequentRegularPattern/basic/WFRIMiner.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedFrequentRegularPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     7495 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedFrequentRegularPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.149628 pami-2024.3.9.2/PAMI/weightedUncertainFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedUncertainFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.150445 pami-2024.3.9.2/PAMI/weightedUncertainFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    29522 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedUncertainFrequentPattern/basic/WUFIM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedUncertainFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4771 2024-03-07 22:55:35.000000 pami-2024.3.9.2/PAMI/weightedUncertainFrequentPattern/basic/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    62897 2024-03-09 07:56:15.153096 pami-2024.3.9.2/PKG-INFO
--rw-r--r--   0 uday       (501) staff       (20)    61478 2024-03-07 22:55:35.000000 pami-2024.3.9.2/README.md
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-03-09 07:56:15.151992 pami-2024.3.9.2/pami.egg-info/
--rw-r--r--   0 uday       (501) staff       (20)    62897 2024-03-09 07:56:14.000000 pami-2024.3.9.2/pami.egg-info/PKG-INFO
--rw-r--r--   0 uday       (501) staff       (20)    17229 2024-03-09 07:56:14.000000 pami-2024.3.9.2/pami.egg-info/SOURCES.txt
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-09 07:56:14.000000 pami-2024.3.9.2/pami.egg-info/dependency_links.txt
--rw-r--r--   0 uday       (501) staff       (20)      228 2024-03-09 07:56:14.000000 pami-2024.3.9.2/pami.egg-info/requires.txt
--rw-r--r--   0 uday       (501) staff       (20)        5 2024-03-09 07:56:14.000000 pami-2024.3.9.2/pami.egg-info/top_level.txt
--rw-r--r--   0 uday       (501) staff       (20)       38 2024-03-09 07:56:15.153684 pami-2024.3.9.2/setup.cfg
--rw-r--r--   0 uday       (501) staff       (20)     1475 2024-03-09 07:56:05.000000 pami-2024.3.9.2/setup.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.602975 pami-2024.4.9.1/
+-rw-r--r--   0 vanithak   (502) staff       (20)    35149 2024-03-12 04:33:29.000000 pami-2024.4.9.1/LICENSE
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.793712 pami-2024.4.9.1/PAMI/
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.795296 pami-2024.4.9.1/PAMI/AssociationRules/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.809616 pami-2024.4.9.1/PAMI/AssociationRules/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    14314 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithConfidence.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14661 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithLeverage.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14622 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithLift.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    20378 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/RuleMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6594 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      139 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.811156 pami-2024.4.9.1/PAMI/correlatedPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.817103 pami-2024.4.9.1/PAMI/correlatedPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    27142 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/basic/CoMine.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    29081 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/basic/CoMinePlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6208 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.818346 pami-2024.4.9.1/PAMI/coveragePattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.826282 pami-2024.4.9.1/PAMI/coveragePattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    15616 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/basic/CMine.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    18923 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/basic/CPPG.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7155 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.838845 pami-2024.4.9.1/PAMI/extras/
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.853335 pami-2024.4.9.1/PAMI/extras/DF2DB/
+-rw-r--r--   0 vanithak   (502) staff       (20)     4360 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/DF2DB.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4287 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/DF2DBPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    10331 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/DenseFormatDF.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5413 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/SparseFormatDF.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3103 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/createTDB.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6948 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/denseDF2DBPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    11940 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/denseDF2DB_dump.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5336 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/sparseDF2DBPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.858573 pami-2024.4.9.1/PAMI/extras/calculateMISValues/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/calculateMISValues/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6468 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/calculateMISValues/usingBeta.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6499 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/calculateMISValues/usingSD.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6964 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/convertMultiTSIntoFuzzy.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.876165 pami-2024.4.9.1/PAMI/extras/dbStats/
+-rw-r--r--   0 vanithak   (502) staff       (20)    14951 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/FuzzyDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    13796 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16034 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/dbStats/SequentialDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16883 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/TemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    12839 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/TransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    15120 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/UncertainTemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    11953 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/UncertainTransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    12679 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/dbStats/UtilityDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.880811 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5238 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     8594 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     8792 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     8313 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.889684 pami-2024.4.9.1/PAMI/extras/generateDatabase/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/generateDatabase/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5685 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     9558 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/generateDatabase/generateTemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5971 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/generateDatabase/generateTransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5156 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/generateLatexGraphFile.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.901708 pami-2024.4.9.1/PAMI/extras/graph/
+-rw-r--r--   0 vanithak   (502) staff       (20)     3223 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/DF2Fig.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3577 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/DF2Tex.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/graph/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2750 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/plotLineGraphFromDictionary.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3599 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/plotLineGraphsFromDataFrame.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4465 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/visualizeFuzzyPatterns.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4240 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/visualizePatterns.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.904228 pami-2024.4.9.1/PAMI/extras/image2Database/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/image2Database/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.906815 pami-2024.4.9.1/PAMI/extras/imageProcessing/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/imageProcessing/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6488 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/imageProcessing/imagery2Databases.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.911056 pami-2024.4.9.1/PAMI/extras/messaging/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/messaging/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      533 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/messaging/discord.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1575 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/messaging/gmail.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.917378 pami-2024.4.9.1/PAMI/extras/neighbours/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/neighbours/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4789 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4415 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4310 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5011 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/plotPointOnMap.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5182 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/plotPointOnMap_dump.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.922472 pami-2024.4.9.1/PAMI/extras/sampleDatasets/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/sampleDatasets/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4023 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/scatterPlotSpatialPoints.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.932869 pami-2024.4.9.1/PAMI/extras/stats/
+-rw-r--r--   0 vanithak   (502) staff       (20)    12724 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/stats/TransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/stats/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4144 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/stats/graphDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    15998 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/stats/sequentialDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16926 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/stats/temporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    12692 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/stats/utilityDatabase.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.964837 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/
+-rw-r--r--   0 vanithak   (502) staff       (20)     8471 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/TemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5543 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/TransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2325 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2254 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2539 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1880 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1843 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2117 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2066 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2262 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/fuzzyDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1121 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1111 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateTransactional.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1625 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1610 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3613 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3603 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/georeferencedTemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/georeferencedTransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4324 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/syntheticUtilityDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3283 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/temporalDatabaseGen.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4842 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/utilityDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3238 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/topKPatterns.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2321 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/uncertaindb_convert.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.969666 pami-2024.4.9.1/PAMI/extras/visualize/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/visualize/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1897 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/visualize/graphs.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.971091 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.976128 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    15253 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    24431 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6856 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.977409 pami-2024.4.9.1/PAMI/frequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.988154 pami-2024.4.9.1/PAMI/frequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    15220 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/Apriori.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14362 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14885 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLATDiffset.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    15362 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLATbitset.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    22703 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/FPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7867 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.991640 pami-2024.4.9.1/PAMI/frequentPattern/closed/
+-rw-r--r--   0 vanithak   (502) staff       (20)    22294 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/closed/CHARM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/closed/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6580 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/closed/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.052468 pami-2024.4.9.1/PAMI/frequentPattern/cuda/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5980 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    15188 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuApriori.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16228 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuAprioriBit.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14636 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuEclat.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16604 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuEclatBit.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16419 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaAprioriGCT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    21681 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaAprioriTID.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    15055 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaEclatGCT.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.056713 pami-2024.4.9.1/PAMI/frequentPattern/maximal/
+-rw-r--r--   0 vanithak   (502) staff       (20)    27531 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/maximal/MaxFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/maximal/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6561 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/maximal/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.064878 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5573 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16543 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelApriori.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14856 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19026 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelFPGrowth.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.073549 pami-2024.4.9.1/PAMI/frequentPattern/topk/
+-rw-r--r--   0 vanithak   (502) staff       (20)    16687 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/topk/FAE.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/topk/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4575 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/topk/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.075074 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.080427 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    32988 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6645 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.085119 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.092414 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    25855 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    32901 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6428 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.094061 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.101426 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    29496 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    32795 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6724 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.103125 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.110674 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    33628 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    39030 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6618 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.114640 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.121669 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    25016 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6463 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.122749 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.128995 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    29275 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    32773 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6678 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.130182 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.138617 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    23633 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6782 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.139762 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.145864 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    23719 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    22366 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6689 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.148990 pami-2024.4.9.1/PAMI/georeferencedFrequentSequencePattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentSequencePattern/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6690 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentSequencePattern/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.150420 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.155070 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    23257 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6178 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.156998 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.160629 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    41228 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/HUFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6179 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.162526 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.169244 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    46718 2024-04-09 02:02:27.000000 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6307 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.170240 pami-2024.4.9.1/PAMI/highUtilityPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.178668 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    37128 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/EFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    30770 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/HMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    32709 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/UPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5166 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    20285 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/efimParallel.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.185414 pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5166 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    18207 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/efimparallel.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.191526 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/
+-rw-r--r--   0 vanithak   (502) staff       (20)    32873 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/HUPMS.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    35784 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/SHUGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5193 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.194856 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6716 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.201955 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    35564 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    40140 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/SHUIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5934 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.205999 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/
+-rw-r--r--   0 vanithak   (502) staff       (20)    40254 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6618 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.207268 pami-2024.4.9.1/PAMI/localPeriodicPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.214488 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    34480 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    23965 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPMBreadth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    22949 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPMDepth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     8385 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.217320 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.222491 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    25470 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    23153 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5921 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.224344 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.234442 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    28279 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    22083 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5398 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.235758 pami-2024.4.9.1/PAMI/partialPeriodicPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.251077 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    26417 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4329 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/Gabstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26878 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/PPPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    20736 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5520 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.257160 pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/
+-rw-r--r--   0 vanithak   (502) staff       (20)    24133 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/PPPClose.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5605 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.274878 pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/
+-rw-r--r--   0 vanithak   (502) staff       (20)    29141 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4278 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.286087 pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5765 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    30910 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.293743 pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6441 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    20928 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/k3PMiner.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.303465 pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/
+-rw-r--r--   0 vanithak   (502) staff       (20)    28158 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5556 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.309577 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.319198 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    27514 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6640 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.320539 pami-2024.4.9.1/PAMI/periodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.340954 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    17905 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    28583 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26383 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    18106 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPMC.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    36300 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PSGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      726 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6545 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26643 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.347343 pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/
+-rw-r--r--   0 vanithak   (502) staff       (20)    24312 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/CPFPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6539 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.356329 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6568 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    23867 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    18982 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.363446 pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/
+-rw-r--r--   0 vanithak   (502) staff       (20)    31832 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7869 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.373650 pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5219 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26749 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.376040 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.383726 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/
+-rw-r--r--   0 vanithak   (502) staff       (20)    19951 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6862 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.387608 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4589 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    17514 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.389862 pami-2024.4.9.1/PAMI/recurringPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/recurringPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.394287 pami-2024.4.9.1/PAMI/recurringPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    29135 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/recurringPattern/basic/RPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/recurringPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6637 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/recurringPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.396535 pami-2024.4.9.1/PAMI/relativeFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.403016 pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    30349 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4261 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.404650 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.407969 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    35406 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/RHUIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6052 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.410534 pami-2024.4.9.1/PAMI/sequence/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequence/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.411261 pami-2024.4.9.1/PAMI/sequentialPatternMining/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.421832 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    42265 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/SPADE.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19986 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/SPAM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6569 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    24786 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/prefixSpan.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.426003 pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6285 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/bide.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.427496 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.436338 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    19114 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26504 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    21391 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7271 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.440665 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/
+-rw-r--r--   0 vanithak   (502) staff       (20)    27859 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7173 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.441985 pami-2024.4.9.1/PAMI/subgraphMining/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.456287 pami-2024.4.9.1/PAMI/subgraphMining/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1241 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2396 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/dfsCode.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      772 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/edge.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2616 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/extendedEdge.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      670 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/frequentSubgraph.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4943 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/graph.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    28244 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/gspan.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1748 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/sparseTriangularMatrix.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      826 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/vertex.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.478210 pami-2024.4.9.1/PAMI/subgraphMining/topK/
+-rw-r--r--   0 vanithak   (502) staff       (20)     1949 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/DFSCode.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      593 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/DFSThread.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1316 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      772 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/edge.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2613 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/extendedEdge.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      674 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/frequentSubgraph.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4295 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/graph.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1486 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/sparseTriangularMatrix.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    20979 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/tkg.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      818 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/vertex.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.482255 pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)    17358 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6756 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.483938 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.506622 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    28610 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/CUFPTree.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26391 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19572 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TUFP.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19454 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TubeP.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    27790 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TubeS.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    25664 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/UFGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19522 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/UVECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4945 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.508127 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.520288 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    30868 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4986 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.524799 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.537346 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    33395 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    33912 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6536 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.538277 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.546173 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    30821 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6603 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.549586 pami-2024.4.9.1/PAMI/weightedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.558446 pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    27246 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/WFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6659 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.563418 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.574205 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    30320 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/WFRIMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7495 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.575583 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.579690 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    31958 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/WUFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4771 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    67204 2024-04-09 02:13:28.599349 pami-2024.4.9.1/PKG-INFO
+-rw-r--r--   0 vanithak   (502) staff       (20)    65760 2024-03-29 21:11:29.000000 pami-2024.4.9.1/README.md
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.589956 pami-2024.4.9.1/pami.egg-info/
+-rw-r--r--   0 vanithak   (502) staff       (20)    67204 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/PKG-INFO
+-rw-r--r--   0 vanithak   (502) staff       (20)    18058 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/SOURCES.txt
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/dependency_links.txt
+-rw-r--r--   0 vanithak   (502) staff       (20)      237 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/requires.txt
+-rw-r--r--   0 vanithak   (502) staff       (20)        5 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/top_level.txt
+-rw-r--r--   0 vanithak   (502) staff       (20)       38 2024-04-09 02:13:28.603191 pami-2024.4.9.1/setup.cfg
+-rw-r--r--   0 vanithak   (502) staff       (20)     1494 2024-04-09 02:13:23.000000 pami-2024.4.9.1/setup.py
```

### Comparing `pami-2024.3.9.2/LICENSE` & `pami-2024.4.9.1/LICENSE`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/AssociationRules/basic/ARWithConfidence.py` & `pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithConfidence.py`

 * *Files 3% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # ----------------------------------------------------
 #
 #
 #             import PAMI.AssociationRules.basic import ARWithConfidence as alg
 #
 #             obj = alg.ARWithConfidence(iFile, minConf)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             associationRules = obj.getPatterns()
 #
 #             print("Total number of Association Rules:", len(associationRules))
 #
 #             obj.save(oFile)
 #
@@ -25,21 +25,22 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -53,24 +54,26 @@
      
 """
 
 
 
 
 from PAMI.AssociationRules.basic import abstract as _ab
+from deprecated import deprecated
+
 
 
 class _Confidence:
     """
-    :param  patterns: dict :
-                      Dictionary containing patterns and its support value.
-    :param  singleItems: list :
-                         List containing all the single frequent items.
-    :param  minConf: int :
-                     Minimum confidence to mine all the satisfying association rules.
+    :param  patterns: Dictionary containing patterns and its support value.
+    :type patterns: dict
+    :param  singleItems: List containing all the single frequent items.
+    :type singleItems: list
+    :param  minConf: Minimum confidence to mine all the satisfying association rules.
+    :type minConf: int
     """
 
     def __init__(self, patterns, singleItems, minConf):
         """
         :param patterns: given frequent patterns
         :type patterns: dict
         :param singleItems: one-length frequent patterns
@@ -136,22 +139,24 @@
 
 class ARWithConfidence:
     """
     :Description: Association Rules are derived from frequent patterns using "confidence" metric.
 
     :Reference:
 
-    :param iFile: str or df :
-                  Name of the Input file to mine the association rules
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of association rules
+    :param  oFile: str :
+                   Name of the output file to store complete set of association rules
+    :param  minConf: float :
+                   The user can specify the minConf in float between the range of 0 to 1.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
 
-    :param minConf: float
-                  The user can specify the minConf in float
-    :par sep: str :
-                  This variable is used to distinguish items from one another in given input file. The default seperator is tab space. However, the users can override their default seperator.
-        
         
     :Attributes:
 
 
         startTime : float
             To record the start time of the mining process
 
@@ -167,33 +172,36 @@
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
-            Format:
-                      >>> python3 ARWithConfidence.py <inputFile> <outputFile> <minConf> <sep>
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 ARWithConfidence.py <inputFile> <outputFile> <minConf> <sep>
 
-            Example:
-                     >>>  python3 ARWithConfidence.py sampleDB.txt patterns.txt 0.5 ' '
+      Example Usage:
 
-            .. note:: minConf will be considered only in 0 to 1.
+      (.venv) $ python3 ARWithConfidence.py sampleDB.txt patterns.txt 0.5 ' '
 
+    .. note:: minConf will be considered only in 0 to 1.
     
     
     **Importing this algorithm into a python program**
     ----------------------------------------------------
     .. code-block:: python
 
             import PAMI.AssociationRules.basic import ARWithConfidence as alg
 
             obj = alg.ARWithConfidence(iFile, minConf)
 
-            obj.startMine()
+            obj.mine()
 
             associationRules = obj.getPatterns()
 
             print("Total number of Association Rules:", len(associationRules))
 
             obj.save(oFile)
 
@@ -282,14 +290,15 @@
                             s = '\t'.join(s)
                             self._frequentPatterns[s.strip()] = int(line[1])
                 except IOError:
                     print("File Not Found")
                     quit()
         return k
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Association rule mining process will start from here
         """
         self._startTime = _ab._time.time()
         k = self._readPatterns()
         a = _Confidence(self._frequentPatterns, k, self._minConf)
@@ -299,14 +308,33 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Association rules successfully  generated from frequent patterns ")
 
+
+
+    def mine(self):
+        """
+        Association rule mining process will start from here
+        """
+        self._startTime = _ab._time.time()
+        k = self._readPatterns()
+        a = _Confidence(self._frequentPatterns, k, self._minConf)
+        a.run()
+        self._finalPatterns = a._finalPatterns
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Association rules successfully  generated from frequent patterns ")
+
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -379,14 +407,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = ARWithConfidence(_ab._sys.argv[1], float(_ab._sys.argv[3]), _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = ARWithConfidence(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Association Rules:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/AssociationRules/basic/ARWithLeverage.py` & `pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithLeverage.py`

 * *Files 9% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # ----------------------------------------------------
 #
 #
 #             import PAMI.AssociationRules.basic import ARWithLeverage as alg
 #
 #             obj = alg.ARWithLeverage(iFile, minConf)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             associationRules = obj.getPatterns()
 #
 #             print("Total number of Association Rules:", len(associationRules))
 #
 #             obj.save(oFile)
 #
@@ -25,21 +25,22 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -50,35 +51,42 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
 from PAMI.AssociationRules.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 
 class _Leverage:
 
     """
-    :param  patterns: dict :
-                   Dictionary containing patterns and its support value.
-    :param  singleItems: list :
-                   List containing all the single frequent items.
-    :param  minConf: int :
-                   Minimum confidence to mine all the satisfying association rules.
+    :param patterns: Dictionary containing patterns and its support value.
+
+    :type patterns: dict
+
+    :param  singleItems: List containing all the single frequent items.
+
+    :type singleItems: list
+
+    :param  minConf: Minimum confidence to mine all the satisfying association rules.
+
+    :type minConf: int
     """
 
     def __init__(self, patterns, singleItems, minConf) -> None:
         """
         :param patterns: given frequent patterns
-        :type inputFile: dict
+        :type patterns: dict
         :param singleItems: one-length frequent patterns
         :type singleItems: list
         :param minConf: minimum confidence
         :type minConf: float
+        :return: None
         """
         self._frequentPatterns = patterns
         self._singleItems = singleItems
         self._minConf = minConf
         self._finalPatterns = {}
 
     def _generation(self, prefix, suffix) -> None:
@@ -103,14 +111,16 @@
     def _generateWithLeverage(self, lhs, rhs) -> float:
         """
         To find association rules satisfying user-specified minConf
         :param lhs: the prefix of association rule.
         :type lhs: str
         :param rhs: the suffix of association rule.
         :type rhs: str
+        :return: the association rule
+        :rtype: float
         """
         s = lhs + '\t' + rhs
         if self._frequentPatterns.get(s) == None:
             return 0
         minimum = self._frequentPatterns[s]
         conf_lhs = minimum / self._frequentPatterns[lhs]
         conf_rhs = minimum / self._frequentPatterns[rhs]
@@ -137,20 +147,22 @@
 
 class ARWithLeverage:
     """
     :Description: Association Rules are derived from frequent patterns using "leverage" metric.
 
     :Reference:
 
-    :param iFile: str or df :
-                  Name of the Input file to mine the association rules
-    :param minConf: float
-                  The user can specify the minConf in float
-    :param sep: str :
-                  This variable is used to distinguish items from one another in given input file. The default seperator is tab space. However, the users can override their default seperator.
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of association rules
+    :param  oFile: str :
+                   Name of the output file to store complete set of association rules
+    :param  minConf: float :
+                   The user can specify the minConf in float between the range of 0 to 1.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
         
         
     :Attributes:
 
 
         startTime : float
             To record the start time of the mining process
@@ -167,33 +179,36 @@
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
-            Format:
-                      >>> python3 ARWithLeverage.py <inputFile> <outputFile> <minConf> <sep>
+    .. code-block:: console
 
-            Example:
-                      >>>  python3 ARWithLeverage.py sampleDB.txt patterns.txt 10.0 ' '
+      Format:
 
-                      .. note:: minConf will be considered only in 0 to 1.
+      (.venv) $ python3 ARWithLeverage.py <inputFile> <outputFile> <minConf> <sep>
+
+      Example Usage:
+
+      (.venv) $ python3 ARWithLeverage.py sampleDB.txt patterns.txt 10.0 ' '
+
+    .. note:: minConf will be considered only in 0 to 1.
 
-    
     
     **Importing this algorithm into a python program**
     ----------------------------------------------------
     .. code-block:: python
 
             import PAMI.AssociationRules.basic import ARWithLeverage as alg
 
             obj = alg.ARWithLeverage(iFile, minConf)
 
-            obj.startMine()
+            obj.mine()
 
             associationRules = obj.getPatterns()
 
             print("Total number of Association Rules:", len(associationRules))
 
             obj.save(oFile)
 
@@ -216,26 +231,32 @@
 
              The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
     def __init__(self, iFile, minConf, sep) -> None:
         """
-        :param inputFile: input file name or path
-        :type inputFile: str
-        :param sep:
+        :param iFile: input file name or path
+        :type iFile: str
+        :param minConf: The user can specify the minConf in float between the range of 0 to 1.
+        :type minConf: float
+        :param sep: This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+        :type sep: str
+        :return: None
         """
         self._iFile = iFile
         self._minConf = minConf
         self._finalPatterns = {}
         self._sep = sep
 
     def _readPatterns(self) -> list:
         """
         To read patterns  of leverage.
+        :return: List of patterns
+        :rtype: list
         """
         self._frequentPatterns = {}
         k = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             pattern, sup = [], []
             if self._iFile.empty:
                 print("its empty..")
@@ -269,14 +290,15 @@
                             s = '\t'.join(s)
                             self._frequentPatterns[s.strip()] = int(line[1])
                 except IOError:
                     print("File Not Found")
                     quit()
         return k
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Association rule mining process will start from here
         """
         self._startTime = _ab._time.time()
         k = self._readPatterns()
         a = _Leverage(self._frequentPatterns, k, self._minConf)
@@ -286,14 +308,31 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Association rules successfully  generated from frequent patterns ")
 
+    def mine(self) -> None:
+        """
+        Association rule mining process will start from here
+        """
+        self._startTime = _ab._time.time()
+        k = self._readPatterns()
+        a = _Leverage(self._frequentPatterns, k, self._minConf)
+        a.run()
+        self._finalPatterns = a._finalPatterns
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Association rules successfully  generated from frequent patterns ")
+
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -333,14 +372,15 @@
         return dataFrame
 
     def save(self, outFile) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the outputfile
         :type outFile: file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
@@ -366,14 +406,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = ARWithLeverage(_ab._sys.argv[1], float(_ab._sys.argv[3]), _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = ARWithLeverage(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Association Rules:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/AssociationRules/basic/ARWithLift.py` & `pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithLift.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
 #             import PAMI.AssociationRules.basic import ARWithLift as alg
 #
 #             obj = alg.ARWithLift(iFile, minConf)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             associationRules = obj.getPatterns()
 #
 #             print("Total number of Association Rules:", len(associationRules))
 #
 #             obj.save(oFile)
 #
@@ -25,21 +25,22 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -51,48 +52,50 @@
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
 
 from PAMI.AssociationRules.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 class Lift:
 
     """
-    :param  patterns: dict :
-                   Dictionary containing patterns and its support value.
-    :param  singleItems: list :
-                   List containing all the single frequent items.
-    :param  minConf: int :
-                   Minimum confidence to mine all the satisfying association rules.
-
+    :param  patterns: Dictionary containing patterns and its support value.
+    :type  patterns: dict
+    :param  singleItems: List containing all the single frequent items.
+    :type  singleItems: list
+    :param  minConf: Minimum confidence to mine all the satisfying association rules.
+    :type  minConf: int
     """
     
     def __init__(self, patterns, singleItems, minConf) -> None:
         """
         :param patterns: given frequent patterns
-        :type inputFile: dict
+        :type patterns: dict
         :param singleItems: one-length frequent patterns
         :type singleItems: list
         :param minConf: minimum confidence
         :type minConf: float
+        :return: None
         """
         self._frequentPatterns = patterns
         self._singleItems = singleItems
         self._minConf = minConf
         self._finalPatterns = {}
 
     def _generation(self, prefix, suffix) -> None:
         """
         To generate the combinations all association rules.
         :param prefix: the prefix of association rule.
         :type prefix: str
         :param suffix: the suffix of association rule.
         :type suffix: str
+        :return: None
         """
         if len(suffix) == 1:
             self._generateWithLift(prefix, suffix[0])
         for i in range(len(suffix)):
             suffix1 = suffix[:i] + suffix[i + 1:]
             prefix1 = prefix + ' ' + suffix[i]
             for j in range(i + 1, len(suffix)):
@@ -103,14 +106,16 @@
     def _generateWithLift(self, lhs, rhs)  -> float:
         """
         To find association rules satisfying user-specified minConf
         :param lhs: the prefix of association rule.
         :type lhs: str
         :param rhs: the suffix of association rule.
         :type rhs: str
+        :return: the association rule
+        :rtype: float
         """
         s = lhs + '\t' + rhs
         if self._frequentPatterns.get(s) == None:
             return 0
         minimum = self._frequentPatterns[s]
         conf_lhs = minimum / self._frequentPatterns[lhs]
         conf_rhs = minimum / self._frequentPatterns[rhs]
@@ -137,22 +142,23 @@
 
 class ARWithLift:
     """
     :Description: Association Rules are derived from frequent patterns using "lift" metric.
 
     :Reference:
 
-    :param iFile: str or df :
-                  Name of the Input file to mine the association rules
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of association rules
+    :param  oFile: str :
+                   Name of the output file to store complete set of association rules
+    :param  minConf: float :
+                   The user can specify the minConf in float between the range of 0 to 1.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
-    :param minConf: float
-                  The user can specify the minConf in float
-    :param sep: str :
-                  This variable is used to distinguish items from one another in given input file. The default seperator is tab space. However, the users can override their default seperator.
-        
         
     :Attributes:
 
 
         startTime : float
             To record the start time of the mining process
 
@@ -168,33 +174,36 @@
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
-            Format:
-                     >>> python3 ARWithLift.py <inputFile> <outputFile> <minConf> <sep>
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 ARWithLift.py <inputFile> <outputFile> <minConf> <sep>
 
-            Example:
-                     >>>  python3 ARWithLift.py sampleDB.txt patterns.txt 0.5 ' '
+      Example Usage:
 
-                     .. note:: minConf will be considered only in 0 to 1.
+      (.venv) $ python3 ARWithLift.py sampleDB.txt patterns.txt 0.5 ' '
 
+    .. note:: minConf will be considered only in 0 to 1.
     
     
     **Importing this algorithm into a python program**
     ----------------------------------------------------
     .. code-block:: python
 
             import PAMI.AssociationRules.basic import ARWithLift as alg
 
             obj = alg.ARWithLift(iFile, minConf)
 
-            obj.startMine()
+            obj.mine()
 
             associationRules = obj.getPatterns()
 
             print("Total number of Association Rules:", len(associationRules))
 
             obj.save(oFile)
 
@@ -217,29 +226,32 @@
 
              The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
     def __init__(self, iFile, minConf, sep) -> None:
         """
-        :param inputFile: input file name or path
-        :type inputFile: str
+        :param iFile: input file name or path
+        :type iFile: str
         :param minConf: minimum confidence
         :type minConf: float
         :param sep: Delimiter of input file
         :type sep: str
+        :return: None
         """
         self._iFile = iFile
         self._minConf = minConf
         self._finalPatterns = {}
         self._sep = sep
 
     def _readPatterns(self) -> list:
         """
         Reading the input file and storing all the frequent patterns and their support respectively in a frequentPatterns variable.
+        :return: list of frequent patterns and their support respectively in a frequentPatterns
+        :rtype: list
         """
         self._frequentPatterns = {}
         k = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             pattern, sup = [], []
             if self._iFile.empty:
                 print("its empty..")
@@ -273,14 +285,15 @@
                             s = '\t'.join(s)
                             self._frequentPatterns[s.strip()] = int(line[1])
                 except IOError:
                     print("File Not Found")
                     quit()
         return k
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Association rule mining process will start from here
         """
         self._startTime = _ab._time.time()
         k = self._readPatterns()
         a = Lift(self._frequentPatterns, k, self._minConf)
@@ -290,14 +303,31 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Association rules successfully  generated from frequent patterns ")
 
+    def mine(self) -> None:
+        """
+        Association rule mining process will start from here
+        """
+        self._startTime = _ab._time.time()
+        k = self._readPatterns()
+        a = Lift(self._frequentPatterns, k, self._minConf)
+        a.run()
+        self._finalPatterns = a._finalPatterns
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Association rules successfully  generated from frequent patterns ")
+
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -337,14 +367,15 @@
         return dataFrame
 
     def save(self, outFile) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the outputfile
         :type outFile: file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
@@ -370,14 +401,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = ARWithLift(_ab._sys.argv[1], float(_ab._sys.argv[3]), _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = ARWithLift(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Association Rules:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/AssociationRules/basic/RuleMiner.py` & `pami-2024.4.9.1/PAMI/AssociationRules/basic/RuleMiner.py`

 * *Files 3% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
 #             import PAMI.AssociationRules.basic import RuleMiner as alg
 #
 #             obj = alg.RuleMiner(iFile, measure, o.5, "\t")
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             associationRules = obj.getPatterns()
 #
 #             print("Total number of Association Rules:", len(associationRules))
 #
 #             obj.save(oFile)
 #
@@ -24,21 +24,22 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -47,14 +48,15 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 from PAMI.AssociationRules.basic import abstract as _ab
+from deprecated import deprecated
 
 class Confidence:
     """
     Association Rules are derived from frequent patterns using "confidence" metric.
     """
     
     def __init__(self, patterns, singleItems, threshold):
@@ -118,17 +120,20 @@
 class Lift:
     """
     Association Rules are derived from frequent patterns using "lift" metric.
     """
     
     def __init__(self, patterns, singleItems, threshold):
         """
-        :param inputFile: input file name or path
-        :type inputFile: str
-        :param sep:
+        :param patterns: given frequent patterns
+        :type patterns: dict
+        :param singleItems: one-length frequent patterns
+        :type singleItems: list
+        :param threshold: threshold for lifting rules
+        :type threshold: float
         """
         self._frequentPatterns = patterns
         self._singleItems = singleItems
         self._threshold = threshold
         self._finalPatterns = {}
         
     def _generation(self, prefix, suffix):
@@ -188,17 +193,20 @@
 class Leverage:
     """
     Association Rules are derived from frequent patterns using "leverage" metric.
     """
     
     def __init__(self, patterns, singleItems, threshold):
         """
-        :param inputFile: input file name or path
-        :type inputFile: str
-        :param sep:
+        :param patterns: given frequent patterns
+        :type patterns: dict
+        :param singleItems: one-length frequent patterns
+        :type singleItems: list
+        :param threshold: threshold for lifting rules
+        :type threshold: float
         """
         self._frequentPatterns = patterns
         self._singleItems = singleItems
         self._threshold = threshold
         self._finalPatterns = {}
         
     def _generation(self, prefix, suffix):
@@ -255,24 +263,28 @@
 class RuleMiner:
     """
     :Description: RuleMiner code is used to extract the association rules from given frequent patterns
 
     :Reference:
 
 
-    :param  inputFile: str :
-                       input file name or path
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of association rules
+    :param  oFile: str :
+                   Name of the output file to store complete set of association rules
+    :param  minConf: float :
+                   The user can specify the minConf in float between the range of 0 to 1.
     :param  frequentPattern: list or dict :
-                             frequent patterns are stored in the form of list or dictionary
+                   frequent patterns are stored in the form of list or dictionary
     :param  measure: str :
-                     condition to calculate the strength of rule
+                   condition to calculate the strength of rule
     :param  threshold: int :
-                       condition to satisfy
+                   condition to satisfy
     :param  sep: str :
-            This variable is used to distinguish items from one another in given input file. The default seperator is tab space. However, the users can override their default seperator.
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
 
         startTime : float
             To record the start time of the mining process
 
@@ -288,33 +300,36 @@
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
-                Format:
-                      >>> python3 RuleMiner.py <inputFile> <outputFile> <minConf> <sep>
+    .. code-block:: console
+
+      Format:
 
-                Example:
-                      >>>  python3 RuleMiner.py sampleDB.txt patterns.txt 0.5 ' '
+      (.venv) $ python3 RuleMiner.py <inputFile> <outputFile> <minConf> <sep>
 
-                      .. note:: minConf will be considered only in 0 to 1.
+      Example Usage:
 
+      (.venv) $ python3 RuleMiner.py sampleDB.txt patterns.txt 0.5 ' '
+
+    .. note:: minConf will be considered only in 0 to 1.
 
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
     .. code-block:: python
 
             import PAMI.AssociationRules.basic import RuleMiner as alg
 
             obj = alg.RuleMiner(iFile, measure, o.5, "\t")
 
-            obj.startMine()
+            obj.mine()
 
             associationRules = obj.getPatterns()
 
             print("Total number of Association Rules:", len(associationRules))
 
             obj.save(oFile)
 
@@ -337,15 +352,20 @@
             startMine()
     """
 
     def __init__(self, iFile, measure, threshold, sep):
         """
         :param iFile: input file name or path
         :type iFile: str
-        :param sep:
+        :param measure: measure
+        :type measure: str
+        :param threshold: threshold for lifting rules
+        :type threshold: float
+        :param sep: Delimiter of input file
+        :type sep: str
         """
         self._iFile = iFile
         self._measure = measure
         self._threshold = threshold
         self._finalPatterns = {}
         self._sep = sep
     
@@ -389,14 +409,15 @@
                             s = '\t'.join(s)
                             self._frequentPatterns[s.strip()] = int(line[1])
                 except IOError:
                     print("File Not Found")
                     quit()
         return k
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Association rule mining process will start from here
         """
         self._startTime = _ab._time.time()
         k = self._readPatterns()
         if self._measure == 'confidence':
@@ -414,14 +435,41 @@
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Association rules successfully  generated from frequent patterns ")
+
+
+    def mine(self):
+        """
+        Association rule mining process will start from here
+        """
+        self._startTime = _ab._time.time()
+        k = self._readPatterns()
+        if self._measure == 'confidence':
+            a = Confidence(self._frequentPatterns, k, self._threshold)
+            a.run()
+            self._finalPatterns = a._finalPatterns
+        if self._measure == 'lift':
+            a = Lift(self._frequentPatterns, k, self._threshold)
+            a.run()
+            self._finalPatterns = a._finalPatterns
+        if self._measure == 'leverage':
+            a = Leverage(self._frequentPatterns, k, self._threshold)
+            a.run()
+            self._finalPatterns = a._finalPatterns
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Association rules successfully  generated from frequent patterns ")
     
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
@@ -503,10 +551,11 @@
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         _ap = RuleMiner('sensorOutput.txt', "lift", 0.5, '\t')
         _ap.startMine()
+        _ap.mine()
         _ap.save('output.txt')
         _ap.printResults()
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/AssociationRules/basic/abstract.py` & `pami-2024.4.9.1/PAMI/AssociationRules/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/correlatedPattern/__init__.py` & `pami-2024.4.9.1/PAMI/correlatedPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/correlatedPattern/basic/CoMine.py` & `pami-2024.4.9.1/PAMI/correlatedPattern/basic/CoMine.py`

 * *Files 3% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # --------------------------------------------------------
 #
 #
 #             from PAMI.correlatedPattern.basic import CoMine as alg
 #
 #             obj = alg.CoMine(iFile, minSup, minAllConf, sep)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             Rules = obj.getPatterns()
 #
 #             print("Total number of  Patterns:", len(Patterns))
 #
 #             obj.save(oFile)
 #
@@ -31,15 +31,15 @@
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -52,14 +52,15 @@
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.correlatedPattern.basic import abstract as _ab
 import pandas as _pd
 from typing import List, Dict, Tuple, Union
+from deprecated import deprecated
 
 class _Node:
     """
     A class used to represent the node of correlatedPatternTree
 
 
     :Attributes:
@@ -305,15 +306,15 @@
     --------------------------------------------------------------------------------
     .. code-block:: python
 
             from PAMI.correlatedPattern.basic import CoMine as alg
 
             obj = alg.CoMine(iFile, minSup, minAllConf,sep)
 
-            obj.startMine()
+            obj.mine()
 
             patterns = obj.getPatterns()
 
             print("Total number of  Patterns:", len(patterns))
 
             obj.savePatterns(oFile)
 
@@ -556,15 +557,16 @@
                         path = path.nodeLink
                     treeBeta = _Tree()
                     for k in prefixPaths:
                         treeBeta.addPrefixPath(k, mapSupportBeta, self._minSup)
                     if len(treeBeta.root.child) > 0:
                         treeBeta.createHeaderList(mapSupportBeta, self._minSup)
                         self._correlatedPatternGrowthGenerate(treeBeta, prefix, prefixLength + 1, mapSupportBeta)
-    
+
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         main method to start
         """
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
@@ -590,14 +592,47 @@
         self._endTime = _ab._time.time()
         self._memoryUSS = float()
         self._memoryRSS = float()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
+    def mine(self) -> None:
+        """
+        main method to start
+        """
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._tree = _Tree()
+        self._finalPatterns = {}
+        self._correlatedOneItem()
+        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v >= self._minSup}
+        _itemSetBuffer = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        for i in self._Database:
+            _transaction = []
+            for j in i:
+                if j in _itemSetBuffer:
+                    _transaction.append(j)
+            _transaction.sort(key=lambda val: self._mapSupport[val], reverse=True)
+            self._tree.addTransaction(_transaction)
+        self._tree.createHeaderList(self._mapSupport, self._minSup)
+        if len(self._tree.headerList) > 0:
+            self._itemSetBuffer = []
+            self._correlatedPatternGrowthGenerate(self._tree, self._itemSetBuffer, 0, self._mapSupport)
+        print("Correlated patterns were generated successfully using CoMine algorithm")
+        self._endTime = _ab._time.time()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -676,14 +711,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = CoMine(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]), _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = CoMine(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]))
         _ap.startMine()
+        _ap.mine()
         print("Total number of Correlated-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/correlatedPattern/basic/CoMinePlus.py` & `pami-2024.4.9.1/PAMI/correlatedPattern/basic/CoMinePlus.py`

 * *Files 5% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 # **Importing this algorithm into a python program**
 # -----------------------------------------------
 #
 #             from PAMI.correlatedPattern.basic import CoMinePlus as alg
 #
 #             obj = alg.CoMinePlus(iFile, minSup, minAllConf, sep)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             correlatedPattern = obj.getPatterns()
 #
 #             print("Total number of correlated Patterns:", len(correlatedPattern))
 #
 #             obj.save(oFile)
 #
@@ -31,15 +31,15 @@
 #
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -51,14 +51,15 @@
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 """
 
 from PAMI.correlatedPattern.basic import abstract as _ab
 import pandas as _pd
 from typing import List, Dict, Tuple, Set, Union, Any, Optional, Generator
+from deprecated import deprecated
 
 
 class _Node:
     """
     A class used to represent the node of correlatedPatternTree
 
     :Attributes:
@@ -317,15 +318,15 @@
     -----------------------------------------------------------------
     .. code-block:: python
 
             from PAMI.correlatedPattern.basic import CoMinePlus as alg
 
             obj = alg.CoMinePlus(iFile, minSup, minAllConf, sep)
 
-            obj.startMine()
+            obj.mine()
 
             correlatedPatterns = obj.getPatterns()
 
             print("Total number of correlated patterns:", len(correlatedPatterns))
 
             obj.save(oFile)
 
@@ -563,14 +564,15 @@
         if type(value) is str:
             if '.' in value:
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Main program to start the operation
         """
 
         self._startTime = _ab._time.time()
         if self._iFile is None:
@@ -599,14 +601,50 @@
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryRSS = float()
         self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
+    def mine(self) -> None:
+        """
+        Main program to start the operation
+        """
+
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        self._tree = _Tree()
+        self._minSup = self._convert(self._minSup)
+        self._correlatedOneItem()
+        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v >= self._minSup}
+        _itemSetBuffer = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        for i in self._Database:
+            _transaction = []
+            for j in i:
+                if j in _itemSetBuffer:
+                    _transaction.append(j)
+            _transaction.sort(key=lambda val: self._mapSupport[val], reverse=True)
+            self._tree.addTransaction(_transaction)
+        self._tree.createHeaderList(self._mapSupport, self._minSup)
+        if len(self._tree.headerList) > 0:
+            self._itemSetBuffer = []
+            self._correlatedPatternGrowthGenerate(self._tree, self._itemSetBuffer, 0, self._mapSupport, self._minAllConf)
+        print("Correlated Frequent patterns were generated successfully using CorrelatedPatternGrowth algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -692,14 +730,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = CoMinePlus(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]), _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = CoMinePlus(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]))
         _ap.startMine()
+        _ap.mine()
         _correlatedPatterns = _ap.getPatterns()
         print("Total number of Correlated-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
```

### Comparing `pami-2024.3.9.2/PAMI/correlatedPattern/basic/__init__.py` & `pami-2024.4.9.1/PAMI/correlatedPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/correlatedPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/correlatedPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/coveragePattern/basic/CMine.py` & `pami-2024.4.9.1/PAMI/coveragePattern/basic/CMine.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # ----------------------------------------------------
 #
 #
 #             from PAMI.coveragePattern.basic import CMine as alg
 #
 #             obj = alg.CMine(iFile, minRF, minCS, maxOR, seperator)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             coveragePattern = obj.getPatterns()
 #
 #             print("Total number of coverage Patterns:", len(coveragePattern))
 #
 #             obj.save(oFile)
 #
@@ -31,15 +31,15 @@
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -54,14 +54,15 @@
 """
 
 
 
 
 from PAMI.coveragePattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 class CMine(_ab._coveragePatterns):
     """
 
     :Description:  CMine algorithms aims to discover the coverage patterns in transactional databases.
 
     :Reference:    Bhargav Sripada, Polepalli Krishna Reddy, Rage Uday Kiran:
@@ -123,15 +124,15 @@
     ----------------------------------------------------
     .. code-block:: python
 
             from PAMI.coveragePattern.basic import CMine as alg
 
             obj = alg.CMine(iFile, minRF, minCS, maxOR, seperator)
 
-            obj.startMine()
+            obj.mine()
 
             coveragePattern = obj.getPatterns()
 
             print("Total number of coverage Patterns:", len(coveragePattern))
 
             obj.save(oFile)
 
@@ -291,14 +292,15 @@
         """
         tidData = list(coverageItems.items())
         length = len(tidData)
         for i in range(length):
             #print(i,tidData[i][0])
             self.genPatterns(tidData[i],tidData[i+1:length])
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """ Main method to start """
 
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         self._creatingItemSets()
@@ -314,14 +316,37 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Coverage patterns were generated successfully using CMine  algorithm")
 
+    def mine(self) -> None:
+        """ Main method to start """
+
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self._creatingItemSets()
+        self._minCS = self._convert(self._minCS)
+        self._minRF =  self._convert(self._minRF)
+        self._maxOR = self._convert(self._maxOR)
+        coverageItems = self.creatingCoverageItems()
+        self._finalPatterns = {k: len(v) for k, v in coverageItems.items()}
+        coverageItemsBitset = self.tidToBitset(coverageItems)
+        self.generateAllPatterns(coverageItemsBitset)
+        self.save('output.txt')
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Coverage patterns were generated successfully using CMine  algorithm")
+
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
@@ -390,14 +415,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 7 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 7:
             _ap = CMine(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         if len(_ab._sys.argv) == 6:
             _ap = CMine(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
+        _ap.mine()
         print("Total number of coverage Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/coveragePattern/basic/CPPG.py` & `pami-2024.4.9.1/PAMI/coveragePattern/basic/CPPG.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # -------------------------------------------------------
 #
 #
 #             from PAMI.coveragePattern.basic import CPPG as alg
 #
 #             obj = alg.CPPG(iFile, minRF, minCS, maxOR)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             coveragePattern = obj.getPatterns()
 #
 #             print("Total number of coverage Patterns:", len(coveragePattern))
 #
 #             obj.save(oFile)
 #
@@ -31,15 +31,15 @@
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -52,14 +52,15 @@
      Copyright (C)  2021 Rage Uday Kiran
      
 """
 
 from PAMI.coveragePattern.basic import abstract as _ab
 import pandas as pd
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 
 _maxPer = float()
 _minSup = float()
 _lno = int()
 
 
@@ -129,15 +130,15 @@
 
     .. code-block:: python
 
             from PAMI.coveragePattern.basic import CPPG as alg
 
             obj = alg.CPPG(iFile, minRF, minCS, maxOR)
 
-            obj.startMine()
+            obj.mine()
 
             coveragePattern = obj.getPatterns()
 
             print("Total number of coverage Patterns:", len(coveragePattern))
 
             obj.save(oFile)
 
@@ -347,14 +348,15 @@
             if '.' in value:
                 value = float(value)
                 value = value
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """ Mining process will start from this function
         """
 
         #global _minSup, _maxPer, _lno
         self._startTime = _ab._time.time()
         if self._iFile is None:
@@ -386,14 +388,53 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Coverage patterns were generated successfully using CPPG algorithm ")
 
+    def mine(self) -> None:
+        """ Mining process will start from this function
+        """
+
+        #global _minSup, _maxPer, _lno
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minRF is None:
+            raise Exception("Please enter the Relative Frequency")
+        if self._maxOR is None:
+            raise Exception("Please enter the Overlap Ratio")
+        if self._minCS is None:
+            raise Exception("Please enter the Coverage Ratio")
+        self._creatingItemSets()
+        self._minRF = self._convert(self._minRF)
+        self._maxOR = self._convert(self._maxOR)
+        self._minCS = self._convert(self._minCS)
+        if self._minRF > len(self._Database) or self._minCS > len(self._Database) or self._maxOR > len(self._Database):
+            raise Exception("Please enter the constraints in range between 0 to 1")
+        generatedItems, pfList = self._coverageOneItem()
+        self._finalPatterns = {k: v for k, v in generatedItems.items()}
+        updatedDatabases = self._updateDatabases(pfList)
+        proData = self._buildProjectedDatabase(updatedDatabases, pfList)
+        for x, y in proData.items():
+            uniqueItems = [x]
+            for i in y:
+                for j in i:
+                    if j not in uniqueItems:
+                        uniqueItems.append(j)
+            self._generateFrequentPatterns(uniqueItems)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Coverage patterns were generated successfully using CPPG algorithm ")
+
     def getMemoryUSS(self) -> float:
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -465,14 +506,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
         if len(_ab._sys.argv) == 7:
             _ap = CPPG(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         if len(_ab._sys.argv) == 6:
             _ap = CPPG(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Coverage Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:",  _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/coveragePattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/coveragePattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/DF2DB/DF2DB.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/DF2DB.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,16 +12,18 @@
 #
 #             obj.getTemporal("outputFileName") # To create temporal database
 #
 #             obj.getUtility("outputFileName") # To create utility database
 #
 
 
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -62,15 +64,15 @@
             obj = db.DF2DB(idf, ">=", 16, "sparse/dense")
 
             obj.getTransactional("outputFileName") # To create transactional database
 
             obj.getTemporal("outputFileName") # To create temporal database
 
             obj.getUtility("outputFileName") # To create utility database
-        """
+    """
 
 
     def __init__(self, inputDF, thresholdValue, condition, DFtype='sparse') -> None:
         self.inputDF = inputDF
         self.thresholdValue = thresholdValue
         self.condition = condition
         self.DFtype = DFtype.lower()
@@ -83,34 +85,37 @@
 
     def getTransactionalDatabase(self, outputFile) -> str:
         """
         create transactional database and return outputFileName
         :param outputFile: file name or path to store database
         :type outputFile: str
         :return: outputFile name
+        :rtype: str
         """
         self.DF2DB.createTransactional(outputFile)
         return self.DF2DB.getFileName()
 
     def getTemporalDatabase(self, outputFile) -> str:
         """
         create temporal database and return outputFile name
         :param outputFile: file name or path to store database
         :type outputFile: str
         :return: outputFile name
+        :rtype: str
         """
         self.DF2DB.createTemporal(outputFile)
         return self.DF2DB.getFileName()
 
     def getUtilityDatabase(self, outputFile) -> str:
         """
         create utility database and return outputFile name
         :param outputFile:  file name or path to store database
         :type outputFile: str
         :return: outputFile name
+        :rtype: str
         """
         self.DF2DB.createUtility(outputFile)
         return self.DF2DB.getFileName()
 
 
 if __name__ == '__main__':
     obj = DF2DB(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])
```

### Comparing `pami-2024.3.9.2/PAMI/extras/DF2DB/DF2DBPlus.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/DF2DBPlus.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,17 +9,20 @@
 #
 #             obj.getTransactional("outputFileName") # To create a transactional database
 #
 #             obj.getTDB("outputFileName")   # To create a temporal database
 #
 #             obj.getUDB("outputFileName")    # To create a utility database
 #
-#
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -80,33 +83,36 @@
 
     def getTransactional(self, outputFile) -> str:
         """
         create transactional database and return outputFileName
         :param outputFile: file name or path to store database
         :type outputFile: str
         :return: outputFile name
+        :rtype: str
         """
         self.DF2DB.createTransactional(outputFile)
         return self.DF2DB.getFileName()
 
     def getTDB(self, outputFile) -> str:
         """
         create temporal database and return outputFile name
         :param outputFile: file name or path to store database
         :type outputFile: str
         :return: outputFile name
+        :rtype: str
         """
         self.DF2DB.createTemporal(outputFile)
         return self.DF2DB.getFileName()
 
     def getUDB(self, outputFile) -> str:
         """
         create utility database and return outputFile name
         :param outputFile:  file name or path to store database
         :type outputFile: str
         :return: outputFile name
+        :rtype: str
         """
         self.DF2DB.createUtility(outputFile)
         return self.DF2DB.getFileName()
 if __name__ == '__main__':
     obj = DF2DBPlus(sys.argv[1], sys.argv[2], sys.argv[3])
     obj.getTransactional(sys.argv[4])
```

### Comparing `pami-2024.3.9.2/PAMI/extras/DF2DB/DenseFormatDF.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/DenseFormatDF.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,17 +14,22 @@
 #             obj.convert2TemporalDatabase("outputFileName") # To create temporal database
 #
 #             obj.convert2MultipleTimeSeries("outputFileName") # To create Mutliple TimeSeries database
 #
 #             obj.convert2UtilityDatabase("outputFileName") # To create utility database
 #
 #             obj.getFileName("outputFileName") # To get file name of the database
+#
+
+
+
+
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -92,20 +97,25 @@
 
     def convert2TransactionalDatabase(self, outputFile: str, condition: str, thresholdValue: Union[int, float]) -> None:
         """
         :Description: Create transactional data base
 
         :Attributes:
 
-             :param outputFile: str :
-                  Write transactional database into outputFile
-             :param condition: str :
-                It is condition to judge the value in dataframe
-             :param thresholdValue: int or float :
-                User defined value.
+             :param outputFile: Write transactional database into outputFile
+
+             :type outputFile: str
+
+             :param condition: It is condition to judge the value in dataframe
+
+             :type condition: str
+
+             :param thresholdValue: User defined value.
+
+             :type thresholdValue: Union[int, float]
         """
 
 
         self.outputFile = outputFile
         with open(outputFile, 'w') as f:
             if condition not in condition_operator:
                 print('Condition error')
@@ -121,23 +131,27 @@
                         f.write(f'{transaction[0]}')
                     else:
                         continue
                     f.write('\n')
 
     def convert2TemporalDatabase(self, outputFile: str, condition: str, thresholdValue: Union[int, float]) -> None:
         """
-         :Description: Create temporal database
+        :Description: Create temporal database
 
-         :param outputFile: str :
-                 Write temporal database into outputFile
-         :param condition: str :
-            It is condition to judge the value in dataframe
-         :param thresholdValue: int or float :
-            User defined value.
+        :param outputFile: Write temporal database into outputFile
+
+        :type outputFile: str
+
+        :param condition: It is condition to judge the value in dataframe
 
+        :type condition: str
+
+        :param thresholdValue: User defined value.
+
+        :type thresholdValue: Union
         """
 
         self.outputFile = outputFile
         with open(outputFile, 'w') as f:
             if condition not in condition_operator:
                 print('Condition error')
             else:
@@ -154,25 +168,29 @@
                     else:
                         continue
                     f.write('\n')
 
     def convert2MultipleTimeSeries(self, interval: int, outputFile: str, condition: str,
                                    thresholdValue: Union[int, float]) -> None:
         """
-         :Description: Create the multiple time series database.
+        :Description: Create the multiple time series database.
 
-         :param outputFile:  str :
-                     Write multiple time series database into outputFile.
-        :param interval: int:
-                    Breaks the given timeseries into intervals.
-        :param condition: str :
-            It is condition to judge the value in dataframe
-        :param thresholdValue: int or float :
-            User defined value.
+        :param outputFile:  Write multiple time series database into outputFile.
+
+        :type outputFile:  str
 
+        :param interval: Breaks the given timeseries into intervals.
+
+        :type interval: int
+
+        :param condition: It is condition to judge the value in dataframe
+
+        :param thresholdValue: User defined value.
+
+        :type thresholdValue: int or float
         """
         self.outputFile = outputFile
         writer = open(self.outputFile, 'w+')
         # with open(self.outputFile, 'w+') as f:
         count = 0
         tids = []
         items = []
@@ -229,18 +247,21 @@
                         f.write(f':{tt}')
                     else:
                         continue
                     f.write('\n')
 
     def convert2UtilityDatabase(self, outputFile: str) -> None:
         """
-         :Description: Create the utility database.
+        :Description: Create the utility database.
+
+        :param outputFile: Write utility database into outputFile
+
+        :type outputFile: str
 
-         :param outputFile:  str :
-                     Write utility database into outputFile
+        :return: None
         """
 
         self.outputFile = outputFile
         with open(self.outputFile, 'w') as f:
             for tid in self.tids:
                 df = self.inputDF.loc[tid].dropna()
                 f.write(f'{df.index[0]}')
@@ -252,14 +273,15 @@
                 for item in df.index[1:]:
                     f.write(f'\t{df.at[item]}')
                 f.write('\n')
 
     def getFileName(self) -> str:
         """
         :return: outputFile name
+        :rtype: str
         """
 
         return self.outputFile
 
 # Dataframes do not run from a terminal
 
 # if __name__ == '__main__':
```

### Comparing `pami-2024.3.9.2/PAMI/extras/DF2DB/SparseFormatDF.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/SparseFormatDF.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,18 +13,20 @@
 #
 #             obj.createTemporal("outputFileName") # To create temporal database
 #
 #             obj.createUtility("outputFileName") # To create utility database
 #
 #             obj.getFileName("outputFileName") # To get file name of the database
 #
-#
-#
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -90,29 +92,30 @@
         self.df = self.df.groupby('tid')['item'].apply(list)
 
     def createTransactional(self, outputFile: str) -> None:
         """
         Create transactional data base
         :param outputFile: Write transactional data base into outputFile
         :type outputFile: str
-
+        :return: None
         """
         self.outputFile = outputFile
         with open(self.outputFile, 'w') as f:
             for line in self.df:
                 f.write(f'{line[0]}')
                 for item in line[1:]:
                     f.write(f',{item}')
                 f.write('\n')
 
     def createTemporal(self, outputFile: str) -> None:
         """
         Create temporal data base
         :param outputFile: Write temporal data base into outputFile
         :type outputFile: str
+        :return: None
         """
 
         self.outputFile = outputFile
         with open(self.outputFile, 'w') as f:
             for tid in self.df.index:
                 f.write(f'{tid}')
                 for item in self.df[tid]:
@@ -120,14 +123,15 @@
                 f.write('\n')
 
     def createUtility(self, outputFile: str) -> None:
         """
         Create the utility database.
         :param outputFile: Write utility database into outputFile
         :type outputFile: str
+        :return: None
         """
 
         self.outputFile = outputFile
         items = self.inputDF.groupby(level=0)['item'].apply(list)
         values = self.inputDF.groupby(level=0)['value'].apply(list)
         sums = self.inputDF.groupby(level=0)['value'].sum()
         index = list(items.index)
```

### Comparing `pami-2024.3.9.2/PAMI/extras/DF2DB/createTDB.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/createTDB.py`

 * *Files 0% similar despite different names*

```diff
@@ -5,15 +5,17 @@
 #
 #             from PAMI.extras.DF2DB import createTDB as ct
 #
 #             obj = ct.createTDB(idf, ">=", 16)
 #
 #             obj.save(oFile)
 #
-#
+
+
+
 
 import sys
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
```

### Comparing `pami-2024.3.9.2/PAMI/extras/DF2DB/denseDF2DBPlus.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/denseDF2DBPlus.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,18 +13,20 @@
 #
 #             obj.createTemporal("outputFileName") # To create temporal database
 #
 #             obj.createUtility("outputFileName") # To create utility database
 #
 #             obj.getFileName("outputFileName") # To get file name of the database
 #
-#
-#
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -65,15 +67,14 @@
 
             obj.createTemporal("outputFileName") # To create temporal database
 
             obj.createUtility("outputFileName") # To create utility database
 
             obj.getFileName("outputFileName") # To get file name of the database
 
-
     """
 
     def __init__(self, inputDF, thresholdConditionDF) -> None:
         self.inputDF = inputDF.T
         self.thresholdConditionDF = thresholdConditionDF
         self.tids = []
         self.items = []
@@ -84,14 +85,15 @@
 
 
     def createTransactional(self, outputFile: str) -> None:
         """
         Create transactional data base
         :param outputFile: Write transactional data base into outputFile
         :type outputFile: str
+        :return: None
         """
 
         self.outputFile = outputFile
         with open(outputFile, 'w') as f:
             for tid in self.tids:
                 transaction = [item for item in self.items if
                                (self.df.at[item, 'condition'] == '>' and self.df.at[item, tid] > self.df.at[item, 'threshold']) or
@@ -113,14 +115,15 @@
 
 
     def createTemporal(self, outputFile: str) -> None:
         """
         Create temporal data base
         :param outputFile: Write temporal data base into outputFile
         :type outputFile: str
+        :return: None
         """
 
         self.outputFile = outputFile
         with open(outputFile, 'w') as f:
             for tid in self.tids:
                 transaction = [item for item in self.items if
                                (self.df.at[item, 'condition'] == '>' and self.df.at[item, tid] > self.df.at[item, 'threshold']) or
@@ -141,14 +144,15 @@
                 f.write('\n')
 
     def createUtility(self, outputFile: str) -> None:
         """
         Create the utility data base.
         :param outputFile: Write utility data base into outputFile
         :type outputFile: str
+        :return: None
         """
 
         self.outputFile = outputFile
         with open(self.outputFile, 'w') as f:
             for tid in self.tids:
                 df = self.inputDF.loc[tid].dropna()
                 f.write(f'{df.index[0]}')
```

### Comparing `pami-2024.3.9.2/PAMI/extras/DF2DB/denseDF2DB_dump.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/denseDF2DB_dump.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,16 +13,18 @@
 #
 #             obj.createTemporal("outputFileName") # To create temporal database
 #
 #             obj.createUtility("outputFileName") # To create utility database
 #
 #             obj.getFileName("outputFileName") # To get file name of the database
 #
-#
-#
+
+
+
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -83,16 +85,19 @@
         self.tids = list(self.inputDF.index)
 
 
     def createTransactional(self, outputFile: str) -> None:
         """
         :Description: Create transactional data base
 
-        :param outputFile: str :
-            Write transactional data base into outputFile
+        :param outputFile: Write transactional data base into outputFile
+
+        :type outputFile: str
+
+        :return: None
 
         """
 
         self.outputFile = outputFile
         with open(outputFile, 'w') as f:
             if self.condition == '>':
                 for tid in self.tids:
@@ -172,19 +177,21 @@
             else:
                 print('Condition error')
 
 
 
     def createTemporal(self, outputFile: str) -> None:
         """
-         :Description: Create temporal data base
+        :Description: Create temporal data base
+
+        :param outputFile: Write temporal data base into outputFile
 
-         :param outputFile: str :
-                 Write temporal data base into outputFile
+        :type outputFile: str
 
+        :return: None
         """
 
         self.outputFile = outputFile
         with open(outputFile, 'w') as f:
             if self.condition == '>':
                 for tid in self.tids:
                     transaction = [item for item in self.items if self.inputDF.at[tid, item] > self.thresholdValue]
@@ -271,17 +278,19 @@
                 print('Condition error')
 
     def createUtility(self, outputFile: str) -> None:
         """
 
         :Description: Create the utility database.
 
-        :param outputFile:  str :
+        :param outputFile:  Write utility database into outputFile
 
-             Write utility database into outputFile
+        :type outputFile: str
+
+        :return: None
 
         """
 
         self.outputFile = outputFile
         with open(self.outputFile, 'w') as f:
             for tid in self.tids:
                 df = self.inputDF.loc[tid].dropna()
@@ -293,14 +302,16 @@
                 for item in df.index[1:]:
                     f.write(f'\t{df.at[item]}')
                 f.write('\n')
 
     def getFileName(self) -> str:
         """
         :return: outputFile name
+
+        :rtype: str
         """
 
         return self.outputFile
 
 
 if __name__ == '__main__':
```

### Comparing `pami-2024.3.9.2/PAMI/extras/DF2DB/sparseDF2DBPlus.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/sparseDF2DBPlus.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,16 +14,20 @@
 #             obj.createTemporal("outputFileName") # To create temporal database
 #
 #             obj.createUtility("outputFileName") # To create utility database
 #
 #             obj.getFileName("outputFileName") # To get file name of the database
 #
 #
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -62,15 +66,15 @@
             obj.createTransactional("outputFileName") # To create transactional database
 
             obj.createTemporal("outputFileName") # To create temporal database
 
             obj.createUtility("outputFileName") # To create utility database
 
              obj.getFileName("outputFileName") # To get file name of the database
-         """
+    """
 
 
     def __init__(self, inputDF, thresholdConditionDF) -> None:
         self.inputDF = inputDF
         self.thresholdConditionDF = thresholdConditionDF
         self.outputFile = ''
         self.df = pd.merge(self.inputDF, self.thresholdConditionDF, left_on='item', right_index=True)
@@ -82,14 +86,15 @@
         self.df = self.df.groupby(level=0)['item'].apply(list)
 
     def createTransactional(self, outputFile: str) -> None:
         """
         Create transactional data base
         :param outputFile: Write transactional data base into outputFile
         :type outputFile: str
+        :return: None
         """
 
         self.outputFile = outputFile
         with open(self.outputFile, 'w') as f:
             for line in self.df:
                 f.write(f'{line[0]}')
                 for item in line[1:]:
@@ -97,14 +102,15 @@
                 f.write('\n')
 
     def createTemporal(self, outputFile: str) -> None:
         """
         Create temporal data base
         :param outputFile: Write temporal data base into outputFile
         :type outputFile: str
+        :return: None
         """
 
         self.outputFile = outputFile
         with open(self.outputFile, 'w') as f:
             for tid in self.df.index:
                 f.write(f'{tid}')
                 for item in self.df[tid]:
@@ -112,14 +118,15 @@
                 f.write('\n')
 
     def createUtility(self, outputFile: str) -> None:
         """
         Create the utility data base.
         :param outputFile: Write utility data base into outputFile
         :type outputFile: str
+        :return: None
         """
 
         self.outputFile = outputFile
         items = self.inputDF.groupby(level=0)['item'].apply(list)
         values = self.inputDF.groupby(level=0)['value'].apply(list)
         sums = self.inputDF.groupby(level=0)['value'].sum()
         index = list(items.index)
```

### Comparing `pami-2024.3.9.2/PAMI/extras/calculateMISValues/usingBeta.py` & `pami-2024.4.9.1/PAMI/extras/calculateMISValues/usingBeta.py`

 * *Files 1% similar despite different names*

```diff
@@ -6,19 +6,22 @@
 #             from PAMI.extras.calculateMISValues import usingBeta as db
 #
 #             obj = db.usingBeta(iFile, 16, "\t")
 #
 #             obj.getPatternsAsDataFrame("outputFileName") # To create patterns as dataframes
 #
 #             obj.save(oFile)
+#
+
+
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -148,15 +151,16 @@
             dataFrame = _pd.DataFrame(data, columns=['Items', 'MIS'])
         return dataFrame
 
     def save(self, outFile: str) -> None:
         """
         Complete set of items and its respective minimum support values will be loaded in to an output file
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x + "\t" + str(int(y))
             writer.write("%s \n" % patternsAndSupport)
```

### Comparing `pami-2024.3.9.2/PAMI/extras/calculateMISValues/usingSD.py` & `pami-2024.4.9.1/PAMI/extras/calculateMISValues/usingSD.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,18 +7,20 @@
 #
 #             obj = db.usingSD(iFile, 16, "\t")
 #
 #             obj.getPatterns("outputFileName") # To create patterns as dataframes
 #
 #             obj.save(oFile)
 #
-#
+
+
+
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -38,14 +40,16 @@
 class usingSD():
     """
 
     :Description: This code is used to calculate multiple minimum support of items in the the given database. Output can be stored in file or as as dataframe.
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
+    :param sd: int :
+                   SD of items to mine complete set of frequent patterns.
     :param  threshold: int :
                    The user can specify threshold either in count or proportion of database size. If the program detects the data type of threshold is integer, then it treats threshold is expressed in count.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     **Importing this algorithm into a python program**
     --------------------------------------------------------
@@ -105,14 +109,15 @@
                 except IOError:
                     print("File Not Found")
 
     def _creatingFrequentItems(self) -> tuple:
         """
         This function creates frequent items from _database.
         :return: frequentTidData that stores frequent items and their tid list.
+        :rtype: tuple
         """
         tidData = {}
         self._lno = 0
         for transaction in self._Database:
             self._lno = self._lno + 1
             for item in transaction:
                 if item not in tidData:
@@ -147,15 +152,16 @@
             dataFrame = _pd.DataFrame(data, columns=['Items', 'MIS'])
         return dataFrame
 
     def save(self, outFile: str) -> None:
         """
         Complete Items and its respective calculated minimum support values will be loaded in to an output file
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x + "\t" + str(y)
             writer.write("%s \n" % patternsAndSupport)
```

### Comparing `pami-2024.3.9.2/PAMI/extras/convertMultiTSIntoFuzzy.py` & `pami-2024.4.9.1/PAMI/extras/convertMultiTSIntoFuzzy.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/dbStats/FuzzyDatabase.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/FuzzyDatabase.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,17 +9,20 @@
 #
 #             obj.run()
 #
 #             obj.printStats()
 #
 #             obj.save(oFile)
 #
-#
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -96,14 +99,16 @@
 
 
     """
     def __init__(self, inputFile: str, sep: str='\t'):
         """
         :param inputFile: input file name or path
         :type inputFile: str
+        :param sep: separator
+        :type sep: str
         """
         self.inputFile = inputFile
         self.database = {}
         self.lengthList = []
         self.utility = {}
         self.sep = sep
 
@@ -169,80 +174,90 @@
         self.lengthList = [len(s) for s in self.database.values()]
         self.utility = {k: v for k, v in sorted(self.utility.items(), key=lambda x:x[1], reverse=True)}
 
     def getDatabaseSize(self) -> int:
         """
         get the size of database
         :return: dataset size
+        :rtype: int
         """
         return len(self.database)
 
     def getTotalNumberOfItems(self) -> int:
         """
         get the number of items in database.
         :return: number of items
+        :rtype: int
         """
         return len(self.getSortedListOfItemFrequencies())
 
     def getMinimumTransactionLength(self) -> int:
         """
         get the minimum transaction length
         :return: minimum transaction length
+        :rtype: int
         """
         return min(self.lengthList)
 
     def getAverageTransactionLength(self) -> float:
         """
         get the average transaction length. It is sum of all transaction length divided by database length.
         :return: average transaction length
+        :rtype: float
         """
         totalLength = sum(self.lengthList)
         return totalLength / len(self.database)
 
     def getMaximumTransactionLength(self) -> int:
         """
         get the maximum transaction length
         :return: maximum transaction length
+        :rtype: int
         """
         return max(self.lengthList)
 
     def getStandardDeviationTransactionLength(self) -> float:
         """
         get the standard deviation transaction length
         :return: standard deviation transaction length
+        :rtype: float
         """
         return statistics.pstdev(self.lengthList)
 
     def getVarianceTransactionLength(self) -> float:
         """
         get the variance transaction length
         :return: variance transaction length
+        :rtype: float
         """
         return statistics.variance(self.lengthList)
 
     def getNumberOfItems(self) -> int:
         """
         get the number of items in database.
         :return: number of items
+        :rtype: int
         """
         return len(self.getSortedListOfItemFrequencies())
 
     def getSparsity(self) -> float:
         # percentage of 0 dense dataframe
         """
         get the sparsity of database
         :return: dataset sparsity
+        :rtype: float
         """
         matrixSize = self.getDatabaseSize()*len(self.getSortedListOfItemFrequencies())
         return (matrixSize - sum(self.getSortedListOfItemFrequencies().values())) / matrixSize
 
     def getSortedListOfItemFrequencies(self) -> dict:
         """
         get sorted list of item frequencies
         :return: item frequencies
+        :rtype: dict
         """
         itemFrequencies = {}
         rangeFrequencies = {}
         for tid in self.database:
             for item in self.database[tid]:
                 itemFrequencies[item] = itemFrequencies.get(item, 0)
                 itemFrequencies[item] += 1
@@ -262,65 +277,72 @@
             rangeFrequencies[va] = values[i]
         return rangeFrequencies
 
     def getTransanctionalLengthDistribution(self) -> dict:
         """
         get transaction length
         :return: transactional length
+        :rtype: dict
         """
         transactionLength = {}
         for length in self.lengthList:
             transactionLength[length] = transactionLength.get(length, 0)
             transactionLength[length] += 1
         return {k: v for k, v in sorted(transactionLength.items(), key=lambda x:x[0])}
 
     def save(self, data: dict, outputFile: str) -> None:
         """
         store data into outputFile
         :param data: input data
         :type data: dict
         :param outputFile: output file name or path to store
         :type outputFile: str
+        :return: None
         """
         with open(outputFile, 'w') as f:
             for key, value in data.items():
                 f.write(f'{key}\t{value}\n')
 
     def getTotalUtility(self) -> int:
         """
         get sum of utility
         :return: total utility
+        :rtype: int
         """
         return sum(list(self.utility.values()))
 
     def getMinimumUtility(self) -> int:
         """
         get the minimum utility
         :return: min utility
+        :rtype: int
         """
         return min(list(self.utility.values()))
 
     def getAverageUtility(self) -> float:
         """
         get the average utility
         :return: average utility
+        :rtype: float
         """
         return sum(list(self.utility.values())) / len(self.utility)
 
     def getMaximumUtility(self) -> int:
         """
         get the maximum utility
         :return: max utility
+        :rtype: int
         """
         return max(list(self.utility.values()))
 
     def getSortedUtilityValuesOfItem(self) -> dict:
         """
         get sorted utility value each item. key is item and value is utility of item
         :return: sorted dictionary utility value of item
+        :rtype: dict
         """
         return self.utility
     
     def printStats(self) -> None:
         print(f'Database size : {self.getDatabaseSize()}')
         print(f'Number of items : {self.getTotalNumberOfItems()}')
         print(f'Minimum Transaction Size : {self.getMinimumTransactionLength()}')
```

### Comparing `pami-2024.3.9.2/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,27 +1,29 @@
-# MultipleTimeSeriesFuzzyDatabaseStats is class to get statistics of multiple time series fuzzy database.
+# MultipleTimeSeriesFuzzyDatabaseStats is class to get stats of multiple time series fuzzy database.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #             from PAMI.extras.dbStats import MultipleTimeSeriesFuzzyDatabaseStats as db
 #
 #             obj = db.MultipleTimeSeriesDatabaseStats(iFile, "\t")
 #
 #             obj.run()
 #
 #             obj.printStats()
 #
 #             obj.save(oFile)
 #
-#
+
+
+
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -39,15 +41,15 @@
 from urllib.request import urlopen
 import sys
 import PAMI.extras.graph.plotLineGraphFromDictionary as plt
 
 
 class MultipleTimeSeriesFuzzyDatabaseStats:
     """
-    :Description:  MultipleTimeSeriesDatabaseStats is class to get statistics of multiple time series fuzzy database.
+    :Description:  MultipleTimeSeriesDatabaseStats is class to get stats of multiple time series fuzzy database.
 
     :Attributes:
 
         :param inputFile: file :
             input file path
         :param sep: str
             separator in file. Default is tab space.
@@ -79,15 +81,15 @@
         getSortedListOfItemFrequencies()
             get sorted list of item frequencies
         getSortedListOfTransactionLength()
             get sorted list of transaction length
         save(data, outputFile)
             store data into outputFile
         printStats()
-            To print all the statistics of the database
+            To print all the stats of the database
         plotGraphs()
             To plot all the graphs of frequency disctribution of items and transaction length distribution in database
    
 
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
@@ -104,14 +106,16 @@
 
     """
 
     def __init__(self, inputFile: str, sep: str='\t'):
         """
         :param inputFile: input file name or path
         :type inputFile: str
+        :param sep: separator
+        :type sep: str
         """
         self.inputFile = inputFile
         self.lengthList = []
         self.sep = sep
         self.database = {}
         self.itemFrequencies = {}
 
@@ -172,64 +176,72 @@
                     quit()
         self.lengthList = [len(s) for s in self._transactions]
 
     def getDatabaseSize(self) -> int:
         """
         get the size of database
         :return: dataset size
+        :rtype: int
         """
         return len(self.database)
 
     def getTotalNumberOfItems(self) -> int:
         """
         get the number of items in database.
         :return: number of items
+        :rtype: int
         """
         return len(self.getSortedListOfItemFrequencies())
 
     def getMinimumTransactionLength(self) -> int:
         """
         get the minimum transaction length
         :return: minimum transaction length
+        :rtype: int
         """
         return min(self.lengthList)
 
     def getAverageTransactionLength(self) -> float:
         """
         get the average transaction length. It is sum of all transaction length divided by database length.
         :return: average transaction length
+        :rtype: float
         """
         totalLength = sum(self.lengthList)
         return totalLength / len(self.database)
 
     def getMaximumTransactionLength(self) -> int:
         """
         get the maximum transaction length
         :return: maximum transaction length
+        :rtype: int
         """
         return max(self.lengthList)
 
     def getStandardDeviationTransactionLength(self) -> float:
         """
         get the standard deviation transaction length
         :return: standard deviation transaction length
+        :rtype: float
         """
         return statistics.pstdev(self.lengthList)
 
     def getVarianceTransactionLength(self) -> float:
         """
         get the variance transaction length
         :return: variance transaction length
+        :rtype: float
         """
         return statistics.variance(self.lengthList)
 
     def getNumberOfItems(self) -> int:
         """
         get the number of items in database.
         :return: number of items
+        :rtype: int
         """
         return len(self.getSortedListOfItemFrequencies())
 
     def convertDataIntoMatrix(self) -> np.ndarray:
         singleItems = self.getSortedListOfItemFrequencies()
         # big_array = np.zeros((self.getDatabaseSize(), len(self.getSortedListOfItemFrequencies())))
         itemsets = {}
@@ -250,32 +262,35 @@
         an_array = np.array(data)
         return an_array
 
     def getSparsity(self) -> float:
         """
         get the sparsity of database. sparsity is percentage of 0 of database.
         :return: database sparsity
+        :rtype: float
         """
         big_array = self.convertDataIntoMatrix()
         n_zeros = np.count_nonzero(big_array == 0)
         return (n_zeros / big_array.size)
 
     def getDensity(self) -> float:
         """
         get the sparsity of database. sparsity is percentage of 0 of database.
         :return: database sparsity
+        :rtype: float
         """
         big_array = self.convertDataIntoMatrix()
         n_zeros = np.count_nonzero(big_array != 0)
         return (n_zeros / big_array.size)
 
     def getSortedListOfItemFrequencies(self) -> dict:
         """
         get sorted list of item frequencies
         :return: item frequencies
+        :rtype: dict
         """
         itemFrequencies = {}
         for line in range(len(self._transactions)):
             times = self._ts[line]
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             for i in range(0, len(items)):
@@ -299,28 +314,30 @@
             rangeFrequencies[va] = values[i]
         return rangeFrequencies
 
     def getTransanctionalLengthDistribution(self) -> dict:
         """
         get transaction length
         :return: transactional length
+        :rtype: dict
         """
         transactionLength = {}
         for length in self.lengthList:
             transactionLength[length] = transactionLength.get(length, 0)
             transactionLength[length] += 1
         return {k: v for k, v in sorted(transactionLength.items(), key=lambda x: x[0])}
 
     def save(self, data: dict, outputFile: str) -> None:
         """
         store data into outputFile
         :param data: input data
         :type data: dict
         :param outputFile: output file name or path to store
         :type outputFile: str
+        :return: None
         """
         with open(outputFile, 'w') as f:
             for key, value in data.items():
                 f.write(f'{key}\t{value}\n')
                    
     def printStats(self) -> None:
         print(f'Database size (total no of transactions) : {self.getDatabaseSize()}')
```

### Comparing `pami-2024.3.9.2/PAMI/extras/dbStats/SequentialDatabase.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/SequentialDatabase.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,18 +9,21 @@
 #
 #             obj.save(oFile)
 #
 #             obj.run()
 #
 #             obj.printStats()
 #
-#
+
+
+
+
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -109,38 +112,47 @@
             obj.run()
 
             obj.printStats()
 
 
     **Executing the code on terminal:**
     -------------------------------------------------
-            Format:
 
-                      >>> python3 SequentialDatabase.py <inputFile>
-            Examples:
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 SequentialDatabase.py <inputFile>
+
+      Example Usage:
+
+      (.venv) $ python3 SequentialDatabase.py sampleDB.txt
+
+      (.venv) $ python3 SequentialDatabase.py sampleDB.txt
 
-                      >>> python3 SequentialDatabase.py sampleDB.txt
-                      >>> python3 SequentialDatabase.py sampleDB.txt
 
-        **Sample run of the importing code:**
-        ----------------------------------------------------
-            import PAMI.extra.DBstats.SequentialDatabase as alg
-            _ap=alg.SequentialDatabase(inputfile,sep)
-            _ap.readDatabase()
-            _ap.printStats()
-            _ap.plotGraphs()
-        **Credits:**
-        ---------------------
-            The complete program was written by Shota Suzuki  under the supervision of Professor Rage Uday Kiran.
+    **Sample run of the importing code:**
+    ----------------------------------------------------
+        import PAMI.extra.DBstats.SequentialDatabase as alg
+        _ap=alg.SequentialDatabase(inputfile,sep)
+        _ap.readDatabase()
+        _ap.printStats()
+        _ap.plotGraphs()
+    **Credits:**
+    ---------------------
+        The complete program was written by Shota Suzuki  under the supervision of Professor Rage Uday Kiran.
     """
 
     def __init__(self, inputFile: str, sep: str='\t') -> None:
         """
         :param inputFile: input file name or path
         :type inputFile: str
+        :param sep: separator
+        :type sep: str
+        :return: None
         """
         self.inputFile = inputFile
         self.seqLengthList = []
         self.subSeqLengthList = []
         self.sep = sep
         self.database = {}
 
@@ -194,136 +206,152 @@
                             self.database[rowNum] = seq
 
 
     def getDatabaseSize(self) -> int:
         """
         get the size of database
         :return: dataset size
+        :rtype: int
         """
         return len(self.database)
 
     def getTotalNumberOfItems(self) -> int:
         """
         get the number of items in database.
         :return: number of items
+        :rtype: int
         """
         return len(self.getSortedListOfItemFrequencies())
 
     def getMinimumSequenceLength(self) -> int:
         """
         get the minimum sequence length
         :return: minimum sequence length
+        :rtype: int
         """
         return min(self.seqLengthList)
 
     def getAverageSubsequencePerSequenceLength(self) -> float:
         """
         get the average subsequence length per sequence length. It is sum of all subsequence length divided by sequence length.
         :return: average subsequence length per sequence length
+        :rtype: float
         """
         totalLength = sum(self.seqLengthList)
         return totalLength / len(self.database)
 
     def getAverageItemPerSubsequenceLength(self) -> float:
 
         """
         get the average Item length per subsequence. It is sum of all item length divided by subsequence length.
         :return: average Item length per subsequence
+        :rtype: float
         """
 
         totalLength = sum(list(map(sum,self.subSeqLengthList)))
         return totalLength / sum(self.seqLengthList)
 
     def getMaximumSequenceLength(self) -> int:
         """
         get the maximum sequence length
         :return: maximum sequence length
+        :rtype: int
         """
         return max(self.seqLengthList)
 
     def getStandardDeviationSequenceLength(self) -> float:
         """
         get the standard deviation sequence length
         :return: standard deviation sequence length
+        :rtype: float
         """
         return statistics.pstdev(self.seqLengthList)
 
     def getVarianceSequenceLength(self) -> float:
         """
         get the variance Sequence length
         :return: variance Sequence length
+        :rtype: float
         """
         return statistics.variance(self.seqLengthList)
 
     def getSequenceSize(self) -> int:
         """
         get the size of sequence
         :return: sequences size
+        :rtype: int
         """
         return sum(self.seqLengthList)
 
     def getMinimumSubsequenceLength(self) -> int:
         """
         get the minimum subsequence length
         :return: minimum subsequence length
+        :rtype: int
         """
         return min(list(map(min,self.subSeqLengthList)))
 
     def getAverageItemPerSequenceLength(self) -> float:
         """
         get the average item length per sequence. It is sum of all item length divided by sequence length.
         :return: average item length per sequence
+        :rtype: float
         """
         totalLength = sum(list(map(sum,self.subSeqLengthList)))
         return totalLength / len(self.database)
 
     def getMaximumSubsequenceLength(self) -> int:
         """
         get the maximum subsequence length
         :return: maximum subsequence length
+        :rtype: int
         """
         return max(list(map(max,self.subSeqLengthList)))
 
     def getStandardDeviationSubsequenceLength(self) -> float:
         """
         get the standard deviation subsequence length
         :return: standard deviation subsequence length
+        :rtype: float
         """
         allList=[]
         for i in self.subSeqLengthList:
             allList=allList+i
         return statistics.pstdev(allList)
 
     def getVarianceSubsequenceLength(self) -> float:
         """
         get the variance subSequence length
         :return: variance subSequence length
+        :rtype: float
         """
         allList = []
         for i in self.subSeqLengthList:
             allList = allList + i
         return statistics.variance(allList)
 
     def getSortedListOfItemFrequencies(self) -> Dict[str, int]:
         """
         get sorted list of item frequencies
         :return: item frequencies
+        :rtype: dict
         """
         itemFrequencies = {}
         for seq in self.database:
             for sub in self.database[seq]:
                 for item in sub:
                     itemFrequencies[item] = itemFrequencies.get(item, 0)
                     itemFrequencies[item] += 1
         return {k: v for k, v in sorted(itemFrequencies.items(), key=lambda x: x[1], reverse=True)}
 
     def getFrequenciesInRange(self) -> Dict[int, int]:
         """
         get sorted list of item frequencies in some range
         :return: item separated by its frequencies
+        :rtype: dict
         """
         fre = self.getSortedListOfItemFrequencies()
         rangeFrequencies = {}
         maximum = max([i for i in fre.values()])
         values = [int(i * maximum / 6) for i in range(1, 6)]
         va = len({key: val for key, val in fre.items() if val > 0 and val < values[0]})
         rangeFrequencies[values[0]] = va
@@ -332,25 +360,27 @@
             rangeFrequencies[values[i]] = va
         return rangeFrequencies
 
     def getSequencialLengthDistribution(self) -> Dict[int, int]:
         """
         get Sequence length Distribution
         :return: Sequence length
+        :rtype: dict
         """
         transactionLength = {}
         for length in self.seqLengthList:
             transactionLength[length] = transactionLength.get(length, 0)
             transactionLength[length] += 1
         return {k: v for k, v in sorted(transactionLength.items(), key=lambda x: x[0])}
 
     def getSubsequencialLengthDistribution(self) -> Dict[int, int]:
         """
         get subSequence length distribution
         :return: subSequence length
+        :rtype: dict
         """
         transactionLength = {}
         for sublen in self.subSeqLengthList:
             for length in sublen:
                 transactionLength[length] = transactionLength.get(length, 0)
                 transactionLength[length] += 1
         return {k: v for k, v in sorted(transactionLength.items(), key=lambda x: x[0])}
```

### Comparing `pami-2024.3.9.2/PAMI/extras/dbStats/TemporalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/TemporalDatabase.py`

 * *Files 9% similar despite different names*

```diff
@@ -10,16 +10,19 @@
 #             obj.save(oFile)
 #
 #             obj.run()
 #
 #             obj.printStats()
 #
 
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -100,15 +103,17 @@
             obj.printStats()
     """
 
     def __init__(self, inputFile: Union[str, pd.DataFrame], sep: str = '\t') -> None:
         """
         :param inputFile: input file name or path
         :type inputFile: str
-        :param sep:
+        :param sep: separator
+        :type sep: str
+        :return: None
         """
         self.inputFile = inputFile
         self.database = {}
         self.lengthList = []
         self.timeStampCount = {}
         self.periodList = []
         self.sep = sep
@@ -178,50 +183,56 @@
             self.periods[key][0] = max(self.periods[key][0], abs(len(self.database) - self.periods[key][1]))
         self.periods = {k: v[0] for k, v in self.periods.items()}
 
     def getDatabaseSize(self) -> int:
         """
         get the size of database
         :return: dataset size
+        :rtype: int
         """
         return len(self.database)
 
     def getMinimumTransactionLength(self) -> int:
         """
         get the minimum transaction length
         :return: minimum transaction length
+        :rtype: int
         """
         return min(self.lengthList)
 
     def getAverageTransactionLength(self) -> float:
         """
         get the average transaction length. It is sum of all transaction length divided by database length.
         :return: average transaction length
+        :rtype: float
         """
         totalLength = sum(self.lengthList)
         return totalLength / len(self.database)
 
     def getMaximumTransactionLength(self) -> int:
         """
         get the maximum transaction length
         :return: maximum transaction length
+        :rtype: int
         """
         return max(self.lengthList)
 
     def getStandardDeviationTransactionLength(self) -> float:
         """
         get the standard deviation transaction length
         :return: standard deviation transaction length
+        :rtype: float
         """
         return statistics.pstdev(self.lengthList)
 
     def getVarianceTransactionLength(self) -> float:
         """
         get the variance transaction length
         :return: variance transaction length
+        :rtype: float
         """
         return statistics.variance(self.lengthList)
 
     def convertDataIntoMatrix(self) -> np.ndarray:
         singleItems = self.getSortedListOfItemFrequencies()
         itemsets = {}
         for tid in self.database:
@@ -240,39 +251,43 @@
         an_array = np.array(data)
         return an_array
 
     def getSparsity(self) -> float:
         """
         get the sparsity of database. sparsity is percentage of 0 of database.
         :return: database sparsity
+        :rtype: float
         """
         big_array = self.convertDataIntoMatrix()
         n_zeros = np.count_nonzero(big_array == 0)
         return (n_zeros / big_array.size)
 
     def getDensity(self) -> float:
         """
         get the sparsity of database. sparsity is percentage of 0 of database.
         :return: database sparsity
+        :rtype: float
         """
         big_array = self.convertDataIntoMatrix()
         n_zeros = np.count_nonzero(big_array == 1)
         return (1.0 - n_zeros / big_array.size)
 
     def getTotalNumberOfItems(self) -> int:
         """
         get the number of items in database.
         :return: number of items
+        :rtype: int
         """
         return len(self.getSortedListOfItemFrequencies())
 
     def getSortedListOfItemFrequencies(self) -> Dict[str, int]:
         """
         get sorted list of item frequencies
         :return: item frequencies
+        :rtype: dict
         """
         itemFrequencies = {}
         for tid in self.database:
             for item in self.database[tid]:
                 itemFrequencies[item] = itemFrequencies.get(item, 0)
                 itemFrequencies[item] += 1
         return {k: v for k, v in sorted(itemFrequencies.items(), key=lambda x: x[1], reverse=True)}
@@ -303,87 +318,97 @@
             rangePeriods[va] = values[i]
         return rangePeriods
 
     def getTransanctionalLengthDistribution(self) -> Dict[int, int]:
         """
         get transaction length
         :return: transactional length
+        :rtype: dict
         """
         transactionLength = {}
         for length in self.lengthList:
             transactionLength[length] = transactionLength.get(length, 0)
             transactionLength[length] += 1
         return {k: v for k, v in sorted(transactionLength.items(), key=lambda x: x[0])}
 
     def save(self, data: dict, outputFile: str) -> None:
         """
         store data into outputFile
         :param data: input data
         :type data: dict
         :param outputFile: output file name or path to store
         :type outputFile: str
+        :return: None
         """
         with open(outputFile, 'w') as f:
             for key, value in data.items():
                 f.write(f'{key}\t{value}\n')
 
     def getMinimumInterArrivalPeriod(self) -> int:
         """
         get the minimum inter arrival period
         :return: minimum inter arrival period
+        :rtype: int
         """
         return min(self.periodList)
 
     def getAverageInterArrivalPeriod(self) -> float:
         """
         get the average inter arrival period. It is sum of all period divided by number of period.
         :return: average inter arrival period
+        :rtype: float
         """
         totalPeriod = sum(self.periodList)
         return totalPeriod / len(self.periodList)
 
     def getMaximumInterArrivalPeriod(self) -> int:
         """
         get the maximum inter arrival period
         :return: maximum inter arrival period
+        :rtype: int
         """
         return max(self.periodList)
 
     def getMinimumPeriodOfItem(self) -> int:
         """
         get the minimum period of the item
         :return: minimum period
+        :rtype: int
         """
         return min([i for i in self.periods.values()])
 
     def getAveragePeriodOfItem(self) -> float:
         """
         get the average period of the item
         :return: average period
+        :rtype: float
         """
         return sum([i for i in self.periods.values()]) / len(self.periods)
 
     def getMaximumPeriodOfItem(self) -> int:
         """
         get the maximum period of the item
         :return: maximum period
+        :rtype: int
         """
         return max([i for i in self.periods.values()])
 
     def getStandardDeviationPeriod(self) -> float:
         """
         get the standard deviation period
         :return: standard deviation period
+        :rtype: float
         """
         return statistics.pstdev(self.periodList)
 
     def getNumberOfTransactionsPerTimestamp(self) -> Dict[int, int]:
         """
         get number of transactions per time stamp
         :return: number of transactions per time stamp as dict
+        :rtype: dict
         """
         maxTS = max(list(self.timeStampCount.keys()))
         return {ts: self.timeStampCount.get(ts, 0) for ts in range(1, maxTS + 1)}
 
     def printStats(self) -> None:
         print(f'Database size : {self.getDatabaseSize()}')
         print(f'Number of items : {self.getTotalNumberOfItems()}')
```

### Comparing `pami-2024.3.9.2/PAMI/extras/dbStats/TransactionalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/TransactionalDatabase.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,19 +8,21 @@
 #             obj = db.TransactionalDatabase(iFile, "\t")
 #
 #             obj.save(oFile)
 #
 #             obj.run()
 #
 #             obj.printStats()
+#
+
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -101,14 +103,17 @@
 
     """
 
     def __init__(self, inputFile: Union[str, pd.DataFrame], sep: str='\t') -> None:
         """
         :param inputFile: input file name or path
         :type inputFile: str
+        :param sep: separator
+        :type sep: str
+        :return: None
         """
         self.inputFile = inputFile
         self.lengthList = []
         self.sep = sep
         self.database = {}
         self.itemFrequencies = {}
 
@@ -153,64 +158,72 @@
                     quit()
         self.lengthList = [len(s) for s in self.database.values()]
 
     def getDatabaseSize(self) -> int:
         """
         get the size of database
         :return: dataset size
+        :rtype: int
         """
         return len(self.database)
 
     def getTotalNumberOfItems(self) -> int:
         """
         get the number of items in database.
         :return: number of items
+        :rtype: int
         """
         return len(self.getSortedListOfItemFrequencies())
 
     def getMinimumTransactionLength(self) -> int:
         """
         get the minimum transaction length
         :return: minimum transaction length
+        :rtype: int
         """
         return min(self.lengthList)
 
     def getAverageTransactionLength(self) -> float:
         """
         get the average transaction length. It is sum of all transaction length divided by database length.
         :return: average transaction length
+        :rtype: float
         """
         totalLength = sum(self.lengthList)
         return totalLength / len(self.database)
 
     def getMaximumTransactionLength(self) -> int:
         """
         get the maximum transaction length
         :return: maximum transaction length
+        :rtype: int
         """
         return max(self.lengthList)
 
     def getStandardDeviationTransactionLength(self) -> float:
         """
         get the standard deviation transaction length
         :return: standard deviation transaction length
+        :rtype: float
         """
         return statistics.pstdev(self.lengthList)
 
     def getVarianceTransactionLength(self) -> float:
         """
         get the variance transaction length
         :return: variance transaction length
+        :rtype: float
         """
         return statistics.variance(self.lengthList)
 
     def getNumberOfItems(self) -> int:
         """
         get the number of items in database.
         :return: number of items
+        :rtype: int
         """
         return len(self.getSortedListOfItemFrequencies())
 
     def convertDataIntoMatrix(self) -> np.ndarray:
         singleItems = self.getSortedListOfItemFrequencies()
         # big_array = np.zeros((self.getDatabaseSize(), len(self.getSortedListOfItemFrequencies())))
         itemsets = {}
@@ -231,32 +244,35 @@
         an_array = np.array(data)
         return an_array
 
     def getSparsity(self) -> float:
         """
         get the sparsity of database. sparsity is percentage of 0 of database.
         :return: database sparsity
+        :rtype: float
         """
         big_array = self.convertDataIntoMatrix()
         n_zeros = np.count_nonzero(big_array == 0)
         return (n_zeros / big_array.size)
 
     def getDensity(self) -> float:
         """
         get the sparsity of database. sparsity is percentage of 0 of database.
         :return: database sparsity
+        :rtype: float
         """
         big_array = self.convertDataIntoMatrix()
         n_zeros = np.count_nonzero(big_array != 0)
         return (n_zeros / big_array.size)
 
     def getSortedListOfItemFrequencies(self) -> dict:
         """
         get sorted list of item frequencies
         :return: item frequencies
+        :rtype: dict
         """
         itemFrequencies = {}
         for tid in self.database:
             for item in self.database[tid]:
                 itemFrequencies[item] = itemFrequencies.get(item, 0)
                 itemFrequencies[item] += 1
         self.itemFrequencies = {k: v for k, v in sorted(itemFrequencies.items(), key=lambda x: x[1], reverse=True)}
@@ -273,29 +289,31 @@
             va = len({key: val for key, val in fre.items() if val < values[i] and val > values[i-1]})
             rangeFrequencies[va] = values[i]
         return rangeFrequencies
 
     def getTransanctionalLengthDistribution(self) -> dict:
         """
         Get transaction length
-        :return: dict
+        :return: a dictionary with transaction length as keys and their total length as values
+        :rtype: dict
         """
         transactionLength = {}
         for length in self.lengthList:
             transactionLength[length] = transactionLength.get(length, 0)
             transactionLength[length] += 1
         return {k: v for k, v in sorted(transactionLength.items(), key=lambda x: x[0])}
 
     def save(self, data: dict, outputFile: str) -> None:
         """
         store data into outputFile
         :param data: input data
         :type data: dict
         :param outputFile: output file name or path to store
         :type outputFile: str
+        :return: None
         """
         with open(outputFile, 'w') as f:
             for key, value in data.items():
                 f.write(f'{key}\t{value}\n')
                    
     def printStats(self) -> None:
         print(f'Database size (total no of transactions) : {self.getDatabaseSize()}')
```

### Comparing `pami-2024.3.9.2/PAMI/extras/dbStats/UncertainTemporalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/UncertainTemporalDatabase.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,16 +11,18 @@
 #
 #             obj.run()
 #
 #             obj.printStats()
 #
 
 
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -102,15 +104,17 @@
 
     """
 
     def __init__(self, inputFile: str, sep: str='\t') -> None:
         """
         :param inputFile: input file name or path
         :type inputFile: str
-        :param sep:
+        :param sep: separator
+        :type sep: str
+        :return: None
         """
         self.inputFile = inputFile
         self.database = {}
         self.lengthList = []
         self.timeStampCount = {}
         self.periodList = []
         self.sep = sep
@@ -180,50 +184,56 @@
         #     self.periodList.append(int(ts)-preTimeStamp)
         #     preTimeStamp = ts
 
     def getDatabaseSize(self) -> int:
         """
         get the size of database
         :return: dataset size
+        :rtype: int
         """
         return len(self.database)
 
     def getMinimumTransactionLength(self) -> int:
         """
         get the minimum transaction length
         :return: minimum transaction length
+        :rtype: int
         """
         return min(self.lengthList)
 
     def getAverageTransactionLength(self) -> float:
         """
         get the average transaction length. It is sum of all transaction length divided by database length.
         :return: average transaction length
+        :rtype: float
         """
         totalLength = sum(self.lengthList)
         return totalLength / len(self.database)
 
     def getMaximumTransactionLength(self) -> int:
         """
         get the maximum transaction length
         :return: maximum transaction length
+        :rtype: int
         """
         return max(self.lengthList)
 
     def getStandardDeviationTransactionLength(self) -> float:
         """
         get the standard deviation transaction length
         :return: standard deviation transaction length
+        :rtype: float
         """
         return statistics.pstdev(self.lengthList)
 
     def getVarianceTransactionLength(self) -> float:
         """
         get the variance transaction length
         :return: variance transaction length
+        :rtype: float
         """
         return statistics.variance(self.lengthList)
 
     def convertDataIntoMatrix(self) -> np.ndarray:
         singleItems = self.getSortedListOfItemFrequencies()
         itemsets = {}
         for tid in self.database:
@@ -242,39 +252,43 @@
         an_array = np.array(data)
         return an_array
 
     def getSparsity(self) -> float:
         """
         get the sparsity of database. sparsity is percentage of 0 of database.
         :return: database sparsity
+        :rtype: float
         """
         big_array = self.convertDataIntoMatrix()
         n_zeros = np.count_nonzero(big_array == 0)
         return (n_zeros / big_array.size)
 
     def getDensity(self) -> float:
         """
         get the sparsity of database. sparsity is percentage of 0 of database.
         :return: database sparsity
+        :rtype: float
         """
         big_array = self.convertDataIntoMatrix()
         n_zeros = np.count_nonzero(big_array == 1)
         return (1.0 - n_zeros / big_array.size)
 
     def getTotalNumberOfItems(self) -> int:
         """
         get the number of items in database.
         :return: number of items
+        :rtype: int
         """
         return len(self.getSortedListOfItemFrequencies())
 
     def getSortedListOfItemFrequencies(self) -> dict:
         """
         get sorted list of item frequencies
         :return: item frequencies
+        :rtype: dict
         """
         itemFrequencies = {}
         for tid in self.database:
             for item in self.database[tid]:
                 itemFrequencies[item] = itemFrequencies.get(item, 0)
                 itemFrequencies[item] += 1
         return {k: v for k, v in sorted(itemFrequencies.items(), key=lambda x: x[1], reverse=True)}
@@ -293,66 +307,73 @@
             rangeFrequencies[va] = values[i]
         return rangeFrequencies
 
     def getTransanctionalLengthDistribution(self) -> dict:
         """
         get transaction length
         :return: transactional length
+        :rtype: dict
         """
         transactionLength = {}
         for length in self.lengthList:
             transactionLength[length] = transactionLength.get(length, 0)
             transactionLength[length] += 1
         return {k: v for k, v in sorted(transactionLength.items(), key=lambda x: x[0])}
 
     def save(self, data: dict, outputFile: str) -> None:
         """
         store data into outputFile
         :param data: input data
         :type data: dict
         :param outputFile: output file name or path to store
         :type outputFile: str
+        :return: None
         """
         with open(outputFile, 'w') as f:
             for key, value in data.items():
                 f.write(f'{key}\t{value}\n')
 
     def getMinimumPeriod(self) -> int:
         """
         get the minimum period
         :return: minimum period
+        :rtype: int
         """
         return min(self.periodList)
 
     def getAveragePeriod(self) -> float:
         """
         get the average period. It is sum of all period divided by number of period.
         :return: average period
+        :rtype: float
         """
         totalPeriod = sum(self.periodList)
         return totalPeriod / len(self.periodList)
 
     def getMaximumPeriod(self) -> int:
         """
         get the maximum period
         :return: maximum period
+        :rtype: int
         """
         return max(self.periodList)
 
     def getStandardDeviationPeriod(self) -> float:
         """
         get the standard deviation period
         :return: standard deviation period
+        :rtype: float
         """
         return statistics.pstdev(self.periodList)
 
     def getNumberOfTransactionsPerTimestamp(self) -> dict:
         """
         get number of transactions per time stamp
         :return: number of transactions per time stamp as dict
+        :rtype: float
         """
         maxTS = max(list(self.timeStampCount.keys()))
         return {ts: self.timeStampCount.get(ts, 0) for ts in range(1, maxTS + 1)}
    
     def printStats(self) -> None:
         print(f'Database size : {self.getDatabaseSize()}')
         print(f'Number of items : {self.getTotalNumberOfItems()}')
```

### Comparing `pami-2024.3.9.2/PAMI/extras/dbStats/UncertainTransactionalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/UncertainTransactionalDatabase.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,16 +9,20 @@
 #
 #             obj.save(oFile)
 #
 #             obj.run()
 #
 #             obj.printStats()
 #
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -92,14 +96,17 @@
 
     """
 
     def __init__(self, inputFile: str, sep: str='\t') -> None:
         """
         :param inputFile: input file name or path
         :type inputFile: str
+        :param sep: separator
+        :type sep: str
+        :return: None
         """
         self.inputFile = inputFile
         self.lengthList = []
         self.sep = sep
         self.database = {}
 
     def run(self) -> None:
@@ -144,64 +151,72 @@
                     quit()
         self.lengthList = [len(s) for s in self.database.values()]
 
     def getDatabaseSize(self) -> int:
         """
         get the size of database
         :return: dataset size
+        :rtype: int
         """
         return len(self.database)
 
     def getTotalNumberOfItems(self) -> int:
         """
         get the number of items in database.
         :return: number of items
+        :rtype: int
         """
         return len(self.getSortedListOfItemFrequencies())
 
     def getMinimumTransactionLength(self) -> int:
         """
         get the minimum transaction length
         :return: minimum transaction length
+        :rtype: int
         """
         return min(self.lengthList)
 
     def getAverageTransactionLength(self) -> float:
         """
         get the average transaction length. It is sum of all transaction length divided by database length.
         :return: average transaction length
+        :rtype: float
         """
         totalLength = sum(self.lengthList)
         return totalLength / len(self.database)
 
     def getMaximumTransactionLength(self) -> int:
         """
         get the maximum transaction length
         :return: maximum transaction length
+        :rtype: int
         """
         return max(self.lengthList)
 
     def getStandardDeviationTransactionLength(self) -> float:
         """
         get the standard deviation transaction length
         :return: standard deviation transaction length
+        :rtype: float
         """
         return statistics.pstdev(self.lengthList)
 
     def getVarianceTransactionLength(self) -> float:
         """
         get the variance transaction length
         :return: variance transaction length
+        :rtype: float
         """
         return statistics.variance(self.lengthList)
 
     def getNumberOfItems(self) -> int:
         """
         get the number of items in database.
         :return: number of items
+        :rtype: int
         """
         return len(self.getSortedListOfItemFrequencies())
 
     def convertDataIntoMatrix(self) -> np.ndarray:
         singleItems = self.getSortedListOfItemFrequencies()
         # big_array = np.zeros((self.getDatabaseSize(), len(self.getSortedListOfItemFrequencies())))
         itemsets = {}
@@ -222,32 +237,35 @@
         an_array = np.array(data)
         return an_array
 
     def getSparsity(self) -> float:
         """
         get the sparsity of database. sparsity is percentage of 0 of database.
         :return: database sparsity
+        :rtype: float
         """
         big_array = self.convertDataIntoMatrix()
         n_zeros = np.count_nonzero(big_array == 0)
         return (n_zeros / big_array.size)
 
     def getDensity(self) -> float:
         """
         get the sparsity of database. sparsity is percentage of 0 of database.
         :return: database sparsity
+        :rtype: float
         """
         big_array = self.convertDataIntoMatrix()
         n_zeros = np.count_nonzero(big_array != 0)
         return (n_zeros / big_array.size)
 
     def getSortedListOfItemFrequencies(self) -> dict:
         """
         get sorted list of item frequencies
         :return: item frequencies
+        :rtype: dict
         """
         itemFrequencies = {}
         for tid in self.database:
             for item in self.database[tid]:
                 itemFrequencies[item] = itemFrequencies.get(item, 0)
                 itemFrequencies[item] += 1
         return {k: v for k, v in sorted(itemFrequencies.items(), key=lambda x: x[1], reverse=True)}
@@ -264,28 +282,30 @@
             rangeFrequencies[va] = values[i]
         return rangeFrequencies
 
     def getTransanctionalLengthDistribution(self) -> dict:
         """
         get transaction length
         :return: transactional length
+        :rtype: dict
         """
         transactionLength = {}
         for length in self.lengthList:
             transactionLength[length] = transactionLength.get(length, 0)
             transactionLength[length] += 1
         return {k: v for k, v in sorted(transactionLength.items(), key=lambda x: x[0])}
 
     def save(self, data: dict, outputFile: str) -> None:
         """
         store data into outputFile
         :param data: input data
         :type data: dict
         :param outputFile: output file name or path to store
         :type outputFile: str
+        :return: None
         """
         with open(outputFile, 'w') as f:
             for key, value in data.items():
                 f.write(f'{key}\t{value}\n')
 
     def printStats(self) -> None:
         print(f'Database size (total no of transactions) : {self.getDatabaseSize()}')
```

### Comparing `pami-2024.3.9.2/PAMI/extras/dbStats/UtilityDatabase.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/UtilityDatabase.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,18 +8,21 @@
 #             obj = db.UtilityDatabase(iFile, "\t")
 #
 #             obj.save(oFile)
 #
 #             obj.run()
 #
 #             obj.printStats()
+#
+
+
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -64,14 +67,17 @@
 
     """
 
     def __init__(self, inputFile: Union[str, pd.DataFrame], sep: str='\t') -> None:
         """
         :param inputFile: input file name or path
         :type inputFile: str
+        :param sep: separator in file
+        :type sep: str or
+        :return: None
         """
         self.inputFile = inputFile
         self.database = {}
         self.lengthList = []
         self.utility = {}
         self.sep = sep
 
@@ -136,93 +142,104 @@
                 self.utility[transaction[i]] += utilities[i]
         self.lengthList = [len(s) for s in self.database.values()]
         self.utility = {k: v for k, v in sorted(self.utility.items(), key=lambda x:x[1], reverse=True)}
 
     def getDatabaseSize(self) -> int:
         """
         get the size of database
-        :return: int
+        :return: size of database
+        :rtype: int
         """
         return len(self.database)
 
     def getTotalNumberOfItems(self) -> int:
         """
         get the number of items in database.
         :return: number of items
+        :rtype: int
         """
         return len(self.getSortedListOfItemFrequencies())
 
     def getMinimumTransactionLength(self) -> int:
         """
         get the minimum transaction length
         :return: minimum transaction length
+        :rtype: int
         """
         return min(self.lengthList)
 
     def getAverageTransactionLength(self) -> float:
         """
         get the average transaction length. It is sum of all transaction length divided by database length.
         :return: average transaction length
+        :rtype: float
         """
         totalLength = sum(self.lengthList)
         return totalLength / len(self.database)
 
     def getMaximumTransactionLength(self) -> int:
         """
         get the maximum transaction length
         :return: maximum transaction length
+        :rtype: int
         """
         return max(self.lengthList)
 
     def getStandardDeviationTransactionLength(self) -> float:
         """
         get the standard deviation transaction length
         :return: standard deviation transaction length
+        :rtype: float
         """
         return statistics.pstdev(self.lengthList)
 
     def getVarianceTransactionLength(self) -> float:
         """
         get the variance transaction length
         :return: variance transaction length
+        :rtype: float
         """
         return statistics.variance(self.lengthList)
 
     def getNumberOfItems(self) -> int:
         """
         get the number of items in database.
         :return: number of items
+        :rtype: int
         """
         return len(self.getSortedListOfItemFrequencies())
 
     def getSparsity(self) -> float:
         # percentage of 0 dense dataframe
         """
         get the sparsity of database
-        :return: float
+        :return: sparsity of database in floating values
+        :rtype: float
         """
         matrixSize = self.getDatabaseSize()*len(self.getSortedListOfItemFrequencies())
         return (matrixSize - sum(self.getSortedListOfItemFrequencies().values())) / matrixSize
 
     def getSortedListOfItemFrequencies(self) -> dict:
         """
         get sorted list of item frequencies
         :return: item frequencies
+        :rtype: dict
         """
         itemFrequencies = {}
         for tid in self.database:
             for item in self.database[tid]:
                 itemFrequencies[item] = itemFrequencies.get(item, 0)
                 itemFrequencies[item] += 1
         return {k: v for k, v in sorted(itemFrequencies.items(), key=lambda x:x[1], reverse=True)}
     
     def getFrequenciesInRange(self) -> dict:
         """
         This function is used to get the Frequencies in range
         :return: Frequencies In Range
+        :rtype: dict
         """
         fre = self.getSortedListOfItemFrequencies()
         rangeFrequencies = {}
         maximum = max([i for i in fre.values()])
         values = [int(i*maximum/6) for i in range(1,6)]
         #print(maximum)
         va = len({key: val for key, val in fre.items() if val > 0 and val < values[0]})
@@ -232,66 +249,73 @@
             va = len({key: val for key, val in fre.items() if val < values[i] and val > values[i-1]})
             rangeFrequencies[va] = values[i]
         return rangeFrequencies
 
     def getTransanctionalLengthDistribution(self) -> dict:
         """
         get transaction length
-        :return: dict
+        :return: a dictionary of Transaction Length Distribution
+        :rtype: dict
         """
         transactionLength = {}
         for length in self.lengthList:
             transactionLength[length] = transactionLength.get(length, 0)
             transactionLength[length] += 1
         return {k: v for k, v in sorted(transactionLength.items(), key=lambda x:x[0])}
 
     def save(self, data, outputFile) -> None:
         """
         store data into outputFile
         :param data: input data
         :type data: dict
         :param outputFile: output file name or path to store
         :type outputFile: str
+        :return: None
         """
         with open(outputFile, 'w') as f:
             for key, value in data.items():
                 f.write(f'{key}\t{value}\n')
 
     def getTotalUtility(self) -> int:
         """
         get sum of utility
         :return: total utility
+        :rtype: int
         """
         return sum(list(self.utility.values()))
 
     def getMinimumUtility(self) -> int:
         """
         get the minimum utility
-        :return: int
+        :return: integer value of minimum utility
+        :rtype: int
         """
         return min(list(self.utility.values()))
 
     def getAverageUtility(self) -> float:
         """
         get the average utility
         :return: average utility
+        :rtype: float
         """
         return sum(list(self.utility.values())) / len(self.utility)
 
     def getMaximumUtility(self) -> int:
         """
         get the maximum utility
-        :return: int
+        :return: integer value of maximum utility
+        :rtype: int
         """
         return max(list(self.utility.values()))
 
     def getSortedUtilityValuesOfItem(self) -> dict:
         """
         get sorted utility value each item. key is item and value is utility of item
         :return: sorted dictionary utility value of item
+        :rtype: dict
         """
         return self.utility
     
     def printStats(self) -> None:
 
         """
         This function is used to print the results
```

### Comparing `pami-2024.3.9.2/PAMI/extras/fuzzyTransformation/abstract.py` & `pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py` & `pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,20 +5,20 @@
 #
 #             from PAMI.extras.FuzzyTransformation import temporalToFuzzy as db
 #
 #             obj = db.temporalToFuzzy(iFile, FuzFile, oFile, "\t" )
 #
 #             obj.startConvert()
 #
-#
-#
+
+
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -151,15 +151,15 @@
             quit()
 
     def _Regions(self, quantity: int) -> None:
         """
         calculate the labelled region of input "quantity"
         :param quantity: represents the quantity of item
         :type quantity: int
-         :return: None
+        :return: None
         """
         self._list = [0] * len(self._LabelKey)
         if self._RegionsCal[0][0] < quantity <= self._RegionsCal[0][1]:
             self._list[0] = 1
             return
         elif quantity >= self._RegionsCal[-1][0]:
             self._list[-1] = 1
```

### Comparing `pami-2024.3.9.2/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py` & `pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,20 +5,20 @@
 #
 #             from PAMI.extras.FuzzyTransformation import transactionalToFuzzyTimeSeries as db
 #
 #             obj = db.transactionalToFuzzy(iFile, FuzFile, oFile, "\t" )
 #
 #             obj.startConvert()
 #
-#
+
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -156,15 +156,15 @@
             quit()
 
     def _Regions(self, quantity: int) -> None:
         """
         calculate the labelled region of input "quantity"
         :param quantity: represents the quantity of item
         :type quantity: int
-         :return: None
+        :return: None
         """
         self._list = [0] * len(self._LabelKey)
         if self._RegionsCal[0][0] < quantity <= self._RegionsCal[0][1]:
             self._list[0] = 1
             return
         elif quantity >= self._RegionsCal[-1][0]:
             self._list[-1] = 1
```

### Comparing `pami-2024.3.9.2/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py` & `pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,18 +5,20 @@
 #
 #             from PAMI.extras.FuzzyTransformation import utilityToFuzzy as db
 #
 #             obj = db.utilityToFuzzy(iFile, FuzFile, oFile, "\t" )
 #
 #             obj.startConvert()
 #
-#
+
+
+
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.3.9.2/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,16 +10,19 @@
 #             obj.save()
 #
 #             obj.createPoint(0,100,0,100) # values can be according to the size of data
 #
 #             obj.saveAsFile("outputFileName") # To create a file
 #
 
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.3.9.2/PAMI/extras/generateDatabase/generateTemporalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/TemporalDatabase.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-# generateTemporalDatabase is a code used to convert the database into Temporal database.
+# TemporalDatabase is a code used to create a synthetic temporal database.
 #
 #  **Importing this algorithm into a python program**
 #  --------------------------------------------------------
 #
-#             from PAMI.extras.generateDatabase import generateTemporalDatabase as db
+#             from PAMI.extras.syntheticDataGenerator import TemporalDatabase as db
 #
-#             obj = db.generateTemporalDatabase(100, 10, 6, oFile, %, "\t")
+#             obj = db.TemporalDatabase(100, 10, 6, oFile, %, "\t")
 #
 #             obj.save()
 #
 #             obj.getFileName("outputFileName") # to create a file
 #
 #             obj.getDatabaseAsDataFrame("outputFileName") # to convert database into dataframe
 #
@@ -28,91 +28,122 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-import random
-import pandas as pd
 from typing import Tuple, List, Union
-import os
+import pandas as pd
+import numpy as np
+import random
 import sys
+import os
 
-class generateTemporalDatabase:
+class TemporalDatabase:
     """
     :Description:   generateTemporalDatabase creates a temporal database and outputs a database or a frame depending on input
 
     :Attributes:
-
-        numOfTransactions: int
+        :param numOfTransactions: int
             number of transactions
-        maxNumOfItem: int
-            maximum value an item can be
-        maxNumOfItemsPerTransaction: int
-            maximum number of items a transaction can be
-        outputFile: str
+        :param avgLenOfTransactions: int
+            average length of transactions
+        :param numItems: int
+            number of items
+        :param outputFile: str
             output file name
-        percentage: int
+        :param percentage: int
             percentage of coinToss for TID of temporalDatabase
-        sep: str
+        :param sep: str
             seperator for database output file
-        typeOfFile: str
+        :param typeOfFile: str
             specify database or dataframe to get corresponding output
 
     :Methods:
-
         getFileName():
             returns filename
         createTemporalFile():
             creates temporal database file or dataframe
         getDatabaseAsDataFrame:
             returns dataframe
+        performCoinFlip():
+            Perform a coin flip with the given probability
+        tuning():
+            Tune the arrayLength to match avgLenOfTransactions
+        createTemporalFile():
+            create Temporal database or dataframe depending on input
 
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
 
-             from PAMI.extras.generateDatabase import generateTemporalDatabase as db
+            from PAMI.extras.generateDatabase import generateTemporalDatabase as db
 
-             obj = db.generateTemporalDatabase(0, 100, 0, 100, 10, 10, 0.5, 0.9, 0.5, 0.9)
-
-             obj.save()
-
-             obj.getFileName("outputFileName") # to create a file
-
-             obj.getDatabaseAsDataFrame("outputFileName") # to convert database into dataframe
-
-             obj.createTemporalFile("outputFileName") # to get outputfile
+            numOfTransactions = 100
+            numItems = 15
+            avgTransactionLength = 6
+            outFileName = 'temporal_ot.txt'
+            sep = '\t'
+            percent = 75
+            frameOrBase = "dataframe" # if you want to get dataframe as output
+            frameOrBase = "database" # if you want to get database/csv/file as output
+
+            temporalDB = db.generateTemporalDatabase(numOfTransactions, avgTransactionLength, numItems, outFileName, percent, sep, frameOrBase )
+            temporalDB.createTemporalFile()
+            print(temporalDB.getDatabaseAsDataFrame())
 
     """
-    def __init__(self, numOfTransactions: int, maxNumOfItems: int, maxNumOfItemsPerTransaction: int, outputFile: str, percentage: int=50,
+    def __init__(self, numOfTransactions: int, avgLenOfTransactions: int, 
+                 numItems: int, outputFile: str, percentage: int=50,
                  sep: str='\t', typeOfFile: str="Database") -> None:
+        
         """
-        :param numOfTransactions: number of transactions
-        :type numOfTransactions: int
-        :param maxNumOfItems: Highest value an item can be
-        :type maxNumOfItems: int
-        :param maxNumOfItemsPerTransaction: max number of items per transaction
-        :type maxNumOfItemsPerTransaction: int
-        :param outputFile: output file/filename
-        :type outputFile: str
-        :param percentage: Chance of coinFlip for temporal TID
-        :type percentage: int
-        :param sep: seperator
-        :type sep: str
-        :param typeOfFile: specify whether database or dataframe to create respective objects. Note: dataframe must be
-                            retrieved later with getDatabaseasDataframe
-        :type typeOfFile: str
+        :Description:   Initialize the generateTemporalDatabase class
+
+        :Attributes:
+            :param numOfTransactions: int
+                number of transactions
+            :param avgLenOfTransactions: int
+                average length of transactions
+            :param numItems: int
+                number of items
+            :param outputFile: str
+                output file name
+            :param percentage: int
+                percentage of coinToss for TID of temporalDatabase
+            :param sep: str
+                seperator for database output file
+            :param typeOfFile: str
+                specify database or dataframe to get corresponding output
+
+        :Methods:
+            getFileName():
+                returns filename
+            createTemporalFile():
+                creates temporal database file or dataframe
+            getDatabaseAsDataFrame:
+                returns dataframe
+            performCoinFlip():
+                Perform a coin flip with the given probability
+            tuning():
+                Tune the arrayLength to match avgLenOfTransactions
+            createTemporalFile():
+                create Temporal database or dataframe depending on input
+        
         """
+
         self.numOfTransactions = numOfTransactions
-        self.maxNumOfItems = maxNumOfItems
-        self.maxNumOfItemsPerTransaction = maxNumOfItemsPerTransaction
+        self.avgLenOfTransactions = avgLenOfTransactions
+        self.numItems = numItems
         self.outputFile = outputFile
-        self.percentage = percentage
+        if percentage > 1:
+            self.percentage = percentage / 100
+        else:
+            self.percentage = percentage
         self.sep = sep
         self.typeOfFile = typeOfFile.lower()
 
     def getFileName(self) -> str:
         """
         return filename
         :return:
@@ -121,120 +152,104 @@
 
     def getDatabaseAsDataFrame(self) -> pd.DataFrame:
         """
         return dataframe
         return: pd.dataframe
         """
         return self.df
+    
+    def performCoinFlip(self, probability: float) -> bool:
+        """Perform a coin flip with the given probability."""
+        result = np.random.choice([0, 1], p=[1 - probability, probability])
+        return result == 1
+
+
+    def tuning(self, array, sumRes) -> list:
+        """
+        Tune the array so that the sum of the values is equal to sumRes
 
+        Parameters:
+        array: list - list of values
+        sumRes: int - target sum
 
-    def createTemporalFile(self) -> None:
+        Returns:
+        array: list - tuned array
+        """
+
+        # first generate a random array of length n whose values average to m
+        values = np.random.randint(1, self.numItems, len(array))
+
+        while np.sum(values) != sumRes:
+            # get index of largest value
+            # if sum is too large, decrease the largest value
+            if np.sum(values) > sumRes:
+                maxIndex = np.argmax(values)
+                values[maxIndex] -= 1
+            # if sum is too small, increase the smallest value
+            else:
+                minIndex = np.argmin(values)
+                values[minIndex] += 1
+
+        # get location of all values greater than numItems
+        
+        for i in range(len(array)):
+            array[i][1] = values[i]
+
+        return array
+
+    def create(self) -> None:
         """
         create Temporal database or dataframe depending on input
         :return:
         """
-        with open(self.outputFile, "w") as outFile:
-            itemFrameSet = list()
-            timeStampList = list()
-            # This hashset will be used to remember which items have
-            # already been added to this item set.
-            timestamp = 1
-            coinFlip = [True, False]
-            alreadyAdded = set()
-            # create an arraylist to store items from the item set that will be generated
-            itemSet = list()
-            # We randomly decide how many items will appear in this transaction
-            randNumOfItems = random.randint(1, self.maxNumOfItemsPerTransaction)
-            # for the number of items that was decided above
-            for j in range(randNumOfItems):
-                # we generate the item randomly and write it to disk
-                item = random.randint(1, self.maxNumOfItems)
-                # if we already added this item to this item set
-                # we choose another one
-                while item in alreadyAdded:
-                    item = random.randint(1, self.maxNumOfItems)
-                alreadyAdded.add(item)
-                itemSet.append(item)
-            # sort the item set
-            itemSet.sort()
-            if self.typeOfFile == "database":
-                outFile.write(str(timestamp) + self.sep)
-                for j in itemSet:
-                    outFile.write(str(j) + self.sep)
-                outFile.write('\n')
-            if self.typeOfFile == "dataframe":
-                timeStampList.append(timestamp)
-                itemFrameSet.append(itemSet)
-            # add item
-            for i in range(self.numOfTransactions - 1):
-                while random.choices(coinFlip, weights=[self.percentage, 100 - self.percentage], k=1)[0]:
-                    timestamp += 1
-                    nextTimestamp = timestamp + 1
-                if not random.choices(coinFlip, weights=[self.percentage, 100 - self.percentage], k=1)[0]:
-                    timestamp += 1
-                    nextTimestamp = timestamp + 1
-                alreadyAdded = set()
-                # create an arraylist to store items from the item set that will be generated
-                itemSet = list()
-                randNumOfItems = random.randint(1, self.maxNumOfItemsPerTransaction)
-                for j in range(randNumOfItems):
-                    # we generate the item randomly and write it to disk
-                    item = random.randint(1, self.maxNumOfItems)
-                    # if we already added this item to this item set
-                    # we choose another one
-                    while item in alreadyAdded:
-                        item = random.randint(1, self.maxNumOfItems)
-                    alreadyAdded.add(item)
-                    itemSet.append(item)
-                # sort the item set
-                itemSet.sort()
-                # writing the item set
-                if self.typeOfFile == "database":
-                    outFile.write(str(timestamp) + self.sep)
-                    for j in itemSet:
-                        outFile.write(str(j) + self.sep)
-                    outFile.write('\n')
-                if self.typeOfFile == "dataframe":
-                    timeStampList.append(timestamp)
-                    itemFrameSet.append(itemSet)
-
-            if self.typeOfFile == "dataframe":
-                data = {
-                    'timestamp': timeStampList,
-                    'transactions': pd.Series(itemFrameSet)
-                }
-                self.df = pd.DataFrame(data)
-        outFile.close()
+
+        db = []
+        lineSize = []
+        for i in range(self.numOfTransactions):
+            db.append([i])
+            if self.performCoinFlip(self.percentage):
+                lineSize.append([i,0])
+        
+        # make it so that sum of lineSize[1] equal to numTransactions * avgLenOfTransactions
+        sumRes = self.numOfTransactions * self.avgLenOfTransactions
+        self.tuning(lineSize, sumRes)
+
+        for i in range(len(lineSize)):
+            if lineSize[i][1] > self.numItems:
+                raise ValueError("Error: Either increase numItems or decrease avgLenOfTransactions or modify percentage")
+            line = np.random.choice(range(1, self.numItems + 1), lineSize[i][1], replace=False)
+            db[lineSize[i][0]].extend(line)
+
+        if self.typeOfFile == "database":
+            with open(self.outputFile, "w") as outFile:
+                for line in db:
+                    outFile.write(self.sep.join(map(str, line)) + '\n')
+            outFile.close()
+
         if self.typeOfFile == "dataframe":
-            os.remove(outFileName)
+            data = {
+                'timestamp': [line[0] for line in db],
+                'transactions': pd.Series([line[1:] for line in db])
+            }
+            self.df = pd.DataFrame(data)
 
+        print("Temporal database created successfully")
 
 
-if __name__ == '__main__':
-    numOfTransactions = 100
-    maxNumOfItems = 10
-    maxNumOfItemsPerTransaction = 6
-    outFileName = 'temporal_out.txt'
-    sep = '\t'
-    frameOrBase = "database"
 
-    temporalDB = generateTemporalDatabase(numOfTransactions, maxNumOfItems, maxNumOfItemsPerTransaction, outFileName)
 
-    temporalDB.createTemporalFile()
 
-    numOfTransactions = 100
-    maxNumOfItems = 10
-    maxNumOfItemsPerTransaction = 6
-    outFileName = 'temporal_ot.txt'
-    sep = '\t'
-    percent = 50
-    frameOrBase = "dataframe"
 
-    temporalDB = generateTemporalDatabase(numOfTransactions, maxNumOfItems, maxNumOfItemsPerTransaction, outFileName, percent, sep, frameOrBase )
 
-    temporalDB.createTemporalFile()
 
-    print(temporalDB.getDatabaseAsDataFrame())
 
-    obj = generateTemporalDatabase(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])
-    obj.createTemporalFile(sys.argv[5])
 
+
+
+
+
+
+
+if __name__ == '__main__':
+
+    obj = TemporalDatabase(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])
+    obj.create(sys.argv[5])
```

### Comparing `pami-2024.3.9.2/PAMI/extras/generateDatabase/generateTransactionalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/TransactionalDatabase.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,20 +1,25 @@
-# generateTransactionalDatabase is a code used to convert the database into Transactional database.
+import numpy as np
+import pandas as pd
+import sys
+
+# TransactionalDatabase is a code used to create a synthetic transactional database.
 #
-# **Importing this algorithm into a python program**
+#  **Importing this algorithm into a python program**
+#  --------------------------------------------------------
+#     from PAMI.extras.syntheticDataGenerator import TransactionalDatabase as db
+#     obj = db(10, 5, 10)
+#     obj.create()
+#     obj.save('db.txt')
+#     print(obj.getTransactions()) to get the transactional database as a pandas dataframe
+
+# **Running the code from the command line**
 # --------------------------------------------------------
-#
-#             from PAMI.extras.generateDatabase import generateTransactionalDatabase as db
-#
-#             obj = db.generateTransactionalDatabase(100, 10, 6, oFile, %, "\t")
-#
-#             obj.save()
-#
-#             obj.getFileName("outputFileName") # to create a file
-#
+#     python TransactionalDatabase.py 10 5 10 db.txt
+#     cat db.txt
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -24,114 +29,163 @@
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
-import random
-import sys
 
+class TransactionalDatabase:
+    """
+    :Description Generate a transactional database with the given number of lines, average number of items per line, and total number of items
+
+    :Attributes:
+    numLines: int  
+        - number of lines
+    avgItemsPerLine: int 
+        - average number of items per line
+    numItems: int 
+        - total number of items
+
+    :Methods:
+        create: 
+            Generate the transactional database
+        save: 
+            Save the transactional database to a file
+        getTransactions: 
+            Get the transactional database
 
-class generateTransactionalDatabase:
+
+
+    
     """
-    :Description:   generateTransactionalDatabase generates a transactional database
 
-   :Attributes:
+    def __init__(self, numLines, avgItemsPerLine, numItems) -> None:
+        """
+        Initialize the transactional database with the given parameters
+
+        Parameters:
+        numLines: int - number of lines
+        avgItemsPerLine: int - average number of items per line
+        numItems: int - total number of items
+        """
 
-        numOfTransactions: int
-            number of transactions
-        maxNumOfDistinctItems: int
-            maximum number of distinct items
-        numOfItemsPerTransaction: int
-            number of items per transaction
-        outFileName: str
-            output file name
-        sep: str
-            seperator in file, default is tab space
+        self.numLines = numLines
+        self.avgItemsPerLine = avgItemsPerLine
+        self.numItems = numItems
+        self.db = []
+    
+    def tuning(self, array, sumRes) -> list:
+        """
+        Tune the array so that the sum of the values is equal to sumRes
 
-   :Methods:
+        Parameters:
+        array: list - list of values
+        sumRes: int - target sum
 
-        getFileName()
-            get output filename
+        Returns:
+        array: list - tuned array
+        """
 
-   **Importing this algorithm into a python program**
-   --------------------------------------------------------
-   .. code-block:: python
+        while np.sum(array) != sumRes:
+            # get index of largest value
+            randIndex = np.random.randint(0, len(array))
+            # if sum is too large, decrease the largest value
+            if np.sum(array) > sumRes:
+                array[randIndex] -= 1
+            # if sum is too small, increase the smallest value
+            else:
+                minIndex = np.argmin(array)
+                array[randIndex] += 1
+        return array
+        
 
-            from PAMI.extras.generateDatabase import generateTransactionalDatabase as db
+    def generateArray(self, nums, avg, maxItems) -> list:
+        """
+        Generate a random array of length n whose values average to m
 
-            obj = db.generateTransactionalDatabase(100, 10, 6, 100, 0File, %, "\t")
+        Parameters:
+        nums: int - number of values
+        avg: int - average value
+        maxItems: int - maximum value
 
-            obj.save()
+        Returns:
+        values: list - random array
+        """
 
-            obj.getFileName("outputFileName") # to create a file
+        # generate n random values
+        values = np.random.randint(1, maxItems, nums)
+
+        sumRes = nums * avg
+
+        self.tuning(values, sumRes)
+
+        # if any value is less than 1, increase it and tune the array again
+        while np.any(values < 1):
+            for i in range(nums):
+                if values[i] < 1:
+                    values[i] += 1
+            self.tuning(values, sumRes)
+
+        while np.any(values > maxItems):
+            for i in range(nums):
+                if values[i] > maxItems:
+                    values[i] -= 1
+            self.tuning(values, sumRes)
+
+
+        # if all values are same then randomly increase one value and decrease another
+        while np.all(values == values[0]):
+            values[np.random.randint(0, nums)] += 1
+            self.tuning(values, sumRes)
+
+        return values
+
+    def create(self) -> None:
+        """
+
+        Generate the transactional database
+
+        Returns:
 
-    """
-    def __init__(self, numOfTransactions: int, maxNumOfDistinctItems: int, numOfItemsPerTransaction: int, outFileName: str, sep: str='\t') -> None:
         """
+        db = set()
 
-        :param numOfTransactions: number of transactions
-        :type numOfTransactions: int
-        :param maxNumOfDistinctItems: distinct items per transactions
-        :type maxNumOfDistinctItems: int 
-        :param numOfItemsPerTransaction: items per transaction
-        :type numOfItemsPerTransaction: int
-        :param outFileName: output filename
-        :type outFileName: str
-        :param sep: seperator
-        :type sep: str
-        """
-        self.numOfTransactions = numOfTransactions
-        self.maxNumOfDistinctItems = maxNumOfDistinctItems
-        self.numOfItemsPerTransaction = numOfItemsPerTransaction
-        self.outFileName = outFileName
-        self.sep = sep
-
-        # make outFile
-        with open(self.outFileName, "w+") as outFile:
-            # For the number of transactions to be generated
-            for i in range(self.numOfTransactions):
-                # This hashset will be used to remember which items have
-                # already been added to this item set.
-                alreadyAdded = set()
-                # create an arraylist to store items from the item set that will be generated
-                itemSet = list()
-                # We randomly decide how many items will appear in this transaction
-                randNumOfItems = random.randrange(self.maxNumOfDistinctItems) + 1
-                # for the number of items that was decided above
-                for j in range(randNumOfItems):
-                    # we generate the item randomly and write it to disk
-                    item = random.randrange(self.maxNumOfDistinctItems) + 1
-                    # if we already added this item to this item set
-                    # we choose another one
-                    while item in alreadyAdded:
-                        item = random.randrange(self.maxNumOfDistinctItems) + 1
-                    alreadyAdded.add(item)
-                    itemSet.append(item)
-                # sort the item set
-                itemSet.sort()
-                # write the item set
-                for j in itemSet:
-                    outFile.write(str(j) + self.sep)
-                outFile.write('\n')
-        # close outFile
-        outFile.close()
-
-
-
-    def getFileName(self) -> str:
-        """
-        return output file name
-        :return: output file name
-        """
-        return self.outFileName
-
-if __name__ == '__main__':
-    numOfTransactions = 500
-    maxNumOfDistinctItems = 1000
-    numOfItemsPerTransaction = 20
-    outFileName = '/Users/Likhitha/Downloads/out.txt'
-
-    tDG = generateTransactionalDatabase(numOfTransactions, maxNumOfDistinctItems, numOfItemsPerTransaction, outFileName)
-    obj = generateTransactionalDatabase(sys.argv[1], sys.argv[2], sys.argv[3],sys.argv[4])
-    obj.getFileName(sys.argv[5])
+        values = self.generate_array(self.numLines, self.avgItemsPerLine, self.numItems)
+
+        for value in values:
+            line = np.random.choice(range(1, self.numItems + 1), value, replace=False)
+            self.db.append(line)
+
+    def save(self, filename) -> None:
+        """
+        Save the transactional database to a file
+
+        Parameters:
+        filename: str - name of the file
+    
+        """
+
+        with open(filename, 'w') as f:
+            for line in self.db:
+                f.write(','.join(map(str, line)) + '\n')
+
+    def getTransactions(self) -> pd.DataFrame:
+        """
+        Get the transactional database
+
+        Returns:
+        db: list - transactional database
+        
+        """
+        df = pd.DataFrame(self.db)
+        return df
+        
+
+if __name__ == "__main__":
+    # test the class
+
+    obj = TransactionalDatabase(sys.argv[1], sys.argv[2], sys.argv[3])
+    obj.create()
+    obj.save(sys.argv[4])
+    # print(obj.getTransactions())
+
```

### Comparing `pami-2024.3.9.2/PAMI/extras/generateLatexGraphFile.py` & `pami-2024.4.9.1/PAMI/extras/generateLatexGraphFile.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/graph/DF2Fig.py` & `pami-2024.4.9.1/PAMI/extras/graph/DF2Fig.py`

 * *Files 23% similar despite different names*

```diff
@@ -10,16 +10,19 @@
 #     obj.plotGraphsFromDataFrame("minSup", "patterns")
 #
 #     obj.plotGraphsFromDataFrame("minSup", "memory")
 #
 #     obj.plotGraphsFromDataFrame("minSup", "runtime")
 #
 
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
      
      This program is distributed in the hope that it will be useful,
@@ -71,14 +74,28 @@
 
     def __init__(self, dataFrame: _pd.DataFrame) -> None:
         self._dataFrame = dataFrame
 
     def plot(self, xColumn, yColumn, algorithm=None) -> None:
         """
         To plot graphs from given dataframe
+
+        :param xColumn: Name of the X-axis of the dataframe
+
+        :type xColumn: str
+
+        :param yColumn: Name of the Y-axis of the dataframe
+
+        :type yColumn: str
+
+        :param algorithm: Specify the column name containing the algorithms
+
+        :type algorithm: str
+
+        :return: None
         """
         if algorithm is None:
             fig = _px.line(self._dataFrame, x=self._dataFrame[xColumn] , y=self._dataFrame[yColumn], color=self._dataFrame.iloc[:, 0], labels={'x': xColumn, 'y': yColumn})
         else:
             fig = _px.line(self._dataFrame, x=self._dataFrame[xColumn], y=self._dataFrame[yColumn],
                            color=self._dataFrame[algorithm], labels={'x': xColumn, 'y': yColumn})
```

### Comparing `pami-2024.3.9.2/PAMI/extras/graph/DF2Tex.py` & `pami-2024.4.9.1/PAMI/extras/graph/DF2Tex.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,16 +9,20 @@
 #
 #     obj.generateLatexCode(result, "minSup", "runtime", "algorithmColumn")
 #
 #     obj.print()
 #
 #     obj.save("outputFile.tex")
 #
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.3.9.2/PAMI/extras/graph/plotLineGraphFromDictionary.py` & `pami-2024.4.9.1/PAMI/extras/graph/plotLineGraphFromDictionary.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,18 +4,21 @@
 # --------------------------------------------------------
 #
 #     from PAMI.extras.graph import plotLineGraphFromDictionary as plt
 #
 #     obj = plt.plotLineGraphFromDictionary(idict, 100, 0, " ")
 #
 #     obj.save()
+#
+
+
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -64,14 +67,15 @@
         :type start: int
         :param title: title of graph
         :type title: str
         :param xlabel: xlabel of graph
         :type xlabel: str
         :param ylabel: ylabel of grapth
         :type ylabel: str
+        :return: None
         """
         end = int(len(data) * end / 100)
         start = int(len(data) * start / 100)
         x = list(range(len(data)))
         y = list(tuple(data.values())[start:end])
         fig, ax = plt.subplots()
         ax.plot(x, y, marker='.')
```

### Comparing `pami-2024.3.9.2/PAMI/extras/graph/plotLineGraphsFromDataFrame.py` & `pami-2024.4.9.1/PAMI/extras/graph/plotLineGraphsFromDataFrame.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,18 +5,21 @@
 #
 #     from PAMI.extras.graph import plotLineGraphsFromDataFrame as plt
 #
 #     obj = plt.plotLineGraphsFromDictionary(idf)
 #
 #     obj.save()
 #
-#
+
+
+
+
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.3.9.2/PAMI/extras/graph/visualizeFuzzyPatterns.py` & `pami-2024.4.9.1/PAMI/extras/graph/visualizeFuzzyPatterns.py`

 * *Files 4% similar despite different names*

```diff
@@ -5,18 +5,21 @@
 #
 #     from PAMI.extras.graph import visualizeFuzzyPatterns as viz
 #
 #     obj = viz.visualizeFuzzyPatterns(iFile, topk)
 #
 #     obj.save()
 #
-#
+
+
+
+
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -61,18 +64,23 @@
         self.file = file
         self.topk = topk
 
     def visualize(self, markerSize: int = 20, zoom: int = 3, width: int = 1500, height: int = 1000) -> None:
         """
         Visualize points produced by pattern miner.
 
-        :param markerSize: integer
-        :param zoom: int
-        :param width: int
-        :param height: int
+        :param markerSize: Size of the marker
+        :type markerSize: int
+        :param zoom: Zoom level
+        :type zoom: int
+        :param width: Width of the graph
+        :type width: int
+        :param height: Height of the graph on the screen
+        :type width: int
+        :return: None
         """
 
         long = []
         lat = []
         name = []
         color = []
         R = G = B = 0
```

### Comparing `pami-2024.3.9.2/PAMI/extras/graph/visualizePatterns.py` & `pami-2024.4.9.1/PAMI/extras/graph/visualizePatterns.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,18 +5,21 @@
 #
 #     from PAMI.extras.graph import visualizePatterns as viz
 #
 #     obj = viz.visualizePatterns(iFile, topk)
 #
 #     obj.save()
 #
-#
+
+
+
+
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -32,47 +35,52 @@
 import pandas as pd
 import sys
 
 
 class visualizePatterns():
     """
 
-   :Description:   visualizePatterns is used to visualize points produced by pattern miner .
+       :Description:   visualizePatterns is used to visualize points produced by pattern miner .
 
-   :Attributes:
+       :Attributes:
 
-        :param file : file
-            store input data as file
-        :param topk : int
-            Takes the value int as input
+            :param file : file
+                store input data as file
+            :param topk : int
+                Takes the value int as input
 
-   **Importing this algorithm into a python program**
-   --------------------------------------------------------
-   .. code-block:: python
+       **Importing this algorithm into a python program**
+       --------------------------------------------------------
+       .. code-block:: python
 
-            from PAMI.extras.graph import visualizePatterns as viz
+                from PAMI.extras.graph import visualizePatterns as viz
 
-            obj = viz.visualizePatterns(iFile, topk)
+                obj = viz.visualizePatterns(iFile, topk)
 
-            obj.save()
+                obj.save()
 
     """
 
     def __init__(self, file: str, topk: int) -> None:
         self.file = file
         self.topk = topk
 
     def visualize(self, markerSize: int = 20, zoom: int = 3, width: int = 1500, height: int = 1000) -> None:
         """
         Visualize points produced by pattern miner.
 
-        :param markerSize: integer
-        :param zoom: int
-        :param width: int
-        :param height: int
+        :param markerSize: Size of the marker
+        :type markerSize: int
+        :param zoom: Zoom level
+        :type zoom: int
+        :param width: Width of the graph
+        :type width: int
+        :param height: Height of the graph on the screen
+        :type width: int
+        :return: None
         """
 
         long = []
         lat = []
         name = []
         color = []
         R = G = B = 0
```

### Comparing `pami-2024.3.9.2/PAMI/extras/imageProcessing/imagery2Databases.py` & `pami-2024.4.9.1/PAMI/extras/imageProcessing/imagery2Databases.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,18 +5,21 @@
 #
 #     from PAMI.extras.imageProcessing import imagery2Databases as db
 #
 #     obj = db.imagery2Databases(detected_objects, 16 )
 #
 #     obj.save()
 #
-#
+
+
+
+
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.3.9.2/PAMI/extras/messaging/discord.py` & `pami-2024.4.9.1/PAMI/extras/messaging/discord.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/messaging/gmail.py` & `pami-2024.4.9.1/PAMI/extras/messaging/gmail.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.3.9.2/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py` & `pami-2024.4.9.1/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,18 +5,21 @@
 #
 #     from PAMI.extras.neighbours import findNeighboursUsingEuclidean as db
 #
 #     obj = db.findNeighboursUsingGeodesic(iFile, oFile, 10, "\t")
 #
 #     obj.save()
 #
-#
+
+
+
+
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.3.9.2/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py` & `pami-2024.4.9.1/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,18 +5,20 @@
 #
 #     from PAMI.extras.neighbours import findNeighboursUsingEuclidean as db
 #
 #     obj = db.findNeighboursUsingGeodesic(iFile, oFile, 10, "\t")
 #
 #     obj.save()
 #
-#
+
+
+
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.3.9.2/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py` & `pami-2024.4.9.1/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,18 +5,20 @@
 #
 #     from PAMI.extras.neighbours import findNeighboursUsingGeodesic as db
 #
 #     obj = db.findNeighboursUsingGeodesic(iFile, oFile, 10, "\t")
 #
 #     obj.save()
 #
-#
+
+
+
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.3.9.2/PAMI/extras/plotPointOnMap.py` & `pami-2024.4.9.1/PAMI/extras/plotPointOnMap.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/plotPointOnMap_dump.py` & `pami-2024.4.9.1/PAMI/extras/plotPointOnMap_dump.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/scatterPlotSpatialPoints.py` & `pami-2024.4.9.1/PAMI/extras/scatterPlotSpatialPoints.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/generateTemporal.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/generateTransactional.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateTransactional.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/syntheticDataGenerator/utilityDatabase.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/utilityDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/topKPatterns.py` & `pami-2024.4.9.1/PAMI/extras/topKPatterns.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/extras/uncertaindb_convert.py` & `pami-2024.4.9.1/PAMI/extras/uncertaindb_convert.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py` & `pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py`

 * *Files 3% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # ----------------------------------------------------------------
 #
 #
 #             from PAMI.faultTolerantFrequentPattern.basic import FTApriori as alg
 #
 #             obj = alg.FTApriori(inputFile,minSup,itemSup,minLength,faultTolerance)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             patterns = obj.getPatterns()
 #
 #             print("Total number of fault-tolerant frequent patterns:", len(patterns))
 #
 #             obj.save("outputFile")
 #
@@ -29,15 +29,15 @@
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -50,14 +50,15 @@
      Copyright (C)  2021 Rage Uday Kiran
      
 """
 
 from PAMI.faultTolerantFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 
 class FTApriori(_ab._faultTolerantFrequentPatterns):
     """
     
     :Description:   FT-Apriori is one of the fundamental algorithm to discover fault-tolerant frequent patterns in a transactional database.
                     This program employs apriori property (or downward closure property) to  reduce the search space effectively.
@@ -124,15 +125,15 @@
     ----------------------------------------------------------------
     .. code-block:: python
     
             from PAMI.faultTolerantFrequentPattern.basic import FTApriori as alg
     
             obj = alg.FTApriori(inputFile,minSup,itemSup,minLength,faultTolerance)
     
-            obj.startMine()
+            obj.mine()
     
             patterns = obj.getPatterns()
     
             print("Total number of fault-tolerant frequent patterns:",  len(patterns))
     
             obj.save("outputFile")
     
@@ -291,14 +292,15 @@
         for i in range(0, len(l) + 1):
             c = _ab._itertools.combinations(l, i)
             for j in c:
                 res = self._countItemSupport(j)
                 if len(j) >= self._minLength and res >= self._minSup:
                     self._finalPatterns[tuple(j)] = res
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Fault-tolerant frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
         self._creatingItemSets()
@@ -313,14 +315,36 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Fault-Tolerant Frequent patterns were generated successfully using FTApriori algorithm ")
 
+    def mine(self) -> None:
+        """
+        Fault-tolerant frequent pattern mining process will start from here
+        """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._itemSup = self._convert(self._itemSup)
+        self._minLength = int(self._minLength)
+        self._faultTolerance = int(self._faultTolerance)
+        self._oneLengthFrequentItems()
+
+        self._getFaultPatterns()
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Fault-Tolerant Frequent patterns were generated successfully using FTApriori algorithm ")
+
     def getMemoryUSS(self) -> float:
         """
 
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
 
@@ -425,14 +449,15 @@
     if len(_ab._sys.argv) == 7 or len(_ab._sys.argv) == 8:
         if len(_ab._sys.argv) == 8:
             _ap = FTApriori(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4],
                             _ab._sys.argv[5], _ab._sys.argv[6], _ab._sys.argv[7], )
         if len(_ab._sys.argv) == 7:
             _ap = FTApriori(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py` & `pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # --------------------------------------------------
 #
 #
 #             from PAMI.faultTolerantFrequentPattern.basic import FTFPGrowth as alg
 #
 #             obj = alg.FTFPGrowth(inputFile,minSup,itemSup,minLength,faultTolerance)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             faultTolerantFrequentPatterns = obj.getPatterns()
 #
 #             print("Total number of fault-tolerant frequent patterns:", len(faultTolerantFrequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -31,15 +31,15 @@
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -54,14 +54,15 @@
 """
 
 
 
 from PAMI.faultTolerantFrequentPattern.basic import abstract as _fp
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 import pandas as pd
+from deprecated import deprecated
 
 _minSup = str()
 _fp._sys.setrecursionlimit(20000)
 
 
 class _Node:
     """
@@ -347,15 +348,15 @@
     -------------------------------------------
     .. code-block:: python
 
             from PAMI.faultTolerantFrequentPattern.basic import FTFPGrowth as alg
 
             obj = alg.FTFPGrowth(inputFile,minSup,itemSup,minLength,faultTolerance)
 
-            obj.startMine()
+            obj.mine()
 
             patterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(patterns))
 
             obj.save(oFile)
 
@@ -545,14 +546,15 @@
 
         """
         temp = str()
         for i in itemSet:
             temp = temp + self.__rankDup[i] + "\t"
         return temp
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Main program to start the operation
         """
         global _minSup
         self.__startTime = _fp._time.time()
         if self._iFile is None:
@@ -577,14 +579,46 @@
         self.__endTime = _fp._time.time()
         self.__memoryUSS = float()
         self.__memoryRSS = float()
         process = _fp._psutil.Process(_fp._os.getpid())
         self.__memoryUSS = process.memory_full_info().uss
         self.__memoryRSS = process.memory_info().rss
 
+    def mine(self) -> None:
+        """
+        Main program to start the operation
+        """
+        global _minSup
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self.__creatingItemSets()
+        self._minSup = self.__convert(self._minSup)
+        _minSup = self._minSup
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        for x, y in self.__rank.items():
+            self.__rankDup[y] = x
+        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Frequent patterns were generated successfully using frequentPatternGrowth algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
 
         :rtype: float
@@ -677,14 +711,15 @@
     if len(_fp._sys.argv) == 7 or len(_fp._sys.argv) == 8:
         if len(_fp._sys.argv) == 8:
             _ap = FTFPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4],
                              _fp._sys.argv[5], _fp._sys.argv[6], _fp._sys.argv[7])
         if len(_fp._sys.argv) == 7:
             _ap = FTFPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_fp._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/faultTolerantFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/frequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/basic/Apriori.py` & `pami-2024.4.9.1/PAMI/frequentPattern/basic/Apriori.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
 #             import PAMI.frequentPattern.basic.Apriori as alg
 #
 #             obj = alg.Apriori(iFile, minSup)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -24,22 +24,22 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
-
+#
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -49,27 +49,28 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 from PAMI.frequentPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 class Apriori(_ab._frequentPatterns):
     """
     :Description: Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 
     :Reference:  Agrawal, R., Imieli ́nski, T., Swami, A.: Mining association rules between sets of items in large databases.
             In: SIGMOD. pp. 207–216 (1993), https://doi.org/10.1145/170035.170072
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
-    :param  minSup: int :
+    :param  minSup: int or float or str :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
 
     :Attributes:
@@ -93,33 +94,37 @@
           To store the transactions of a database in list
 
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
-            Format:
-                      >>> python3 Apriori.py <inputFile> <outputFile> <minSup>
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 Apriori.py <inputFile> <outputFile> <minSup>
+
+      Example Usage:
 
-            Example:
-                      >>>  python3 Apriori.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 Apriori.py sampleDB.txt patterns.txt 10.0
 
-            .. note:: minSup will be considered in percentage of database transactions
+    .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
-    ----------------------------------------------------
+    -----------------------------------------------------
 
     .. code-block:: python
 
             import PAMI.frequentPattern.basic.Apriori as alg
 
             obj = alg.Apriori(iFile, minSup)
 
-            obj.startMine()
+            obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.save(oFile)
 
@@ -191,16 +196,23 @@
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
         To convert the user specified minSup value
+
         :param value: user specified minSup value
+
+        :type value: int or float or str
+
         :return: converted type
+
+        :rtype: int or float
+
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
@@ -209,18 +221,23 @@
             else:
                 value = int(value)
         return value
 
     def _candidateToFrequent(self, candidateList: List[set]) -> Dict[frozenset, int]:
         """
         Generates frequent patterns from the candidate patterns
-        :param candidateList: Candidate patterns will be given as input
+
+        :param candidateList: Candidate pattern will be given as input
+
         :type candidateList: list
+
         :return: returning set of all frequent patterns
+
         :rtype: dict
+
         """
 
         candidateToFrequentList = {}
         for i in self._Database:
             dictionary = {frozenset(j): int(candidateToFrequentList.get(frozenset(j), 0)) + 1 for j in candidateList if
                           j.issubset(i)}
             candidateToFrequentList.update(dictionary)
@@ -228,29 +245,38 @@
                                    value >= self._minSup}
 
         return candidateToFrequentList
 
     @staticmethod
     def _frequentToCandidate(frequentList: Dict[frozenset, int], length: int) -> List[set]:
         """
+
         Generates candidate patterns from the frequent patterns
+
         :param frequentList: set of all frequent patterns to generate candidate patterns of each of size is length
+
         :type frequentList: dict
+
         :param length: size of each candidate patterns to be generated
+
         :type length: int
+
         :return: set of candidate patterns in sorted order
+
         :rtype: list
+
         """
 
         frequentToCandidateList = []
         for i in frequentList:
             nextList = [i | j for j in frequentList if len(i | j) == length and (i | j) not in frequentToCandidateList]
             frequentToCandidateList.extend(nextList)
         return sorted(frequentToCandidateList)
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
         self._creatingItemSets()
@@ -273,73 +299,129 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Frequent patterns were generated successfully using Apriori algorithm ")
 
+    def mine(self) -> None:
+        """
+        Frequent pattern mining process will start from here
+        """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        itemsList = sorted(list(set.union(*self._Database)))  # because Database is list
+        items = [{i} for i in itemsList]
+        itemsCount = len(items)
+        self._minSup = self._convert(self._minSup)
+        self._finalPatterns = {}
+        for i in range(1, itemsCount):
+            frequentSet = self._candidateToFrequent(items)
+            for x, y in frequentSet.items():
+                sample = str()
+                for k in x:
+                    sample = sample + k + "\t"
+                self._finalPatterns[sample] = y
+            items = self._frequentToCandidate(frequentSet, i + 1)
+            if len(items) == 0:
+                break  # finish apriori
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using Apriori algorithm ")
+
     def getMemoryUSS(self) -> float:
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
+
         :rtype: float
+
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
+
         :rtype: float
+
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
+
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
+
         :rtype: float
+
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
+
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
+
         :rtype: pd.DataFrame
+
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile) -> None:
         """
+
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
+
         :type outFile: csvfile
+
+        :return: None
+
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self) -> Dict[str, int]:
         """
+
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
+
         :rtype: dict
+
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the result
         """
@@ -353,14 +435,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = Apriori(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = Apriori(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ap._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/basic/ECLAT.py` & `pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLAT.py`

 * *Files 5% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 # **Importing this algorithm into a python program**
 # ------------------------------------------------------------------
 #
 #             import PAMI.frequentPattern.basic.ECLAT as alg
 #
 #             obj = alg.ECLAT(iFile, minSup)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -24,22 +24,22 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
-
+#
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -49,14 +49,15 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 from PAMI.frequentPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 class ECLAT(_ab._frequentPatterns):
     """
     :Description: ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 
     :Reference:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
             372-390 (2000), https://ieeexplore.ieee.org/document/846291
@@ -90,32 +91,36 @@
         Database : list
           To store the transactions of a database in list
 
 
     **Methods to execute code on terminal**
     ------------------------------------------
 
-            Format:
-                      >>> python3 ECLAT.py <inputFile> <outputFile> <minSup>
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 ECLAT.py <inputFile> <outputFile> <minSup>
+
+      Example Usage:
 
-            Example:
-                      >>>  python3 ECLAT.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 ECLAT.py sampleDB.txt patterns.txt 10.0
 
-            .. note:: minSup will be considered in percentage of database transactions
+    .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ------------------------------------------------------------------
     .. code-block:: python
 
             import PAMI.frequentPattern.basic.ECLAT as alg
 
             obj = alg.ECLAT(iFile, minSup)
 
-            obj.startMine()
+            obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.save(oFile)
 
@@ -151,14 +156,18 @@
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
 
     def _creatingItemSets(self) -> float:
         """
         Storing the complete transactions of the database/input file in a database variable
+
+        :return: the complete transactions of the database/input file in a database variable
+
+        :rtype: float
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
@@ -182,15 +191,21 @@
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _getUniqueItemList(self) -> list:
         """
+
         Generating one frequent patterns
+
+        :return: list of unique patterns
+
+        :rtype: list
+
         """
         self._finalPatterns = {}
         candidate = {}
         uniqueItem = []
         for i in range(len(self._Database)):
             for j in range(len(self._Database[i])):
                 if self._Database[i][j] not in candidate:
@@ -203,19 +218,23 @@
                 self._finalPatterns[key] = [value]
                 uniqueItem.append(key)
         uniqueItem.sort()
         return uniqueItem
 
     def _generateFrequentPatterns(self, candidateFrequent: list) -> None:
         """
+
         It will generate the combinations of frequent items
+
         :param candidateFrequent :it represents the items with their respective transaction identifiers
+
         :type candidateFrequent: list
-        :return: returning transaction dictionary
-        :rtype: dict
+
+        :return: None
+
         """
         new_freqList = []
         for i in range(0, len(candidateFrequent)):
             item1 = candidateFrequent[i]
             i1_list = item1.split()
             for j in range(i + 1, len(candidateFrequent)):
                 item2 = candidateFrequent[j]
@@ -229,30 +248,37 @@
                 else: break
 
         if len(new_freqList) > 0:
                 self._generateFrequentPatterns(new_freqList)
 
     def _convert(self, value) -> float:
         """
+
         To convert the user specified minSup value
+
         :param value: user specified minSup value
+
         :return: converted type
+
+        :rtype: float
+
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Frequent pattern mining process will start from here
         """
 
         self._startTime = _ab._time.time()
         if self._iFile is None:
@@ -269,71 +295,117 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Frequent patterns were generated successfully using ECLAT algorithm")
 
+    def mine(self) -> None:
+        """
+        Frequent pattern mining process will start from here
+        """
+
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        uniqueItemList = self._getUniqueItemList()
+        self._generateFrequentPatterns(uniqueItemList)
+        for x, y in self._finalPatterns.items():
+            self._finalPatterns[x] = len(y[0])
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using ECLAT algorithm")
+
     def getMemoryUSS(self) -> float:
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
+
         :rtype: float
+
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
+
         :rtype: float
+
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
+
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
+
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
+
         :rtype: pd.DataFrame
+
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
     def save(self, outFile: str) -> None:
         """
+
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
+
         :type outFile: csvfile
+
+        :return: None
+
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self) -> dict:
         """
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
+
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         Function used to print the results
@@ -348,14 +420,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = ECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = ECLAT(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print(_ap.getPatternsAsDataFrame())
         print("Total Memory in USS:",  _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/basic/ECLATDiffset.py` & `pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLATDiffset.py`

 * *Files 3% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 # **Importing this algorithm into a python program**
 # ---------------------------------------------------------
 #
 #             import PAMI.frequentPattern.basic.ECLATDiffset as alg
 #
 #             obj = alg.ECLATDiffset(iFile, minSup)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.savePatterns(oFile)
 #
@@ -24,22 +24,23 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
 
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -51,14 +52,15 @@
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
 # from abstract import *
 
 from PAMI.frequentPattern.basic import abstract as _ab
+from deprecated import deprecated
 
 
 class ECLATDiffset(_ab._frequentPatterns):
     """
     :Description:   ECLATDiffset uses diffset to extract the frequent patterns in a transactional database.
 
     :Reference:  KDD '03: Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining
@@ -91,34 +93,38 @@
           To store the total amount of RSS memory consumed by the program
         
         Database : list
           To store the transactions of a database in list
           
         
     **Methods to execute code on terminal**
-    ----------------------------------------
-    
-            Format:
-                      >>> python3 ECLATbitset.py <inputFile> <outputFile> <minSup>
-    
-            Example:
-                      >>> python3 ECLATbitset.py sampleDB.txt patterns.txt 10.0
-    
-            .. note:: minSup will be considered in percentage of database transactions
-    
+    ------------------------------------------
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 ECLATDiffset.py <inputFile> <outputFile> <minSup>
+
+      Example Usage:
+
+      (.venv) $ python3 ECLATDiffset.py sampleDB.txt patterns.txt 10.0
+
+    .. note:: minSup will be considered in percentage of database transactions
+
     
     **Importing this algorithm into a python program**
     ---------------------------------------------------------
     .. code-block:: python
 
             import PAMI.frequentPattern.basic.ECLATDiffset as alg
 
             obj = alg.ECLATDiffset(iFile, minSup)
 
-            obj.startMine()
+            obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.savePatterns(oFile)
 
@@ -260,14 +266,15 @@
                         newList.append(newKey)
                     else: 
                         break
 
         if len(newList) > 0:
             self._runDeclat(newList)
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
 
         self._startTime = _ab._time.time()
         self._Database = []
@@ -290,14 +297,44 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Frequent patterns were generated successfully using ECLAT Diffset algorithm")
 
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+
+        self._startTime = _ab._time.time()
+        self._Database = []
+        self._finalPatterns = {}
+        self._diffSets = {}
+        self._trans_set = set()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        #print(len(self._Database))
+        self._minSup = self._convert(self._minSup)
+        uniqueItemList = []
+        uniqueItemList = self._getUniqueItemList()
+        self._runDeclat(uniqueItemList)
+        self._finalPatterns = self._diffSets
+        #print(len(self._finalPatterns), len(uniqueItemList))
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using ECLAT Diffset algorithm")
+
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -369,14 +406,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = ECLATDiffset(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = ECLATDiffset(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print(_ap.getPatternsAsDataFrame())
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/basic/ECLATbitset.py` & `pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLATbitset.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-#ECLATbitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+# ECLATbitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 #
-#**Importing this algorithm into a python program**
-#---------------------------------------------------------
+# **Importing this algorithm into a python program**
+# ---------------------------------------------------------
 #
 #             import PAMI.frequentPattern.basic.ECLATbitset as alg
 #
 #             obj = alg.ECLATbitset(iFile, minSup)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -24,22 +24,22 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
-
+#
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -49,14 +49,15 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
 from PAMI.frequentPattern.basic import abstract as _ab
+from deprecated import deprecated
 
 class ECLATbitset(_ab._frequentPatterns):
     """
     :Description:  ECLATbitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 
     :Reference:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
             372-390 (2000), https://ieeexplore.ieee.org/document/846291
@@ -88,34 +89,38 @@
           To store the total amount of RSS memory consumed by the program
 
         Database : list
           To store the transactions of a database in list
 
 
     **Methods to execute code on terminal**
-    ----------------------------------------
+    ------------------------------------------
+
+    .. code-block:: console
 
-            Format:
-                      >>> python3 ECLATDiffset.py <inputFile> <outputFile> <minSup>
+      Format:
 
-            Example:
-                      >>> python3 ECLATDiffset.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 ECLATbitset.py <inputFile> <outputFile> <minSup>
 
-            .. note:: minSup will be considered in percentage of database transactions
+      Example Usage:
+
+      (.venv) $ python3 ECLATbitset.py sampleDB.txt patterns.txt 10.0
+
+    .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ---------------------------------------------------------
     .. code-block:: python
 
             import PAMI.frequentPattern.basic.ECLATbitset as alg
 
             obj = alg.ECLATbitset(iFile, minSup)
 
-            obj.startMine()
+            obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.save(oFile)
 
@@ -153,16 +158,22 @@
     _mapSupport = {}
     _lno = 0
 
 
     def _convert(self, value):
         """
         To convert the user specified minSup value
+
         :param value: user specified minSup value
+
+        :type value: int
+
         :return: converted type
+
+        :rtype: int or float or string
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
@@ -205,15 +216,19 @@
                 except IOError:
                     print("File Not Found")
         self._minSup = self._convert(self._minSup)
 
     def creatingFrequentItems(self):
         """
         This function creates frequent items from _database.
+
         :return: frequentTidData that stores frequent items and their tid list.
+
+        :rtype: Dict
+
         """
         tidData = {}
         self._lno = 0
         for transaction in self._Database:
             self._lno = self._lno + 1
             for item in transaction:
                 if item not in tidData:
@@ -222,23 +237,24 @@
                     tidData[item].append(self._lno)
         frequentTidData = {k: v for k, v in tidData.items() if len(v) >= self._minSup}
         frequentTidData = dict(sorted(frequentTidData.items(), reverse=True, key=lambda x: len(x[1])))
         return frequentTidData
 
     def tidToBitset(self,itemset):
         """
+
         This function converts tid list to bitset.
 
-        Parameters:
+        :param itemset: frequent itemset that generated
 
-            itemSet: frequent itemset that generated
+        :type itemset: Dict
 
-        Returns:
+        :return: patterns with original item names
 
-            patterns with original item names.
+        :rtype: Dict
 
         """
         bitset = {}
 
         for k,v in itemset.items():
             bitset[k] = 0b1
             bitset[k] = (bitset[k] << int(v[0])) | 0b1
@@ -246,18 +262,25 @@
                 diff = int(v[i]) - int(v[i-1])
                 bitset[k] = (bitset[k] << diff) | 0b1
             bitset[k] = (bitset[k] << (self._lno - int(v[i])))
         return bitset
 
     def genPatterns(self,prefix,tidData):
         """
+
         This function generate frequent pattern about prefix.
-        :param prefix: String
-        :param tidData: list
-        :return:
+
+        :param prefix: prefix of pattern to generate patterns
+
+        :type prefix: str
+
+        :param tidData: tidData for pattern generation
+
+        :type tidData: list
+
         """
         # variables to store frequent item set and
         itemset = prefix[0]
 
         # Get the length of tidData
         length = len(tidData)
 
@@ -270,23 +293,27 @@
                 frequentItemset = itemset + '\t' + tidData[i][0]
                 self._finalPatterns[frequentItemset] = count
                 self.genPatterns((frequentItemset,tid),tidData[i+1:length])
 
     def genAllFrequentPatterns(self,frequentItems):
         """
         This function generates all frequent patterns.
+
         :param frequentItems: frequent items
-        :return:
+
+        :type frequentItems: Dict
+
         """
         tidData = list(frequentItems.items())
         length = len(tidData)
         for i in range(length):
             #print(i,tidData[i][0])
             self.genPatterns(tidData[i],tidData[i+1:length])
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         We start with the scanning the itemSets and store the bitsets respectively.
         We form the combinations of single items and  check with minSup condition to check the frequency of patterns
         """
 
@@ -305,14 +332,40 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Frequent patterns were generated successfully using Eclat_bitset algorithm")
 
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        We start with the scanning the itemSets and store the bitsets respectively.
+        We form the combinations of single items and  check with minSup condition to check the frequency of patterns
+        """
+
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+
+        self._creatingItemSets()
+        frequentItems = self.creatingFrequentItems()
+        self._finalPatterns = {k: len(v) for k, v in frequentItems.items()}
+        frequentItemsBitset = self.tidToBitset(frequentItems)
+        self.genAllFrequentPatterns(frequentItemsBitset)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using Eclat_bitset algorithm")
+
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -383,14 +436,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = ECLATbitset(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = ECLATbitset(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/basic/FPGrowth.py` & `pami-2024.4.9.1/PAMI/frequentPattern/basic/FPGrowth.py`

 * *Files 3% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #             from PAMI.frequentPattern.basic import FPGrowth as alg
 #
 #             obj = alg.FPGrowth(iFile, minSup)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -24,20 +24,21 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -47,14 +48,15 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 from PAMI.frequentPattern.basic import abstract as _fp
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 _minSup = str()
 _fp._sys.setrecursionlimit(20000)
 
 
 class _Node:
     """
@@ -85,15 +87,15 @@
         self.children = children
 
     def addChild(self, node) -> None:
         """
         Retrieving the child from the tree
         :param node: Children node of the tree
         :type node: Node
-        :return: Updates the children nodes and parent nodes
+        :return: None
         """
         self.children[node.itemId] = node
         node.parent = self
 
 
 class _Tree:
     """
@@ -123,19 +125,27 @@
     def __init__(self) -> None:
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
     def addTransaction(self, transaction, count) -> None:
         """
+
         Adding transaction into tree
+
         :param transaction: it represents the one transaction in database
+
         :type transaction: list
+
         :param count: frequency of item
+
         :type count: int
+
+        :return: None
+
         """
 
         # This method takes transaction as input and returns the tree
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
@@ -149,18 +159,22 @@
             else:
                 currentNode = currentNode.children[transaction[i]]
                 currentNode.freq += count
 
     def getFinalConditionalPatterns(self, alpha) -> Tuple[List[List[int]], List[int], Dict[int, int]]:
         """
         Generates the conditional patterns for a node
-        Parameters:
-            alpha: node to generate conditional patterns
-        Returns:
-            returns conditional patterns, frequency of each item in conditional patterns
+
+        :param alpha: node to generate conditional patterns
+
+        :type alpha:
+
+        :return: conditional patterns, frequency of each item in conditional patterns
+
+        :rtype: Tuple, List and Dict
 
         """
         finalPatterns = []
         finalFreq = []
         for i in self.summaries[alpha]:
             set1 = i.freq
             set2 = []
@@ -173,21 +187,29 @@
                 finalFreq.append(set1)
         finalPatterns, finalFreq, info = self.getConditionalTransactions(finalPatterns, finalFreq)
         return finalPatterns, finalFreq, info
 
     @staticmethod
     def getConditionalTransactions(ConditionalPatterns, conditionalFreq) -> Tuple[List[List[int]], List[int], Dict[int, int]]:
         """
+
         To calculate the frequency of items in conditional patterns and sorting the patterns
-        Parameters:
-        ConditionalPatterns: paths of a node
-        conditionalFreq: frequency of each item in the path
 
-        Returns:
-            conditional patterns and frequency of each item in transactions
+        :param ConditionalPatterns: paths of a node
+
+        :type ConditionalPatterns: Dict
+
+        :param conditionalFreq: frequency of each item in the path
+
+        :type conditionalFreq: Dict
+
+        :return: conditional patterns and frequency of each item in transactions
+
+        :rtype: Tuple, List, Dict
+
         """
         global _minSup
         pat = []
         freq = []
         data1 = {}
         for i in range(len(ConditionalPatterns)):
             for j in ConditionalPatterns[i]:
@@ -204,19 +226,25 @@
                 pat.append(trans)
                 freq.append(conditionalFreq[count])
             count += 1
         return pat, freq, up_dict
 
     def generatePatterns(self, prefix) -> Tuple[List[List[int]], List[int]]:
         """
+
         To generate the frequent patterns
-        Parameters:
-        prefix: an empty list
-        Returns:
-        Frequent patterns that are extracted from fp-tree
+
+        :param prefix: an empty list
+
+        :type prefix: list
+
+        :return: Frequent patterns that are extracted from fp-tree
+
+        :rtype: Tuple, List
+
         """
         for i in sorted(self.summaries, key=lambda x: (self.info.get(x), -x)):
             pattern = prefix[:]
             pattern.append(i)
             yield pattern, self.info[i]
             patterns, freq, info = self.getFinalConditionalPatterns(i)
             conditionalTree = _Tree()
@@ -275,32 +303,37 @@
             it represents the Tree class
         finalPatterns : dict
             it represents to store the patterns
 
 
     **Methods to execute code on terminal**
     --------------------------------------------------------
-        Format:
-                  >>> python3 FPGrowth.py <inputFile> <outputFile> <minSup>
 
-        Example:
-                  >>> python3 FPGrowth.py sampleDB.txt patterns.txt 10.0
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 FPGrowth.py <inputFile> <outputFile> <minSup>
 
-        .. note:: minSup will be considered in percentage of database transactions
+      Example Usage:
+
+      (.venv) $ python3 FPGrowth.py sampleDB.txt patterns.txt 10.0
+
+    .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
 
             from PAMI.frequentPattern.basic import FPGrowth as alg
 
             obj = alg.FPGrowth(iFile, minSup)
 
-            obj.startMine()
+            obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.savePatterns(oFile)
 
@@ -376,17 +409,23 @@
                             self.__Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def __convert(self, value) -> float:
         """
+
         To convert the type of user specified minSup value
+
         :param value: user specified minSup value
+
         :return: converted type
+
+        :rtype: float
+
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
@@ -394,15 +433,21 @@
                 value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
     def __frequentOneItem(self) -> List[str]:
         """
+
         Generating One frequent items sets
+
+        :return: list of generated frequent items set
+
+        :rtype: list
+
         """
         self.__mapSupport = {}
         for tr in self.__Database:
             for i in range(0, len(tr)):
                 if tr[i] not in self.__mapSupport:
                     self.__mapSupport[tr[i]] = 1
                 else:
@@ -410,19 +455,28 @@
         self.__mapSupport = {k: v for k, v in self.__mapSupport.items() if v >= self._minSup}
         genList = [k for k, v in sorted(self.__mapSupport.items(), key=lambda x: x[1], reverse=True)]
         self.__rank = dict([(index, item) for (item, index) in enumerate(genList)])
         return genList
 
     def __updateTransactions(self, itemSet) -> List[List[int]]:
         """
+
         Updates the items in transactions with rank of items according to their support
+
         :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
                     rank = {'a':0, 'b':1, 'c':2, 'd':3}
-        Parameters:
-        itemSet: list of one-frequent items
+
+        :param itemSet: list of one-frequent items
+
+        :type itemSet: list
+
+        :return: list of updated items with rank of items according to their support
+
+        :rtype: list
+
         """
         list1 = []
         for tr in self.__Database:
             list2 = []
             for i in range(len(tr)):
                 if tr[i] in itemSet:
                     list2.append(self.__rank[tr[i]])
@@ -430,44 +484,56 @@
                 list2.sort()
                 list1.append(list2)
         return list1
 
     @staticmethod
     def __buildTree(transactions, info) -> _Tree:
         """
+
         Builds the tree with updated transactions
-        Parameters:
-            transactions: updated transactions
-            info: support details of each item in transactions
 
-        Returns:
-            transactions compressed in fp-tree
+        :param transactions: updated transactions
+
+        :type transactions: list
+
+        :param info: support details of each item in transactions
+
+        :type info: dict
+
+        :return: transactions compressed in fp-tree
+
+        :rtype: Tree
 
         """
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(transactions)):
             rootNode.addTransaction(transactions[i], 1)
         return rootNode
 
     def __savePeriodic(self, itemSet) -> str:
         """
+
         The duplication items and their ranks
-        Parameters:
-            itemSet: frequent itemSet that generated
 
-        Returns:
-            patterns with original item names.
+        :param itemSet: frequent itemSet that generated
+
+        :type itemSet: dict
+
+        :return: patterns with original item names.
+
+        :rtype: string
 
         """
         temp = str()
         for i in itemSet:
             temp = temp + self.__rankDup[i] + "\t"
         return temp
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Main program to start the operation
         """
         global _minSup
         self.__startTime = _fp._time.time()
         if self._iFile is None:
@@ -492,18 +558,52 @@
         self.__endTime = _fp._time.time()
         self.__memoryUSS = float()
         self.__memoryRSS = float()
         process = _fp._psutil.Process(_fp._os.getpid())
         self.__memoryUSS = process.memory_full_info().uss
         self.__memoryRSS = process.memory_info().rss
 
+    def mine(self) -> None:
+        """
+        Main program to start the operation
+        """
+        global _minSup
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self.__creatingItemSets()
+        self._minSup = self.__convert(self._minSup)
+        _minSup = self._minSup
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        for x, y in self.__rank.items():
+            self.__rankDup[y] = x
+        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Frequent patterns were generated successfully using frequentPatternGrowth algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
+
         :rtype: float
         """
 
         return self.__memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
@@ -513,39 +613,47 @@
         """
 
         return self.__memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
+
         :rtype: float
         """
 
         return self.__endTime - self.__startTime
 
     def getPatternsAsDataFrame(self) -> _fp._pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
+
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self.__finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
+
         :type outFile: csvfile
+
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self.__finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
@@ -571,14 +679,15 @@
     _ap = str()
     if len(_fp._sys.argv) == 4 or len(_fp._sys.argv) == 5:
         if len(_fp._sys.argv) == 5:
             _ap = FPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4])
         if len(_fp._sys.argv) == 4:
             _ap = FPGrowth(_fp._sys.argv[1], _fp._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Frequent Patterns:", len( _ap.getPatterns()))
         _ap.save(_fp._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/frequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/closed/CHARM.py` & `pami-2024.4.9.1/PAMI/frequentPattern/closed/CHARM.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # --------------------------------------------------------------
 #
 #
 #             from PAMI.frequentPattern.closed import CHARM as alg
 #
 #             obj = alg.CHARM(iFile, minSup)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Closed Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.savePatterns(oFile)
 #
@@ -25,21 +25,21 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
-
+#
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -49,14 +49,15 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
 from PAMI.frequentPattern.closed import abstract as _ab
+from deprecated import deprecated
 
 
 class CHARM(_ab._frequentPatterns):
     """
     :Description: CHARM is an algorithm to discover closed frequent patterns in a transactional database. Closed frequent patterns are patterns if there exists no superset that has the same support count as this original itemset. This algorithm employs depth-first search technique to find the complete set of closed frequent patterns in a
 
 
@@ -109,32 +110,36 @@
         hashing : dict
             stores the patterns with their support to check for the closed property
 
 
     **Methods to execute code on terminal**
     --------------------------------------------------------------
 
-            Format:
-                      >>> python3 CHARM.py <inputFile> <outputFile> <minSup>
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 CHARM.py <inputFile> <outputFile> <minSup>
+
+      Example Usage:
 
-            Example:
-                      >>> python3 CHARM.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 CHARM.py sampleDB.txt patterns.txt 10.0
 
-            .. note:: minSup will be considered in percentage of database transactions
+    .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     --------------------------------------------------------------
     .. code-block:: python
 
             from PAMI.frequentPattern.closed import CHARM as alg
 
             obj = alg.CHARM(iFile, minSup)
 
-            obj.startMine()
+            obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Closed Frequent Patterns:", len(frequentPatterns))
 
             obj.savePatterns(oFile)
 
@@ -177,17 +182,23 @@
     _itemSetCount = 0
     _maxItemId = 0
     _tableSize = 10000
     _writer = None
 
     def _convert(self, value):
         """
+
         To convert the type of user specified minSup value
+
         :param value: user specified minSup value
+
+        :type value: int or float or str
+
         :return: converted type
+
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._lno * value)
         if type(value) is str:
             if '.' in value:
@@ -262,16 +273,19 @@
             _flist[x] = t1
         _flist = [key for key, value in sorted(_flist.items(), key=lambda x: x[1])]
         return _flist
 
     def _calculate(self, tidSet):
         """
         To calculate the hashcode of pattern
+
         :param tidSet: the timestamps of a pattern
+
         :type tidSet: list
+
         :rtype: int
         """
 
         hashcode = 0
         for i in tidSet:
             hashcode += i
         if hashcode < 0:
@@ -297,18 +311,25 @@
                 return True
         return False
 
     def _save(self, prefix, suffix, tidSetx):
         """
         Check for the closed property (patterns with same support), if found deletes the subsets and stores
         supersets and also saves the patterns that satisfy the closed property
+
         :param prefix: the prefix of a pattern
+
+        :type prefix: frequent item or pattern
+
         :param suffix: the suffix of a patterns
+
         :type suffix: list
+
         :param tidSetx: the timestamp of a patterns
+
         :type tidSetx: list
         """
         if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
         prefix = list(set(prefix))
@@ -389,14 +410,15 @@
                     classItemSets.append(itemY)
                     classTidSets.append(y)
             if len(classItemSets) > 0:
                 newPrefix = list(set(itemSetx)) + prefix
                 self._processEquivalenceClass(newPrefix, classItemSets, classTidSets)
                 self._save(prefix, list(set(itemSetx)), tidSetX)
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Mining process will start from here by extracting the frequent patterns from the database. It performs prefix
         equivalence to generate the combinations and closed frequent patterns.
         """
         self._startTime = _ab._time.time()
         _plist = self._creatingItemsets()
@@ -437,14 +459,62 @@
         self._endTime = _ab._time.time()
         _process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = _process.memory_full_info().uss
         self._memoryRSS = _process.memory_info().rss
 
+    def mine(self):
+        """
+        Mining process will start from here by extracting the frequent patterns from the database. It performs prefix
+        equivalence to generate the combinations and closed frequent patterns.
+        """
+        self._startTime = _ab._time.time()
+        _plist = self._creatingItemsets()
+        self._finalPatterns = {}
+        self._hashing = {}
+        for i in range(len(_plist)):
+            itemX = _plist[i]
+            if itemX is None:
+                continue
+            tidSetx = self._tidList[itemX]
+            itemSetx = [itemX]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(_plist)):
+                itemY = _plist[j]
+                if itemY is None:
+                    continue
+                tidSetY = self._tidList[itemY]
+                y1 = list(set(tidSetx).intersection(tidSetY))
+                if len(y1) < self._minSup:
+                    continue
+                if len(tidSetx) == len(tidSetY) and len(y1) == len(tidSetx):
+                    _plist.insert(j, None)
+                    itemSetx.append(itemY)
+                elif len(tidSetx) < len(tidSetY) and len(y1) == len(tidSetx):
+                    itemSetx.append(itemY)
+                elif len(tidSetx) > len(tidSetY) and len(y1) == len(tidSetY):
+                    _plist.insert(j, None)
+                    itemSets.append(itemY)
+                    tidSets.append(y1)
+                else:
+                    itemSets.append(itemY)
+                    tidSets.append(y1)
+            if len(itemSets) > 0:
+                self._processEquivalenceClass(itemSetx, itemSets, tidSets)
+            self._save(None, itemSetx, tidSetx)
+        print("Closed Frequent patterns were generated successfully using CHARM algorithm")
+        self._endTime = _ab._time.time()
+        _process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = _process.memory_full_info().uss
+        self._memoryRSS = _process.memory_info().rss
+
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -517,14 +587,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = CHARM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = CHARM(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Closed Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         _memRSS = _ap.getMemoryRSS()
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/closed/abstract.py` & `pami-2024.4.9.1/PAMI/frequentPattern/closed/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/cuda/abstract.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/cuda/cuApriori.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuApriori.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
 #             import PAMI.frequentPattern.cuda.cuApriori as alg
 #
 #             obj = alg.cuApriori(iFile, minSup)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -25,21 +25,21 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
-
+#
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -47,15 +47,15 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-
+from deprecated import deprecated
 from PAMI.frequentPattern.cuda import abstract as _ab
 # import abstract as _ab
 
 class cuApriori(_ab._frequentPatterns):
     """
     :Description: cuApriori is one of the fundamental algorithm to discover frequent patterns using Cuda in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 
@@ -92,33 +92,37 @@
           To store the transactions of a database in list
 
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
-            Format:
-                      >>> python3 cuApriori.py <inputFile> <outputFile> <minSup>
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 cuApriori.py <inputFile> <outputFile> <minSup>
+
+      Example Usage:
 
-            Example:
-                      >>>  python3 cuApriori.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 cuApriori.py sampleDB.txt patterns.txt 10.0
 
-            .. note:: minSup will be considered in percentage of database transactions
+    .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
 
     .. code-block:: python
 
              import PAMI.frequentPattern.cuda.cuApriori as alg
 
              obj = alg.cuApriori(iFile, minSup)
 
-             obj.startMine()
+             obj.mine()
 
              frequentPatterns = obj.getPatterns()
 
              print("Total number of Frequent Patterns:", len(frequentPatterns))
 
              obj.save(oFile)
 
@@ -209,17 +213,23 @@
                             self._Database.append(set(temp))
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _convert(self, value):
         """
+
         To convert the user specified minSup value
+
         :param value: user specified minSup value
+
+        :type value: int or float or str
+
         :return: converted type
+
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
@@ -246,14 +256,15 @@
             ArraysAndItems[k] = _ab._cp.array(v, dtype=_ab._np.uint32)
             if len(v) >= self._minSup:
                 self._finalPatterns[k] = len(v)
                 newArraysAndItems[k] = ArraysAndItems[k]
 
         return newArraysAndItems
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
         self._creatingItemSets()
@@ -283,14 +294,51 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Frequent patterns were generated successfully using cuApriori algorithm ")
 
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+
+        ArraysAndItems = self.arraysAndItems()
+
+        while len(ArraysAndItems) > 0:
+            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
+            newArraysAndItems = {}
+            keys = list(ArraysAndItems.keys())
+            for i in range(len(ArraysAndItems)):
+                # print(i, "/", len(ArraysAndItems), end="\r")
+                iList = list(keys[i])
+                for j in range(i + 1, len(ArraysAndItems)):
+                    jList = list(keys[j])
+                    union = tuple(sorted(set(iList + jList)))
+                    intersect = _ab._cp.intersect1d(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]],
+                                                    assume_unique=True)
+                    if len(intersect) >= self._minSup and union not in self._finalPatterns:
+                        newArraysAndItems[union] = intersect
+                        self._finalPatterns[union] = len(intersect)
+            ArraysAndItems = newArraysAndItems
+            # print()
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using cuApriori algorithm ")
+
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -329,15 +377,15 @@
         # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csvfile
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
@@ -362,14 +410,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = cuApriori(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = cuApriori(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in s:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/cuda/cuAprioriBit.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuAprioriBit.py`

 * *Files 11% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
 #             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
 #
 #             obj = alg.cuAprioriBit(iFile, minSup)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -25,18 +25,20 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
+
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -46,14 +48,15 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 # from PAMI.frequentPattern.cuda import abstract as _ab
 import abstract as _ab
+from deprecated import deprecated
 
 
 class cuAprioriBit(_ab._frequentPatterns):
     """
     :Description: cuAprioriBit is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 
     :Reference:  Agrawal, R., Imieli ́nski, T., Swami, A.: Mining association rules between sets of items in large databases.
@@ -89,33 +92,37 @@
           To store the transactions of a database in list
 
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
-            Format:
-                      >>> python3 cuAprioriBit.py <inputFile> <outputFile> <minSup>
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 cuAprioriBit.py <inputFile> <outputFile> <minSup>
 
-            Example:
-                      >>>  python3 cuAprioriBit.py sampleDB.txt patterns.txt 10.0
+      Example Usage:
 
-            .. note:: minSup will be considered in percentage of database transactions
+      (.venv) $ python3 cuAprioriBit.py sampleDB.txt patterns.txt 10.0
+
+    .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
 
     .. code-block:: python
 
             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
 
             obj = alg.cuAprioriBit(iFile, minSup)
 
-            obj.startMine()
+            obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.save(oFile)
 
@@ -204,17 +211,23 @@
                             self._Database.append(set(temp))
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _convert(self, value):
         """
+
         To convert the user specified minSup value
+
         :param value: user specified minSup value
+
+        :type value: int or float or str
+
         :return: converted type
+
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
@@ -255,14 +268,15 @@
                 bitRep[k][i // 32] |= 1 << 31 - (i % 32)
 
         for k, v in bitRep.items():
             bitRep[k] = _ab._cp.array(v)
 
         return bitRep
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
         self._creatingItemSets()
@@ -297,14 +311,56 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Frequent patterns were generated successfully using cuAprioriBit algorithm ")
 
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+
+        ArraysAndItems = self.arraysAndItems()
+        ArraysAndItems = self.createBitRepresentation(ArraysAndItems)
+
+        while len(ArraysAndItems) > 0:
+            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
+            newArraysAndItems = {}
+            keys = list(ArraysAndItems.keys())
+            for i in range(len(ArraysAndItems)):
+                # print(i, "/", len(ArraysAndItems), end="\r")
+                iList = list(keys[i])
+                for j in range(i + 1, len(ArraysAndItems)):
+                    unionData = _ab._cp.bitwise_and(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]])
+                    sum = _ab._cp.zeros(1, dtype=_ab._np.uint32)
+                    self._sumKernel((len(unionData) // 32 + 1,), (32,),
+                                    (unionData, sum, _ab._cp.uint32(len(unionData))))
+                    sum = sum[0]
+                    jList = list(keys[j])
+                    union = tuple(sorted(set(iList + jList)))
+                    if sum >= self._minSup and union not in self._finalPatterns:
+                        newArraysAndItems[union] = unionData
+                        string = "\t".join(union)
+                        self._finalPatterns[string] = sum
+            ArraysAndItems = newArraysAndItems
+            # print()
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using cuAprioriBit algorithm ")
+
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -343,15 +399,15 @@
         # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csvfile
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
@@ -378,14 +434,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = cuAprioriBit(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = cuAprioriBit(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/cuda/cuEclat.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuEclat.py`

 * *Files 9% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
 #             import PAMI.frequentPattern.cuda.cuEclat as alg
 #
 #             obj = alg.cuEclat(iFile, minSup)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -25,21 +25,21 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
-
+#
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -50,14 +50,15 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
 # from PAMI.frequentPattern.cuda import abstract as _ab
 import abstract as _ab
+from deprecated import deprecated
 
 class cuEclat(_ab._frequentPatterns):
     """
     :Description: ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 
     :Reference:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
             372-390 (2000), https://ieeexplore.ieee.org/document/846291
@@ -94,33 +95,37 @@
           To store the transactions of a database in list
 
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
-            Format:
-                      >>> python3 cuEclat.py <inputFile> <outputFile> <minSup>
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 cuEclat.py <inputFile> <outputFile> <minSup>
 
-            Example:
-                      >>>  python3 cuEclat.py sampleDB.txt patterns.txt 10.0
+      Example Usage:
 
-            .. note:: minSup will be considered in percentage of database transactions
+      (.venv) $ python3 cuEclat.py sampleDB.txt patterns.txt 10.0
+
+    .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
 
     .. code-block:: python
 
              import PAMI.frequentPattern.cuda.cuEclat as alg
 
              obj = alg.cuEclat(iFile, minSup)
 
-             obj.startMine()
+             obj.mine()
 
              frequentPatterns = obj.getPatterns()
 
              print("Total number of Frequent Patterns:", len(frequentPatterns))
 
              obj.save(oFile)
 
@@ -196,17 +201,23 @@
                             self._Database.append(set(temp))
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _convert(self, value):
         """
+
         To convert the user specified minSup value
+
         :param value: user specified minSup value
+
+        :type value: int or float or str
+
         :return: converted type
+
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
@@ -237,15 +248,15 @@
             ArraysAndItems[k] = _ab._cp.array(v, dtype=_ab._np.uint32)
             if len(v) >= self._minSup:
                 self._finalPatterns[k] = len(v)
                 newArraysAndItems[k] = ArraysAndItems[k]
 
         return newArraysAndItems
 
-    
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
         self._creatingItemSets()
@@ -278,14 +289,55 @@
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Frequent patterns were generated successfully using cuEclat algorithm ")
+
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+
+        ArraysAndItems = self._arraysAndItems()
+
+        while len(ArraysAndItems) > 0:
+            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
+            newArraysAndItems = {}
+            keys = list(ArraysAndItems.keys())
+            for i in range(len(ArraysAndItems)):
+                iList = list(keys[i])
+                for j in range(i+1, len(ArraysAndItems)):
+                    # print(i, "/", len(ArraysAndItems), end="\r")
+                    jList = list(keys[j])
+                    if iList[:-1] == jList[:-1] and iList[-1] != jList[-1]:
+                        union = iList + [jList[-1]]
+                        union = tuple(union)
+                        intersect = _ab._cp.intersect1d(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]], assume_unique=True)
+                        if len(intersect) >= self._minSup:
+                            newArraysAndItems[union] = intersect
+                            self._finalPatterns[union] = len(intersect)
+                    else:
+                        break
+
+            ArraysAndItems = newArraysAndItems
+            # print()
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using cuEclat algorithm ")
             
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
@@ -360,14 +412,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = cuEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = cuEclat(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in s:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/cuda/cuEclatBit.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuEclatBit.py`

 * *Files 10% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
 #             import PAMI.frequentPattern.cuda.cuEclatBit as alg
 #
 #             obj = alg.cuEclatBit(iFile, minSup)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -25,18 +25,21 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
+
+
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -47,14 +50,15 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
 # from PAMI.frequentPattern.cuda import abstract as _ab
 import abstract as _ab
+from deprecated import deprecated
 
 class cuEclatBit(_ab._frequentPatterns):
     """
     :Description: ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 
     :Reference:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
             372-390 (2000), https://ieeexplore.ieee.org/document/846291
@@ -89,33 +93,37 @@
           To store the transactions of a database in list
 
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
-            Format:
-                      >>> python3 cuEclatBit.py <inputFile> <outputFile> <minSup>
+    .. code-block:: console
+
+      Format:
 
-            Example:
-                      >>>  python3 cuEclatBit.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 cuEclatBit.py <inputFile> <outputFile> <minSup>
 
-            .. note:: minSup will be considered in percentage of database transactions
+      Example Usage:
+
+      (.venv) $ python3 cuEclatBit.py sampleDB.txt patterns.txt 10.0
+
+    .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
 
     .. code-block:: python
 
              import PAMI.frequentPattern.cuda.cuEclatBit as alg
 
              obj = alg.cuEclatBit(iFile, minSup)
 
-             obj.startMine()
+             obj.mine()
 
              frequentPatterns = obj.getPatterns()
 
              print("Total number of Frequent Patterns:", len(frequentPatterns))
 
              obj.save(oFile)
 
@@ -209,17 +217,23 @@
                             self._Database.append(set(temp))
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _convert(self, value):
         """
+
         To convert the user specified minSup value
+
         :param value: user specified minSup value
+
+        :type value: int or float or str
+
         :return: converted type
+
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
@@ -261,15 +275,15 @@
                 bitRep[k][i // 32] |= 1 << 31 - (i % 32)
 
         for k, v in bitRep.items():
             bitRep[k] = _ab._cp.array(v)
 
         return bitRep
 
-    
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
         self._creatingItemSets()
@@ -307,14 +321,60 @@
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Frequent patterns were generated successfully using cuEclatBit algorithm ")
+
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        itemsList = sorted(list(set.union(*self._Database)))  # because Database is list
+        self._minSup = self._convert(self._minSup)
+
+        ArraysAndItems = self.arraysAndItems()
+
+        ArraysAndItems = self.createBitRepresentation(ArraysAndItems)
+
+        while len(ArraysAndItems) > 0:
+            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
+            newArraysAndItems = {}
+            keys = list(ArraysAndItems.keys())
+            for i in range(len(ArraysAndItems)):
+                iList = list(keys[i])
+                # print(i, "/", len(ArraysAndItems), end="\r")
+                for j in range(i+1, len(ArraysAndItems)):
+                    jList = list(keys[j])
+                    union = []
+                    if iList[:-1] == jList[:-1] and iList[-1] != jList[-1]:
+                        union = iList + [jList[-1]]
+                        union = tuple(union)
+                        unionData = _ab._cp.bitwise_and(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]])
+                        sum = _ab._cp.zeros(1, dtype=_ab._np.uint32)
+                        self._sumKernel((len(unionData) // 32 + 1,), (32,), (unionData, sum, _ab._cp.uint32(len(unionData))))
+                        sum = sum[0]
+                        if sum >= self._minSup and union not in self._finalPatterns:
+                            newArraysAndItems[union] = unionData
+                            string = "\t".join(union)
+                            self._finalPatterns[string] = sum
+            ArraysAndItems = newArraysAndItems
+            # print()
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using cuEclatBit algorithm ")
             
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
@@ -355,15 +415,15 @@
         # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csvfile
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
@@ -389,14 +449,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = cuEclatBit(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = cuEclatBit(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/cuda/cudaAprioriGCT.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaEclatGCT.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,18 +1,17 @@
-# cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns using CUDA in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
-#
+# cudaEclatGCT is one of the fundamental algorithm to discover frequent patterns in a transactional database. It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It  employs downward closure property to  reduce the search space effectively.
 #
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------
+# --------------------------------------------------------
 #
-#             import PAMI.frequentPattern.cuda.cudaAprioriGCT as alg
+#             from PAMI.frequentPattern.cuda.cudaEclatGCT as alg
 #
-#             obj = alg.cuAprioriGCT(iFile, minSup)
+#             obj = alg.FPGrowth(iFile, minSup)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -25,18 +24,22 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
+
+
+
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -44,86 +47,92 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
+from deprecated import deprecated
 from PAMI.frequentPattern.basic import abstract as _ab
-# import abstract as _ab
+
+minSup = str()
+_ab._sys.setrecursionlimit(20000)
 
 import os
 import csv
 import time
 import numpy as np
-import pycuda.gpuarray as gpuarray
+import pycuda.gpuarray as _gpuarray
 import pycuda.autoinit
 import psutil
 
 
-class cudaAprioriGCT(_ab._frequentPatterns):
+class cudaEclatGCT:
     """
-    :Description: cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+    :Description: Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 
     :Reference:  Agrawal, R., Imieli ́nski, T., Swami, A.: Mining association rules between sets of items in large databases.
                 In: SIGMOD. pp. 207–216 (1993), https://doi.org/10.1145/170035.170072
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int :
-                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         startTime : float
-              To record the start time of the mining process
+            To record the start time of the mining process
 
         endTime : float
-              To record the completion time of the mining process
+            To record the completion time of the mining process
 
         finalPatterns : dict
-              Storing the complete set of patterns in a dictionary variable
+            Storing the complete set of patterns in a dictionary variable
 
         memoryUSS : float
-              To store the total amount of USS memory consumed by the program
+            To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
-              To store the total amount of RSS memory consumed by the program
+            To store the total amount of RSS memory consumed by the program
 
         Database : list
-              To store the transactions of a database in list
+            To store the transactions of a database in list
 
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
-                Format:
-                          >>> python3 cudaAprioriGCT.py <inputFile> <outputFile> <minSup>
+    .. code-block:: console
 
-                Example:
-                          >>>  python3 cudaAprioriGCT.py sampleDB.txt patterns.txt 10.0
+      Format:
 
-                .. note:: minSup will be considered in percentage of database transactions
+      (.venv) $ python3 cudaEclatGCT.py <inputFile> <outputFile> <minSup>
+
+      Example Usage:
+
+      (.venv) $ python3 cudaEclatGCT.py sampleDB.txt patterns.txt 10.0
+
+    .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
-
     .. code-block:: python
 
-            import PAMI.frequentPattern.cuda.cuAprioriGCT as alg
+            import PAMI.frequentPattern.cuda.cuAprioriBit as alg
 
-            obj = alg.cuAprioriGCT(iFile, minSup)
+            obj = alg.cuAprioriBit(iFile, minSup)
 
-            obj.startMine()
+            obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.save(oFile)
 
@@ -140,17 +149,15 @@
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
     **Credits:**
     -------------
-
-                The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
-
+             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
     """
 
     __time = 0
     __memRSS = 0
     __memUSS = 0
     __GPU_MEM = 0
     _minSup = 0
@@ -160,14 +167,28 @@
         self._iFile = filePath
         self._sep = sep
         self._minSup = minSup
         self.__time = 0
         self.__memRSS = 0
         self.__memUSS = 0
 
+    """def read_data(self, data_path, sep):
+
+
+        data = []
+        if not os.path.isfile(data_path):
+            raise ValueError('Invalid data path.' + data_path)
+        with open(data_path, 'r') as f:
+            file = csv.reader(f, delimiter=sep, quotechar='\r')
+            lineNo = 1
+            for row in file:
+                data.append([str(item) for item in row if item != ''])
+                lineNo += 1
+        return data, lineNo"""
+
     def __creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
         self.__Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
@@ -196,17 +217,23 @@
                             self.__Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def __convert(self, value):
         """
+
         To convert the type of user specified minSup value
+
         :param value: user specified minSup value
+
+        :type value: int or float or str
+
         :return: converted type
+
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
@@ -214,15 +241,15 @@
                 value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
     def compute_vertical_bitvector_data(self):
         """
-        Converting database into bit vector
+        Converting  database into bit vector
         """
         # ---build item to idx mapping---#
         idx = 0
         item2idx = {}
         for transaction in self.__Database:
             for item in transaction:
                 if not item in item2idx:
@@ -230,15 +257,15 @@
                     idx += 1
         idx2item = {idx: str(int(item)) for item, idx in item2idx.items()}
         # ---build vertical data---#
         vb_data = np.zeros((len(item2idx), len(self.__Database)), dtype=np.uint16)
         for trans_id, transaction in enumerate(self.__Database):
             for item in transaction:
                 vb_data[item2idx[item], trans_id] = 1
-        vb_data = gpuarray.to_gpu(vb_data.astype(np.uint16))
+        vb_data = _gpuarray.to_gpu(vb_data.astype(np.uint16))
         return vb_data, idx2item
 
     def getRuntime(self):
         """
         Calculating the total amount of time taken by the mining process
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
@@ -247,15 +274,14 @@
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
-
         return self.__memRSS
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
@@ -276,119 +302,120 @@
         Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def get_numberOfPatterns(self):
+
         return len(self._finalPatterns)
 
-    def getPatternsAsDataFrame(self):
-        """
-        Storing final frequent patterns in a dataframe
-        :return: returning frequent patterns in a dataframe
-        :rtype: pd.DataFrame
+    def eclat(self, basePattern, final, vb_data, idx2item, item2idx):
         """
+        param basePattern: base pattern used for the mining process after completion of the mining process
+        type basePattern:
+        param final: final pattern used for the mining process after completion of the mining process
+        type final:
+        param vb_data: vb_data used for the mining process after completion of the mining process
+        type vb_data:
+        param idx2item: idx2item used for the mining process after completion of the mining process
+        type idx2item:
+        param item2idx: item2idx used for the mining process after completion of the mining process
+        type item2idx:
+        """
+        newBasePattern = []
+        for i in range(0, len(basePattern)):
+            item1 = basePattern[i]
+            i1_list = item1.split()
+            for j in range(i + 1, len(basePattern)):
+                item2 = basePattern[j]
+                i2_list = item2.split()
+                if i1_list[:-1] == i2_list[:-1]:
+                    unionOfKey = list(set(i1_list) | set(i2_list))
+                    unionOfKey.sort()
+                    valueList = []
+                    for key in unionOfKey:
+                        valueList.append(item2idx[key])
+                    total = vb_data[valueList[0]]
+                    for k in range(1, len(valueList)):
+                        total = total.__mul__(vb_data[valueList[k]])
+                    support = _gpuarray.sum(total).get()
+                    if support >= self._minSup:
+                        newBasePattern.append(" ".join(unionOfKey))
+                        final[" ".join(unionOfKey)] = support
 
-        dataFrame = {}
-        data = []
-        for a, b in self._finalPatterns.items():
-            data.append([a, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataFrame
-
-    def save(self, outFile):
-        """
-        Complete set of frequent patterns will be loaded in to an output file
-        :param outFile: name of the output file
-        :type outFile: file
-        """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            if type(x) == tuple:
-                pattern = ""
-                for item in x:
-                    pattern = pattern + str(item) + " "
-                s1 = pattern + ":" + str(y)
-            else:
-                s1 = str(x) + ":" + str(y)
-            writer.write("%s \n" % s1)
+        if len(newBasePattern) > 0:
+            self.eclat(newBasePattern, final, vb_data, idx2item, item2idx)
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
         startTime = time.time()
-        basePattern = {}
+        basePattern = []
         final = {}
 
         self.__creatingItemSets()
         self._minSup = self.__convert(self._minSup)
         minSup = self._minSup
         vb_data, idx2item = self.compute_vertical_bitvector_data()
 
         for i in range(len(vb_data)):
-            if gpuarray.sum(vb_data[i]).get() >= self._minSup:
-                basePattern[idx2item[i]] = [i]
-                final[idx2item[i]] = gpuarray.sum(vb_data[i]).get()
-
-        while len(basePattern) > 0:
-            temp = {}
-            keysList = list(basePattern.keys())
-            valuesList = list(basePattern.values())
-            for i in range(len(basePattern) - 1):
-                keyI = keysList[i].split(" ")
-                keyI = [int(x) for x in keyI]
-
-                for j in range(i + 1, len(basePattern)):
-                    keyJ = keysList[j].split(" ")
-                    keyJ = [int(x) for x in keyJ]
-                    values = set(valuesList[i])
-                    for val in valuesList[j]:
-                        values.add(val)
-                    values = list(sorted(values))
-                    totalArray = vb_data[values[0]]
-                    for k in range(1, len(values)):
-                        totalArray = totalArray.__mul__(vb_data[values[k]])
-                    support = gpuarray.sum(totalArray).get()
-                    if support >= self._minSup:
-                        combinedKey = " ".join(
-                            str(x) for x in sorted(set(keyI) | set(keyJ)))
-                        temp[combinedKey] = values
-                        final[str(combinedKey)] = support
-            basePattern = temp
-
+            if _gpuarray.sum(vb_data[i]).get() >= self._minSup:
+                basePattern.append(idx2item[i])
+                final[idx2item[i]] = _gpuarray.sum(vb_data[i]).get()
+
+        # reverse idx2item
+        item2idx = {idx2item[i]: i for i in idx2item}
+        self.eclat(basePattern, final, vb_data, idx2item, item2idx)
         self.__time = time.time() - startTime
         self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
         self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
         self._finalPatterns = final
         self.__GPU_MEM = vb_data.nbytes
 
-    def printResults(self):
+    def mine(self):
         """
-        This function is used to print the results
+        Frequent pattern mining process will start from here
         """
-        print("Total number of Coverage Patterns:", len(self.getPatterns()))
-        print("GPU MEM: ", _ap.getGPUMemory())
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        startTime = time.time()
+        basePattern = []
+        final = {}
+
+        self.__creatingItemSets()
+        self._minSup = self.__convert(self._minSup)
+        minSup = self._minSup
+        vb_data, idx2item = self.compute_vertical_bitvector_data()
+
+        for i in range(len(vb_data)):
+            if _gpuarray.sum(vb_data[i]).get() >= self._minSup:
+                basePattern.append(idx2item[i])
+                final[idx2item[i]] = _gpuarray.sum(vb_data[i]).get()
+
+        # reverse idx2item
+        item2idx = {idx2item[i]: i for i in idx2item}
+        self.eclat(basePattern, final, vb_data, idx2item, item2idx)
+        self.__time = time.time() - startTime
+        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
+        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
+        self._finalPatterns = final
+        self.__GPU_MEM = vb_data.nbytes
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cudaEclatGCT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cudaEclatGCT(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
+        _ap.save(_ap._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("GPU MEM: ", _ap.getGPUMemory())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/cuda/cudaAprioriTID.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaAprioriGCT.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-# cudaAprioriTID is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+# cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns using CUDA in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 #
 #
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
-#             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
+#             import PAMI.frequentPattern.cuda.cudaAprioriGCT as alg
 #
-#             obj = alg.cuAprioriBit(iFile, minSup)
+#             obj = alg.cuAprioriGCT(iFile, minSup)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -25,21 +25,21 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
-
+#
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -47,268 +47,227 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-
-
-import abstract as _ab
+from deprecated import deprecated
+from PAMI.frequentPattern.basic import abstract as _ab
+# import abstract as _ab
 
 import os
-import csv
 import time
 import numpy as np
-import pycuda.gpuarray as _gpuarray
-import pycuda.autoinit
+import pycuda.gpuarray as gpuarray
 import psutil
-import pycuda.driver as cuda
-from pycuda.compiler import SourceModule
-import pycuda
-
-deviceIntersection = SourceModule("""
-    __global__ void intersection(int *compareThis, int *compareThat, int *resultStart,
-                                 int *values, int *result, int resultX, int resultY){
-        const int tidX = blockIdx.x * blockDim.x + threadIdx.x;
-        const int tidY = blockIdx.y * blockDim.y + threadIdx.y;
-        int resultIndex = resultStart[tidX] + tidY;
-
-        // ignore if tidX or tidY is out of bounds or if the value comparing with is 0
-        if (tidX > resultX-1 || tidY > resultY-1 || values[compareThis[tidX] + tidY] == 0) return;
-
-        for (int i = 0; i < resultY; i++){
-            if ( values[compareThat[tidX] + i] == 0) return;
-            if ( values[compareThis[tidX] + tidY] == values[compareThat[tidX] + i]){
-                result[resultIndex] = values[compareThis[tidX] + tidY];
-                return;
-            }
-        }
-
-        //result[resultIndex] = values[compareThis[tidX] + tidY];
 
-    }
 
-"""
-                                  )
-
-
-class cudaAprioriTID:
+class cudaAprioriGCT(_ab._frequentPatterns):
     """
-    :Description: Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+    :Description: cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 
     :Reference:  Agrawal, R., Imieli ́nski, T., Swami, A.: Mining association rules between sets of items in large databases.
-            In: SIGMOD. pp. 207–216 (1993), https://doi.org/10.1145/170035.170072
+                In: SIGMOD. pp. 207–216 (1993), https://doi.org/10.1145/170035.170072
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         startTime : float
-          To record the start time of the mining process
+              To record the start time of the mining process
 
         endTime : float
-          To record the completion time of the mining process
+              To record the completion time of the mining process
 
         finalPatterns : dict
-          Storing the complete set of patterns in a dictionary variable
+              Storing the complete set of patterns in a dictionary variable
 
         memoryUSS : float
-          To store the total amount of USS memory consumed by the program
+              To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
-          To store the total amount of RSS memory consumed by the program
+              To store the total amount of RSS memory consumed by the program
 
         Database : list
-          To store the transactions of a database in list
+              To store the transactions of a database in list
 
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
-            Format:
-                      >>> python3 cudaAprioriTID.py <inputFile> <outputFile> <minSup>
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 cudaAprioriGCT.py <inputFile> <outputFile> <minSup>
 
-            Example:
-                      >>>  python3 cudaAprioriTID.py sampleDB.txt patterns.txt 10.0
+      Example Usage:
 
-            .. note:: minSup will be considered in percentage of database transactions
+      (.venv) $ python3 cudaAprioriGCT.py sampleDB.txt patterns.txt 10.0
+
+    .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
 
     .. code-block:: python
 
-             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
+            import PAMI.frequentPattern.cuda.cuAprioriGCT as alg
 
-             obj = alg.cuAprioriBit(iFile, minSup)
+            obj = alg.cuAprioriGCT(iFile, minSup)
 
-             obj.startMine()
+            obj.mine()
 
-             frequentPatterns = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-             print("Total number of Frequent Patterns:", len(frequentPatterns))
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-             obj.save(oFile)
+            obj.save(oFile)
 
-             Df = obj.getPatternInDataFrame()
+            Df = obj.getPatternInDataFrame()
 
-             memUSS = obj.getMemoryUSS()
+            memUSS = obj.getMemoryUSS()
 
-             print("Total Memory in USS:", memUSS)
+            print("Total Memory in USS:", memUSS)
 
-             memRSS = obj.getMemoryRSS()
+            memRSS = obj.getMemoryRSS()
 
-             print("Total Memory in RSS", memRSS)
+            print("Total Memory in RSS", memRSS)
 
-             run = obj.getRuntime()
+            run = obj.getRuntime()
 
-             print("Total ExecutionTime in seconds:", run)
+            print("Total ExecutionTime in seconds:", run)
 
 
     **Credits:**
     -------------
 
-             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+                The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
 
     """
 
     __time = 0
     __memRSS = 0
     __memUSS = 0
     __GPU_MEM = 0
-    filePath = ""
-    _iFile = " "
-    _sep = ""
     _minSup = 0
-    Patterns = {}
+    _finalPatterns = {}
 
-    def __init__(self, filePath, sep, minSup):
-        self.filePath = filePath
-        self.sep = sep
-        self.minSup = minSup
+    def __init__(self, filePath, minSup, sep):
+        self._iFile = filePath
+        self._sep = sep
+        self._minSup = minSup
         self.__time = 0
         self.__memRSS = 0
         self.__memUSS = 0
 
-    def _creatingItemSets(self):
+    def __creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-        self._Database = {}
-        lineNumber = 1
+        self.__Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                temp = self._iFile['Transactions'].tolist()
+                self.__Database = self._iFile['Transactions'].tolist()
 
-            for k in temp:
-                self._Database.append(set(k))
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
-                    for i in range(len(line)):
-                        if line[i] in self._Database:
-                            self._Database[i].append(lineNumber)
-                        else:
-                            self._Database[i] = [lineNumber]
-                    lineNumber += 1
-
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(set(temp))
+                    self.__Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line.strip()
+                            line = line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(set(temp))
+                            self.__Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _convert(self, value):
+    def __convert(self, value):
         """
+
         To convert the type of user specified minSup value
+
         :param value: user specified minSup value
+
+        :type value: int or float or str
+
         :return: converted type
+
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self._Database) * value)
+                value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
-    """def _readFile(self, fileName, separator):
-        
-        Reads a file and stores the data in a dictionary
-
-        Args:
-            fileName: string
-            separator: string
-
-        Returns:
-            dictionary: dictionary
-        
-        file = open(fileName, 'r')
-        dictionary = {}
-        lineNumber = 1
-        for line in file:
-            line = line.strip()
-            line = line.split(separator)
-            for i in range(len(line)):
-                if line[i] in dictionary:
-                    dictionary[line[i]].append(lineNumber)
-                else:
-                    dictionary[line[i]] = [lineNumber]
-            lineNumber += 1
-
-        # sort dictionary by size of values
-        dictionary = dict(
-            sorted(dictionary.items(), key=lambda x: len(x[1]), reverse=True))
-        return dictionary, lineNumber
+    def compute_vertical_bitvector_data(self):
+        """
+        Converting database into bit vector
         """
+        # ---build item to idx mapping---#
+        idx = 0
+        item2idx = {}
+        for transaction in self.__Database:
+            for item in transaction:
+                if not item in item2idx:
+                    item2idx[item] = idx
+                    idx += 1
+        idx2item = {idx: str(int(item)) for item, idx in item2idx.items()}
+        # ---build vertical data---#
+        vb_data = np.zeros((len(item2idx), len(self.__Database)), dtype=np.uint16)
+        for trans_id, transaction in enumerate(self.__Database):
+            for item in transaction:
+                vb_data[item2idx[item], trans_id] = 1
+        vb_data = gpuarray.to_gpu(vb_data.astype(np.uint16))
+        return vb_data, idx2item
+
     def getRuntime(self):
         """
         Calculating the total amount of time taken by the mining process
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-
         return self.__time
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
+
         return self.__memRSS
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
@@ -326,147 +285,174 @@
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self.Patterns
+        return self._finalPatterns
 
     def get_numberOfPatterns(self):
-        return len(self.Patterns)
+        return len(self._finalPatterns)
 
+    def getPatternsAsDataFrame(self):
+        """
+        Storing final frequent patterns in a dataframe
+        :return: returning frequent patterns in a dataframe
+        :rtype: pd.DataFrame
+        """
+
+        dataFrame = {}
+        data = []
+        for a, b in self._finalPatterns.items():
+            data.append([a, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataFrame
+
+    def save(self, outFile):
+        """
+        Complete set of frequent patterns will be loaded in to an output file
+        :param outFile: name of the output file
+        :type outFile: csvfile
+        """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            if type(x) == tuple:
+                pattern = ""
+                for item in x:
+                    pattern = pattern + str(item) + " "
+                s1 = pattern + ":" + str(y)
+            else:
+                s1 = str(x) + ":" + str(y)
+            writer.write("%s \n" % s1)
+
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        dev_Intersection = deviceIntersection.get_function("intersection")
         startTime = time.time()
+        basePattern = {}
         final = {}
 
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
+        self.__creatingItemSets()
+        self._minSup = self.__convert(self._minSup)
         minSup = self._minSup
+        vb_data, idx2item = self.compute_vertical_bitvector_data()
+
+        for i in range(len(vb_data)):
+            if gpuarray.sum(vb_data[i]).get() >= self._minSup:
+                basePattern[idx2item[i]] = [i]
+                final[idx2item[i]] = gpuarray.sum(vb_data[i]).get()
+
+        while len(basePattern) > 0:
+            temp = {}
+            keysList = list(basePattern.keys())
+            valuesList = list(basePattern.values())
+            for i in range(len(basePattern) - 1):
+                keyI = keysList[i].split(" ")
+                keyI = [int(x) for x in keyI]
+
+                for j in range(i + 1, len(basePattern)):
+                    keyJ = keysList[j].split(" ")
+                    keyJ = [int(x) for x in keyJ]
+                    values = set(valuesList[i])
+                    for val in valuesList[j]:
+                        values.add(val)
+                    values = list(sorted(values))
+                    totalArray = vb_data[values[0]]
+                    for k in range(1, len(values)):
+                        totalArray = totalArray.__mul__(vb_data[values[k]])
+                    support = gpuarray.sum(totalArray).get()
+                    if support >= self._minSup:
+                        combinedKey = " ".join(
+                            str(x) for x in sorted(set(keyI) | set(keyJ)))
+                        temp[combinedKey] = values
+                        final[str(combinedKey)] = support
+            basePattern = temp
 
+        self.__time = time.time() - startTime
+        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
+        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
+        self._finalPatterns = final
+        self.__GPU_MEM = vb_data.nbytes
+
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        startTime = time.time()
+        basePattern = {}
+        final = {}
 
-        data = dict(filter(lambda x: len(x[1]) >= self.minSup, self._Database()))
-        for key, value in data.items():
-            final[key] = len(value)
-
-        while len(data) > 1:
-            # sort data by size of values
-            data = dict(
-                sorted(data.items(), key=lambda x: len(x[1]), reverse=True))
-
-            values = list(data.values())
-            maxLength = values[0]
-            for i in range(1, len(values)):
-                while len(values[i]) != len(maxLength):
-                    values[i].append(0)
-
-            values = np.array(values)
-            resultSize = 0
-
-            compareThis = []
-            compareThat = []
-            resultStart = []
-            counter = 0
-
-            for i in range(len(values)):
-                for j in range(i+1, len(values)):
-                    resultSize += 1
-                    compareThis.append(i*len(maxLength))
-                    compareThat.append(j*len(maxLength))
-                    resultStart.append(counter)
-                    counter += len(maxLength)
-            result = np.zeros((resultSize, len(maxLength)), dtype=np.int32)
-
-            # convert all to uint32
-            compareThis = np.array(compareThis, dtype=np.uint32)
-            compareThat = np.array(compareThat, dtype=np.uint32)
-            resultStart = np.array(resultStart, dtype=np.uint32)
-            values = np.array(values, dtype=np.uint32)
-            result = np.array(result, dtype=np.uint32)
-
-            # allocate memory on GPU
-            compareThis_gpu = cuda.mem_alloc(compareThis.nbytes)
-            compareThat_gpu = cuda.mem_alloc(compareThat.nbytes)
-            resultStart_gpu = cuda.mem_alloc(resultStart.nbytes)
-            values_gpu = cuda.mem_alloc(values.nbytes)
-            result_gpu = cuda.mem_alloc(result.nbytes)
-
-            # add all nbytes to GPU_MEM
-            sumBytes = compareThis.nbytes + compareThat.nbytes + resultStart.nbytes + values.nbytes + result.nbytes
-            if sumBytes > self.__GPU_MEM:
-                self.__GPU_MEM = sumBytes
-
-            # copy data to GPU
-            cuda.memcpy_htod(compareThis_gpu, compareThis)
-            cuda.memcpy_htod(compareThat_gpu, compareThat)
-            cuda.memcpy_htod(resultStart_gpu, resultStart)
-            cuda.memcpy_htod(values_gpu, values)
-            cuda.memcpy_htod(result_gpu, result)
-
-            blockDim = (32, 32, 1)
-            gridDim = (resultSize//32 + 1, len(maxLength)//32 + 1, 1)
-
-            dev_Intersection(compareThis_gpu, compareThat_gpu,
-                             resultStart_gpu, values_gpu, result_gpu,
-                             np.uint32(resultSize), np.uint32(len(maxLength)),
-                             block=blockDim, grid=gridDim)
-
-            # copy data back to CPU
-            cuda.Context.synchronize()
-            cuda.memcpy_dtoh(result, result_gpu)
-
-            # free GPU memory
-            cuda.DeviceAllocation.free(compareThis_gpu)
-            cuda.DeviceAllocation.free(compareThat_gpu)
-            cuda.DeviceAllocation.free(resultStart_gpu)
-            cuda.DeviceAllocation.free(values_gpu)
-            cuda.DeviceAllocation.free(result_gpu)
-
-            keys = list(data.keys())
-            # convert all to string and add " "
-            for i in range(len(keys)):
-                keys[i] = str(keys[i]) + " "
-            data = {}
-            index = 0
-            for i in range(len(keys)):
-                for j in range(i+1, len(keys)):
-                    newResult = list(sorted(set(result[index])))
-                    newResult = list(filter(lambda x: x > 0, newResult))
-                    if len(newResult) >= self.minSup:
-                        keyI = keys[i].split()
-                        keyJ = keys[j].split()
-                        combinedKey = " ".join(list(str(x) for x in (
-                            sorted(int(x) for x in (set(keyI) | set(keyJ))))))
-                        if combinedKey not in final:
-                            data[combinedKey] = newResult
-                            final[combinedKey] = len(newResult)
-                    index += 1
+        self.__creatingItemSets()
+        self._minSup = self.__convert(self._minSup)
+        minSup = self._minSup
+        vb_data, idx2item = self.compute_vertical_bitvector_data()
 
+        for i in range(len(vb_data)):
+            if gpuarray.sum(vb_data[i]).get() >= self._minSup:
+                basePattern[idx2item[i]] = [i]
+                final[idx2item[i]] = gpuarray.sum(vb_data[i]).get()
+
+        while len(basePattern) > 0:
+            temp = {}
+            keysList = list(basePattern.keys())
+            valuesList = list(basePattern.values())
+            for i in range(len(basePattern) - 1):
+                keyI = keysList[i].split(" ")
+                keyI = [int(x) for x in keyI]
+
+                for j in range(i + 1, len(basePattern)):
+                    keyJ = keysList[j].split(" ")
+                    keyJ = [int(x) for x in keyJ]
+                    values = set(valuesList[i])
+                    for val in valuesList[j]:
+                        values.add(val)
+                    values = list(sorted(values))
+                    totalArray = vb_data[values[0]]
+                    for k in range(1, len(values)):
+                        totalArray = totalArray.__mul__(vb_data[values[k]])
+                    support = gpuarray.sum(totalArray).get()
+                    if support >= self._minSup:
+                        combinedKey = " ".join(
+                            str(x) for x in sorted(set(keyI) | set(keyJ)))
+                        temp[combinedKey] = values
+                        final[str(combinedKey)] = support
+            basePattern = temp
 
         self.__time = time.time() - startTime
         self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
         self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
-        self.Patterns = final
+        self._finalPatterns = final
+        self.__GPU_MEM = vb_data.nbytes
+
+    def printResults(self):
+        """
+        This function is used to print the results
+        """
+        print("Total number of Coverage Patterns:", len(self.getPatterns()))
+        print("GPU MEM: ", _ap.getGPUMemory())
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = cudaAprioriTID(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = cudaAprioriTID(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("GPU MEM: ", _ap.getGPUMemory())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
-
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/cuda/cudaEclatGCT.py` & `pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelECLAT.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,17 +1,18 @@
-# cudaEclatGCT is one of the fundamental algorithm to discover frequent patterns in a transactional database. It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It  employs downward closure property to  reduce the search space effectively.
+# ParallelEclat is an algorithm to discover frequent patterns in a transactional database. This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
+#  ----------------------------------------------------
 #
-#             from PAMI.frequentPattern.cuda.cudaEclatGCT as alg
 #
-#             obj = alg.FPGrowth(iFile, minSup)
+#             import PAMI.frequentPattern.pyspark.parallelECLAT as alg
 #
-#             obj.startMine()
+#             obj = alg.parallelECLAT(iFile, minSup, numWorkers)
+#
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -25,18 +26,21 @@
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
-#
-#
+
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -44,43 +48,39 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from PAMI.frequentPattern.basic import abstract as _ab
-
-minSup = str()
-_ab._sys.setrecursionlimit(20000)
-
-import os
-import csv
-import time
-import numpy as np
-import pycuda.gpuarray as _gpuarray
-import pycuda.autoinit
-import psutil
+from pyspark import SparkConf, SparkContext
+# import abstract as _ab
+from PAMI.frequentPattern.pyspark import abstract as _ab
+from abc import ABC as _ABC, abstractmethod as _abstractmethod
+from deprecated import deprecated
 
 
-class cudaEclatGCT:
+class parallelECLAT(_ab._frequentPatterns):
     """
-    :Description: Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+    :Description: ParallelEclat is an algorithm to discover frequent patterns in a transactional database.
+     This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
 
-    :Reference:  Agrawal, R., Imieli ́nski, T., Swami, A.: Mining association rules between sets of items in large databases.
-                In: SIGMOD. pp. 207–216 (1993), https://doi.org/10.1145/170035.170072
+    :Reference:
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  numPartitions: int :
+                   The number of partitions. On each worker node, an executor process is started and this process performs processing.The processing unit of worker node is partition
+
 
     :Attributes:
 
         startTime : float
             To record the start time of the mining process
 
         endTime : float
@@ -91,40 +91,44 @@
 
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
-        Database : list
-            To store the transactions of a database in list
-
+        lno : int
+            the number of transactions
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
-            Format:
-                     >>> python3 cudaEclatGCT.py <inputFile> <outputFile> <minSup>
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 parallelECLAT.py <inputFile> <outputFile> <minSup> <numWorkers>
+
+      Example Usage:
 
-            Example:
-                     >>>  python3 cudaEclatGCT.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 parallelECLAT.py sampleDB.txt patterns.txt 10.0 3
+
+    .. note:: minSup will be considered in percentage of database transactions
 
-                    .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
     .. code-block:: python
 
-            import PAMI.frequentPattern.cuda.cuAprioriBit as alg
+            import PAMI.frequentPattern.pyspark.parallelECLAT as alg
 
-            obj = alg.cuAprioriBit(iFile, minSup)
+            obj = alg.parallelECLAT(iFile, minSup, numWorkers)
 
-            obj.startMine()
+            obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.save(oFile)
 
@@ -140,239 +144,276 @@
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
     **Credits:**
-    -------------
-             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+    ----------------------------------------------------
+             The complete program was written by Yudai Masu under the supervision of Professor Rage Uday Kiran.
+
     """
 
-    __time = 0
-    __memRSS = 0
-    __memUSS = 0
-    __GPU_MEM = 0
-    _minSup = 0
+    _minSup = float()
+    _numPartitions = int()
+    _startTime = float()
+    _endTime = float()
     _finalPatterns = {}
+    _iFile = " "
+    _oFile = " "
+    _sep = " "
+    _memoryUSS = float()
+    _memoryRSS = float()
+    _lno = int()
 
-    def __init__(self, filePath, minSup, sep):
-        self._iFile = filePath
-        self._sep = sep
-        self._minSup = minSup
-        self.__time = 0
-        self.__memRSS = 0
-        self.__memUSS = 0
-
-    """def read_data(self, data_path, sep):
-
-
-        data = []
-        if not os.path.isfile(data_path):
-            raise ValueError('Invalid data path.' + data_path)
-        with open(data_path, 'r') as f:
-            file = csv.reader(f, delimiter=sep, quotechar='\r')
-            lineNo = 1
-            for row in file:
-                data.append([str(item) for item in row if item != ''])
-                lineNo += 1
-        return data, lineNo"""
-
-    def __creatingItemSets(self):
-        """
-        Storing the complete transactions of the database/input file in a database variable
-        """
-        self.__Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            if self._iFile.empty:
-                print("its empty..")
-            i = self._iFile.columns.values.tolist()
-            if 'Transactions' in i:
-                self.__Database = self._iFile['Transactions'].tolist()
-
-            # print(self.Database)
-        if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
-                for line in data:
-                    line.strip()
-                    line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    self.__Database.append(temp)
-            else:
-                try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line = line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self.__Database.append(temp)
-                except IOError:
-                    print("File Not Found")
-                    quit()
-
-    def __convert(self, value):
-        """
-        To convert the type of user specified minSup value
-        :param value: user specified minSup value
-        :return: converted type
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self.__Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self.__Database) * value)
-            else:
-                value = int(value)
-        return value
-
-    def compute_vertical_bitvector_data(self):
-        """
-        Converting  database into bit vector
-        """
-        # ---build item to idx mapping---#
-        idx = 0
-        item2idx = {}
-        for transaction in self.__Database:
-            for item in transaction:
-                if not item in item2idx:
-                    item2idx[item] = idx
-                    idx += 1
-        idx2item = {idx: str(int(item)) for item, idx in item2idx.items()}
-        # ---build vertical data---#
-        vb_data = np.zeros((len(item2idx), len(self.__Database)), dtype=np.uint16)
-        for trans_id, transaction in enumerate(self.__Database):
-            for item in transaction:
-                vb_data[item2idx[item], trans_id] = 1
-        vb_data = _gpuarray.to_gpu(vb_data.astype(np.uint16))
-        return vb_data, idx2item
+    def __init__(self, iFile, minSup, numWorkers, sep="\t"):
+        super().__init__(iFile, minSup, int(numWorkers), sep)
 
-    def getRuntime(self):
+    def getMemoryUSS(self):
         """
-        Calculating the total amount of time taken by the mining process
-        :return: returning total amount of runtime taken by the mining process
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
-        return self.__time
+
+        return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
-        return self.__memRSS
 
-    def getMemoryUSS(self):
+        return self._memoryRSS
+
+    def getRuntime(self):
         """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-        :return: returning USS memory consumed by the mining process
+        Calculating the total amount of runtime taken by the mining process
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-        return self.__memUSS
 
-    def getGPUMemory(self):
+        return self._endTime - self._startTime
+
+    def getPatternsAsDataFrame(self):
         """
-        To calculate the total memory consumed by GPU
-        :return: return GPU memory
-        :rtype: int
+        Storing final frequent patterns in a dataframe
+        :return: returning frequent patterns in a dataframe
+        :rtype: pd.DataFrame
         """
 
-        return self.__GPU_MEM
+        dataFrame = {}
+        data = []
+        for a, b in self._finalPatterns.items():
+            data.append([a, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataFrame
+
+    def save(self, outFile):
+        """
+        Complete set of frequent patterns will be loaded in to an output file
+        :param outFile: name of the output file
+        :type outFile: csvfile
+        """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            s1 = x + ":" + str(y)
+            writer.write("%s \n" % s1)
+            
+    def printResults(self):
+        """
+        This method prints all the stats
+        """
+        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def get_numberOfPatterns(self):
+    def _genPatterns(self, suffix, pattern, data):
+        """
+        This function is used to generate patterns
+        :param suffix: the suffix of the generated ptterns
+
+        :type suffix: str
+
+        :param pattern: the pattern of the generated ptterns
+
+        :type pattern: str
+
+        :param data: the data of the generated ptterns after completion of the mining process
 
-        return len(self._finalPatterns)
+        :type data: str
+        """
+        freqPatterns = {}
+        index = data.index(suffix)
+        for i in range(index + 1, len(data)):
+            tid = pattern[1].intersection(data[i][1])
+            if len(tid) >= self._minSup:
+                freqPattern = pattern[0] + ' ' + data[i][0]
+                freqPatterns[freqPattern] = len(tid)
+                freqPatterns.update(self._genPatterns(data[i], (freqPattern, tid), data))
+        return freqPatterns
 
-    def eclat(self, basePattern, final, vb_data, idx2item, item2idx):
+    def printResults(self):
+        """
+        This function is used to print the results
         """
-        param basePattern:
-        type basePattern:
-        param final:
-        type final:
-        param vb_data:
-        type vb_data:
-        param idx2item:
-        type idx2item:
-        param item2idx:
-        type item2idx:
-        """
-        newBasePattern = []
-        for i in range(0, len(basePattern)):
-            item1 = basePattern[i]
-            i1_list = item1.split()
-            for j in range(i + 1, len(basePattern)):
-                item2 = basePattern[j]
-                i2_list = item2.split()
-                if i1_list[:-1] == i2_list[:-1]:
-                    unionOfKey = list(set(i1_list) | set(i2_list))
-                    unionOfKey.sort()
-                    valueList = []
-                    for key in unionOfKey:
-                        valueList.append(item2idx[key])
-                    total = vb_data[valueList[0]]
-                    for k in range(1, len(valueList)):
-                        total = total.__mul__(vb_data[valueList[k]])
-                    support = _gpuarray.sum(total).get()
-                    if support >= self._minSup:
-                        newBasePattern.append(" ".join(unionOfKey))
-                        final[" ".join(unionOfKey)] = support
+        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
-        if len(newBasePattern) > 0:
-            self.eclat(newBasePattern, final, vb_data, idx2item, item2idx)
+    def _convert(self, value):
+        """
+        To convert the user specified minSup value
+        :param value: user specified minSup value
+        :type value: int or float or str
+        :return: converted type
+        """
+        print(value, type(value))
+        if type(value) is int:
+            value = int(value)
+        elif type(value) is float:
+            value = (self._lno * value)
+        elif type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (self._lno * value)
+            else:
+                value = int(value)
+        else:
+            print("None")
+        print(type(value), value)
+        return value
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        startTime = time.time()
-        basePattern = []
-        final = {}
-
-        self.__creatingItemSets()
-        self._minSup = self.__convert(self._minSup)
-        minSup = self._minSup
-        vb_data, idx2item = self.compute_vertical_bitvector_data()
-
-        for i in range(len(vb_data)):
-            if _gpuarray.sum(vb_data[i]).get() >= self._minSup:
-                basePattern.append(idx2item[i])
-                final[idx2item[i]] = _gpuarray.sum(vb_data[i]).get()
-
-        # reverse idx2item
-        item2idx = {idx2item[i]: i for i in idx2item}
-        self.eclat(basePattern, final, vb_data, idx2item, item2idx)
-        self.__time = time.time() - startTime
-        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
-        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
-        self._finalPatterns = final
-        self.__GPU_MEM = vb_data.nbytes
+
+        self._startTime = _ab._time.time()
+        conf = SparkConf().setAppName("Parallel ECLAT").setMaster("local[*]")
+        sc = SparkContext(conf=conf)
+
+        data = sc.textFile(self._iFile, self._numPartitions) \
+            .map(lambda line: [int(y) for y in line.rstrip().split(self._sep)]).persist()
+        self._lno = data.count()
+        self._minSup = self._convert(self._minSup)
+
+        frequentItems = None
+        frequentItems = data.zipWithIndex() \
+            .flatMap(lambda x: [(str(item), x[1]) for item in x[0]]) \
+            .groupByKey() \
+            .filter(lambda x: len(x[1]) >= self._minSup) \
+            .sortBy(lambda x: len(x[1])) \
+            .mapValues(set) \
+            .persist()
+        data.unpersist()
+        # elif 'temporal' in self._iFile:
+        #     frequentItems = data.flatMap(lambda trans: [(str(item), trans[0]) for item in trans[1:]]) \
+        #         .groupByKey() \
+        #         .filter(lambda x: len(x[1]) >= self._minSup) \
+        #         .mapValues(set) \
+        #         .persist()
+        #     data.unpersist()
+        # else:
+        #     pass
+        #     # print("may be not able to process the input file")
+
+        freqItems = dict(frequentItems.collect())
+        # print(len(freqItems))
+        self._finalPatterns = {k: len(v) for k, v in freqItems.items()}
+
+        freqPatterns = list(frequentItems.map(lambda x: self._genPatterns(x, x, list(freqItems.items())))
+                            .filter(lambda x: len(x) != 0).collect())
+        for value in freqPatterns:
+            self._finalPatterns.update(value)
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using Parallel ECLAT algorithm")
+        sc.stop()
+
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+
+        self._startTime = _ab._time.time()
+        conf = SparkConf().setAppName("Parallel ECLAT").setMaster("local[*]")
+        sc = SparkContext(conf=conf)
+
+        data = sc.textFile(self._iFile, self._numPartitions) \
+            .map(lambda line: [int(y) for y in line.rstrip().split(self._sep)]).persist()
+        self._lno = data.count()
+        self._minSup = self._convert(self._minSup)
+
+        frequentItems = None
+        frequentItems = data.zipWithIndex() \
+            .flatMap(lambda x: [(str(item), x[1]) for item in x[0]]) \
+            .groupByKey() \
+            .filter(lambda x: len(x[1]) >= self._minSup) \
+            .sortBy(lambda x: len(x[1])) \
+            .mapValues(set) \
+            .persist()
+        data.unpersist()
+        # elif 'temporal' in self._iFile:
+        #     frequentItems = data.flatMap(lambda trans: [(str(item), trans[0]) for item in trans[1:]]) \
+        #         .groupByKey() \
+        #         .filter(lambda x: len(x[1]) >= self._minSup) \
+        #         .mapValues(set) \
+        #         .persist()
+        #     data.unpersist()
+        # else:
+        #     pass
+        #     # print("may be not able to process the input file")
+
+        freqItems = dict(frequentItems.collect())
+        # print(len(freqItems))
+        self._finalPatterns = {k: len(v) for k, v in freqItems.items()}
+
+        freqPatterns = list(frequentItems.map(lambda x: self._genPatterns(x, x, list(freqItems.items())))
+                            .filter(lambda x: len(x) != 0).collect())
+        for value in freqPatterns:
+            self._finalPatterns.update(value)
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using Parallel ECLAT algorithm")
+        sc.stop()
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = parallelECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = cudaEclatGCT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = cudaEclatGCT(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = parallelECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ap._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("GPU MEM: ", _ap.getGPUMemory())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _ap.mine()
+        _finalPatterns = _ap.getPatterns()
+        print("Total number of Frequent Patterns:", len(_finalPatterns))
+        _ap.save(_ab._sys.argv[2])
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/maximal/MaxFPGrowth.py` & `pami-2024.4.9.1/PAMI/frequentPattern/maximal/MaxFPGrowth.py`

 * *Files 3% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 # **Importing this algorithm into a python program**
 # ---------------------------------------------------------
 #
 #             from PAMI.frequentPattern.maximal import MaxFPGrowth as alg
 #
 #             obj = alg.MaxFPGrowth("../basic/sampleTDB.txt", "2")
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save("patterns")
 #
@@ -24,22 +24,22 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
-
+#
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -49,14 +49,15 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
 from PAMI.frequentPattern.maximal import abstract as _ab
+from deprecated import deprecated
 
 
 _minSup = str()
 global maximalTree
 
 
 class _Node(object):
@@ -135,17 +136,23 @@
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
         #self.maximalTree = _MPTree()
 
     def addTransaction(self, transaction):
         """
+
         Adding transactions into tree
+
         :param transaction: represents the transaction in a database
+
+        :type transaction: list
+
         :return: tree
+
         """
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
                 newNode.counter = 1
                 currentNode.addChild(newNode)
@@ -156,18 +163,27 @@
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
                 currentNode.counter += 1
 
     def addConditionalTransaction(self, transaction, count):
         """
+
         Loading the database into a tree
+
         :param transaction: conditional transaction of a node
+
+        :type transaction: list
+
         :param count: the support of conditional transaction
+
+        :type count: int
+
         :return: conditional tree
+
         """
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
                 newNode.counter = count
                 currentNode.addChild(newNode)
@@ -180,14 +196,15 @@
                 currentNode = currentNode.children[transaction[i]]
                 currentNode.counter += count
 
     def getConditionalPatterns(self, alpha):
         """
         Generates all the conditional patterns of respective node
         :param alpha: it represents the Node in tree
+        :type alpha: int
         :return: conditional patterns of a node
         """
         finalPatterns = []
         finalSets = []
         for i in self.summaries[alpha]:
             set1 = i.counter
             set2 = []
@@ -201,15 +218,17 @@
         finalPatterns, finalSets, info = self.conditionalTransactions(finalPatterns, finalSets)
         return finalPatterns, finalSets, info
 
     def conditionalTransactions(self, condPatterns, condFreq):
         """
         sorting and removing the items from conditional transactions which don't satisfy minSup
         :param condPatterns: conditional patterns if a node
+        :type condPatterns: list
         :param condFreq: frequency at leaf node of conditional transaction
+        :type condFreq: int
         :return: conditional patterns and their frequency respectively
         """
         global _minSup
         pat = []
         tids = []
         data1 = {}
         for i in range(len(condPatterns)):
@@ -230,25 +249,30 @@
             count += 1
         return pat, tids, updatedDict
 
     def removeNode(self, nodeValue):
         """
         To remove the node from the original tree
         :param nodeValue: leaf node of tree
+        :type nodeValue: int
         :return: tree after deleting node
         """
         for i in self.summaries[nodeValue]:
             del i.parent.children[nodeValue]
             i = None
 
     def generatePatterns(self, prefix, patterns, maximalTree):
         """
         Generates the patterns
         :param prefix: forms the combination of items
+        :type prefix: str
+        :param patterns: the patterns we want to generate for this node
+        :type patterns: list
         :return: the maximal frequent patterns
+        :rtype: list
         """
         for i in sorted(self.summaries, key=lambda x: (self.info.get(x), -x)):
             pattern = prefix[:]
             pattern.append(i)
             condPatterns, tids, info = self.getConditionalPatterns(i)
             conditional_tree = _Tree()
             conditional_tree.info = info.copy()
@@ -287,14 +311,15 @@
         self.item = item
         self.children = children
 
     def addChild(self, node):
         """
         To add the children details to a parent node
         :param node: children node
+        :type node: _MNode
         :return: adding children details to parent node
         """
         self.children[node.item] = node
         node.parent = self
 
 
 class _MPTree(object):
@@ -323,14 +348,15 @@
         self.root = _MNode(None, {})
         self.summaries = {}
 
     def addTransaction(self, transaction):
         """
         To construct the maximal frequent pattern into maximal tree
         :param transaction: the maximal frequent patterns extracted till now
+        :type transaction: list
         :return: the maximal tree
         """
         currentNode = self.root
         transaction.sort()
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _MNode(transaction[i], {})
@@ -343,14 +369,15 @@
             else:
                 currentNode = currentNode.children[transaction[i]]
 
     def checkerSub(self, items):
         """
         To check the subset of pattern present in tree
         :param items: the sub frequent pattern
+        :type items: list
         :return: checks if subset present in the tree
         """
         items.sort(reverse=True)
         item = items[0]
         if item not in self.summaries:
             return 1
         else:
@@ -381,14 +408,17 @@
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int or float or str :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+    :param  maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
 
     :Attributes:
 
@@ -421,33 +451,38 @@
         finalPatterns : dict
             it represents to store the patterns
 
 
     **Methods to execute code on terminal**
     ---------------------------------------------------------
 
-            Format:
-                      >>> python3 MaxFPGrowth.py <inputFile> <outputFile> <minSup>
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 MaxFPGrowth.py <inputFile> <outputFile> <minSup>
+
+      Example Usage:
 
-            Example:
-                      >>> python3 MaxFPGrowth.py sampleDB.txt patterns.txt 0.3
+      (.venv) $ python3 MaxFPGrowth.py sampleDB.txt patterns.txt 0.3
+
+    .. note:: minSup will be considered in percentage of database transactions
 
-            .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ---------------------------------------------------------
 
     .. code-block:: python
 
             from PAMI.frequentPattern.maximal import MaxFPGrowth as alg
 
             obj = alg.MaxFPGrowth("../basic/sampleTDB.txt", "2")
 
-            obj.startMine()
+            obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.savePatterns("patterns")
 
@@ -540,14 +575,15 @@
         self._rank = dict([(index, item) for (item, index) in enumerate(genList)])
         return _mapSupport, genList
 
     def _updateTransactions(self, oneLength):
         """
         To sort the transactions in their support descending order and allocating ranks respectively
         :param oneLength: 1-length frequent items in dictionary
+        :type oneLength: dict
         :return: returning the sorted list
         :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
                     rank = {'a':0, 'b':1, 'c':2, 'd':3}
         """
         list1 = []
         for tr in self._Database:
             list2 = []
@@ -558,30 +594,33 @@
                 list2.sort()
                 list1.append(list2)
         return list1
 
 
     def _buildTree(self, data, info):
         """
-        creating the root node as null in fp-tree and and adding all transactions into tree.
+        creating the root node as null in fp-tree and adding all transactions into tree.
         :param data: updated transactions
+        :type data: dict
         :param info: rank of items in transactions
+        :type info: dict
         :return: fp-tree
         """
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
             rootNode.addTransaction(data[i])
         return rootNode
 
 
     def _convert(self, value):
         """
         To convert the type of user specified minSup value
         :param value: user specified minSup value
+        :type value: int or float or str
         :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
@@ -592,21 +631,23 @@
                 value = int(value)
         return value
 
     def _convertItems(self, itemSet):
         """
         To convert the item ranks into their original item names
         :param itemSet: itemSet or a pattern
+        :type itemSet: list
         :return: original pattern
         """
         t1 = []
         for i in itemSet:
             t1.append(self._rankdup[i])
         return t1
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Mining process will start from this function
         """
 
         global _minSup
         self._startTime = _ab._time.time()
@@ -637,14 +678,52 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Maximal Frequent patterns were generated successfully using MaxFp-Growth algorithm ")
 
+    def mine(self):
+        """
+        Mining process will start from this function
+        """
+
+        global _minSup
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        _minSup = self._minSup
+        generatedItems, pfList = self._frequentOneItem()
+        updatedTransactions = self._updateTransactions(generatedItems)
+        for x, y in self._rank.items():
+            self._rankdup[y] = x
+        info = {self._rank[k]: v for k, v in generatedItems.items()}
+        patterns = {}
+        self._finalPatterns = {}
+        self._maximalTree = _MPTree()
+        Tree = self._buildTree(updatedTransactions, info)
+        Tree.generatePatterns([], patterns, self._maximalTree)
+        for x, y in patterns.items():
+            pattern = str()
+            x = self._convertItems(x)
+            for i in x:
+                pattern = pattern + i + "\t"
+            self._finalPatterns[pattern] = y
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Maximal Frequent patterns were generated successfully using MaxFp-Growth algorithm ")
+
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -682,15 +761,15 @@
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to a output file
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csvfile
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
@@ -716,14 +795,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = MaxFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = MaxFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         _ap.save(_ab._sys.argv[2])
         print("Total number of Maximal Frequent Patterns:", len(_ap.getPatterns()))
         print("Total Memory in USS:",  _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/maximal/__init__.py` & `pami-2024.4.9.1/PAMI/frequentPattern/maximal/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/maximal/abstract.py` & `pami-2024.4.9.1/PAMI/frequentPattern/maximal/abstract.py`

 * *Files 1% similar despite different names*

```diff
@@ -171,11 +171,11 @@
         """
 
         pass
 
     @_abstractmethod
     def printResults(self):
         """
-        To print the statistics.
+        To print the stats.
         """
 
         pass
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/pyspark/abstract.py` & `pami-2024.4.9.1/PAMI/frequentPattern/pyspark/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/pyspark/parallelApriori.py` & `pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelApriori.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 #  **Importing this algorithm into a python program**
 #  ---------------------------------------------------
 #
 #             import PAMI.frequentPattern.pyspark.parallelApriori as alg
 #
 #             obj = alg.parallelApriori(iFile, minSup, numWorkers)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -25,21 +25,22 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -48,14 +49,15 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 from PAMI.frequentPattern.pyspark import abstract as _ab
+from deprecated import deprecated
 
 
 class parallelApriori(_ab._frequentPatterns):
     """
 
     :Description: Parallel Apriori is an algorithm to discover frequent patterns in a transactional database. This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
 
@@ -95,34 +97,39 @@
 
         lno : int
                 the number of transactions
 
     
 
     **Methods to execute code on terminal**
-    -----------------------------------------
-    
-            Format:
-                      >>>  python3 parallelApriori.py <inputFile> <outputFile> <minSup> <numWorkers>
-    
-            Example:
-                      >>>  python3 parallelApriori.py sampleDB.txt patterns.txt 10.0 3
-    
-            .. note:: minSup will be considered in percentage of database transactions
+    -------------------------------------------
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 parallelApriori.py <inputFile> <outputFile> <minSup> <numWorkers>
+
+      Example Usage:
+
+      (.venv) $ python3 parallelApriori.py sampleDB.txt patterns.txt 10.0 3
+
+    .. note:: minSup will be considered in percentage of database transactions
+
     
     
     **Importing this algorithm into a python program**
     ----------------------------------------------------------------------------------
     .. code-block:: python
     
                 import PAMI.frequentPattern.pyspark.parallelApriori as alg
     
                 obj = alg.parallelApriori(iFile, minSup, numWorkers)
     
-                obj.startMine()
+                obj.mine()
     
                 frequentPatterns = obj.getPatterns()
     
                 print("Total number of Frequent Patterns:", len(frequentPatterns))
     
                 obj.save(oFile)
     
@@ -254,15 +261,15 @@
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
     
     def printResults(self):
         """
-        This method prints all the statistics
+        This method prints all the stats
         """
         print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:", self.getRuntime())
 
     @staticmethod
@@ -297,14 +304,15 @@
         candidates = set([tuple(set(item[0]).union(set(item[1]))) for item in [x for x in candidates]])
         candidates = list({item for item in candidates if len(item) == length})
         return candidates
 
     def _genFrequentItems(self, database):
         """
         Get frequent items which length is 1
+        :param database: database to get frequent items
         :return: frequent items which length is 1
         :rtype: dict
         """
         frequentItems = dict(database.flatMap(lambda x: [(item, 1) for item in x])
                              .reduceByKey(lambda x, y: x + y)
                              .filter(lambda c: c[1] >= self._minSup)
                              .collect())
@@ -312,15 +320,15 @@
 
     def _getAllFrequentPatterns(self, database, frequentItems):
         """
         Get all frequent patterns and save them to self.oFile
         :param database: database
         :type : RDD
         :param frequentItems: dict
-        :return:
+        :type frequentItems: dict
         """
 
         # Get candidate patterns that length is 2
         candidates = list(_ab._c(frequentItems.keys(), 2))
         length = 3
         while len(candidates) != 0:
             # if each itemset of candidates is in each transaction, then create (itemset,1)
@@ -333,32 +341,62 @@
             candidates = self._genCandidateItemsets(list(frequentPatterns.keys()), length)
             length += 1
 
     def _convert(self, value):
         """
         To convert the user specified minSup value
         :param value: user specified minSup value
+        :type value: int or float or str
         :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._lno * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (self._lno * value)
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
-        :return:
+        """
+        self._startTime = _ab._time.time()
+
+        # setting SparkConf and SparkContext to process in parallel
+        conf = _ab._SparkConf().setAppName("parallelApriori").setMaster("local[*]")
+        sc = _ab._SparkContext(conf=conf)
+        # sc.addFile("file:///home/hadoopuser/Spark_code/abstract.py")
+
+        # read database from iFile
+        database = sc.textFile(self._iFile, self._numPartitions).map(
+            lambda x: {int(y) for y in x.rstrip().split(self._sep)})
+        self._lno = database.count()
+        # Calculating minSup as a percentage
+        self._minSup = self._convert(self._minSup)
+
+        oneFrequentItems = self._genFrequentItems(database)
+        self._finalPatterns = oneFrequentItems
+        self._getAllFrequentPatterns(database, oneFrequentItems)
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using Parallel Apriori algorithm")
+        sc.stop()
+
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
 
         # setting SparkConf and SparkContext to process in parallel
         conf = _ab._SparkConf().setAppName("parallelApriori").setMaster("local[*]")
         sc = _ab._SparkContext(conf=conf)
         # sc.addFile("file:///home/hadoopuser/Spark_code/abstract.py")
@@ -386,14 +424,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = parallelApriori(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = parallelApriori(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
+        _ap.mine()
         _finalPatterns = _ap.getPatterns()
         print("Total number of Frequent Patterns:", len(_finalPatterns))
         _ap.savePatterns(_ab._sys.argv[2])
         _memUSS = _ap.getMemoryUSS()
         print("Total Memory in USS:", _memUSS)
         _memRSS = _ap.getMemoryRSS()
         print("Total Memory in RSS", _memRSS)
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/pyspark/parallelECLAT.py` & `pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-# ParallelEclat is an algorithm to discover frequent patterns in a transactional database. This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
+# VBFTMine is one of the fundamental algorithm to discover fault-tolerant frequent patterns in an uncertain transactional database based on bitset representation.
 #
 # **Importing this algorithm into a python program**
-#  ----------------------------------------------------
+# --------------------------------------------------------
 #
 #
-#             import PAMI.frequentPattern.pyspark.parallelECLAT as alg
+#             import PAMI.uncertainFaultTolerantFrequentPattern.basic.VBFTMine as alg
 #
-#             obj = alg.parallelECLAT(iFile, minSup, numWorkers)
+#             obj = alg.VBFTMine(iFile, minSup, itemSup, minLength, faultTolerance)
 #
 #             obj.startMine()
 #
-#             frequentPatterns = obj.getPatterns()
+#             faultTolerantFrequentPattern = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of Fault Tolerant Frequent Patterns:", len(faultTolerantFrequentPattern))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -25,18 +25,15 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
-
-
-
-
+#
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -47,308 +44,429 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from pyspark import SparkConf, SparkContext
-# import abstract as _ab
-from PAMI.frequentPattern.pyspark import abstract as _ab
-from abc import ABC as _ABC, abstractmethod as _abstractmethod
+import pandas as pd
+from deprecated import deprecated
 
+import numpy as _np
+from PAMI.faultTolerantFrequentPattern.basic import abstract as _ab
 
-class parallelECLAT(_ab._frequentPatterns):
+class VBFTMine(_ab._faultTolerantFrequentPatterns):
     """
-    :Description: ParallelEclat is an algorithm to discover frequent patterns in a transactional database.
-     This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
-
-    :Reference:
+    
+    :Description:  VBFTMine is one of the fundamental algorithm to discover fault tolerant frequent patterns in an uncertain transactional database based on
+                   bitset representation.
+                   This program employs apriori property (or downward closure property) to  reduce the search space effectively.
+
+    :Reference:   Koh, JL., Yo, PW. (2005). An Efficient Approach for Mining Fault-Tolerant Frequent Patterns Based on Bit Vector Representations.
+                  In: Zhou, L., Ooi, B.C., Meng, X. (eds) Database Systems for Advanced Applications. DASFAA 2005. Lecture Notes in Computer Science,
+                  vol 3453. Springer, Berlin, Heidelberg. https://doi.org/10.1007/11408079_51
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
+                   Name of the Input file to mine complete set of uncertain Fault Tolerant FrequentFrequent Patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: int :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+                   Name of the output file to store complete set of uncertain Fault Tolerant FrequentFrequent Patterns
+    :param  minSup: float or int or str :
+                    The user can specify minSup either in count or proportion of database size.
+                    If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+                    Otherwise, it will be treated as float.
+                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+    :param  itemSup: int or float :
+                    Frequency of an item
+    :param minLength: int
+                    minimum length of a pattern
+    :param faultTolerance: int :
+                    The ability of a pattern mining algorithm to handle errors or inconsistencies in the data without completely failing or producing incorrect results.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-    :param  numPartitions: int :
-                   The number of partitions. On each worker node, an executor process is started and this process performs processing.The processing unit of worker node is partition
-
 
     :Attributes:
 
         startTime : float
-            To record the start time of the mining process
+          To record the start time of the mining process
 
         endTime : float
-            To record the completion time of the mining process
+          To record the completion time of the mining process
 
         finalPatterns : dict
-            Storing the complete set of patterns in a dictionary variable
+          Storing the complete set of patterns in a dictionary variable
 
         memoryUSS : float
-            To store the total amount of USS memory consumed by the program
+          To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+          To store the total amount of RSS memory consumed by the program
 
-        lno : int
-            the number of transactions
+        Database : list
+          To store the transactions of a database in list
 
 
-    **Methods to execute code on terminal**
-    ----------------------------------------------------
+    **Executing the code on terminal**:
+    ------------------------------------
+    .. code-block:: console
 
-            Format:
-                      >>> python3 parallelECLAT.py <inputFile> <outputFile> <minSup> <numWorkers>
 
-            Example:
-                      >>> python3 parallelECLAT.py sampleDB.txt patterns.txt 10.0 3
+       Format:
 
-            .. note:: minSup will be considered in percentage of database transactions
+       (.venv) $ python3 VBFTMine.py <inputFile> <outputFile> <minSup> <itemSup> <minLength> <faultTolerance>
 
+       Examples usage:
 
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
-    .. code-block:: python
-
-            import PAMI.frequentPattern.pyspark.parallelECLAT as alg
+       (.venv) $ python3 VBFTMine.py sampleDB.txt patterns.txt 10.0 3.0 3 1
 
-            obj = alg.parallelECLAT(iFile, minSup, numWorkers)
 
-            obj.startMine()
+               .. note:: minSup will be considered in times of minSup and count of database transactions
 
-            frequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+    **Sample run of the importing code**:
+    --------------------------------------------
+    .. code-block:: python
+    
+            import PAMI.faultTolerantFrequentPattern.basic.VBFTMine as alg
 
-            obj.save(oFile)
+            obj = alg.VBFTMine(iFile, minSup, itemSup, minLength, faultTolerance)
 
-            Df = obj.getPatternInDataFrame()
+            obj.startMine()
 
-            memUSS = obj.getMemoryUSS()
+            faultTolerantFrequentPattern = obj.getPatterns()
 
-            print("Total Memory in USS:", memUSS)
+            print("Total number of Fault Tolerant Frequent Patterns:", len(faultTolerantFrequentPattern))
 
-            memRSS = obj.getMemoryRSS()
+            obj.save(oFile)
 
-            print("Total Memory in RSS", memRSS)
+            Df = obj.getPatternInDataFrame()
 
-            run = obj.getRuntime()
+            print("Total Memory in USS:", obj.getMemoryUSS())
 
-            print("Total ExecutionTime in seconds:", run)
+            print("Total Memory in RSS", obj.getMemoryRSS())
 
+            print("Total ExecutionTime in seconds:", obj.getRuntime())
 
-    **Credits:**
-    ----------------------------------------------------
-             The complete program was written by Yudai Masu under the supervision of Professor Rage Uday Kiran.
+    **Credits**:
+    ------------
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
     _minSup = float()
-    _numPartitions = int()
+    _itemSup = float()
+    _minLength = int()
+    _faultTolerance = int()
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
+    _plist = []
     _memoryUSS = float()
     _memoryRSS = float()
-    _lno = int()
+    _Database = []
+    _mapSupport = {}
+
+    def _creatingItemSets(self):
+        """
+        Storing the complete transactions of the database/input file in a database variable
+        """
+        self._Database = []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
+            temp = []
+            if self._iFile.empty:
+                print("its empty..")
+            i = self._iFile.columns.values.tolist()
+            if 'Transactions' in i:
+                temp = self._iFile['Transactions'].tolist()
+
+            for k in temp:
+                self._Database.append(set(k))
+        if isinstance(self._iFile, str):
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._Database.append(set(temp))
+            else:
+                try:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            for i in temp:
+                                if i not in self._plist:
+                                    self._plist.append(i)
+                            self._Database.append(set(temp))
+                except IOError:
+                    print("File Not Found")
+                    quit()
+
+    def _convert(self, value):
+        """
+        To convert the user specified minSup value
+
+        :param value: user specified minSup value
+        :return: converted type
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
+
+    def _Count(self, tids):
+        count = 0
+        for i in tids:
+            if i == 1:
+                count += 1
+        return count
+
+    def _save(self, prefix, suffix, tidsetx):
+        if (prefix == None):
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        prefix = list(set(prefix))
+        prefix.sort()
+        val = self._Count(tidsetx)
+        if len(prefix) > self._faultTolerance:
+            self._finalPatterns[tuple(prefix)] = val
+
+    def _processEquivalenceClass(self, prefix, itemsets, tidsets):
+        if (len(itemsets) == 1):
+            i = itemsets[0]
+            tidi = tidsets[0]
+            self._save(prefix, [i], tidi)
+            return
+        for i in range(len(itemsets)):
+            itemx = itemsets[i]
+            if (itemx == None):
+                continue
+            tidsetx = tidsets[i]
+            classItemsets = []
+            classtidsets = []
+            itemsetx = [itemx]
+            for j in range(i + 1, len(itemsets)):
+                itemj = itemsets[j]
+                tidsetj = tidsets[j]
+                y = list(_np.array(tidsetx) & _np.array(tidsetj))
+                total = self._Count(y)
+                if total >= self._minSup:
+                    classItemsets.append(itemj)
+                    classtidsets.append(y)
+            if (len(classItemsets) > 0):
+                newprefix = list(set(itemsetx)) + prefix
+                self._processEquivalenceClass(newprefix, classItemsets, classtidsets)
+            self._save(prefix, list(set(itemsetx)), tidsetx)
+
+    def _oneLengthFrequentItems(self):
+        """
+        To calculate the one Length items
+        """
+        Vector = {}
+        items = []
+        for i in self._Database:
+            for j in self._plist:
+                count = 0
+                if j in i:
+                    count = 1
+                if j in Vector:
+                    Vector[j].append(count)
+                else:
+                    Vector[j] = [count]
+        for x, y in Vector.items():
+            v = self._Count(y)
+            if v >= self._itemSup:
+                items.append(x)
+        return Vector, items
 
-    def __init__(self, iFile, minSup, numWorkers, sep="\t"):
-        super().__init__(iFile, minSup, int(numWorkers), sep)
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._itemSup = self._convert(self._itemSup)
+        self._minLength = int(self._minLength)
+        self._faultTolerance = int(self._faultTolerance)
+        Vector, plist = self._oneLengthFrequentItems()
+        for i in range(len(plist)):
+            itemx = plist[i]
+            tidsetx = Vector[itemx]
+            itemsetx = [itemx]
+            itemsets = []
+            tidsets = []
+            for j in range(i + 1, len(plist)):
+                itemj = plist[j]
+                tidsetj = Vector[itemj]
+                y1 = list(_np.array(tidsetx) | _np.array(tidsetj))
+                total = self._Count(y1)
+                if total >= self._minSup:
+                    itemsets.append(itemj)
+                    tidsets.append(y1)
+            if (len(itemsets) > 0):
+                self._processEquivalenceClass(itemsetx, itemsets, tidsets)
+            self._save(None, itemsetx, tidsetx)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Fault-Tolerant Frequent patterns were generated successfully using VBFTMine algorithm ")
+
+    def Mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._itemSup = self._convert(self._itemSup)
+        self._minLength = int(self._minLength)
+        self._faultTolerance = int(self._faultTolerance)
+        Vector, plist = self._oneLengthFrequentItems()
+        for i in range(len(plist)):
+            itemx = plist[i]
+            tidsetx = Vector[itemx]
+            itemsetx = [itemx]
+            itemsets = []
+            tidsets = []
+            for j in range(i + 1, len(plist)):
+                itemj = plist[j]
+                tidsetj = Vector[itemj]
+                y1 = list(_np.array(tidsetx) | _np.array(tidsetj))
+                total = self._Count(y1)
+                if total >= self._minSup:
+                    itemsets.append(itemj)
+                    tidsets.append(y1)
+            if (len(itemsets) > 0):
+                self._processEquivalenceClass(itemsetx, itemsets, tidsets)
+            self._save(None, itemsetx, tidsetx)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Fault-Tolerant Frequent patterns were generated successfully using VBFTMine algorithm ")
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b])
+            s = str()
+            for i in a:
+                s = s + i + ' '
+            data.append([s, b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
-        :type outFile: csvfile
+        :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y)
+            s = str()
+            for i in x:
+                s = s + i + '\t'
+            s1 = s.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
-            
-    def printResults(self):
-        """
-        This method prints all the statistics
-        """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def _genPatterns(self, suffix, pattern, data):
-        """
-        This function is used to generate patterns
-        param suffix:
-        return:
-        param pattern:
-        return:
-        param data:
-        type:
-        """
-        freqPatterns = {}
-        index = data.index(suffix)
-        for i in range(index + 1, len(data)):
-            tid = pattern[1].intersection(data[i][1])
-            if len(tid) >= self._minSup:
-                freqPattern = pattern[0] + ' ' + data[i][0]
-                freqPatterns[freqPattern] = len(tid)
-                freqPatterns.update(self._genPatterns(data[i], (freqPattern, tid), data))
-        return freqPatterns
-
     def printResults(self):
         """
         This function is used to print the results
         """
         print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:", self.getRuntime())
 
-    def _convert(self, value):
-        """
-        To convert the user specified minSup value
-        :param value: user specified minSup value
-        :return: converted type
-        """
-        print(value, type(value))
-        if type(value) is int:
-            value = int(value)
-        elif type(value) is float:
-            value = (self._lno * value)
-        elif type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._lno * value)
-            else:
-                value = int(value)
-        else:
-            print("None")
-        print(type(value), value)
-        return value
-
-    def startMine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-
-        self._startTime = _ab._time.time()
-        conf = SparkConf().setAppName("Parallel ECLAT").setMaster("local[*]")
-        sc = SparkContext(conf=conf)
-
-        data = sc.textFile(self._iFile, self._numPartitions) \
-            .map(lambda line: [int(y) for y in line.rstrip().split(self._sep)]).persist()
-        self._lno = data.count()
-        self._minSup = self._convert(self._minSup)
-
-        frequentItems = None
-        frequentItems = data.zipWithIndex() \
-            .flatMap(lambda x: [(str(item), x[1]) for item in x[0]]) \
-            .groupByKey() \
-            .filter(lambda x: len(x[1]) >= self._minSup) \
-            .sortBy(lambda x: len(x[1])) \
-            .mapValues(set) \
-            .persist()
-        data.unpersist()
-        # elif 'temporal' in self._iFile:
-        #     frequentItems = data.flatMap(lambda trans: [(str(item), trans[0]) for item in trans[1:]]) \
-        #         .groupByKey() \
-        #         .filter(lambda x: len(x[1]) >= self._minSup) \
-        #         .mapValues(set) \
-        #         .persist()
-        #     data.unpersist()
-        # else:
-        #     pass
-        #     # print("may be not able to process the input file")
-
-        freqItems = dict(frequentItems.collect())
-        # print(len(freqItems))
-        self._finalPatterns = {k: len(v) for k, v in freqItems.items()}
-
-        freqPatterns = list(frequentItems.map(lambda x: self._genPatterns(x, x, list(freqItems.items())))
-                            .filter(lambda x: len(x) != 0).collect())
-        for value in freqPatterns:
-            self._finalPatterns.update(value)
-
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using Parallel ECLAT algorithm")
-        sc.stop()
-
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = parallelECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = parallelECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+    if len(_ab._sys.argv) == 7 or len(_ab._sys.argv) == 8:
+        if len(_ab._sys.argv) == 8:
+            _ap = VBFTMine(_ab._sys.argv[1], _ab._sys.argv[3],  _ab._sys.argv[4],
+                            _ab._sys.argv[5], _ab._sys.argv[6], _ab._sys.argv[7],)
+        if len(_ab._sys.argv) == 7:
+            _ap = VBFTMine(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         _ap.startMine()
-        _finalPatterns = _ap.getPatterns()
-        print("Total number of Frequent Patterns:", len(_finalPatterns))
+        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
+        _ap = VBFTMine('/Users/Likhitha/Downloads/fault/sample4.txt', 5, 3, 2, 1, ' ')
+        _ap.startMine()
+        _ap.printResults()
+        print(_ap.getPatternsAsDataFrame())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/pyspark/parallelFPGrowth.py` & `pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelFPGrowth.py`

 * *Files 4% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 #  **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
 #             import PAMI.frequentPattern.pyspark.parallelFPGrowth as alg
 #
 #             obj = alg.parallelFPGrowth(iFile, minSup, numWorkers)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -24,22 +24,23 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
 
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -54,14 +55,15 @@
 
 
 # from pyspark import SparkConf, SparkContext
 from collections import defaultdict
 from PAMI.frequentPattern.pyspark import abstract as _ab
 from operator import add
 from pyspark import SparkConf as _SparkConf, SparkContext as _SparkContext
+from deprecated import deprecated
 
 
 class Node:
     """
     :Attribute:
         item : int
             Storing item of a node
@@ -100,17 +102,18 @@
         self.nodeLink = {}
         self.itemCount = defaultdict(int)
 
 
     def addTransaction(self, transaction, count):
         """
         Add transaction to tree
-        :param transaction: list
-        :param count: int
-        :return:
+        :param transaction: Transaction to add
+        :type transaction: list
+        :param count: Number of nodes
+        :type count: int
         """
         current = self.root
         for item in transaction:
             if item not in current.children:
                 current.children[item] = Node(item, transaction[0:transaction.index(item)])
                 current.children[item].count += count
                 self.addNodeToNodeLink(current.children[item])
@@ -119,27 +122,28 @@
             self.itemCount[item] += count
             current = current.children[item]
 
 
     def addNodeToNodeLink(self, node):
         """
         Add node to self.nodeLink
-        :param node: Node
-        :return:
+        :param node: Node to add
+        :type node: Node
         """
         if node.item not in self.nodeLink:
             self.nodeLink[node.item] = [node]
         else:
             self.nodeLink[node.item].append(node)
 
 
     def generateConditionalTree(self, item):
         """
         Generate conditional tree based on item
-        :param item: str or int
+        :param item: Item to be considered as a condition
+        :type item: str or int
         :return: Tree
         """
         tree = Tree()
         for node in self.nodeLink[item]:
             tree.addTransaction(node.prefix, node.count)
         return tree
 
@@ -183,32 +187,37 @@
           To store the total amount of RSS memory consumed by the program
 
         lno : int
                 the number of transactions
     
     **Methods to execute code on terminal**
     ----------------------------------------------------
-        Format:
-                  >>> python3 parallelFPGrowth.py <inputFile> <outputFile> <minSup> <numWorkers>
 
-        Example:
-                  >>>  python3 parallelFPGrowth.py sampleDB.txt patterns.txt 10.0 3
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 parallelFPGrowth.py <inputFile> <outputFile> <minSup> <numWorkers>
+
+      Example Usage:
+
+      (.venv) $ python3 parallelFPGrowth.py sampleDB.txt patterns.txt 10.0 3
 
-        .. note:: minSup will be considered in percentage of database transactions
+    .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
     .. code-block:: python
     
                     import PAMI.frequentPattern.pyspark.parallelFPGrowth as alg
     
                     obj = alg.parallelFPGrowth(iFile, minSup, numWorkers)
     
-                    obj.startMine()
+                    obj.mine()
     
                     frequentPatterns = obj.getPatterns()
     
                     print("Total number of Frequent Patterns:", len(frequentPatterns))
     
                     obj.save(oFile)
     
@@ -247,15 +256,15 @@
     _memoryRSS = float()
     _lno = int()
 
 
     def __init__(self, iFile, minSup, numWorkers, sep='\t'):
         super().__init__(iFile, minSup, int(numWorkers), sep)
 
-
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
 
         self._startTime = _ab._time.time()
 
@@ -291,28 +300,74 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         sc.stop()
 
         print("Frequent patterns were generated successfully using Parallel FPGrowth algorithm")
 
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+
+        self._startTime = _ab._time.time()
+
+        conf = _SparkConf().setAppName("Parallel FPGrowth").setMaster("local[*]")
+        sc = _SparkContext(conf=conf)
+
+        rdd = sc.textFile(self._iFile, self._numPartitions)\
+            .map(lambda x: x.rstrip().split('\t'))\
+            .persist()
+
+        self._lno = rdd.count()
+        self._minSup = self._convert(self._minSup)
+
+        freqItems = rdd.flatMap(lambda trans: [(item, 1) for item in trans])\
+            .reduceByKey(add)\
+            .filter(lambda x: x[1] >= self._minSup)\
+            .sortBy(lambda x: x[1], ascending=False)\
+            .collect()
+        self._finalPatterns = dict(freqItems)
+        self._FPList = [x[0] for x in freqItems]
+        rank = dict([(item, index) for (index, item) in enumerate(self._FPList)])
+
+        workByPartition = rdd.flatMap(lambda x: self.genCondTransaction(x, rank)).groupByKey()
+
+        trees = workByPartition.foldByKey(Tree(), lambda tree, data: self.buildTree(tree, data))
+        freqPatterns = trees.flatMap(lambda tree_tuple: self.genAllFrequentPatterns(tree_tuple))
+        result = freqPatterns.map(lambda ranks_count: (tuple([self._FPList[z] for z in ranks_count[0]]), ranks_count[1]))\
+            .collect()
+
+        self._finalPatterns.update(dict(result))
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        sc.stop()
+
+        print("Frequent patterns were generated successfully using Parallel FPGrowth algorithm")
+
 
     def getPartitionId(self, value):
         """
         Get partition id of item
-        :param item: int
+        :param value: value to get partition id
+        :type value: int
         :return: integer
         """
         return value % self._numPartitions
 
     def genCondTransaction(self, trans, rank):
         """
         Generate conditional transactions from transaction
-        :param transaction : list
-        :param rank: dict
+        :param trans : transactions to generate conditional transactions
+        :type trans: list
+        :param rank: rank of conditional transactions to generate conditional transactions
+        :type rank: dict
         :return: list
         """
         newTrans = [rank[item] for item in trans if item in rank.keys()]
         newTrans = sorted(newTrans)
         condTrans = {}
         for i in reversed(newTrans):
             partition = self.getPartitionId(i)
@@ -320,42 +375,48 @@
                 condTrans[partition] = newTrans[:newTrans.index(i)+1]
         return [x for x in condTrans.items()]
 
     @staticmethod
     def buildTree(tree, data):
         """
         Build tree from data
-        :param tree: Tree
-        :param data: list
+        :param tree: tree to build
+        :type tree: Tree
+        :param data: data to build
+        :type data: list
         :return: tree
         """
         for trans in data:
             tree.addTransaction(trans, 1)
         return tree
 
     def genAllFrequentPatterns(self, tree_tuple):
         """
         Generate all frequent patterns
         :param tree_tuple: (partition id, tree)
+        :type tree_tuple: tuple
         :return: dict
         """
         itemList = sorted(tree_tuple[1].itemCount.items(), key=lambda x: x[1])
         itemList = [x[0] for x in itemList]
         freqPatterns = {}
         for item in itemList:
             if self.getPartitionId(item) == tree_tuple[0]:
                 freqPatterns.update(self.genFreqPatterns(item, [item], tree_tuple[1]))
         return freqPatterns.items()
 
     def genFreqPatterns(self, item, prefix, tree):
         """
         Generate new frequent patterns based on item.
         :param item: item
+        :type item: int
         :param prefix: prefix frequent pattern
-        :param tree: tree
+        :type prefix: str
+        :param tree: tree to generate patterns
+        :type tree: Tree
         :return:
         """
         condTree = tree.generateConditionalTree(item)
         freqPatterns = {}
         freqItems = {}
         for i in condTree.nodeLink.keys():
             freqItems[i] = 0
@@ -446,14 +507,15 @@
         print("Total ExecutionTime in ms:", self.getRuntime())
 
 
     def _convert(self, value):
         """
         To convert the user specified minSup value
         :param value: user specified minSup value
+        :type value: int or float or str
         :return: converted type
         """
         if type(value) is int:
             value = int(value)
         elif type(value) is float:
             value = (self._lno * value)
         elif type(value) is str:
@@ -471,14 +533,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = parallelFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = parallelFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
+        _ap.mine()
         _finalPatterns = _ap.getPatterns()
         print("Total number of Frequent Patterns:", len(_finalPatterns))
         # _ap.save(_ab._sys.argv[2])
         _memUSS = _ap.getMemoryUSS()
         print("Total Memory in USS:", _memUSS)
         _memRSS = _ap.getMemoryRSS()
         print("Total Memory in RSS", _memRSS)
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/topk/FAE.py` & `pami-2024.4.9.1/PAMI/frequentPattern/topk/FAE.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,43 +1,45 @@
 # Top - K is and algorithm to discover top frequent patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
 # ---------------------------------------------------------
 #
-#         import PAMI.frequentPattern.topK.FAE as alg
+#             import PAMI.frequentPattern.topK.FAE as alg
 #
-#         obj = alg.FAE(iFile, K)
+#             obj = alg.FAE(iFile, K)
 #
-#         obj.startMine()
+#             obj.mine()
 #
-#         topKFrequentPatterns = obj.getPatterns()
+#             topKFrequentPatterns = obj.getPatterns()
 #
-#         print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
+#             print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
 #
-#         obj.save(oFile)
+#             obj.save(oFile)
 #
-#         Df = obj.getPatternInDataFrame()
+#             Df = obj.getPatternInDataFrame()
 #
-#         memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#         print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#         memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#         print("Total Memory in RSS", memRSS)
-#
-#         run = obj.getRuntime()
-#
-#         print("Total ExecutionTime in seconds:", run)
-
+#             print("Total Memory in RSS", memRSS)
 #
+#             run = obj.getRuntime()
 #
+#             print("Total ExecutionTime in seconds:", run)
 #
+
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -46,29 +48,34 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 from PAMI.frequentPattern.topk import abstract as _ab
+from deprecated import deprecated
 
 
 class FAE(_ab._frequentPatterns):
     """
     :Description: Top - K is and algorithm to discover top frequent patterns in a transactional database.
 
 
     :Reference:   Zhi-Hong Deng, Guo-Dong Fang: Mining Top-Rank-K Frequent Patterns: DOI: 10.1109/ICMLC.2007.4370261 · Source: IEEE Xplore
                   https://ieeexplore.ieee.org/document/4370261
+
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  k: int :
                     User specified count of top frequent patterns
+    :param minimum: int :
+                    Minimum number of frequent patterns to consider in analysis
+
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
 
     :Attributes:
 
@@ -88,34 +95,39 @@
           To store the total amount of RSS memory consumed by the program
 
         finalPatterns : dict
             it represents to store the patterns
 
 
     **Methods to execute code on terminal**
-    ----------------------------------------
+    -------------------------------------------
+
+    .. code-block:: console
 
-        Format:
+      Format:
 
-           >>> python3 FAE.py <inputFile> <outputFile> <K>
+      (.venv) $ python3 FAE.py <inputFile> <outputFile> <K>
 
-        Examples:
+      Example Usage:
+
+      (.venv) $ python3 FAE.py sampleDB.txt patterns.txt 10
+
+    .. note:: k will be considered as count of top frequent patterns to consider in analysis
 
-           >>> python3 FAE.py sampleDB.txt patterns.txt 10
 
 
     **Importing this algorithm into a python program**
     ---------------------------------------------------------
     .. code-block:: python
 
         import PAMI.frequentPattern.topK.FAE as alg
 
         obj = alg.FAE(iFile, K)
 
-        obj.startMine()
+        obj.mine()
 
         topKFrequentPatterns = obj.getPatterns()
 
         print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
 
         obj.save(oFile)
 
@@ -285,32 +297,67 @@
             self._Generation(newPrefix, classItemSets, classTidSets)
             self._save(prefix, list(set(itemSetX)), tidSetI)
 
     def _convert(self, value):
         """
         to convert the type of user specified minSup value
         :param value: user specified minSup value
+        :type value: int or float or str
         :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = ((len(self._Database)) * value)
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
             Main function of the program
+        """
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._k is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._k = self._convert(self._k)
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetI).intersection(tidSetJ))
+                if len(y1) >= self._minimum:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print(" TopK frequent patterns were successfully generated using FAE algorithm.")
+        self._endTime = _ab._time.time()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
+    def mine(self):
+        """
+            Main function of the program
         """
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._k is None:
             raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
@@ -335,82 +382,89 @@
         self._memoryUSS = float()
         self._memoryRSS = float()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
-                    :return: returning USS memory consumed by the mining process
+        :return: returning USS memory consumed by the mining process
 
-                    :rtype: float
+        :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
 
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+        """
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
 
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
+        """
+        Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
 
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
     def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to an output file
+        """
+        Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
 
         :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
+        """
+        Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
 
         :rtype: dict
         """
         return self._finalPatterns
 
     def printTOPK(self):
-        """ this function is used to print the results
+        """
+        This function is used to print the results
         """
         print("Top K Frequent  Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
@@ -418,14 +472,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Top K Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/frequentPattern/topk/abstract.py` & `pami-2024.4.9.1/PAMI/frequentPattern/topk/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/fuzzyCorrelatedPattern/__init__.py` & `pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py` & `pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py`

 * *Files 11% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # **Importing this algorithm into a python program**
 # -------------------------------------------------------
 #
 #             from PAMI.fuzzyCorrelatedPattern.basic import FCPGrowth as alg
 #
 #             obj = alg.FCPGrowth("input.txt",2,0.4)
 #
-#             obj.startTimeMine()
+#             obj.mine()
 #
 #             correlatedFuzzyFrequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Correlated Fuzzy Frequent Patterns:", len(correlatedFuzzyFrequentPatterns))
 #
 #             obj.save("output")
 #
@@ -26,16 +26,20 @@
 #
 #             run = obj.getRuntime
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
+
+
 from PAMI.fuzzyCorrelatedPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
 
     :Attributes:
@@ -191,14 +195,29 @@
     """
     :Description:   FCPGrowth is the algorithm to discover Correlated Fuzzy-frequent patterns in a transactional database.
                     it is based on traditional fuzzy frequent pattern mining.
 
     :Reference:   Lin, N.P., & Chueh, H. (2007). Fuzzy correlation rules mining.
                   https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.416.6053&rep=rep1&type=pdf
 
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param minAllConf: float :
+                    The user can specify minAllConf values within the range (0, 1).
+
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
 
         iFile : file
             Name of the input file to mine complete set of fuzzy spatial frequent patterns
         oFile : file
             Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : int
@@ -258,30 +277,36 @@
             To find element with same tid as given
         WriteOut(prefix, prefixLen, item, sumIUtil,ratio)
             To Store the patten      
 
     **Executing the code on terminal :**
     ------------------------------------------
 
-            Format:
-                      >>> python3 FCPGrowth.py <inputFile> <outputFile> <minSup> <minAllConf> <sep>
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 FCPGrowth.py <inputFile> <outputFile> <minSup> <minAllConf> <sep>
+
+      Example Usage:
+
+      (.venv) $ python3 FCPGrowth.py sampleTDB.txt output.txt 2 0.2
+
+    .. note:: minSup will be considered in percentage of database transactions
 
-            Examples:
-                      >>> python3 FCPGrowth.py sampleTDB.txt output.txt 2 0.2
-                    
 
     **Sample run of importing the code:**
     -----------------------------------------
     .. code-block:: python
 
             from PAMI.fuzzyCorrelatedPattern.basic import FCPGrowth as alg
 
             obj = alg.FCPGrowth("input.txt",2,0.4)
 
-            obj.startTimeMine()
+            obj.mine()
 
             correlatedFuzzyFrequentPatterns = obj.getPatterns()
 
             print("Total number of Correlated Fuzzy Frequent Patterns:", len(correlatedFuzzyFrequentPatterns))
 
             obj.save("output")
 
@@ -333,14 +358,26 @@
         self._dbLen = 0
         self._transactions = []
         self._fuzzyValues = []
 
     def _compareItems(self, o1: _FFList, o2: _FFList) -> int:
         """
         A Function that sort all FFI-list in ascending order of Support
+
+        :param o1: First FFI-list
+
+        :type o1: _FFList
+
+        :param o2: Second FFI-list
+
+        :type o1: _FFList
+
+        :return: Comparision Value
+
+        :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
             # return int(o1.item) - int(o2.item)
             return 1
         else:
             return compare
@@ -370,15 +407,18 @@
         return None
 
     def _convert(self, value: Union[int, float, str]) -> float:
         """
         To convert the given user specified value
 
         :param value: user specified value
+
         :return: converted value
+
+        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._transactions) * value)
         if type(value) is str:
             if '.' in value:
@@ -387,14 +427,16 @@
             else:
                 value = int(value)
         return value
     
     def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+
+        :return: None
         """
         self._transactions, self._fuzzyValues = [], []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
@@ -425,14 +467,15 @@
                             quantities = parts[1].split()
                             self._transactions.append([x for x in items])
                             self._fuzzyValues.append([x for x in quantities])
                 except IOError:
                     print("File Not Found")
                     quit()
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """ 
         Frequent pattern mining process will startTime from here
         """
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         for tr in range(len(self._transactions)):
@@ -529,14 +572,119 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Fuzzy Correlated Patterns Successfully generated using FCPGrowth algorithms")
 
+
+    def mine(self) -> None:
+        """
+        Frequent pattern mining process will startTime from here
+        """
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        for tr in range(len(self._transactions)):
+            items = self._transactions[tr]
+            quantities = self._fuzzyValues[tr]
+            for i in range(0, len(items)):
+                item = items[i]
+                regions = _Regions(item, float(quantities[i]), 3, self._mapItemRegionSum)
+                if item in self._mapItemsLowSum.keys():
+                    low = self._mapItemsLowSum[item]
+                    low += regions.low
+                    self._mapItemsLowSum[item] = low
+                else:
+                    self._mapItemsLowSum[item] = regions.low
+                if item in self._mapItemsMidSum.keys():
+                    mid = self._mapItemsMidSum[item]
+                    mid += regions.middle
+                    self._mapItemsMidSum[item] = mid
+                else:
+                    self._mapItemsMidSum[item] = regions.middle
+                if item in self._mapItemsHighSum.keys():
+                    high = self._mapItemsHighSum[item]
+                    high += regions.high
+                    self._mapItemsHighSum[item] = high
+                else:
+                    self._mapItemsHighSum[item] = regions.high
+        listOfFFIList = []
+        mapItemsToFFLIST = {}
+        self._minSup = self._convert(self._minSup)
+        #minSup = self._minSup
+        self._minAllConf = float(self._minAllConf)
+        for item1 in self._mapItemsLowSum.keys():
+            item = item1
+            region = 'N'
+            low = self._mapItemsLowSum[item]
+            mid = self._mapItemsMidSum[item]
+            high = self._mapItemsHighSum[item]
+            if low >= mid and low >= high:
+                self._mapItemSum[item] = low
+                self._mapItemRegions[item] = "L"
+                region = 'L'
+            elif mid >= low and mid >= high:
+                self._mapItemSum[item] = mid
+                self._mapItemRegions[item] = "M"
+                region = 'M'
+            elif high >= low and high >= mid:
+                self._mapItemRegions[item] = "H"
+                region = 'H'
+                self._mapItemSum[item] = high
+            if self._mapItemSum[item] >= self._minSup:
+                fuList = _FFList(item, region)
+                mapItemsToFFLIST[item] = fuList
+                listOfFFIList.append(fuList)
+        listOfFFIList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for tr in range(len(self._transactions)):
+            items = self._transactions[tr]
+            quantities = self._fuzzyValues[tr]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                regions = _Regions(pair.item, float(quantities[i]), 3, self._temp)
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if self._mapItemRegions[pair.item] == "L":
+                        pair.quantity = regions.low
+                        pair.region = 'L'
+                    elif self._mapItemRegions[pair.item] == "M":
+                        pair.region = 'M'
+                        pair.quantity = regions.middle
+                    elif self._mapItemRegions[pair.item] == "H":
+                        pair.quantity = regions.high
+                        pair.region = 'H'
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                remainUtil = 0
+                for j in range(len(revisedTransaction) - 1, i - 1, -1):
+                    remainUtil += revisedTransaction[j].quantity
+                if pair.quantity > remainUtil:
+                    remainingUtility = pair.quantity
+                else:
+                    remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(pair.item) is not None:
+                    FFListOfItem = mapItemsToFFLIST[pair.item]
+                    element = Element(tid, pair.quantity, remainingUtility)
+                    FFListOfItem.addElement(element)
+            tid += 1
+        self._FSFIMining(self._itemSetBuffer, 0, listOfFFIList, self._minSup)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Fuzzy Correlated Patterns Successfully generated using FCPGrowth algorithms")
+
     def _FSFIMining(self, prefix: List[_FFList], prefixLen: int, FSFIM: List[_FFList], minSup: float) -> None:
         """
         Generates FFSI from prefix
 
         :param prefix: the prefix patterns of FFSI
         :type prefix: len
         :param prefixLen: the length of prefix
@@ -640,14 +788,15 @@
         :type prefix: list
         :param prefixLen: length of prefix
         :type prefixLen: int
         :param item: the last item
         :type item: FFList
         :param ratio: the ratio of itemSet
         :type ratio: float
+        :return: None
         """
         self._itemsCnt += 1
         res = ""
         for i in range(0, prefixLen):
             res += str(prefix[i].item) + "." + str(prefix[i].region) + '\t'
         res += str(item.item) + "." + str(item.region)
         #res1 = str(item.sumIUtil) + " : " + str(ratio) + "\n"
@@ -714,14 +863,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = FCPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]), _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = FCPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]))
         _ap.startMine()
+        _ap.mine()
         print("Total number of Fuzzy Correlated Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:",  _ap.getMemoryUSS())
         print("Total Memory in RSS",  _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/fuzzyCorrelatedPattern/basic/__init__.py` & `pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/fuzzyCorrelatedPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/fuzzyFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py` & `pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,24 +1,26 @@
-# Fuzzy Frequent  Pattern-Miner is desired to find all  frequent fuzzy patterns which is on-trivial and challenging problem
-# to its huge search space.we are using efficient pruning techniques to reduce the search space.
+# F3PMiner algorithm discovers the fuzzy partial periodic patterns in quantitative Irregulat multiple timeseries databases.
+#
 #
 # **Importing this algorithm into a python program**
-# ---------------------------------------------------------
+# ----------------------------------------------------
+#
+#             import PAMI.fuzzyPartialPeriodicPattern.basic.F3PMiner as alg
 #
-#             from PAMI.fuzzyFrequentPattern import FFIMiner as alg
+#             obj = alg.F3PMiner(iFile, minSup, sep)
 #
-#             obj = alg.FFIMiner("input.txt", 2)
+#             obj.mine()
 #
-#             obj.startMine()
+#             fuzzyPartialPeriodicPatterns = obj.getPatterns()
 #
-#             fuzzyFrequentPattern = obj.getPatterns()
+#             print("Total number of Fuzzy Partial Periodic Patterns:", len(fuzzyPartialPeriodicPatterns))
 #
-#             print("Total number of Fuzzy Frequent Patterns:", len(fuzzyFrequentPattern))
+#             obj.save(oFile)
 #
-#             obj.save("outputFile")
+#             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -26,78 +28,93 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
 
+     This program is free software: you can redistribute it and/or modify
+     it under the terms of the GNU General Public License as published by
+     the Free Software Foundation, either version 3 of the License, or
+     (at your option) any later version.
+
+     This program is distributed in the hope that it will be useful,
+     but WITHOUT ANY WARRANTY; without even the implied warranty of
+     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+     GNU General Public License for more details.
+
+     You should have received a copy of the GNU General Public License
+     along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from PAMI.fuzzyFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from PAMI.fuzzyPartialPeriodicPatterns.basic import abstract as _ab
+from deprecated import deprecated
 
 
 class _FFList:
     """
-    A class represent a Fuzzy List of an element
+     A class represent a Fuzzy List of an element
 
     :Attributes:
 
-        item: int
-            the item name
-        sumIUtil: float
-            the sum of utilities of a fuzzy item in database
-        sumRUtil: float
-            the sum of resting values of a fuzzy item in database
-        elements: list
-            a list of elements contain tid,Utility and resting values of element in each transaction
+         item: int
+             the item name
+         sumIUtil: float
+             the sum of utilities of an fuzzy item in database
+         sumRUtil: float
+             the sum of resting values of a fuzzy item in database
+         elements: list
+             a list of elements contain tid,Utility and resting values of element in each transaction
 
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
+
         printElement(e)
             Method to print elements
+
     """
 
-    def __init__(self, itemName: int) -> None:
+    def __init__(self, itemName):
         self.item = itemName
         self.sumIUtil = 0.0
-        self.sumRUtil = 0.0
         self.elements = []
 
-    def addElement(self, element) -> None:
+    def addElement(self, element):
         """
         A Method that add a new element to FFList
 
         :param element: an element to be added to FFList
-        :param element: Element
+        :type element: Element
         """
         self.sumIUtil += element.iUtils
-        self.sumRUtil += element.rUtils
         self.elements.append(element)
 
-    def printElement(self) -> None:
+    def printElement(self):
         """
         A method to print elements
         """
         for ele in self.elements:
             print(ele.tid, ele.iUtils, ele.rUtils)
 
 
@@ -106,69 +123,76 @@
     A class represents an Element of a fuzzy list
 
     :Attributes:
 
         tid : int
             keep tact of transaction id
         iUtils: float
-            the utility of a fuzzy item in the transaction
+            the utility of an fuzzy item in the transaction
         rUtils : float
-            the  resting value of a fuzzy item in the transaction
+            the  resting value of an fuzzy item in the transaction
     """
 
-    def __init__(self, tid: int, iUtil: float, rUtil: float) -> None:
+    def __init__(self, tid, iUtil):
         self.tid = tid
         self.iUtils = iUtil
-        self.rUtils = rUtil
 
 
 class _Pair:
     """
     A class to store item and it's quantity together
     """
 
-    def __init__(self) -> None:
+    def __init__(self):
         self.item = 0
         self.quantity = 0
 
 
-class FFIMiner(_ab._fuzzyFrequentPattenrs):
+class F3PMiner(_ab._fuzzyPartialPeriodicPatterns):
     """
-    :Description:   Fuzzy Frequent  Pattern-Miner is desired to find all  frequent fuzzy patterns which is on-trivial and challenging problem
-                    to its huge search space.we are using efficient pruning techniques to reduce the search space.
+    :Description:   F3PMiner algorithm discovers the fuzzy partial periodic patterns in quantitative Irregulat multiple timeseries databases.
+    
+    :Reference:
+
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
 
-    :Reference:   Lin, Chun-Wei & Li, Ting & Fournier Viger, Philippe & Hong, Tzung-Pei. (2015).
-                  A fast Algorithm for mining fuzzy frequent itemsets. Journal of Intelligent & Fuzzy Systems. 29.
-                  2373-2379. 10.3233/IFS-151936.
-                  https://www.researchgate.net/publication/286510908_A_fast_Algorithm_for_mining_fuzzy_frequent_itemSets
 
     :Attributes:
 
         iFile : string
-            Name of the input file to mine complete set of fuzzy  frequent patterns
-        fmFile : string
-            Name of the fuzzy membership file to mine complete set of fuzzy  frequent patterns
+            Name of the input file to mine complete set of fuzzy spatial frequent patterns
         oFile : string
-            Name of the oFile file to store complete set of fuzzy  frequent patterns
+               Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : float
             The user given minimum support
         memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+                To store the total amount of RSS memory consumed by the program
         startTime:float
-            To record the start time of the mining process
+               To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         itemsCnt: int
             To record the number of fuzzy spatial itemSets generated
-        mapItemsLowSum: map
-            To keep track of low region values of items
+        mapItemsGSum: map
+            To keep track of G region values of items
         mapItemsMidSum: map
-            To keep track of middle region values of items
-        mapItemsHighSum: map
-            To keep track of high region values of items
+            To keep track of M region values of items
+        mapItemsHSum: map
+            To keep track of H region values of items
         mapItemSum: map
             To keep track of sum of Fuzzy Values of items
         mapItemRegions: map
             To Keep track of fuzzy regions of item
         jointCnt: int
             To keep track of the number of ffi-list that was constructed
         BufferSize: int
@@ -192,132 +216,154 @@
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         convert(value):
             To convert the given user specified value
         compareItems(o1, o2)
             A Function that sort all ffi-list in ascending order of Support
-        FSFIMining(prefix, prefixLen, FSFIM, minSup)
+        F3PMining(prefix, prefixLen, FSFIM, minSup)
             Method generate ffi from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
         findElementWithTID(uList, tid)
             To find element with same tid as given
         WriteOut(prefix, prefixLen, item, sumIUtil)
             To Store the patten
 
     **Executing the code on terminal :**
-    ----------------------------------------
-            Format:
-                    >>> python3 FFIMiner.py <inputFile> <outputFile> <minSup> <separator>
+    ---------------------------------------
 
-            Examples:
-                    >>> python3  FFIMiner.py sampleTDB.txt output.txt 6
+    .. code-block:: console
 
+      Format:
 
-    **Sample run of importing the code:**
-    ------------------------------------------
-    .. code-block:: python
+      (.venv) $ python3 F3PMiner.py <inputFile> <outputFile> <minSup> <separator>
 
-            from PAMI.fuzzyFrequentPattern import FFIMiner as alg
+      Example Usage:
 
-            obj = alg.FFIMiner("input.txt", 2)
+      (.venv) $ python3  F3PMiner.py sampleTDB.txt output.txt 6
 
-            obj.startMine()
+    .. note:: minSup will be considered in percentage of database transactions
 
-            fuzzyFrequentPattern = obj.getPatterns()
 
-            print("Total number of Fuzzy Frequent Patterns:", len(fuzzyFrequentPattern))
+    **Sample run of importing the code:**
+    --------------------------------------
 
-            obj.save("outputFile")
+        from PAMI.fuzzyPartialPeriodicPatterns import F3PMiner as alg
 
-            memUSS = obj.getMemoryUSS()
+        obj = alg.F3PMiner("input.txt", 2)
 
-            print("Total Memory in USS:", memUSS)
+        obj.mine()
 
-            memRSS = obj.getMemoryRSS()
+        fuzzyPartialPeriodicPatterns = obj.getPatterns()
 
-            print("Total Memory in RSS", memRSS)
+        print("Total number of Fuzzy Frequent Patterns:", len(fuzzyPartialPeriodicPatterns))
 
-            run = obj.getRuntime()
+        obj.save("outputFile")
 
-            print("Total ExecutionTime in seconds:", run)
+        memUSS = obj.getMemoryUSS()
 
+        print("Total Memory in USS:", memUSS)
 
-    **Credits:**
-    ---------------
-            The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
+        memRSS = obj.getMemoryRSS()
 
-    """
+        print("Total Memory in RSS", memRSS)
 
+        run = obj.getRuntime()
+
+        print("Total ExecutionTime in seconds:", run)
+
+    **Credits:**
+    -------------
+        The complete program was written by PALLA Likhitha under the supervision of Professor Rage Uday Kiran.
+    """
+    
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
-    _fuzFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _sep = "\t"
 
-    def __init__(self, iFile: str, minSup: float, sep: str="\t") -> None:
+    def __init__(self, iFile, minSup, sep="\t"):
         super().__init__(iFile, minSup, sep)
         self._startTime = 0
         self._endTime = 0
         self._itemsCnt = 0
         self._mapItemSum = {}
+        self._mapItemRegions = {}
         self._joinsCnt = 0
         self._BufferSize = 200
         self._itemSetBuffer = []
         self._transactions = []
         self._fuzzyValues = []
+        self._ts = []
         self._finalPatterns = {}
         self._dbLen = 0
 
-    def _compareItems(self, o1: _FFList, o2: _FFList) -> int:
+    def _compareItems(self, o1, o2):
         """
         A Function that sort all ffi-list in ascending order of Support
+
+        :param o1: First FFI-list
+
+        :type o1: _FFList
+
+        :param o2: Second FFI-list
+
+        :type o1: _FFList
+
+        :return: Comparision Value
+
+        :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
             if o1.item < o2.item:
                 return -1
             elif o1.item > o2.item:
                 return 1
             else:
                 return 0
         else:
             return compare
 
-    def _convert(self, value) -> Union[int, float]:
+    def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
+
+        :type value: int or float or str
+
         :return: converted value
+
+        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (self._dbLen * value)
             else:
                 value = int(value)
         return value
 
-    def _creatingItemsets(self) -> None:
+    def _creatingItemsets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-        self._transactions, self._fuzzyValues, self._Database = [], [], []
+        self._transactions, self._fuzzyValues, self._Database, self._ts = [], [], [], []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 self._transactions = self._iFile['Transactions'].tolist()
             if 'fuzzyValues' in i:
@@ -327,57 +373,131 @@
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line = line.decode("utf-8")
                     line = line.split("\n")[0]
                     parts = line.split(":")
                     parts[0] = parts[0].strip()
-                    parts[1] = parts[1].strip()
+                    parts[2] = parts[2].strip()
                     items = parts[0].split(self._sep)
-                    quantities = parts[1].split(self._sep)
+                    quantities = parts[2].split(self._sep)
                     self._transactions.append([x for x in items])
-                    self._fuzzyValues.append([float(x) for x in quantities])
+                    self._fuzzyValues.append([x for x in quantities])
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line = line.split("\n")[0]
+                            line = line.strip()
                             parts = line.split(":")
                             parts[0] = parts[0].strip()
                             parts[1] = parts[1].strip()
-                            items = parts[0].split(self._sep)
-                            quantities = parts[1].split(self._sep)
-                            self._transactions.append([x for x in items])
-                            self._fuzzyValues.append([float(x) for x in quantities])
+                            parts[2] = parts[2].strip()
+                            times = parts[0].split(self._sep)
+                            items = parts[1].split(self._sep)
+                            quantities = parts[2].split(self._sep)
+                            #print(times, items, quantities)
+                            _time = [x for x in times if x]
+                            items = [x for x in items if x]
+                            quantities = [float(x) for x in quantities if x]
+                            tempList = []
+                            for k in range(len(_time)):
+                                ite = "(" + _time[k] + "," + items[k] + ")"
+                                tempList.append(ite)
+                            self._ts.append([x for x in times])
+                            self._transactions.append([x for x in tempList])
+                            self._fuzzyValues.append([x for x in quantities])
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def startMine(self) -> None:
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
+        """
+        fuzzy-Frequent pattern mining process will start from here
+        """
+        self._startTime = _ab._time.time()
+        self._creatingItemsets()
+        for line in range(len(self._transactions)):
+            times = self._ts[line]
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            self._dbLen += 1
+            for i in range(0, len(items)):
+                item = items[i]
+                if item in self._mapItemSum:
+                    self._mapItemSum[item] += quantities[i]
+                else:
+                    self._mapItemSum[item] = quantities[i]
+        listOfffilist = []
+        mapItemsToFFLIST = {}
+        #self._minSup = float(self._minSup)
+        self._minSup = self._convert(self._minSup)
+        minSup = self._minSup
+        for item1 in self._mapItemSum.keys():
+            item = item1
+            # print(type(self._mapItemSum[item]))
+            if self._mapItemSum[item] >= self._minSup:
+                fuList = _FFList(item)
+                mapItemsToFFLIST[item] = fuList
+                listOfffilist.append(fuList)
+        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                pair.quantity = quantities[i]
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                if mapItemsToFFLIST.get(pair.item) is not None:
+                    FFListOfItem = mapItemsToFFLIST[pair.item]
+                    element = _Element(tid, pair.quantity)
+                    FFListOfItem.addElement(element)
+            tid += 1
+        self._F3PMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+
+    def mine(self):
         """
         fuzzy-Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
         self._creatingItemsets()
         for line in range(len(self._transactions)):
+            times = self._ts[line]
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             self._dbLen += 1
             for i in range(0, len(items)):
                 item = items[i]
                 if item in self._mapItemSum:
                     self._mapItemSum[item] += quantities[i]
                 else:
                     self._mapItemSum[item] = quantities[i]
         listOfffilist = []
         mapItemsToFFLIST = {}
-        #self._minSup = self._convert(self._minSup)
-        # minSup = self.minSup
+        #self._minSup = float(self._minSup)
+        self._minSup = self._convert(self._minSup)
+        minSup = self._minSup
         for item1 in self._mapItemSum.keys():
             item = item1
+            # print(type(self._mapItemSum[item]))
             if self._mapItemSum[item] >= self._minSup:
                 fuList = _FFList(item)
                 mapItemsToFFLIST[item] = fuList
                 listOfffilist.append(fuList)
         listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
         tid = 0
         for line in range(len(self._transactions)):
@@ -391,86 +511,82 @@
                 item = pair.item
                 if self._mapItemSum[item] >= self._minSup:
                     if pair.quantity > 0:
                         revisedTransaction.append(pair)
             revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
             for i in range(len(revisedTransaction) - 1, -1, -1):
                 pair = revisedTransaction[i]
-                remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i, -1):
-                    remainUtil += revisedTransaction[j].quantity
-                remainingUtility = remainUtil
                 if mapItemsToFFLIST.get(pair.item) is not None:
                     FFListOfItem = mapItemsToFFLIST[pair.item]
-                    element = _Element(tid, pair.quantity, remainingUtility)
+                    element = _Element(tid, pair.quantity)
                     FFListOfItem.addElement(element)
             tid += 1
-        self._FFIMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
+        self._F3PMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def _FFIMining(self, prefix, prefixLen, FSFIM, minSup):
+    def _F3PMining(self, prefix, prefixLen, FSFIM, minSup):
         """
         Generates ffi from prefix
 
         :param prefix: the prefix patterns of ffi
         :type prefix: len
         :param prefixLen: the length of prefix
         :type prefixLen: int
         :param FSFIM: the Fuzzy list of prefix itemSets
         :type FSFIM: list
         :param minSup: the minimum support of
-        :type minSup: int or flaot
+        :type minSup:int
         """
         for i in range(0, len(FSFIM)):
             X = FSFIM[i]
+            exULs = []
             if X.sumIUtil >= minSup:
                 self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
-            if X.sumRUtil >= minSup:
-                exULs = []
                 for j in range(i + 1, len(FSFIM)):
                     Y = FSFIM[j]
                     exULs.append(self._construct(X, Y))
                     self._joinsCnt += 1
                 self._itemSetBuffer.insert(prefixLen, X.item)
-                self._FFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup)
+                self._F3PMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup)
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
-        """
+       """
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
 
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime - self._startTime
 
-    def _construct(self, px, py) -> _FFList:
+    def _construct(self, px, py):
         """
         A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
 
         :param px:the itemSet px
         :type px:ffi-List
         :param py:itemSet py
         :type py:ffi-List
@@ -478,19 +594,19 @@
         :rtype :ffi-List
         """
         pxyUL = _FFList(py.item)
         for ex in px.elements:
             ey = self._findElementWithTID(py, ex.tid)
             if ey is None:
                 continue
-            eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)), ey.rUtils)
+            eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)))
             pxyUL.addElement(eXY)
         return pxyUL
 
-    def _findElementWithTID(self, uList, tid) -> _Element:
+    def _findElementWithTID(self, uList, tid):
         """
         To find element with same tid as given
 
         :param uList: fuzzyList
         :type uList: ffi-List
         :param tid: transaction id
         :type tid: int
@@ -506,98 +622,90 @@
                 first = mid + 1
             elif List[mid].tid > tid:
                 last = mid - 1
             else:
                 return List[mid]
         return None
 
-    def _WriteOut(self, prefix: list, prefixLen: int, item: int, sumIUtil: float) -> None:
+    def _WriteOut(self, prefix, prefixLen, item, sumIUtil):
         """
         To Store the patten
 
         :param prefix: prefix of itemSet
         :type prefix: list
         :param prefixLen: length of prefix
         :type prefixLen: int
         :param item: the last item
         :type item: int
         :param sumIUtil: sum of utility of itemSet
         :type sumIUtil: float
+
         """
         self._itemsCnt += 1
         res = ""
         for i in range(0, prefixLen):
-            res += str(prefix[i])  + "\t"
+            res += str(prefix[i]) + "\t"
         res += str(item)
         res1 = str(sumIUtil)
         self._finalPatterns[res] = res1
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
-    def getPatterns(self) -> dict:
+    def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def save(self, outFile) -> dict:
+    def save(self, outFile):
         """
-        Complete set of frequent patterns will be loaded in to an output file
+        Complete set of frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Fuzzy Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Fuzzy Partial Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = FFIMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = F3PMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = FFIMiner(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = F3PMiner(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.mine()
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
+        _ap.printResults()
     else:
-        _ap = FFIMiner('sample.txt', 1, ' ')
-        _ap.startMine()
-        print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save('output.txt')
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py` & `pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 # Fuzzy Frequent  Pattern-Miner is desired to find all  frequent fuzzy patterns which is on-trivial and challenging problem
+#
 # to its huge search space.we are using efficient pruning techniques to reduce the search space.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
+# ---------------------------------------------------------
 #
-#             from PAMI.fuzzyFrequentPattern import FFIMiner_old as alg
+#             from PAMI.fuzzyFrequentPattern import FFIMiner as alg
 #
 #             obj = alg.FFIMiner("input.txt", 2)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             fuzzyFrequentPattern = obj.getPatterns()
 #
 #             print("Total number of Fuzzy Frequent Patterns:", len(fuzzyFrequentPattern))
 #
 #             obj.save("outputFile")
 #
@@ -26,16 +27,18 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -44,32 +47,34 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
+
 from PAMI.fuzzyFrequentPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
 
     :Attributes:
 
-         item: int
-             the item name
-         sumIUtil: float
-             the sum of utilities of a fuzzy item in database
-         sumRUtil: float
-             the sum of resting values of a fuzzy item in database
-         elements: list
-             a list of elements contain tid,Utility and resting values of element in each transaction
+        item: int
+            the item name
+        sumIUtil: float
+            the sum of utilities of a fuzzy item in database
+        sumRUtil: float
+            the sum of resting values of a fuzzy item in database
+        elements: list
+            a list of elements contain tid,Utility and resting values of element in each transaction
 
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
         printElement(e)
             Method to print elements
@@ -82,15 +87,18 @@
         self.elements = []
 
     def addElement(self, element) -> None:
         """
         A Method that add a new element to FFList
 
         :param element: an element to be added to FFList
-        :param element: Element
+
+        :type element: Element
+
+        :return: None
         """
         self.sumIUtil += element.iUtils
         self.sumRUtil += element.rUtils
         self.elements.append(element)
 
     def printElement(self) -> None:
         """
@@ -116,51 +124,14 @@
 
     def __init__(self, tid: int, iUtil: float, rUtil: float) -> None:
         self.tid = tid
         self.iUtils = iUtil
         self.rUtils = rUtil
 
 
-class _Regions:
-    """
-    A class calculate the regions
-
-    :Attributes:
-
-        low : int
-            low region value
-        middle: int
-            middle region value
-        high : int
-            high region values
-    """
-
-    def __init__(self, quantity: int, regionsNumber: int) -> None:
-        self.low = 0
-        self.middle = 0
-        self.high = 0
-        if regionsNumber == 3:  # if we have 3 regions
-            if 0 < quantity <= 1:
-                self.low = 1
-                self.high = 0
-                self.middle = 0
-            elif 1 < quantity <= 6:
-                self.low = float((6 - quantity) / 5)
-                self.middle = float((quantity - 1) / 5)
-                self.high = 0
-            elif 6 < quantity <= 11:
-                self.low = 0
-                self.middle = float((11 - quantity) / 5)
-                self.high = float((quantity - 6) / 5)
-            else:
-                self.low = 0
-                self.middle = 0
-                self.high = 1
-
-
 class _Pair:
     """
     A class to store item and it's quantity together
     """
 
     def __init__(self) -> None:
         self.item = 0
@@ -173,28 +144,43 @@
                     to its huge search space.we are using efficient pruning techniques to reduce the search space.
 
     :Reference:   Lin, Chun-Wei & Li, Ting & Fournier Viger, Philippe & Hong, Tzung-Pei. (2015).
                   A fast Algorithm for mining fuzzy frequent itemsets. Journal of Intelligent & Fuzzy Systems. 29.
                   2373-2379. 10.3233/IFS-151936.
                   https://www.researchgate.net/publication/286510908_A_fast_Algorithm_for_mining_fuzzy_frequent_itemSets
 
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param fuzFile: str :
+                    The user can specify fuzFile.
+
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
 
         iFile : string
             Name of the input file to mine complete set of fuzzy  frequent patterns
         fmFile : string
             Name of the fuzzy membership file to mine complete set of fuzzy  frequent patterns
         oFile : string
-               Name of the oFile file to store complete set of fuzzy  frequent patterns
+            Name of the oFile file to store complete set of fuzzy  frequent patterns
         minSup : float
             The user given minimum support
         memoryRSS : float
-                To store the total amount of RSS memory consumed by the program
+            To store the total amount of RSS memory consumed by the program
         startTime:float
-               To record the start time of the mining process
+            To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         itemsCnt: int
             To record the number of fuzzy spatial itemSets generated
         mapItemsLowSum: map
             To keep track of low region values of items
         mapItemsMidSum: map
@@ -237,158 +223,143 @@
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
         findElementWithTID(uList, tid)
             To find element with same tid as given
         WriteOut(prefix, prefixLen, item, sumIUtil)
             To Store the patten
 
+
     **Executing the code on terminal :**
-    -----------------------------------------
+    ------------------------------------------
+
+    .. code-block:: console
+
+      Format:
 
-            Format:
-                    >>> python3 FFIMinerMiner.py <inputFile> <outputFile> <minSup> <separator>
-            Examples:
-                    >>> python3  FFIMinerMiner.py sampleTDB.txt output.txt 6
+      (.venv) $ python3 FFIMiner.py <inputFile> <outputFile> <minSup> <separator>
 
-                    >>> python3  FFIMinerMiner.py sampleTDB.txt output.txt 0.3
+      Example Usage:
 
-                        (it will consider '\t' as a separator)
+      (.venv) $ python3  FFIMiner.py sampleTDB.txt output.txt 6
+
+    .. note:: minSup will be considered in percentage of database transactions
 
-                    >>> python3  FFIMinerMiner.py sampleTDB.txt output.txt 6 , (it consider ',' as a separator)
 
     **Sample run of importing the code:**
-    ----------------------------------------
+    ------------------------------------------
+    .. code-block:: python
 
-        from PAMI.fuzzyFrequentPattern import FFIMiner as alg
+            from PAMI.fuzzyFrequentPattern import FFIMiner as alg
 
-        obj = alg.FFIMiner("input.txt", "fuzzyMembership.txt" 2)
+            obj = alg.FFIMiner("input.txt", 2)
 
-        obj.startMine()
+            obj.mine()
 
-        fuzzyFrequentPattern = obj.getPatterns()
+            fuzzyFrequentPattern = obj.getPatterns()
 
-        print("Total number of Fuzzy Frequent Patterns:", len(fuzzyFrequentPattern))
+            print("Total number of Fuzzy Frequent Patterns:", len(fuzzyFrequentPattern))
 
-        obj.save("outputFile")
+            obj.save("outputFile")
 
-        memUSS = obj.getMemoryUSS()
+            memUSS = obj.getMemoryUSS()
 
-        print("Total Memory in USS:", memUSS)
+            print("Total Memory in USS:", memUSS)
 
-        memRSS = obj.getMemoryRSS()
+            memRSS = obj.getMemoryRSS()
 
-        print("Total Memory in RSS", memRSS)
+            print("Total Memory in RSS", memRSS)
 
-        run = obj.getRuntime()
+            run = obj.getRuntime()
 
-        print("Total ExecutionTime in seconds:", run)
+            print("Total ExecutionTime in seconds:", run)
 
 
     **Credits:**
-    -------------
-        The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
+    ---------------
+            The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
 
     """
+
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _fuzFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _sep = "\t"
 
-    def __init__(self, iFile: str, fuzFile: str, minSup: float, sep: str="\t") -> None:
-        super().__init__(iFile, fuzFile, minSup, sep)
+    def __init__(self, iFile: str, minSup: float, sep: str="\t") -> None:
+        super().__init__(iFile, minSup, sep)
         self._startTime = 0
         self._endTime = 0
         self._itemsCnt = 0
-        self._mapItemsLowSum = {}
-        self._mapItemsMidSum = {}
-        self._mapItemsHighSum = {}
         self._mapItemSum = {}
-        self._mapItemRegions = {}
         self._joinsCnt = 0
         self._BufferSize = 200
         self._itemSetBuffer = []
         self._transactions = []
         self._fuzzyValues = []
         self._finalPatterns = {}
-        self._RegionsCal = []
-        self._LabelKeyOne = {}
-        self._LabelKey = {}
-        self._RegionsLabel = []
         self._dbLen = 0
 
     def _compareItems(self, o1: _FFList, o2: _FFList) -> int:
         """
         A Function that sort all ffi-list in ascending order of Support
+
+        :param o1: First FFI-list
+
+        :type o1: _FFList
+
+        :param o2: Second FFI-list
+
+        :type o1: _FFList
+
+        :return: Comparision Value
+
+        :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
             if o1.item < o2.item:
                 return -1
             elif o1.item > o2.item:
                 return 1
             else:
                 return 0
         else:
             return compare
 
-    def _convert(self, value: Union[int, float, str]) -> float:
+    def _convert(self, value) -> Union[int, float]:
         """
         To convert the given user specified value
 
         :param value: user specified value
+
+        :type value: int or float or str
+
         :return: converted value
+
+        :rtype: int or float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (self._dbLen * value)
             else:
                 value = int(value)
         return value
 
-    def _fuzzyMembershipFunc(self) -> None:
-        try:
-            with open(self._fuzFile, 'r', encoding='utf-8') as f:
-                count = 0
-                for line in f:
-                    line = line.split("\n")[0]
-                    parts = line.split(" ")
-                    lowerBound = parts[0].strip()
-                    upperBound = parts[1].strip()
-                    lb_Label = parts[2].strip()
-                    ub_Label = parts[3].strip()
-                    self._RegionsCal.append([int(lowerBound), int(upperBound)])
-                    self._RegionsLabel.append([lb_Label, ub_Label])
-                    for i in range(0, 2):
-                        if lb_Label.capitalize() not in self._LabelKey:
-                            self._LabelKey[lb_Label.capitalize()] = count
-                            count += 1
-                        if ub_Label.capitalize() not in self._LabelKey:
-                            self._LabelKey[ub_Label.capitalize()] = count
-                            count += 1
-            self._LabelKeyOne = {v:k for k,v in self._LabelKey.items()}
-            print(self._LabelKey)
-            print(self._LabelKeyOne)
-            print(self._RegionsLabel)
-            print(self._RegionsCal)
-        except IOError:
-            print("File Not Found")
-            quit()
-
     def _creatingItemsets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
         """
         self._transactions, self._fuzzyValues, self._Database = [], [], []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
@@ -403,181 +374,185 @@
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line = line.decode("utf-8")
                     line = line.split("\n")[0]
                     parts = line.split(":")
                     parts[0] = parts[0].strip()
-                    parts[2] = parts[2].strip()
+                    parts[1] = parts[1].strip()
                     items = parts[0].split(self._sep)
-                    quantities = parts[2].split(self._sep)
+                    quantities = parts[1].split(self._sep)
                     self._transactions.append([x for x in items])
-                    self._fuzzyValues.append([x for x in quantities])
+                    self._fuzzyValues.append([float(x) for x in quantities])
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line = line.split("\n")[0]
                             parts = line.split(":")
                             parts[0] = parts[0].strip()
-                            parts[2] = parts[2].strip()
+                            parts[1] = parts[1].strip()
                             items = parts[0].split(self._sep)
-                            quantities = parts[2].split(self._sep)
+                            quantities = parts[1].split(self._sep)
                             self._transactions.append([x for x in items])
-                            self._fuzzyValues.append([x for x in quantities])
+                            self._fuzzyValues.append([float(x) for x in quantities])
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _Regions(self, quantity: float) -> None:
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self) -> None:
         """
-        :param quantity:
-        :type quantity:
+        fuzzy-Frequent pattern mining process will start from here
         """
-        self.list = [0] * len(self._LabelKey)
-        if self._RegionsCal[0][0] < quantity <= self._RegionsCal[0][1]:
-            self.list[0] = 1
-            return
-        elif quantity >= self._RegionsCal[-1][0]:
-            self.list[-1] = 1
-            return
-        else:
-            for i in range(1, len(self._RegionsCal) - 1):
-                if self._RegionsCal[i][0] < quantity <= self._RegionsCal[i][1]:
-                    base = self._RegionsCal[i][1] - self._RegionsCal[i][0]
-                    for pos in range(0, 2):
-                        if self._RegionsLabel[i][pos].islower():
-                            self.list[self._LabelKey[self._RegionsLabel[i][pos].capitalize()]] = float(
-                                (self._RegionsCal[i][1] - quantity) / base)
-                        else:
-                            self.list[self._LabelKey[self._RegionsLabel[i][pos].capitalize()]] = float(
-                                (quantity - self._RegionsCal[i][0]) / base)
-            return
+        self._startTime = _ab._time.time()
+        self._creatingItemsets()
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            self._dbLen += 1
+            for i in range(0, len(items)):
+                item = items[i]
+                if item in self._mapItemSum:
+                    self._mapItemSum[item] += quantities[i]
+                else:
+                    self._mapItemSum[item] = quantities[i]
+        listOfffilist = []
+        mapItemsToFFLIST = {}
+        #self._minSup = self._convert(self._minSup)
+        # minSup = self.minSup
+        for item1 in self._mapItemSum.keys():
+            item = item1
+            if self._mapItemSum[item] >= self._minSup:
+                fuList = _FFList(item)
+                mapItemsToFFLIST[item] = fuList
+                listOfffilist.append(fuList)
+        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                pair.quantity = quantities[i]
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                remainUtil = 0
+                for j in range(len(revisedTransaction) - 1, i, -1):
+                    remainUtil += revisedTransaction[j].quantity
+                remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(pair.item) is not None:
+                    FFListOfItem = mapItemsToFFLIST[pair.item]
+                    element = _Element(tid, pair.quantity, remainingUtility)
+                    FFListOfItem.addElement(element)
+            tid += 1
+        self._FFIMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
-    def startMine(self) -> None:
+    def mine(self) -> None:
         """
         fuzzy-Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
         self._creatingItemsets()
         for line in range(len(self._transactions)):
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             self._dbLen += 1
             for i in range(0, len(items)):
-                regions = self._Regions(float(quantities[i]))
-                print(regions)
                 item = items[i]
-                if item in self._mapItemsLowSum.keys():
-                    low = self._mapItemsLowSum[item]
-                    low += regions.low
-                    self._mapItemsLowSum[item] = low
-                else:
-                    self._mapItemsLowSum[item] = regions.low
-                if item in self._mapItemsMidSum.keys():
-                    mid = self._mapItemsMidSum[item]
-                    mid += regions.middle
-                    self._mapItemsMidSum[item] = mid
+                if item in self._mapItemSum:
+                    self._mapItemSum[item] += quantities[i]
                 else:
-                    self._mapItemsMidSum[item] = regions.middle
-                if item in self._mapItemsHighSum.keys():
-                    high = self._mapItemsHighSum[item]
-                    high += regions.high
-                    self._mapItemsHighSum[item] = high
-                else:
-                    self._mapItemsHighSum[item] = regions.high
+                    self._mapItemSum[item] = quantities[i]
         listOfffilist = []
         mapItemsToFFLIST = {}
-        self._minSup = self._convert(self._minSup)
+        #self._minSup = self._convert(self._minSup)
         # minSup = self.minSup
-        for item1 in self._mapItemsLowSum.keys():
+        for item1 in self._mapItemSum.keys():
             item = item1
-            low = self._mapItemsLowSum[item]
-            mid = self._mapItemsMidSum[item]
-            high = self._mapItemsHighSum[item]
-            if low >= mid and low >= high:
-                self._mapItemSum[item] = low
-                self._mapItemRegions[item] = "L"
-            elif mid >= low and mid >= high:
-                self._mapItemSum[item] = mid
-                self._mapItemRegions[item] = "M"
-            elif high >= low and high >= mid:
-                self._mapItemRegions[item] = "H"
-                self._mapItemSum[item] = high
             if self._mapItemSum[item] >= self._minSup:
                 fuList = _FFList(item)
                 mapItemsToFFLIST[item] = fuList
                 listOfffilist.append(fuList)
         listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
         tid = 0
         for line in range(len(self._transactions)):
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             revisedTransaction = []
             for i in range(0, len(items)):
                 pair = _Pair()
                 pair.item = items[i]
-                regions = self._Regions(float(quantities[i]), 3)
+                pair.quantity = quantities[i]
                 item = pair.item
                 if self._mapItemSum[item] >= self._minSup:
-                    if self._mapItemRegions[pair.item] == "L":
-                        pair.quantity = regions.low
-                    elif self._mapItemRegions[pair.item] == "M":
-                        pair.quantity = regions.middle
-                    elif self._mapItemRegions[pair.item] == "H":
-                        pair.quantity = regions.high
                     if pair.quantity > 0:
                         revisedTransaction.append(pair)
             revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
             for i in range(len(revisedTransaction) - 1, -1, -1):
                 pair = revisedTransaction[i]
                 remainUtil = 0
                 for j in range(len(revisedTransaction) - 1, i, -1):
                     remainUtil += revisedTransaction[j].quantity
                 remainingUtility = remainUtil
                 if mapItemsToFFLIST.get(pair.item) is not None:
                     FFListOfItem = mapItemsToFFLIST[pair.item]
                     element = _Element(tid, pair.quantity, remainingUtility)
                     FFListOfItem.addElement(element)
             tid += 1
-        self._FSFIMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
+        self._FFIMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def _FSFIMining(self, prefix: List[int], prefixLen: int, FSFIM: List[_FFList], minSup: float) -> None:
-        """Generates ffi from prefix
+    def _FFIMining(self, prefix, prefixLen, FSFIM, minSup):
+        """
+        Generates ffi from prefix
 
         :param prefix: the prefix patterns of ffi
         :type prefix: len
         :param prefixLen: the length of prefix
         :type prefixLen: int
         :param FSFIM: the Fuzzy list of prefix itemSets
         :type FSFIM: list
         :param minSup: the minimum support of
-        :type minSup:int
+        :type minSup: int or flaot
         """
         for i in range(0, len(FSFIM)):
             X = FSFIM[i]
             if X.sumIUtil >= minSup:
                 self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
             if X.sumRUtil >= minSup:
                 exULs = []
                 for j in range(i + 1, len(FSFIM)):
                     Y = FSFIM[j]
                     exULs.append(self._construct(X, Y))
                     self._joinsCnt += 1
                 self._itemSetBuffer.insert(prefixLen, X.item)
-                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup)
+                self._FFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup)
 
     def getMemoryUSS(self) -> float:
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
@@ -595,15 +570,15 @@
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime - self._startTime
 
-    def _construct(self, px: _FFList, py: _FFList) -> _FFList:
+    def _construct(self, px, py) -> _FFList:
         """
         A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
 
         :param px:the itemSet px
         :type px:ffi-List
         :param py:itemSet py
         :type py:ffi-List
@@ -615,15 +590,15 @@
             ey = self._findElementWithTID(py, ex.tid)
             if ey is None:
                 continue
             eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)), ey.rUtils)
             pxyUL.addElement(eXY)
         return pxyUL
 
-    def _findElementWithTID(self, uList: _FFList, tid: int) -> Union[_Element, None]:
+    def _findElementWithTID(self, uList, tid) -> _Element:
         """
         To find element with same tid as given
 
         :param uList: fuzzyList
         :type uList: ffi-List
         :param tid: transaction id
         :type tid: int
@@ -639,32 +614,33 @@
                 first = mid + 1
             elif List[mid].tid > tid:
                 last = mid - 1
             else:
                 return List[mid]
         return None
 
-    def _WriteOut(self, prefix: List[int], prefixLen: int, item: int, sumIUtil: float) -> None:
+    def _WriteOut(self, prefix: list, prefixLen: int, item: int, sumIUtil: float) -> None:
         """
         To Store the patten
 
         :param prefix: prefix of itemSet
         :type prefix: list
         :param prefixLen: length of prefix
         :type prefixLen: int
         :param item: the last item
         :type item: int
         :param sumIUtil: sum of utility of itemSet
         :type sumIUtil: float
+        :return: None
         """
         self._itemsCnt += 1
         res = ""
         for i in range(0, prefixLen):
-            res += str(prefix[i]) + "." + str(self._mapItemRegions[prefix[i]]) + "\t"
-        res += str(item) + "." + str(self._mapItemRegions.get(item))
+            res += str(prefix[i])  + "\t"
+        res += str(item)
         res1 = str(sumIUtil)
         self._finalPatterns[res] = res1
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
 
@@ -675,29 +651,31 @@
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
-    def getPatterns(self) -> Dict[str, str]:
+    def getPatterns(self) -> dict:
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def save(self, outFile: str) -> None:
+    def save(self, outFile) -> dict:
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: dictionary of frequent patterns
+        :rtype: dict
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
@@ -715,15 +693,24 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = FFIMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = FFIMiner(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
+        _ap = FFIMiner('sample.txt', 1, ' ')
+        _ap.startMine()
+        _ap.mine()
+        print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save('output.txt')
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/fuzzyFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py` & `pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,26 +1,25 @@
-# Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
-# which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
-# techniques to reduce the search space.
+# GPFPMiner is a Extension of ECLAT algorithm,which  stands for Equivalence Class Clustering and bottom-up
+# Lattice Traversal to mine the geo referenced peridoic frequent patterns.
 #
 # **Importing this algorithm into a python program**
-# ---------------------------------------------------------
-# .. code-block:: python
+# --------------------------------------------------------
 #
-#             from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
 #
-#             obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
+#             import PAMI.geoReferencedPeridicFrequentPattern.GPFPMiner as alg
 #
-#             obj.startMine()
+#             obj = alg.GPFPMiner("sampleTDB.txt", "sampleN.txt", 5, 3)
 #
-#             fuzzySpatialFrequentPatterns = obj.getPatterns()
+#             obj.mine()
 #
-#             print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
+#             Patterns = obj.getPatterns()
 #
-#             obj.save("outputFile")
+#             print("Total number of Geo Referenced Periodic-Frequent Patterns:", len(Patterns))
+#
+#             obj.save("outFile")
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -28,16 +27,18 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -47,195 +48,132 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from PAMI.fuzzyGeoreferencedFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
-
-
-class _FFList:
-    """
-    A class represent a Fuzzy List of an element
-
-    :Attributes:
+from  PAMI.geoReferencedPeriodicFrequentPattern.basic import abstract as _ab
+from deprecated import deprecated
 
-         item: int
-             the item name
-         sumIUtil: float
-             the sum of utilities of a fuzzy item in database
-         sumRUtil: float
-             the sum of resting values of a fuzzy item in database
-         elements: list
-             a list of elements contain tid,Utility and resting values of element in each transaction
-
-    :Methods:
 
-        addElement(element)
-            Method to add an element to this fuzzy list and update the sums at the same time.
-        printElement(e)
-            Method to print elements
+class GPFPMiner(_ab._geoReferencedPeriodicFrequentPatterns):
+    """ 
+    :Description:   GPFPMiner is a Extension of ECLAT algorithm,which  stands for Equivalence Class Clustering and bottom-up
+                    Lattice Traversal to mine the geo referenced peridoic frequent patterns.
+        
+    :Reference:
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Geo-referenced periodic frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Geo-referenced periodic frequent patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param nFile: str :
+                   Name of the input file to mine complete set of Geo-referenced periodic frequent patterns
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
-    """
-
-    def __init__(self, itemName: str) -> None:
-        self.item = itemName
-        self.sumIUtil = 0.0
-        self.sumRUtil = 0.0
-        self.elements = []
-
-    def addElement(self, element) -> None:
-        """
-        A Method that add a new element to FFList
-
-        :param element: an element to be added to FFList
-        :param element: Element
-        """
-        self.sumIUtil += element.iUtils
-        self.sumRUtil += element.rUtils
-        self.elements.append(element)
-
-    def printElement(self) -> None:
-        """
-        A Method to Print elements in the FFList
-        """
-        for ele in self.elements:
-            print(ele.tid, ele.iUtils, ele.rUtils)
-
-
-class _Element:
-    """
-    A class represents an Element of a fuzzy list
 
     :Attributes:
 
-        tid : int
-            keep tact of transaction id
-        iUtils: float
-            the utility of a fuzzy item in the transaction
-        rUtils : float
-            the neighbourhood resting value of a fuzzy item in the transaction
-    """
-
-    def __init__(self, tid: int, iUtil: float, rUtil: float) -> None:
-        self.tid = tid
-        self.iUtils = iUtil
-        self.rUtils = rUtil
-
+        iFile : str
+            Input file name or path of the input file
+        nFile: str:
+           Name of Neighbourhood file name
+        minSup: float or int or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        maxPer: float or int or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            However, the users can override their default separator.
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
+        finalPatterns: dict
+            Storing the complete set of patterns in a dictionary variable
+        oFile : str
+            Name of the output file to store complete set of frequent patterns
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
+        Database : list
+            To store the complete set of transactions available in the input database/file
 
-class _Pair:
-    """
-    A class to store item and it's quantity together
-    """
+    :Methods:
 
-    def __init__(self) -> None:
-        self.item = 0
-        self.quantity = 0
+            startMine()
+                Mining process will start from here
+            getPatterns()
+                Complete set of patterns will be retrieved with this function
+            save(oFile)
+                Complete set of frequent patterns will be loaded in to a output file
+            getPatternsAsDataFrames()
+                Complete set of frequent patterns will be loaded in to a dataframe
+            getMemoryUSS()
+                Total amount of USS memory consumed by the mining process will be retrieved from this function
+            getMemoryRSS()
+                Total amount of RSS memory consumed by the mining process will be retrieved from this function
+            getRuntime()
+                Total amount of runtime taken by the mining process will be retrieved from this function
+            creatingItemSets(iFileName)
+                Storing the complete transactions of the database/input file in a database variable
+            frequentOneItem()
+                Generating one frequent patterns
+            convert(value):
+                To convert the given user specified value    
+            getNeighbourItems(keySet):
+                A function to get common neighbours of a itemSet
+             mapNeighbours(file):
+                A function to map items to their neighbours
 
+    **Executing the code on terminal :**
+    ----------------------------------------
 
-class FFSPMiner(_ab._fuzzySpatialFrequentPatterns):
-    """
-    :Description:   Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
-                    which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
-                    techniques to reduce the search space.
-
-    :Reference:   Reference: P. Veena, B. S. Chithra, R. U. Kiran, S. Agarwal and K. Zettsu, "Discovering Fuzzy Frequent
-                  Spatial Patterns in Large Quantitative Spatiotemporal databases," 2021 IEEE International Conference on Fuzzy Systems
-                  (FUZZ-IEEE), 2021, pp. 1-8, doi: 10.1109/FUZZ45933.2021.9494594.
+    .. code-block:: console
 
-    :Attributes:
+      Format:
 
-        iFile : file
-            Name of the input file to mine complete set of fuzzy spatial frequent patterns
-        oFile : file
-               Name of the oFile file to store complete set of fuzzy spatial frequent patterns
-        minSup : float
-            The user given minimum support
-        neighbors: map
-            keep track of neighbours of elements
-        memoryRSS : float
-                To store the total amount of RSS memory consumed by the program
-        startTime:float
-               To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        itemsCnt: int
-            To record the number of fuzzy spatial itemSets generated
-        mapItemsLowSum: map
-            To keep track of low region values of items
-        mapItemsMidSum: map
-            To keep track of middle region values of items
-        mapItemsHighSum: map
-            To keep track of high region values of items
-        mapItemSum: map
-            To keep track of sum of Fuzzy Values of items
-        mapItemRegions: map
-            To Keep track of fuzzy regions of item
-        jointCnt: int
-            To keep track of the number of FFI-list that was constructed
-        BufferSize: int
-            represent the size of Buffer
-        itemBuffer list
-            to keep track of items in buffer
+      (.venv) $ python3 GPFPMiner.py <inputFile> <outputFile> <neighbourFile> <minSup> <maxPer>
 
-    :Methods:
+      Example Usage:
 
-        startMine()
-            Mining process will start from here
-        getPatterns()
-            Complete set of patterns will be retrieved with this function
-        save(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
-        getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
-        getMemoryUSS()
-            Total amount of USS memory consumed by the mining process will be retrieved from this function
-        getMemoryRSS()
-            Total amount of RSS memory consumed by the mining process will be retrieved from this function
-        getRuntime()
-            Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value):
-            To convert the given user specified value
-        FSFIMining( prefix, prefixLen, fsFim, minSup)
-            Method generate FFI from prefix
-        construct(px, py)
-            A function to construct Fuzzy itemSet from 2 fuzzy itemSets
-        Intersection(neighbourX,neighbourY)
-            Return common neighbours of 2 itemSet Neighbours
-        findElementWithTID(uList, tid)
-            To find element with same tid as given
-        WriteOut(prefix, prefixLen, item, sumIUtil,period)
-            To Store the patten
+      (.venv) $ python3 GPFPMiner.py sampleTDB.txt output.txt sampleN.txt 0.5 0.3
 
-    **Executing the code on terminal :**
-    --------------------------------------
-            Format:
-                    >>> python3 FFSPMiner.py <inputFile> <outputFile> <neighbours> <minSup> <sep>
+    .. note:: minSup & maxPer will be considered in percentage of database transactions
 
-            Examples:
-                    >>> python3  FFSPMiner.py sampleTDB.txt output.txt sampleN.txt 3
 
 
-    **Sample run of importing the code:**
-    ----------------------------------------
+    **Sample run of importing the code :**
+    -----------------------------------------
     .. code-block:: python
+    
+            import PAMI.geoReferencedPeridicFrequentPattern.GPFPMiner as alg
 
-            from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
-
-            obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
+            obj = alg.GPFPMiner("sampleTDB.txt", "sampleN.txt", 5, 3)
 
-            obj.startMine()
+            obj.mine()
 
-            fuzzySpatialFrequentPatterns = obj.getPatterns()
+            Patterns = obj.getPatterns()
 
-            print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
+            print("Total number of Geo Referenced Periodic-Frequent Patterns:", len(Patterns))
 
-            obj.save("outputFile")
+            obj.save("outFile")
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
@@ -243,428 +181,438 @@
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
     --------------
-            The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
+        The complete program was written by P.RaviKumar under the supervision of Professor Rage Uday Kiran.
     """
 
+    _minSup = " "
+    _maxPer = " "
     _startTime = float()
     _endTime = float()
-    _minSup = str()
-    _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _nFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
+    _Database = []
     _sep = "\t"
-    _transactions = []
-    _fuzzyValues = []
+    _lno = 0
 
-    def __init__(self, iFile: str, nFile: str, minSup: float, sep: str="\t") -> None:
-        super().__init__(iFile, nFile, minSup, sep)
-        self._mapItemNeighbours = {}
-        self._startTime = 0
-        self._endTime = 0
-        self._mapItemSum = {}
-        self._joinsCnt = 0
-        self._BufferSize = 200
-        self._itemSetBuffer = []
-        self._finalPatterns = {}
-        self._dbLen = 0
-        self._itemsCnt = 0
+    def __init__(self, iFile, nFile, minSup, maxPer, sep="\t"):
+        super().__init__(iFile, nFile, minSup, maxPer, sep)
+        self._NeighboursMap = {}
 
-    def _compareItems(self, o1, o2) -> int:
+    def _creatingItemSets(self):
         """
-        A Function that sort all FFI-list in ascending order of Support
+        Storing the complete transactions of the database/input file in a database variable
+        """
+        self._Database = []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
+            data, ts = [], []
+            if self._iFile.empty:
+                print("its empty..")
+            i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
+            if 'Transactions' in i:
+                data = self._iFile['Transactions'].tolist()
+            for i in range(len(data)):
+                tr = [ts[i][0]]
+                tr = tr + data[i]
+                self._Database.append(tr)
+        if isinstance(self._iFile, str):
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._Database.append(temp)
+            else:
+                try:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line = line.rstrip()
+                            temp = [i.strip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._Database.append(temp)
+                except IOError:
+                    print("File Not Found")
+                    quit()
+
+    # function to get frequent one pattern
+    def _frequentOneItem(self):
+        """
+        Generating one frequent patterns
         """
-        compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
-        if compare == 0:
-            return 0
-        else:
-            return compare
 
-    def _convert(self, value) -> float:
+        candidate = {}
+        for i in self._Database:
+            self._lno += 1
+            n = int(i[0])
+            for j in i[1:]:
+                if j not in candidate:
+                    candidate[j] = [1, abs(0-n), n, [n]]
+                else:
+                    candidate[j][0] += 1
+                    candidate[j][1] = max(candidate[j][1], abs(n - candidate[j][2]))
+                    candidate[j][2] = n
+                    candidate[j][3].append(n)
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        #print(self._minSup, self._maxPer)
+        self._tidList = {k: v[3] for k, v in candidate.items() if v[0] >= self._minSup and v[1] <= self._maxPer}
+        candidate = {k: [v[0], v[1]] for k, v in candidate.items() if v[0] >= self._minSup and v[1] <= self._maxPer}
+        plist = [key for key, value in sorted(candidate.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
+        return plist
+
+    def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
+
+        :type value: int or float or str
+
         :return: converted value
+
+        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (self._dbLen * value)
+            value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
-                value = (self._dbLen * value)
+                value = float(value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _creatingItemSets(self) -> None:
+    def _getSupportAndPeriod(self, timeStamps):
         """
-        Storing the complete transactions of the database/input file in a database variable
+        calculates the support and periodicity with list of timestamps
+
+        :param timeStamps: timestamps of a pattern
+        :type timeStamps: list
         """
-        self._transactions, self._fuzzyValues = [], []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            if self._iFile.empty:
-                print("its empty..")
-            i = self._iFile.columns.values.tolist()
-            if 'Transactions' in i:
-                self.transactions = self._iFile['Transactions'].tolist()
-            if 'fuzzyValues' in i:
-                self.fuzzyValues = self._iFile['Utilities'].tolist()
+        timeStamps.sort()
+        cur = 0
+        per = 0
+        sup = 0
+        for j in range(len(timeStamps)):
+            per = max(per, timeStamps[j] - cur)
+            if per > self._maxPer:
+                return [0, 0]
+            cur = timeStamps[j]
+            sup += 1
+        per = max(per, self._lno - cur)
+        return [sup, per]
 
-        if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
-                for line in data:
-                    line = line.decode("utf-8")
-                    line = line.split("\n")[0]
-                    parts = line.split(":")
-                    parts[0] = parts[0].strip()
-                    parts[1] = parts[1].strip()
-                    items = parts[0].split(self._sep)
-                    quantities = parts[1].split(self._sep)
-                    self._transactions.append([x for x in items])
-                    self._fuzzyValues.append([float(x) for x in quantities])
-            else:
-                try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line = line.split("\n")[0]
-                            parts = line.split(":")
-                            parts[0] = parts[0].strip()
-                            parts[1] = parts[1].strip()
-                            items = parts[0].split(self._sep)
-                            quantities = parts[1].split(self._sep)
-                            self._transactions.append([x for x in items])
-                            self._fuzzyValues.append([float(x) for x in quantities])
-                except IOError:
-                    print("File Not Found")
-                    quit()
+    def _save(self, prefix, suffix, tidSetX):
+        """
+        Saves the patterns that satisfy the periodic frequent property.
 
-    def _mapNeighbours(self) -> None:
-        self._mapItemNeighbours = {}
+        :param prefix: the prefix of a pattern
+        :type prefix: list or None
+        :param suffix: the suffix of a patterns
+        :type suffix: list
+        :param tidSetX: the timestamp of a patterns
+        :type tidSetX: list
+
+
+        """
+        if prefix == None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        val = self._getSupportAndPeriod(tidSetX)
+        if val[0] >= self._minSup and val[1] <= self._maxPer:
+            self._finalPatterns[tuple(prefix)] = val
+
+    def _Generation(self, prefix, itemSets, tidSets):
+        """
+        Generates the patterns that satisfy the periodic frequent property.
+
+        :param prefix: the prefix of a pattern
+        :type prefix: list or None
+        :param itemSets: the item sets of a patterns
+        :type itemSets: list
+        :param tidSets: the timestamp of a patterns
+        :type tidSets: list
+        """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
+            return
+        for i in range(len(itemSets)):
+            itemX = itemSets[i]
+            if itemX == None:
+                continue
+            tidSetX = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemX]
+            neighboursItemsI = self._getNeighbourItems(itemSets[i])
+            for j in range(i + 1, len(itemSets)):
+                neighboursItemsJ = self._getNeighbourItems(itemSets[i])
+                if not itemSets[j] in neighboursItemsI:
+                    continue
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = list(set(tidSetX).intersection(tidSetJ))
+                if len(y) >= self._minSup:
+                    ne = list(set(neighboursItemsI).intersection(neighboursItemsJ))
+                    x = []
+                    x = x + [itemX]
+                    x = x + [itemJ]
+                    self._NeighboursMap[tuple(x)] = ne
+                    classItemSets.append(itemJ)
+                    classTidSets.append(y)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetX)
+
+    def _getNeighbourItems(self, keySet):
+        """
+        A function to get Neighbours of a item
+
+        :param keySet:itemSet
+        :type keySet:str or tuple
+        :return: set of common neighbours
+        :rtype:set
+        """
+        itemNeighbours = self._NeighboursMap.keys()
+        if isinstance(keySet, str):
+            if self._NeighboursMap.get(keySet) is None:
+                return []
+            itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(keySet))))
+        if isinstance(keySet, tuple):
+            keySet = list(keySet)
+            for j in range(0, len(keySet)):
+                i = keySet[j]
+                itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(i))))
+        return itemNeighbours
+
+    def mapNeighbours(self):
+        """
+        A function to map items to their Neighbours
+        """
+        self._NeighboursMap = {}
         if isinstance(self._nFile, _ab._pd.DataFrame):
-            data, items = [], []
+            data = []
             if self._nFile.empty:
                 print("its empty..")
             i = self._nFile.columns.values.tolist()
-            if 'items' in i:
-                items = self._nFile['items'].tolist()
             if 'Neighbours' in i:
                 data = self._nFile['Neighbours'].tolist()
-            for k in range(len(items)):
-                self._mapItemNeighbours[items[k]] = data[k]
-
+            for i in data:
+                self._NeighboursMap[i[0]] = i[1:]
         if isinstance(self._nFile, str):
             if _ab._validators.url(self._nFile):
                 data = _ab._urlopen(self._nFile)
                 for line in data:
+                    line.strip()
                     line = line.decode("utf-8")
-                    line = line.split("\n")[0]
-                    parts = [i.rstrip() for i in line.split(self._sep)]
-                    parts = [x for x in parts]
-                    item = parts[0]
-                    neigh1 = []
-                    for i in range(1, len(parts)):
-                        neigh1.append(parts[i])
-                    self._mapItemNeighbours[item] = neigh1
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._NeighboursMap[temp[0]] = temp[1:]
             else:
                 try:
                     with open(self._nFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line = line.split("\n")[0]
-                            parts = [i.rstrip() for i in line.split(self._sep)]
-                            parts = [x for x in parts]
-                            item = parts[0]
-                            neigh1 = []
-                            for i in range(1, len(parts)):
-                                neigh1.append(parts[i])
-                            self._mapItemNeighbours[item] = neigh1
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._NeighboursMap[temp[0]] = temp[1:]
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def startMine(self) -> None:
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
+
+        # global items_sets, endTime, startTime
         self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
         self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self.mapNeighbours()
         self._finalPatterns = {}
-        self._mapNeighbours()
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            self._dbLen += 1
-            for i in range(0, len(items)):
-                item = items[i]
-                if item in self._mapItemSum:
-                    self._mapItemSum[item] += quantities[i]
-                else:
-                    self._mapItemSum[item] = quantities[i]
-        listOfFFList = []
-        mapItemsToFFLIST = {}
-        #self._minSup = self._convert(self._minSup)
-        for item1 in self._mapItemSum.keys():
-            item = item1
-            if self._mapItemSum[item] >= self._minSup:
-                fuList = _FFList(item)
-                mapItemsToFFLIST[item] = fuList
-                listOfFFList.append(fuList)
-        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        tid = 0
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            revisedTransaction = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                pair.quantity = quantities[i]
-                item = pair.item
-                if self._mapItemSum[item] >= self._minSup:
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i, -1):
-                    if self._mapItemNeighbours.get(pair.item[0]) is None:
-                        continue
-                    if revisedTransaction[j].item[0] in self._mapItemNeighbours[pair.item[0]]:
-                        remainUtil += revisedTransaction[j].quantity
-                remainingUtility = remainUtil
-                if mapItemsToFFLIST.get(pair.item) is not None:
-                    FFListOfItem = mapItemsToFFLIST[pair.item]
-                    element = _Element(tid, pair.quantity, remainingUtility)
-                    FFListOfItem.addElement(element)
-            tid += 1
-        itemNeighbours = list(self._mapItemNeighbours.keys())
-        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemX = plist[i]
+            tidSetX = self._tidList[itemX]
+            itemSetX = [itemX]
+            itemSets = []
+            tidSets = []
+            neighboursItems = self._getNeighbourItems(plist[i])
+            for j in range(i + 1, len(plist)):
+                if not plist[j] in neighboursItems:
+                    continue
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                if len(y1) >= self._minSup:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+            self._save(None, itemSetX, tidSetX)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
+        print("Spatial Periodic Frequent patterns were generated successfully using SpatialEclat algorithm")
 
-    def _FSFIMining(self, prefix: List, prefixLen: int, FSFIM: List, minSup: float, itemNeighbours: List):
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
         """
-        Generates FFSPMiner from prefix
 
-        :param prefix: the prefix patterns of FFSPMiner
-        :type prefix: len
-        :param prefixLen: the length of prefix
-        :type prefixLen: int
-        :param FSFIM: the Fuzzy list of prefix itemSets
-        :type FSFIM: list
-        :param minSup: the minimum support of
-        :type minSup:int
-        :param itemNeighbours: the set of common neighbours of prefix
-        :type itemNeighbours: list or set
-        """
-        for i in range(0, len(FSFIM)):
-            X = FSFIM[i]
-            if X.sumIUtil >= minSup:
-                self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
-            newNeighbours = self._Intersection(self._mapItemNeighbours.get(X.item[0]), itemNeighbours)
-            if X.sumRUtil >= minSup:
-                exULs = []
-                for j in range(i + 1, len(FSFIM)):
-                    Y = FSFIM[j]
-                    if Y.item[0] in newNeighbours:
-                        exULs.append(self._construct(X, Y))
-                        self._joinsCnt += 1
-                self._itemSetBuffer.insert(prefixLen, X.item)
-                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup, newNeighbours)
-
-    def _Intersection(self, neighbourX: List, neighbourY: List) -> List:
-        """
-        A function to get common neighbours from 2 itemSets
-
-        :param neighbourX: the set of neighbours of itemSet 1
-        :type neighbourX: set or list
-        :param neighbourY: the set of neighbours of itemSet 2
-        :type neighbourY: set or list
-        :return : set of common neighbours of 2 itemSets
-        :rtype :set
-        """
-        result = []
-        if neighbourX is None or neighbourY is None:
-            return result
-        for i in range(0, len(neighbourX)):
-            if neighbourX[i] in neighbourY:
-                result.append(neighbourX[i])
-        return result
+        # global items_sets, endTime, startTime
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self.mapNeighbours()
+        self._finalPatterns = {}
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemX = plist[i]
+            tidSetX = self._tidList[itemX]
+            itemSetX = [itemX]
+            itemSets = []
+            tidSets = []
+            neighboursItems = self._getNeighbourItems(plist[i])
+            for j in range(i + 1, len(plist)):
+                if not plist[j] in neighboursItems:
+                    continue
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                if len(y1) >= self._minSup:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+            self._save(None, itemSetX, tidSetX)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Spatial Periodic Frequent patterns were generated successfully using SpatialEclat algorithm")
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
-       """
+        """
+
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
 
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-        return self._endTime - self._startTime
 
-    def _construct(self, px: _FFList, py: _FFList) -> _FFList:
-        """
-        A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
-
-        :param px:the itemSet px
-        :type px:FFI-List
-        :param py:itemSet py
-        :type py:FFI-List
-        :return :the itemSet of pxy(px and py)
-        :rtype :FFI-List
-        """
-        pxyUL = _FFList(py.item)
-        for ex in px.elements:
-            ey = self._findElementWithTID(py, ex.tid)
-            if ey is None:
-                continue
-            eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)), ey.rUtils)
-            pxyUL.addElement(eXY)
-        return pxyUL
-
-    def _findElementWithTID(self, uList: _FFList, tid: int) -> _Element:
-        """
-        To find element with same tid as given
-
-        :param uList:fuzzyList
-        :type uList:FFI-List
-        :param tid:transaction id
-        :type tid:int
-        :return:element tid as given
-        :rtype: element if exist or None
-        """
-        List = uList.elements
-        first = 0
-        last = len(List) - 1
-        while first <= last:
-            mid = (first + last) >> 1
-            if List[mid].tid < tid:
-                first = mid + 1
-            elif List[mid].tid > tid:
-                last = mid - 1
-            else:
-                return List[mid]
-        return None
-
-    def _WriteOut(self, prefix: List, prefixLen: int, item: int, sumIUtil: float) -> None:
-        """
-        To Store the patten
-
-        :param prefix: prefix of itemSet
-        :type prefix: list
-        :param prefixLen: length of prefix
-        :type prefixLen: int
-        :param item: the last item
-        :type item: int
-        :param sumIUtil: sum of utility of itemSet
-        :type sumIUtil: float
-        """
-        self._itemsCnt += 1
-        res = ""
-        for i in range(0, prefixLen):
-            res += str(prefix[i]) + "\t"
-        res += str(item)
-        res1 = str(sumIUtil)
-        self._finalPatterns[res] = res1
+        return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+            pat = ""
+            for i in a:
+                pat += str(i) + "\t"
+            data.append([pat, b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Period'])
         return dataFrame
 
-    def getPatterns(self) -> Dict[str, str]:
+    def save(self, outFile):
         """
-        Function to send the set of frequent patterns after completion of the mining process
-
-        :return: returning frequent patterns
-        :rtype: dict
-        """
-        return self._finalPatterns
-
-    def save(self, outFile: str) -> None:
-        """
-        Complete set of frequent patterns will be loaded in to an output file
+        Complete set of frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
-        self.oFile = outFile
-        writer = open(self.oFile, 'w+')
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + " : " + str(y)
+            pat = ""
+            for i in x:
+                pat += str(i) + "\t"
+            patternsAndSupport = pat + ": " + str(y[0]) + ": " + str(y[1])
             writer.write("%s \n" % patternsAndSupport)
 
-    def printResults(self) -> None:
+    def getPatterns(self):
+        """
+        Function to send the set of frequent patterns after completion of the mining process
+
+        :return: returning frequent patterns
+        :rtype: dict
+        """
+        return self._finalPatterns
+    
+    def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Spatial Fuzzy Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Spatial Periodic-Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
+        if len(_ab._sys.argv) == 7:
+            _ap = GPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         if len(_ab._sys.argv) == 6:
-            _ap = FFSPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = FFSPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = GPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
-        print("Total number of Spatial Fuzzy Frequent  Patterns:", len(_ap.getPatterns()))
+        _ap.mine()
+        print("Total number of Spatial Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
-        _ap = FFSPMiner('sample.txt', 'nei.txt', 1, ' ')
-        _ap.startMine()
-        print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save('output.txt')
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

### Comparing `pami-2024.3.9.2/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py` & `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,20 @@
+# Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
+# which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
+# techniques to reduce the search space.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
-#     .. code-block:: python
+# ---------------------------------------------------------
+# .. code-block:: python
 #
 #             from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
 #
 #             obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             fuzzySpatialFrequentPatterns = obj.getPatterns()
 #
 #             print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
 #
 #             obj.save("outputFile")
 #
@@ -25,16 +28,18 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -45,58 +50,61 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.fuzzyGeoreferencedFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
 
     :Attributes:
 
-        item: int
-            the item name
-        sumIUtil: float
-            the sum of utilities of a fuzzy item in database
-        sumRUtil: float
-            the sum of resting values of a fuzzy item in database
-        elements: list
-            a list of elements contain tid,Utility and resting values of element in each transaction
+         item: int
+             the item name
+         sumIUtil: float
+             the sum of utilities of a fuzzy item in database
+         sumRUtil: float
+             the sum of resting values of a fuzzy item in database
+         elements: list
+             a list of elements contain tid,Utility and resting values of element in each transaction
 
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
         printElement(e)
-            Method to print elements            
+            Method to print elements
 
     """
 
-    def __init__(self, itemName):
+    def __init__(self, itemName: str) -> None:
         self.item = itemName
         self.sumIUtil = 0.0
         self.sumRUtil = 0.0
         self.elements = []
 
-    def addElement(self, element):
+    def addElement(self, element) -> None:
         """
         A Method that add a new element to FFList
 
         :param element: an element to be added to FFList
         :param element: Element
+        :return: None
         """
         self.sumIUtil += element.iUtils
         self.sumRUtil += element.rUtils
         self.elements.append(element)
 
-    def printElement(self):
+    def printElement(self) -> None:
         """
         A Method to Print elements in the FFList
         """
         for ele in self.elements:
             print(ele.tid, ele.iUtils, ele.rUtils)
 
 
@@ -110,91 +118,68 @@
             keep tact of transaction id
         iUtils: float
             the utility of a fuzzy item in the transaction
         rUtils : float
             the neighbourhood resting value of a fuzzy item in the transaction
     """
 
-    def __init__(self, tid, iUtil, rUtil):
+    def __init__(self, tid: int, iUtil: float, rUtil: float) -> None:
         self.tid = tid
         self.iUtils = iUtil
         self.rUtils = rUtil
 
 
-class _Regions:
-    """
-    A class calculate the regions
-
-    :Attributes:
-
-        low : int
-            low region value
-        middle: int
-            middle region value
-        high : int
-            high region values
-    """
-
-    def __init__(self, quantity, regionsNumber):
-        self.low = 0
-        self.middle = 0
-        self.high = 0
-        if regionsNumber == 3:  # if we have 3 regions
-            if 0 < quantity <= 1:
-                self.low = 1
-                self.high = 0
-                self.middle = 0
-            elif 1 < quantity <= 6:
-                self.low = float((6 - quantity) / 5)
-                self.middle = float((quantity - 1) / 5)
-                self.high = 0
-            elif 6 < quantity <= 11:
-                self.low = 0
-                self.middle = float((11 - quantity) / 5)
-                self.high = float((quantity - 6) / 5)
-            else:
-                self.low = 0
-                self.middle = 0
-                self.high = 1
-
-
 class _Pair:
     """
     A class to store item and it's quantity together
     """
 
-    def __init__(self):
+    def __init__(self) -> None:
         self.item = 0
         self.quantity = 0
 
 
 class FFSPMiner(_ab._fuzzySpatialFrequentPatterns):
     """
     :Description:   Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
                     which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
                     techniques to reduce the search space.
 
-    Reference:   Reference: P. Veena, B. S. Chithra, R. U. Kiran, S. Agarwal and K. Zettsu, "Discovering Fuzzy Frequent
-                 Spatial Patterns in Large Quantitative Spatiotemporal databases," 2021 IEEE International Conference on Fuzzy Systems
-                 (FUZZ-IEEE), 2021, pp. 1-8, doi: 10.1109/FUZZ45933.2021.9494594.
+    :Reference:   Reference: P. Veena, B. S. Chithra, R. U. Kiran, S. Agarwal and K. Zettsu, "Discovering Fuzzy Frequent
+                  Spatial Patterns in Large Quantitative Spatiotemporal databases," 2021 IEEE International Conference on Fuzzy Systems
+                  (FUZZ-IEEE), 2021, pp. 1-8, doi: 10.1109/FUZZ45933.2021.9494594.
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param nFile: str :
+                   Name of the input file to mine complete set of frequent patterns
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
 
     :Attributes:
 
         iFile : file
             Name of the input file to mine complete set of fuzzy spatial frequent patterns
         oFile : file
-            Name of the oFile file to store complete set of fuzzy spatial frequent patterns
+               Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : float
             The user given minimum support
         neighbors: map
             keep track of neighbours of elements
         memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+                To store the total amount of RSS memory consumed by the program
         startTime:float
-            To record the start time of the mining process
+               To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         itemsCnt: int
             To record the number of fuzzy spatial itemSets generated
         mapItemsLowSum: map
             To keep track of low region values of items
         mapItemsMidSum: map
@@ -223,50 +208,52 @@
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
-            Total amount of runtime taken by the mining process will be retrieved from this function            
+            Total amount of runtime taken by the mining process will be retrieved from this function
         convert(value):
             To convert the given user specified value
         FSFIMining( prefix, prefixLen, fsFim, minSup)
             Method generate FFI from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
         Intersection(neighbourX,neighbourY)
             Return common neighbours of 2 itemSet Neighbours
         findElementWithTID(uList, tid)
             To find element with same tid as given
         WriteOut(prefix, prefixLen, item, sumIUtil,period)
             To Store the patten
-    
+
     **Executing the code on terminal :**
-    ---------------------------------------
+    ----------------------------------------
 
-            Format:
-                    >>> python3 FFSPMiner_old.py <inputFile> <outputFile> <neighbours> <minSup> <sep>
+    .. code-block:: console
 
-            Examples:
-                    >>> python3  FFSPMiner_old.py sampleTDB.txt output.txt sampleN.txt 3
+      Format:
 
-                    >>> python3  FFSPMiner_old.py sampleTDB.txt output.txt sampleN.txt 0.3
+      (.venv) $ python3 FFSPMiner.py <inputFile> <outputFile> <neighbours> <minSup> <sep>
 
-                    >>> python3  FFSPMiner_old.py sampleTDB.txt output.txt sampleN.txt 3
+      Example Usage:
 
+      (.venv) $ python3  FFSPMiner.py sampleTDB.txt output.txt sampleN.txt 3
+
+    .. note:: minSup will be considered in percentage of database transactions
 
     **Sample run of importing the code:**
     ----------------------------------------
-        
+    .. code-block:: python
+
             from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
 
             obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
 
-            obj.startMine()
+            obj.mine()
 
             fuzzySpatialFrequentPatterns = obj.getPatterns()
 
             print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
 
             obj.save("outputFile")
 
@@ -279,80 +266,91 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    ---------------
+    --------------
             The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
     """
-    
+
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _nFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _sep = "\t"
     _transactions = []
     _fuzzyValues = []
 
-    def __init__(self, iFile, nFile, minSup, sep="\t"):
+    def __init__(self, iFile: str, nFile: str, minSup: float, sep: str="\t") -> None:
         super().__init__(iFile, nFile, minSup, sep)
         self._mapItemNeighbours = {}
         self._startTime = 0
         self._endTime = 0
-        self._itemsCnt = 0
-        self._mapItemsLowSum = {}
-        self._mapItemsMidSum = {}
-        self._mapItemsHighSum = {}
         self._mapItemSum = {}
-        self._mapItemRegions = {}
         self._joinsCnt = 0
         self._BufferSize = 200
         self._itemSetBuffer = []
         self._finalPatterns = {}
         self._dbLen = 0
+        self._itemsCnt = 0
 
-    def _compareItems(self, o1, o2):
+    def _compareItems(self, o1, o2) -> int:
         """
-        A Function that sort all FFI-list in ascending order of Support
+        A Function that sort all ffi-list in ascending order of Support
+
+        :param o1: First FFI-list
+
+        :type o1: _FFList
+
+        :param o2: Second FFI-list
+
+        :type o1: _FFList
+
+        :return: Comparision Value
+
+        :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
-            return int(o1.item) - int(o2.item)
+            return 0
         else:
             return compare
 
-    def _convert(self, value):
+    def _convert(self, value) -> float:
         """
         To convert the given user specified value
 
         :param value: user specified value
+        :type value: int or float or str
         :return: converted value
+        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
             if '.' in value:
                 value = (self._dbLen * value)
             else:
                 value = int(value)
         return value
 
-    def _creatingItemSets(self):
+    def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._transactions, self._fuzzyValues = [], []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
@@ -364,36 +362,36 @@
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line = line.decode("utf-8")
                     line = line.split("\n")[0]
                     parts = line.split(":")
                     parts[0] = parts[0].strip()
-                    parts[2] = parts[2].strip()
+                    parts[1] = parts[1].strip()
                     items = parts[0].split(self._sep)
-                    quantities = parts[2].split(self._sep)
-                    self.transactions.append([x for x in items])
-                    self.fuzzyValues.append([x for x in quantities])
+                    quantities = parts[1].split(self._sep)
+                    self._transactions.append([x for x in items])
+                    self._fuzzyValues.append([float(x) for x in quantities])
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line = line.split("\n")[0]
                             parts = line.split(":")
                             parts[0] = parts[0].strip()
-                            parts[2] = parts[2].strip()
+                            parts[1] = parts[1].strip()
                             items = parts[0].split(self._sep)
-                            quantities = parts[2].split(self._sep)
+                            quantities = parts[1].split(self._sep)
                             self._transactions.append([x for x in items])
-                            self._fuzzyValues.append([x for x in quantities])
+                            self._fuzzyValues.append([float(x) for x in quantities])
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _mapNeighbours(self):
+    def _mapNeighbours(self) -> None:
         self._mapItemNeighbours = {}
         if isinstance(self._nFile, _ab._pd.DataFrame):
             data, items = [], []
             if self._nFile.empty:
                 print("its empty..")
             i = self._nFile.columns.values.tolist()
             if 'items' in i:
@@ -428,96 +426,131 @@
                             for i in range(1, len(parts)):
                                 neigh1.append(parts[i])
                             self._mapItemNeighbours[item] = neigh1
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def startMine(self):
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self) -> None:
         """
         Frequent pattern mining process will start from here
+        :return: None
         """
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._finalPatterns = {}
         self._mapNeighbours()
         for line in range(len(self._transactions)):
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             self._dbLen += 1
             for i in range(0, len(items)):
-                regions = _Regions(int(quantities[i]), 3)
                 item = items[i]
-                if item in self._mapItemsLowSum.keys():
-                    low = self._mapItemsLowSum[item]
-                    low += regions.low
-                    self._mapItemsLowSum[item] = low
-                else:
-                    self._mapItemsLowSum[item] = regions.low
-                if item in self._mapItemsMidSum.keys():
-                    mid = self._mapItemsMidSum[item]
-                    mid += regions.middle
-                    self._mapItemsMidSum[item] = mid
+                if item in self._mapItemSum:
+                    self._mapItemSum[item] += quantities[i]
                 else:
-                    self._mapItemsMidSum[item] = regions.middle
-                if item in self._mapItemsHighSum.keys():
-                    high = self._mapItemsHighSum[item]
-                    high += regions.high
-                    self._mapItemsHighSum[item] = high
+                    self._mapItemSum[item] = quantities[i]
+        listOfFFList = []
+        mapItemsToFFLIST = {}
+        #self._minSup = self._convert(self._minSup)
+        for item1 in self._mapItemSum.keys():
+            item = item1
+            if self._mapItemSum[item] >= self._minSup:
+                fuList = _FFList(item)
+                mapItemsToFFLIST[item] = fuList
+                listOfFFList.append(fuList)
+        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                pair.quantity = quantities[i]
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                remainUtil = 0
+                for j in range(len(revisedTransaction) - 1, i, -1):
+                    if self._mapItemNeighbours.get(pair.item[0]) is None:
+                        continue
+                    if revisedTransaction[j].item[0] in self._mapItemNeighbours[pair.item[0]]:
+                        remainUtil += revisedTransaction[j].quantity
+                remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(pair.item) is not None:
+                    FFListOfItem = mapItemsToFFLIST[pair.item]
+                    element = _Element(tid, pair.quantity, remainingUtility)
+                    FFListOfItem.addElement(element)
+            tid += 1
+        itemNeighbours = list(self._mapItemNeighbours.keys())
+        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+
+    def mine(self) -> None:
+        """
+        Frequent pattern mining process will start from here
+        :return: None
+        """
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        self._mapNeighbours()
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            self._dbLen += 1
+            for i in range(0, len(items)):
+                item = items[i]
+                if item in self._mapItemSum:
+                    self._mapItemSum[item] += quantities[i]
                 else:
-                    self._mapItemsHighSum[item] = regions.high
+                    self._mapItemSum[item] = quantities[i]
         listOfFFList = []
         mapItemsToFFLIST = {}
-        self._minSup = self._convert(self._minSup)
-        for item1 in self._mapItemsLowSum.keys():
+        #self._minSup = self._convert(self._minSup)
+        for item1 in self._mapItemSum.keys():
             item = item1
-            low = self._mapItemsLowSum[item]
-            mid = self._mapItemsMidSum[item]
-            high = self._mapItemsHighSum[item]
-            if low >= mid and low >= high:
-                self._mapItemSum[item] = low
-                self._mapItemRegions[item] = "L"
-            elif mid >= low and mid >= high:
-                self._mapItemSum[item] = mid
-                self._mapItemRegions[item] = "M"
-            elif high >= low and high >= mid:
-                self._mapItemRegions[item] = "H"
-                self._mapItemSum[item] = high
             if self._mapItemSum[item] >= self._minSup:
                 fuList = _FFList(item)
                 mapItemsToFFLIST[item] = fuList
                 listOfFFList.append(fuList)
         listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
         tid = 0
         for line in range(len(self._transactions)):
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             revisedTransaction = []
             for i in range(0, len(items)):
                 pair = _Pair()
                 pair.item = items[i]
-                regions = _Regions(int(quantities[i]), 3)
+                pair.quantity = quantities[i]
                 item = pair.item
                 if self._mapItemSum[item] >= self._minSup:
-                    if self._mapItemRegions[pair.item] == "L":
-                        pair.quantity = regions.low
-                    elif self._mapItemRegions[pair.item] == "M":
-                        pair.quantity = regions.middle
-                    elif self._mapItemRegions[pair.item] == "H":
-                        pair.quantity = regions.high
                     if pair.quantity > 0:
                         revisedTransaction.append(pair)
             revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
             for i in range(len(revisedTransaction) - 1, -1, -1):
                 pair = revisedTransaction[i]
                 remainUtil = 0
                 for j in range(len(revisedTransaction) - 1, i, -1):
-                    if self._mapItemNeighbours.get(pair.item) is None:
+                    if self._mapItemNeighbours.get(pair.item[0]) is None:
                         continue
-                    if revisedTransaction[j].item in self._mapItemNeighbours[pair.item]:
+                    if revisedTransaction[j].item[0] in self._mapItemNeighbours[pair.item[0]]:
                         remainUtil += revisedTransaction[j].quantity
                 remainingUtility = remainUtil
                 if mapItemsToFFLIST.get(pair.item) is not None:
                     FFListOfItem = mapItemsToFFLIST[pair.item]
                     element = _Element(tid, pair.quantity, remainingUtility)
                     FFListOfItem.addElement(element)
             tid += 1
@@ -526,15 +559,15 @@
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def _FSFIMining(self, prefix, prefixLen, FSFIM, minSup, itemNeighbours):
+    def _FSFIMining(self, prefix: List, prefixLen: int, FSFIM: List, minSup: float, itemNeighbours: List):
         """
         Generates FFSPMiner from prefix
 
         :param prefix: the prefix patterns of FFSPMiner
         :type prefix: len
         :param prefixLen: the length of prefix
         :type prefixLen: int
@@ -545,26 +578,26 @@
         :param itemNeighbours: the set of common neighbours of prefix
         :type itemNeighbours: list or set
         """
         for i in range(0, len(FSFIM)):
             X = FSFIM[i]
             if X.sumIUtil >= minSup:
                 self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
-            newNeighbours = self._Intersection(self._mapItemNeighbours.get(X.item), itemNeighbours)
+            newNeighbours = self._Intersection(self._mapItemNeighbours.get(X.item[0]), itemNeighbours)
             if X.sumRUtil >= minSup:
                 exULs = []
                 for j in range(i + 1, len(FSFIM)):
                     Y = FSFIM[j]
-                    if Y.item in newNeighbours:
+                    if Y.item[0] in newNeighbours:
                         exULs.append(self._construct(X, Y))
                         self._joinsCnt += 1
                 self._itemSetBuffer.insert(prefixLen, X.item)
                 self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup, newNeighbours)
 
-    def _Intersection(self, neighbourX, neighbourY):
+    def _Intersection(self, neighbourX: List, neighbourY: List) -> List:
         """
         A function to get common neighbours from 2 itemSets
 
         :param neighbourX: the set of neighbours of itemSet 1
         :type neighbourX: set or list
         :param neighbourY: the set of neighbours of itemSet 2
         :type neighbourY: set or list
@@ -575,44 +608,44 @@
         if neighbourX is None or neighbourY is None:
             return result
         for i in range(0, len(neighbourX)):
             if neighbourX[i] in neighbourY:
                 result.append(neighbourX[i])
         return result
 
-    def getMemoryUSS(self):
+    def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
         return self._memoryRSS
 
-    def getRuntime(self):
+    def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
 
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime - self._startTime
 
-    def _construct(self, px, py):
+    def _construct(self, px: _FFList, py: _FFList) -> _FFList:
         """
         A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
 
         :param px:the itemSet px
         :type px:FFI-List
         :param py:itemSet py
         :type py:FFI-List
@@ -624,15 +657,15 @@
             ey = self._findElementWithTID(py, ex.tid)
             if ey is None:
                 continue
             eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)), ey.rUtils)
             pxyUL.addElement(eXY)
         return pxyUL
 
-    def _findElementWithTID(self, uList, tid):
+    def _findElementWithTID(self, uList: _FFList, tid: int) -> _Element:
         """
         To find element with same tid as given
 
         :param uList:fuzzyList
         :type uList:FFI-List
         :param tid:transaction id
         :type tid:int
@@ -648,73 +681,75 @@
                 first = mid + 1
             elif List[mid].tid > tid:
                 last = mid - 1
             else:
                 return List[mid]
         return None
 
-    def _WriteOut(self, prefix, prefixLen, item, sumIUtil):
+    def _WriteOut(self, prefix: List, prefixLen: int, item: int, sumIUtil: float) -> None:
         """
         To Store the patten
 
         :param prefix: prefix of itemSet
         :type prefix: list
         :param prefixLen: length of prefix
         :type prefixLen: int
         :param item: the last item
         :type item: int
         :param sumIUtil: sum of utility of itemSet
         :type sumIUtil: float
+        :return: None
         """
         self._itemsCnt += 1
         res = ""
         for i in range(0, prefixLen):
-            res += str(prefix[i]) + "." + str(self._mapItemRegions[prefix[i]]) + "\t"
-        res += str(item) + "." + str(self._mapItemRegions.get(item))
+            res += str(prefix[i]) + "\t"
+        res += str(item)
         res1 = str(sumIUtil)
         self._finalPatterns[res] = res1
 
-    def getPatternsAsDataFrame(self):
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
-    def getPatterns(self):
+    def getPatterns(self) -> Dict[str, str]:
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + " : " + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
-    def printResults(self):
+    def printResults(self) -> None:
         """
         This function is used to print the results
         """
         print("Total number of Spatial Fuzzy Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
@@ -724,14 +759,23 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = FFSPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = FFSPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Spatial Fuzzy Frequent  Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS",  _ap.getMemoryRSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
+        _ap = FFSPMiner('sample.txt', 'nei.txt', 1, ' ')
+        _ap.startMine()
+        _ap.mine()
+        print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save('output.txt')
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py` & `pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,154 +1,201 @@
-# Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
-# which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
+# Fuzzy Periodic Frequent Pattern Miner is desired to find all fuzzy periodic frequent patterns which is
+# on-trivial and challenging problem to its huge search space.we are using efficient pruning
 # techniques to reduce the search space.
 #
-# **Importing this algorithm into a python program**
-# --------------------------------------------------------
+# Sample run of importing the code:
+# ----------------------------------------
 #
-#     from PAMI.fuzzyGeoreferencedPeriodicFrequentPattern import FGPFPMiner as alg
+#             from PAMI.fuzzyPeriodicFrequentPattern.basic import FPFPMiner as alg
 #
-#     obj = alg.FFSPMiner("input.txt", "neighbours.txt", 3, 4)
+#             obj =alg.FPFPMiner("input.txt",2,3)
 #
-#     obj.startMine()
+#             obj.mine()
 #
-#     print("Total number of fuzzy frequent spatial patterns:", len(obj.getPatterns()))
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#     obj.save("outputFile")
+#             print("Total number of Fuzzy Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#     print("Total Memory in USS:", obj.getMemoryUSS())
+#             obj.save("output.txt")
 #
-#     print("Total Memory in RSS", obj.getMemoryRSS())
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total ExecutionTime in seconds:", obj.getRuntime())
+#             print("Total Memory in USS:", memUSS)
+#
+#             memRSS = obj.getMemoryRSS()
+#
+#             print("Total Memory in RSS", memRSS)
+#
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
 #
 
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
 
+     This program is free software: you can redistribute it and/or modify
+     it under the terms of the GNU General Public License as published by
+     the Free Software Foundation, either version 3 of the License, or
+     (at your option) any later version.
+
+     This program is distributed in the hope that it will be useful,
+     but WITHOUT ANY WARRANTY; without even the implied warranty of
+     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+     GNU General Public License for more details.
+
+     You should have received a copy of the GNU General Public License
+     along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
-import PAMI.fuzzyGeoreferencedPeriodicFrequentPattern.basic.abstract as _ab
+from PAMI.fuzzyPeriodicFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
     :Attributes:
 
-         item: int
-             the item name
-         sumIUtil: float
-             the sum of utilities of a fuzzy item in database
-         sumRUtil: float
-             the sum of resting values of a fuzzy item in database
-         elements: list
-             a list of elements contain tid,Utility and resting values of element in each transaction
+        item: int
+            the item name
+        sumLUtil: float
+            the sum of utilities of a fuzzy item in database
+        sumRUtil: float
+            the sum of resting values of a fuzzy item in database
+        elements: list
+            list of elements contain tid,Utility and resting values of element in each transaction
+        maxPeriod: int
+            it represents the max period of a item
 
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
         printElement(e)
             Method to print elements
+
     """
 
-    def __init__(self, itemName):
+    def __init__(self, itemName: str) -> None:
         self.item = itemName
-        self.isPeriodic = False
-        self.sumIUtil = 0.0
+        self.sumLUtil = 0.0
         self.sumRUtil = 0.0
         self.elements = []
+        self.maxPeriod = 0
 
-    def addElement(self, element):
+    def addElement(self, element) -> None:
         """
         A Method that add a new element to FFList
 
         :param element: an element to be added to FFList
-        :param element: Element
+        :type element: Element
+        :return: None
         """
-        self.sumIUtil += element.iUtils
+        self.sumLUtil += element.lUtils
         self.sumRUtil += element.rUtils
         self.elements.append(element)
+        self.maxPeriod = max(self.maxPeriod, element.period)
 
-    def printElement(self):
+    def printElement(self) -> None:
         """
-        A Method to Print elements in the FFList object
+        A Method to Print elements in the FFList
+        :return: None
         """
         for ele in self.elements:
-            print(ele.tid, ele.iUtils, ele.rUtils)
+            print(ele.tid, ele.lUtils, ele.rUtils, ele.period)
 
 
 class _Element:
     """
-    A class represents an Element of a fuzzy list
+        A class represents an Element of a fuzzy list
 
-    :Attributes:
+        :Attributes:
 
         tid : int
             keep tact of transaction id
-        iUtils: float
+        lUtils: float
             the utility of a fuzzy item in the transaction
         rUtils : float
-            the neighbourhood resting value of a fuzzy item in the transaction
+            the resting value of a fuzzy item in the transaction
+        period: int
+            represent the period of the element
     """
 
-    def __init__(self, tid, iUtil, rUtil):
+    def __init__(self, tid: int, iUtil: float, rUtil: float, period: int) -> None:
         self.tid = tid
-        self.iUtils = iUtil
+        self.lUtils = iUtil
         self.rUtils = rUtil
+        self.period = period
 
 
 class _Pair:
     """
-    A class to store item and it's quantity together
+    A class to store item name and quantity together.
     """
 
-    def __init__(self):
+    def __init__(self) -> None:
         self.item = 0
         self.quantity = 0
 
 
-class FGPFPMiner(_ab._fuzzySpatialFrequentPatterns):
+class FPFPMiner(_ab._fuzzyPeriodicFrequentPatterns):
     """
-    :Description:   Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
-                    which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
+    :Description:   Fuzzy Periodic Frequent Pattern Miner is desired to find all fuzzy periodic frequent patterns which is
+                    on-trivial and challenging problem to its huge search space.we are using efficient pruning
                     techniques to reduce the search space.
-         
-    :Reference:
-    
+
+    :Reference:   R. U. Kiran et al., "Discovering Fuzzy Periodic-Frequent Patterns in Quantitative Temporal Databases,"
+                  2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), Glasgow, UK, 2020, pp.
+                  1-8, doi: 10.1109/FUZZ48607.2020.9177579.
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
 
         iFile : file
             Name of the input file to mine complete set of fuzzy spatial frequent patterns
         oFile : file
-            Name of the oFile file to store complete set of fuzzy spatial frequent patterns
+               Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : float
-            The user given minimum support
-        neighbors: map
-            keep track of neighbours of elements
+            The user given support
+        period: int
+            periodicity of an element
         memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+                To store the total amount of RSS memory consumed by the program
         startTime:float
-            To record the start time of the mining process
+               To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         itemsCnt: int
             To record the number of fuzzy spatial itemSets generated
         mapItemsLowSum: map
             To keep track of low region values of items
         mapItemsMidSum: map
@@ -161,14 +208,20 @@
             To Keep track of fuzzy regions of item
         jointCnt: int
             To keep track of the number of FFI-list that was constructed
         BufferSize: int
             represent the size of Buffer
         itemBuffer list
             to keep track of items in buffer
+        maxTID: int
+            represent the maximum tid of the database
+        lastTIDs: map
+            represent the last tid of fuzzy items
+        itemsToRegion: map
+            represent items with respective regions
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
@@ -184,529 +237,544 @@
             Total amount of runtime taken by the mining process will be retrieved from this function
         convert(value):
             To convert the given user specified value
         FSFIMining( prefix, prefixLen, fsFim, minSup)
             Method generate FFI from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
-        Intersection(neighbourX,neighbourY)
-            Return common neighbours of 2 itemSet Neighbours
-        findElementWithTID(uList, tid)
+        findElementWithTID(UList, tid)
             To find element with same tid as given
         WriteOut(prefix, prefixLen, item, sumIUtil,period)
             To Store the patten
 
     **Executing the code on terminal :**
-    --------------------------------------------
-            Format:
-                    >>> python3 FGPFPMiner.py <inputFile> <outputFile> <neighbours> <minSup> <maxPer> <sep>
-
-            Examples:
-                    >>> python3  FGPFPMiner.py sampleTDB.txt output.txt sampleN.txt 3 4
-           
-    **Sample run of importing the code:**
     ----------------------------------------
-    .. code-block:: python
-    
-        from PAMI.fuzzyGeoreferencedPeriodicFrequentPattern import FGPFPMiner as alg
-        
-        obj = alg.FFSPMiner("input.txt", "neighbours.txt", 3, 4)
-        
-        obj.startMine()
-        
-        print("Total number of fuzzy frequent spatial patterns:", len(obj.getPatterns()))
-        
-        obj.save("outputFile")
-        
-        print("Total Memory in USS:", obj.getMemoryUSS())
-        
-        print("Total Memory in RSS", obj.getMemoryRSS())
-        
-        print("Total ExecutionTime in seconds:", obj.getRuntime())
-    
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 FPFPMiner.py <inputFile> <outputFile> <minSup> <maxPer> <sep>
+
+      Example Usage:
+
+      (.venv) $ python3  FPFPMiner.py sampleTDB.txt output.txt 2 3
+
+    .. note:: minSup will be considered in percentage of database transactions
+
+
+    **Sample run of importing the code:**
+    --------------------------------------
+
+        from PAMI.fuzzyPeriodicFrequentPattern.basic import FPFPMiner as alg
+
+        obj =alg.FPFPMiner("input.txt",2,3)
+
+        obj.mine()
+
+        periodicFrequentPatterns = obj.getPatterns()
+
+        print("Total number of Fuzzy Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+
+        obj.save("output.txt")
+
+        memUSS = obj.getMemoryUSS()
+
+        print("Total Memory in USS:", memUSS)
+
+        memRSS = obj.getMemoryRSS()
+
+        print("Total Memory in RSS", memRSS)
+
+        run = obj.getRuntime()
+
+        print("Total ExecutionTime in seconds:", run)
+
     **Credits:**
-    ----------------
-            The complete program was written by B.Sai Chitra and Kundai Kwangwari under the supervision of Professor Rage Uday Kiran.
-    """
+    --------------
+            The complete program was written by Sai Chitra.B under the supervision of Professor Rage Uday Kiran.
 
+    """
     _startTime = float()
     _endTime = float()
-    _minSup = str()
+    _minSup = float()
     _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
-    _nFile = " "
-    _FuzFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _sep = "\t"
-    _transactionsDB = []
-    _fuzzyValuesDB = []
+    _sep = " "
+    _Database = []
+    _transactions = []
+    _fuzzyValues = []
     _ts = []
 
-    def __init__(self, iFile, nFile, minSup, maxPer, sep):
-        super().__init__(iFile, nFile, minSup, maxPer, sep)
-        self._mapItemNeighbours = {}
-        self._startTime = 0
-        self._endTime = 0
-        self._itemsCnt = 0
-        self._itemSupData = {}
-        self._mapItemSum = {}
-        self._joinsCnt = 0
+    def __init__(self, iFile: Union[str, _ab._pd.DataFrame], minSup: Union[int, float], period: Union[int, float], sep: str="\t") -> None:
+        super().__init__(iFile, minSup, period, sep)
+        self._oFile = ""
         self._BufferSize = 200
         self._itemSetBuffer = []
+        self._mapItemSum = {}
         self._finalPatterns = {}
-        self._finalPeriodicPatterns = {}
-        self._tidList = {}
+        self._joinsCnt = 0
+        self._itemsCnt = 0
+        self._startTime = float()
+        self._endTime = float()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
         self._dbLen = 0
 
-    def _compareItems(self, o1, o2):
+    def _compareItems(self, o1, o2) -> int:
         """
         A Function that sort all FFI-list in ascending order of Support
+
+        :param o1: First FFI-list
+
+        :type o1: _FFList
+
+        :param o2: Second FFI-list
+
+        :type o1: _FFList
+
+        :return: Comparision Value
+
+        :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
             return int(o1.item) - int(o2.item)
         else:
             return compare
 
-    def _convert(self, value):
+    def _convert(self, value) -> float:
         """
         To convert the given user specified value
 
         :param value: user specified value
+
+        :type value: int or float or str
+
         :return: converted value
+
+        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
             if '.' in value:
-                value = float(value)
+                value = (self._dbLen * value)
             else:
                 value = int(value)
         return value
 
-    def _creatingItemSets(self):
+    def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+
+        :return: None
         """
-        self._transactionsDB, self._fuzzyValuesDB, self._ts = [], [], []
+        data, self._transactions, self._fuzzyValues, ts = [], [], [], []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                self._ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._transactionsDB = self._iFile['Transactions'].tolist()
+                self._transactions = self._iFile['Transactions'].tolist()
             if 'fuzzyValues' in i:
-                self._fuzzyValuesDB = self._iFile['fuzzyValues'].tolist()
-
+                self._fuzzyValues = self._iFile['fuzzyValues'].tolist()
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
+                count = 0
                 for line in data:
                     line = line.decode("utf-8")
                     line = line.split("\n")[0]
                     parts = line.split(":")
+                    parts[0] = parts[0].strip()
+                    parts[1] = parts[1].strip()
                     items = parts[0].split(self._sep)
                     quantities = parts[1].split(self._sep)
                     self._ts.append(int(items[0]))
-                    self._transactionsDB.append([x for x in items[1:]])
-                    self._fuzzyValuesDB.append([float(x) for x in quantities])
+                    self._transactions.append([x for x in items[1:]])
+                    self._fuzzyValues.append([float(x) for x in quantities])
+                    count += 1
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
+                        count = 0
                         for line in f:
                             line = line.split("\n")[0]
                             parts = line.split(":")
                             parts[0] = parts[0].strip()
                             parts[1] = parts[1].strip()
                             items = parts[0].split(self._sep)
                             quantities = parts[1].split(self._sep)
                             self._ts.append(int(items[0]))
-                            self._transactionsDB.append([x for x in items[1:]])
-                            self._fuzzyValuesDB.append([float(x) for x in quantities])
+                            self._transactions.append([x for x in items[1:]])
+                            self._fuzzyValues.append([float(x) for x in quantities])
+                            count += 1
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _mapNeighbours(self):
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self) -> None:
         """
-        A function to map items to their Neighbours
+        Fuzzy periodic Frequent pattern mining process will start from here
         """
-        self._mapItemNeighbours = {}
-        if isinstance(self._nFile, _ab._pd.DataFrame):
-            data, items = [], []
-            if self._nFile.empty:
-                print("its empty..")
-            i = self._nFile.columns.values.tolist()
-            if 'items' in i:
-                items = self._nFile['items'].tolist()
-            if 'Neighbours' in i:
-                data = self._nFile['Neighbours'].tolist()
-            for k in range(len(items)):
-                self._mapItemNeighbours[items[k]] = data[k]
-
-        if isinstance(self._nFile, str):
-            if _ab._validators.url(self._nFile):
-                data = _ab._urlopen(self._nFile)
-                for line in data:
-                    line = line.decode("utf-8")
-                    line = line.split("\n")[0]
-                    parts = [i.rstrip() for i in line.split(self._sep)]
-                    parts = [x for x in parts]
-                    item = parts[0]
-                    neigh1 = []
-                    for i in range(1, len(parts)):
-                        neigh1.append(parts[i])
-                    self._mapItemNeighbours[item] = neigh1
-            else:
-                try:
-                    with open(self._nFile, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line = line.split("\n")[0]
-                            parts = [i.rstrip() for i in line.split(self._sep)]
-                            parts = [x for x in parts]
-                            item = parts[0]
-                            neigh1 = []
-                            for i in range(1, len(parts)):
-                                neigh1.append(parts[i])
-                            self._mapItemNeighbours[item] = neigh1
-                except IOError:
-                    print(self._nFile)
-                    print("File Not Found")
-                    quit()
+        maxTID = 0
+        lastTIDs = {}
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        tid = int()
+        for line in range(len(self._transactions)):
+            tid = int(self._ts[line])
+            self._dbLen += 1
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            if tid < maxTID:
+                maxTID = tid
+            for i in range(0, len(items)):
+                item = items[i]
+                if item in self._mapItemSum:
+                    self._mapItemSum[item] += quantities[i]
+                else:
+                    self._mapItemSum[item] = quantities[i]
+        listOfFFIList = []
+        mapItemsToFFLIST = {}
+        # self._minSup = self._convert(self._minSup)
+        self._minSup = float(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        for item1 in self._mapItemSum.keys():
+            item = item1
+            if self._mapItemSum[item] >= self._minSup:
+                fUList = _FFList(item)
+                k = tuple([item])
+                mapItemsToFFLIST[k] = fUList
+                listOfFFIList.append(fUList)
+                lastTIDs[item] = tid
+        listOfFFIList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        for line in range(len(self._transactions)):
+            tid = int(self._ts[line])
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                item = pair.item
+                pair.quantity = quantities[i]
+                if self._mapItemSum[item] >= self._minSup:
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                remainUtil = 0
+                for j in range(len(revisedTransaction) - 1, i - 1, -1):
+                    remainUtil += revisedTransaction[j].quantity
+                if pair.quantity > remainUtil:
+                    remainingUtility = pair.quantity
+                else:
+                    remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(tuple([pair.item])) is not None:
+                    FFListOfItem = mapItemsToFFLIST[tuple([pair.item])]
+                    if len(FFListOfItem.elements) == 0:
+                        element = _Element(tid, pair.quantity, remainingUtility, 0)
+                    else:
+                        if lastTIDs[pair.item] == tid:
+                            element = _Element(tid, pair.quantity, remainingUtility, maxTID - tid)
+                        else:
+                            lastTid = FFListOfItem.elements[-1].tid
+                            curPer = tid - lastTid
+                            element = _Element(tid, pair.quantity, remainingUtility, curPer)
+                    FFListOfItem.addElement(element)
+        self._FPFPMining(self._itemSetBuffer, 0, listOfFFIList)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
-    def startMine(self):
+    def mine(self) -> None:
         """
-        Frequent pattern mining process will start from here
+        Fuzzy periodic Frequent pattern mining process will start from here
         """
+        maxTID = 0
+        lastTIDs = {}
         self._startTime = _ab._time.time()
-        self._mapNeighbours()
         self._creatingItemSets()
         self._finalPatterns = {}
-        recent_occur = {}
-        for line in range(len(self._transactionsDB)):
-            item_list = self._transactionsDB[line]
-            fuzzyValues_list = self._fuzzyValuesDB[line]
-            ts = self._ts[line]
+        tid = int()
+        for line in range(len(self._transactions)):
+            tid = int(self._ts[line])
             self._dbLen += 1
-            """
-            The section below is for:
-            1.Finding the support of each item's region in the entire database
-            2.Finding the periodic patterns of the data
-            3.Trimming off the patterns whose support is less than minSupport
-            """
-            for i in range(0, len(item_list)):
-                item = item_list[i]
-                if item in self._tidList:
-                    self._tidList[item].append(ts - recent_occur[item][-1])
-                    recent_occur[item].append(ts)
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            if tid < maxTID:
+                maxTID = tid
+            for i in range(0, len(items)):
+                item = items[i]
+                if item in self._mapItemSum:
+                    self._mapItemSum[item] += quantities[i]
                 else:
-                    self._tidList[item] = [ts]
-                    recent_occur[item] = [ts]
-                fuzzy_ref = fuzzyValues_list[i]
-                if item[0] in self._mapItemNeighbours:
-                    if item in self._itemSupData.keys():
-                        self._itemSupData[item] += fuzzy_ref
-                    else:
-                        self._itemSupData[item] = fuzzy_ref
-        for item in self._tidList.keys():
-            self._tidList[item].append(len(self._transactionsDB) - recent_occur[item][-1])
-        del recent_occur
-        """
-        Using Maximum Scalar Cardinality Value strategy to narrow down search space and generate candidate fuzzy periodic-frequent items. 
-        Step1. Identify the regional representative (region with max support). This is the representative that will be tested to see if its greater than given minSup
-        Step2. prune out all items whose regional support is less than the given minSup
-        Step3. At the end, sort the list of stored Candidate Frequent-Periodic Patterns in ascending order
-        """
-
-        listOfFFList = []
+                    self._mapItemSum[item] = quantities[i]
+        listOfFFIList = []
         mapItemsToFFLIST = {}
-        region_label = []
-        #self._minSup = self._convert(self._minSup)
-        for item in self._itemSupData.keys():
-            if self._itemSupData[item] >= self._minSup:
-                self._mapItemSum[item] = self._itemSupData[item]
-                fuList = _FFList(item)
-                if int(self._maxPer) >= max(self._tidList[item]):
-                    fuList.isPeriodic = True
-                mapItemsToFFLIST[item] = fuList
-                listOfFFList.append(fuList)
-        del self._itemSupData
-        del self._tidList
-        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        tid = 0
-        for j in range(len(self._transactionsDB)):
-            item_list = list(set(self._transactionsDB[j]).intersection(set(self._mapItemSum.keys())))
-            fuzzy_list = [self._fuzzyValuesDB[j][i] for i in range(len(self._fuzzyValuesDB[j])) if self._transactionsDB[j][i] in self._mapItemSum.keys()]
+        # self._minSup = self._convert(self._minSup)
+        self._minSup = float(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        for item1 in self._mapItemSum.keys():
+            item = item1
+            if self._mapItemSum[item] >= self._minSup:
+                fUList = _FFList(item)
+                k = tuple([item])
+                mapItemsToFFLIST[k] = fUList
+                listOfFFIList.append(fUList)
+                lastTIDs[item] = tid
+        listOfFFIList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        for line in range(len(self._transactions)):
+            tid = int(self._ts[line])
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
             revisedTransaction = []
-            for i in range(0, len(item_list)):
+            for i in range(0, len(items)):
                 pair = _Pair()
-                pair.item = item_list[i]
-                fuzzy_ref = fuzzy_list[i]
-                pair.quantity = fuzzy_ref
-                if pair.quantity > 0:
-                    revisedTransaction.append(pair)
+                pair.item = items[i]
+                item = pair.item
+                pair.quantity = quantities[i]
+                if self._mapItemSum[item] >= self._minSup:
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
             revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            qaunt = {}
             for i in range(len(revisedTransaction) - 1, -1, -1):
                 pair = revisedTransaction[i]
-                qaunt[pair.item[0]] = pair.quantity
                 remainUtil = 0
-                temp = list(set(self._mapItemNeighbours[pair.item[0]]).intersection(set(qaunt.keys())))
-                # print(temp, self._mapItemNeighbours[pair.item[0]], qaunt)
-                for j in temp:
-                    remainUtil += float(qaunt[j])
-                del temp
-                remainingUtility = remainUtil
-                FFListObject = mapItemsToFFLIST[pair.item]
-                element = _Element(tid, pair.quantity, remainingUtility)
-                FFListObject.addElement(element)
-            del qaunt
-            tid += 1
-        itemNeighbours = list(self._mapItemNeighbours.keys())
-        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
+                for j in range(len(revisedTransaction) - 1, i - 1, -1):
+                    remainUtil += revisedTransaction[j].quantity
+                if pair.quantity > remainUtil:
+                    remainingUtility = pair.quantity
+                else:
+                    remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(tuple([pair.item])) is not None:
+                    FFListOfItem = mapItemsToFFLIST[tuple([pair.item])]
+                    if len(FFListOfItem.elements) == 0:
+                        element = _Element(tid, pair.quantity, remainingUtility, 0)
+                    else:
+                        if lastTIDs[pair.item] == tid:
+                            element = _Element(tid, pair.quantity, remainingUtility, maxTID - tid)
+                        else:
+                            lastTid = FFListOfItem.elements[-1].tid
+                            curPer = tid - lastTid
+                            element = _Element(tid, pair.quantity, remainingUtility, curPer)
+                    FFListOfItem.addElement(element)
+        self._FPFPMining(self._itemSetBuffer, 0, listOfFFIList)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def _FSFIMining(self, prefix, prefixLen, FSFIM, minSup, itemNeighbours):
+    def _FPFPMining(self, prefix, prefixLen, fsFim):
+
         """
-        Generates FFSPMiner from prefix
+        Generates FPFP from prefix
 
-        :param prefix: the prefix patterns of FFSPMiner
+        :param prefix: the prefix patterns of FPFP
         :type prefix: len
         :param prefixLen: the length of prefix
         :type prefixLen: int
-        :param FSFIM: the Fuzzy list of prefix itemSets
-        :type FSFIM: list
-        :param minSup: the minimum support of
-        :type minSup:int
-        :param itemNeighbours: the set of common neighbours of prefix
-        :type itemNeighbours: list or set
-        """
-        for i in range(0, len(FSFIM)):
-            _FFListObject1 = FSFIM[i]
-            if _FFListObject1.sumIUtil >= minSup:
-                self._WriteOut(prefix, prefixLen, _FFListObject1, _FFListObject1.sumIUtil)
-            newNeighbourList = self._Intersection(self._mapItemNeighbours.get(_FFListObject1.item[0]), itemNeighbours)
-            if _FFListObject1.sumRUtil >= minSup:
+        :param fsFim: the Fuzzy list of prefix itemSets
+        :type fsFim: list
+        """
+        for i in range(0, len(fsFim)):
+            X = fsFim[i]
+            if X.sumLUtil >= self._minSup and X.maxPeriod <= self._maxPer:
+                self._WriteOut(prefix, prefixLen, X.item, X.sumLUtil, X.maxPeriod)
+            if X.sumRUtil >= self._minSup:
                 exULs = []
-                for j in range(i + 1, len(FSFIM)):
-                    _FFListObject2 = FSFIM[j]
-                    if _FFListObject2.item in newNeighbourList:
-                        exULs.append(self._construct(_FFListObject1, _FFListObject2))
-                        self._joinsCnt += 1
-                self._itemSetBuffer.insert(prefixLen, _FFListObject1.item)
-                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup, newNeighbourList)
-
-    def _Intersection(self, neighbourX, neighbourY):
-        """
-        A function to get common neighbours from 2 itemSets
-
-        :param neighbourX: the set of neighbours of itemSet 1
-        :type neighbourX: set or list
-        :param neighbourY: the set of neighbours of itemSet 2
-        :type neighbourY: set or list
-        :return : set of common neighbours of 2 itemSets
-        :rtype :set
-        """
-        result = []
-        if neighbourX is None or neighbourY is None:
-            return result
-        for i in range(0, len(neighbourX)):
-            if neighbourX[i] in neighbourY:
-                result.append(neighbourX[i])
-        return result
+                for j in range(i + 1, len(fsFim)):
+                    Y = fsFim[j]
+                    exULs.append(self._construct(X, Y))
+                    self._joinsCnt += 1
+                self._itemSetBuffer.insert(prefixLen, X.item)
+                self._FPFPMining(self._itemSetBuffer, prefixLen + 1, exULs)
 
-    def getMemoryUSS(self):
+    def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
         return self._memoryRSS
 
-    def getRuntime(self):
+    def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime - self._startTime
 
-    def _construct(self, _FFListObject1, _FFListObject2):
+    def _construct(self, px: _FFList, py: _FFList) -> _FFList:
         """
-        A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
+        A function to construct a new Fuzzy item set from 2 fuzzy itemSets
 
-        :param _FFListObject1:the itemSet px
-        :type _FFListObject1:FFI-List
-        :param _FFListObject2:itemSet py
-        :type _FFListObject2:FFI-List
-        :return :the itemSet of pxy(px and py)
+        :param px:the item set px
+        :type px:FFI-List
+        :param py:item set py
+        :type py:FFI-List
+        :return :the item set of pxy(px and py)
         :rtype :FFI-List
         """
-        recent_occur, first_occur, tid = 0, 0, 0
-        periodlist = []
-        _newFFListObject = _FFList(_FFListObject2.item)
-        for Ob1Element in _FFListObject1.elements:
-            Ob2Element = self._findElementWithTID(_FFListObject2, Ob1Element.tid)
-            if Ob2Element is None:
+        pxyUL = _FFList(py.item)
+        prev = 0
+        for ex in px.elements:
+            ey = self._findElementWithTID(py, ex.tid)
+            if ey is None:
                 continue
-            tid = Ob1Element.tid
-            if len(periodlist) == 0:
-                periodlist.append(abs(first_occur - tid))
-                recent_occur = tid
-            else:
-                periodlist.append(tid - recent_occur)
-                recent_occur = tid
-            newElement = _Element(Ob1Element.tid, min([Ob1Element.iUtils, Ob2Element.iUtils], key=lambda x: float(x)),
-                                  Ob2Element.rUtils)
-            _newFFListObject.addElement(newElement)
-
-        if periodlist and int(self._maxPer) >= max(periodlist):
-            _newFFListObject.isPeriodic = True
-        else:
-            _newFFListObject.isPeriodic = False
-        return _newFFListObject
+            eXY = _Element(ex.tid, min([ex.lUtils, ey.lUtils], key=lambda x: float(x)), ey.rUtils, ex.tid - prev)
+            pxyUL.addElement(eXY)
+            prev = ex.tid
+        return pxyUL
 
-    def _findElementWithTID(self, uList, tid):
+    def _findElementWithTID(self, UList, tid) -> _Element:
         """
         To find element with same tid as given
 
-        :param uList:fuzzyList
-        :type uList:FFI-List
+        :param UList: fuzzy list
+        :type UList:FFI-List
         :param tid:transaction id
         :type tid:int
-        :return:element tid as given
+        :return:element eith tid as given
         :rtype: element if exist or None
         """
-        List = uList.elements
+        List = UList.elements
         first = 0
         last = len(List) - 1
         while first <= last:
             mid = (first + last) >> 1
             if List[mid].tid < tid:
                 first = mid + 1
             elif List[mid].tid > tid:
                 last = mid - 1
             else:
                 return List[mid]
         return None
 
-    def _WriteOut(self, prefix, prefixLen, _FFListObject, sumIUtil):
+    def _WriteOut(self, prefix: List[int], prefixLen: int, item: int, sumLUtil: float, period: int) -> None:
         """
         To Store the patten
 
         :param prefix: prefix of itemSet
         :type prefix: list
         :param prefixLen: length of prefix
         :type prefixLen: int
         :param item: the last item
         :type item: int
-        :param sumIUtil: sum of utility of itemSet
-        :type sumIUtil: float
+        :param sumLUtil: sum of utility of itemSet
+        :type sumLUtil: float
+        :param period: represent the period of itemSet
+        :type period: int
+        :return: None
         """
-        item = _FFListObject.item
         self._itemsCnt += 1
         res = ""
         for i in range(0, prefixLen):
-            res += str(prefix[i]) + "\t"
+            res += str(prefix[i]) +  "\t"
         res += str(item)
-        res1 = str(sumIUtil)
-        self._finalPatterns[res] = res1
-
-        if _FFListObject.isPeriodic:
-            self._finalPeriodicPatterns[res] = res1
+        #res1 = str(sumLUtil) + " : " + str(period)
+        self._finalPatterns[res] = [sumLUtil, period]
 
-    def getPatternsAsDataFrame(self):
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
-        for a, b in self._finalPeriodicPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        for a, b in self._finalPatterns.items():
+            data.append([a.replace('\t', ' '), b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
-    def getPatterns(self):
+    def getPatterns(self) -> Dict[str, str]:
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self._finalPeriodicPatterns
+        return self._finalPatterns
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
-        self.oFile = outFile
-        keylist = (self._finalPatterns.keys())
-        writer = open(self.oFile, 'w+')
-        for x in keylist:
-            patternsAndSupport = x.strip() + ":" + str(self._finalPatterns[x])
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            patternsAndSupport = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % patternsAndSupport)
 
-    def printResults(self):
+    def printResults(self) -> None:
         """
-        This function is used to print the result
+        This function is used to print the results
         """
-        print("Total number of Spatial Fuzzy Periodic-Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Fuzzy Periodic-Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = FGPFPMiner(_ab._sys.argv[1], _ab._sys.argv[2], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5],
-                             _ab._sys.argv[6])
-        if len(_ab._sys.argv) == 5:
-            _ap = FGPFPMiner(_ab._sys.argv[1], _ab._sys.argv[2], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 6:  # to  include a user specified separator
+            _ap = FPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:  # to consider "\t" as a separator
+            _ap = FPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        print("Total number of Spatial Fuzzy Periodic Frequent  Patterns:", len(_ap.getPatterns()))
+        _ap.mine()
+        print("Total number of Fuzzy Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS",  _ap.getMemoryRSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
-        _ap.save("outputfile.txt")
     else:
-        _ap = FGPFPMiner('sample.txt','nei.txt', 1, 10, ' ')
+        _ap = FPFPMiner('sample.txt', 1, 10, ' ')
         _ap.startMine()
+        _ap.mine()
         print("Total number of Fuzzy Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save('output.txt')
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
-
```

### Comparing `pami-2024.3.9.2/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py` & `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,29 +1,42 @@
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
+#     .. code-block:: python
 #
-#     from PAMI.fuzzyGeoreferencedPeriodicFrequentPattern import FGPFPMiner as alg
+#             from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
 #
-#     obj = alg.FFSPMiner("input.txt", "neighbours.txt", 3, 4)
+#             obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
 #
-#     obj.startMine()
+#             obj.mine()
 #
-#     print("Total number of fuzzy frequent spatial patterns:", len(obj.getPatterns()))
+#             fuzzySpatialFrequentPatterns = obj.getPatterns()
 #
-#     obj.save("outputFile")
+#             print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
 #
-#     print("Total Memory in USS:", obj.getMemoryUSS())
+#             obj.save("outputFile")
 #
-#     print("Total Memory in RSS", obj.getMemoryRSS())
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total ExecutionTime in seconds:", obj.getRuntime())
+#             print("Total Memory in USS:", memUSS)
 #
+#             memRSS = obj.getMemoryRSS()
+#
+#             print("Total Memory in RSS", memRSS)
+#
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
+#
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -32,62 +45,63 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
-import pandas as pd
-import plotly.express as px
-import PAMI.fuzzyGeoreferencedPeriodicFrequentPattern.basic.abstract as _ab
+
+from PAMI.fuzzyGeoreferencedFrequentPattern.basic import abstract as _ab
+from deprecated import deprecated
 
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
 
     :Attributes:
 
-         item: int
-             the item name
-         sumIUtil: float
-             the sum of utilities of a fuzzy item in database
-         sumRUtil: float
-             the sum of resting values of a fuzzy item in database
-         elements: list
-             a list of elements contain tid,Utility and resting values of element in each transaction
+        item: int
+            the item name
+        sumIUtil: float
+            the sum of utilities of a fuzzy item in database
+        sumRUtil: float
+            the sum of resting values of a fuzzy item in database
+        elements: list
+            a list of elements contain tid,Utility and resting values of element in each transaction
+
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
         printElement(e)
-            Method to print elements
+            Method to print elements            
+
     """
 
     def __init__(self, itemName):
         self.item = itemName
-        self.isPeriodic = False
         self.sumIUtil = 0.0
         self.sumRUtil = 0.0
         self.elements = []
 
     def addElement(self, element):
         """
         A Method that add a new element to FFList
 
         :param element: an element to be added to FFList
-        :param element: Element
+        :type element: Element
         """
         self.sumIUtil += element.iUtils
         self.sumRUtil += element.rUtils
         self.elements.append(element)
 
     def printElement(self):
         """
-        A Method to Print elements in the FFList object
+        A Method to Print elements in the FFList
         """
         for ele in self.elements:
             print(ele.tid, ele.iUtils, ele.rUtils)
 
 
 class _Element:
     """
@@ -105,29 +119,84 @@
 
     def __init__(self, tid, iUtil, rUtil):
         self.tid = tid
         self.iUtils = iUtil
         self.rUtils = rUtil
 
 
+class _Regions:
+    """
+    A class calculate the regions
+
+    :Attributes:
+
+        low : int
+            low region value
+        middle: int
+            middle region value
+        high : int
+            high region values
+    """
+
+    def __init__(self, quantity, regionsNumber):
+        self.low = 0
+        self.middle = 0
+        self.high = 0
+        if regionsNumber == 3:  # if we have 3 regions
+            if 0 < quantity <= 1:
+                self.low = 1
+                self.high = 0
+                self.middle = 0
+            elif 1 < quantity <= 6:
+                self.low = float((6 - quantity) / 5)
+                self.middle = float((quantity - 1) / 5)
+                self.high = 0
+            elif 6 < quantity <= 11:
+                self.low = 0
+                self.middle = float((11 - quantity) / 5)
+                self.high = float((quantity - 6) / 5)
+            else:
+                self.low = 0
+                self.middle = 0
+                self.high = 1
+
+
 class _Pair:
     """
     A class to store item and it's quantity together
     """
 
     def __init__(self):
         self.item = 0
         self.quantity = 0
 
 
-class FGPFPMiner(_ab._fuzzySpatialFrequentPatterns):
+class FFSPMiner(_ab._fuzzySpatialFrequentPatterns):
     """
-    Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
-    which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
-    techniques to reduce the search space.
+    :Description:   Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
+                    which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
+                    techniques to reduce the search space.
+
+    Reference:   Reference: P. Veena, B. S. Chithra, R. U. Kiran, S. Agarwal and K. Zettsu, "Discovering Fuzzy Frequent
+                 Spatial Patterns in Large Quantitative Spatiotemporal databases," 2021 IEEE International Conference on Fuzzy Systems
+                 (FUZZ-IEEE), 2021, pp. 1-8, doi: 10.1109/FUZZ45933.2021.9494594.
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param nFile: str :
+                   Name of the input file to mine complete set of frequent patterns
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
 
     :Attributes:
 
         iFile : file
             Name of the input file to mine complete set of fuzzy spatial frequent patterns
         oFile : file
             Name of the oFile file to store complete set of fuzzy spatial frequent patterns
@@ -171,97 +240,110 @@
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
-            Total amount of runtime taken by the mining process will be retrieved from this function
+            Total amount of runtime taken by the mining process will be retrieved from this function            
         convert(value):
             To convert the given user specified value
         FSFIMining( prefix, prefixLen, fsFim, minSup)
             Method generate FFI from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
         Intersection(neighbourX,neighbourY)
             Return common neighbours of 2 itemSet Neighbours
         findElementWithTID(uList, tid)
             To find element with same tid as given
         WriteOut(prefix, prefixLen, item, sumIUtil,period)
             To Store the patten
-
+    
     **Executing the code on terminal :**
     ----------------------------------------
 
-            Format:
-                    >>> python3 FGPFPMiner_old.py <inputFile> <outputFile> <neighbours> <minSup> <maxPer> <sep>
-            Examples:
-                    >>> python3  FGPFPMiner_old.py sampleTDB.txt output.txt sampleN.txt 3 4  (minSup will be considered in support count or frequency)
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 FFSPMiner_old.py <inputFile> <outputFile> <neighbours> <minSup> <sep>
+
+      Example Usage:
+
+      (.venv) $ python3  FFSPMiner_old.py sampleTDB.txt output.txt sampleN.txt 3
+
+      (.venv) $ python3  FFSPMiner_old.py sampleTDB.txt output.txt sampleN.txt 0.3
+
+      (.venv) $ python3  FFSPMiner_old.py sampleTDB.txt output.txt sampleN.txt 3
+
+    .. note:: minSup will be considered in percentage of database transactions
+
 
     **Sample run of importing the code:**
-    ------------------------------------------
+    ----------------------------------------
+        
+            from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
+
+            obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
+
+            obj.mine()
 
-        from PAMI.fuzzyGeoreferencedPeriodicFrequentPattern import FGPFPMiner as alg
+            fuzzySpatialFrequentPatterns = obj.getPatterns()
 
-        obj = alg.FFSPMiner("input.txt", "neighbours.txt", 3, 4)
+            print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
 
-        obj.startMine()
+            obj.save("outputFile")
 
-        print("Total number of fuzzy frequent spatial patterns:", len(obj.getPatterns()))
+            memUSS = obj.getMemoryUSS()
 
-        obj.save("outputFile")
+            print("Total Memory in USS:", memUSS)
 
-        print("Total Memory in USS:", obj.getMemoryUSS())
+            memRSS = obj.getMemoryRSS()
 
-        print("Total Memory in RSS", obj.getMemoryRSS())
+            print("Total Memory in RSS", memRSS)
 
-        print("Total ExecutionTime in seconds:", obj.getRuntime())
+            run = obj.getRuntime()
+
+            print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    ----------------
-            The complete program was written by B.Sai Chitra and Kundai Kwangwari under the supervision of Professor Rage Uday Kiran.
+    ---------------
+            The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
     """
-
+    
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _nFile = " "
-    _FuzFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _sep = "\t"
-    _transactionsDB = []
-    _fuzzyValuesDB = []
+    _transactions = []
+    _fuzzyValues = []
 
-    def __init__(self, iFile, nFile, FuzFile, minSup, maxPer, sep):
-        super().__init__(iFile, nFile, FuzFile, minSup, maxPer, sep)
+    def __init__(self, iFile, nFile, minSup, sep="\t"):
+        super().__init__(iFile, nFile, minSup, sep)
         self._mapItemNeighbours = {}
         self._startTime = 0
         self._endTime = 0
         self._itemsCnt = 0
-        self._itemSupData = {}
+        self._mapItemsLowSum = {}
+        self._mapItemsMidSum = {}
+        self._mapItemsHighSum = {}
         self._mapItemSum = {}
-        self._finalClosedPeriodicPatterns = {}
         self._mapItemRegions = {}
-        self._fuzzyRegionReferenceMap = {}
         self._joinsCnt = 0
         self._BufferSize = 200
         self._itemSetBuffer = []
         self._finalPatterns = {}
-        self._finalPeriodicPatterns = {}
-        self._tidList = {}
         self._dbLen = 0
-        self._regionsNumber = 0
-        self._RegionsCal = []
-        self._RegionsLabel = []
-        self._LabelKey = {}
 
     def _compareItems(self, o1, o2):
         """
         A Function that sort all FFI-list in ascending order of Support
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
@@ -270,89 +352,67 @@
             return compare
 
     def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
+        :type value: int or float or str
         :return: converted value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
             if '.' in value:
-                value = float(value)
+                value = (self._dbLen * value)
             else:
                 value = int(value)
         return value
 
-    def _fuzzyMembershipFunc(self):
-
-        try:
-            with open(self._FuzFile, 'r', encoding='utf-8') as f:
-                count = 0
-                for line in f:
-                    line = line.split("\n")[0]
-                    parts = line.split(" ")
-                    lowerBound = parts[0].strip()
-                    upperBound = parts[1].strip()
-                    lb_Label = parts[2].strip()
-                    ub_Label = parts[3].strip()
-                    self._RegionsCal.append([int(lowerBound), int(upperBound)])
-                    self._RegionsLabel.append([lb_Label, ub_Label])
-                    for i in range(0, 2):
-                        if lb_Label.capitalize() not in self._LabelKey:
-                            self._LabelKey[lb_Label.capitalize()] = count
-                            count += 1
-                        if ub_Label.capitalize() not in self._LabelKey:
-                            self._LabelKey[ub_Label.capitalize()] = count
-                            count += 1
-        except IOError:
-            print("File Not Found")
-            quit()
-
     def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-        self._transactionsDB, self._fuzzyValuesDB = [], []
+        self._transactions, self._fuzzyValues = [], []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._transactionsDB = self._iFile['Transactions'].tolist()
+                self.transactions = self._iFile['Transactions'].tolist()
             if 'fuzzyValues' in i:
-                self._fuzzyValuesDB = self._iFile['fuzzyValues'].tolist()
+                self.fuzzyValues = self._iFile['Utilities'].tolist()
 
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line = line.decode("utf-8")
                     line = line.split("\n")[0]
                     parts = line.split(":")
+                    parts[0] = parts[0].strip()
+                    parts[2] = parts[2].strip()
                     items = parts[0].split(self._sep)
                     quantities = parts[2].split(self._sep)
-                    self._transactionsDB.append([x for x in items])
-                    self._fuzzyValuesDB.append([x for x in quantities])
+                    self.transactions.append([x for x in items])
+                    self.fuzzyValues.append([x for x in quantities])
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line = line.split("\n")[0]
                             parts = line.split(":")
                             parts[0] = parts[0].strip()
                             parts[2] = parts[2].strip()
                             items = parts[0].split(self._sep)
                             quantities = parts[2].split(self._sep)
-                            self._transactionsDB.append([x for x in items])
-                            self._fuzzyValuesDB.append([x for x in quantities])
+                            self._transactions.append([x for x in items])
+                            self._fuzzyValues.append([x for x in quantities])
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _mapNeighbours(self):
         self._mapItemNeighbours = {}
         if isinstance(self._nFile, _ab._pd.DataFrame):
@@ -389,142 +449,204 @@
                             parts = [x for x in parts]
                             item = parts[0]
                             neigh1 = []
                             for i in range(1, len(parts)):
                                 neigh1.append(parts[i])
                             self._mapItemNeighbours[item] = neigh1
                 except IOError:
-                    print(self._nFile)
                     print("File Not Found")
                     quit()
 
-    def _Regions(self, quantity):
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
         """
-        param quantity:
-        type quantity:
+        Frequent pattern mining process will start from here
         """
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        self._mapNeighbours()
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            self._dbLen += 1
+            for i in range(0, len(items)):
+                regions = _Regions(int(quantities[i]), 3)
+                item = items[i]
+                if item in self._mapItemsLowSum.keys():
+                    low = self._mapItemsLowSum[item]
+                    low += regions.low
+                    self._mapItemsLowSum[item] = low
+                else:
+                    self._mapItemsLowSum[item] = regions.low
+                if item in self._mapItemsMidSum.keys():
+                    mid = self._mapItemsMidSum[item]
+                    mid += regions.middle
+                    self._mapItemsMidSum[item] = mid
+                else:
+                    self._mapItemsMidSum[item] = regions.middle
+                if item in self._mapItemsHighSum.keys():
+                    high = self._mapItemsHighSum[item]
+                    high += regions.high
+                    self._mapItemsHighSum[item] = high
+                else:
+                    self._mapItemsHighSum[item] = regions.high
+        listOfFFList = []
+        mapItemsToFFLIST = {}
+        self._minSup = self._convert(self._minSup)
+        for item1 in self._mapItemsLowSum.keys():
+            item = item1
+            low = self._mapItemsLowSum[item]
+            mid = self._mapItemsMidSum[item]
+            high = self._mapItemsHighSum[item]
+            if low >= mid and low >= high:
+                self._mapItemSum[item] = low
+                self._mapItemRegions[item] = "L"
+            elif mid >= low and mid >= high:
+                self._mapItemSum[item] = mid
+                self._mapItemRegions[item] = "M"
+            elif high >= low and high >= mid:
+                self._mapItemRegions[item] = "H"
+                self._mapItemSum[item] = high
+            if self._mapItemSum[item] >= self._minSup:
+                fuList = _FFList(item)
+                mapItemsToFFLIST[item] = fuList
+                listOfFFList.append(fuList)
+        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                regions = _Regions(int(quantities[i]), 3)
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if self._mapItemRegions[pair.item] == "L":
+                        pair.quantity = regions.low
+                    elif self._mapItemRegions[pair.item] == "M":
+                        pair.quantity = regions.middle
+                    elif self._mapItemRegions[pair.item] == "H":
+                        pair.quantity = regions.high
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                remainUtil = 0
+                for j in range(len(revisedTransaction) - 1, i, -1):
+                    if self._mapItemNeighbours.get(pair.item) is None:
+                        continue
+                    if revisedTransaction[j].item in self._mapItemNeighbours[pair.item]:
+                        remainUtil += revisedTransaction[j].quantity
+                remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(pair.item) is not None:
+                    FFListOfItem = mapItemsToFFLIST[pair.item]
+                    element = _Element(tid, pair.quantity, remainingUtility)
+                    FFListOfItem.addElement(element)
+            tid += 1
+        itemNeighbours = list(self._mapItemNeighbours.keys())
+        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
-        self._list = [0] * len(self._LabelKey)
-        if self._RegionsCal[0][0] < quantity <= self._RegionsCal[0][1]:
-            self._list[0] = 1
-            return
-        elif quantity >= self._RegionsCal[-1][0]:
-            self._list[-1] = 1
-            return
-        else:
-            for i in range(1, len(self._RegionsCal) - 1):
-                if self._RegionsCal[i][0] < quantity <= self._RegionsCal[i][1]:
-                    base = self._RegionsCal[i][1] - self._RegionsCal[i][0]
-                    for pos in range(0, 2):
-                        if self._RegionsLabel[i][pos].islower():
-                            self._list[self._LabelKey[self._RegionsLabel[i][pos].capitalize()]] = float(
-                                (self._RegionsCal[i][1] - quantity) / base)
-                        else:
-                            self._list[self._LabelKey[self._RegionsLabel[i][pos].capitalize()]] = float(
-                                (quantity - self._RegionsCal[i][0]) / base)
-        return
-
-    def startMine(self):
+    def mine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
-        self._mapNeighbours()
         self._creatingItemSets()
-        self._fuzzyMembershipFunc()
         self._finalPatterns = {}
-        recent_occur = {}
-        for line in range(len(self._transactionsDB)):
-            item_list = self._transactionsDB[line]
-            fuzzyValues_list = self._fuzzyValuesDB[line]
+        self._mapNeighbours()
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
             self._dbLen += 1
-            """
-            This section below is for:
-            1.Finding the support of each item's region in the entire database
-            2.Finding the periodic patterns of the data
-            3.Trimming off the patterns whose support is less than minSupport
-            """
-            for i in range(0, len(item_list)):
-                item = item_list[i]
-                if item in self._tidList:
-                    self._tidList[item].append(self._dbLen - recent_occur[item][-1])
-                    recent_occur[item].append(self._dbLen)
+            for i in range(0, len(items)):
+                regions = _Regions(int(quantities[i]), 3)
+                item = items[i]
+                if item in self._mapItemsLowSum.keys():
+                    low = self._mapItemsLowSum[item]
+                    low += regions.low
+                    self._mapItemsLowSum[item] = low
                 else:
-                    self._tidList[item] = [self._dbLen]
-                    recent_occur[item] = [self._dbLen]
-                fuzzy_ref = fuzzyValues_list[i]
-                if item in self._mapItemNeighbours:
-                    if fuzzy_ref not in self._fuzzyRegionReferenceMap:
-                        self._Regions(int(fuzzy_ref))
-                        self._fuzzyRegionReferenceMap[fuzzy_ref] = self._list
-
-                    if item in self._itemSupData.keys():
-                        self._itemSupData[item] = [sum(i) for i in zip(self._itemSupData[item],
-                                                                       self._fuzzyRegionReferenceMap[fuzzy_ref])]
-                    else:
-                        self._itemSupData[item] = self._fuzzyRegionReferenceMap[fuzzy_ref]
-
-        for item in self._tidList.keys():
-            self._tidList[item].append(len(self._transactionsDB) - recent_occur[item][-1])
-        del recent_occur
-        """
-        Using Maximum Scalar Cardinality Value strategy to narrow down search space and generate candidate fuzzy periodic-frequent items. 
-        Step1. Identify the regional representative (region with max support). This is the representative that will be tested to see if its greater than given minSup
-        Step2. prune out all items whose regional support is less than the given minSup
-        Step3. At the end, sort the list of stored Candidate Frequent-Periodic Patterns in ascending order
-        """
-
+                    self._mapItemsLowSum[item] = regions.low
+                if item in self._mapItemsMidSum.keys():
+                    mid = self._mapItemsMidSum[item]
+                    mid += regions.middle
+                    self._mapItemsMidSum[item] = mid
+                else:
+                    self._mapItemsMidSum[item] = regions.middle
+                if item in self._mapItemsHighSum.keys():
+                    high = self._mapItemsHighSum[item]
+                    high += regions.high
+                    self._mapItemsHighSum[item] = high
+                else:
+                    self._mapItemsHighSum[item] = regions.high
         listOfFFList = []
         mapItemsToFFLIST = {}
-        region_label = []
-        for i in range(0, len(self._RegionsLabel)):
-            if self._RegionsLabel[i][1] not in region_label:
-                region_label.append(str(self._RegionsLabel[i][1]))
-
         self._minSup = self._convert(self._minSup)
-        for item in self._itemSupData.keys():
-            if max(self._itemSupData[item]) >= self._minSup:
-                self._mapItemSum[item] = max(self._itemSupData[item])
-                self._mapItemRegions[item] = region_label[self._itemSupData[item].index(self._mapItemSum[item])]
+        for item1 in self._mapItemsLowSum.keys():
+            item = item1
+            low = self._mapItemsLowSum[item]
+            mid = self._mapItemsMidSum[item]
+            high = self._mapItemsHighSum[item]
+            if low >= mid and low >= high:
+                self._mapItemSum[item] = low
+                self._mapItemRegions[item] = "L"
+            elif mid >= low and mid >= high:
+                self._mapItemSum[item] = mid
+                self._mapItemRegions[item] = "M"
+            elif high >= low and high >= mid:
+                self._mapItemRegions[item] = "H"
+                self._mapItemSum[item] = high
+            if self._mapItemSum[item] >= self._minSup:
                 fuList = _FFList(item)
-                if int(self._maxPer) >= max(self._tidList[item]):
-                    fuList.isPeriodic = True
                 mapItemsToFFLIST[item] = fuList
                 listOfFFList.append(fuList)
-
-        del self._itemSupData
-        del self._tidList
         listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
         tid = 0
-        for j in range(len(self._transactionsDB)):
-            item_list = list(set(self._transactionsDB[j]).intersection(set(self._mapItemSum.keys())))
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
             revisedTransaction = []
-            for i in range(0, len(item_list)):
+            for i in range(0, len(items)):
                 pair = _Pair()
-                pair.item = item_list[i]
-                fuzzy_ref = str(self._fuzzyValuesDB[j][self._transactionsDB[j].index(pair.item)])
-                pair.quantity = self._fuzzyRegionReferenceMap[fuzzy_ref][
-                    region_label.index(self._mapItemRegions[pair.item])]
-                if pair.quantity > 0:
-                    revisedTransaction.append(pair)
+                pair.item = items[i]
+                regions = _Regions(int(quantities[i]), 3)
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if self._mapItemRegions[pair.item] == "L":
+                        pair.quantity = regions.low
+                    elif self._mapItemRegions[pair.item] == "M":
+                        pair.quantity = regions.middle
+                    elif self._mapItemRegions[pair.item] == "H":
+                        pair.quantity = regions.high
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
             revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            qaunt = {}
             for i in range(len(revisedTransaction) - 1, -1, -1):
                 pair = revisedTransaction[i]
-                qaunt[pair.item] = pair.quantity
                 remainUtil = 0
-                temp = list(set(self._mapItemNeighbours[pair.item]).intersection(set(qaunt.keys())))
-                for j in temp:
-                    remainUtil += float(qaunt[j])
-                del temp
+                for j in range(len(revisedTransaction) - 1, i, -1):
+                    if self._mapItemNeighbours.get(pair.item) is None:
+                        continue
+                    if revisedTransaction[j].item in self._mapItemNeighbours[pair.item]:
+                        remainUtil += revisedTransaction[j].quantity
                 remainingUtility = remainUtil
-                FFListObject = mapItemsToFFLIST[pair.item]
-                element = _Element(tid, pair.quantity, remainingUtility)
-                FFListObject.addElement(element)
-            del qaunt
+                if mapItemsToFFLIST.get(pair.item) is not None:
+                    FFListOfItem = mapItemsToFFLIST[pair.item]
+                    element = _Element(tid, pair.quantity, remainingUtility)
+                    FFListOfItem.addElement(element)
             tid += 1
         itemNeighbours = list(self._mapItemNeighbours.keys())
         self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
@@ -543,27 +665,27 @@
         :type FSFIM: list
         :param minSup: the minimum support of
         :type minSup:int
         :param itemNeighbours: the set of common neighbours of prefix
         :type itemNeighbours: list or set
         """
         for i in range(0, len(FSFIM)):
-            _FFListObject1 = FSFIM[i]
-            if _FFListObject1.sumIUtil >= minSup:
-                self._WriteOut(prefix, prefixLen, _FFListObject1, _FFListObject1.sumIUtil)
-            newNeighbourList = self._Intersection(self._mapItemNeighbours.get(_FFListObject1.item), itemNeighbours)
-            if _FFListObject1.sumRUtil >= minSup:
+            X = FSFIM[i]
+            if X.sumIUtil >= minSup:
+                self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
+            newNeighbours = self._Intersection(self._mapItemNeighbours.get(X.item), itemNeighbours)
+            if X.sumRUtil >= minSup:
                 exULs = []
                 for j in range(i + 1, len(FSFIM)):
-                    _FFListObject2 = FSFIM[j]
-                    if _FFListObject2.item in newNeighbourList:
-                        exULs.append(self._construct(_FFListObject1, _FFListObject2))
+                    Y = FSFIM[j]
+                    if Y.item in newNeighbours:
+                        exULs.append(self._construct(X, Y))
                         self._joinsCnt += 1
-                self._itemSetBuffer.insert(prefixLen, _FFListObject1.item)
-                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup, newNeighbourList)
+                self._itemSetBuffer.insert(prefixLen, X.item)
+                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup, newNeighbours)
 
     def _Intersection(self, neighbourX, neighbourY):
         """
         A function to get common neighbours from 2 itemSets
 
         :param neighbourX: the set of neighbours of itemSet 1
         :type neighbourX: set or list
@@ -599,53 +721,39 @@
         """
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
 
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime - self._startTime
 
-    def _construct(self, _FFListObject1, _FFListObject2):
+    def _construct(self, px, py):
         """
         A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
 
-        :param _FFListObject1:the itemSet px
-        :type _FFListObject1:FFI-List
-        :param _FFListObject2:itemSet py
-        :type _FFListObject2:FFI-List
+        :param px:the itemSet px
+        :type px:FFI-List
+        :param py:itemSet py
+        :type py:FFI-List
         :return :the itemSet of pxy(px and py)
         :rtype :FFI-List
         """
-        recent_occur, first_occur, tid = 0, 0, 0
-        periodlist = []
-        _newFFListObject = _FFList(_FFListObject2.item)
-        for Ob1Element in _FFListObject1.elements:
-            Ob2Element = self._findElementWithTID(_FFListObject2, Ob1Element.tid)
-            if Ob2Element is None:
+        pxyUL = _FFList(py.item)
+        for ex in px.elements:
+            ey = self._findElementWithTID(py, ex.tid)
+            if ey is None:
                 continue
-            tid = Ob1Element.tid
-            if len(periodlist) == 0:
-                periodlist.append(abs(first_occur - tid))
-                recent_occur = tid
-            else:
-                periodlist.append(tid - recent_occur)
-                recent_occur = tid
-            newElement = _Element(Ob1Element.tid, min([Ob1Element.iUtils, Ob2Element.iUtils], key=lambda x: float(x)),
-                                  Ob2Element.rUtils)
-            _newFFListObject.addElement(newElement)
-
-        if periodlist and int(self._maxPer) >= max(periodlist):
-            _newFFListObject.isPeriodic = True
-        else:
-            _newFFListObject.isPeriodic = False
-        return _newFFListObject
+            eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)), ey.rUtils)
+            pxyUL.addElement(eXY)
+        return pxyUL
 
     def _findElementWithTID(self, uList, tid):
         """
         To find element with same tid as given
 
         :param uList:fuzzyList
         :type uList:FFI-List
@@ -663,153 +771,91 @@
                 first = mid + 1
             elif List[mid].tid > tid:
                 last = mid - 1
             else:
                 return List[mid]
         return None
 
-    def _WriteOut(self, prefix, prefixLen, _FFListObject, sumIUtil):
+    def _WriteOut(self, prefix, prefixLen, item, sumIUtil):
         """
         To Store the patten
 
         :param prefix: prefix of itemSet
         :type prefix: list
         :param prefixLen: length of prefix
         :type prefixLen: int
-        :param _FFListObject: the last item
-        :type _FFListObject: int
+        :param item: the last item
+        :type item: int
         :param sumIUtil: sum of utility of itemSet
         :type sumIUtil: float
         """
-        item = _FFListObject.item
         self._itemsCnt += 1
         res = ""
         for i in range(0, prefixLen):
             res += str(prefix[i]) + "." + str(self._mapItemRegions[prefix[i]]) + "\t"
         res += str(item) + "." + str(self._mapItemRegions.get(item))
         res1 = str(sumIUtil)
         self._finalPatterns[res] = res1
 
-        if _FFListObject.isPeriodic:
-            self._finalPeriodicPatterns[res] = res1
-
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
-        for a, b in self._finalPeriodicPatterns.items():
+        for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self._finalPeriodicPatterns
+        return self._finalPatterns
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self.oFile = outFile
-        keylist = (self._finalPatterns.keys())
         writer = open(self.oFile, 'w+')
-        for x in keylist:
-            patternsAndSupport = x.strip() + ":" + str(self._finalPatterns[x])
+        for x, y in self._finalPatterns.items():
+            patternsAndSupport = x.strip() + " : " + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
     def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Spatial Fuzzy Periodic-Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Spatial Fuzzy Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
-
-    def getPatternsAsDataframe(self):
-
-        """
-        :return: returning periodic frequent patterns in a dataframe
-        :rtype: pd.DataFrame
-        """
-
-        data = []
-        dataFrame = _ab._pd.DataFrame()
-        for a, b in self._finalPeriodicPatterns.items():
-            data.append([a, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataFrame
-
-    def generateLatexCode(self, result):
-
-        titles = result.columns.tolist()
-        titles.remove("minsup")
-        titles.remove("algorithm")
-        for i in range(0, len(titles)):
-            legendary = pd.unique(result[['algorithm']].values.ravel())
-            color = ['red', 'blue', 'green', 'black', 'yellow']
-            xaxis = result["minsup"].values.tolist()
-            yaxis = result[titles[i]].values.tolist()
-            algo = result["algorithm"].values.tolist()
-            x_label = "minsup"
-            filename = titles[i]
-            latexwriter = open(filename + "Latexfile.tex", "w")
-            latexwriter.write("")
-            latexwriter.write("\\begin{axis}[\n\txlabel={\\Huge{" + x_label + "}},")
-            latexwriter.write("\n\tylabel={\\Huge{" + titles[i] + "}},")
-            latexwriter.write("\n\txmin=" + str(min(xaxis)) + ", xmax=" + str(max(xaxis)) + ",")
-
-            for num in range(0, len(legendary)):
-                latexwriter.write("\n\\addplot+  [" + color[num] + "]\n\tcoordinates {\n")
-                for num2 in range(0, len(xaxis)):
-                    if (legendary[num] == algo[num2]):
-                        latexwriter.write("(" + str(xaxis[num2]) + "," + str(yaxis[num2]) + ")\n")
-                latexwriter.write("\t};   \\addlegendentry{" + legendary[num] + "}\n")
-                if (num + 1 == len(legendary)):
-                    latexwriter.write("\\end{axis}")
-        print("Latex file generated successfully")
-
-    def generateGraphs(result):
-
-        fig = px.line(result, x='minsup', y='patterns', color='algorithm', title='Patterns)', markers=True)
-        fig.show()
-        fig = px.line(result, x='minsup', y='runtime', color='algorithm', title='Runtime)', markers=True)
-        fig.show()
-        fig = px.line(result, x='minsup', y='memoryUSS', color='algorithm', title='MemoryUSS)', markers=True)
-        fig.show()
-        fig = px.line(result, x='minsup', y='memoryRSS', color='algorithm', title='MemoryRSS)', markers=True)
-        fig.show()
+        print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 7:
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
-            _ap = FGPFPMiner(_ab._sys.argv[1], _ab._sys.argv[2], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5],
-                             _ab._sys.argv[6])
+            _ap = FFSPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = FGPFPMiner(_ab._sys.argv[1], _ab._sys.argv[2], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+            _ap = FFSPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        print("Total number of Spatial Fuzzy Periodic Frequent  Patterns:", len(_ap.getPatterns()))
+        _ap.mine()
+        print("Total number of Spatial Fuzzy Frequent  Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS",  _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
-        _ap.save("outputfile.txt")
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
-
```

### Comparing `pami-2024.3.9.2/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/UVECLAT.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,606 +1,509 @@
-# F3PMiner algorithm discovers the fuzzy partial periodic patterns in quantitative Irregulat multiple timeseries databases.
-#
+# UVEclat is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
 #
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------
+# --------------------------------------------------------
+#
 #
-#     import PAMI.fuzzyPartialPeriodicPattern.basic.F3PMiner as alg
+#     from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
 #
-#     obj = alg.F3PMiner(iFile, minSup, sep)
+#     obj = alg.UVEclat(iFile, minSup)
 #
 #     obj.startMine()
 #
-#     fuzzyPartialPeriodicPatterns = obj.getPatterns()
+#     frequentPatterns = obj.getPatterns()
 #
-#     print("Total number of Fuzzy Partial Periodic Patterns:", len(fuzzyPartialPeriodicPatterns))
+#     print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #     obj.save(oFile)
 #
-#     Df = obj.getPatternInDataFrame()
+#     Df = obj.getPatternsAsDataFrame()
 #
 #     memUSS = obj.getMemoryUSS()
 #
 #     print("Total Memory in USS:", memUSS)
 #
 #     memRSS = obj.getMemoryRSS()
 #
 #     print("Total Memory in RSS", memRSS)
 #
 #     run = obj.getRuntime()
 #
 #     print("Total ExecutionTime in seconds:", run)
 #
 
-
 __copyright__ = """
-
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
-     This program is distributed in the hope that it will be useful,
-     but WITHOUT ANY WARRANTY; without even the implied warranty of
-     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-     GNU General Public License for more details.
-
-     You should have received a copy of the GNU General Public License
-     along with this program.  If not, see <https://www.gnu.org/licenses/>.
-
-     This program is free software: you can redistribute it and/or modify
-     it under the terms of the GNU General Public License as published by
-     the Free Software Foundation, either version 3 of the License, or
-     (at your option) any later version.
-
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
+     Copyright (C)  2021 Rage Uday Kiran
 """
 
-from PAMI.fuzzyPartialPeriodicPatterns.basic import abstract as _ab
-
-
-class _FFList:
-    """
-     A class represent a Fuzzy List of an element
-
-    :Attributes:
-
-         item: int
-             the item name
-         sumIUtil: float
-             the sum of utilities of an fuzzy item in database
-         sumRUtil: float
-             the sum of resting values of a fuzzy item in database
-         elements: list
-             a list of elements contain tid,Utility and resting values of element in each transaction
-
-    :Methods:
-
-        addElement(element)
-            Method to add an element to this fuzzy list and update the sums at the same time.
+import operator as _operator
+from PAMI.uncertainFrequentPattern.basic import abstract as _ab
 
-        printElement(e)
-            Method to print elements
 
-    """
-
-    def __init__(self, itemName):
-        self.item = itemName
-        self.sumIUtil = 0.0
-        self.elements = []
-
-    def addElement(self, element):
-        """
-        A Method that add a new element to FFList
-
-        :param element: an element to be added to FFList
-        :param element: Element
-        """
-        self.sumIUtil += element.iUtils
-        self.elements.append(element)
-
-    def printElement(self):
-        """
-        A method to print elements
-        """
-        for ele in self.elements:
-            print(ele.tid, ele.iUtils, ele.rUtils)
+_minSup = float()
+_finalPatterns = {}
 
 
-class _Element:
+class _Item:
     """
-    A class represents an Element of a fuzzy list
-
+    A class used to represent the item with probability in transaction of dataset
     :Attributes:
-
-        tid : int
-            keep tact of transaction id
-        iUtils: float
-            the utility of an fuzzy item in the transaction
-        rUtils : float
-            the  resting value of an fuzzy item in the transaction
-    """
-
-    def __init__(self, tid, iUtil):
-        self.tid = tid
-        self.iUtils = iUtil
-
-
-class _Pair:
-    """
-    A class to store item and it's quantity together
+        item : int or word
+            Represents the name of the item
+        probability : float
+            Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self):
-        self.item = 0
-        self.quantity = 0
+    def __init__(self, item, probability):
+        self.item = item
+        self.probability = probability
 
 
-class F3PMiner(_ab._fuzzyPartialPeriodicPatterns):
+class UVEclat(_ab._frequentPatterns):
     """
-    :Description:   F3PMiner algorithm discovers the fuzzy partial periodic patterns in quantitative Irregulat multiple timeseries databases.
-    
+    :Description: It is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
     :Reference:
-
+    Carson Kai-Sang Leung, Lijing Sun: "Equivalence class transformation based mining of frequent itemsets from uncertain data",
+    SAC '11: Proceedings of the 2011 ACM Symposium on Applied ComputingMarch, 2011, Pages 983–984,
+    https://doi.org/10.1145/1982185.1982399
     :Attributes:
-
-        iFile : string
-            Name of the input file to mine complete set of fuzzy spatial frequent patterns
-        oFile : string
-               Name of the oFile file to store complete set of fuzzy spatial frequent patterns
-        minSup : float
-            The user given minimum support
+        iFile : file
+            Name of the Input file or path of the input file
+        oFile : file
+            Name of the output file or path of the output file
+        minSup : float or int or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            However, the users can override their default separator.
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
         memoryRSS : float
-                To store the total amount of RSS memory consumed by the program
+            To store the total amount of RSS memory consumed by the program
         startTime:float
-               To record the start time of the mining process
+            To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
-        itemsCnt: int
-            To record the number of fuzzy spatial itemSets generated
-        mapItemsGSum: map
-            To keep track of G region values of items
-        mapItemsMidSum: map
-            To keep track of M region values of items
-        mapItemsHSum: map
-            To keep track of H region values of items
-        mapItemSum: map
-            To keep track of sum of Fuzzy Values of items
-        mapItemRegions: map
-            To Keep track of fuzzy regions of item
-        jointCnt: int
-            To keep track of the number of ffi-list that was constructed
-        BufferSize: int
-            represent the size of Buffer
-        itemBuffer list
-            to keep track of items in buffer
-
+        Database : list
+            To store the transactions of a database in list
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+        lno : int
+            To represent the total no of transaction
+        tree : class
+            To represents the Tree class
+        itemSetCount : int
+            To represents the total no of patterns
+        finalPatterns : dict
+            To store the complete patterns
     :Methods:
-
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
+        storePatternsInFile(oFile)
             Complete set of frequent patterns will be loaded in to a output file
-        getPatternsAsDataFrame()
+        getPatternsInDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value):
-            To convert the given user specified value
-        compareItems(o1, o2)
-            A Function that sort all ffi-list in ascending order of Support
-        F3PMining(prefix, prefixLen, FSFIM, minSup)
-            Method generate ffi from prefix
-        construct(px, py)
-            A function to construct Fuzzy itemSet from 2 fuzzy itemSets
-        findElementWithTID(uList, tid)
-            To find element with same tid as given
-        WriteOut(prefix, prefixLen, item, sumIUtil)
-            To Store the patten
-
-    **Executing the code on terminal :**
-    -------------------------------------
-        Format:
-            >>> python3 F3PMiner.py <inputFile> <outputFile> <minSup> <separator>
-        Examples:
-            >>> python3  F3PMiner.py sampleTDB.txt output.txt 6
-
-    **Sample run of importing the code:**
-    --------------------------------------
-
-        from PAMI.fuzzyPartialPeriodicPatterns import F3PMiner as alg
-
-        obj = alg.F3PMiner("input.txt", 2)
-
-        obj.startMine()
-
-        fuzzyPartialPeriodicPatterns = obj.getPatterns()
-
-        print("Total number of Fuzzy Frequent Patterns:", len(fuzzyPartialPeriodicPatterns))
-
-        obj.save("outputFile")
-
-        memUSS = obj.getMemoryUSS()
-
-        print("Total Memory in USS:", memUSS)
-
-        memRSS = obj.getMemoryRSS()
-
-        print("Total Memory in RSS", memRSS)
-
-        run = obj.getRuntime()
-
-        print("Total ExecutionTime in seconds:", run)
-
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
+        frequentOneItem()
+            Extracts the one-length frequent patterns from database
+    **Methods to execute code on terminal**
+    ------------------------------------------
+            Format:
+                      >>> python3 uveclat.py <inputFile> <outputFile> <minSup>
+            Example:
+                      >>>  python3 uveclat.py sampleTDB.txt patterns.txt 3
+                      .. note:: minSup  will be considered in support count or frequency
+    **Importing this algorithm into a python program**
+    ---------------------------------------------------
+    .. code-block:: python
+            from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
+            obj = alg.UVEclat(iFile, minSup)
+            obj.startMine()
+            frequentPatterns = obj.getPatterns()
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            obj.save(oFile)
+            Df = obj.getPatternsAsDataFrame()
+            memUSS = obj.getmemoryUSS()
+            print("Total Memory in USS:", memUSS)
+            memRSS = obj.getMemoryRSS()
+            print("Total Memory in RSS", memRSS)
+            run = obj.getRuntime()
+            print("Total ExecutionTime in seconds:", run)
     **Credits:**
-    -------------
-        The complete program was written by PALLA Likhitha under the supervision of Professor Rage Uday Kiran.
+    ---------------
+         The complete program was written by   P.Likhitha    under the supervision of Professor Rage Uday Kiran.
     """
-    
     _startTime = float()
     _endTime = float()
     _minSup = str()
-    _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
+    _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _sep = "\t"
-
-    def __init__(self, iFile, minSup, sep="\t"):
-        super().__init__(iFile, minSup, sep)
-        self._startTime = 0
-        self._endTime = 0
-        self._itemsCnt = 0
-        self._mapItemSum = {}
-        self._mapItemRegions = {}
-        self._joinsCnt = 0
-        self._BufferSize = 200
-        self._itemSetBuffer = []
-        self._transactions = []
-        self._fuzzyValues = []
-        self._ts = []
-        self._finalPatterns = {}
-        self._dbLen = 0
-
-    def _compareItems(self, o1, o2):
-        """
-        A Function that sort all ffi-list in ascending order of Support
-        """
-        compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
-        if compare == 0:
-            if o1.item < o2.item:
-                return -1
-            elif o1.item > o2.item:
-                return 1
-            else:
-                return 0
-        else:
-            return compare
+    _Database = []
+    _tidList = {}
+    _rank = {}
 
-    def _convert(self, value):
+    def _creatingItemSets(self):
         """
-        To convert the given user specified value
-
-        :param value: user specified value
-        :return: converted value
+        Scans the dataset
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (self._dbLen * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._dbLen * value)
-            else:
-                value = int(value)
-        return value
-
-    def _creatingItemsets(self):
-        """
-        Storing the complete transactions of the database/input file in a database variable
-        """
-        self._transactions, self._fuzzyValues, self._Database, self._ts = [], [], [], []
+        self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._transactions = self._iFile['Transactions'].tolist()
-            if 'fuzzyValues' in i:
-                self._fuzzyValues = self._iFile['fuzzyValues'].tolist()
+                self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
+                self._Database.append(tr)
+
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
+                    line.strip()
                     line = line.decode("utf-8")
-                    line = line.split("\n")[0]
-                    parts = line.split(":")
-                    parts[0] = parts[0].strip()
-                    parts[2] = parts[2].strip()
-                    items = parts[0].split(self._sep)
-                    quantities = parts[2].split(self._sep)
-                    self._transactions.append([x for x in items])
-                    self._fuzzyValues.append([x for x in quantities])
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    tr = []
+                    for i in temp:
+                        i1 = i.index('(')
+                        i2 = i.index(')')
+                        item = i[0:i1]
+                        probability = float(i[i1 + 1:i2])
+                        product = _Item(item, probability)
+                        tr.append(product)
+                    self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line = line.strip()
-                            parts = line.split(":")
-                            parts[0] = parts[0].strip()
-                            parts[1] = parts[1].strip()
-                            parts[2] = parts[2].strip()
-                            times = parts[0].split(self._sep)
-                            items = parts[1].split(self._sep)
-                            quantities = parts[2].split(self._sep)
-                            #print(times, items, quantities)
-                            _time = [x for x in times if x]
-                            items = [x for x in items if x]
-                            quantities = [float(x) for x in quantities if x]
-                            tempList = []
-                            for k in range(len(_time)):
-                                ite = "(" + _time[k] + "," + items[k] + ")"
-                                tempList.append(ite)
-                            self._ts.append([x for x in times])
-                            self._transactions.append([x for x in tempList])
-                            self._fuzzyValues.append([x for x in quantities])
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            tr = []
+                            for i in temp:
+                                i1 = i.index('(')
+                                i2 = i.index(')')
+                                item = i[0:i1]
+                                probability = float(i[i1 + 1:i2])
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    quit()
+
+    def _frequentOneItem(self):
+        """
+        takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        """
+
+        mapSupport = {}
+        k = 0
+        for i in self._Database:
+            k += 1
+            for j in i:
+                if j.item not in mapSupport:
+                    mapSupport[str(j.item)] = j.probability
+                    self._tidList[str(j.item)] = {k: j.probability}
+                else:
+                    mapSupport[str(j.item)] += j.probability
+                    self._tidList[str(j.item)].update({k: j.probability})
+        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
+        plist = dict( sorted(mapSupport.items(), key=_operator.itemgetter(1),reverse=True))
+        return list(plist.keys())
+
+    @staticmethod
+    def _check(i, x):
+        """
+        To check the presence of item or pattern in transaction
+        :param x: it represents the pattern
+        :type x : list
+        :param i : represents the uncertain self.Database
+        :type i : list
+        """
+
+        # This method taken a transaction as input and returns the tree
+        for m in x:
+            k = 0
+            for n in i:
+                if m == n.item:
+                    k += 1
+            if k == 0:
+                return 0
+        return 1
+
+    @staticmethod
+    def _convert(value):
+        """
+        To convert the type of user specified minSup value
+        :param value: user specified minSup value
+        :return: converted type minSup value
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = float(value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+            else:
+                value = int(value)
+        return value
+
+    def _removeFalsePositives(self):
+        """
+        To remove the false positive patterns generated in frequent patterns
+        :return: patterns with accurate probability
+        """
+        global _finalPatterns
+        periods = {}
+        for i in self._Database:
+            for x, y in _finalPatterns.items():
+                if len(x) == 1:
+                    periods[x] = y
+                else:
+                    s = 1
+                    check = self._check(i, x)
+                    if check == 1:
+                        for j in i:
+                            if j.item in x:
+                                s *= j.probability
+                        if x in periods:
+                            periods[x] += s
+                        else:
+                            periods[x] = s
+        for x, y in periods.items():
+            if y >= self._minSup:
+                sample = str()
+                for i in x:
+                    sample = sample + i + "\t"
+                self._finalPatterns[sample] = y
+
+    @staticmethod
+    def _Intersection(tidSetx, tidSetY):
+        """
+        This function is used to find the intersection
+        :param tidSetx: the timestamp of a patterns
+        :type tidSetx: dict
+        :param tidSetY: the timestamp of a patterns
+        :type tidSetY: dict
+        """
+        tids = []
+        support = []
+        tidDict = {}
+        for x, y in tidSetx.items():
+            for x1, y1 in tidSetY.items():
+                if x == x1:
+                    tids.append(x)
+                    support.append(y * y1)
+                    tidDict.update({x: y * y1})
+        return tidDict
+
+    def _calculateExpSup(self, tidList):
+        """
+        This function is used to calculate support of tidList
+        :param tidList: timestamp of a list.
+        :type tidList: List
+        """
+        return sum(tidList.values())
+
+    def _save(self, prefix, suffix, tidSetI):
+        """
+        Saves the patterns that satisfy the periodic frequent property.
+        :param prefix: the prefix of a pattern
+        :type prefix: list
+        :param suffix: the suffix of a patterns
+        :type suffix: list
+        :param tidSetI: the timestamp of a patterns
+        :type tidSetI: dict
+        """
+
+        global _finalPatterns
+        if prefix is None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        val = self._calculateExpSup(tidSetI)
+        _finalPatterns[tuple(prefix)] = val
+
+    def _Generation(self, prefix, itemSets, tidSets):
+        """
+        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+        :param prefix:  main equivalence prefix
+        :type prefix: periodic-frequent item or pattern
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
+        :type itemSets: list
+        :param tidSets: timestamps of the items in the argument itemSets
+        :type tidSets: list
+        """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
+            return
+        for i in range(len(itemSets)):
+            itemI = itemSets[i]
+            if itemI is None:
+                continue
+            tidSetI = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemI]
+            for j in range(i + 1, len(itemSets)):
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = self._Intersection(tidSetI, tidSetJ)
+                if self._calculateExpSup(y) >= self._minSup:
+                    classItemSets.append(itemJ)
+                    classTidSets.append(y)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetI)
 
     def startMine(self):
         """
-        fuzzy-Frequent pattern mining process will start from here
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
+        global _minSup
         self._startTime = _ab._time.time()
-        self._creatingItemsets()
-        for line in range(len(self._transactions)):
-            times = self._ts[line]
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            self._dbLen += 1
-            for i in range(0, len(items)):
-                item = items[i]
-                if item in self._mapItemSum:
-                    self._mapItemSum[item] += quantities[i]
-                else:
-                    self._mapItemSum[item] = quantities[i]
-        listOfffilist = []
-        mapItemsToFFLIST = {}
-        #self._minSup = float(self._minSup)
+        self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
-        minSup = self._minSup
-        for item1 in self._mapItemSum.keys():
-            item = item1
-            # print(type(self._mapItemSum[item]))
-            if self._mapItemSum[item] >= self._minSup:
-                fuList = _FFList(item)
-                mapItemsToFFLIST[item] = fuList
-                listOfffilist.append(fuList)
-        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        tid = 0
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            revisedTransaction = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                pair.quantity = quantities[i]
-                item = pair.item
-                if self._mapItemSum[item] >= self._minSup:
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                if mapItemsToFFLIST.get(pair.item) is not None:
-                    FFListOfItem = mapItemsToFFLIST[pair.item]
-                    element = _Element(tid, pair.quantity)
-                    FFListOfItem.addElement(element)
-            tid += 1
-        self._F3PMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
+        _minSup = self._minSup
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i+1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = self._Intersection(tidSetI, tidSetJ)
+                if self._calculateExpSup(y1) >= self._minSup:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+            self._save(None, itemSetX, tidSetI)
+        self._removeFalsePositives()
+        print("Frequent patterns were generated from uncertain databases successfully using PUF algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
         self._memoryRSS = float()
+        self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def _F3PMining(self, prefix, prefixLen, FSFIM, minSup):
-        """
-        Generates ffi from prefix
-
-        :param prefix: the prefix patterns of ffi
-        :type prefix: len
-        :param prefixLen: the length of prefix
-        :type prefixLen: int
-        :param FSFIM: the Fuzzy list of prefix itemSets
-        :type FSFIM: list
-        :param minSup: the minimum support of
-        :type minSup:int
-        """
-        for i in range(0, len(FSFIM)):
-            X = FSFIM[i]
-            exULs = []
-            if X.sumIUtil >= minSup:
-                self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
-                for j in range(i + 1, len(FSFIM)):
-                    Y = FSFIM[j]
-                    exULs.append(self._construct(X, Y))
-                    self._joinsCnt += 1
-                self._itemSetBuffer.insert(prefixLen, X.item)
-                self._F3PMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup)
-
     def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
-       """
-        return self._memoryRSS
-
-    def getRuntime(self):
         """
-        Calculating the total amount of runtime taken by the mining process
 
+        return self._memoryRSS
 
+    def getRuntime(self):
+        """Calculating the total amount of runtime taken by the mining process
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
-       """
-        return self._endTime - self._startTime
-
-    def _construct(self, px, py):
         """
-        A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
 
-        :param px:the itemSet px
-        :type px:ffi-List
-        :param py:itemSet py
-        :type py:ffi-List
-        :return :the itemSet of pxy(px and py)
-        :rtype :ffi-List
-        """
-        pxyUL = _FFList(py.item)
-        for ex in px.elements:
-            ey = self._findElementWithTID(py, ex.tid)
-            if ey is None:
-                continue
-            eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)))
-            pxyUL.addElement(eXY)
-        return pxyUL
-
-    def _findElementWithTID(self, uList, tid):
-        """
-        To find element with same tid as given
-
-        :param uList: fuzzyList
-        :type uList: ffi-List
-        :param tid: transaction id
-        :type tid: int
-        :return: element  tid as given
-        :rtype: element if exit or None
-        """
-        List = uList.elements
-        first = 0
-        last = len(List) - 1
-        while first <= last:
-            mid = (first + last) >> 1
-            if List[mid].tid < tid:
-                first = mid + 1
-            elif List[mid].tid > tid:
-                last = mid - 1
-            else:
-                return List[mid]
-        return None
-
-    def _WriteOut(self, prefix, prefixLen, item, sumIUtil):
-        """
-        To Store the patten
-
-        :param prefix: prefix of itemSet
-        :type prefix: list
-        :param prefixLen: length of prefix
-        :type prefixLen: int
-        :param item: the last item
-        :type item: int
-        :param sumIUtil: sum of utility of itemSet
-        :type sumIUtil: float
-
-        """
-        self._itemsCnt += 1
-        res = ""
-        for i in range(0, prefixLen):
-            res += str(prefix[i]) + "\t"
-        res += str(item)
-        res1 = str(sumIUtil)
-        self._finalPatterns[res] = res1
+        return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """
-        Storing final frequent patterns in a dataframe
-
+        """Storing final frequent patterns in a dataframe
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataFrame
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataframe
 
-    def getPatterns(self):
+    def save(self, oFile):
+        """Complete set of frequent patterns will be loaded in to an output file
+        :param oFile: name of the output file
+        :type oFile: csv file
         """
-        Function to send the set of frequent patterns after completion of the mining process
+        self.oFile = oFile
+        writer = open(self.oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            s1 = x.strip() + ":" + str(y)
+            writer.write("%s \n" % s1)
 
+    def getPatterns(self):
+        """ Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def save(self, outFile):
-        """
-        Complete set of frequent patterns will be loaded in to a output file
-
-        :param outFile: name of the output file
-        :type outFile: file
-        """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y)
-            writer.write("%s \n" % patternsAndSupport)
-
     def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Fuzzy Partial Periodic Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = F3PMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = F3PMiner(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        print("Total number of Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        _ap.printResults()
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.3.9.2/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py` & `pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/fuzzyPeriodicFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py` & `pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,647 +1,603 @@
-# Fuzzy Periodic Frequent Pattern Miner is desired to find all fuzzy periodic frequent patterns which is
-# on-trivial and challenging problem to its huge search space.we are using efficient pruning
-# techniques to reduce the search space.
+# STEclat is one of the fundamental algorithm to discover geo refereneced partial periodic-frequent patterns in a transactional database.
 #
-# Sample run of importing the code:
-#     -------------------------------
+# **Importing this algorithm into a python program**
+# --------------------------------------------------------
 #
-#         from PAMI.fuzzyPeriodicFrequentPattern.basic import FPFPMiner as alg
+#             import PAMI.georeferencedPartialPeriodicPattern.STEclat as alg
 #
-#         obj =alg.FPFPMiner("input.txt",2,3)
+#             obj = alg.STEclat("sampleTDB.txt", "sampleN.txt", 3, 4)
 #
-#         obj.startMine()
+#             obj.mine()
 #
-#         periodicFrequentPatterns = obj.getPatterns()
+#             partialPeriodicSpatialPatterns = obj.getPatterns()
 #
-#         print("Total number of Fuzzy Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Periodic Spatial Frequent Patterns:", len(partialPeriodicSpatialPatterns))
 #
-#         obj.save("output.txt")
+#             obj.save("outFile")
 #
-#         memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#         print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#         memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#         print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#         run = obj.getRuntime()
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
 #
-#         print("Total ExecutionTime in seconds:", run)
-
 
-__copyright__ = """(Copyright (C)  2021 Rage Uday Kiran
 
-     This program is free software: you can redistribute it and/or modify
-     it under the terms of the GNU General Public License as published by
-     the Free Software Foundation, either version 3 of the License, or
-     (at your option) any later version.
 
-     This program is distributed in the hope that it will be useful,
-     but WITHOUT ANY WARRANTY; without even the implied warranty of
-     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-     GNU General Public License for more details.
 
-     You should have received a copy of the GNU General Public License
-     along with this program.  If not, see <https://www.gnu.org/licenses/>.
+__copyright__ = """
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-)"""
-
-
-from PAMI.fuzzyPeriodicFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
-
+     Copyright (C)  2021 Rage Uday Kiran
 
-class _FFList:
-    """
-    A class represent a Fuzzy List of an element
-    :Attributes:
+"""
 
-        item: int
-            the item name
-        sumLUtil: float
-            the sum of utilities of a fuzzy item in database
-        sumRUtil: float
-            the sum of resting values of a fuzzy item in database
-        elements: list
-            list of elements contain tid,Utility and resting values of element in each transaction
-        maxPeriod: int
-            it represents the max period of a item
 
-    :Methods:
+from PAMI.georeferencedPartialPeriodicPattern.basic import abstract as _ab
+from deprecated import deprecated
 
-        addElement(element)
-            Method to add an element to this fuzzy list and update the sums at the same time.
-        printElement(e)
-            Method to print elements
 
+class STEclat(_ab._partialPeriodicSpatialPatterns):
     """
+    :Description:   STEclat is one of the fundamental algorithm to discover georefereneced partial periodic-frequent patterns in a transactional database.
 
-    def __init__(self, itemName: str) -> None:
-        self.item = itemName
-        self.sumLUtil = 0.0
-        self.sumRUtil = 0.0
-        self.elements = []
-        self.maxPeriod = 0
-
-    def addElement(self, element) -> None:
-        """
-        A Method that add a new element to FFList
-
-        :param element: an element to be added to FFList
-        :type element: Element
-        """
-        self.sumLUtil += element.lUtils
-        self.sumRUtil += element.rUtils
-        self.elements.append(element)
-        self.maxPeriod = max(self.maxPeriod, element.period)
-
-    def printElement(self) -> None:
-        """
-            A Method to Print elements in the FFList
-        """
-        for ele in self.elements:
-            print(ele.tid, ele.lUtils, ele.rUtils, ele.period)
-
-
-class _Element:
-    """
-        A class represents an Element of a fuzzy list
-
-        :Attributes:
-
-        tid : int
-            keep tact of transaction id
-        lUtils: float
-            the utility of a fuzzy item in the transaction
-        rUtils : float
-            the resting value of a fuzzy item in the transaction
-        period: int
-            represent the period of the element
-    """
-
-    def __init__(self, tid: int, iUtil: float, rUtil: float, period: int) -> None:
-        self.tid = tid
-        self.lUtils = iUtil
-        self.rUtils = rUtil
-        self.period = period
-
-
-class _Pair:
-    """
-    A class to store item name and quantity together.
-    """
-
-    def __init__(self) -> None:
-        self.item = 0
-        self.quantity = 0
-
-
-class FPFPMiner(_ab._fuzzyPeriodicFrequentPatterns):
-    """
-    :Description:   Fuzzy Periodic Frequent Pattern Miner is desired to find all fuzzy periodic frequent patterns which is
-                    on-trivial and challenging problem to its huge search space.we are using efficient pruning
-                    techniques to reduce the search space.
-
-    :Reference:   R. U. Kiran et al., "Discovering Fuzzy Periodic-Frequent Patterns in Quantitative Temporal Databases,"
-                  2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), Glasgow, UK, 2020, pp.
-                 1-8, doi: 10.1109/FUZZ48607.2020.9177579.
+    :Reference:   R. Uday Kiran, C. Saideep, K. Zettsu, M. Toyoda, M. Kitsuregawa and P. Krishna Reddy,
+                 "Discovering Partial Periodic Spatial Patterns in Spatiotemporal Databases," 2019 IEEE International
+                  Conference on Big Data (Big Data), 2019, pp. 233-238, doi: 10.1109/BigData47090.2019.9005693.
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Geo-referenced Partial Periodic patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Geo-referenced Partial Periodic patterns
+    :param  minPS: int or float or str :
+                   The user can specify minPS either in count or proportion of database size. If the program detects the data type of minPS is integer, then it treats minPS is expressed in count. Otherwise, it will be treated as float.
+    :param maxIAT: int or float or str :
+                   The user can specify maxIAT either in count or proportion of database size. If the program detects the data type of maxIAT is integer, then it treats maxIAT is expressed in count. Otherwise, it will be treated as float.
+    :param nFile: str :
+                   Name of the input file to mine complete set of Geo-referenced Partial Periodic patterns
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
-        iFile : file
-            Name of the input file to mine complete set of fuzzy spatial frequent patterns
-        oFile : file
-               Name of the oFile file to store complete set of fuzzy spatial frequent patterns
-        minSup : float
-            The user given support
-        period: int
-            periodicity of an element
-        memoryRSS : float
-                To store the total amount of RSS memory consumed by the program
+        iFile : str
+            Input file name or path of the input file
+        nFile: str:
+           Name of Neighbourhood file name
+        maxIAT: float or int or str
+            The user can specify maxIAT either in count or proportion of database size.
+            If the program detects the data type of maxIAT is integer, then it treats maxIAT is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxIAT=10 will be treated as integer, while maxIAT=10.0 will be treated as float
+        minPS: float or int or str
+            The user can specify minPS either in count or proportion of database size.
+            If the program detects the data type of minPS is integer, then it treats minPS is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minPS=10 will be treated as integer, while minPS=10.0 will be treated as float
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            However, the users can override their default separator.
         startTime:float
-               To record the start time of the mining process
+            To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
-        itemsCnt: int
-            To record the number of fuzzy spatial itemSets generated
-        mapItemsLowSum: map
-            To keep track of low region values of items
-        mapItemsMidSum: map
-            To keep track of middle region values of items
-        mapItemsHighSum: map
-            To keep track of high region values of items
-        mapItemSum: map
-            To keep track of sum of Fuzzy Values of items
-        mapItemRegions: map
-            To Keep track of fuzzy regions of item
-        jointCnt: int
-            To keep track of the number of FFI-list that was constructed
-        BufferSize: int
-            represent the size of Buffer
-        itemBuffer list
-            to keep track of items in buffer
-        maxTID: int
-            represent the maximum tid of the database
-        lastTIDs: map
-            represent the last tid of fuzzy items
-        itemsToRegion: map
-            represent items with respective regions
+        finalPatterns: dict
+            Storing the complete set of patterns in a dictionary variable
+        oFile : str
+            Name of the output file to store complete set of frequent patterns
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
+        Database : list
+            To store the complete set of transactions available in the input database/file
+
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
-        getPatternsAsDataFrame()
+        getPatternsAsDataFrames()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingItemSets(iFileName)
+            Storing the complete transactions of the database/input file in a database variable
+        frequentOneItem()
+            Generating one frequent patterns
         convert(value):
             To convert the given user specified value
-        FSFIMining( prefix, prefixLen, fsFim, minSup)
-            Method generate FFI from prefix
-        construct(px, py)
-            A function to construct Fuzzy itemSet from 2 fuzzy itemSets
-        findElementWithTID(UList, tid)
-            To find element with same tid as given
-        WriteOut(prefix, prefixLen, item, sumIUtil,period)
-            To Store the patten
+        getNeighbourItems(keySet):
+            A function to get common neighbours of a itemSet
+         mapNeighbours(file):
+            A function to map items to their neighbours
 
     **Executing the code on terminal :**
-    --------------------------------------
+    ----------------------------------------
 
-        Format:
-            >>> python3 FPFPMiner.py <inputFile> <outputFile> <minSup> <maxPer> <sep>
+    .. code-block:: console
 
-        Examples:
-            >>> python3  FPFPMiner.py sampleTDB.txt output.txt 2 3 (minSup and maxPer will be considered in support count or frequency)
+      Format:
 
-    **Sample run of importing the code:**
-    --------------------------------------
+      (.venv) $ python3 STEclat.py <inputFile> <outputFile> <neighbourFile>  <minPS>  <maxIAT>
 
-        from PAMI.fuzzyPeriodicFrequentPattern.basic import FPFPMiner as alg
+      Example Usage:
 
-        obj =alg.FPFPMiner("input.txt",2,3)
+      (.venv) $ python3 STEclat.py sampleTDB.txt output.txt sampleN.txt 0.2 0.5
 
-        obj.startMine()
+    .. note:: maxIAT & minPS will be considered in percentage of database transactions
 
-        periodicFrequentPatterns = obj.getPatterns()
 
-        print("Total number of Fuzzy Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+    **Sample run of importing the code :**
+    --------------------------------------
+    .. code-block:: python
+    
+            import PAMI.georeferencedPartialPeriodicPattern.STEclat as alg
 
-        obj.save("output.txt")
+            obj = alg.STEclat("sampleTDB.txt", "sampleN.txt", 3, 4)
 
-        memUSS = obj.getMemoryUSS()
+            obj.mine()
 
-        print("Total Memory in USS:", memUSS)
+            partialPeriodicSpatialPatterns = obj.getPatterns()
 
-        memRSS = obj.getMemoryRSS()
+            print("Total number of Periodic Spatial Frequent Patterns:", len(partialPeriodicSpatialPatterns))
 
-        print("Total Memory in RSS", memRSS)
+            obj.save("outFile")
 
-        run = obj.getRuntime()
+            memUSS = obj.getMemoryUSS()
 
-        print("Total ExecutionTime in seconds:", run)
+            print("Total Memory in USS:", memUSS)
 
-    **Credits:**
-    --------------
-            The complete program was written by Sai Chitra.B under the supervision of Professor Rage Uday Kiran.
+            memRSS = obj.getMemoryRSS()
+
+            print("Total Memory in RSS", memRSS)
+
+            run = obj.getRuntime()
 
+            print("Total ExecutionTime in seconds:", run)
+
+    **Credits:**
+    -------------
+        The complete program was written by P. Likhitha under the supervision of Professor Rage Uday Kiran.
     """
+
+    _maxIAT = " "
+    _minPS = " "
     _startTime = float()
     _endTime = float()
-    _minSup = float()
-    _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
+    _nFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _sep = " "
     _Database = []
-    _transactions = []
-    _fuzzyValues = []
-    _ts = []
-
-    def __init__(self, iFile: Union[str, _ab._pd.DataFrame], minSup: Union[int, float], period: Union[int, float], sep: str="\t") -> None:
-        super().__init__(iFile, minSup, period, sep)
-        self._oFile = ""
-        self._BufferSize = 200
-        self._itemSetBuffer = []
-        self._mapItemSum = {}
-        self._finalPatterns = {}
-        self._joinsCnt = 0
-        self._itemsCnt = 0
-        self._startTime = float()
-        self._endTime = float()
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._dbLen = 0
+    _sep = "\t"
+    _lno = 0
+
+    def __init__(self, iFile, nFile, minPS, maxIAT, sep="\t"):
+        super().__init__(iFile, nFile, minPS, maxIAT,  sep)
+        self._NeighboursMap = {}
 
-    def _compareItems(self, o1, o2) -> int:
+    def _creatingItemSets(self):
         """
-        A Function that sort all FFI-list in ascending order of Support
+        Storing the complete transactions of the database/input file in a database variable
         """
-        compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
-        if compare == 0:
-            return int(o1.item) - int(o2.item)
-        else:
-            return compare
+        self._Database = []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
+            data, ts = [], []
+            if self._iFile.empty:
+                print("its empty..")
+            i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
+            if 'Transactions' in i:
+                data = self._iFile['Transactions'].tolist()
+            for i in range(len(data)):
+                tr = [ts[i][0]]
+                tr = tr + data[i]
+                self._Database.append(tr)
+
+        if isinstance(self._iFile, str):
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._Database.append(temp)
+            else:
+                try:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._Database.append(temp)
+                except IOError:
+                    print("File Not Found")
+                    quit()
+
+    # function to get frequent one pattern
+    def _frequentOneItem(self):
+        """
+        Generating one frequent patterns
+        """
+        self._tidList = {}
+        self._mapSupport = {}
+        self._maxIAT = self._convert(self._maxIAT)
+        for line in self._Database:
+            s = line
+            n = int(s[0])
+            for i in range(1, len(s)):
+                si = s[i]
+                if self._mapSupport.get(si) is None:
+                    self._mapSupport[si] = [0, n]
+                    self._tidList[si] = [n]
+                else:
+                    lp = n - self._mapSupport[si][1]
+                    if lp <= self._maxIAT:
+                        self._mapSupport[si][0] += 1
+                    self._mapSupport[si][1] = n
+                    self._tidList[si].append(n)
+        self._minPS = self._convert(self._minPS)
+        self._mapSupport = {k: v[0] for k, v in self._mapSupport.items() if v[0] >= self._minPS}
+        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        return plist
 
-    def _convert(self, value) -> float:
+    def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
+
+        :type value: int or float or str
+
         :return: converted value
+
+        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (self._dbLen * value)
+            value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
-                value = (self._dbLen * value)
+                value = float(value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _creatingItemSets(self) -> None:
+    def _getPeriodicSupport(self, timeStamps):
         """
-        Storing the complete transactions of the database/input file in a database variable
+        calculates the support and periodicity with list of timestamps
 
+        :param timeStamps: timestamps of a pattern
+        :type timeStamps: list
         """
-        data, self._transactions, self._fuzzyValues, ts = [], [], [], []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            if self._iFile.empty:
+        timeStamps.sort()
+        per = 0
+        for i in range(len(timeStamps) - 1):
+            j = i + 1
+            if abs(timeStamps[j] - timeStamps[i]) <= self._maxIAT:
+                per += 1
+        return per
+
+    def _save(self, prefix, suffix, tidSetX):
+        """
+        Saves the patterns that satisfy the periodic frequent property.
+
+        :param prefix: the prefix of a pattern
+        :type prefix: list or None
+        :param suffix: the suffix of a patterns
+        :type suffix: list
+        :param tidSetX: the timestamp of a patterns
+        :type tidSetX: list
+
+        """
+        if prefix is None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        val = self._getPeriodicSupport(tidSetX)
+        if val >= self._minPS:
+            self._finalPatterns[tuple(prefix)] = val
+
+    def _Generation(self, prefix, itemSets, tidSets):
+        """
+        Generates the patterns that satisfy the periodic frequent property.
+
+        :param prefix: the prefix of a pattern
+        :type prefix: list or None
+        :param itemSets: the item sets of a patterns
+        :type itemSets: list
+        :param tidSets: the timestamp of a patterns
+        :type tidSets: list
+        """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidi = tidSets[0]
+            self._save(prefix, [i], tidi)
+            return
+        for i in range(len(itemSets)):
+            itemI = itemSets[i]
+            if itemI is None:
+                continue
+            tidSetX = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemI]
+            for j in range(i + 1, len(itemSets)):
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = list(set(tidSetX).intersection(tidSetJ))
+                val = self._getPeriodicSupport(y)
+                if val >= self._minPS:
+                    classItemSets.append(itemJ)
+                    classTidSets.append(y)
+            newprefix = list(set(itemSetX)) + prefix
+            self._Generation(newprefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetX)
+
+    def _getNeighbourItems(self, keySet):
+        """
+        A function to get Neighbours of an item
+
+        :param keySet:itemSet
+        :type keySet:str or tuple
+        :return: set of common neighbours
+        :rtype:set
+        """
+        itemNeighbours = self._NeighboursMap.keys()
+        if isinstance(keySet, str):
+            if self._NeighboursMap.get(keySet) is None:
+                return []
+            itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(keySet))))
+        if isinstance(keySet, tuple):
+            keySet = list(keySet)
+            for j in range(0, len(keySet)):
+                i = keySet[j]
+                itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(i))))
+        return itemNeighbours
+
+    def mapNeighbours(self):
+        """
+        A function to map items to their Neighbours
+        """
+        self._NeighboursMap = {}
+        if isinstance(self._nFile, _ab._pd.DataFrame):
+            data = []
+            if self._nFile.empty:
                 print("its empty..")
-            i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                self._ts = self._iFile['TS'].tolist()
-            if 'Transactions' in i:
-                self._transactions = self._iFile['Transactions'].tolist()
-            if 'fuzzyValues' in i:
-                self._fuzzyValues = self._iFile['fuzzyValues'].tolist()
-        if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
-                count = 0
+            i = self._nFile.columns.values.tolist()
+            if 'Neighbours' in i:
+                data = self._nFile['Neighbours'].tolist()
+            for i in data:
+                self._NeighboursMap[i[0]] = i[1:]
+        if isinstance(self._nFile, str):
+            if _ab._validators.url(self._nFile):
+                data = _ab._urlopen(self._nFile)
                 for line in data:
+                    line.strip()
                     line = line.decode("utf-8")
-                    line = line.split("\n")[0]
-                    parts = line.split(":")
-                    parts[0] = parts[0].strip()
-                    parts[1] = parts[1].strip()
-                    items = parts[0].split(self._sep)
-                    quantities = parts[1].split(self._sep)
-                    self._ts.append(int(items[0]))
-                    self._transactions.append([x for x in items[1:]])
-                    self._fuzzyValues.append([float(x) for x in quantities])
-                    count += 1
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._NeighboursMap[temp[0]] = temp[1:]
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
-                        count = 0
+                    with open(self._nFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line = line.split("\n")[0]
-                            parts = line.split(":")
-                            parts[0] = parts[0].strip()
-                            parts[1] = parts[1].strip()
-                            items = parts[0].split(self._sep)
-                            quantities = parts[1].split(self._sep)
-                            self._ts.append(int(items[0]))
-                            self._transactions.append([x for x in items[1:]])
-                            self._fuzzyValues.append([float(x) for x in quantities])
-                            count += 1
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._NeighboursMap[temp[0]] = temp[1:]
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def startMine(self) -> None:
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
         """
-        Fuzzy periodic Frequent pattern mining process will start from here
+        Frequent pattern mining process will start from here
         """
-        maxTID = 0
-        lastTIDs = {}
+
+        # global items_sets, endTime, startTime
         self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
         self._creatingItemSets()
+        #self._minSup = self._convert(self._minSup)
+        self.mapNeighbours()
         self._finalPatterns = {}
-        tid = int()
-        for line in range(len(self._transactions)):
-            tid = int(self._ts[line])
-            self._dbLen += 1
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            if tid < maxTID:
-                maxTID = tid
-            for i in range(0, len(items)):
-                item = items[i]
-                if item in self._mapItemSum:
-                    self._mapItemSum[item] += quantities[i]
-                else:
-                    self._mapItemSum[item] = quantities[i]
-        listOfFFIList = []
-        mapItemsToFFLIST = {}
-        # self._minSup = self._convert(self._minSup)
-        self._minSup = float(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        for item1 in self._mapItemSum.keys():
-            item = item1
-            if self._mapItemSum[item] >= self._minSup:
-                fUList = _FFList(item)
-                k = tuple([item])
-                mapItemsToFFLIST[k] = fUList
-                listOfFFIList.append(fUList)
-                lastTIDs[item] = tid
-        listOfFFIList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        for line in range(len(self._transactions)):
-            tid = int(self._ts[line])
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            revisedTransaction = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                item = pair.item
-                pair.quantity = quantities[i]
-                if self._mapItemSum[item] >= self._minSup:
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i - 1, -1):
-                    remainUtil += revisedTransaction[j].quantity
-                if pair.quantity > remainUtil:
-                    remainingUtility = pair.quantity
-                else:
-                    remainingUtility = remainUtil
-                if mapItemsToFFLIST.get(tuple([pair.item])) is not None:
-                    FFListOfItem = mapItemsToFFLIST[tuple([pair.item])]
-                    if len(FFListOfItem.elements) == 0:
-                        element = _Element(tid, pair.quantity, remainingUtility, 0)
-                    else:
-                        if lastTIDs[pair.item] == tid:
-                            element = _Element(tid, pair.quantity, remainingUtility, maxTID - tid)
-                        else:
-                            lastTid = FFListOfItem.elements[-1].tid
-                            curPer = tid - lastTid
-                            element = _Element(tid, pair.quantity, remainingUtility, curPer)
-                    FFListOfItem.addElement(element)
-        self._FPFPMining(self._itemSetBuffer, 0, listOfFFIList)
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemX = plist[i]
+            tidSetX = self._tidList[itemX]
+            itemSetX = [itemX]
+            itemSets = []
+            tidSets = []
+            neighboursItems = self._getNeighbourItems(plist[i])
+            for j in range(i + 1, len(plist)):
+                if not plist[j] in neighboursItems:
+                    continue
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                val = self._getPeriodicSupport(y1)
+                if val >= self._minPS:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+            self._save(None, itemSetX, tidSetX)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
+        print("Spatial Periodic Frequent patterns were generated successfully using SpatialEclat algorithm")
 
-    def _FPFPMining(self, prefix, prefixLen, fsFim):
-
-        """Generates FPFP from prefix
-
-        :param prefix: the prefix patterns of FPFP
-        :type prefix: len
-        :param prefixLen: the length of prefix
-        :type prefixLen: int
-        :param fsFim: the Fuzzy list of prefix itemSets
-        :type fsFim: list
-        """
-        for i in range(0, len(fsFim)):
-            X = fsFim[i]
-            if X.sumLUtil >= self._minSup and X.maxPeriod <= self._maxPer:
-                self._WriteOut(prefix, prefixLen, X.item, X.sumLUtil, X.maxPeriod)
-            if X.sumRUtil >= self._minSup:
-                exULs = []
-                for j in range(i + 1, len(fsFim)):
-                    Y = fsFim[j]
-                    exULs.append(self._construct(X, Y))
-                    self._joinsCnt += 1
-                self._itemSetBuffer.insert(prefixLen, X.item)
-                self._FPFPMining(self._itemSetBuffer, prefixLen + 1, exULs)
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
 
-    def getMemoryUSS(self) -> float:
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        # global items_sets, endTime, startTime
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self._creatingItemSets()
+        #self._minSup = self._convert(self._minSup)
+        self.mapNeighbours()
+        self._finalPatterns = {}
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemX = plist[i]
+            tidSetX = self._tidList[itemX]
+            itemSetX = [itemX]
+            itemSets = []
+            tidSets = []
+            neighboursItems = self._getNeighbourItems(plist[i])
+            for j in range(i + 1, len(plist)):
+                if not plist[j] in neighboursItems:
+                    continue
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                val = self._getPeriodicSupport(y1)
+                if val >= self._minPS:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+            self._save(None, itemSetX, tidSetX)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Spatial Periodic Frequent patterns were generated successfully using SpatialEclat algorithm")
 
+    def getMemoryUSS(self):
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
+    def getMemoryRSS(self):
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
-       """
-        return self._memoryRSS
-
-    def getRuntime(self) -> float:
-        """Calculating the total amount of runtime taken by the mining process
+        """
 
+        return self._memoryRSS
 
+    def getRuntime(self):
+        """
+        Calculating the total amount of runtime taken by the mining process
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
-       """
-        return self._endTime - self._startTime
-
-    def _construct(self, px: _FFList, py: _FFList) -> _FFList:
-        """
-        A function to construct a new Fuzzy item set from 2 fuzzy itemSets
-
-        :param px:the item set px
-        :type px:FFI-List
-        :param py:item set py
-        :type py:FFI-List
-        :return :the item set of pxy(px and py)
-        :rtype :FFI-List
-        """
-        pxyUL = _FFList(py.item)
-        prev = 0
-        for ex in px.elements:
-            ey = self._findElementWithTID(py, ex.tid)
-            if ey is None:
-                continue
-            eXY = _Element(ex.tid, min([ex.lUtils, ey.lUtils], key=lambda x: float(x)), ey.rUtils, ex.tid - prev)
-            pxyUL.addElement(eXY)
-            prev = ex.tid
-        return pxyUL
-
-    def _findElementWithTID(self, UList, tid) -> _Element:
-        """
-        To find element with same tid as given
-
-        :param UList: fuzzy list
-        :type UList:FFI-List
-        :param tid:transaction id
-        :type tid:int
-        :return:element eith tid as given
-        :rtype: element if exist or None
-        """
-        List = UList.elements
-        first = 0
-        last = len(List) - 1
-        while first <= last:
-            mid = (first + last) >> 1
-            if List[mid].tid < tid:
-                first = mid + 1
-            elif List[mid].tid > tid:
-                last = mid - 1
-            else:
-                return List[mid]
-        return None
-
-    def _WriteOut(self, prefix: List[int], prefixLen: int, item: int, sumLUtil: float, period: int) -> None:
         """
-        To Store the patten
 
-        :param prefix: prefix of itemSet
-        :type prefix: list
-        :param prefixLen: length of prefix
-        :type prefixLen: int
-        :param item: the last item
-        :type item: int
-        :param sumLUtil: sum of utility of itemSet
-        :type sumLUtil: float
-        :param period: represent the period of itemSet
-        :type period: int
-        """
-        self._itemsCnt += 1
-        res = ""
-        for i in range(0, prefixLen):
-            res += str(prefix[i]) +  "\t"
-        res += str(item)
-        #res1 = str(sumLUtil) + " : " + str(period)
-        self._finalPatterns[res] = [sumLUtil, period]
+        return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+            pat = ""
+            for i in a:
+                pat += str(i) + ' '
+            data.append([pat, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicSupport'])
         return dataFrame
 
-    def getPatterns(self) -> Dict[str, str]:
-        """
-        Function to send the set of frequent patterns after completion of the mining process
-
-        :return: returning frequent patterns
-        :rtype: dict
-        """
-        return self._finalPatterns
-
-    def save(self, outFile: str) -> None:
+    def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
-
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
+            pat = ""
+            for i in x:
+                pat += str(i) + '\t'
+            patternsAndSupport = pat.strip() + ": " + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
-    def printResults(self) -> None:
+    def getPatterns(self):
         """
-        this function is used to print the results
+        Function to send the set of frequent patterns after completion of the mining process
+        :return: returning frequent patterns
+        :rtype: dict
         """
-        print("Total number of Fuzzy Periodic-Frequent Patterns:", len(self.getPatterns()))
+        return self._finalPatterns
+
+    def printResults(self):
+        """
+        This function is used to print the results
+        """
+        print("Total number of  Spatial Partial Periodic Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:  # to  include a user specified separator
-            _ap = FPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:  # to consider "\t" as a separator
-            _ap = FPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
+        if len(_ab._sys.argv) == 7:
+            _ap = STEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
+        if len(_ab._sys.argv) == 6:
+            _ap = STEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
-        print("Total number of Fuzzy Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.mine()
+        print("Total number of Spatial Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
+        print("Total ExecutionTime in seconds:",  _ap.getRuntime())
     else:
-        _ap = FPFPMiner('sample.txt', 1, 10, ' ')
-        _ap.startMine()
-        print("Total number of Fuzzy Periodic-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save('output.txt')
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
+
+
```

### Comparing `pami-2024.3.9.2/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py` & `pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,36 +1,40 @@
 # Sample run of importing the code:
 # -------------------------------------
 #
-#         from PAMI.fuzzyPeriodicFrequentPattern.basic import FPFPMiner as alg
+#             from PAMI.fuzzyPeriodicFrequentPattern.basic import FPFPMiner as alg
 #
-#         obj =alg.FPFPMiner("input.txt",2,3)
+#             obj =alg.FPFPMiner("input.txt",2,3)
 #
-#         obj.startMine()
+#             obj.mine()
 #
-#         periodicFrequentPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#         print("Total number of Fuzzy Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Fuzzy Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#         obj.save("output.txt")
+#             obj.save("output.txt")
 #
-#         memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#         print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#         memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#         print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#         run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#         print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
+#
+
+
 
 
-__copyright__ = """(Copyright (C)  2021 Rage Uday Kiran
+__copyright__ = """
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -49,18 +53,19 @@
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-)"""
+"""
 
 
 from PAMI.fuzzyPeriodicFrequentPattern.basic import abstract as _ab
+from deprecated import deprecated
 
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
 
     :Attributes:
@@ -94,15 +99,15 @@
         self.maxPeriod = 0
 
     def addElement(self, element):
         """
         A Method that add a new element to FFList
 
         :param element: an element to be added to FFList
-        :param element: Element
+        :type element: Element
         """
         self.sumLUtil += element.lUtils
         self.sumRUtil += element.rUtils
         self.elements.append(element)
         self.maxPeriod = max(self.maxPeriod, element.period)
 
     def printElement(self):
@@ -185,14 +190,29 @@
 
 class FPFPMiner(_ab._fuzzyPeriodicFrequentPatterns):
     """
     :Description:   Fuzzy Periodic Frequent Pattern Miner is desired to find all fuzzy periodic frequent patterns which is
                     on-trivial and challenging problem to its huge search space.we are using efficient pruning
                     techniques to reduce the search space.
 
+    :Reference:
+
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
 
         iFile : file
             Name of the input file to mine complete set of fuzzy spatial frequent patterns
         oFile : file
                Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : float
@@ -254,30 +274,37 @@
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
         findElementWithTID(UList, tid)
             To find element with same tid as given
         WriteOut(prefix, prefixLen, item, sumIUtil,period)
             To Store the patten
 
     **Executing the code on terminal :**
-    -------------------------------------
+    ---------------------------------------
+
+    .. code-block:: console
+
+      Format:
 
-        Format:
-            >>> python3 FPFPMiner_old.py <inputFile> <outputFile> <minSup> <maxPer> <sep>
+      (.venv) $ python3 FPFPMiner_old.py <inputFile> <outputFile> <minSup> <maxPer> <sep>
+
+      Example Usage:
+
+      (.venv) $ python3  FPFPMiner_old.py sampleTDB.txt output.txt 2 3
+
+    .. note:: minSup will be considered in percentage of database transactions
 
-        Examples:
-            >>> python3  FPFPMiner_old.py sampleTDB.txt output.txt 2 3
 
     **Sample run of importing the code:**
     --------------------------------------
 
         from PAMI.fuzzyPeriodicFrequentPattern.basic import FPFPMiner as alg
 
         obj =alg.FPFPMiner("input.txt",2,3)
 
-        obj.startMine()
+        obj.mine()
 
         periodicFrequentPatterns = obj.getPatterns()
 
         print("Total number of Fuzzy Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
         obj.save("output.txt")
 
@@ -331,27 +358,44 @@
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._dbLen = 0
 
     def _compareItems(self, o1, o2):
         """
         A Function that sort all FFI-list in ascending order of Support
+
+        :param o1: First FFI-list
+
+        :type o1: _FFList
+
+        :param o2: Second FFI-list
+
+        :type o1: _FFList
+
+        :return: Comparision Value
+
+        :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
             return int(o1.item) - int(o2.item)
         else:
             return compare
 
     def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
+
+        :type value: int  or float or str
+
         :return: converted value
+
+        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
             if '.' in value:
@@ -406,14 +450,15 @@
                             self._transactions.append([x for x in items])
                             self._fuzzyValues.append([x for x in quantities])
                             count += 1
                 except IOError:
                     print("File Not Found")
                     quit()
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Fuzzy periodic Frequent pattern mining process will start from here
         """
         maxTID = 0
         lastTIDs = {}
         self._startTime = _ab._time.time()
@@ -522,14 +567,130 @@
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
+    def mine(self):
+        """
+        Fuzzy periodic Frequent pattern mining process will start from here
+        """
+        maxTID = 0
+        lastTIDs = {}
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        tid = int()
+        for line in range(len(self._transactions)):
+            tid = int(self._ts[line])
+            self._dbLen += 1
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            if tid < maxTID:
+                maxTID = tid
+            for i in range(0, len(items)):
+                regions = _Regions(int(quantities[i]), 3)
+                item = items[i]
+                if item in self._mapItemsLowSum.keys():
+                    low = self._mapItemsLowSum[item]
+                    low += regions.low
+                    self._mapItemsLowSum[item] = low
+                else:
+                    self._mapItemsLowSum[item] = regions.low
+                if item in self._mapItemMidSum.keys():
+                    mid = self._mapItemMidSum[item]
+                    mid += regions.middle
+                    self._mapItemMidSum[item] = mid
+                else:
+                    self._mapItemMidSum[item] = regions.middle
+                if item in self._mapItemsHighSum.keys():
+                    high = self._mapItemsHighSum[item]
+                    high += regions.high
+                    self._mapItemsHighSum[item] = high
+                else:
+                    self._mapItemsHighSum[item] = regions.high
+        listOfFFIList = []
+        mapItemsToFFLIST = {}
+        itemsToRegion = {}
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        for item1 in self._mapItemsLowSum.keys():
+            item = item1
+            low = self._mapItemsLowSum[item]
+            mid = self._mapItemMidSum[item]
+            high = self._mapItemsHighSum[item]
+            if low >= mid and low >= high:
+                self._mapItemSum[item] = low
+                self._mapItemRegions[item] = "L"
+                itemsToRegion[item] = "L"
+            elif mid >= low and mid >= high:
+                self._mapItemSum[item] = mid
+                self._mapItemRegions[item] = "M"
+                itemsToRegion[item] = "M"
+            elif high >= low and high >= mid:
+                self._mapItemRegions[item] = "H"
+                self._mapItemSum[item] = high
+                itemsToRegion[item] = "H"
+            if self._mapItemSum[item] >= self._minSup:
+                fUList = _FFList(item)
+                k = tuple([item, itemsToRegion.get(item)])
+                mapItemsToFFLIST[k] = fUList
+                listOfFFIList.append(fUList)
+                lastTIDs[item] = tid
+        listOfFFIList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        for line in range(len(self._transactions)):
+            tid = int(self._ts[line])
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                regions = _Regions(int(quantities[i]), 3)
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if self._mapItemRegions[pair.item] == "L":
+                        pair.quantity = regions.low
+                    elif self._mapItemRegions[pair.item] == "M":
+                        pair.quantity = regions.middle
+                    elif self._mapItemRegions[pair.item] == "H":
+                        pair.quantity = regions.high
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                remainUtil = 0
+                for j in range(len(revisedTransaction) - 1, i - 1, -1):
+                    remainUtil += revisedTransaction[j].quantity
+                if pair.quantity > remainUtil:
+                    remainingUtility = pair.quantity
+                else:
+                    remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(tuple([pair.item, itemsToRegion[pair.item]])) is not None:
+                    FFListOfItem = mapItemsToFFLIST[tuple([pair.item, itemsToRegion[pair.item]])]
+                    if len(FFListOfItem.elements) == 0:
+                        element = _Element(tid, pair.quantity, remainingUtility, 0)
+                    else:
+                        if lastTIDs[pair.item] == tid:
+                            element = _Element(tid, pair.quantity, remainingUtility, maxTID - tid)
+                        else:
+                            lastTid = FFListOfItem.elements[-1].tid
+                            curPer = tid - lastTid
+                            element = _Element(tid, pair.quantity, remainingUtility, curPer)
+                    FFListOfItem.addElement(element)
+        self._FSFIMining(self._itemSetBuffer, 0, listOfFFIList, self._minSup)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+
     def _FSFIMining(self, prefix, prefixLen, fsFim, minSup):
 
         """
         Generates FPFP from prefix
 
         :param prefix: the prefix patterns of FPFP
         :type prefix: len
@@ -565,24 +726,24 @@
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
-       """
+        """
         return self._memoryRSS
 
     def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
-
+        """
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
-       """
+        """
         return self._endTime - self._startTime
 
     def _construct(self, px, py):
         """
         A function to construct a new Fuzzy item set from 2 fuzzy itemSets
 
         :param px:the item set px
@@ -604,18 +765,18 @@
         return pxyUL
 
     def _findElementWithTID(self, UList, tid):
         """
         To find element with same tid as given
 
         :param UList: fuzzy list
-        :type UList:FFI-List
-        :param tid:transaction id
-        :type tid:int
-        :return:element with tid as given
+        :type UList: FFI-List
+        :param tid: transaction id
+        :type tid: int
+        :return: element with tid as given
         :rtype: element if exist or None
         """
         List = UList.elements
         first = 0
         last = len(List) - 1
         while first <= last:
             mid = (first + last) >> 1
@@ -686,29 +847,30 @@
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % patternsAndSupport)
 
     def printResults(self):
         """
         This function is used to print the results
-         """
+        """
         print("Total number of Fuzzy Periodic-Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:  # to  include a user specified separator
             _ap = FPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:  # to consider "\t" as a separator
             _ap = FPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Fuzzy Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py` & `pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,39 +1,40 @@
-# GPFPMiner is a Extension of ECLAT algorithm,which  stands for Equivalence Class Clustering and bottom-up
-# Lattice Traversal to mine the geo referenced peridoic frequent patterns.
+# 3pEclat is the fundamental approach to mine the partial periodic frequent patterns.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
+#             from PAMI.periodicFrequentPattern.basic import PPP_ECLAT as alg
 #
-#     import PAMI.geoReferencedPeridicFrequentPattern.GPFPMiner as alg
+#             obj = alg.PPP_ECLAT(iFile, minPS, period)
 #
-#     obj = alg.GPFPMiner("sampleTDB.txt", "sampleN.txt", 5, 3)
+#             obj.startMine()
 #
-#     obj.startMine()
+#             Patterns = obj.getPatterns()
 #
-#     Patterns = obj.getPatterns()
+#             print("Total number of partial periodic patterns:", len(Patterns))
 #
-#     print("Total number of Geo Referenced Periodic-Frequent Patterns:", len(Patterns))
+#             obj.save(oFile)
 #
-#     obj.save("outFile")
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
 #
 
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -45,497 +46,505 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from  PAMI.geoReferencedPeriodicFrequentPattern.basic import abstract as _ab
 
 
-class GPFPMiner(_ab._geoReferencedPeriodicFrequentPatterns):
-    """ 
-    :Description:   GPFPMiner is a Extension of ECLAT algorithm,which  stands for Equivalence Class Clustering and bottom-up
-                    Lattice Traversal to mine the geo referenced peridoic frequent patterns.
-        
-    :Reference:
+from PAMI.partialPeriodicPattern.basic import abstract as _ab
+from typing import List, Dict, Tuple, Set, Union, Any, Generator
+import pandas as pd
+
+from PAMI.partialPeriodicPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
+class PPP_ECLAT(_ab._partialPeriodicPatterns):
+    """
+    :Descripition:   3pEclat is the fundamental approach to mine the partial periodic frequent patterns.
+
+    :Reference:   R. Uday Kirana,b,∗ , J.N. Venkateshd, Masashi Toyodaa , Masaru Kitsuregawaa,c , P. Krishna Reddy Discovering partial periodic-frequent patterns in a transactional database
+                  https://www.tkl.iis.u-tokyo.ac.jp/new/uploads/publication_file/file/774/JSS_2017.pdf
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent pattern's
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  minPS: float:
+                   Minimum partial periodic pattern...
+    :param  period: float:
+                   Minimum partial periodic...
+
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
-        iFile : str
-            Input file name or path of the input file
-        nFile: str:
-           Name of Neighbourhood file name
-        minSup: float or int or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+        self.iFile : file
+            Name of the Input file or path of the input file
+        self. oFile : file
+            Name of the output file or path of the output file
+        minPS: float or int or str
+            The user can specify minPS either in count or proportion of database size.
+            If the program detects the data type of minPS is integer, then it treats minPS is expressed in count.
             Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer: float or int or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats minSup is expressed in count.
+            Example: minPS=10 will be treated as integer, while minPS=10.0 will be treated as float
+        period: float or int or str
+            The user can specify period either in count or proportion of database size.
+            If the program detects the data type of period is integer, then it treats period is expressed in count.
             Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+            Example: period=10 will be treated as integer, while period=10.0 will be treated as float
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        finalPatterns: dict
-            Storing the complete set of patterns in a dictionary variable
-        oFile : str
-            Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
         Database : list
-            To store the complete set of transactions available in the input database/file
+            To store the transactions of a database in list
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+        lno : int
+            it represents the total no of transactions
+        tree : class
+            it represents the Tree class
+        finalPatterns : dict
+            it represents to store the patterns
+        tidList : dict
+            stores the timestamps of an item
 
     :Methods:
 
-            startMine()
-                Mining process will start from here
-            getPatterns()
-                Complete set of patterns will be retrieved with this function
-            save(oFile)
-                Complete set of frequent patterns will be loaded in to a output file
-            getPatternsAsDataFrames()
-                Complete set of frequent patterns will be loaded in to a dataframe
-            getMemoryUSS()
-                Total amount of USS memory consumed by the mining process will be retrieved from this function
-            getMemoryRSS()
-                Total amount of RSS memory consumed by the mining process will be retrieved from this function
-            getRuntime()
-                Total amount of runtime taken by the mining process will be retrieved from this function
-            creatingItemSets(iFileName)
-                Storing the complete transactions of the database/input file in a database variable
-            frequentOneItem()
-                Generating one frequent patterns
-            convert(value):
-                To convert the given user specified value    
-            getNeighbourItems(keySet):
-                A function to get common neighbours of a itemSet
-             mapNeighbours(file):
-                A function to map items to their neighbours
-
-    **Executing the code on terminal :**
-    --------------------------------------
+        startMine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        save(oFile)
+            Complete set of frequent patterns will be loaded in to an  output file
+        getPatternsAsDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingOneitemSets()
+            Scan the database and store the items with their timestamps which are periodic frequent
+        getPeriodAndSupport()
+            Calculates the support and period for a list of timestamps.
+        Generation()
+            Used to implement prefix class equivalence method to generate the periodic patterns recursively
+
+    **Executing the code on terminal:**
+    ----------------------------------------
+      .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 PPP_ECLAT.py <inputFile> <outputFile> <minPS> <period>
+
+       Examples:
 
-        Format:
-            >>> python3 GPFPMiner.py <inputFile> <outputFile> <neighbourFile> <minSup> <maxPer>
+       (.venv) $ python3 PPP_ECLAT.py sampleDB.txt patterns.txt 0.3 0.4
 
-        Examples:
-            >>> python3 GPFPMiner.py sampleTDB.txt output.txt sampleN.txt 0.5 0.3 (minSup & maxPer will be considered in percentage of database transactions)
 
-           
-    **Sample run of importing the code :**
+    **Sample run of importing the code:**
     -----------------------------------------
-    .. code-block:: python
-    
-            import PAMI.geoReferencedPeridicFrequentPattern.GPFPMiner as alg
+    ...     code-block:: python
 
-            obj = alg.GPFPMiner("sampleTDB.txt", "sampleN.txt", 5, 3)
+            from PAMI.periodicFrequentPattern.basic import PPP_ECLAT as alg
+
+            obj = alg.PPP_ECLAT(iFile, minPS,period)
 
             obj.startMine()
 
             Patterns = obj.getPatterns()
 
-            print("Total number of Geo Referenced Periodic-Frequent Patterns:", len(Patterns))
+            print("Total number of partial periodic patterns:", len(Patterns))
+
+            obj.save(oFile)
 
-            obj.save("outFile")
+            Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    --------------
-        The complete program was written by P.RaviKumar under the supervision of Professor Rage Uday Kiran.
+    ------------------
+    The complete program was written by P.RaviKumar  under the supervision of Professor Rage Uday Kiran.\n
+
     """
 
-    _minSup = " "
-    _maxPer = " "
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
-    _nFile = " "
+    _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _Database = []
-    _sep = "\t"
+    _mapSupport = {}
+    _itemsetCount = 0
+    _writer = None
+    _minPS = str()
+    _period = str()
+    _tidList = {}
     _lno = 0
+    _Database = []
+
+    def _convert(self, value) -> Union[int, float]:
+        """
+        To convert the given user specified value
+        :param value: user specified value
+
+        :return: converted value
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
+
+    def _getPeriodicSupport(self, timeStamps: list) -> int:
+        """
+        calculates the support and periodicity with list of timestamps.
 
-    def __init__(self, iFile, nFile, minSup, maxPer, sep="\t"):
-        super().__init__(iFile, nFile, minSup, maxPer, sep)
-        self._NeighboursMap = {}
+        :param timeStamps : timestamps of a pattern
+        :type timeStamps : list
+        :return: list
+        """
+        timeStamps.sort()
+        per = 0
+        for i in range(len(timeStamps) - 1):
+            j = i + 1
+            if abs(timeStamps[j] - timeStamps[i]) <= self._period:
+                per += 1
+        return per
 
-    def _creatingItemSets(self):
+    def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            data, ts = [], []
+            data, tids = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
+                tids = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 data = self._iFile['Transactions'].tolist()
             for i in range(len(data)):
-                tr = [ts[i][0]]
+                tr = [tids[i][0]]
                 tr = tr + data[i]
                 self._Database.append(tr)
+            self._lno = len(self._Database)
+
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
+                    self._lno += 1
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
                     self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line = line.rstrip()
-                            temp = [i.strip() for i in line.split(self._sep)]
+                            self._lno += 1
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    # function to get frequent one pattern
-    def _frequentOneItem(self):
-        """Generating one frequent patterns"""
-
-        candidate = {}
-        for i in self._Database:
-            self._lno += 1
-            n = int(i[0])
-            for j in i[1:]:
-                if j not in candidate:
-                    candidate[j] = [1, abs(0-n), n, [n]]
-                else:
-                    candidate[j][0] += 1
-                    candidate[j][1] = max(candidate[j][1], abs(n - candidate[j][2]))
-                    candidate[j][2] = n
-                    candidate[j][3].append(n)
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        #print(self._minSup, self._maxPer)
-        self._tidList = {k: v[3] for k, v in candidate.items() if v[0] >= self._minSup and v[1] <= self._maxPer}
-        candidate = {k: [v[0], v[1]] for k, v in candidate.items() if v[0] >= self._minSup and v[1] <= self._maxPer}
-        plist = [key for key, value in sorted(candidate.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
-        return plist
-
-    def _convert(self, value):
+    def _creatingOneitemSets(self) -> List[str]:
         """
-        To convert the given user specified value
-
-        :param value: user specified value
-        :return: converted value
+        Scans the Temporal database / Input file and stores the 1-length partial-periodic patterns.
+        :return: list
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
-
-    def _getSupportAndPeriod(self, timeStamps):
-        """
-        calculates the support and periodicity with list of timestamps
-
-        :param timeStamps: timestamps of a pattern
-        :type timeStamps: list
-        """
-        timeStamps.sort()
-        cur = 0
-        per = 0
-        sup = 0
-        for j in range(len(timeStamps)):
-            per = max(per, timeStamps[j] - cur)
-            if per > self._maxPer:
-                return [0, 0]
-            cur = timeStamps[j]
-            sup += 1
-        per = max(per, self._lno - cur)
-        return [sup, per]
+        plist = []
+        self._tidList = {}
+        self._mapSupport = {}
+        self._period = self._convert(self._period)
+        for line in self._Database:
+            s = line
+            n = int(s[0])
+            for i in range(1, len(s)):
+                si = s[i]
+                if self._mapSupport.get(si) is None:
+                    self._mapSupport[si] = [0, n]
+                    self._tidList[si] = [n]
+                else:
+                    lp = n - self._mapSupport[si][1]
+                    if lp <= self._period:
+                        self._mapSupport[si][0] += 1
+                    self._mapSupport[si][1] = n
+                    self._tidList[si].append(n)
+        self._minPS = self._convert(self._minPS)
+        self._mapSupport = {k: v[0] for k, v in self._mapSupport.items() if v[0] >= self._minPS}
+        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        return plist
 
-    def _save(self, prefix, suffix, tidSetX):
+    def _save(self, prefix: List[str], suffix: List[str], tidSetX: List[int]) -> None:
         """
-        Saves the patterns that satisfy the periodic frequent property.
+        saves the patterns that satisfy the partial periodic property.
 
         :param prefix: the prefix of a pattern
-        :type prefix: list or None
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetX: the timestamp of a patterns
-        :type tidSetX: list
-
-
+        :type prefix: list
+        :param suffix : the suffix of a patterns
+        :type suffix : list
+        :param tidSetX : the timestamp of a patterns
+        :type tidSetX : list
+        :return: None
         """
-        if prefix == None:
+
+        if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
-        val = self._getSupportAndPeriod(tidSetX)
-        if val[0] >= self._minSup and val[1] <= self._maxPer:
-            self._finalPatterns[tuple(prefix)] = val
-
-    def _Generation(self, prefix, itemSets, tidSets):
-        """
-        Generates the patterns that satisfy the periodic frequent property.
-
-        :param prefix: the prefix of a pattern
-        :type prefix: list or None
-        :param itemSets: the item sets of a patterns
-        :type itemSets: list
-        :param tidSets: the timestamp of a patterns
-        :type tidSets: list
+        val = self._getPeriodicSupport(tidSetX)
+        if val >= self._minPS:
+            sample = str()
+            for i in prefix:
+                sample = sample + i + "\t"
+            self._finalPatterns[sample] = val
+
+    def _Generation(self, prefix: List[str], itemSets: List[str], tidSets: List[list]) -> None:
+        """
+        Generates the patterns following Equivalence-class methods
+
+        :param prefix :  main equivalence prefix
+        :type prefix : partial-periodic item or pattern
+        :param itemSets : patterns which are items combined with prefix and satisfying the periodicity
+                        and partial property with their timestamps
+        :type itemSets : list
+        :param tidSets : timestamps of the items in the argument itemSets
+        :type tidSets : list
+        :return: None
         """
         if len(itemSets) == 1:
             i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
+            tidi = tidSets[0]
+            self._save(prefix, [i], tidi)
             return
         for i in range(len(itemSets)):
-            itemX = itemSets[i]
-            if itemX == None:
+            itemI = itemSets[i]
+            if itemI is None:
                 continue
             tidSetX = tidSets[i]
             classItemSets = []
             classTidSets = []
-            itemSetX = [itemX]
-            neighboursItemsI = self._getNeighbourItems(itemSets[i])
+            itemSetX = [itemI]
             for j in range(i + 1, len(itemSets)):
-                neighboursItemsJ = self._getNeighbourItems(itemSets[i])
-                if not itemSets[j] in neighboursItemsI:
-                    continue
                 itemJ = itemSets[j]
                 tidSetJ = tidSets[j]
                 y = list(set(tidSetX).intersection(tidSetJ))
-                if len(y) >= self._minSup:
-                    ne = list(set(neighboursItemsI).intersection(neighboursItemsJ))
-                    x = []
-                    x = x + [itemX]
-                    x = x + [itemJ]
-                    self._NeighboursMap[tuple(x)] = ne
+                val = self._getPeriodicSupport(y)
+                if val >= self._minPS:
                     classItemSets.append(itemJ)
                     classTidSets.append(y)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
+            newprefix = list(set(itemSetX)) + prefix
+            self._Generation(newprefix, classItemSets, classTidSets)
             self._save(prefix, list(set(itemSetX)), tidSetX)
 
-    def _getNeighbourItems(self, keySet):
+
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self) -> None:
         """
-        A function to get Neighbours of a item
+        Main program start with extracting the periodic frequent items from the database and
+        performs prefix equivalence to form the combinations and generates partial-periodic patterns.
+        :return: None
 
-        :param keySet:itemSet
-        :type keySet:str or tuple
-        :return: set of common neighbours
-        :rtype:set
-        """
-        itemNeighbours = self._NeighboursMap.keys()
-        if isinstance(keySet, str):
-            if self._NeighboursMap.get(keySet) is None:
-                return []
-            itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(keySet))))
-        if isinstance(keySet, tuple):
-            keySet = list(keySet)
-            for j in range(0, len(keySet)):
-                i = keySet[j]
-                itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(i))))
-        return itemNeighbours
-
-    def mapNeighbours(self):
-        """
-        A function to map items to their Neighbours
-        """
-        self._NeighboursMap = {}
-        if isinstance(self._nFile, _ab._pd.DataFrame):
-            data = []
-            if self._nFile.empty:
-                print("its empty..")
-            i = self._nFile.columns.values.tolist()
-            if 'Neighbours' in i:
-                data = self._nFile['Neighbours'].tolist()
-            for i in data:
-                self._NeighboursMap[i[0]] = i[1:]
-        if isinstance(self._nFile, str):
-            if _ab._validators.url(self._nFile):
-                data = _ab._urlopen(self._nFile)
-                for line in data:
-                    line.strip()
-                    line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    self._NeighboursMap[temp[0]] = temp[1:]
-            else:
-                try:
-                    with open(self._nFile, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self._NeighboursMap[temp[0]] = temp[1:]
-                except IOError:
-                    print("File Not Found")
-                    quit()
+        """
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        plist = self._creatingOneitemSets()
+        self._finalPatterns = {}
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetX = self._tidList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                val = self._getPeriodicSupport(y1)
+                if val >= self._minPS:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+            self._save(None, itemSetX, tidSetX)
+        print("Partial Periodic Patterns were generated successfully using 3PEclat algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
-    def startMine(self):
-        """Frequent pattern mining process will start from here"""
+    def Mine(self) -> None:
+        """
+        Main program start with extracting the periodic frequent items from the database and
+        performs prefix equivalence to form the combinations and generates partial-periodic patterns.
+        :return: None
 
-        # global items_sets, endTime, startTime
+        """
         self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
         self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        self.mapNeighbours()
+        plist = self._creatingOneitemSets()
         self._finalPatterns = {}
-        plist = self._frequentOneItem()
         for i in range(len(plist)):
-            itemX = plist[i]
-            tidSetX = self._tidList[itemX]
-            itemSetX = [itemX]
+            itemI = plist[i]
+            tidSetX = self._tidList[itemI]
+            itemSetX = [itemI]
             itemSets = []
             tidSets = []
-            neighboursItems = self._getNeighbourItems(plist[i])
             for j in range(i + 1, len(plist)):
-                if not plist[j] in neighboursItems:
-                    continue
                 itemJ = plist[j]
                 tidSetJ = self._tidList[itemJ]
                 y1 = list(set(tidSetX).intersection(tidSetJ))
-                if len(y1) >= self._minSup:
+                val = self._getPeriodicSupport(y1)
+                if val >= self._minPS:
                     itemSets.append(itemJ)
                     tidSets.append(y1)
             self._Generation(itemSetX, itemSets, tidSets)
             self._save(None, itemSetX, tidSetX)
+        print("Partial Periodic Patterns were generated successfully using 3PEclat algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
         self._memoryRSS = float()
+        self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Spatial Periodic Frequent patterns were generated successfully using SpatialEclat algorithm")
 
-    def getMemoryUSS(self):
+    def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self):
+    def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
-        """
-        Storing final frequent patterns in a dataframe
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+        """Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            pat = ""
-            for i in a:
-                pat += str(i) + "\t"
-            data.append([pat, b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Period'])
-        return dataFrame
+            data.append([a.replace('\t', ' '), b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicSupport'])
+        return dataframe
 
-    def save(self, outFile):
-        """
-        Complete set of frequent patterns will be loaded in to a output file
+    def save(self, outFile: str) -> None:
+        """Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: csv file
+        :type outFile: file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            pat = ""
-            for i in x:
-                pat += str(i) + "\t"
-            patternsAndSupport = pat + ": " + str(y[0]) + ": " + str(y[1])
-            writer.write("%s \n" % patternsAndSupport)
+            s1 = x.strip() + ":" + str(y)
+            writer.write("%s \n" % s1)
 
-    def getPatterns(self):
-        """
-        Function to send the set of frequent patterns after completion of the mining process
+    def getPatterns(self) -> Dict[str, int]:
+        """ Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
-    
-    def printResults(self):
+
+    def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
-        print("Total number of Spatial Periodic-Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Partial Periodic Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
-        if len(_ab._sys.argv) == 7:
-            _ap = GPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
-            _ap = GPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+            _ap = PPP_ECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:
+            _ap = PPP_ECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        print("Total number of Spatial Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Partial Periodic Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
+        for i in [100, 200, 300, 400, 500]:
+            _ap = PPP_ECLAT('/Users/Likhitha/Downloads/temporal_T10I4D100K.csv', i, 5000, '\t')
+            _ap.startMine()
+            print("Total number of Maximal Partial Periodic Patterns:", len(_ap.getPatterns()))
+            _ap.save('/Users/Likhitha/Downloads/output.txt')
+            print("Total Memory in USS:", _ap.getMemoryUSS())
+            print("Total Memory in RSS", _ap.getMemoryRSS())
+            print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.3.9.2/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/georeferencedFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py` & `pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,577 +1,656 @@
-# FSPGrowth is a transactional database and a spatial (or neighborhood) file, FSPM aims to discover all of those patterns
-# that satisfy the user-specified minimum support (minSup) and maximum distance (maxDist) constraints
+#  Copyright (C)  2021 Rage Uday Kiran
 #
-# **Importing this algorithm into a python program**
-# -------------------------------------------------------
 #
-#         from PAMI.georeferencedFrequentPattern.basic import FSPGrowth as alg
+#      This program is free software: you can redistribute it and/or modify
+#      it under the terms of the GNU General Public License as published by
+#      the Free Software Foundation, either version 3 of the License, or
+#      (at your option) any later version.
 #
-#         obj = alg.FSPGrowth("sampleTDB.txt", "sampleN.txt", 5)
+#      This program is distributed in the hope that it will be useful,
+#      but WITHOUT ANY WARRANTY; without even the implied warranty of
+#      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+#      GNU General Public License for more details.
 #
-#         obj.startMine()
-#
-#         spatialFrequentPatterns = obj.getPatterns()
-#
-#         print("Total number of Spatial Frequent Patterns:", len(spatialFrequentPatterns))
-#
-#         obj.save("outFile")
-#
-#         memUSS = obj.getMemoryUSS()
-#
-#         print("Total Memory in USS:", memUSS)
-#
-#         memRSS = obj.getMemoryRSS()
-#
-#         print("Total Memory in RSS", memRSS)
-#
-#         run = obj.getRuntime()
-#
-#         print("Total ExecutionTime in seconds:", run)
-#
-
-__copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+#      You should have received a copy of the GNU General Public License
+#      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 
-     This program is free software: you can redistribute it and/or modify
-     it under the terms of the GNU General Public License as published by
-     the Free Software Foundation, either version 3 of the License, or
-     (at your option) any later version.
+from PAMI.multipleMinimumSupportBasedFrequentPattern.basic import abstract as _fp
+from deprecated import deprecated
 
-     This program is distributed in the hope that it will be useful,
-     but WITHOUT ANY WARRANTY; without even the implied warranty of
-     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-     GNU General Public License for more details.
-
-     You should have received a copy of the GNU General Public License
-     along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
-"""
-
-
-from PAMI.georeferencedFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
+_fp._sys.setrecursionlimit(20000)
+MIS = {}
 
 class _Node:
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
-        item : int
-            Storing item of a node
-        count : int
+        itemId: int
+            storing item of a node
+        counter: int
             To maintain the support of node
-        children : dict
+        parent: node
+            To maintain the parent of node
+        children: list
             To maintain the children of node
-        prefix : list
-            To maintain the prefix of node
+
+    :Methods:
+
+        addChild(node)
+            Updates the nodes children list and parent for the given node
+
     """
 
-    def __init__(self, item, count, children):
-        self.item = item
-        self.count = count
+    def __init__(self, item, children):
+        self.itemId = item
+        self.counter = 1
+        self.parent = None
         self.children = children
-        self.prefix = []
+
+    def addChild(self, node):
+        """
+        Retrieving the child from the tree
+
+        :param node: Children node.
+        :type node: Node
+        :return: Updates the children nodes and parent nodes
+        """
+        self.children[node.itemId] = node
+        node.parent = self
 
 
 class _Tree:
     """
     A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
 
         root : Node
             The first node of the tree set to Null.
-        nodeLink : dict
-            Stores the nodes which shares same item
+        summaries : dictionary
+            Stores the nodes itemId which shares same itemId
+        info : dictionary
+            frequency of items in the transactions
 
     :Methods:
 
-        createTree(transaction,count)
-            Adding transaction into the tree
-        linkNode(node)
-            Adding node to nodeLink
-        createCPB(item,neighbour)
-            Create conditional pattern base based on item and neighbour
-        mergeTree(tree,fpList)
-            Merge tree into yourself
-        createTransactions(fpList)
-            Create transactions from yourself
-        getPattern(item,suffixItem,minSup,neighbour)
-            Get frequent patterns based on suffixItem
-        mining(minSup,isResponsible = lambda x:True,neighbourhood=None)
-            Mining yourself
+        addTransaction(transaction, freq)
+            adding items of  transactions into the tree as nodes and freq is the count of nodes
+        getFinalConditionalPatterns(node)
+            getting the conditional patterns from fp-tree for a node
+        getConditionalPatterns(patterns, frequencies)
+            sort the patterns by removing the items with lower minSup
+        generatePatterns(prefix)
+            generating the patterns from fp-tree
     """
 
     def __init__(self):
-        self.root = _Node(None, 0, {})
-        self.nodeLink = _ab._OrderedDict()
+        self.root = _Node(None, {})
+        self.summaries = {}
+        self.info = {}
+
+    def addTransaction(self, transaction, count):
+        """
+        adding transaction into tree
+
+        :param transaction: it represents the one transactions in database
+        :type transaction: list
+        :param count: frequency of item
+        :type count: int
+        """
+
+        # This method takes transaction as input and returns the tree
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i] not in currentNode.children:
+                newNode = _Node(transaction[i], {})
+                newNode.freq = count
+                currentNode.addChild(newNode)
+                if transaction[i] in self.summaries:
+                    self.summaries[transaction[i]].append(newNode)
+                else:
+                    self.summaries[transaction[i]] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i]]
+                currentNode.freq += count
 
-    def createTree(self, transaction, count):
+    def getFinalConditionalPatterns(self, alpha, support):
         """
-        Create tree or add transaction into yourself.
+        generates the conditional patterns for a node
 
-        :param transaction: list
-        :param count: int
-        :return: Tree
-        """
-        current = self.root
-        for item in transaction:
-            if item not in current.children:
-                current.children[item] = _Node(item, count, {})
-                current.children[item].prefix = transaction[0:transaction.index(item)]
-                self.linkNode(current.children[item])
-            else:
-                current.children[item].count += count
-            current = current.children[item]
-        return self
-
-    def linkNode(self, node):
-        """
-        Maintain link of node by adding node to nodeLink
-
-        :param node: Node
-        :return:
-        """
-        if node.item in self.nodeLink:
-            self.nodeLink[node.item].append(node)
-        else:
-            self.nodeLink[node.item] = []
-            self.nodeLink[node.item].append(node)
-
-    def createCPB(self, item, neighbour) :
-        """
-        Create conditional pattern base based on item and neighbour
-
-        :param item: int
-        :param neighbour: dict
-        :return: Tree
-        """
-        pTree = _Tree()
-        for node in self.nodeLink[item]:
-            # print(node.item, neighbour[node.item])
-            if node.item in neighbour:
-                node.prefix = [item for item in node.prefix if item in neighbour.get(node.item)]
-            pTree.createTree(node.prefix, node.count)
-        return pTree
-
-    def mergeTree(self, tree, fpList):
-        """
-        Merge tree into yourself
-
-        :param tree: Tree
-        :param fpList: list
-        :return: Tree
-        """
-        transactions = tree.createTransactions(fpList)
-        for transaction in transactions:
-            self.createTree(transaction, 1)
-        return self
-
-    def createTransactions(self, fpList):
-        """
-        Create transactions that configure yourself
-
-        :param fpList: list
-        :return: transaction list
-        """
-        transactions = []
-        flist = [x for x in fpList if x in self.nodeLink]
-        for item in reversed(flist):
-            for node in self.nodeLink[item]:
-                if node.count != 0:
-                    transaction = node.prefix
-                    transaction.append(node.item)
-                    transactions.extend([transaction for i in range(node.count)])
-                    current = self.root
-                    for i in transaction:
-                        current = current.children[i]
-                        current.count -= node.count
-        return transactions
-
-    def getPattern(self, item, suffixItem, minSup, neighbour):
-        """
-        Get frequent patterns based on suffixItem
-
-        :param item: int
-        :param suffixItem: tuple
-        :param minSup: int
-        :param neighbour: dict
-        :return: list
-        """
-        pTree = self.createCPB(item, neighbour)
-        frequentItems = {}
-        frequentPatterns = []
-        for i in pTree.nodeLink.keys():
-            frequentItems[i] = 0
-            for node in pTree.nodeLink[i]:
-                frequentItems[i] += node.count
-        frequentItems = {key: value for key, value in frequentItems.items() if value >= minSup}
-        for i in frequentItems:
-            pattern = suffixItem + "\t" + i
-            frequentPatterns.append((pattern, frequentItems[i]))
-            frequentPatterns.extend(pTree.getPattern(i, pattern, minSup, neighbour))
-        return frequentPatterns
-
-    def mining(self, minSup:callable[int, float], neighbourhood: [Dict[int, List[int]]] = None):
-        """
-        Pattern mining on your own
-
-        :param minSup: int
-        :param neighbourhood: function
-        :param neighbourhood: dict
-        :return: list
-        """
-        frequentPatterns = []
-        flist = sorted([item for item in self.nodeLink.keys()])
-        for item in reversed(flist):
-            frequentPatterns.extend(self.getPattern(item, item, minSup, neighbourhood))
-        return frequentPatterns
+        :param alpha: node to generate conditional patterns
+        :return: returns conditional patterns, frequency of each item in conditional patterns
+
+        """
+        finalPatterns = []
+        finalFreq = []
+        for i in self.summaries[alpha]:
+            set1 = i.freq
+            set2 = []
+            while i.parent.itemId is not None:
+                set2.append(i.parent.itemId)
+                i = i.parent
+            if len(set2) > 0:
+                set2.reverse()
+                finalPatterns.append(set2)
+                finalFreq.append(set1)
+        finalPatterns, finalFreq, info = self.getConditionalTransactions(finalPatterns, finalFreq, support)
+        return finalPatterns, finalFreq, info
+
+    @staticmethod
+    def getConditionalTransactions(ConditionalPatterns, conditionalFreq, support):
+        """
+        To calculate the frequency of items in conditional patterns and sorting the patterns
+
+        :param ConditionalPatterns: paths of a node
+        :param conditionalFreq: frequency of each item in the path
+        :return: conditional patterns and frequency of each item in transactions
+        """
+        #global _minSup
+        pat = []
+        freq = []
+        data1 = {}
+        for i in range(len(ConditionalPatterns)):
+            for j in ConditionalPatterns[i]:
+                if j in data1:
+                    data1[j] += conditionalFreq[i]
+                else:
+                    data1[j] = conditionalFreq[i]
+        up_dict = {k: v for k, v in data1.items() if v >= support}
+        #up_dict = data1.copy()
+        count = 0
+        for p in ConditionalPatterns:
+            p1 = [v for v in p if v in up_dict]
+            trans = sorted(p1, key=lambda x: (up_dict.get(x), -x), reverse=True)
+            if len(trans) > 0:
+                pat.append(trans)
+                freq.append(conditionalFreq[count])
+            count += 1
+        return pat, freq, up_dict
+
+    def generatePatterns(self, prefix):
+        """
+        To generate the frequent patterns
+
+        :param prefix: an empty list
+        :return: Frequent patterns that are extracted from fp-tree
+
+        """
+        global minMIS
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x), -x)):
+            pattern = prefix[:]
+            pattern.append(i)
+            if self.info[i] >= minMIS:
+              yield pattern, self.info[i]
+            patterns, freq, info = self.getFinalConditionalPatterns(i, self.info[i])
+            conditionalTree = _Tree()
+            conditionalTree.info = info.copy()
+            for pat in range(len(patterns)):
+                conditionalTree.addTransaction(patterns[pat], freq[pat])
+            if len(patterns) > 0:
+                for q in conditionalTree.generatePatterns(pattern):
+                    yield q
 
+minMIS = 0
 
-class FSPGrowth(_ab._spatialFrequentPatterns):
+class CFPGrowthPlus(_fp._frequentPatterns):
     """
-    :Description:   Given a transactional database and a spatial (or neighborhood) file, FSPM aims to discover all of those patterns
-                    that satisfy the user-specified minimum support (minSup) and maximum distance (maxDist) constraints
 
-    :Reference:   Rage, Uday & Fournier Viger, Philippe & Zettsu, Koji & Toyoda, Masashi & Kitsuregawa, Masaru. (2020).
-                  Discovering Frequent Spatial Patterns in Very Large Spatiotemporal Databases.
+    :Description:
+
+    :Reference:   R. Uday Kiran P. Krishna Reddy Novel techniques to reduce search space in multiple minimum supports-based frequent
+                  pattern mining algorithms. 11-20 2011 EDBT https://doi.org/10.1145/1951365.1951370
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Uncertain Multiple Minimum Support Based Frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Uncertain Minimum Support Based Frequent patterns
+    :param  minSup: str:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
 
     :Attributes:
 
         iFile : file
             Input file name or path of the input file
-        nFile : file
-            Neighbourhood file name or path of the neighbourhood file
+        MIS: file or dictionary
+            Multiple minimum supports of all items in the database
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            However, the users can override their default separator.
         oFile : file
-            Name of the output file or the path of output file
-        minSup : float
-            The user can specify minSup either in count or proportion of database size.
-        finalPatterns : dict
-            Storing the complete set of patterns in a dictionary variable
+            Name of the output file or the path of the output file
         startTime:float
             To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+        Database : list
+            To store the transactions of a database in list
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+        lno : int
+            it represents the total no of transactions
+        tree : class
+            it represents the Tree class
+        finalPatterns : dict
+            it represents to store the patterns
+
 
     :Methods:
 
         startMine()
-            This function starts pattern mining.
+            Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
+        savePatterns(oFile)
             Complete set of frequent patterns will be loaded in to a output file
-        getPatternsInDataFrame()
+        getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        getNeighbour(string)
-            This function changes string to tuple(neighbourhood).
-        getFrequentItems(database)
-            This function create frequent items from database.
-        genCondTransaction(transaction, rank)
-            This function generates conditional transaction for processing on each workers.
-        getPartitionId(item)
-            This function generates partition id
-        mapNeighbourToNumber(neighbour, rank)
-            This function maps neighbourhood to number.
-            Because in this program, each item is mapped to number based on fpList so that it can be distributed.
-            So the contents of neighbourhood must also be mapped to a number.
-        createFPTree()
-            This function creates FPTree.
-        getAllFrequentPatterns(data, fpList, ndata)
-            This function generates all frequent patterns
-
-    **Executing the code on terminal :**
-    --------------------------------------
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
+        frequentOneItem()
+            Extracts the one-frequent patterns from transactions
+
+    **Executing the code on terminal:**
+    ------------------------------------
+     .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 CFPGrowthPlus.py <inputFile> <outputFile>
 
-        Format:
-            >>> python3 FSPGrowth.py <inputFile> <outputFile> <neighbourFile> <minSup>
+       Examples:
 
-        Examples:
-            >>> python3 FSPGrowth.py sampleTDB.txt output.txt sampleN.txt 0.5
+       (.venv) $ python3 CFPGrowthPlus.py sampleDB.txt patterns.txt MISFile.txt
 
-    **Sample run of importing the code :**
+
+                .. note:: minSup  will be considered in support count or frequency
+
+
+    **Sample run of the importing code:**
     ----------------------------------------
     .. code-block:: python
 
-        from PAMI.georeferencedFrequentPattern.basic import FSPGrowth as alg
+            from PAMI.multipleMinimumSupportBasedFrequentPattern.basic import CFPGrowthPlus as alg
 
-        obj = alg.FSPGrowth("sampleTDB.txt", "sampleN.txt", 5)
+            obj = alg.CFPGrowthPlus(iFile, mIS)
 
-        obj.startMine()
+            obj.startMine()
 
-        spatialFrequentPatterns = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-        print("Total number of Spatial Frequent Patterns:", len(spatialFrequentPatterns))
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-        obj.save("outFile")
+            obj.savePatterns(oFile)
 
-        memUSS = obj.getMemoryUSS()
+            Df = obj.getPatternInDataFrame()
 
-        print("Total Memory in USS:", memUSS)
+            memUSS = obj.getMemoryUSS()
 
-        memRSS = obj.getMemoryRSS()
+            print("Total Memory in USS:", memUSS)
 
-        print("Total Memory in RSS", memRSS)
+            memRSS = obj.getMemoryRSS()
 
-        run = obj.getRuntime()
+            print("Total Memory in RSS", memRSS)
 
-        print("Total ExecutionTime in seconds:", run)
+            run = obj.getRuntime()
+
+            print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    --------------
-        The complete program was written by Yudai Masu under the supervision of Professor Rage Uday Kiran.
+    ---------------
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
+
     """
 
-    _minSup = float()
-    _startTime = float()
-    _endTime = float()
-    _finalPatterns = {}
+    __startTime = float()
+    __endTime = float()
+    _MIS = str
+    __finalPatterns = {}
     _iFile = " "
-    _nFile = " "
     _oFile = " "
     _sep = " "
-    _lno = 0
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _Database = []
-    _neighbourList = {}
-    _fpList = []
+    __memoryUSS = float()
+    __memoryRSS = float()
+    __Database = []
+    __mapSupport = {}
+    __lno = 0
+    __tree = _Tree()
+    __rank = {}
+    __rankDup = {}
 
-    def _readDatabase(self):
-        """
-        Read input file and neighborhood file
+    def __init__(self, iFile, MIS, sep='\t'):
+        super().__init__(iFile, MIS, sep)
+
+    def __creatingItemSets(self):
         """
+        Storing the complete transactions of the database/input file in a database variable
 
-        self._Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
+        """
+        self.__Database = []
+        if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-            self._lno = len(self._Database)
+                self.__Database = self._iFile['Transactions'].tolist()
+
+            # print(self.Database)
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if _fp._validators.url(self._iFile):
+                data = _fp._urlopen(self._iFile)
                 for line in data:
-                    line.strip()
-                    self._lno += 1
+                    line = line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    self.__Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line.strip()
-                            self._lno += 1
+                            line = line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            self.__Database.append(temp)
                 except IOError:
-                    print("File Not Found1")
+                    print("File Not Found")
                     quit()
 
-        self._neighbourList = {}
-        if isinstance(self._nFile, _ab._pd.DataFrame):
-            data, items = [], []
-            if self._nFile.empty:
+    def _getMISValues(self):
+        """
+        Storing the Minimum supports given by the user for each item in the database
+
+        """
+        self._MISValues = {}
+        if isinstance(self._MIS, _fp._pd.DataFrame):
+            items, MIS = [], []
+            if self._MIS.empty:
                 print("its empty..")
-            i = self._nFile.columns.values.tolist()
-            if 'item' in i:
-                items = self._nFile['items'].tolist()
-            if 'Neighbours' in i:
-                data = self._nFile['Neighbours'].tolist()
-            for k in range(len(items)):
-                self._neighbourList[items[k][0]] = data[k]
-            # print(self.Database)
-        if isinstance(self._nFile, str):
-            if _ab._validators.url(self._nFile):
-                data = _ab._urlopen(self._nFile)
+            i = self._MIS.columns.values.tolist()
+            if 'items' in i:
+                items = self._MIS['items'].tolist()
+            if 'MIS' in i:
+                MIS = self._MIS['MIS'].tolist()
+            for i in range(len(items)):
+                self._MISValues[items[i]] = MIS[i]
+
+        if isinstance(self._MIS, str):
+            if _fp._validators.url(self._MIS):
+                data = _fp._urlopen(self._MIS)
                 for line in data:
-                    line.strip()
+                    line = line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._neighbourList[temp[0]] = temp[1:]
+                    self._MISValues[temp[0]] = int(temp[1])
             else:
                 try:
-                    with open(self._nFile, 'r', encoding='utf-8') as f:
+                    with open(self._MIS, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line.strip()
+                            line = line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._neighbourList[temp[0]] = temp[1:]
+                            self._MISValues[temp[0]] = int(temp[1])
                 except IOError:
-                    print("File Not Found2")
+                    print("File Not Found")
                     quit()
 
-    def _getFrequentItems(self):
+    def __convert(self, value):
         """
-        Create frequent items and self.fpList from self.Database
-        """
-        oneFrequentItem = {}
-        for transaction in self._Database:
-            for item in transaction:
-                oneFrequentItem[item] = oneFrequentItem.get(item, 0) + 1
-        self._finalPatterns = {key: value for key, value in oneFrequentItem.items() if value >= self._minSup}
-        self._fpList = list(dict(sorted(oneFrequentItem.items(), key=lambda x: x[1], reverse=True)))
-
-    def _createFPTree(self):
-        """ create FP Tree and self.fpList from self.Database"""
-        FPTree = _Tree()
-        for transaction in self._Database:
-            FPTree.createTree(transaction, 1)
-        return FPTree
+        to convert the type of user specified minSup value
 
-    def _sortTransaction(self):
-        """
-        Sort each transaction of self.Database based on self.fpList
-        """
-        for i in range(len(self._Database)):
-            self._Database[i] = [item for item in self._Database[i] if item in self._fpList]
-            self._Database[i].sort(key=lambda value: self._fpList.index(value))
-
-    def _convert(self, value):
-        """
-        To convert the given user specified value
-
-        :param value: user specified value
-        :return: converted value
+        :param value: user specified minSup value
+        :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (self._lno * value)
+            value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (self._lno * value)
+                value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
+    def __frequentOneItem(self):
+        """
+        Generating One frequent items sets
+        """
+        global minMIS
+        self.__mapSupport = {}
+        for tr in self.__Database:
+            for i in range(1, len(tr)):
+                if tr[i] not in self.__mapSupport:
+                    self.__mapSupport[tr[i]] = 1
+                else:
+                    self.__mapSupport[tr[i]] += 1
+        self.__mapSupport = {k: v for k, v in self.__mapSupport.items() if v >= min(self._MISValues.values())}
+        minMIS = min(self._MISValues.values())
+        genList = [k for k, v in sorted(self.__mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self.__rank = dict([(index, item) for (item, index) in enumerate(genList)])
+        return genList
+
+    def __updateTransactions(self, itemSet):
+        """
+        Updates the items in transactions with rank of items according to their support
+
+        :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
+                    rank = {'a':0, 'b':1, 'c':2, 'd':3}
+
+        :param itemSet: list of one-frequent items
+
+        """
+        list1 = []
+        for tr in self.__Database:
+            list2 = []
+            for i in range(len(tr)):
+                if tr[i] in itemSet:
+                    list2.append(self.__rank[tr[i]])
+            if len(list2) >= 1:
+                list2.sort()
+                list1.append(list2)
+        return list1
+
+    @staticmethod
+    def __buildTree(transactions, info):
+        """
+        Builds the tree with updated transactions
+
+        :param transactions: updated transactions
+        :param info: support details of each item in transactions.
+        :return: transactions compressed in fp-tree
+        """
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(transactions)):
+            rootNode.addTransaction(transactions[i], 1)
+        return rootNode
+
+    def __savePeriodic(self, itemSet):
+        """
+        The duplication items and their ranks
+
+        :param itemSet: frequent itemSet that generated
+        :return: patterns with original item names.
+
+        """
+        temp = str()
+        for i in itemSet:
+            temp = temp + self.__rankDup[i] + " "
+        return temp
+
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
-        start pattern mining from here
+        main program to start the operation
+
         """
-        self._startTime = _ab._time.time()
-        self._finalPatterns = {}
-        self._readDatabase()
-        print(len(self._Database), len(self._neighbourList))
-        self._minSup = self._convert(self._minSup)
-        self._getFrequentItems()
-        self._sortTransaction()
-        _FPTree = self._createFPTree()
-        self._finalPatterns.update(dict(_FPTree.mining(self._minSup, self._neighbourList)))
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent Spatial Patterns successfully generated using FSPGrowth")
+        global MIS
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self.__creatingItemSets()
+        self._getMISValues()
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        for x, y in self.__rank.items():
+            MIS[y] = self._MISValues[x]
+            self.__rankDup[y] = x
+        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Frequent patterns were generated successfully using frequentPatternGrowth algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
+    def Mine(self):
+        """
+        main program to start the operation
+
+        """
+        global MIS
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self.__creatingItemSets()
+        self._getMISValues()
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        for x, y in self.__rank.items():
+            MIS[y] = self._MISValues[x]
+            self.__rankDup[y] = x
+        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Frequent patterns were generated successfully using frequentPatternGrowth algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
 
     def getMemoryUSS(self):
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryUSS
+        return self.__memoryUSS
 
     def getMemoryRSS(self):
         """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
+        return self.__memoryRSS
 
     def getRuntime(self):
         """Calculating the total amount of runtime taken by the mining process
 
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
-        return self._endTime - self._startTime
+        return self.__endTime - self.__startTime
 
     def getPatternsAsDataFrame(self):
         """Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
-        for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        for a, b in self.__finalPatterns.items():
+            data.append([a, b])
+            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
-    def save(self, oFile):
-        """
-        Complete set of frequent patterns will be loaded in to a output file
+    def save(self, outFile):
+        """Complete set of frequent patterns will be loaded in to a output file
 
-        :param oFile: name of the output file
-        :type oFile: csv file
+        :param outFile: name of the output file
+        :type outFile: file
         """
-        self._oFile = oFile
+        self._oFile = outFile
         writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
+        for x, y in self.__finalPatterns.items():
+            s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
-        """
-        Function to send the set of frequent patterns after completion of the mining process
+        """ Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
+        return self.__finalPatterns
 
-        return self._finalPatterns
-
-    def printResults(self):
+    def printResults(self) -> None:
         """
-        This function is used to print the results
+        this function is used to print the results
+        :return: None
         """
-        print("Total number of Spatial Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = FSPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = FSPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+    if len(_fp._sys.argv) == 4 or len(_fp._sys.argv) == 5:
+        if len(_fp._sys.argv) == 5:
+            _ap = CFPGrowthPlus(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4])
+        if len(_fp._sys.argv) == 4:
+            _ap = CFPGrowthPlus(_fp._sys.argv[1], _fp._sys.argv[3])
         _ap.startMine()
-        print("Total number of Spatial Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
+        _Patterns = _ap.getPatterns()
+        print("Total number of Frequent Patterns:", len(_Patterns))
+        _ap.savePatterns(_fp._sys.argv[2])
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

### Comparing `pami-2024.3.9.2/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py` & `pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,41 +1,44 @@
 #  SpatialEclat is an Extension of ECLAT algorithm,which  stands for Equivalence Class Clustering and bottom-up
 #  Lattice Traversal.It is one of the popular methods of Association Rule mining. It is a more efficient and
 #  scalable version of the Apriori algorithm.
 #
 #  **Importing this algorithm into a python program**
 #  ---------------------------------------------------
 #
-#     from PAMI.georeferencedFrequentPattern.basic import SpatialECLAT as alg
+#             from PAMI.georeferencedFrequentPattern.basic import SpatialECLAT as alg
 #
-#     obj = alg.SpatialECLAT("sampleTDB.txt", "sampleN.txt", 5)
+#             obj = alg.SpatialECLAT("sampleTDB.txt", "sampleN.txt", 5)
 #
-#     obj.startMine()
+#             obj.mine()
 #
-#     spatialFrequentPatterns = obj.getPatterns()
+#             spatialFrequentPatterns = obj.getPatterns()
 #
-#     print("Total number of Spatial Frequent Patterns:", len(spatialFrequentPatterns))
+#             print("Total number of Spatial Frequent Patterns:", len(spatialFrequentPatterns))
 #
-#     obj.save("outFile")
+#             obj.save("outFile")
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
+#
+
+
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -46,25 +49,40 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.georeferencedFrequentPattern.basic import abstract as _ab
+from deprecated import deprecated
 
 
 class SpatialECLAT(_ab._spatialFrequentPatterns):
     """
     :Description:   Spatial Eclat is a Extension of ECLAT algorithm,which  stands for Equivalence Class Clustering and bottom-up
                     Lattice Traversal.It is one of the popular methods of Association Rule mining. It is a more efficient and
                     scalable version of the Apriori algorithm.
 
     :Reference:   Rage, Uday & Fournier Viger, Philippe & Zettsu, Koji & Toyoda, Masashi & Kitsuregawa, Masaru. (2020).
                   Discovering Frequent Spatial Patterns in Very Large Spatiotemporal Databases.
 
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Geo-referenced frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Geo-referenced frequent patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param nFile: str :
+                   Name of the input file to mine complete set of Geo-referenced frequent patterns
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
         nFile: str
             Name of Neighbourhood file name
         minSup: int or float or str
@@ -119,30 +137,37 @@
             A function to get common neighbours of a itemSet
         mapNeighbours(file):
             A function to map items to their neighbours
 
     **Executing the code on terminal :**
     ----------------------------------------
 
-        Format:
-            >>> python3 SpatialECLAT.py <inputFile> <outputFile> <neighbourFile> <minSup>
-            
-        Examples:
-            >>> python3 SpatialECLAT.py sampleTDB.txt output.txt sampleN.txt 0.5 (minSup will be considered in percentage of database transactions)
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 SpatialECLAT.py <inputFile> <outputFile> <neighbourFile> <minSup>
+
+      Example Usage:
+
+      (.venv) $ python3 SpatialECLAT.py sampleTDB.txt output.txt sampleN.txt 0.5
+
+    .. note:: minSup will be considered in percentage of database transactions
+
 
 
     **Sample run of importing the code :**
     ------------------------------------------
     .. code-block:: python
 
         from PAMI.georeferencedFrequentPattern.basic import SpatialECLAT as alg
         
         obj = alg.SpatialECLAT("sampleTDB.txt", "sampleN.txt", 5)
 
-        obj.startMine()
+        obj.mine()
 
         spatialFrequentPatterns = obj.getPatterns()
 
         print("Total number of Spatial Frequent Patterns:", len(spatialFrequentPatterns))
 
         obj.save("outFile")
 
@@ -177,18 +202,17 @@
     _sep = "\t"
 
     def __init__(self, iFile, nFile, minSup, sep="\t"):
         super().__init__(iFile, nFile, minSup, sep)
         self._NeighboursMap = {}
 
     def _creatingItemSets(self):
-        """Storing the complete transactions of the database/input file in a database variable
-
-
-            """
+        """
+        Storing the complete transactions of the database/input file in a database variable
+        """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 self._Database = self._iFile['Transactions'].tolist()
@@ -213,15 +237,17 @@
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
     # function to get frequent one pattern
     def _frequentOneItem(self):
-        """Generating one frequent patterns"""
+        """
+        Generating one frequent patterns
+        """
         self._finalPatterns = {}
         candidate = {}
         for i in range(len(self._Database)):
             for j in range(len(self._Database[i])):
                 if self._Database[i][j] not in candidate:
                     candidate[self._Database[i][j]] = [i]
                 else:
@@ -229,15 +255,17 @@
         self._finalPatterns = {keys: value for keys, value in candidate.items() if len(value) >= self._minSup}
 
     def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
+        :type value: int or float or str
         :return: converted value
+        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
@@ -245,15 +273,16 @@
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
     @staticmethod
     def _dictKeysToInt(iList):
-        """Converting dictionary keys to integer elements
+        """
+        Converting dictionary keys to integer elements
 
         :param iList: Dictionary with patterns as keys and their support count as a value
         :type iList: dict
         :returns: list of integer patterns to represent dictionary keys
         :rtype: list
         """
 
@@ -288,15 +317,16 @@
                 if len(intersectionList) >= self._minSup:
                     itemList.sort()
                     if tuple(itemList) not in tidList:
                         tidList[tuple(set(itemList))] = intersectionList
         return tidList
 
     def _generateSpatialFrequentPatterns(self, tidList):
-        """It will generate the combinations of frequent items from a list of items
+        """
+        It will generate the combinations of frequent items from a list of items
 
         :param tidList :it represents the items with their respective transaction identifiers
         :type tidList: dictionary
         :return: returning transaction dictionary
         :rtype: dict
         """
         tidList1 = {}
@@ -317,15 +347,14 @@
                         tidList1[tuple(itemList)] = intersectionList
 
         return tidList1
 
     def _getNeighbourItems(self, keySet):
         """
         A function to get Neighbours of a item
-
         :param keySet:itemSet
         :type keySet:str or tuple
         :return: set of common neighbours
         :rtype:set
         """
         itemNeighbours = self._NeighboursMap.keys()
         if isinstance(keySet, str):
@@ -338,15 +367,14 @@
             for j in range(0, len(keySet)):
                 i = keySet[j]
                 itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(i))))
         return itemNeighbours
 
     def _mapNeighbours(self):
         """
-
         A function to map items to their Neighbours
         """
         self._NeighboursMap = {}
         if isinstance(self._nFile, _ab._pd.DataFrame):
             data, items = [], []
             if self._nFile.empty:
                 print("its empty..")
@@ -375,16 +403,52 @@
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._NeighboursMap[temp[0]] = temp[1:]
                 except IOError:
                     print("File Not Found")
                     quit()
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
-        """Frequent pattern mining process will start from here"""
+        """
+        Frequent pattern mining process will start from here
+        """
+
+        # global items_sets, endTime, startTime
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._mapNeighbours()
+        self._finalPatterns = {}
+        self._frequentOneItem()
+        frequentSet = self._generateSpatialFrequentPatterns(self._finalPatterns)
+        for x, y in frequentSet.items():
+            if x not in self._finalPatterns:
+                self._finalPatterns[x] = y
+        while 1:
+            frequentSet = self._eclatGeneration(frequentSet)
+            for x, y in frequentSet.items():
+                if x not in self._finalPatterns:
+                    self._finalPatterns[x] = y
+            if len(frequentSet) == 0:
+                break
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Spatial Frequent patterns were generated successfully using SpatialECLAT algorithm")
+
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
 
         # global items_sets, endTime, startTime
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
@@ -407,43 +471,46 @@
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Spatial Frequent patterns were generated successfully using SpatialECLAT algorithm")
 
     def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
-
+        """
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
+        """
+        Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
@@ -455,41 +522,44 @@
                 for i in a:
                     pat = pat + a + ' '
             data.append([pat.strip(), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
     def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to a output file
+        """
+        Complete set of frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             pat = str()
             if type(x) == str:
                 pat = x 
             if type(x) == list:
                 for i in x:
                     pat = pat + x + '\t'
             patternsAndSupport = pat.strip() + ":" + str(len(y))
             writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
+        """
+        Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
-        """ This function is used to print the results
+        """
+        This function is used to print the results
         """
         print("Total number of Spatial Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
@@ -497,14 +567,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = SpatialECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = SpatialECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Spatial Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/georeferencedFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/georeferencedFrequentSequencePattern/abstract.py` & `pami-2024.4.9.1/PAMI/georeferencedFrequentSequencePattern/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,36 +1,39 @@
-# STEclat is one of the fundamental algorithm to discover geo refereneced partial periodic-frequent patterns in a transactional database.
-#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     import PAMI.georeferencedPartialPeriodicPattern.STEclat as alg
 #
-#     obj = alg.STEclat("sampleTDB.txt", "sampleN.txt", 3, 4)
+#             import PAMI.periodicFrequentPattern.topk.TopkPFPGrowth as alg
+#
+#             obj = alg.TopkPFPGrowth(iFile, k, maxPer,oFile)
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     partialPeriodicSpatialPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#     print("Total number of Periodic Spatial Frequent Patterns:", len(partialPeriodicSpatialPatterns))
+#             print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#     obj.save("outFile")
+#             obj.save(oFile)
 #
-#     memUSS = obj.getMemoryUSS()
+#             Df = obj.getPatternInDataFrame()
 #
-#     print("Total Memory in USS:", memUSS)
+#             memUSS = obj.getMemoryUSS()
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     run = obj.getRuntime()
+#             print("Total Memory in RSS", memRSS)
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             run = obj.getRuntime()
 #
+#             print("Total ExecutionTime in seconds:", run)
+#
+
+
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -43,146 +46,148 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-
-from PAMI.georeferencedPartialPeriodicPattern.basic import abstract as _ab
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
 
 
-class STEclat(_ab._partialPeriodicSpatialPatterns):
+class TopkPFPGrowth(_ab._periodicFrequentPatterns):
     """
-    :Description:   STEclat is one of the fundamental algorithm to discover georefereneced partial periodic-frequent patterns in a transactional database.
+    :Description:   Top - K is and algorithm to discover top periodic frequent patterns in a temporal database.
+
+    :Reference:   Komate Amphawan, Philippe Lenca, Athasit Surarerks: "Mining Top-K Periodic-Frequent Pattern from Transactional Databases without Support Threshold"
+                  International Conference on Advances in Information Technology: https://link.springer.com/chapter/10.1007/978-3-642-10392-6_3
 
-    :Reference:   R. Uday Kiran, C. Saideep, K. Zettsu, M. Toyoda, M. Kitsuregawa and P. Krishna Reddy,
-                 "Discovering Partial Periodic Spatial Patterns in Spatiotemporal Databases," 2019 IEEE International
-                  Conference on Big Data (Big Data), 2019, pp. 233-238, doi: 10.1109/BigData47090.2019.9005693.
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of periodic frequent pattern's
+    :param  oFile: str :
+                   Name of the output file to store complete set of periodic frequent pattern's
+    :param  maxPer: str:
+                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
+
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        nFile: str:
-           Name of Neighbourhood file name
-        maxIAT: float or int or str
-            The user can specify maxIAT either in count or proportion of database size.
-            If the program detects the data type of maxIAT is integer, then it treats maxIAT is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxIAT=10 will be treated as integer, while maxIAT=10.0 will be treated as float
-        minPS: float or int or str
-            The user can specify minPS either in count or proportion of database size.
-            If the program detects the data type of minPS is integer, then it treats minPS is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minPS=10 will be treated as integer, while minPS=10.0 will be treated as float
+        k: int
+            User specified counte of top frequent patterns
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
+        oFile : str
+            Name of the output file or the path of the output file
         startTime:float
             To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
-        oFile : str
-            Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        Database : list
-            To store the complete set of transactions available in the input database/file
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
-        getPatternsAsDataFrames()
+        getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets(iFileName)
-            Storing the complete transactions of the database/input file in a database variable
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
         frequentOneItem()
-            Generating one frequent patterns
-        convert(value):
-            To convert the given user specified value
-        getNeighbourItems(keySet):
-            A function to get common neighbours of a itemSet
-         mapNeighbours(file):
-            A function to map items to their neighbours
-
-    **Executing the code on terminal :**
-    --------------------------------------
+            Generates one frequent patterns
+        eclatGeneration(candidateList)
+            It will generate the combinations of frequent items
+        generateFrequentPatterns(tidList)
+            It will generate the combinations of frequent items from a list of items
+
+    **Executing the code on terminal:**
+    -------------------------------------
+   .. code-block:: console
+
+
+       Format:
 
-        Format:
-            >>> python3 STEclat.py <inputFile> <outputFile> <neighbourFile>  <minPS>  <maxIAT>
+       (.venv) $ python3 TopkPFP.py <inputFile> <outputFile> <k> <maxPer>
 
-        Examples:
-            >>> python3 STEclat.py sampleTDB.txt output.txt sampleN.txt 0.2 0.5 (maxIAT & minPS will be considered in percentage of database transactions)
+       Examples:
 
-    **Sample run of importing the code :**
-    --------------------------------------
+       (.venv) $ python3 TopkPFP.py sampleDB.txt patterns.txt 10 3
+
+
+    **Sample run of the importing code:**
+    ---------------------------------------
     .. code-block:: python
-    
-            import PAMI.georeferencedPartialPeriodicPattern.STEclat as alg
 
-            obj = alg.STEclat("sampleTDB.txt", "sampleN.txt", 3, 4)
+            import PAMI.periodicFrequentPattern.topk.TopkPFPGrowth as alg
+
+            obj = alg.TopkPFPGrowth(iFile, k, maxPer)
 
             obj.startMine()
 
-            partialPeriodicSpatialPatterns = obj.getPatterns()
+            periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of Periodic Spatial Frequent Patterns:", len(partialPeriodicSpatialPatterns))
+            print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
 
-            obj.save("outFile")
+            obj.save(oFile)
+
+            Df = obj.getPatternInDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    -------------
-        The complete program was written by P. Likhitha under the supervision of Professor Rage Uday Kiran.
+    --------------
+            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+
     """
 
-    _maxIAT = " "
-    _minPS = " "
     _startTime = float()
     _endTime = float()
+    _k = int()
+    _maxPer = " "
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
-    _nFile = " "
+    _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _sep = "\t"
-    _lno = 0
-
-    def __init__(self, iFile, nFile, minPS, maxIAT, sep="\t"):
-        super().__init__(iFile, nFile, minPS, maxIAT,  sep)
-        self._NeighboursMap = {}
+    _tidList = {}
+    _lno = int()
+    _minimum = int()
+    _mapSupport = {}
 
     def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
 
         """
         self._Database = []
@@ -191,19 +196,20 @@
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'TS' in i:
                 ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 data = self._iFile['Transactions'].tolist()
+            if 'Patterns' in i:
+                data = self._iFile['Patterns'].tolist()
             for i in range(len(data)):
                 tr = [ts[i][0]]
                 tr = tr + data[i]
                 self._Database.append(tr)
-
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
@@ -217,39 +223,14 @@
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    # function to get frequent one pattern
-    def _frequentOneItem(self):
-        """Generating one frequent patterns"""
-        self._tidList = {}
-        self._mapSupport = {}
-        self._maxIAT = self._convert(self._maxIAT)
-        for line in self._Database:
-            s = line
-            n = int(s[0])
-            for i in range(1, len(s)):
-                si = s[i]
-                if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [0, n]
-                    self._tidList[si] = [n]
-                else:
-                    lp = n - self._mapSupport[si][1]
-                    if lp <= self._maxIAT:
-                        self._mapSupport[si][0] += 1
-                    self._mapSupport[si][1] = n
-                    self._tidList[si].append(n)
-        self._minPS = self._convert(self._minPS)
-        self._mapSupport = {k: v[0] for k, v in self._mapSupport.items() if v[0] >= self._minPS}
-        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        return plist
-
     def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
         :return: converted value
         """
@@ -261,263 +242,291 @@
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _getPeriodicSupport(self, timeStamps):
+    def _frequentOneItem(self):
+        """
+        Generating one frequent patterns
         """
-        calculates the support and periodicity with list of timestamps
 
-        :param timeStamps: timestamps of a pattern
-        :type timeStamps: list
+        self._mapSupport = {}
+        self._tidList = {}
+        n = 0
+        for line in self._Database:
+            self._lno += 1
+            n = int(line[0])
+            for i in range(1, len(line)):
+                si = line[i]
+                if self._mapSupport.get(si) is None:
+                    self._mapSupport[si] = [1, abs(0 - n), n]
+                    self._tidList[si] = [n]
+                else:
+                    self._mapSupport[si][0] += 1
+                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
+                    self._mapSupport[si][2] = n
+                    self._tidList[si].append(n)
+        for x, y in self._mapSupport.items():
+            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(n - self._mapSupport[x][2]))
+        self._maxPer = self._convert(self._maxPer)
+        self._k = self._convert(self._k)
+        self._mapSupport = {k: [v[0], v[1]] for k, v in self._mapSupport.items() if v[1] <= self._maxPer}
+        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
+        self._finalPatterns = {}
+        #print(len(plist))
+        for i in plist:
+            if len(self._finalPatterns) >= self._k:
+                break
+            else:
+                self._finalPatterns[i] = [self._mapSupport[i][0], self._mapSupport[i][1]]
+        self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
+        plist = list(self._finalPatterns.keys())
+        return plist
+
+    def _getSupportAndPeriod(self, timeStamps):
+        """To calculate the periodicity and support
+
+        :param timeStamps: Timestamps of an item set
+        :return: support, periodicity
         """
+
+        global lno
         timeStamps.sort()
-        per = 0
-        for i in range(len(timeStamps) - 1):
-            j = i + 1
-            if abs(timeStamps[j] - timeStamps[i]) <= self._maxIAT:
-                per += 1
-        return per
+        cur = 0
+        per = list()
+        sup = 0
+        for j in range(len(timeStamps)):
+            per.append(timeStamps[j] - cur)
+            cur = timeStamps[j]
+            sup += 1
+        per.append(self._lno - cur)
+        if len(per) == 0:
+            return [0, 0]
+        return [sup, max(per)]
 
-    def _save(self, prefix, suffix, tidSetX):
-        """
-        Saves the patterns that satisfy the periodic frequent property.
+    def _save(self, prefix, suffix, tidSetI):
+        """Saves the patterns that satisfy the periodic frequent property.
 
         :param prefix: the prefix of a pattern
-        :type prefix: list or None
+        :type prefix: list
         :param suffix: the suffix of a patterns
         :type suffix: list
-        :param tidSetX: the timestamp of a patterns
-        :type tidSetX: list
-
-
+        :param tidSetI: the timestamp of a patterns
+        :type tidSetI: list
         """
+
         if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
-        val = self._getPeriodicSupport(tidSetX)
-        if val >= self._minPS:
-            self._finalPatterns[tuple(prefix)] = val
+        val = self._getSupportAndPeriod(tidSetI)
+        sample = str()
+        for i in prefix:
+            sample = sample + i + " "
+        if len(self._finalPatterns) < self._k:
+            if val[0] >= self._minimum:
+                self._finalPatterns[sample] = val
+                self._finalPatterns = {k: v for k, v in
+                                  sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
+                self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
+        else:
+            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1][0]):
+                if val[0] > y[0]:
+                    del self._finalPatterns[x]
+                    self._finalPatterns[x] = y
+                    self._finalPatterns = {k: v for k, v in
+                                          sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
+                    self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
+                    return
 
     def _Generation(self, prefix, itemSets, tidSets):
         """
-        Generates the patterns that satisfy the periodic frequent property.
+        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
 
-        :param prefix: the prefix of a pattern
-        :type prefix: list or None
-        :param itemSets: the item sets of a patterns
+        :param prefix:  main equivalence prefix
+        :type prefix: periodic-frequent item or pattern
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity
+                            and frequent with their timestamps
         :type itemSets: list
-        :param tidSets: the timestamp of a patterns
+        :param tidSets: timestamps of the items in the argument itemSets
         :type tidSets: list
         """
         if len(itemSets) == 1:
             i = itemSets[0]
-            tidi = tidSets[0]
-            self._save(prefix, [i], tidi)
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
             return
         for i in range(len(itemSets)):
             itemI = itemSets[i]
             if itemI is None:
                 continue
-            tidSetX = tidSets[i]
+            tidSetI = tidSets[i]
             classItemSets = []
             classTidSets = []
             itemSetX = [itemI]
             for j in range(i + 1, len(itemSets)):
                 itemJ = itemSets[j]
                 tidSetJ = tidSets[j]
-                y = list(set(tidSetX).intersection(tidSetJ))
-                val = self._getPeriodicSupport(y)
-                if val >= self._minPS:
+                y = list(set(tidSetI).intersection(tidSetJ))
+                val = self._getSupportAndPeriod(y)
+                if val[0] >= self._minimum and val[1] <= self._maxPer:
                     classItemSets.append(itemJ)
                     classTidSets.append(y)
-            newprefix = list(set(itemSetX)) + prefix
-            self._Generation(newprefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetX)
-
-    def _getNeighbourItems(self, keySet):
-        """
-        A function to get Neighbours of an item
-
-        :param keySet:itemSet
-        :type keySet:str or tuple
-        :return: set of common neighbours
-        :rtype:set
-        """
-        itemNeighbours = self._NeighboursMap.keys()
-        if isinstance(keySet, str):
-            if self._NeighboursMap.get(keySet) is None:
-                return []
-            itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(keySet))))
-        if isinstance(keySet, tuple):
-            keySet = list(keySet)
-            for j in range(0, len(keySet)):
-                i = keySet[j]
-                itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(i))))
-        return itemNeighbours
-
-    def mapNeighbours(self):
-        """
-        A function to map items to their Neighbours
-        """
-        self._NeighboursMap = {}
-        if isinstance(self._nFile, _ab._pd.DataFrame):
-            data = []
-            if self._nFile.empty:
-                print("its empty..")
-            i = self._nFile.columns.values.tolist()
-            if 'Neighbours' in i:
-                data = self._nFile['Neighbours'].tolist()
-            for i in data:
-                self._NeighboursMap[i[0]] = i[1:]
-        if isinstance(self._nFile, str):
-            if _ab._validators.url(self._nFile):
-                data = _ab._urlopen(self._nFile)
-                for line in data:
-                    line.strip()
-                    line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    self._NeighboursMap[temp[0]] = temp[1:]
-            else:
-                try:
-                    with open(self._nFile, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self._NeighboursMap[temp[0]] = temp[1:]
-                except IOError:
-                    print("File Not Found")
-                    quit()
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetI)
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
-        """Frequent pattern mining process will start from here"""
-
-        # global items_sets, endTime, startTime
+        """
+        Main function of the program
+        """
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
+        if self._k is None:
+            raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
-        #self._minSup = self._convert(self._minSup)
-        self.mapNeighbours()
-        self._finalPatterns = {}
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemX = plist[i]
-            tidSetX = self._tidList[itemX]
-            itemSetX = [itemX]
+        _plist = self._frequentOneItem()
+        for i in range(len(_plist)):
+            itemI = _plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
             itemSets = []
             tidSets = []
-            neighboursItems = self._getNeighbourItems(plist[i])
-            for j in range(i + 1, len(plist)):
-                if not plist[j] in neighboursItems:
-                    continue
-                itemJ = plist[j]
+            for j in range(i + 1, len(_plist)):
+                itemJ = _plist[j]
                 tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetX).intersection(tidSetJ))
-                val = self._getPeriodicSupport(y1)
-                if val >= self._minPS:
+                y1 = list(set(tidSetI).intersection(tidSetJ))
+                val = self._getSupportAndPeriod(y1)
+                if val[0] >= self._minimum and val[1] <= self._maxPer:
                     itemSets.append(itemJ)
                     tidSets.append(y1)
             self._Generation(itemSetX, itemSets, tidSets)
-            self._save(None, itemSetX, tidSetX)
+        print("TopK Periodic Frequent patterns were generated successfully")
         self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
+        _process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
         self._memoryUSS = float()
+        self._memoryUSS = _process.memory_full_info().uss
+        self._memoryRSS = _process.memory_info().rss
+
+    def Mine(self):
+        """
+        Main function of the program
+        """
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._k is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        _plist = self._frequentOneItem()
+        for i in range(len(_plist)):
+            itemI = _plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(_plist)):
+                itemJ = _plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetI).intersection(tidSetJ))
+                val = self._getSupportAndPeriod(y1)
+                if val[0] >= self._minimum and val[1] <= self._maxPer:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print("TopK Periodic Frequent patterns were generated successfully")
+        self._endTime = _ab._time.time()
+        _process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Spatial Periodic Frequent patterns were generated successfully using SpatialEclat algorithm")
+        self._memoryUSS = float()
+        self._memoryUSS = _process.memory_full_info().uss
+        self._memoryRSS = _process.memory_info().rss
 
     def getMemoryUSS(self):
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
+        """
+        Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            pat = ""
-            for i in a:
-                pat += str(i) + ' '
-            data.append([pat, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicSupport'])
+            data.append([a, b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
     def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to an output file
+        """Complete set of frequent patterns will be loaded in to a output file
+
         :param outFile: name of the output file
         :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            pat = ""
-            for i in x:
-                pat += str(i) + '\t'
-            patternsAndSupport = pat.strip() + ": " + str(y)
+            patternsAndSupport = x.replace(' ', '\t') + ":" + f'{y[0]}:{y[1]}'
             writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
         """ Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
-        """
-        This function is used to print the results
-        """
-        print("Total number of  Spatial Partial Periodic Patterns:", len(self.getPatterns()))
+        print("Top K Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
-        if len(_ab._sys.argv) == 7:
-            _ap = STEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
-            _ap = STEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+            _ap = TopkPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:
+            _ap = TopkPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        print("Total number of Spatial Frequent Patterns:", len(_ap.getPatterns()))
+        print("Top K Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in USS:",  _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:",  _ap.getRuntime())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
-
-
```

### Comparing `pami-2024.3.9.2/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/highUtilityFrequentPattern/basic/HUFIM.py` & `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/EFIM.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,45 +1,44 @@
-# HUFIM (High Utility Frequent Itemset Miner) algorithm helps us to mine High Utility Frequent ItemSets (HUFIs) from transactional databases.
+# EFIM is one of the fastest algorithm to mine High Utility ItemSets from transactional databases.
+#
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
+#             from PAMI.highUtilityPattern.basic import EFIM as alg
 #
-#             from PAMI.highUtilityFrequentPattern.basic import HUFIM as alg
-#
-#             obj =alg.HUFIM("input.txt", 35, 20)
+#             obj=alg.EFIM("input.txt",35)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             Patterns = obj.getPatterns()
 #
-#             print("Total number of high utility frequent Patterns:", len(Patterns))
+#             print("Total number of high utility Patterns:", len(Patterns))
 #
 #             obj.save("output")
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
-
-
+#
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -49,111 +48,106 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-
-from PAMI.highUtilityFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Union
+from PAMI.highUtilityPattern.basic import abstract as _ab
+from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 
 class _Transaction:
     """
-    A class to store Transaction of a database
+        A class to store Transaction of a database
 
     :Attributes:
 
         items: list
             A list of items in transaction 
         utilities: list
             A list of utilities of items in transaction
         transactionUtility: int
             represent total sum of all utilities in the database
         prefixUtility:
             prefix Utility values of item
         offset:
             an offset pointer, used by projected transactions
-        support:
-            maintains the support of the transaction
     :Methods:
 
         projectedTransaction(offsetE):
             A method to create new Transaction from existing starting from offsetE until the end
         getItems():
             return items in transaction
         getUtilities():
             return utilities in transaction
         getLastPosition():
             return last position in a transaction
         removeUnpromisingItems():
             A method to remove items which are having low values when compared with minUtil
         insertionSort():
             A method to sort all items in the transaction
-        getSupport():
-            returns the support of the transaction
     """
     offset = 0
     prefixUtility = 0
-    support = 1
 
-    def __init__(self, items: List[int], utilities: List[int], transactionUtility: int) -> None:
+    def __init__(self, items: list, utilities: list, transactionUtility: int) -> None:
         self.items = items
         self.utilities = utilities
         self.transactionUtility = transactionUtility
-        self.support = 1
 
     def projectTransaction(self, offsetE: int) -> '_Transaction':
         """
         A method to create new Transaction from existing transaction starting from offsetE until the end
         :param offsetE: an offset over the original transaction for projecting the transaction
         :type offsetE: int
+        :return: a new transaction after projecting the transaction starting from offsetE until the end of the transaction
+        :rtype: _Transaction
         """
         new_transaction = _Transaction(self.items, self.utilities, self.transactionUtility)
         utilityE = self.utilities[offsetE]
         new_transaction.prefixUtility = self.prefixUtility + utilityE
         new_transaction.transactionUtility = self.transactionUtility - utilityE
-        new_transaction.support = self.support
         for i in range(self.offset, offsetE):
             new_transaction.transactionUtility -= self.utilities[i]
         new_transaction.offset = offsetE + 1
         return new_transaction
 
-    def getItems(self) -> List[int]:
+    def getItems(self) -> list:
         """
         A method to return items in transaction
+        :return: list of items in transaction after projecting the transaction starting from offsetE until the end of the transaction
+        :rtype: list
         """
         return self.items
 
-    def getUtilities(self) -> List[int]:
+    def getUtilities(self) -> list:
         """
         A method to return utilities in transaction
+        :return: list of utilities in transaction
+        :rtype: list
         """
         return self.utilities
 
     def getLastPosition(self) -> int:
         """
         A method to return last position in a transaction
+        :return: last position in a transaction after projecting the transaction starting from offsetE until the end of the transaction
+        :rtype: int
         """
 
         return len(self.items) - 1
 
-    def getSupport(self) -> int:
-        """
-        A method to return support in a transaction
-        """
-
-        return self.support
-
-    def removeUnpromisingItems(self, oldNamesToNewNames: Dict[int, int]) -> None:
+    def removeUnpromisingItems(self, oldNamesToNewNames: dict) -> None:
         """
         A method to remove items which are not present in the map passed to the function
         :param oldNamesToNewNames: A map represent old names to new names
         :type oldNamesToNewNames: map
+        :return: None
         """
         tempItems = []
         tempUtilities = []
         for idx, item in enumerate(self.items):
             if item in oldNamesToNewNames:
                 tempItems.append(oldNamesToNewNames[item])
                 tempUtilities.append(self.utilities[idx])
@@ -162,14 +156,15 @@
         self.items = tempItems
         self.utilities = tempUtilities
         self.insertionSort()
 
     def insertionSort(self) -> None:
         """
         A method to sort items in order
+        :return: None
         """
         for i in range(1, len(self.items)):
             key = self.items[i]
             utilityJ = self.utilities[i]
             j = i - 1
             while j >= 0 and key < self.items[j]:
                 self.items[j + 1] = self.items[j]
@@ -199,40 +194,43 @@
         getTransactions():
             return transactions in database
 
     """
     transactions = []
     maxItem = 0
     
-    def __init__(self, datasetPath: Union[str, _ab._pd.DataFrame], sep: str) -> None:
+    def __init__(self,datasetPath: Union[str, _ab._pd.DataFrame], sep: str) -> None:
         self.strToInt = {}
         self.intToStr = {}
+        self.transactions = []
+        self.maxItem = 0
         self.cnt = 1
         self.sep = sep
-        self.createItemSets(datasetPath)
+        self.createItemsets(datasetPath)
 
-    def createItemSets(self, datasetPath: List[str]) -> None:
+    def createItemsets(self, datasetPath: Union[str, _ab._pd.DataFrame]) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :param datasetPath: It represents the peth for the dataset
+        :type datasetPath: str
+        :return: None
         """
         self.Database = []
-        self.transactions = []
         if isinstance(datasetPath, _ab._pd.DataFrame):
-            utilities, data, utilitySum = [], [], []
+            utilities, data, transactionUtility = [], [], []
             if datasetPath.empty:
                 print("its empty..")
             i = datasetPath.columns.values.tolist()
             if 'Transactions' in i:
                 data = datasetPath['Transactions'].tolist()
             if 'Utilities' in i:
                 utilities = datasetPath['Utilities'].tolist()
             if 'UtilitySum' in i:
-                utilitySum = datasetPath['UtilitySum'].tolist()
-            for k in range(len(data)):
-                self.transactions.append(self.createTransaction(data[k], utilities[k], utilitySum[k]))
+                transactionUtility = datasetPath['UtilitySum'].tolist()
+            self.transactions.append(self.createTransaction(data, utilities, transactionUtility))
         if isinstance(datasetPath, str):
             if _ab._validators.url(datasetPath):
                 data = _ab._urlopen(datasetPath)
                 for line in data:
                     line = line.decode("utf-8")
                     trans_list = line.strip().split(':')
                     transactionUtility = int(trans_list[1])
@@ -247,37 +245,41 @@
                         for line in f:
                             trans_list = line.strip().split(':')
                             transactionUtility = int(trans_list[1])
                             itemsString = trans_list[0].strip().split(self.sep)
                             itemsString = [x for x in itemsString if x]
                             utilityString = trans_list[2].strip().split(self.sep)
                             utilityString = [x for x in utilityString if x]
-                            self.transactions.append(self.createTransaction(itemsString, utilityString, transactionUtility))
+                            self.transactions.append(
+                                self.createTransaction(itemsString, utilityString, transactionUtility))
+
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def createTransaction(self, items: List[str], utilities: List[str], utilitySum: int) -> _Transaction:
+    def createTransaction(self, itemsString: list, utilityString: list, transactionUtility: int) -> '_Transaction':
         """
         A method to create Transaction from dataset given
-            
-        :Attributes:
+        :param itemsString: List of strings representing transactions
+        :type itemsString: list
+        :param utilityString: List of strings representing utility
+        :type utilityString: list
+        :param transactionUtility: Integer representing transaction utility
+        :type transactionUtility: int
+        :return: created Transaction from the given dataset
+        :rtype: _Transaction
+        """
+
 
-        :param items: represent a single line of database
-        :type items: list
-        :param utilities: represent the utilities of items
-        :type utilities: list
-        :param utilitySum: represent  the utilitySum
-        :type items: int
-        :return : Transaction
-        :rtype: Transactions
-        """
-        transactionUtility = utilitySum
-        itemsString = items
-        utilityString = utilities
+        '''trans_list = line.strip().split(':')
+        transactionUtility = int(trans_list[1])
+        itemsString = trans_list[0].strip().split(self.sep)
+        itemsString = [x for x in itemsString if x]
+        utilityString = trans_list[2].strip().split(self.sep)
+        utilityString = [x for x in utilityString if x]'''
         items = []
         utilities = []
         for idx, item in enumerate(itemsString):
             if self.strToInt.get(item) is None:
                 self.strToInt[item] = self.cnt
                 self.intToStr[self.cnt] = item
                 self.cnt += 1
@@ -287,75 +289,85 @@
             items.append(item_int)
             utilities.append(int(utilityString[idx]))
         return _Transaction(items, utilities, transactionUtility)
 
     def getMaxItem(self) -> int:
         """
         A method to return name of the largest item
+        :return: the largest item
+        :rtype: int
         """
         return self.maxItem
 
-    def getTransactions(self) -> List[_Transaction]:
+    def getTransactions(self) -> list:
         """
         A method to return transactions from database
+        :return: the list of transactions from database
+        :rtype: list
         """
         return self.transactions
 
 
-class HUFIM(_ab._utilityPatterns):
+class EFIM(_ab._utilityPatterns):
     """
-    :Description:  HUFIM (High Utility Frequent Itemset Miner) algorithm helps us to mine High Utility Frequent ItemSets (HUFIs) from transactional databases.
+    :Description:   EFIM is one of the fastest algorithm to mine High Utility ItemSets from transactional databases.
+    
+    :Reference:      Zida, S., Fournier-Viger, P., Lin, J.CW. et al. EFIM: a fast and memory efficient algorithm for
+                    high-utility itemset mining. Knowl Inf Syst 51, 595–625 (2017). https://doi.org/10.1007/s10115-016-0986-0
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of High Utility patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of High Utility patterns
+    :param minUtil: int :
+                   The user given minUtil value.
+    :param candidateCount: int
+                   Number of candidates specified by user
+    :param maxMemory: int
+                   Maximum memory used by this program for running
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
-    :Reference:     Kiran, R.U., Reddy, T.Y., Fournier-Viger, P., Toyoda, M., Reddy, P.K., & Kitsuregawa, M. (2019).
-                   Efficiently Finding High Utility-Frequent Itemsets Using Cutoff and Suffix Utility. PAKDD 2019.
-                   DOI: 10.1007/978-3-030-16145-3_15
-    
     :Attributes:
 
         iFile : file
-            Name of the input file to mine complete set of patterns
+            Name of the input file to mine complete set of high utility patterns
         oFile : file
-            Name of the output file to store complete set of patterns
+            Name of the output file to store complete set of high utility patterns
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
         startTime:float
             To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         minUtil : int
             The user given minUtil value
-        minSup : float
-            The user given minSup value
-        highUtilityFrequentItemSets: map
-            set of high utility frequent itemSets
+        highUtilityitemSets: map
+            set of high utility itemSets
         candidateCount: int
              Number of candidates 
         utilityBinArrayLU: list
              A map to hold the local utility values of the items in database
         utilityBinArraySU: list
             A map to hold the subtree utility values of the items is database
         oldNamesToNewNames: list
             A map which contains old names, new names of items as key value pairs
         newNamesToOldNames: list
             A map which contains new names, old names of items as key value pairs
-        singleItemSetsSupport: map
-            A map which maps from single itemsets (items) to their support
-        singleItemSetsUtility: map
-            A map which maps from single itemsets (items) to their utilities
         maxMemory: float
             Maximum memory used by this program for running
         patternCount: int
-            Number of RHUI's
+            Number of HUI's
         itemsToKeep: list
-            keep only the promising items i.e items that can extend other items to form RHUIs
+            keep only the promising items ie items having local utility values greater than or equal to minUtil
         itemsToExplore: list
-            list of items that needs to be explored
+            list of items that have subtreeUtility value greater than or equal to minUtil
 
-    :Methods:
+    :Methods :
 
         startMine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         save(oFile)
                 Complete set of patterns will be loaded in to a output file
@@ -363,53 +375,59 @@
                 Complete set of patterns will be loaded in to a dataframe
         getMemoryUSS()
                 Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
                 Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
                Total amount of runtime taken by the mining process will be retrieved from this function
-        backTrackingHUFIM(transactionsOfP, itemsToKeep, itemsToExplore, prefixLength)
-               A method to mine the RHUIs Recursively
+        backTrackingEFIM(transactionsOfP, itemsToKeep, itemsToExplore, prefixLength)
+               A method to mine the HUIs Recursively
         useUtilityBinArraysToCalculateUpperBounds(transactionsPe, j, itemsToKeep)
                A method to calculate the sub-tree utility and local utility of all items that can extend itemSet P and e
         output(tempPosition, utility)
-               A method to output a relative-high-utility itemSet to file or memory depending on what the user chose
-        isEqual(transaction1, transaction2)
+               A method to output a high-utility itemSet to file or memory depending on what the user chose
+        is_equal(transaction1, transaction2)
                A method to Check if two transaction are identical
         useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(dataset)
               A method to calculate the sub tree utility values for single items
         sortDatabase(self, transactions)
               A Method to sort transaction
-        sortTransaction(self, trans1, trans2)
+        sort_transaction(self, trans1, trans2)
               A Method to sort transaction
         useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset)
-             A method to calculate local utility values for single itemSets
+             A method to calculate local utility values for single itemsets
 
-    **Executing the code on terminal**
-    --------------------------------------------
-        Format:
-                  >>>  python3 HUFIM.py <inputFile> <outputFile> <minUtil> <sep>
-
-        Examples:
-                  >>>  python3 HUFIM.py sampleTDB.txt output.txt 35 20
-                  >>>  python3 HUFIM.py sampleTDB.txt output.txt 35 20
+    **Executing the code on terminal:**
+    ------------------------------------------
 
-    **Sample run of importing the code**
-    -----------------------------------------------
-    .. code-block:: python
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 EFIM.py <inputFile> <outputFile> <minUtil> <sep>
 
-            from PAMI.highUtilityFrequentPattern.basic import HUFIM as alg
+      Example Usage:
 
-            obj=alg.HUFIM("input.txt", 35, 20)
+      (.venv) $ python3 EFIM sampleTDB.txt output.txt 35
 
-            obj.startMine()
+    .. note:: maxMemory will be considered as Maximum memory used by this program for running
+
+    Sample run of importing the code:
+    -------------------------------------
+    .. code-block:: python
+        
+            from PAMI.highUtilityPattern.basic import EFIM as alg
+
+            obj=alg.EFIM("input.txt",35)
+
+            obj.mine()
 
             Patterns = obj.getPatterns()
 
-            print("Total number of high utility frequent Patterns:", len(Patterns))
+            print("Total number of high utility Patterns:", len(Patterns))
 
             obj.save("output")
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
@@ -418,235 +436,239 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
    
     **Credits:**
-    --------------------
-            The complete program was written by pradeep pallikila under the supervision of Professor Rage Uday Kiran.
+    -------------------
+        The complete program was written by pradeep pallikila under the supervision of Professor Rage Uday Kiran.
      
     """
 
-    _highUtilityFrequentItemSets = []
+    _highUtilityitemSets = []
     _candidateCount = 0
     _utilityBinArrayLU = {}
     _utilityBinArraySU = {}
     _oldNamesToNewNames = {}
     _newNamesToOldNames = {}
-    _singleItemSetsSupport = {}
-    _singleItemSetsUtility = {}
     _strToInt = {}
     _intToStr = {}
-    _temp = [0]*5000
+    _Neighbours = {}
+    _temp = [0] * 5000
     _patternCount = int()
     _maxMemory = 0
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
     _iFile = " "
-    _oFile = " "
     _nFile = " "
     _lno = 0
     _sep = "\t"
     _minUtil = 0
-    _minSup = 0
     _memoryUSS = float()
     _memoryRSS = float()
+    _startTime = _ab._time.time()
 
-    def __init__(self, iFile: str, minUtil: Union[int, float], minSup: Union[int, float], sep: str="\t") -> None:
-        super().__init__(iFile, minUtil, minSup, sep)
-
-    def _convert(self, value) -> Union[int, float]:
-        """
-        To convert the given user specified value
-        :param value: user specified value
-        :return: converted value
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._dataset.getTransactions()) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._dataset.getTransactions()) * value)
-            else:
-                value = int(value)
-        return value
-
+    def __init__(self, iFile, minUtil, sep="\t") -> None:
+        super().__init__(iFile, minUtil, sep)
+        self._sep = sep
+        self._highUtilityitemSets = []
+        self._candidateCount = 0
+        self._utilityBinArrayLU = {}
+        self._utilityBinArraySU = {}
+        self._oldNamesToNewNames = {}
+        self._newNamesToOldNames = {}
+        self._strToInt = {}
+        self._intToStr = {}
+        self._Neighbours = {}
+        self._temp = [0] * 5000
+        self._patternCount = 0
+        self._maxMemory = 0
+        self._endTime = float()
+        self._finalPatterns = {}
+        self._lno = 0
+        self._memoryUSS = float()
+        self._memoryRSS = float()
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
-        High Utility Frequent Pattern mining start here
+        Start the EFIM algorithm.
+        :return: None
         """
         self._startTime = _ab._time.time()
-        self._finalPatterns = {}
-        self._dataset = []
         self._dataset = _Dataset(self._iFile, self._sep)
-        self._singleItemSetsSupport = _ab._defaultdict(int)
-        self._singleItemSetsUtility = _ab._defaultdict(int)
         self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
         self._minUtil = int(self._minUtil)
-        self._minSup = self._convert(self._minSup)
         itemsToKeep = []
         for key in self._utilityBinArrayLU.keys():
-            if self._utilityBinArrayLU[key] >= self._minUtil and self._singleItemSetsSupport[key] >= self._minSup:
+            if self._utilityBinArrayLU[key] >= self._minUtil:
                 itemsToKeep.append(key)
-        itemsToKeep = sorted(itemsToKeep, key=lambda x: self._singleItemSetsUtility[x], reverse=True)
+        itemsToKeep = sorted(itemsToKeep, key=lambda x: self._utilityBinArrayLU[x])
         currentName = 1
         for idx, item in enumerate(itemsToKeep):
             self._oldNamesToNewNames[item] = currentName
             self._newNamesToOldNames[currentName] = item
             itemsToKeep[idx] = currentName
             currentName += 1
         for transaction in self._dataset.getTransactions():
             transaction.removeUnpromisingItems(self._oldNamesToNewNames)
         self._sortDatabase(self._dataset.getTransactions())
         emptyTransactionCount = 0
         for transaction in self._dataset.getTransactions():
             if len(transaction.getItems()) == 0:
                 emptyTransactionCount += 1
         self._dataset.transactions = self._dataset.transactions[emptyTransactionCount:]
-        # calculating suffix utility values
-        totalUtility = 0
-        for item in itemsToKeep:
-            totalUtility += self._singleItemSetsUtility[self._newNamesToOldNames[item]]
-        # piItems
-        piItems = []
-        for item in itemsToKeep:
-            if totalUtility >= self._minUtil:
-                piItems.append(item)
-                totalUtility -= self._singleItemSetsUtility[self._newNamesToOldNames[item]]
-            else:
-                break
         self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
         itemsToExplore = []
-        for item in piItems:
+        for item in itemsToKeep:
             if self._utilityBinArraySU[item] >= self._minUtil:
                 itemsToExplore.append(item)
-        self._backTrackingHUFIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
+        self._backTrackingEFIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("High Utility Frequent patterns were generated successfully using HUFIM algorithm")
+        print("High Utility patterns were generated successfully using EFIM algorithm")
 
-    def _backTrackingHUFIM(self, transactionsOfP: List[_Transaction], itemsToKeep: List[int], itemsToExplore: List[int], prefixLength: int) -> None:
+    def mine(self) -> None:
         """
-        A method to mine the HUFIs Recursively
-
-        :Attributes:
+        Start the EFIM algorithm.
+        :return: None
+        """
+        self._startTime = _ab._time.time()
+        self._dataset = _Dataset(self._iFile, self._sep)
+        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
+        self._minUtil = int(self._minUtil)
+        itemsToKeep = []
+        for key in self._utilityBinArrayLU.keys():
+            if self._utilityBinArrayLU[key] >= self._minUtil:
+                itemsToKeep.append(key)
+        itemsToKeep = sorted(itemsToKeep, key=lambda x: self._utilityBinArrayLU[x])
+        currentName = 1
+        for idx, item in enumerate(itemsToKeep):
+            self._oldNamesToNewNames[item] = currentName
+            self._newNamesToOldNames[currentName] = item
+            itemsToKeep[idx] = currentName
+            currentName += 1
+        for transaction in self._dataset.getTransactions():
+            transaction.removeUnpromisingItems(self._oldNamesToNewNames)
+        self._sortDatabase(self._dataset.getTransactions())
+        emptyTransactionCount = 0
+        for transaction in self._dataset.getTransactions():
+            if len(transaction.getItems()) == 0:
+                emptyTransactionCount += 1
+        self._dataset.transactions = self._dataset.transactions[emptyTransactionCount:]
+        self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
+        itemsToExplore = []
+        for item in itemsToKeep:
+            if self._utilityBinArraySU[item] >= self._minUtil:
+                itemsToExplore.append(item)
+        self._backTrackingEFIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("High Utility patterns were generated successfully using EFIM algorithm")
 
+    def _backTrackingEFIM(self, transactionsOfP: list, itemsToKeep: list, itemsToExplore: list, prefixLength: int) -> None:
+        """
+        A method to mine the HUIs Recursively
         :param transactionsOfP: the list of transactions containing the current prefix P
         :type transactionsOfP: list
         :param itemsToKeep: the list of secondary items in the p-projected database
         :type itemsToKeep: list
         :param itemsToExplore: the list of primary items in the p-projected database
         :type itemsToExplore: list
         :param prefixLength: current prefixLength
         :type prefixLength: int
+        :return: None
         """
-        # print("###############")
-        # print("P is", [self.dataset.intToStr.get(x) for x in self.temp[:prefixLength]])
-        # print("items to explore", [self.dataset.intToStr.get(x) for x in [self.newNamesToOldNames[y] for y  in itemsToExplore]])
-        # print("items to keep", [self.dataset.intToStr.get(x) for x in [self.newNamesToOldNames[y] for y in itemsToKeep]])
-        # print("--------------")
         self._candidateCount += len(itemsToExplore)
         for idx, e in enumerate(itemsToExplore):
-            # print("exploring item", self.dataset.intToStr.get(self.newNamesToOldNames[e]))
             transactionsPe = []
             utilityPe = 0
-            supportPe = 0
-            previousTransaction = []
+            previousTransaction = transactionsOfP[0]
             consecutiveMergeCount = 0
             for transaction in transactionsOfP:
                 items = transaction.getItems()
                 if e in items:
                     positionE = items.index(e)
                     if transaction.getLastPosition() == positionE:
                         utilityPe += transaction.getUtilities()[positionE] + transaction.prefixUtility
-                        supportPe += transaction.getSupport()
                     else:
                         projectedTransaction = transaction.projectTransaction(positionE)
                         utilityPe += projectedTransaction.prefixUtility
-                        if previousTransaction == []:
+                        if previousTransaction == transactionsOfP[0]:
                             previousTransaction = projectedTransaction
                         elif self._isEqual(projectedTransaction, previousTransaction):
                             if consecutiveMergeCount == 0:
                                 items = previousTransaction.items[previousTransaction.offset:]
                                 utilities = previousTransaction.utilities[previousTransaction.offset:]
-                                support = previousTransaction.getSupport()
                                 itemsCount = len(items)
                                 positionPrevious = 0
                                 positionProjection = projectedTransaction.offset
                                 while positionPrevious < itemsCount:
                                     utilities[positionPrevious] += projectedTransaction.utilities[positionProjection]
                                     positionPrevious += 1
                                     positionProjection += 1
                                 previousTransaction.prefixUtility += projectedTransaction.prefixUtility
                                 sumUtilities = previousTransaction.prefixUtility
                                 previousTransaction = _Transaction(items, utilities, previousTransaction.transactionUtility + projectedTransaction.transactionUtility)
                                 previousTransaction.prefixUtility = sumUtilities
-                                previousTransaction.support = support
-                                previousTransaction.support += projectedTransaction.getSupport()
                             else:
                                 positionPrevious = 0
                                 positionProjected = projectedTransaction.offset
                                 itemsCount = len(previousTransaction.items)
                                 while positionPrevious < itemsCount:
                                     previousTransaction.utilities[positionPrevious] += projectedTransaction.utilities[
                                         positionProjected]
                                     positionPrevious += 1
                                     positionProjected += 1
                                 previousTransaction.transactionUtility += projectedTransaction.transactionUtility
                                 previousTransaction.prefixUtility += projectedTransaction.prefixUtility
-                                previousTransaction.support += projectedTransaction.getSupport()
                             consecutiveMergeCount += 1
                         else:
                             transactionsPe.append(previousTransaction)
-                            supportPe += previousTransaction.getSupport()
                             previousTransaction = projectedTransaction
                             consecutiveMergeCount = 0
                     transaction.offset = positionE
-            if previousTransaction != []:
+            if previousTransaction != transactionsOfP[0]:
                 transactionsPe.append(previousTransaction)
-                supportPe += previousTransaction.getSupport()
-            # print("support is", supportPe)
             self._temp[prefixLength] = self._newNamesToOldNames[e]
-            if (utilityPe >= self._minUtil) and (supportPe >= self._minSup):
-                self._output(prefixLength, utilityPe, supportPe)
-            if supportPe >= self._minSup:
-                self._useUtilityBinArraysToCalculateUpperBounds(transactionsPe, idx, itemsToKeep)
-                newItemsToKeep = []
-                newItemsToExplore = []
-                for l in range(idx + 1, len(itemsToKeep)):
-                    itemK = itemsToKeep[l]
-                    if self._utilityBinArraySU[itemK] >= self._minUtil:
-                        newItemsToExplore.append(itemK)
-                        newItemsToKeep.append(itemK)
-                    elif self._utilityBinArrayLU[itemK] >= self._minUtil:
-                        newItemsToKeep.append(itemK)
-                if len(transactionsPe) != 0:
-                    self._backTrackingHUFIM(transactionsPe, newItemsToKeep, newItemsToExplore, prefixLength + 1)
+            if utilityPe >= self._minUtil:
+                self._output(prefixLength, utilityPe)
+            self._useUtilityBinArraysToCalculateUpperBounds(transactionsPe, idx, itemsToKeep)
+            newItemsToKeep = []
+            newItemsToExplore = []
+            for l in range(idx + 1, len(itemsToKeep)):
+                itemK = itemsToKeep[l]
+                if self._utilityBinArraySU[itemK] >= self._minUtil:
+                    newItemsToExplore.append(itemK)
+                    newItemsToKeep.append(itemK)
+                elif self._utilityBinArrayLU[itemK] >= self._minUtil:
+                    newItemsToKeep.append(itemK)
+            if len(transactionsPe) != 0:
+                self._backTrackingEFIM(transactionsPe, newItemsToKeep, newItemsToExplore, prefixLength + 1)
 
-    def _useUtilityBinArraysToCalculateUpperBounds(self, transactionsPe: List[_Transaction], j: int, itemsToKeep: List[int]) -> None:
+    def _useUtilityBinArraysToCalculateUpperBounds(self, transactionsPe: list, j: int, itemsToKeep: list) -> None:
         """
         A method to  calculate the subtree utility and local utility of all items that can extend itemSet P U {e}
-
-        :Attributes:
-
         :param transactionsPe: transactions the projected database for P U {e}
-        :type transactionsPe: list or Dataset
+        :type transactionsPe: list
         :param j:the position of j in the list of promising items
         :type j:int
         :param itemsToKeep :the list of promising items
-        :type itemsToKeep: list or Dataset
+        :type itemsToKeep: list
+        :return: None
         """
         for i in range(j + 1, len(itemsToKeep)):
             item = itemsToKeep[i]
             self._utilityBinArrayLU[item] = 0
             self._utilityBinArraySU[item] = 0
         for transaction in transactionsPe:
             sumRemainingUtility = 0
@@ -655,41 +677,34 @@
                 item = transaction.getItems()[i]
                 if item in itemsToKeep:
                     sumRemainingUtility += transaction.getUtilities()[i]
                     self._utilityBinArraySU[item] += sumRemainingUtility + transaction.prefixUtility
                     self._utilityBinArrayLU[item] += transaction.transactionUtility + transaction.prefixUtility
                 i -= 1
 
-    def _output(self, tempPosition: int, utility: int, support: int):
+    def _output(self, tempPosition: int, utility: int) -> None:
         """
-         Method to print itemSets
-
-         :Attributes:
-
-         :param tempPosition: position of last item 
-         :type tempPosition : int 
-         :param utility: total utility of itemSet
-         :type utility: int
-         :param support: support of an itemSet
-         :type support: int
+        Method to print high utility items
+        :param tempPosition: position of last item
+        :type tempPosition : int
+        :param utility: total utility of itemSet
+        :type utility: int
+        :return: None
         """
         self._patternCount += 1
         s1 = str()
         for i in range(0, tempPosition+1):
             s1 += self._dataset.intToStr.get((self._temp[i]))
             if i != tempPosition:
                 s1 += "\t"
-        self._finalPatterns[s1] = [utility, support]
+        self._finalPatterns[s1] = str(utility)
 
-    def _isEqual(self, transaction1: _Transaction, transaction2: _Transaction) -> bool:
+    def _isEqual(self, transaction1: '_Transaction', transaction2: '_Transaction') -> bool:
         """
          A method to Check if two transaction are identical
-
-         :Attributes:
-
          :param  transaction1: the first transaction
          :type  transaction1: Trans
          :param  transaction2:    the second transaction
          :type  transaction2: Trans
          :return : whether both are identical or not
          :rtype: bool
         """
@@ -702,142 +717,129 @@
         while position1 < len(transaction1.items):
             if transaction1.items[position1] != transaction2.items[position2]:
                 return False
             position1 += 1
             position2 += 1
         return True
 
-    def _useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self, dataset: _Dataset) -> None:
+    def _useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self, dataset: '_Dataset') -> None:
         """
         Scan the initial database to calculate the subtree utility of each item using a utility-bin array
-
-        :Attributes:
-
         :param dataset: the transaction database
-        :type dataset: Dataset
+        :type dataset: list
+        :return: None
         """
         for transaction in dataset.getTransactions():
             sumSU = 0
             i = len(transaction.getItems()) - 1
             while i >= 0:
                 item = transaction.getItems()[i]
-                currentUtility = transaction.getUtilities()[i]
-                sumSU += currentUtility
+                sumSU += transaction.getUtilities()[i]
                 if item in self._utilityBinArraySU.keys():
                     self._utilityBinArraySU[item] += sumSU
                 else:
                     self._utilityBinArraySU[item] = sumSU
                 i -= 1
 
-    def _sortDatabase(self, transactions: List[_Transaction]) -> None:
+    def _sortDatabase(self, transactions: list) -> None:
         """
-        A Method to sort transaction
-
-        :Attributes:
-
+        A Method to sort transactions
         :param transactions: transaction of items
-        :type transactions: list
-        :return: sorted transactions
-        :rtype: Trans or list
+        :type transactions: Transaction
+        :return: None
         """
-        compareItems = _ab._functools.cmp_to_key(self._sortTransaction)
-        transactions.sort(key=compareItems)
+        cmp_items = _ab._functools.cmp_to_key(self.sort_transaction)
+        transactions.sort(key=cmp_items)
 
-    def _sortTransaction(self, trans1: _Transaction, trans2: _Transaction) -> int:
+    def sort_transaction(self, trans1: '_Transaction', trans2: '_Transaction') -> int:
         """
         A Method to sort transaction
-
-        :Attributes:
-
         :param trans1: the first transaction
         :type trans1: Trans
         :param trans2:the second transaction
         :type trans2: Trans
         :return: sorted transaction
-        :rtype:Trans
+        :rtype: int
         """
-        transItemsX = trans1.getItems()
-        transItemsY = trans2.getItems()
-        pos1 = len(transItemsX) - 1
-        pos2 = len(transItemsY) - 1
-        if len(transItemsX) < len(transItemsY):
+        trans1_items = trans1.getItems()
+        trans2_items = trans2.getItems()
+        pos1 = len(trans1_items) - 1
+        pos2 = len(trans2_items) - 1
+        if len(trans1_items) < len(trans2_items):
             while pos1 >= 0:
-                sub = transItemsY[pos2] - transItemsX[pos1]
+                sub = trans2_items[pos2] - trans1_items[pos1]
                 if sub != 0:
                     return sub
                 pos1 -= 1
                 pos2 -= 1
             return -1
-        elif len(transItemsX) > len(transItemsY):
+        elif len(trans1_items) > len(trans2_items):
             while pos2 >= 0:
-                sub = transItemsY[pos2] - transItemsX[pos1]
+                sub = trans2_items[pos2] - trans1_items[pos1]
                 if sub != 0:
                     return sub
                 pos1 -= 1
                 pos2 -= 1
             return 1
         else:
             while pos2 >= 0:
-                sub = transItemsY[pos2] - transItemsX[pos1]
+                sub = trans2_items[pos2] - trans1_items[pos1]
                 if sub != 0:
                     return sub
                 pos1 -= 1
                 pos2 -= 1
             return 0
 
-    def _useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset: _Dataset) -> None:
+    def _useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset: '_Dataset') -> None:
         """
-        A method to calculate local utility of single itemSets
-        :Attributes:
-
+        A method to calculate local utility of single itemset
         :param dataset: the transaction database
-        :type dataset: databases
-
+        :type dataset: dataset
+        :return: None
         """
         for transaction in dataset.getTransactions():
-            for idx, item in enumerate(transaction.getItems()):
-                self._singleItemSetsSupport[item] += 1
-                self._singleItemSetsUtility[item] += transaction.getUtilities()[idx]
+            for item in transaction.getItems():
                 if item in self._utilityBinArrayLU:
                     self._utilityBinArrayLU[item] += transaction.transactionUtility
                 else:
                     self._utilityBinArrayLU[item] = transaction.transactionUtility
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self) -> '_pd.DataFrame':
         """
         Storing final patterns in a dataframe
         :return: returning patterns in a dataframe
         :rtype: pd.DataFrame
         """
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Utility', 'Support'])
+            data.append([a.replace('\t', ' '), b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Utility'])
 
         return dataFrame
     
-    def getPatterns(self) -> Dict[str, List[Union[int, float]]]:
+    def getPatterns(self) -> dict:
         """
         Function to send the set of patterns after completion of the mining process
         :return: returning patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
+        self.oFile = outFile
+        writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
+            patternsAndSupport = x.strip() + ":" + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
@@ -856,33 +858,42 @@
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime-self._startTime
-    
+
     def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of High Utility Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of High Utility Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
 
+
 if __name__ == '__main__':
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:    #includes separator
-            _ap = HUFIM(_ab._sys.argv[1], int(_ab._sys.argv[3]), float(_ab._sys.argv[4]), _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:    #takes "\t" as a separator
-            _ap = HUFIM(_ab._sys.argv[1], int(_ab._sys.argv[3]), float(_ab._sys.argv[4]))
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+        if len(_ab._sys.argv) == 5:    #includes separator
+            _ap = EFIM(_ab._sys.argv[1], int(_ab._sys.argv[3]), _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:    #takes "\t" as a separator
+            _ap = EFIM(_ab._sys.argv[1], int(_ab._sys.argv[3]))
         _ap.startMine()
-        print("Total number of High Utility Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.mine()
+        print("Total number of High Utility Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total Memory in RSS",  _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
+        _ap = EFIM('/Users/likhitha/Downloads/Utility_T10I4D100K.csv', 50000, '\t')
+        _ap.startMine()
+        _ap.mine()
+        print("Total number of High Utility Patterns:", len(_ap.getPatterns()))
+        _ap.save('/Users/likhitha/Downloads/UPGrowth_output.txt')
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.3.9.2/PAMI/highUtilityFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py` & `pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/HUFIM.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-# Spatial High Utility Frequent ItemSet Mining (SHUFIM) aims to discover all itemSets in a spatioTemporal database
-# that satisfy the user-specified minimum utility, minimum support and maximum distance constraints
+# HUFIM (High Utility Frequent Itemset Miner) algorithm helps us to mine High Utility Frequent ItemSets (HUFIs) from transactional databases.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#             from PAMI.highUtilityGeoreferencedFrequentPattern.basic import SHUFIM as alg
 #
-#             obj=alg.SHUFIM("input.txt","Neighbours.txt",35,20)
+#             from PAMI.highUtilityFrequentPattern.basic import HUFIM as alg
 #
-#             obj.startMine()
+#             obj =alg.HUFIM("input.txt", 35, 20)
 #
-#             patterns = obj.getPatterns()
+#             obj.mine()
 #
-#             print("Total number of Spatial high utility frequent Patterns:", len(patterns))
+#             Patterns = obj.getPatterns()
+#
+#             print("Total number of high utility frequent Patterns:", len(Patterns))
 #
 #             obj.save("output")
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
-
+#
 
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -50,148 +50,166 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 
-from PAMI.highUtilityGeoreferencedFrequentPattern.basic import abstract as _ab
-from functools import cmp_to_key as _comToKey
+from PAMI.highUtilityFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Union
+from deprecated import deprecated
+
 
 class _Transaction:
     """
     A class to store Transaction of a database
 
     :Attributes:
 
         items: list
             A list of items in transaction 
         utilities: list
-            A list of utilites of items in transaction
+            A list of utilities of items in transaction
         transactionUtility: int
             represent total sum of all utilities in the database
-        pmus: list
-            represent the pmu (probable maximum utility) of each element in the transaction
-        prefixutility:
+        prefixUtility:
             prefix Utility values of item
         offset:
             an offset pointer, used by projected transactions
         support:
             maintains the support of the transaction
     :Methods:
 
         projectedTransaction(offsetE):
-            A method to create new Transaction from existing till offsetE
+            A method to create new Transaction from existing starting from offsetE until the end
         getItems():
             return items in transaction
         getUtilities():
             return utilities in transaction
-        getPmus():
-            return pmus in transaction
         getLastPosition():
             return last position in a transaction
         removeUnpromisingItems():
-            A method to remove items with low Utility than minUtil
+            A method to remove items which are having low values when compared with minUtil
         insertionSort():
             A method to sort all items in the transaction
         getSupport():
             returns the support of the transaction
     """
     offset = 0
     prefixUtility = 0
     support = 1
-    
-    def __init__(self, items, utilities, transactionUtility, pmus=None):
+
+    def __init__(self, items: List[int], utilities: List[int], transactionUtility: int) -> None:
         self.items = items
         self.utilities = utilities
         self.transactionUtility = transactionUtility
-        if pmus is not None:
-            self.pmus = pmus
         self.support = 1
 
-    def projectTransaction(self, offsetE):
+    def projectTransaction(self, offsetE: int) -> '_Transaction':
         """
-        A method to create new Transaction from existing till offsetE
+        A method to create new Transaction from existing transaction starting from offsetE until the end
+
         :param offsetE: an offset over the original transaction for projecting the transaction
+
         :type offsetE: int
+
+        :return: a new transaction starting from offsetE until the end of the transaction
+
+        :rtype: _Transaction
         """
-        newTransaction = _Transaction(self.items, self.utilities, self.transactionUtility)
+        new_transaction = _Transaction(self.items, self.utilities, self.transactionUtility)
         utilityE = self.utilities[offsetE]
-        newTransaction.prefixUtility = self.prefixUtility + utilityE
-        newTransaction.transactionUtility = self.transactionUtility - utilityE
-        newTransaction.support = self.support
+        new_transaction.prefixUtility = self.prefixUtility + utilityE
+        new_transaction.transactionUtility = self.transactionUtility - utilityE
+        new_transaction.support = self.support
         for i in range(self.offset, offsetE):
-            newTransaction.transactionUtility -= self.utilities[i]
-        newTransaction.offset = offsetE + 1
-        return newTransaction
+            new_transaction.transactionUtility -= self.utilities[i]
+        new_transaction.offset = offsetE + 1
+        return new_transaction
 
-    def getItems(self):
+    def getItems(self) -> List[int]:
         """
         A method to return items in transaction
-        """
-        return self.items
 
-    def getPmus(self):
-        """
-        A method to return pmus in transaction
+        :return: the list of items in transaction starting from offsetE until the end of the transactions
+
+        :rtype: list
         """
-        return self.pmus
+        return self.items
 
-    def getUtilities(self):
+    def getUtilities(self) -> List[int]:
         """
         A method to return utilities in transaction
+
+        :return: the list of utilities in transaction starting from offsetE until the end of the transaction
+
+        :rtype: list
         """
         return self.utilities
 
-    # get the last position in this transaction
-    def getLastPosition(self):
+    def getLastPosition(self) -> int:
         """
         A method to return last position in a transaction
+
+        :return: the last position in a transaction
+
+        :rtype: int
         """
+
         return len(self.items) - 1
 
-    def getSupport(self):
+    def getSupport(self) -> int:
         """
-        A method to return support of a transaction (number of transactions in the original database having the items present in this transaction)
+        A method to return support in a transaction
+
+        :return: the support in a transaction
+
+        :rtype: int
         """
+
         return self.support
 
-    def removeUnpromisingItems(self, oldNamesToNewNames):
+    def removeUnpromisingItems(self, oldNamesToNewNames: Dict[int, int]) -> None:
         """
-        A method to remove items with low Utility than minUtil
-        :param oldNamesToNewNames: A map represent old namses to new names
+        A method to remove items which are not present in the map passed to the function
+
+        :param oldNamesToNewNames: A map represent old names to new names
+
         :type oldNamesToNewNames: map
+
+        :return: None
         """
         tempItems = []
         tempUtilities = []
         for idx, item in enumerate(self.items):
             if item in oldNamesToNewNames:
                 tempItems.append(oldNamesToNewNames[item])
                 tempUtilities.append(self.utilities[idx])
             else:
                 self.transactionUtility -= self.utilities[idx]
         self.items = tempItems
         self.utilities = tempUtilities
         self.insertionSort()
 
-    def insertionSort(self):
+    def insertionSort(self) -> None:
         """
         A method to sort items in order
+        :return: None
         """
         for i in range(1, len(self.items)):
             key = self.items[i]
             utilityJ = self.utilities[i]
             j = i - 1
             while j >= 0 and key < self.items[j]:
                 self.items[j + 1] = self.items[j]
                 self.utilities[j + 1] = self.utilities[j]
                 j -= 1
             self.items[j + 1] = key
             self.utilities[j + 1] = utilityJ
-
+        
 
 class _Dataset:
     """
     A class represent the list of transactions in this dataset
 
     :Attributes:
 
@@ -209,395 +227,475 @@
         getTransactions():
             return transactions in database
 
     """
     transactions = []
     maxItem = 0
     
-    def __init__(self, datasetPath, sep):
+    def __init__(self, datasetPath: Union[str, _ab._pd.DataFrame], sep: str) -> None:
         self.strToInt = {}
         self.intToStr = {}
         self.cnt = 1
         self.sep = sep
-        self.transactions = []
         self.createItemSets(datasetPath)
 
-    def createItemSets(self, datasetPath):
+    def createItemSets(self, datasetPath: List[str]) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+
+        :param datasetPath: list of paths to the input file to store
+
+        :type datasetPath: list
+
+        :return: None
+
         """
-        pmuString = None
+        self.Database = []
+        self.transactions = []
         if isinstance(datasetPath, _ab._pd.DataFrame):
-            utilities, data, utilitySum, pmuString = [], [], [], []
+            utilities, data, utilitySum = [], [], []
             if datasetPath.empty:
                 print("its empty..")
             i = datasetPath.columns.values.tolist()
             if 'Transactions' in i:
                 data = datasetPath['Transactions'].tolist()
             if 'Utilities' in i:
                 utilities = datasetPath['Utilities'].tolist()
             if 'UtilitySum' in i:
                 utilitySum = datasetPath['UtilitySum'].tolist()
-            if 'pmuString' in i:
-                utilitySum = datasetPath['pmuString'].tolist()
             for k in range(len(data)):
-                self.transactions.append(self.createTransaction(data[k], utilities[k], utilitySum[k], pmuString[k]))
+                self.transactions.append(self.createTransaction(data[k], utilities[k], utilitySum[k]))
         if isinstance(datasetPath, str):
             if _ab._validators.url(datasetPath):
                 data = _ab._urlopen(datasetPath)
                 for line in data:
                     line = line.decode("utf-8")
                     trans_list = line.strip().split(':')
                     transactionUtility = int(trans_list[1])
                     itemsString = trans_list[0].strip().split(self.sep)
                     itemsString = [x for x in itemsString if x]
                     utilityString = trans_list[2].strip().split(self.sep)
                     utilityString = [x for x in utilityString if x]
-                    if len(trans_list) == 4:
-                        pmuString = trans_list[3].strip().split(self.sep)
-                        pmuString = [x for x in pmuString if x]
-                    self.transactions.append(self.createTransaction(itemsString, utilityString, transactionUtility, pmuString))
+                    self.transactions.append(self.createTransaction(itemsString, utilityString, transactionUtility))
             else:
                 try:
                     with open(datasetPath, 'r', encoding='utf-8') as f:
                         for line in f:
                             trans_list = line.strip().split(':')
                             transactionUtility = int(trans_list[1])
                             itemsString = trans_list[0].strip().split(self.sep)
                             itemsString = [x for x in itemsString if x]
                             utilityString = trans_list[2].strip().split(self.sep)
                             utilityString = [x for x in utilityString if x]
-                            if len(trans_list) == 4:
-                                pmuString = trans_list[3].strip().split(self.sep)
-                                pmuString = [x for x in pmuString if x]
-                            self.transactions.append(
-                                self.createTransaction(itemsString, utilityString, transactionUtility, pmuString))
+                            self.transactions.append(self.createTransaction(itemsString, utilityString, transactionUtility))
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def createTransaction(self, items, utilities, utilitySum, pmustring):
+    def createTransaction(self, items: List[str], utilities: List[str], utilitySum: int) -> _Transaction:
         """
         A method to create Transaction from dataset given
-            
-        :Attributes:
 
-        :param items: represent a utility items in a transaction
-        :param utilities: represent utility of an item in transaction
-        :param utilitySum: represent utility sum of  transaction
-        :type items: list of items
+        :param items: represent a single line of database
+
+        :type items: list
+
+        :param utilities: represent the utilities of items
+
         :type utilities: list
+
+        :param utilitySum: represent  the utilitySum
+
         :type utilitySum: int
-        :return : Transaction
-        :rtype: Trans
+
+        :return: a Transaction from given dataset
+
+        :rtype: _Transaction
         """
         transactionUtility = utilitySum
         itemsString = items
         utilityString = utilities
-        pmuString = pmustring
         items = []
         utilities = []
-        pmus = []
         for idx, item in enumerate(itemsString):
-            if (self.strToInt).get(item) is None:
+            if self.strToInt.get(item) is None:
                 self.strToInt[item] = self.cnt
                 self.intToStr[self.cnt] = item
                 self.cnt += 1
-            itemInt = self.strToInt.get(item)
-            if itemInt > self.maxItem:
-                self.maxItem = itemInt
-            items.append(itemInt)
+            item_int = self.strToInt.get(item)
+            if item_int > self.maxItem:
+                self.maxItem = item_int
+            items.append(item_int)
             utilities.append(int(utilityString[idx]))
-            if pmuString != None:
-                pmus.append(int(pmuString[idx]))
-        if pmuString == None:
-            pmus = None
-        return _Transaction(items, utilities, transactionUtility, pmus)
+        return _Transaction(items, utilities, transactionUtility)
 
-    def getMaxItem(self):
+    def getMaxItem(self) -> int:
         """
         A method to return name of the largest item
+
+        :return: the name of the largest item in the dataset
+
+        :rtype: int
         """
         return self.maxItem
 
-    def getTransactions(self):
+    def getTransactions(self) -> List[_Transaction]:
         """
         A method to return transactions from database
+
+        :return: the list of transactions from database which have the highest utility
+
+        :rtype: list
         """
         return self.transactions
 
 
-class SHUFIM(_ab._utilityPatterns):
+class HUFIM(_ab._utilityPatterns):
     """
-    :Description:  Spatial High Utility Frequent ItemSet Mining (SHUFIM) aims to discover all itemSets in a spatioTemporal database
-                   that satisfy the user-specified minimum utility, minimum support and maximum distance constraints
-    :Reference:    10.1007/978-3-030-37188-3_17
+    :Description:  HUFIM (High Utility Frequent Itemset Miner) algorithm helps us to mine High Utility Frequent ItemSets (HUFIs) from transactional databases.
+
+
+    :Reference:     Kiran, R.U., Reddy, T.Y., Fournier-Viger, P., Toyoda, M., Reddy, P.K., & Kitsuregawa, M. (2019).
+                   Efficiently Finding High Utility-Frequent Itemsets Using Cutoff and Suffix Utility. PAKDD 2019.
+                   DOI: 10.1007/978-3-030-16145-3_15
+
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Geo-referenced frequent sequence patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Geo-referenced frequent sequence patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param minUtil: int :
+                   The user given minUtil value.
+    :param candidateCount: int
+                   Number of candidates
+    :param maxMemory: int
+                   Maximum memory used by this program for running
+    :param nFile: str :
+                   Name of the input file to mine complete set of Geo-referenced frequent sequence patterns
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
+    
     :Attributes:
 
         iFile : file
-            Name of the input file to mine complete set of frequent patterns
-        nFile : file
-            Name of the Neighbours file that contain neighbours of items
+            Name of the input file to mine complete set of patterns
         oFile : file
-            Name of the output file to store complete set of frequent patterns
+            Name of the output file to store complete set of patterns
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
         startTime:float
             To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         minUtil : int
-            The user given minUtil
+            The user given minUtil value
         minSup : float
             The user given minSup value
-        highUtilityFrequentSpatialItemSets: map
-            set of high utility itemSets
+        highUtilityFrequentItemSets: map
+            set of high utility frequent itemSets
         candidateCount: int
              Number of candidates 
         utilityBinArrayLU: list
-             A map to hold the pmu values of the items in database
+             A map to hold the local utility values of the items in database
         utilityBinArraySU: list
             A map to hold the subtree utility values of the items is database
         oldNamesToNewNames: list
-            A map to hold the subtree utility values of the items is database
+            A map which contains old names, new names of items as key value pairs
         newNamesToOldNames: list
-            A map to store the old name corresponding to new name
-        Neighbours : map
-            A dictionary to store the neighbours of a item
+            A map which contains new names, old names of items as key value pairs
+        singleItemSetsSupport: map
+            A map which maps from single itemsets (items) to their support
+        singleItemSetsUtility: map
+            A map which maps from single itemsets (items) to their utilities
         maxMemory: float
             Maximum memory used by this program for running
         patternCount: int
-            Number of SHUFI's (Spatial High Utility Frequent Itemsets)
+            Number of RHUI's
         itemsToKeep: list
-            keep only the promising items ie items whose supersets can be required patterns
+            keep only the promising items i.e items that can extend other items to form RHUIs
         itemsToExplore: list
-            keep items that subtreeUtility grater than minUtil
+            list of items that needs to be explored
 
-    :Methods :
+    :Methods:
 
         startMine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         save(oFile)
-                Complete set of frequent patterns will be loaded in to a output file
+                Complete set of patterns will be loaded in to a output file
         getPatternsAsDataFrame()
-                Complete set of frequent patterns will be loaded in to a dataframe
+                Complete set of patterns will be loaded in to a dataframe
         getMemoryUSS()
                 Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
                 Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
                Total amount of runtime taken by the mining process will be retrieved from this function
-        calculateNeighbourIntersection(self, prefixLength)
-               A method to return common Neighbours of items
-        backtrackingEFIM(transactionsOfP, itemsToKeep, itemsToExplore, prefixLength)
-               A method to mine the SHUIs Recursively
-        useUtilityBinArraysToCalculateUpperBounds(transactionsPe, j, itemsToKeep, neighbourhoodList)
-               A method to  calculate the sub-tree utility and local utility of all items that can extend itemSet P and e
+        backTrackingHUFIM(transactionsOfP, itemsToKeep, itemsToExplore, prefixLength)
+               A method to mine the RHUIs Recursively
+        useUtilityBinArraysToCalculateUpperBounds(transactionsPe, j, itemsToKeep)
+               A method to calculate the sub-tree utility and local utility of all items that can extend itemSet P and e
         output(tempPosition, utility)
-               A method ave a high-utility itemSet to file or memory depending on what the user chose
+               A method to output a relative-high-utility itemSet to file or memory depending on what the user chose
         isEqual(transaction1, transaction2)
                A method to Check if two transaction are identical
-        intersection(lst1, lst2)
-               A method that return the intersection of 2 list
         useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(dataset)
-              Scan the initial database to calculate the subtree utility of each items using a utility-bin array
+              A method to calculate the sub tree utility values for single items
         sortDatabase(self, transactions)
-              A Method to sort transaction in the order of PMU
+              A Method to sort transaction
         sortTransaction(self, trans1, trans2)
-              A Method to sort transaction in the order of PMU
+              A Method to sort transaction
         useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset)
-             A method to scan the database using utility bin array to calculate the pmus
+             A method to calculate local utility values for single itemSets
+
+    **Executing the code on terminal**
+    --------------------------------------------
 
-    **Executing the code on terminal :**
-    -----------------------------------------
-            Format:
-                    >>> python3 SHUFIM.py <inputFile> <outputFile> <Neighbours> <minUtil> <minSup> <sep>
-            Examples:
-                    >>> python3 SHUFIM.py sampleTDB.txt output.txt sampleN.txt 35 20
+    .. code-block:: console
 
-    **Sample run of importing the code:**
-    -----------------------------------------
+      Format:
+
+      (.venv) $ python3 HUFIM.py <inputFile> <outputFile> <minUtil> <sep>
+
+      Example Usage:
+
+      (.venv) $ python3 HUFIM.py sampleTDB.txt output.txt 35 20
+
+      (.venv) $ python3 HUFIM.py sampleTDB.txt output.txt 35 20
+
+    .. note:: minSup will be considered in percentage of database transactions
+
+
+    **Sample run of importing the code**
+    -----------------------------------------------
     .. code-block:: python
 
-            from PAMI.highUtilityGeoreferencedFrequentPattern.basic import SHUFIM as alg
+            from PAMI.highUtilityFrequentPattern.basic import HUFIM as alg
 
-            obj=alg.SHUFIM("input.txt","Neighbours.txt",35,20)
+            obj=alg.HUFIM("input.txt", 35, 20)
 
-            obj.startMine()
+            obj.mine()
 
-            patterns = obj.getPatterns()
+            Patterns = obj.getPatterns()
 
-            print("Total number of Spatial high utility frequent Patterns:", len(patterns))
+            print("Total number of high utility frequent Patterns:", len(Patterns))
 
             obj.save("output")
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
-
+   
     **Credits:**
-    ---------------------
-
-            The complete program was written by Pradeep Pallikila under the supervision of Professor Rage Uday Kiran.
-
+    --------------------
+            The complete program was written by pradeep pallikila under the supervision of Professor Rage Uday Kiran.
+     
     """
+
+    _highUtilityFrequentItemSets = []
     _candidateCount = 0
     _utilityBinArrayLU = {}
     _utilityBinArraySU = {}
     _oldNamesToNewNames = {}
     _newNamesToOldNames = {}
     _singleItemSetsSupport = {}
     _singleItemSetsUtility = {}
-    _strToint = {}
-    _intTostr = {}
-    _Neighbours = {}
-    _temp = [0] * 5000
+    _strToInt = {}
+    _intToStr = {}
+    _temp = [0]*5000
+    _patternCount = int()
     _maxMemory = 0
     _startTime = float()
     _endTime = float()
-    _minSup = str()
-    _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _nFile = " "
+    _lno = 0
     _sep = "\t"
     _minUtil = 0
+    _minSup = 0
     _memoryUSS = float()
     _memoryRSS = float()
-    
-    def __init__(self, iFile, nFile, minUtil, minSup, sep="\t"):
-        super().__init__(iFile, nFile, minUtil, minSup, sep)
 
-    def _convert(self, value):
-        """
-        To convert the type of user specified minSup value
+    def __init__(self, iFile: str, minUtil: Union[int, float], minSup: Union[int, float], sep: str="\t") -> None:
+        super().__init__(iFile, minUtil, minSup, sep)
 
-        :param value: user specified minSup value
-        :return: converted type
+    def _convert(self, value) -> Union[int, float]:
+        """
+        To convert the given user specified value
+        :param value: user specified value
+        :type value: int or float or str
+        :return: converted value
+        :rtype: int or float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._dataset.getTransactions()) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self._dataset.getTransactions()) * value)
             else:
                 value = int(value)
         return value
 
-    def startMine(self):
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self) -> None:
         """
         High Utility Frequent Pattern mining start here
+        :return: None
         """
         self._startTime = _ab._time.time()
-        self._patternCount = 0
         self._finalPatterns = {}
+        self._dataset = []
         self._dataset = _Dataset(self._iFile, self._sep)
         self._singleItemSetsSupport = _ab._defaultdict(int)
         self._singleItemSetsUtility = _ab._defaultdict(int)
+        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
         self._minUtil = int(self._minUtil)
         self._minSup = self._convert(self._minSup)
-        with open(self._nFile, 'r') as o:
-            lines = o.readlines()
-            for line in lines:
-                line = line.split("\n")[0]
-                line_split = line.split(self._sep)
-                item = self._dataset.strToInt.get(line_split[0])
-                lst = []
-                for i in range(1, len(line_split)):
-                    lst.append(self._dataset.strToInt.get(line_split[i]))
-                self._Neighbours[item] = lst
-        o.close()
-        InitialMemory = _ab._psutil.virtual_memory()[3]
-        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
-        _itemsToKeep = []
+        itemsToKeep = []
         for key in self._utilityBinArrayLU.keys():
             if self._utilityBinArrayLU[key] >= self._minUtil and self._singleItemSetsSupport[key] >= self._minSup:
-                _itemsToKeep.append(key)
-        # sorting items in decreasing order of their utilities
-        _itemsToKeep = sorted(_itemsToKeep, key=lambda x: self._singleItemSetsUtility[x], reverse=True)
-        _currentName = 1
-        for idx, item in enumerate(_itemsToKeep):
-            self._oldNamesToNewNames[item] = _currentName
-            self._newNamesToOldNames[_currentName] = item
-            _itemsToKeep[idx] = _currentName
-            _currentName += 1
+                itemsToKeep.append(key)
+        itemsToKeep = sorted(itemsToKeep, key=lambda x: self._singleItemSetsUtility[x], reverse=True)
+        currentName = 1
+        for idx, item in enumerate(itemsToKeep):
+            self._oldNamesToNewNames[item] = currentName
+            self._newNamesToOldNames[currentName] = item
+            itemsToKeep[idx] = currentName
+            currentName += 1
         for transaction in self._dataset.getTransactions():
             transaction.removeUnpromisingItems(self._oldNamesToNewNames)
         self._sortDatabase(self._dataset.getTransactions())
-        _emptyTransactionCount = 0
+        emptyTransactionCount = 0
         for transaction in self._dataset.getTransactions():
             if len(transaction.getItems()) == 0:
-                _emptyTransactionCount += 1
-        self._dataset.transactions = self._dataset.transactions[_emptyTransactionCount:]
-        # calculating neighborhood suffix utility values
-        _secondary = []
-        for idx, item in enumerate(_itemsToKeep):
-            _cumulativeUtility = self._singleItemSetsUtility[self._newNamesToOldNames[item]]
-            if self._newNamesToOldNames[item] in self._Neighbours:
-                neighbors = [self._oldNamesToNewNames[y] for y in self._Neighbours[self._newNamesToOldNames[item]] if y in self._oldNamesToNewNames]
-                for i in range(idx+1, len(_itemsToKeep)):
-                    _nextItem = _itemsToKeep[i]
-                    if _nextItem in neighbors:
-                        _cumulativeUtility += self._singleItemSetsUtility[self._newNamesToOldNames[_nextItem]]
-            if _cumulativeUtility >= self._minUtil:
-                _secondary.append(item)         
+                emptyTransactionCount += 1
+        self._dataset.transactions = self._dataset.transactions[emptyTransactionCount:]
+        # calculating suffix utility values
+        totalUtility = 0
+        for item in itemsToKeep:
+            totalUtility += self._singleItemSetsUtility[self._newNamesToOldNames[item]]
+        # piItems
+        piItems = []
+        for item in itemsToKeep:
+            if totalUtility >= self._minUtil:
+                piItems.append(item)
+                totalUtility -= self._singleItemSetsUtility[self._newNamesToOldNames[item]]
+            else:
+                break
         self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
-        _itemsToExplore = []
-        for item in _secondary:
+        itemsToExplore = []
+        for item in piItems:
             if self._utilityBinArraySU[item] >= self._minUtil:
-                _itemsToExplore.append(item)
-        _commonitems = []
-        for i in range(self._dataset.maxItem):
-            _commonitems.append(i)
-        self._backtrackingEFIM(self._dataset.getTransactions(), _itemsToKeep, _itemsToExplore, 0)
-        _finalMemory = _ab._psutil.virtual_memory()[3]
-        memory = (_finalMemory - InitialMemory) / 10000
-        if memory > self._maxMemory:
-            self._maxMemory = memory
+                itemsToExplore.append(item)
+        self._backTrackingHUFIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print('Spatial High Utility Frequent Itemsets generated successfully using SHUFIM algorithm')
+        print("High Utility Frequent patterns were generated successfully using HUFIM algorithm")
 
-    def _backtrackingEFIM(self, transactionsOfP, itemsToKeep, itemsToExplore, prefixLength):
+    def mine(self) -> None:
         """
-        A method to mine the SHUFIs Recursively
-
-        :Attributes:
+        High Utility Frequent Pattern mining start here
+        :return: None
+        """
+        self._startTime = _ab._time.time()
+        self._finalPatterns = {}
+        self._dataset = []
+        self._dataset = _Dataset(self._iFile, self._sep)
+        self._singleItemSetsSupport = _ab._defaultdict(int)
+        self._singleItemSetsUtility = _ab._defaultdict(int)
+        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
+        self._minUtil = int(self._minUtil)
+        self._minSup = self._convert(self._minSup)
+        itemsToKeep = []
+        for key in self._utilityBinArrayLU.keys():
+            if self._utilityBinArrayLU[key] >= self._minUtil and self._singleItemSetsSupport[key] >= self._minSup:
+                itemsToKeep.append(key)
+        itemsToKeep = sorted(itemsToKeep, key=lambda x: self._singleItemSetsUtility[x], reverse=True)
+        currentName = 1
+        for idx, item in enumerate(itemsToKeep):
+            self._oldNamesToNewNames[item] = currentName
+            self._newNamesToOldNames[currentName] = item
+            itemsToKeep[idx] = currentName
+            currentName += 1
+        for transaction in self._dataset.getTransactions():
+            transaction.removeUnpromisingItems(self._oldNamesToNewNames)
+        self._sortDatabase(self._dataset.getTransactions())
+        emptyTransactionCount = 0
+        for transaction in self._dataset.getTransactions():
+            if len(transaction.getItems()) == 0:
+                emptyTransactionCount += 1
+        self._dataset.transactions = self._dataset.transactions[emptyTransactionCount:]
+        # calculating suffix utility values
+        totalUtility = 0
+        for item in itemsToKeep:
+            totalUtility += self._singleItemSetsUtility[self._newNamesToOldNames[item]]
+        # piItems
+        piItems = []
+        for item in itemsToKeep:
+            if totalUtility >= self._minUtil:
+                piItems.append(item)
+                totalUtility -= self._singleItemSetsUtility[self._newNamesToOldNames[item]]
+            else:
+                break
+        self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
+        itemsToExplore = []
+        for item in piItems:
+            if self._utilityBinArraySU[item] >= self._minUtil:
+                itemsToExplore.append(item)
+        self._backTrackingHUFIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("High Utility Frequent patterns were generated successfully using HUFIM algorithm")
 
+    def _backTrackingHUFIM(self, transactionsOfP: List[_Transaction], itemsToKeep: List[int], itemsToExplore: List[int], prefixLength: int) -> None:
+        """
+        A method to mine the HUFIs Recursively
         :param transactionsOfP: the list of transactions containing the current prefix P
         :type transactionsOfP: list
         :param itemsToKeep: the list of secondary items in the p-projected database
         :type itemsToKeep: list
         :param itemsToExplore: the list of primary items in the p-projected database
         :type itemsToExplore: list
         :param prefixLength: current prefixLength
         :type prefixLength: int
+        :return: None
         """
+        # print("###############")
+        # print("P is", [self.dataset.intToStr.get(x) for x in self.temp[:prefixLength]])
+        # print("items to explore", [self.dataset.intToStr.get(x) for x in [self.newNamesToOldNames[y] for y  in itemsToExplore]])
+        # print("items to keep", [self.dataset.intToStr.get(x) for x in [self.newNamesToOldNames[y] for y in itemsToKeep]])
+        # print("--------------")
         self._candidateCount += len(itemsToExplore)
         for idx, e in enumerate(itemsToExplore):
-            initialMemory = _ab._psutil.virtual_memory()[3]
+            # print("exploring item", self.dataset.intToStr.get(self.newNamesToOldNames[e]))
             transactionsPe = []
             utilityPe = 0
             supportPe = 0
             previousTransaction = []
             consecutiveMergeCount = 0
             for transaction in transactionsOfP:
                 items = transaction.getItems()
@@ -647,363 +745,269 @@
                             supportPe += previousTransaction.getSupport()
                             previousTransaction = projectedTransaction
                             consecutiveMergeCount = 0
                     transaction.offset = positionE
             if previousTransaction != []:
                 transactionsPe.append(previousTransaction)
                 supportPe += previousTransaction.getSupport()
+            # print("support is", supportPe)
             self._temp[prefixLength] = self._newNamesToOldNames[e]
-            if utilityPe >= self._minUtil and supportPe >= self._minSup:
+            if (utilityPe >= self._minUtil) and (supportPe >= self._minSup):
                 self._output(prefixLength, utilityPe, supportPe)
             if supportPe >= self._minSup:
-                neighbourhoodList = self._calculateNeighbourIntersection(prefixLength)
-                #print(neighbourhoodList)
-                self._useUtilityBinArraysToCalculateUpperBounds(transactionsPe, idx, itemsToKeep, neighbourhoodList)
+                self._useUtilityBinArraysToCalculateUpperBounds(transactionsPe, idx, itemsToKeep)
                 newItemsToKeep = []
                 newItemsToExplore = []
                 for l in range(idx + 1, len(itemsToKeep)):
                     itemK = itemsToKeep[l]
                     if self._utilityBinArraySU[itemK] >= self._minUtil:
-                        if itemK in neighbourhoodList:
-                            newItemsToExplore.append(itemK)
-                            newItemsToKeep.append(itemK)
+                        newItemsToExplore.append(itemK)
+                        newItemsToKeep.append(itemK)
                     elif self._utilityBinArrayLU[itemK] >= self._minUtil:
-                        if itemK in neighbourhoodList:
-                            newItemsToKeep.append(itemK)
-                self._backtrackingEFIM(transactionsPe, newItemsToKeep, newItemsToExplore, prefixLength + 1)
-            finalMemory = _ab._psutil.virtual_memory()[3]
-            memory = (finalMemory - initialMemory) / 10000
-            if self._maxMemory < memory:
-                self._maxMemory = memory
+                        newItemsToKeep.append(itemK)
+                if len(transactionsPe) != 0:
+                    self._backTrackingHUFIM(transactionsPe, newItemsToKeep, newItemsToExplore, prefixLength + 1)
 
-    def _useUtilityBinArraysToCalculateUpperBounds(self, transactionsPe, j, itemsToKeep, neighbourhoodList):
+    def _useUtilityBinArraysToCalculateUpperBounds(self, transactionsPe: List[_Transaction], j: int, itemsToKeep: List[int]) -> None:
         """
         A method to  calculate the subtree utility and local utility of all items that can extend itemSet P U {e}
 
         :Attributes:
 
         :param transactionsPe: transactions the projected database for P U {e}
-        :type transactionsPe: list
+        :type transactionsPe: list or Dataset
         :param j:the position of j in the list of promising items
         :type j:int
         :param itemsToKeep :the list of promising items
-        :type itemsToKeep: list
-
+        :type itemsToKeep: list or Dataset
+        :return: None
         """
         for i in range(j + 1, len(itemsToKeep)):
             item = itemsToKeep[i]
             self._utilityBinArrayLU[item] = 0
             self._utilityBinArraySU[item] = 0
         for transaction in transactionsPe:
-            length = len(transaction.getItems())
-            i = length - 1
+            sumRemainingUtility = 0
+            i = len(transaction.getItems()) - 1
             while i >= transaction.offset:
                 item = transaction.getItems()[i]
                 if item in itemsToKeep:
-                    remainingUtility = 0
-                    if self._newNamesToOldNames[item] in self._Neighbours:
-                        itemNeighbours = self._Neighbours[self._newNamesToOldNames[item]]
-                        for k in range(i, length):
-                            transaction_item = transaction.getItems()[k]
-                            if self._newNamesToOldNames[transaction_item] in itemNeighbours and transaction_item in neighbourhoodList:
-                                remainingUtility += transaction.getUtilities()[k]
-
-                    remainingUtility += transaction.getUtilities()[i]
-                    self._utilityBinArraySU[item] += remainingUtility + transaction.prefixUtility
+                    sumRemainingUtility += transaction.getUtilities()[i]
+                    self._utilityBinArraySU[item] += sumRemainingUtility + transaction.prefixUtility
                     self._utilityBinArrayLU[item] += transaction.transactionUtility + transaction.prefixUtility
                 i -= 1
 
-    def _calculateNeighbourIntersection(self, prefixLength):
+    def _output(self, tempPosition: int, utility: int, support: int):
         """
-        A method to find common Neighbours
+        Method to print itemSets
 
         :Attributes:
 
-        :param prefixLength: the prefix itemSet
-        :type prefixLength:int
-
-        """
-        intersectionList = self._Neighbours.get(self._temp[0])
-        for i in range(1, prefixLength+1):
-            intersectionList = self._intersection(self._Neighbours[self._temp[i]], intersectionList)
-        finalIntersectionList = []
-        if intersectionList is None:
-            return finalIntersectionList
-        for item in intersectionList:
-            if item in self._oldNamesToNewNames:
-                finalIntersectionList.append(self._oldNamesToNewNames[item])
-        return finalIntersectionList
-    
-    def _output(self, tempPosition, utility, support):
+        :param tempPosition: position of last item
+        :type tempPosition : int
+        :param utility: total utility of itemSet
+        :type utility: int
+        :param support: support of an itemSet
+        :type support: int
         """
-         A method save all high-utility itemSet to file or memory depending on what the user chose
-
-         :Attributes:
-
-         :param tempPosition: position of last item
-         :type tempPosition : int 
-         :param utility: total utility of itemSet
-         :type utility: int
-         :param support: support of an itemSet
-         :type support: int
-         """
         self._patternCount += 1
         s1 = str()
         for i in range(0, tempPosition+1):
             s1 += self._dataset.intToStr.get((self._temp[i]))
             if i != tempPosition:
                 s1 += "\t"
         self._finalPatterns[s1] = [utility, support]
 
-    def _isEqual(self, transaction1, transaction2):
+    def _isEqual(self, transaction1: _Transaction, transaction2: _Transaction) -> bool:
         """
-         A method to Check if two transaction are identical
-
-         :Attributes:
-
-         :param  transaction1: the first transaction
-         :type  transaction1: Trans
-         :param  transaction2:   the second transaction
-         :type  transaction2: Trans
-         :return : whether both are identical or not
-         :rtype: bool
+        A method to Check if two transaction are identical
+        :param  transaction1: the first transaction
+        :type  transaction1: Trans
+        :param  transaction2:    the second transaction
+        :type  transaction2: Trans
+        :return : whether both are identical or not
+        :rtype: bool
         """
-
         length1 = len(transaction1.items) - transaction1.offset
         length2 = len(transaction2.items) - transaction2.offset
         if length1 != length2:
             return False
         position1 = transaction1.offset
         position2 = transaction2.offset
         while position1 < len(transaction1.items):
             if transaction1.items[position1] != transaction2.items[position2]:
                 return False
             position1 += 1
             position2 += 1
         return True
-    
-    def _intersection(self, lst1, lst2):
-        """
-        A method that return the intersection of 2 list
-
-        :param  lst1: items neighbour to item1
-        :type lst1: list
-        :param lst2: items neighbour to item2
-        :type lst2: list
-        :return :intersection of two lists
-        :rtype : list
-        """
-        temp = set(lst2)
-        lst3 = [value for value in lst1 if value in temp]
-        return lst3
 
-    def _useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self, dataset):
+    def _useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self, dataset: _Dataset) -> None:
         """
         Scan the initial database to calculate the subtree utility of each item using a utility-bin array
-
-        :Attributes:
-
         :param dataset: the transaction database
         :type dataset: Dataset
+        :return : None
         """
         for transaction in dataset.getTransactions():
-            items = transaction.getItems()
-            utilities = transaction.getUtilities()
-            for idx, item in enumerate(items):
-                if item not in self._utilityBinArraySU:
-                    self._utilityBinArraySU[item] = 0
-                if self._newNamesToOldNames[item] not in self._Neighbours:
-                    self._utilityBinArraySU[item] += utilities[idx]
-                    continue
-                i = idx + 1
-                sumSu = utilities[idx]
-                while i < len(items):
-                    if self._newNamesToOldNames[items[i]] in self._Neighbours[self._newNamesToOldNames[item]]:
-                        sumSu += utilities[i]
-                    i += 1
-                self._utilityBinArraySU[item] += sumSu
+            sumSU = 0
+            i = len(transaction.getItems()) - 1
+            while i >= 0:
+                item = transaction.getItems()[i]
+                currentUtility = transaction.getUtilities()[i]
+                sumSU += currentUtility
+                if item in self._utilityBinArraySU.keys():
+                    self._utilityBinArraySU[item] += sumSU
+                else:
+                    self._utilityBinArraySU[item] = sumSU
+                i -= 1
 
-    def _sortDatabase(self, transactions):
+    def _sortDatabase(self, transactions: List[_Transaction]) -> None:
         """
-        A Method to sort transaction in the order of PMU
-
-        :Attributes:
-
-        :param transactions: transaction of items
-        :type transactions: Transaction
-        :return: sorted transaction
-        :rtype: Trans
+        A Method to sort transaction
+        :param transactions: transactions of items
+        :type transactions: list
+        :return: None
         """
-        cmp_items = _comToKey(self._sortTransaction)
-        transactions.sort(key=cmp_items)
+        compareItems = _ab._functools.cmp_to_key(self._sortTransaction)
+        transactions.sort(key=compareItems)
 
-    def _sortTransaction(self, trans1, trans2):
+    def _sortTransaction(self, trans1: _Transaction, trans2: _Transaction) -> int:
         """
-        A Method to sort transaction in the order of PMU
-
-        :Attributes:
-
+        A Method to sort transaction
         :param trans1: the first transaction
         :type trans1: Trans
         :param trans2:the second transaction
         :type trans2: Trans
         :return: sorted transaction
-        :rtype: Trans
+        :rtype: int
         """
-        trans1_items = trans1.getItems()
-        trans2_items = trans2.getItems()
-        pos1 = len(trans1_items) - 1
-        pos2 = len(trans2_items) - 1
-        if len(trans1_items) < len(trans2_items):
+        transItemsX = trans1.getItems()
+        transItemsY = trans2.getItems()
+        pos1 = len(transItemsX) - 1
+        pos2 = len(transItemsY) - 1
+        if len(transItemsX) < len(transItemsY):
             while pos1 >= 0:
-                sub = trans2_items[pos2] - trans1_items[pos1]
+                sub = transItemsY[pos2] - transItemsX[pos1]
                 if sub != 0:
                     return sub
                 pos1 -= 1
                 pos2 -= 1
             return -1
-        elif len(trans1_items) > len(trans2_items):
+        elif len(transItemsX) > len(transItemsY):
             while pos2 >= 0:
-                sub = trans2_items[pos2] - trans1_items[pos1]
+                sub = transItemsY[pos2] - transItemsX[pos1]
                 if sub != 0:
                     return sub
                 pos1 -= 1
                 pos2 -= 1
             return 1
         else:
             while pos2 >= 0:
-                sub = trans2_items[pos2] - trans1_items[pos1]
+                sub = transItemsY[pos2] - transItemsX[pos1]
                 if sub != 0:
                     return sub
                 pos1 -= 1
                 pos2 -= 1
             return 0
 
-    def _useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset):
+    def _useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset: _Dataset) -> None:
         """
-        A method to scan the database using utility bin array to calculate the pmus
-
-        :Attributes:
-
+        A method to calculate local utility of single itemSets
         :param dataset: the transaction database
-        :type dataset: dataset
+        :type dataset: databases
+        :return: None
         """
         for transaction in dataset.getTransactions():
             for idx, item in enumerate(transaction.getItems()):
                 self._singleItemSetsSupport[item] += 1
                 self._singleItemSetsUtility[item] += transaction.getUtilities()[idx]
-                pmu = transaction.getUtilities()[idx]
-                if item in self._Neighbours:
-                    neighbors = self._Neighbours[item]
-                    for idx, item in enumerate(transaction.getItems()):
-                        if item in neighbors:
-                            pmu += transaction.getUtilities()[idx]
                 if item in self._utilityBinArrayLU:
-                    # self._utilityBinArrayLU[item] += transaction.getPmus()[idx]
-                    self._utilityBinArrayLU[item] += pmu
+                    self._utilityBinArrayLU[item] += transaction.transactionUtility
                 else:
-                    # self._utilityBinArrayLU[item] = transaction.getPmus()[idx]
-                    self._utilityBinArrayLU[item] = pmu
+                    self._utilityBinArrayLU[item] = transaction.transactionUtility
 
-    def getPatternsAsDataFrame(self):
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final patterns in a dataframe
-
         :return: returning patterns in a dataframe
         :rtype: pd.DataFrame
         """
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b[0], b[1]])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Utility', 'Support'])
 
         return dataFrame
     
-    def getPatterns(self):
+    def getPatterns(self) -> Dict[str, List[Union[int, float]]]:
         """
         Function to send the set of patterns after completion of the mining process
-
         :return: returning patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
-        Complete set of patterns will be loaded in to an output file
-
+        Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
-        self.oFile = outFile
-        writer = open(self.oFile, 'w+')
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % patternsAndSupport)
 
-    def getMemoryUSS(self):
+    def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
         return self._memoryRSS
 
-    def getRuntime(self):
+    def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime-self._startTime
     
-    def printResults(self):
+    def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of Spatial High Utility Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of High Utility Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
 
-def main():
-    inputFile = '/home/nakamura/workspace/labwork/PAMI/PAMI/highUtilityGeoreferencedFrequentPattern/basic/mushroom_utility_spmf.txt'
-    neighborFile = '/home/nakamura/workspace/labwork/PAMI/PAMI/highUtilityGeoreferencedFrequentPattern/basic/mushroom_utility_spmf.txt'
-
-    minUtilCount = 10000
-    minSup = 100
-    seperator = ' '  
-    obj = SHUFIM(iFile=inputFile, nFile=neighborFile, minUtil=minUtilCount, minSup=minSup, sep=seperator)    #initialize
-    obj.startMine()   
-    obj.printResults()
-    print(obj.getPatterns())
-
-
 if __name__ == '__main__':
-    main()
-    # _ap = str()
-    # if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
-    #     if len(_ab._sys.argv) == 7:
-    #         _ap = SHUFIM(_ab._sys.argv[1], _ab._sys.argv[3], int(_ab._sys.argv[4]), _ab._sys.argv[5], _ab._sys.argv[6])
-    #     if len(_ab._sys.argv) == 6:
-    #         _ap = SHUFIM(_ab._sys.argv[1], _ab._sys.argv[3], int(_ab._sys.argv[4]), _ab._sys.argv[5])
-    #     _ap.startMine()
-    #     print("Total number of Spatial High Utility Frequent Patterns:", len(_ap.getPatterns()))
-    #     _ap.save(_ab._sys.argv[2])
-    #     print("Total Memory in USS:", _ap.getMemoryUSS())
-    #     print("Total Memory in RSS", _ap.getMemoryRSS())
-    #     print("Total ExecutionTime in seconds:", _ap.getRuntime())
-    # else:
-    #     print("Error! The number of input parameters do not match the total number of parameters provided")
+    _ap = str()
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:    #includes separator
+            _ap = HUFIM(_ab._sys.argv[1], int(_ab._sys.argv[3]), float(_ab._sys.argv[4]), _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:    #takes "\t" as a separator
+            _ap = HUFIM(_ab._sys.argv[1], int(_ab._sys.argv[3]), float(_ab._sys.argv[4]))
+        _ap.startMine()
+        _ap.mine()
+        print("Total number of High Utility Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in seconds:", _ap.getRuntime())
+    else:
+        print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

### Comparing `pami-2024.3.9.2/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/highUtilityPattern/basic/EFIM.py` & `pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/RHUIM.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,36 +1,39 @@
-# EFIM is one of the fastest algorithm to mine High Utility ItemSets from transactional databases.
-#
+# RHUIM algorithm helps us to mine Relative High Utility itemSets from transactional databases.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#             from PAMI.highUtilityPattern.basic import EFIM as alg
 #
-#             obj=alg.EFIM("input.txt",35)
+#             from PAMI.relativeHighUtilityPattern.basic import RHUIM as alg
+#
+#             obj = alg.RHUIM("input.txt", 35, 20)
 #
 #             obj.startMine()
 #
-#             Patterns = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
-#             print("Total number of high utility Patterns:", len(Patterns))
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#             obj.save("output")
+#             obj.savePatterns(oFile)
+#
+#             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
 
 
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
@@ -46,34 +49,36 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from PAMI.highUtilityPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
+import pandas as pd
+from deprecated import deprecated
+from PAMI.relativeHighUtilityPattern.basic import abstract as _ab
 
 
 class _Transaction:
     """
-        A class to store Transaction of a database
+    A class to store Transaction of a database
 
     :Attributes:
 
         items: list
             A list of items in transaction 
         utilities: list
             A list of utilities of items in transaction
         transactionUtility: int
             represent total sum of all utilities in the database
         prefixUtility:
             prefix Utility values of item
         offset:
-            an offset pointer, used by projected transactions
+            an offset pointer, used by projected transaction
+
     :Methods:
 
         projectedTransaction(offsetE):
             A method to create new Transaction from existing starting from offsetE until the end
         getItems():
             return items in transaction
         getUtilities():
@@ -92,14 +97,15 @@
         self.items = items
         self.utilities = utilities
         self.transactionUtility = transactionUtility
 
     def projectTransaction(self, offsetE: int) -> '_Transaction':
         """
         A method to create new Transaction from existing transaction starting from offsetE until the end
+
         :param offsetE: an offset over the original transaction for projecting the transaction
         :type offsetE: int
         """
         new_transaction = _Transaction(self.items, self.utilities, self.transactionUtility)
         utilityE = self.utilities[offsetE]
         new_transaction.prefixUtility = self.prefixUtility + utilityE
         new_transaction.transactionUtility = self.transactionUtility - utilityE
@@ -107,35 +113,40 @@
             new_transaction.transactionUtility -= self.utilities[i]
         new_transaction.offset = offsetE + 1
         return new_transaction
 
     def getItems(self) -> list:
         """
         A method to return items in transaction
+        :return: list
         """
         return self.items
 
     def getUtilities(self) -> list:
         """
         A method to return utilities in transaction
+        :return: list
         """
         return self.utilities
 
     def getLastPosition(self) -> int:
         """
         A method to return last position in a transaction
+        :return: int
         """
 
         return len(self.items) - 1
 
     def removeUnpromisingItems(self, oldNamesToNewNames: dict) -> None:
         """
         A method to remove items which are not present in the map passed to the function
+
         :param oldNamesToNewNames: A map represent old names to new names
         :type oldNamesToNewNames: map
+        :return: None
         """
         tempItems = []
         tempUtilities = []
         for idx, item in enumerate(self.items):
             if item in oldNamesToNewNames:
                 tempItems.append(oldNamesToNewNames[item])
                 tempUtilities.append(self.utilities[idx])
@@ -144,14 +155,15 @@
         self.items = tempItems
         self.utilities = tempUtilities
         self.insertionSort()
 
     def insertionSort(self) -> None:
         """
         A method to sort items in order
+        :return: None
         """
         for i in range(1, len(self.items)):
             key = self.items[i]
             utilityJ = self.utilities[i]
             j = i - 1
             while j >= 0 and key < self.items[j]:
                 self.items[j + 1] = self.items[j]
@@ -161,63 +173,60 @@
             self.utilities[j + 1] = utilityJ
         
 
 class _Dataset:
     """
     A class represent the list of transactions in this dataset
 
-    :Attributes:
+   :Attributes:
 
-        transactions :
+        transactions:
             the list of transactions in this dataset
         maxItem:
             the largest item name
         
-    :methods:
+   :methods:
 
         createTransaction(line):
             Create a transaction object from a line from the input file
         getMaxItem():
             return Maximum Item
         getTransactions():
             return transactions in database
 
     """
     transactions = []
     maxItem = 0
-    
-    def __init__(self,datasetPath: Union[str, _ab._pd.DataFrame], sep: str) -> None:
+    def __init__(self, datasetPath: str, sep: str) -> None:
         self.strToInt = {}
         self.intToStr = {}
-        self.transactions = []
-        self.maxItem = 0
         self.cnt = 1
         self.sep = sep
-        self.createItemsets(datasetPath)
+        self.createItemSets(datasetPath)
 
-    def createItemsets(self, datasetPath: Union[str, _ab._pd.DataFrame]) -> None:
+    def createItemSets(self, datasetPath: str) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
-        : param datasetPath: It represents the peth for the dataset
-        : type datasetPath: str
-        : return: None
+        :return: None
         """
-        self.Database = []
+        self.transactions = []
+        itemsets, utilities, utilityValues = [], [], []
         if isinstance(datasetPath, _ab._pd.DataFrame):
-            utilities, data, transactionUtility = [], [], []
+            utilities, data, utilityValues = [], [], []
             if datasetPath.empty:
                 print("its empty..")
             i = datasetPath.columns.values.tolist()
             if 'Transactions' in i:
-                data = datasetPath['Transactions'].tolist()
+                itemsets = datasetPath['Transactions'].tolist()
             if 'Utilities' in i:
-                utilities = datasetPath['Utilities'].tolist()
+                utilities = datasetPath['Patterns'].tolist()
             if 'UtilitySum' in i:
-                transactionUtility = datasetPath['UtilitySum'].tolist()
-            self.transactions.append(self.createTransaction(data, utilities, transactionUtility))
+                utilityValues = datasetPath['utilitySum'].tolist()
+            for k in range(len(itemsets)):
+                self.transactions.append(self.createTransaction(itemsets[k], utilities[k], utilityValues[k]))
         if isinstance(datasetPath, str):
             if _ab._validators.url(datasetPath):
                 data = _ab._urlopen(datasetPath)
                 for line in data:
                     line = line.decode("utf-8")
                     trans_list = line.strip().split(':')
                     transactionUtility = int(trans_list[1])
@@ -232,38 +241,37 @@
                         for line in f:
                             trans_list = line.strip().split(':')
                             transactionUtility = int(trans_list[1])
                             itemsString = trans_list[0].strip().split(self.sep)
                             itemsString = [x for x in itemsString if x]
                             utilityString = trans_list[2].strip().split(self.sep)
                             utilityString = [x for x in utilityString if x]
-                            self.transactions.append(
-                                self.createTransaction(itemsString, utilityString, transactionUtility))
-
+                            self.transactions.append(self.createTransaction(itemsString, utilityString, transactionUtility))
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def createTransaction(self, itemsString: list, utilityString: list, transactionUtility: int) -> '_Transaction':
+    def createTransaction(self, itemSet: list, utilities: list, utilitySum: int) -> _Transaction:
         """
         A method to create Transaction from dataset given
             
         :Attributes:
 
-        :param line: represent a single line of database
-        :type line: string
-        :return : Transaction
-        :rtype: Trans
-        """
-        '''trans_list = line.strip().split(':')
-        transactionUtility = int(trans_list[1])
-        itemsString = trans_list[0].strip().split(self.sep)
-        itemsString = [x for x in itemsString if x]
-        utilityString = trans_list[2].strip().split(self.sep)
-        utilityString = [x for x in utilityString if x]'''
+        :param itemSet: represent a transactions itemset in database
+        :type itemSet: list
+        :param utilities: utility values of respective transaction itemSets
+        :type utilities: list
+        :param utilitySum: represent the sum of utility Sum
+        :type utilitySum: int
+        :return : Transaction.
+        :rtype: Transaction
+        """
+        transactionUtility = utilitySum
+        itemsString = itemSet
+        utilityString = utilities
         items = []
         utilities = []
         for idx, item in enumerate(itemsString):
             if self.strToInt.get(item) is None:
                 self.strToInt[item] = self.cnt
                 self.intToStr[self.cnt] = item
                 self.cnt += 1
@@ -273,67 +281,85 @@
             items.append(item_int)
             utilities.append(int(utilityString[idx]))
         return _Transaction(items, utilities, transactionUtility)
 
     def getMaxItem(self) -> int:
         """
         A method to return name of the largest item
+        :return; int
         """
         return self.maxItem
 
     def getTransactions(self) -> list:
         """
         A method to return transactions from database
+        :return: list
         """
         return self.transactions
 
 
-class EFIM(_ab._utilityPatterns):
+class RHUIM(_ab._utilityPatterns):
     """
-    :Description:   EFIM is one of the fastest algorithm to mine High Utility ItemSets from transactional databases.
-    
-    :Reference:      Zida, S., Fournier-Viger, P., Lin, J.CW. et al. EFIM: a fast and memory efficient algorithm for
-                    high-utility itemset mining. Knowl Inf Syst 51, 595–625 (2017). https://doi.org/10.1007/s10115-016-0986-0
+    :Description:   RHUIM algorithm helps us to mine Relative High Utility itemSets from transactional databases.
     
+    :Reference:   R. U. Kiran, P. Pallikila, J. M. Luna, P. Fournier-Viger, M. Toyoda and P. K. Reddy,
+                 "Discovering Relative High Utility Itemsets in Very Large Transactional Databases Using Null-Invariant Measure,"
+                  2021 IEEE International Conference on Big Data (Big Data), Orlando, FL, USA, 2021, pp. 252-262,
+                  doi: 10.1109/BigData52589.2021.9672064.
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Relative High Utility patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Relative High Utility patterns
+    :param  minSup: float or int or str :
+                    minSup measure constraints the minimum number of transactions in a database where a pattern must appear
+                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  minUtil: int :
+                   The minimum utility threshold.
+
     :Attributes:
 
         iFile : file
-            Name of the input file to mine complete set of high utility patterns
+            Name of the input file to mine complete set of patterns
         oFile : file
-            Name of the output file to store complete set of high utility patterns
+            Name of the output file to store complete set of patterns
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime:float
+        startTime : float
             To record the start time of the mining process
-        endTime:float
+        endTime : float
             To record the completion time of the mining process
         minUtil : int
             The user given minUtil value
-        highUtilityitemSets: map
-            set of high utility itemSets
-        candidateCount: int
+        minUR : float
+            The user given minUR value
+        relativeHighUtilityItemSets : map
+            set of relative high utility itemSets
+        candidateCount : int
              Number of candidates 
-        utilityBinArrayLU: list
+        utilityBinArrayLU : list
              A map to hold the local utility values of the items in database
-        utilityBinArraySU: list
+        utilityBinArraySU : list
             A map to hold the subtree utility values of the items is database
-        oldNamesToNewNames: list
+        oldNamesToNewNames : list
             A map which contains old names, new names of items as key value pairs
-        newNamesToOldNames: list
+        newNamesToOldNames : list
             A map which contains new names, old names of items as key value pairs
-        maxMemory: float
+        maxMemory : float
             Maximum memory used by this program for running
-        patternCount: int
-            Number of HUI's
-        itemsToKeep: list
-            keep only the promising items ie items having local utility values greater than or equal to minUtil
-        itemsToExplore: list
-            list of items that have subtreeUtility value greater than or equal to minUtil
+        patternCount : int
+            Number of RHUI's
+        itemsToKeep : list
+            keep only the promising items i.e items that can extend other items to form RHUIs
+        itemsToExplore : list
+            list of items that needs to be explored
 
-    :Methods :
+    :Methods:
 
         startMine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         save(oFile)
                 Complete set of patterns will be loaded in to a output file
@@ -341,181 +367,182 @@
                 Complete set of patterns will be loaded in to a dataframe
         getMemoryUSS()
                 Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
                 Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
                Total amount of runtime taken by the mining process will be retrieved from this function
-        backTrackingEFIM(transactionsOfP, itemsToKeep, itemsToExplore, prefixLength)
-               A method to mine the HUIs Recursively
+        backTrackingRHUIM(transactionsOfP, itemsToKeep, itemsToExplore, prefixLength)
+               A method to mine the RHUIs Recursively
         useUtilityBinArraysToCalculateUpperBounds(transactionsPe, j, itemsToKeep)
                A method to calculate the sub-tree utility and local utility of all items that can extend itemSet P and e
         output(tempPosition, utility)
-               A method to output a high-utility itemSet to file or memory depending on what the user chose
+               A method to output a relative-high-utility itemSet to file or memory depending on what the user chose
         is_equal(transaction1, transaction2)
                A method to Check if two transaction are identical
         useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(dataset)
               A method to calculate the sub tree utility values for single items
         sortDatabase(self, transactions)
               A Method to sort transaction
         sort_transaction(self, trans1, trans2)
               A Method to sort transaction
         useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset)
-             A method to calculate local utility values for single itemsets
+             A method to calculate local utility values for single itemSets
+
+    **Methods to execute code on terminal**
+    -------------------------------------------
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 RHUIM.py <inputFile> <outputFile> <minUtil> <sep>
+
+      Example usage:
 
-    **Executing the code on terminal:**
-    ------------------------------------------
-        Format:
+      (.venv) $ python3 RHUIM.py sampleTDB.txt output.txt 35 20
 
-                  >>> python3 EFIM.py <inputFile> <outputFile> <minUtil> <sep>
-        Examples:
 
-                  >>> python3 EFIM sampleTDB.txt output.txt 35
-                  >>> python3 EFIM sampleTDB.txt output.txt 35
+              .. note:: minSup will be considered in times of minSup and count of database transactions
 
-    Sample run of importing the code:
-    -------------------------------------
+
+
+    **Importing this algorithm into a python program**
+    -----------------------------------------------------
     .. code-block:: python
-        
-            from PAMI.highUtilityPattern.basic import EFIM as alg
 
-            obj=alg.EFIM("input.txt",35)
+            from PAMI.relativeHighUtilityPattern.basic import RHUIM as alg
+
+            obj=alg.RHUIM("input.txt", 35, 20)
 
             obj.startMine()
 
-            Patterns = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-            print("Total number of high utility Patterns:", len(Patterns))
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            obj.save("output")
+            obj.savePatterns(oFile)
 
-            memUSS = obj.getMemoryUSS()
+            Df = obj.getPatternsAsDataFrame()
+
+            memUSS = obj.getmemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
-   
+
     **Credits:**
-    -------------------
-        The complete program was written by pradeep pallikila under the supervision of Professor Rage Uday Kiran.
-     
+    -----------------
+             The complete program was written by  Pradeep Pallikila  under the supervision of Professor Rage Uday Kiran.
+
     """
 
-    _highUtilityitemSets = []
+    _relativeHighUtilityItemSets = []
     _candidateCount = 0
     _utilityBinArrayLU = {}
     _utilityBinArraySU = {}
     _oldNamesToNewNames = {}
     _newNamesToOldNames = {}
+    _singleItemSetsUtilities = {}
     _strToInt = {}
     _intToStr = {}
-    _Neighbours = {}
-    _temp = [0] * 5000
+    _temp = [0]*5000
     _patternCount = int()
     _maxMemory = 0
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
     _iFile = " "
+    _oFile = " "
     _nFile = " "
     _lno = 0
     _sep = "\t"
     _minUtil = 0
+    _minUR = 0
     _memoryUSS = float()
     _memoryRSS = float()
-    _startTime = _ab._time.time()
 
-    def __init__(self, iFile, minUtil, sep="\t") -> None:
-        super().__init__(iFile, minUtil, sep)
-        self._sep = sep
-        self._highUtilityitemSets = []
-        self._candidateCount = 0
-        self._utilityBinArrayLU = {}
-        self._utilityBinArraySU = {}
-        self._oldNamesToNewNames = {}
-        self._newNamesToOldNames = {}
-        self._strToInt = {}
-        self._intToStr = {}
-        self._Neighbours = {}
-        self._temp = [0] * 5000
-        self._patternCount = 0
-        self._maxMemory = 0
-        self._endTime = float()
-        self._finalPatterns = {}
-        self._lno = 0
-        self._memoryUSS = float()
-        self._memoryRSS = float()
+    def __init__(self, iFile: str, minUtil: int, minUR: float, sep: str="\t") -> None:
+        super().__init__(iFile, minUtil, minUR, sep)
 
     def startMine(self) -> None:
         """
-        Start the EFIM algorithm.
+        Mining process will start from this function
         :return: None
         """
         self._startTime = _ab._time.time()
         self._dataset = _Dataset(self._iFile, self._sep)
+        self._finalPatterns = {}
         self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
-        self._minUtil = int(self._minUtil)
+        _minUtil = int(self._minUtil)
+        _minUR = float(self._minUR)
+        # print(minUR)
+        self._singleItemSetsUtilities = _ab._defaultdict(int)
         itemsToKeep = []
         for key in self._utilityBinArrayLU.keys():
-            if self._utilityBinArrayLU[key] >= self._minUtil:
+            if self._utilityBinArrayLU[key] >= _minUtil:
                 itemsToKeep.append(key)
         itemsToKeep = sorted(itemsToKeep, key=lambda x: self._utilityBinArrayLU[x])
         currentName = 1
         for idx, item in enumerate(itemsToKeep):
             self._oldNamesToNewNames[item] = currentName
             self._newNamesToOldNames[currentName] = item
             itemsToKeep[idx] = currentName
             currentName += 1
         for transaction in self._dataset.getTransactions():
             transaction.removeUnpromisingItems(self._oldNamesToNewNames)
-        self._sortDatabase(self._dataset.getTransactions())
+        self.sortDatabase(self._dataset.getTransactions())
         emptyTransactionCount = 0
         for transaction in self._dataset.getTransactions():
             if len(transaction.getItems()) == 0:
                 emptyTransactionCount += 1
         self._dataset.transactions = self._dataset.transactions[emptyTransactionCount:]
         self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
         itemsToExplore = []
         for item in itemsToKeep:
-            if self._utilityBinArraySU[item] >= self._minUtil:
+            if self._utilityBinArraySU[item] >= _minUtil:
                 itemsToExplore.append(item)
-        self._backTrackingEFIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
+        utilitySum = 0
+        self._backTrackingRHUIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0, utilitySum)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("High Utility patterns were generated successfully using EFIM algorithm")
+        print("Relative High Utility patterns were generated successfully using RHUIM algorithm")
 
-    def _backTrackingEFIM(self, transactionsOfP: list, itemsToKeep: list, itemsToExplore: list, prefixLength: int) -> None:
+    def _backTrackingRHUIM(self, transactionsOfP: list, itemsToKeep: list, itemsToExplore: list, prefixLength: int, utilitySumP: int) -> None:
         """
-        A method to mine the HUIs Recursively
+        A method to mine the RHUIs Recursively
 
         :Attributes:
 
         :param transactionsOfP: the list of transactions containing the current prefix P
         :type transactionsOfP: list
         :param itemsToKeep: the list of secondary items in the p-projected database
         :type itemsToKeep: list
         :param itemsToExplore: the list of primary items in the p-projected database
         :type itemsToExplore: list
         :param prefixLength: current prefixLength
         :type prefixLength: int
+        :param utilitySumP: a variable to hold sum of utilities of all items in P
+        :type utilitySumP int
+        :return: None
         """
         self._candidateCount += len(itemsToExplore)
         for idx, e in enumerate(itemsToExplore):
             transactionsPe = []
             utilityPe = 0
+            utilitySumPe = utilitySumP + self._singleItemSetsUtilities[e]
             previousTransaction = transactionsOfP[0]
             consecutiveMergeCount = 0
             for transaction in transactionsOfP:
                 items = transaction.getItems()
                 if e in items:
                     positionE = items.index(e)
                     if transaction.getLastPosition() == positionE:
@@ -556,41 +583,45 @@
                             transactionsPe.append(previousTransaction)
                             previousTransaction = projectedTransaction
                             consecutiveMergeCount = 0
                     transaction.offset = positionE
             if previousTransaction != transactionsOfP[0]:
                 transactionsPe.append(previousTransaction)
             self._temp[prefixLength] = self._newNamesToOldNames[e]
-            if utilityPe >= self._minUtil:
-                self._output(prefixLength, utilityPe)
+            utility_ratio_pe = float(utilityPe / utilitySumPe)
+            if (utilityPe >= self._minUtil) and (utility_ratio_pe * 100 >= self._minUR):
+                self._output(prefixLength, utilityPe, utility_ratio_pe)
             self._useUtilityBinArraysToCalculateUpperBounds(transactionsPe, idx, itemsToKeep)
             newItemsToKeep = []
             newItemsToExplore = []
             for l in range(idx + 1, len(itemsToKeep)):
                 itemK = itemsToKeep[l]
-                if self._utilityBinArraySU[itemK] >= self._minUtil:
+                utility_sum_pek = utilitySumPe + self._singleItemSetsUtilities[itemK]
+                subtree_utility_ratio = float(self._utilityBinArraySU[itemK] / utility_sum_pek)
+                local_utility_ratio = float(self._utilityBinArrayLU[itemK] / utility_sum_pek)
+                if self._utilityBinArraySU[itemK] >= self._minUtil and subtree_utility_ratio * 100 >= self._minUR:
                     newItemsToExplore.append(itemK)
                     newItemsToKeep.append(itemK)
-                elif self._utilityBinArrayLU[itemK] >= self._minUtil:
+                elif self._utilityBinArrayLU[itemK] >= self._minUtil and local_utility_ratio * 100 >= self._minUR:
                     newItemsToKeep.append(itemK)
-            if len(transactionsPe) != 0:
-                self._backTrackingEFIM(transactionsPe, newItemsToKeep, newItemsToExplore, prefixLength + 1)
+            self._backTrackingRHUIM(transactionsPe, newItemsToKeep, newItemsToExplore, prefixLength + 1, utilitySumPe)
 
     def _useUtilityBinArraysToCalculateUpperBounds(self, transactionsPe: list, j: int, itemsToKeep: list) -> None:
         """
         A method to  calculate the subtree utility and local utility of all items that can extend itemSet P U {e}
 
         :Attributes:
 
         :param transactionsPe: transactions the projected database for P U {e}
-        :type transactionsPe: list
-        :param j:the position of j in the list of promising items
+        :type transactionsPe: list or Dataset
+        :param j: the position of j in the list of promising items
         :type j:int
         :param itemsToKeep :the list of promising items
-        :type itemsToKeep: list
+        :type itemsToKeep: list or Dataset
+        :return: None
         """
         for i in range(j + 1, len(itemsToKeep)):
             item = itemsToKeep[i]
             self._utilityBinArrayLU[item] = 0
             self._utilityBinArraySU[item] = 0
         for transaction in transactionsPe:
             sumRemainingUtility = 0
@@ -599,43 +630,46 @@
                 item = transaction.getItems()[i]
                 if item in itemsToKeep:
                     sumRemainingUtility += transaction.getUtilities()[i]
                     self._utilityBinArraySU[item] += sumRemainingUtility + transaction.prefixUtility
                     self._utilityBinArrayLU[item] += transaction.transactionUtility + transaction.prefixUtility
                 i -= 1
 
-    def _output(self, tempPosition: int, utility: int) -> None:
+    def _output(self, tempPosition: int, utility: int, utilityRatio: float) -> None:
         """
-        Method to print high utility items
+         Method to print relative high utility itemSet
 
-        :Attributes:
+         :Attributes:
 
-        :param tempPosition: position of last item
-        :type tempPosition : int
-        :param utility: total utility of itemSet
-        :type utility: int
+         :param tempPosition: position of last item 
+         :type tempPosition : int 
+         :param utility: total utility of itemSet
+         :type utility: int
+         :param utilityRatio: utility ratio of an itemSet
+         :type utilityRatio: float
+         :return: None
         """
         self._patternCount += 1
         s1 = str()
         for i in range(0, tempPosition+1):
             s1 += self._dataset.intToStr.get((self._temp[i]))
             if i != tempPosition:
                 s1 += "\t"
-        self._finalPatterns[s1] = str(utility)
+        self._finalPatterns[s1] = [utility, utilityRatio]
 
-    def _isEqual(self, transaction1: '_Transaction', transaction2: '_Transaction') -> bool:
+    def _isEqual(self, transaction1: _Transaction, transaction2: _Transaction) -> bool:
         """
          A method to Check if two transaction are identical
 
          :Attributes:
 
-         :param  transaction1: the first transaction
-         :type  transaction1: Trans
-         :param  transaction2:    the second transaction
-         :type  transaction2: Trans
+         :param  transaction1: the first transaction.
+         :type  transaction1: Transaction
+         :param  transaction2:   The second transaction.
+         :type  transaction2: Transaction
          :return : whether both are identical or not
          :rtype: bool
         """
         length1 = len(transaction1.items) - transaction1.offset
         length2 = len(transaction2.items) - transaction2.offset
         if length1 != length2:
             return False
@@ -644,61 +678,64 @@
         while position1 < len(transaction1.items):
             if transaction1.items[position1] != transaction2.items[position2]:
                 return False
             position1 += 1
             position2 += 1
         return True
 
-    def _useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self, dataset: '_Dataset') -> None:
+    def _useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self, dataset: _Dataset) -> None:
         """
         Scan the initial database to calculate the subtree utility of each item using a utility-bin array
 
         :Attributes:
 
         :param dataset: the transaction database
-        :type dataset: list
+        :type dataset: Dataset
+        :return: None
         """
         for transaction in dataset.getTransactions():
             sumSU = 0
             i = len(transaction.getItems()) - 1
             while i >= 0:
                 item = transaction.getItems()[i]
-                sumSU += transaction.getUtilities()[i]
+                currentUtility = transaction.getUtilities()[i]
+                sumSU += currentUtility
+                self._singleItemSetsUtilities[item] += currentUtility
                 if item in self._utilityBinArraySU.keys():
                     self._utilityBinArraySU[item] += sumSU
                 else:
                     self._utilityBinArraySU[item] = sumSU
                 i -= 1
 
-    def _sortDatabase(self, transactions: list) -> None:
+    def sortDatabase(self, transactions: list) -> None:
         """
         A Method to sort transaction
 
         :Attributes:
 
         :param transactions: transaction of items
-        :type transactions: Transaction
-        :return: sorted transactions
-        :rtype: Trans
+        :type transactions: list
+        :return: sorted transactions.
+        :rtype: Transactions or list
         """
         cmp_items = _ab._functools.cmp_to_key(self.sort_transaction)
         transactions.sort(key=cmp_items)
 
-    def sort_transaction(self, trans1: '_Transaction', trans2: '_Transaction') -> int:
+    def sort_transaction(self, trans1: _Transaction, trans2: _Transaction) -> int:
         """
         A Method to sort transaction
 
         :Attributes:
 
-        :param trans1: the first transaction
-        :type trans1: Trans
-        :param trans2:the second transaction
-        :type trans2: Trans
-        :return: sorted transaction
-        :rtype:    Trans
+        :param trans1: the first transaction .
+        :type trans1: Transaction
+        :param trans2:the second transaction.
+        :type trans2: Transaction
+        :return: sorted transaction.
+        :rtype:   Transaction
         """
         trans1_items = trans1.getItems()
         trans2_items = trans2.getItems()
         pos1 = len(trans1_items) - 1
         pos2 = len(trans2_items) - 1
         if len(trans1_items) < len(trans2_items):
             while pos1 >= 0:
@@ -721,114 +758,121 @@
                 sub = trans2_items[pos2] - trans1_items[pos1]
                 if sub != 0:
                     return sub
                 pos1 -= 1
                 pos2 -= 1
             return 0
 
-    def _useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset: '_Dataset') -> None:
+    def _useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset: _Dataset) -> None:
         """
-        A method to calculate local utility of single itemset
+        A method to calculate local utility of single itemSets
 
         :Attributes:
 
-        :param dataset: the transaction database
-        :type dataset: dataset
+        :param dataset: the transaction database.
+        :type dataset: database
+        :return: None
+
         """
         for transaction in dataset.getTransactions():
             for item in transaction.getItems():
                 if item in self._utilityBinArrayLU:
                     self._utilityBinArrayLU[item] += transaction.transactionUtility
                 else:
                     self._utilityBinArrayLU[item] = transaction.transactionUtility
 
-    def getPatternsAsDataFrame(self) -> '_pd.DataFrame':
-        """
-        Storing final patterns in a dataframe
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+        """Storing final patterns in a dataframe
+
         :return: returning patterns in a dataframe
         :rtype: pd.DataFrame
             """
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Utility'])
+            data.append([a.replace('\t', ' '), b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Utility', 'UtilityRatio'])
 
         return dataFrame
     
     def getPatterns(self) -> dict:
-        """
-        Function to send the set of patterns after completion of the mining process
+        """ Function to send the set of patterns after completion of the mining process
+
         :return: returning patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
-        :type outFile: csv file
+        :type outFile: file
+        :return: None
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y)
+            patternsAndSupport = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % patternsAndSupport)
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
        """
         return self._memoryRSS
 
     def getRuntime(self) -> float:
-        """
-        Calculating the total amount of runtime taken by the mining process
+        """Calculating the total amount of runtime taken by the mining process
+
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
        """
         return self._endTime-self._startTime
 
     def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
-        print("Total number of High Utility Patterns:", len(self.getPatterns()))
+        print("Total number of Relative Utility Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == '__main__':
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-        if len(_ab._sys.argv) == 5:    #includes separator
-            _ap = EFIM(_ab._sys.argv[1], int(_ab._sys.argv[3]), _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:    #takes "\t" as a separator
-            _ap = EFIM(_ab._sys.argv[1], int(_ab._sys.argv[3]))
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:    #includes separator
+            _ap = RHUIM(_ab._sys.argv[1], int(_ab._sys.argv[3]), float(_ab._sys.argv[4]), _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:    #takes "\t" as a separator
+            _ap = RHUIM(_ab._sys.argv[1], int(_ab._sys.argv[3]), float(_ab._sys.argv[4]))
         _ap.startMine()
-        print("Total number of High Utility Patterns:", len(_ap.getPatterns()))
+        print("Total number of Relative High Utility Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS",  _ap.getMemoryRSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
-        _ap = EFIM('/Users/likhitha/Downloads/Utility_T10I4D100K.csv', 50000, '\t')
+        _ap = RHUIM('/Users/likhitha/Downloads/utility_datasets/Utility_T10I4D100K.csv', 150000, 0.6, '\t')
         _ap.startMine()
-        print("Total number of High Utility Patterns:", len(_ap.getPatterns()))
-        _ap.save('/Users/likhitha/Downloads/UPGrowth_output.txt')
+        print("Total number of Relative High Utility Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.3.9.2/PAMI/highUtilityPattern/basic/HMiner.py` & `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/HMiner.py`

 * *Files 18% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # --------------------------------------------------------
 #
 #
 #             from PAMI.highUtilityPattern.basic import HMiner as alg
 #
 #             obj = alg.HMiner("input.txt", 35)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             Patterns = obj.getPatterns()
 #
 #             print("Total number of high utility Patterns:", len(Patterns))
 #
 #             obj.save("output")
 #
@@ -23,20 +23,22 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
+
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -47,14 +49,15 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.highUtilityPattern.basic import abstract as _ab
+from deprecated import deprecated
 
 
 class _Element:
     """
     A class represents an Element of a utility list.
 
     :Attributes :
@@ -137,14 +140,29 @@
 
 class HMiner(_ab._utilityPatterns):
     """
     :Description:   High Utility itemSet Mining (HMIER) is an importent algorithm to miner High utility items from the database.
 
     :Reference:
 
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of High Utility patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of High Utility patterns
+    :param minUtil: int :
+                   The user given minUtil value.
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
 
         iFile : file
             Name of the input file to mine complete set of frequent patterns
         oFile : file
             Name of the output file to store complete set of frequent patterns
         memoryRSS : float
@@ -189,31 +207,37 @@
         updateElement(z, culs, st, excul, newT, ex, duppos, ey_ts)
             A method to updates vales for duplicates
         construcCUL(x, culs, st, minUtil, length, exnighbors)
             A method to construct CUL's database
 
     **Executing the code on terminal:**
     --------------------------------------------
-        Format:
 
-                  >>> python3 HMiner.py <inputFile> <outputFile> <minUtil>
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 HMiner.py <inputFile> <outputFile> <minUtil>
+
+      Example Usage:
 
-        Examples:
+      (.venv) $ python3 HMiner.py sampleTDB.txt output.txt 35
+
+    .. note:: minSup will be considered in percentage of database transactions
 
-                  >>> python3 HMiner.py sampleTDB.txt output.txt 35
 
     Sample run of importing the code:
     --------------------------------------
     .. code-block:: python
 
             from PAMI.highUtilityPattern.basic import HMiner as alg
         
             obj = alg.HMiner("input.txt",35)
         
-            obj.startMine()
+            obj.mine()
         
             Patterns = obj.getPatterns()
         
             print("Total number of high utility Patterns:", len(Patterns))
         
             obj.save("output")
         
@@ -256,17 +280,29 @@
         self._huiCount = 0
         self._candidates = 0
         self._mapOfTWU = {}
         self._minutil = 0
         self._mapFMAP = {}
         self._finalPatterns = {}
 
-    def _HMiner(self, o1, o2):
+    def _HMiner(self, o1, o2) -> int:
         """
-        A method to sort  list of huis in TWU ascending order
+        A Function that sort all FFI-list in ascending order of Support
+
+        :param o1: First FFI-list
+
+        :type o1: _FFList
+
+        :param o2: Second FFI-list
+
+        :type o1: _FFList
+
+        :return: Comparision Value
+
+        :rtype: int
         """
         compare = self._mapOfTWU[o1.item] - self._mapOfTWU[o2.item]
         if compare == 0:
             return int(o1.item) - int(o2.item)
         else:
             return compare
 
@@ -309,14 +345,15 @@
                             utilities = parts[2].split(self._sep)
                             self._utilities.append(utilities)
                             self._utilitySum.append(int(parts[1]))
                 except IOError:
                     print("File Not Found")
                     quit()
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Main program to start the operation
         """
         self._startTime = _ab._time.time()
         self._creteItemsets()
         self._finalPatterns = {}
@@ -404,20 +441,112 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryRSS = float()
         self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("High Utility patterns were generated successfully using HMiner algorithm")
 
+    def mine(self):
+        """
+        Main program to start the operation
+        """
+        self._startTime = _ab._time.time()
+        self._creteItemsets()
+        self._finalPatterns = {}
+        for line in range(len(self._transactions)):
+            items_str = self._transactions[line]
+            utility_str = self._utilities[line]
+            transUtility = self._utilitySum[line]
+            for i in range(0, len(items_str)):
+                item = items_str[i]
+                twu = self._mapOfTWU.get(item)
+                if twu == None:
+                    twu = transUtility
+                else:
+                    twu += transUtility
+                self._mapOfTWU[item] = twu
+        listOfCUList = []
+        hashTable = {}
+        mapItemsToCUList = {}
+        minutil = self._minUtil
+        for item in self._mapOfTWU.keys():
+            if self._mapOfTWU.get(item) >= self._minUtil:
+                uList = _CUList(item)
+                mapItemsToCUList[item] = uList
+                listOfCUList.append(uList)
+        listOfCUList.sort(key=_ab._functools.cmp_to_key(self._HMiner))
+        tid = 1
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            utilities = self._utilities[line]
+            ru = 0
+            newTwu = 0
+            tx_key = []
+            revisedTrans = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                pair.utility = int(utilities[i])
+                if self._mapOfTWU.get(pair.item) >= self._minUtil:
+                    revisedTrans.append(pair)
+                    tx_key.append(pair.item)
+                    newTwu += pair.utility
+            revisedTrans.sort(key=_ab._functools.cmp_to_key(self._HMiner))
+            tx_key1 = tuple(tx_key)
+            if len(revisedTrans) > 0:
+                if tx_key1 not in hashTable.keys():
+                    hashTable[tx_key1] = len(mapItemsToCUList[revisedTrans[len(revisedTrans) - 1].item].elements)
+                    for i in range(len(revisedTrans) - 1, -1, -1):
+                        pair = revisedTrans[i]
+                        cuListoFItems = mapItemsToCUList.get(pair.item)
+                        element = _Element(tid, pair.utility, ru, 0, 0)
+                        if i > 0:
+                            element.ppos = len(mapItemsToCUList[revisedTrans[i - 1].item].elements)
+                        else:
+                            element.ppos = - 1
+                        cuListoFItems.addElements(element)
+                        ru += pair.utility
+                else:
+                    pos = hashTable[tx_key1]
+                    ru = 0
+                    for i in range(len(revisedTrans) - 1, -1, -1):
+                        cuListoFItems = mapItemsToCUList[revisedTrans[i].item]
+                        cuListoFItems.elements[pos].nu += revisedTrans[i].utility
+                        cuListoFItems.elements[pos].nru += ru
+                        cuListoFItems.sumnu += revisedTrans[i].utility
+                        cuListoFItems.sumnru += ru
+                        ru += revisedTrans[i].utility
+                        pos = cuListoFItems.elements[pos].ppos
+                    # EUCS
+            for i in range(len(revisedTrans) - 1, -1, -1):
+                pair = revisedTrans[i]
+                mapFMAPItem = self._mapFMAP.get(pair.item)
+                if mapFMAPItem == None:
+                    mapFMAPItem = {}
+                    self._mapFMAP[pair.item] = mapFMAPItem
+                for j in range(i + 1, len(revisedTrans)):
+                    pairAfter = revisedTrans[j]
+                    twuSUm = mapFMAPItem.get(pairAfter.item)
+                    if twuSUm is None:
+                        mapFMAPItem[pairAfter.item] = newTwu
+                    else:
+                        mapFMAPItem[pairAfter.item] = twuSUm + newTwu
+            tid += 1
+        self._ExploreSearchTree([], listOfCUList, minutil)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("High Utility patterns were generated successfully using HMiner algorithm")
+
     def _ExploreSearchTree(self, prefix, uList, minutil):
         """
         A method to find all high utility itemSets
-
-        :Attributes:
-
         :parm prefix:it represents all items in prefix
         :type prefix:list
         :parm uList:projected Utility list
         :type uList: lists
         :parm minutil:user minUtil
         :type minutil:int
         """
@@ -432,17 +561,14 @@
             if x.sumnu + x.sumCu + x.sumnru + x.sumCru >= minutil:
                 exULs = self._construcCUL(x, uList, i, minutil, len(soted_prefix))
                 self._ExploreSearchTree(soted_prefix, exULs, minutil)
 
     def _construcCUL(self, x, culs, st, minutil, length):
         """
         A method to construct CUL's database
-
-        :Attributes:
-
         :parm x: Compact utility list
         :type x: Node
         :parm culs:list of Compact utility list
         :type culs:lists
         :parm st: starting pos of culs
         :type st:int
         :parm minutil: user minUtil
@@ -528,23 +654,22 @@
                     excul[j].sumCpu += x.sumCu
                 filter_culs.append(excul[j])
         return filter_culs
 
     def _UpdateCLosed(self, x, culs, st, excul, newT, ex, ey_tid, length):
         """
         A method to update closed values
-
-        :Attributes:
-
         :parm x: Compact utility list
         :type x: lists
         :parm culs:list of Compact utility list
         :type culs:lists
         :parm st: starting pos of culs
         :type st:int
+        :parm excul: list of culs
+        :type excul: list
         :parm newT:transaction to be updated
         :type newT:list
         :parm ex: element ex
         :type ex:element
         :parm ey_tid:list of tss
         :type ey_tid:ts
         :parm length: length of x
@@ -594,17 +719,14 @@
             excul[newT[j]].elements[pos].pu += ex.nu
             nru = nru + eyy.nu - ex.pu
             pos = excul[newT[j]].elements[pos].ppos
 
     def _saveitemSet(self, prefix, prefixLen, item, utility):
         """
         A method to save itemSets
-
-        :Attributes:
-
         :parm prefix: it represents all items in prefix
         :type prefix :list
         :parm prefixLen: length of prefix
         :type prefixLen:int
         :parm item:item
         :type item: int
         :parm utility:utility of itemSet
@@ -690,14 +812,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:  # includes separator
             _ap = HMiner(_ab._sys.argv[1], int(_ab._sys.argv[3]), _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:  # to consider "\t" as a separator
             _ap = HMiner(_ab._sys.argv[1], int(_ab._sys.argv[3]))
         _ap.startMine()
+        _ap.mine()
         print("Total number of huis:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS",  _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/highUtilityPattern/basic/UPGrowth.py` & `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/UPGrowth.py`

 * *Files 12% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # --------------------------------------------------------
 #
 #
 #             from PAMI.highUtilityPattern.basic import UPGrowth as alg
 #
 #             obj=alg.UPGrowth("input.txt",35)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             highUtilityPattern = obj.getPatterns()
 #
 #             print("Total number of Spatial Frequent Patterns:", len(highUtilityPattern))
 #
 #             obj.save("output")
 #
@@ -23,21 +23,22 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -49,14 +50,15 @@
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.highUtilityPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 
 class _UPItem:
     """
     A class to represent the UPItem
 
     :Attributes:
@@ -81,28 +83,33 @@
     def __init__(self, name: int, utility: int):
         self.name = name
         self.utility = utility
 
     def getUtility(self) -> int:
         """
         method to get node utility
+        :return: Utility value of Node
+        :rtype: int
         """
         return self.utility
 
     def setUtility(self, utility: int) -> None:
         """
         method to set node utility
         :param utility: the utility to set
         :type utility: int
+        :return: None
         """
         self.utility = utility
 
     def getName(self) -> int:
         """
         method to get name for particular item
+        :return: name of particular item
+        :rtype: int
         """
         return self.name
 
 
 class _UPNode:
     """
     A class that represent UPNode
@@ -140,14 +147,16 @@
         self.parent = -1
 
     def getChildWithId(self, name: int) -> int:
         """
         method to get child node Return the immediate child of this node having a given name
         :param name: represent id of item
         :type name: int
+        :return: id of child node with same itemid
+        :rtype: int
         """
         for child in self.childs:
             if child.itemId == name:
                 return child
         return -1
 
 
@@ -195,14 +204,16 @@
     def addTransaction(self, transaction: list, RTU: int) -> int:
         """
         A Method to add new Transaction to tree
         :param transaction: the reorganised transaction
         :type transaction: list
         :param RTU :reorganised transaction utility
         :type RTU: int
+        :return: the number of transactions added
+        :rtype: int
         """
         currentNode = self.root
         NumberOfNodes = 0
         RemainingUtility = 0
         # for idx, item in enumerate(transaction):
         #     itemName = item.name
         #     child = currentNode.getChildWithId(itemName)
@@ -241,14 +252,16 @@
         :type localPath: list
         :param pathUtility: the Utility of path
         :type pathUtility: int
         :param mapItemToMinimumItemutility: the map storing minimum item utility
         :type mapItemToMinimumItemutility: map
         :param pathCount: the Path count
         :type pathCount: int
+        :return: The number of nodes in the tree that have been added or removed from the tree
+        :rtype: int
         """
         currentLocalNode = self.root
         RemainingUtility = 0
         NumberOfNodes = 0
         # for item in localPath:
         #     RemainingUtility += mapItemToMinimumItemutility[item] * pathCount
         # for item in localPath:
@@ -285,14 +298,16 @@
         A method to Insert a new node in the UP-Tree as child of a parent node
         :param currentlocalNode: The parent Node
         :type currentlocalNode: UPNode
         :param itemName: name of item in new Node
         :type itemName: int
         :param nodeUtility: Utility of new node
         :type nodeUtility: int
+        :return: The newly created UPNode
+        :rtype: _UPNode
         """
         newNode = _UPNode()
         newNode.itemId = itemName
         newNode.count = 1
         newNode.nodeUtility = nodeUtility
         newNode.parent = currentlocalNode
         currentlocalNode.childs.append(newNode)
@@ -308,27 +323,42 @@
         return newNode
 
     def createHeaderList(self, mapItemToTwu: dict) -> None:
         """
         A Method for creating the list of items in the header table, in descending order of TWU or path utility.
         :param mapItemToTwu: the Utilities of each item
         :type mapItemToTwu: map
+        :return: None
         """
         self.headerList = list(self.mapItemNodes.keys())
         self.headerList = sorted(self.headerList, key=lambda x: mapItemToTwu[x], reverse=True)
 
 
 class UPGrowth(_ab._utilityPatterns):
     """
     :Description:   UP-Growth is two-phase algorithm to mine High Utility Itemsets from transactional databases.
 
     :Reference:     Vincent S. Tseng, Cheng-Wei Wu, Bai-En Shie, and Philip S. Yu. 2010. UP-Growth: an efficient algorithm for high utility itemset mining.
                     In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '10).
                     Association for Computing Machinery, New York, NY, USA, 253–262. DOI:https://doi.org/10.1145/1835804.1835839
 
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of High Utility patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of High Utility patterns
+    :param minUtil: int :
+                   The user given minUtil value.
+    :param candidateCount: int
+                   Number of candidates specified by user
+    :param maxMemory: int
+                   Maximum memory used by this program for running
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
 
         iFile : file
             Name of the input file to mine complete set of frequent patterns
         oFile : file
             Name of the output file to store complete set of frequent patterns
         memoryRSS : float
@@ -371,30 +401,37 @@
         getMemoryRSS()
                 Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
                Total amount of runtime taken by the mining process will be retrieved from this function
 
     **Executing the code on terminal:**
     ---------------------------------------------
-        Format:
 
-                  >>> python3 UPGrowth <inputFile> <outputFile> <Neighbours> <minUtil> <sep>
-        Examples:
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 UPGrowth <inputFile> <outputFile> <Neighbours> <minUtil> <sep>
+
+      Example Usage:
+
+      (.venv) $ python3 UPGrowth sampleTDB.txt output.txt sampleN.txt 35
+
+    .. note:: maxMemory will be considered as Maximum memory used by this program for running
 
-                  >>> python3 UPGrowth sampleTDB.txt output.txt sampleN.txt 35
 
     Sample run of importing the code:
     -------------------------------------
     .. code-block:: python
     
             from PAMI.highUtilityPattern.basic import UPGrowth as alg
 
             obj=alg.UPGrowth("input.txt",35)
 
-            obj.startMine()
+            obj.mine()
 
             highUtilityPattern = obj.getPatterns()
 
             print("Total number of Spatial Frequent Patterns:", len(highUtilityPattern))
 
             obj.save("output")
 
@@ -436,14 +473,15 @@
 
     def __init__(self, iFile: str, minUtil: int, sep: str='\t') -> None:
         super().__init__(iFile, minUtil, sep)
 
     def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             timeStamp, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
@@ -468,17 +506,107 @@
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             self._Database.append(line)
                 except IOError:
                     print("File Not Found")
                     quit()
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Mining process will start from here
+        :return: None
+        """
+        self._startTime = _ab._time.time()
+        tree = _UPTree()
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        for line in self._Database:
+            line = line.split("\n")[0]
+            transaction = line.strip().split(':')
+            items = transaction[0].split(self._sep)
+            transactionUtility = int(transaction[1])
+            for item in items:
+                Item = int(item)
+                if Item in self._MapItemToTwu:
+                    self._MapItemToTwu[Item] += transactionUtility
+                else:
+                    self._MapItemToTwu[Item] = transactionUtility
+        for line in self._Database:
+            line = line.split("\n")[0]
+            transaction = line.strip().split(':')
+            items = transaction[0].split(self._sep)
+            utilities = transaction[2].split(self._sep)
+            remainingUtility = 0
+            revisedTransaction = []
+            for idx, item in enumerate(items):
+                Item = int(item)
+                utility = int(utilities[idx])
+                if self._MapItemToTwu[Item] >= self._minUtil:
+                    element = _UPItem(Item, utility)
+                    revisedTransaction.append(element)
+                    remainingUtility += utility
+                    if Item in self._MapItemToMinimumUtility:
+                        minItemUtil = self._MapItemToMinimumUtility[Item]
+                        if minItemUtil >= utility:
+                            self._MapItemToMinimumUtility[Item] = utility
+                    else:
+                        self._MapItemToMinimumUtility[Item] = utility
+            revisedTransaction = sorted(revisedTransaction, key=lambda x: self._MapItemToTwu[x.name], reverse=True)
+            self._ParentNumberOfNodes += tree.addTransaction(revisedTransaction, remainingUtility)
+        tree.createHeaderList(self._MapItemToTwu)
+        alpha = []
+        self._finalPatterns = {}
+        # print("number of nodes in parent tree", self.ParentNumberOfNodes)
+        self._UPGrowth(tree, alpha)
+        # self.phuis = sorted(self.phuis, key=lambda x: len(x))
+        # print(self.phuis[0:10])
+        for line in self._Database:
+            line = line.split("\n")[0]
+            transaction = line.strip().split(':')
+            items = transaction[0].split(self._sep)
+            utilities = transaction[2].split(self._sep)
+            mapItemToUtility = {}
+            for idx, item in enumerate(items):
+                Item = int(item)
+                utility = int(utilities[idx])
+                if self._MapItemToTwu[Item] >= self._minUtil:
+                    mapItemToUtility[Item] = utility
+            for itemset in self._phuis:
+                l = len(itemset)
+                count = 0
+                utility = 0
+                for item in itemset:
+                    item = int(item)
+                    if item in mapItemToUtility:
+                        utility += mapItemToUtility[item]
+                        count += 1
+                if count == l:
+                    self._MapItemsetsToUtilities[tuple(itemset)] += utility
+
+        for itemset in self._phuis:
+            util = self._MapItemsetsToUtilities[tuple(itemset)]
+            if util >= self._minUtil:
+                s = str()
+                for item in itemset:
+                    s = s + str(item)
+                    s = s + "\t"
+                self._finalPatterns[s] = util
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("High Utility patterns were generated successfully using UPGrowth algorithm")
+
+    def mine(self) -> None:
+        """
+        Mining process will start from here
+        :return: None
         """
         self._startTime = _ab._time.time()
         tree = _UPTree()
         self._creatingItemSets()
         self._finalPatterns = {}
         for line in self._Database:
             line = line.split("\n")[0]
@@ -562,14 +690,15 @@
     def _UPGrowth(self, tree: _UPTree, alpha: list) -> None:
         """
         A Method to Mine UP Tree recursively
         :param tree: UPTree to mine
         :type tree: UPTree
         :param alpha: prefix itemset
         :type alpha: list
+        :return: None
         """
         for item in reversed(tree.headerList):
             localTree = self._createLocalTree(tree, item)
             node = tree.mapItemNodes[item]
             ItemTotalUtility = 0
             while node != -1:
                 ItemTotalUtility += node.nodeUtility
@@ -585,14 +714,16 @@
     def _createLocalTree(self, tree: _UPTree, item: int) -> _UPTree:
         """
         A Method to Construct conditional pattern base
         :param tree: the UPtree
         :type tree: UP Tree
         :param item: item that need to construct conditional patterns
         :type item: int
+        :return: the conditional pattern based UPTree
+        :rtype: _UPTree
         """
         prefixPaths = []
         path = tree.mapItemNodes[item]
         itemPathUtility = {}
         while path != -1:
             nodeUtility = path.nodeUtility
             if path.parent != -1:
@@ -625,14 +756,15 @@
                                                                 pathCount)
         localTree.createHeaderList(itemPathUtility)
         return localTree
 
     def PrintStats(self) -> None:
         """
         A Method to print number of phuis
+        :return: None
         """
         print('number of PHUIS are ' + str(len(self._phuis)))
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
         :return: returning frequent patterns in a dataframe
@@ -655,14 +787,15 @@
         return self._finalPatterns
 
     def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + " : " + str(y)
             writer.write("%s\n" % patternsAndSupport)
 
@@ -690,14 +823,15 @@
         :rtype: float
         """
         return self._endTime - self._startTime
 
     def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
         print("Total number of High Utility Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
@@ -705,21 +839,23 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = UPGrowth(_ab._sys.argv[1], int(_ab._sys.argv[3]), _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = UPGrowth(_ab._sys.argv[1], int(_ab._sys.argv[3]))
         _ap.startMine()
+        _ap.mine()
         print("Total number of High Utility Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         _ap = UPGrowth('/Users/likhitha/Downloads/Utility_T10I4D100K.csv', 50000, '\t')
         _ap.startMine()
+        _ap.mine()
         print("Total number of High Utility Patterns:", len(_ap.getPatterns()))
         _ap.save('/Users/likhitha/Downloads/UPGrowth_output.txt')
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/highUtilityPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/highUtilityPattern/basic/efimParallel.py` & `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/efimParallel.py`

 * *Files 7% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # --------------------------------------------------------
 #
 #
 #             from PAMI.highUtilitySpatialPattern.basic import efimParallel as alg
 #
 #             obj=alg.efimParallel("input.txt","Neighbours.txt",35)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             Patterns = obj.getPatterns()
 #
 #             print("Total number of Spatial High-Utility Patterns:", len(Patterns))
 #
 #             obj.save("output")
 #
@@ -25,16 +25,19 @@
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -49,14 +52,15 @@
 """
 
 import os
 import mmap
 import time
 import psutil
 from joblib import Parallel, delayed
+from deprecated import deprecated
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -77,15 +81,28 @@
 
 class efimParallel(_ab._utilityPatterns):
     """
     :Description:  EFIM is one of the fastest algorithm to mine High Utility ItemSets from transactional databases.
     
     :Reference:    Zida, S., Fournier-Viger, P., Lin, J.CW. et al. EFIM: a fast and memory efficient algorithm for
                    high-utility itemset mining. Knowl Inf Syst 51, 595–625 (2017). https://doi.org/10.1007/s10115-016-0986-0
-    
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of High Utility patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of High Utility patterns
+    :param minUtil: int :
+                   The user given minUtil value.
+    :param candidateCount: int
+                   Number of candidates specified by user
+    :param maxMemory: int
+                   Maximum memory used by this program for running
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
     :Attributes:
 
         inputFile (str):
             The input file path.
         minUtil (int):
             The minimum utility threshold.
         sep (str):
@@ -126,31 +143,37 @@
         getMemoryUSS():
             Get the Unique Set Size (USS) memory usage of the algorithm.
         printResults():
             Print the results of the algorithm.
 
     **Executing the code on terminal:**
     ------------------------------------------
-        Format:
 
-                  >>> python3 efimParallel.py <inputFile> <outputFile> <minUtil> <sep>
-        Examples:
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 efimParallel.py <inputFile> <outputFile> <minUtil> <sep>
+
+      Example Usage:
 
-                  >>> python3 efimParallel sampleTDB.txt output.txt 35
-                  >>> python3 efimParallel sampleTDB.txt output.txt 35
+      (.venv) $ python3 efimParallel sampleTDB.txt output.txt 35
 
-     Sample run of importing the code:
-    --------------------------------------
+    .. note:: maxMemory will be considered as Maximum memory used by this program for running
+
+
+    **Importing this algorithm into a python program**
+    -------------------------------------------------------
     .. code-block:: python
 
             from PAMI.highUtilityPattern.basic import efimParallel as alg
 
             obj = alg.efimParallel("input.txt",35)
 
-            obj.startMine()
+            obj.mine()
 
             Patterns = obj.getPatterns()
 
             print("Total number of high utility Patterns:", len(Patterns))
 
             obj.save("output")
 
@@ -163,15 +186,15 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    -----------------------------
+    ----------------
             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
     """
 
     def __init__(self, iFile, minUtil, sep="\t", threads=1):
         super().__init__(iFile, minUtil, sep)
         self.inputFile = iFile
         self.minUtil = minUtil
@@ -264,25 +287,21 @@
         return filtered_transactions, primary, secondary
     
 
     def _binarySearch(self, arr, item):
         """
         Do a binary search on the given array to find the given item.
 
-        :Attributes:
+        :param arr: The array to search in.
 
-            arr (list):
-                The array to search in.
-            item (int):
-                The item to search for.
+        :type arr: list
 
-        :return:
-
-            mid (int): The index of the item if found, -1 otherwise.
+        :param item: The item to search for
 
+        :type item: int
         """
 
         low = 0
         high = len(arr) - 1
         mid = 0
 
         while low <= high:
@@ -296,22 +315,25 @@
 
         return -1
 
     def _project(self, beta, file_data, secondary):
         """
         Project the given beta itemset on the given database.
 
-        :Attributes:
+        :param beta: The beta itemset to be projected
+
+        :type beta: list
 
-            beta (list):
-                The beta itemset to project.
-            file_data (dict):
-                The database to project on.
-            secondary (set):
-                The set of secondary items.
+        :param file_data: The database to project on.
+
+        :type file_data: list
+
+        :param secondary: The set of secondary items
+
+        :type secondary: set
 
         :return:
 
             projected_db (dict):
                 The projected database.
             local_utils (dict):
                 The local utilities of the projected database.
@@ -380,17 +402,17 @@
     
 
     def _search(self, collections):
 
         """
         Search for frequent patterns in the given collections.
 
-        :Attributes:
+        :param collections: list of collections to search in
 
-            collections (list): The collections to search in.
+        :type collections: list
         """
 
         if (self.threads > 1):
             with Parallel(n_jobs=self.threads) as parallel:
                 while len(collections) > 0:
                     new_collections = []
 
@@ -419,21 +441,39 @@
                             # self.Patterns[tuple(beta)] = utility
                             self.Patterns[pattern] = utility
                         if len(primary) > 0:
                             new_collections.append([beta, projected_db, primary, secondary])
 
                 collections = new_collections
 
-
-
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Start the EFIM algorithm.
+        """
+
+        ps = psutil.Process(os.getpid())
+
+        self.start = time.time()
 
-        :return:    None
+        fileData, primary, secondary = self._read_file()
+
+        collection = [[[], fileData, primary, secondary]]
+
+        self._search(collection)
+
+        self.memoryRSS = ps.memory_info().rss
+        self.memoryUSS = ps.memory_full_info().uss
+
+        end = time.time()
+        self.runtime = end - self.start
+
+    def mine(self):
+        """
+        Start the EFIM algorithm.
         """
 
         ps = psutil.Process(os.getpid())
 
         self.start = time.time()
 
         fileData, primary, secondary = self._read_file()
@@ -537,34 +577,37 @@
 
     # # inputFile = "EFIM/Utility_pumsb.csv"
     # # minUtil = 7500000
 
     # sep = " "
     # f = efimParallel(inputFile, minUtil, sep, 1)
     # f.startMine()
+    # f.mine()
     # print("# of patterns: " + str(len(f.getPatterns())))
     # print("Time taken: " + str(f.getRuntime()))
     # f.savePatterns("mine.txt")
 
 
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:    #includes separator
             _ap = efimParallel(_ab._sys.argv[1], int(_ab._sys.argv[3]), _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:    #takes "\t" as a separator
             _ap = efimParallel(_ab._sys.argv[1], int(_ab._sys.argv[3]))
         _ap.startMine()
+        _ap.mine()
         print("Total number of High Utility Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS",  _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
         _ap = efimParallel('/Users/likhitha/Downloads/Utility_T10I4D100K.csv', 50000, '\t')
         _ap.startMine()
+        _ap.mine()
         print("Total number of High Utility Patterns:", len(_ap.getPatterns()))
         _ap.save('/Users/likhitha/Downloads/UPGrowth_output.txt')
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/highUtilityPattern/parallel/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/highUtilityPattern/parallel/efimparallel.py` & `pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/efimparallel.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # --------------------------------------------------------
 #
 #
 #             from PAMI.highUtilityPattern.parallel import efimparallel as alg
 #
 #             obj=alg.efimparallel("input.txt",35)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             highUtilityPattern = obj.getPatterns()
 #
 #             print("Total number of Spatial Frequent Patterns:", len(highUtilityPattern))
 #
 #             obj.save("output")
 #
@@ -23,21 +23,22 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
 
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -52,25 +53,38 @@
 """
 
 import os
 import mmap
 import time
 import psutil
 from joblib import Parallel, delayed
+from deprecated import deprecated
 
 
 from PAMI.highUtilityPattern.parallel import abstract as _ab
 
 class efimParallel(_ab._utilityPatterns):
     """
     :Description:   EFIM is one of the fastest algorithm to mine High Utility ItemSets from transactional databases.
     
     :Reference:     Zida, S., Fournier-Viger, P., Lin, J.CW. et al. EFIM: a fast and memory efficient algorithm for
                      high-utility itemset mining. Knowl Inf Syst 51, 595–625 (2017). https://doi.org/10.1007/s10115-016-0986-0
-    
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of High Utility patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of High Utility patterns
+    :param minUtil: int :
+                   The user given minUtil value.
+    :param maxMemory: int
+                   Maximum memory used by this program for running
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
 
         inputFile (str):
             The input file path.
         minUtil (int):
             The minimum utility threshold.
         sep (str):
@@ -208,20 +222,21 @@
         return filtered_transactions, primary, secondary
     
 
     def _binarySearch(self, arr, item):
         """
         Do a binary search on the given array to find the given item.
 
-        :Attributes:
+        :param arr: The array to search in
 
-            arr (list):
-                The array to search in.
-            item (int):
-                The item to search for.
+        :type arr: list
+
+        :param item: The item to search for
+
+        :type item: int
 
         :return:
 
             mid (int):
                 The index of the item if found, -1 otherwise.
 
         """
@@ -241,22 +256,25 @@
 
         return -1
 
     def _project(self, beta, file_data, secondary):
         """
         Project the given beta itemset on the given database.
 
-        :Attributes:
+        :param beta: The beta itemset to project
+
+        :type beta: list
+
+        :param file_data: The database to project on
+
+        :type file_data: dict
 
-            beta (list):
-                The beta itemset to project.
-            file_data (dict):
-                The database to project on.
-            secondary (set):
-                The set of secondary items.
+        :param secondary: The set of secondary items
+
+        :type secondary: set
 
         :return:
 
             projected_db (dict): The projected database.
             local_utils (dict): The local utilities of the projected database.
             subtree_utils (dict): The subtree utilities of the projected database.
             utility (int): The utility of the projected database.
@@ -322,18 +340,17 @@
     
 
     def _search(self, collections):
 
         """
         Search for frequent patterns in the given collections.
 
-        :Attributes:
+        :param collections: The collections to search in
 
-            collections (list):
-                The collections to search in.
+        :type collections: list
         """
 
         if (self.threads > 1):
             with Parallel(n_jobs=self.threads) as parallel:
                 while len(collections) > 0:
                     new_collections = []
 
@@ -363,20 +380,39 @@
                             self.Patterns[pattern] = utility
                         if len(primary) > 0:
                             new_collections.append([beta, projected_db, primary, secondary])
 
                 collections = new_collections
 
 
-
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Start the EFIM algorithm.
+        """
+
+        ps = psutil.Process(os.getpid())
 
-        :return: None
+        self.start = time.time()
+
+        fileData, primary, secondary = self._read_file()
+
+        collection = [[[], fileData, primary, secondary]]
+
+        self._search(collection)
+
+        self.memoryRSS = ps.memory_info().rss
+        self.memoryUSS = ps.memory_full_info().uss
+
+        end = time.time()
+        self.runtime = end - self.start
+
+    def mine(self):
+        """
+        Start the EFIM algorithm.
         """
 
         ps = psutil.Process(os.getpid())
 
         self.start = time.time()
 
         fileData, primary, secondary = self._read_file()
@@ -417,43 +453,47 @@
 
         return dataFrame
 
     def getPatterns(self):
         """
         Get the patterns discovered by the algorithm.
 
-        :return:
-            dict: A dictionary containing the discovered patterns.
+        :return: A dictionary containing the discovered patterns.
+
+        :rtype: dict
         """
         return self.Patterns
 
     def getRuntime(self):
         """
         Get the runtime of the algorithm.
 
-        :return:
-            float: The runtime in seconds.
+        :return: The runtime in seconds.
+
+        :rtype: float
         """
         return self.runtime
 
     def getMemoryRSS(self):
         """
         Get the Resident Set Size (RSS) memory usage of the algorithm.
 
-        :return:
-            int: The RSS memory usage in bytes.
+        :return: The RSS memory usage in bytes.
+
+        :rtype: int
         """
         return self.memoryRSS
 
     def getMemoryUSS(self):
         """
         Get the Unique Set Size (USS) memory usage of the algorithm.
 
-        :return:
-            int: The USS memory usage in bytes.
+        :return: The USS memory usage in bytes.
+
+        :rtype: int
         """
         return self.memoryUSS
 
 
     def printResults(self):
         """
         This function is used to print the results
@@ -480,33 +520,36 @@
 
     # # inputFile = "EFIM/Utility_pumsb.csv"
     # # minUtil = 7500000
 
     # sep = " "
     # f = efimParallel(inputFile, minUtil, sep, 1)
     # f.startMine()
+    # f.mine()
     # print("# of patterns: " + str(len(f.getPatterns())))
     # print("Time taken: " + str(f.getRuntime()))
     # f.savePatterns("mine.txt")
 
 
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:    #includes separator
             _ap = efimParallel(_ab._sys.argv[1], int(_ab._sys.argv[3]), _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:    #takes "\t" as a separator
             _ap = efimParallel(_ab._sys.argv[1], int(_ab._sys.argv[3]))
         _ap.startMine()
+        _ap.mine()
         print("Total number of High Utility Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS",  _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
         _ap = efimParallel('/Users/likhitha/Downloads/Utility_T10I4D100K.csv', 50000, '\t')
         _ap.startMine()
+        _ap.mine()
         print("Total number of High Utility Patterns:", len(_ap.getPatterns()))
         _ap.save('/Users/likhitha/Downloads/UPGrowth_output.txt')
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/highUtilityPatternsInStreams/HUPMS.py` & `pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/HUPMS.py`

 * *Files 4% similar despite different names*

```diff
@@ -3,40 +3,44 @@
 # It stores the database of the current window in form of HUSTree and adjusts the tree based on upcoming
 # transactions removing the oldest batch.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.highUtilitySpatialPattern.basic import HUPMS as alg
+#             from PAMI.highUtilitySpatialPattern.basic import HUPMS as alg
 #
-#     obj=alg.HUPMS("input.txt","Neighbours.txt",35)
+#             obj=alg.HUPMS("input.txt","Neighbours.txt",35)
 #
-#     obj.startMine()
+#             obj.mine()
 #
-#     Patterns = obj.getPatterns()
+#             Patterns = obj.getPatterns()
 #
-#     print("Total number of Spatial High-Utility Patterns:", len(Patterns))
+#             print("Total number of Spatial High-Utility Patterns:", len(Patterns))
 #
-#     obj.save("output")
+#             obj.save("output")
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
+#
+
+
+
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -50,14 +54,15 @@
 
 """
 
 import abstract as _hus
 import pandas as pd
 from functools import reduce
 from operator import and_ 
+from deprecated import deprecated
 
 _minSup = str()
 _hus._sys.setrecursionlimit(20000)
 
 
 class _Node:
     """
@@ -107,29 +112,32 @@
         self.utility[batchIndex] = utility
         self.parent = None
 
     def addUtility(self, utility, batchIndex):
         """
         Adds utility to the node
 
+        :param utility : Next utility value to be added
 
-        :param utility : int
-            Net utility value to be added
-        :param batchIndex : int
-            index of the batch with respect to window to which the node belongs
+        :type utility : int
+
+        :param batchIndex : Index of the batch with respect to window to which the node belongs
+
+        :type batchIndex : int
         """
 
         self.utility[batchIndex] += utility
 
     def removeUtility(self, utility):
         """
         Removes utility from the node
 
-        :param utility : int
-                Net utility value to be removed
+        :param utility : Next utility value to be removed
+
+        :type utility : int
         """
 
         self.utility -= utility
 
     def shiftUtility(self):
         """
         Shifts the utility values of the node to the left
@@ -171,23 +179,25 @@
         self.table = dict()
         self.orderedItems = list()
 
     def updateUtility(self, item, utility, node):
         """
         Updates the utility of the item in the header table
 
+        :param item: name of the item to which utility needs to be updated
 
-        :param item : str
-            name of the item to which utility needs to be updated
+        :type item: str
 
-        :param utility : int
-            Net utility of the item to be updated
+        :param utility: Next utility of the item to be updated
 
-        :param node : _Node
-            pointer to the node in the tree to which the item belongs
+        :type utility: int
+
+        :param node: pointer to the node in the tree to which the item belongs
+
+        :type node: _Node
         """
 
         if item in self.table:
             self.table[item][0] += utility
             tempNode = self.table[item][1]
 
             while tempNode.next is not None:
@@ -200,30 +210,36 @@
     
         self.itemOrdering()
 
     def addUtility(self, item, utility):
         """
         Adds utility to the item in the header table
 
-        :param item : str
-            name of the item to which utility needs to be added
-        :param utility : int
-            Net utility of the item to be added
+        :param item: Name of the item to which utility needs to be added
+
+        :type item: str
+
+        :param utility: Next utility of the item to be added
+
+        :type utility: int
         """
         self.table[item][0] += utility
     
 
     def removeUtility(self, item, utility):
         """
         Removes utility from the item in the header table
 
-        :param item : str
-            name of the item to which utility needs to be removed
-        :param utility : int
-            Net utility of the item to be removed
+        :param item: Name of the item to which utility needs to be removed
+
+        :type item: str
+
+        :param utility: The utility of the item to be removed
+
+        :type utility: int
         """
         self.table[item][0] -= utility
 
         if self.table[item][0] == 0:
             del self.table[item]
 
         self.itemOrdering()
@@ -280,20 +296,21 @@
         self.batchIndex = 0
         self.windowUtility = 0
 
     def addTransaction(self, transaction, utility):
         """
         Adds transaction to the tree
 
-            :param transaction : list
-                list of items in the transaction
+        :param transaction: list of items in the transaction
 
-            :param utility : int
-                Net utility of the transaction
+        :type transaction: list
 
+        :param utility: Next utility of the transaction
+
+        :type utility: int
         """
         transaction.sort(key = lambda x: x[0])
         currentNode = self.root
         self.windowUtility += utility
         for item in transaction:
             if item in currentNode.children:
                 currentNode = currentNode.children[item]
@@ -326,17 +343,17 @@
 
 
     def removeBatchUtility(self, tempNode):
         
         """
         Removes the utility of the oldest batch from each subtree
 
-        :param tempNode : _Node
-            pointer to the node in the tree
+        :param tempNode:  pointer to the node in the tree
 
+        :type tempNode: _Node
         """
         if tempNode is None:
             return
 
         for item in tempNode.children:
             self.removeBatchUtility(tempNode.children[item])
 
@@ -372,14 +389,26 @@
                     It stores the database of the cuurent window in form of HUSTree and adjusts the tree based on upcoming
                     transactions removing the oldest batch.
 
     :References:   Chowdhury Farhan Ahmed and Syed Khairuzzaman Tanbeer and Byeong-Soo Jeong and Ho-Jin Choi : Interactive
                    mining of high utility patterns over data streams. Expert Systems with Applications Vol 39, 11979 - 11991, 2012.
                    https://doi.org/10.1016/j.eswa.2012.03.062
 
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of High Utility patterns in Streams
+    :param  oFile: str :
+                   Name of the output file to store complete set of High Utility patterns in Streams
+    :param minUtil: int :
+                   Minimum utility threshold
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
+
     :Attributes:
 
         __startTime : float
             start time of the mining process
 
         __endTime : float
             end time of the mining process
@@ -461,29 +490,34 @@
         getPatternsAsDataFrame()
             Returns the final patterns generated in each window as a pandas dataframe
 
         getRuntime()
             Returns the runtime of the algorithm
 
         printResults()
-            Prints the statistics of the mining process
+            Prints the stats of the mining process
 
         save()
             Saves the patterns generated from each window in a file
 
 
     **Executing the code on terminal:**
     -------------------------------------
-            Format:
-                >>> python3 HUPMS.py <inputFile> <outputFile> <minUtil> <windowSize> <paneSize> <separator>
 
-            Example:
-                >>> python3 HUPMS.py retail.txt output.txt 107 100 1000 ','
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 HUPMS.py <inputFile> <outputFile> <minUtil> <windowSize> <paneSize> <separator>
+
+      Example Usage:
 
-            .. note::    Here minimum utility is 107, Window size is 100 and pane size is 1000. The separator is comma for the input file)
+      (.venv) $ python3 HUPMS.py retail.txt output.txt 107 100 1000 ','
+
+    .. note:: Here minimum utility is 107, Window size is 100 and pane size is 1000. The separator is comma for the input file
 
     **Credits:**
     -------------
 
     The code is written by Vipul Chhabra under the supervision of Prof. Rage Uday Kiran.
     
     """
@@ -552,22 +586,25 @@
                     print("File Not Found")
                     quit()
 
     def createPrefixBranch(self, root):
         """
         Creates the prefix branch of the node
 
+        :param root: pointer to the root node of the sub-tree
+
+        :type root: Node
+
+        :return stack: list of the nodes in prefix branch
+
+        :rtype: list
 
-        :param root : _Node
-                pointer to the root node of the sub-tree
+        :return curUtil: Utility of the prefix branch
 
-        :return stack : list
-                    list of the nodes in prefix branch
-        :return curUtil : int
-                    utility of the prefix branch
+        :rtype: int
         """
         stack = []
 
         while(root is not None):
             stack.append(root)
             root = root.parent
 
@@ -575,16 +612,17 @@
         curUtil = sum(chosenItemset.utility)
         return stack, curUtil
 
     def fixUtility(self, root):
         """
         Fixes the utility of the nodes in the tree by merging the utility values
 
-        :param root : _Node
-                pointer to the root node of the subtree
+        :param root : pointer to the root node of the subtree
+
+        :type root: Node
         """
         
         if(root is None):
             return
         
         if(len(root.utility) > 1):
             root.utility = [sum(root.utility)]
@@ -592,24 +630,30 @@
         for child in root.children:
             self.fixUtility(root.children[child])
 
     def createConditionalTree(self, root, transactions, minUtil):
         """
         Creates the conditional tree for the given prefix tree
 
+        :param root: pointer to the root node of the prefix tree
 
-        :param root : _Node
-            pointer to the root node of the prefix tree
-        :param transactions : list
-            list of transactions in prefix tree
-        :param minUtil : int
-            minimum utility threshold
+        :type root: Node
+
+        :param transactions: list of transactions in prefix tree
 
-        :return: tempTree : _HUSTree
-                conditional tree for the given prefix tree
+        :type transactions: list
+
+        :param minUtil: minimum utility threshold
+
+        :type minUtil: int
+
+
+        :return tempTree: conditional tree for the given prefix tree
+
+        :rtype: _HUSTree
         """
         
         for transaction in transactions:
             for item in transaction["transaction"]:
                 if(root.headerTable.table[item][0] < minUtil):
                     transaction["transaction"].remove(item)
 
@@ -622,37 +666,48 @@
 
         return tempTree
     
     def contains(self, superset, subset):
         """
         Checks if the superset contains the subset
 
-        :param superset : list
-            list of items in the superset
-        :param subset : list
-            list of items in the subset
+        :param superset: list of items in the superset
+
+        :type superset: list
+
+        :param subset: list of items in the subset
+
+        :type subset: list
 
-        :return: bool :
-                    True if superset contains subset, False otherwise
+        :return bool: True if the superset contains the subset, False otherwise
+
+        :rtype: bool
         """
      
         return reduce(and_, [i in superset for i in subset])
 
     def treeGenerations(self, root, netUtil, candidatePattern, curItem = []):
         """
         Generates the tree of the high utility patterns
 
-        :param root : _Node
-            pointer to the root of the tree
-        :param netUtil : int
-            Net utility of the transaction
-        :param candidatePattern : list
-            Candidate patterns generated with utility
-        :param curItem : list
-            List of items in the current itemsets
+        :param root: pointer to the root of the tree
+
+        :type root: _Node
+
+        :param netUtil: Net utility of the transaction
+
+        :type netUtil: int
+
+        :param candidatePattern: Candidate patterns generated with utility
+
+        :type candidatePattern: list
+
+        :param curItem: list of items in the current itemsets
+
+        :type curItem: list
         """
 
         if(root is None):
             return
 
 
         for item in reversed(root.headerTable.orderedItems):
@@ -688,15 +743,15 @@
 
                 else:
                     candidatePattern[len(newItemset)].append(newItemset)
 
                 if(len(conditionalTree.headerTable.table) != 0):
                     self.treeGenerations(conditionalTree, netUtil, candidatePattern, newItemset)
 
-
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         This function will start the mining process
         """
         global _minUtil
         self.__startTime = _hus._time.time()
         if self._iFile is None:
@@ -765,20 +820,102 @@
         self.__endTime = _hus._time.time()
         self.__memoryUSS = float()
         self.__memoryRSS = float()
         process = _hus._psutil.Process(_hus._os.getpid())
         self.__memoryUSS = process.memory_full_info().uss
         self.__memoryRSS = process.memory_info().rss
 
+    def mine(self):
+        """
+        This function will start the mining process
+        """
+        global _minUtil
+        self.__startTime = _hus._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minUtil is None:
+            raise Exception("Please enter the Minimum Support")
+        if self._windowSize is None:
+            raise Exception("Please enter the Window Size")
+        if self._paneSize is None:
+            raise Exception("Please enter the Pane Size")
+        self.__windowSize = int(self._windowSize)
+        self.__paneSize = int(self._paneSize)
+
+        self._createItemsets()
+        self._minUtil = float(self._minUtil)
+        self.__tree = _HUSTree(self.__windowSize, self.__paneSize)
+
+        transactionwiseUtility = []
+
+        for i in range(len(self._transactions)):
+            curTrans = {}
+            for j in range(len(self._transactions[i])):
+                curTrans[self._transactions[i][j]] = self._utilities[i][j]
+            transactionwiseUtility.append(curTrans)
+
+        for i in range(0, self.__windowSize):
+            self.__tree.batchIndex = i
+            for j in range(0, self.__paneSize):
+                self.__tree.addTransaction(self._transactions[i * self.__paneSize + j],
+                                           self._utilitySum[i * self.__paneSize + j])
+
+        startIndex = 0
+        endIndex = self.__windowSize * self.__paneSize
+
+        while (endIndex <= len(self._transactions)):
+
+            filteredItemsets = {}
+
+            self.treeGenerations(self.__tree, self._minUtil, filteredItemsets)
+
+            results = []
+
+            for itemSetLen in filteredItemsets:
+                for itemSet in filteredItemsets[itemSetLen]:
+                    itemSetUtility = 0
+                    for transId in range(startIndex, endIndex):
+                        if (self.contains(list(transactionwiseUtility[transId].keys()), itemSet)):
+                            for item in itemSet:
+                                itemSetUtility += transactionwiseUtility[transId][item]
+
+                    if (itemSetUtility >= self._minUtil):
+                        results.append([itemSet, itemSetUtility])
+
+            self.__finalPatterns[(startIndex, endIndex)] = results
+
+            if (endIndex >= len(self._transactions)):
+                break
+
+            self.__tree.removeBatch()
+
+            for i in range(0, self.__paneSize):
+                self.__tree.addTransaction(self._transactions[endIndex + i], self._utilitySum[endIndex + i])
+
+            startIndex += self.__paneSize
+            endIndex += self.__paneSize
+
+        self.__endTime = _hus._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _hus._psutil.Process(_hus._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
     def printTree(self, root, level = 0):
         """
         Prints the tree in a readable format.
 
         :param root: CPSTreeNode object for the root of the tree
+
+        :type root: _Node
+
         :param level: Current level of the root node
+
+        :type level: int
         """
 
         print('  ' * level, level, root.itemName, root.utility, root.parent.itemName if root.parent else None )
         
         for child in root.children.values():
             self.printTree(child, level + 1)
 
@@ -864,14 +1001,15 @@
     _ap = str()
     if len(_hus._sys.argv) == 6 or len(_hus._sys.argv) == 7:
         if len(_hus._sys.argv) == 7:
             _ap = HUPMS(_hus._sys.argv[1], _hus._sys.argv[2], _hus._sys.argv[3], _hus._sys.argv[4], _hus._sys.argv[5], _hus._sys.argv[6])
         if len(_hus._sys.argv) == 6:
             _ap = HUPMS(_hus._sys.argv[1], _hus._sys.argv[2], _hus._sys.argv[3], _hus._sys.argv[4], _hus._sys.argv[5])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Windows Processes:", len( _ap.getPatterns()))
 
         _ap.getPatternsAsDataFrame().to_csv("result.csv", index = False, sep='\t')
         _ap.save()
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
```

### Comparing `pami-2024.3.9.2/PAMI/highUtilityPatternsInStreams/SHUGrowth.py` & `pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/SHUGrowth.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,26 +1,36 @@
 #  Copyright (C)  2021 Rage Uday Kiran
 #
-#      This program is free software: you can redistribute it and/or modify
-#      it under the terms of the GNU General Public License as published by
-#      the Free Software Foundation, either version 3 of the License, or
-#      (at your option) any later version.
-#
-#      This program is distributed in the hope that it will be useful,
-#      but WITHOUT ANY WARRANTY; without even the implied warranty of
-#      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-#      GNU General Public License for more details.
+#             This program is free software: you can redistribute it and/or modify
+#
+#             it under the terms of the GNU General Public License as published by
+#
+#             the Free Software Foundation, either version 3 of the License, or
+#
+#             (at your option) any later version.
+#
+#             This program is distributed in the hope that it will be useful,
+#
+#             but WITHOUT ANY WARRANTY; without even the implied warranty of
+#
+#             MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+#
+#             GNU General Public License for more details.
+#
+#             You should have received a copy of the GNU General Public License
+#
+#             along with this program.  If not, see <https://www.gnu.org/licenses/>.
 #
-#      You should have received a copy of the GNU General Public License
-#      along with this program.  If not, see <https://www.gnu.org/licenses/>.
+
 
 import abstract as _hus
 import pandas as pd
 from functools import reduce
-from operator import and_ 
+from operator import and_
+from deprecated import deprecated
 
 _minSup = str()
 _hus._sys.setrecursionlimit(20000)
 
 
 class _Node:
     """
@@ -74,28 +84,32 @@
         self.parent = None
         self.tail = None
 
     def addUtility(self, utility, batchIndex):
         """
         Adds utility to the node
 
-        :param utility : int
-            Net utility value to be added
-        :param batchIndex : int
-            index of the batch with respect to window to which the node belongs
+        :param utility : Net utility value to be added
+
+        :type utility : int
+
+        :param batchIndex : index of the batch with respect to window to which the node belongs
+
+        :type batchIndex : int
         """
 
         self.utility[batchIndex] += utility
 
     def removeUtility(self, utility):
         """
         Removes utility from the node
 
-        :param utility : int
-            Net utility value to be removed
+        :param utility: Net utility value to be removed
+
+        :type utility : int
         """
 
         self.utility -= utility
 
     def shiftUtility(self):
         """
         Shifts the utility values of the node to the left
@@ -120,15 +134,15 @@
     :Attributes:
 
         table : dict
             dictionary of items as keys and list of utility and pointer to the node in the tree as values
             representing the header table
 
         orderedItems : list
-            list of items in the header table in lexicographical order
+            List of items in the header table in lexicographical order
 
     :Methods:
 
         updateUtility(item, utility, node)
             Updates the utility of the item with node pointer in the header table
 
         addUtility(item, utility)
@@ -145,21 +159,25 @@
         self.table = dict()
         self.orderedItems = list()
 
     def updateUtility(self, item, utility, node):
         """
         Updates the utility of the item in the header table
 
-        :param item : str
-            name of the item to which utility needs to be updated
+        :param item: name of the item to which utility is to be updated
+
+        :type item: str
+
+        :param utility: Net utility of the item to be updated
+
+        :type utility: int
 
-        :param utility : int
-                Net utility of the item to be updated
-        :param node : _Node
-                pointer to the node in the tree to which the item belongs
+        :param node: pointer to the node in the tree to which the item belongs
+
+        :type node: _Node
         """
 
         if item in self.table:
             self.table[item][0] += utility
             tempNode = self.table[item][1]
 
             while tempNode.next is not None:
@@ -172,30 +190,36 @@
     
         self.itemOrdering()
 
     def addUtility(self, item, utility):
         """
         Adds utility to the item in the header table
 
-        :param item : str
-            name of the item to which utility needs to be added
-        :param utility : int
-            Net utility of the item to be added
+        :param item: name of the item to which utility needs to be added
+
+        :type item:str
+
+        :param utility: Net utility of the item to be added
+
+        :type utility: int
         """
         self.table[item][0] += utility
     
 
     def removeUtility(self, item, utility):
         """
         Removes utility from the item in the header table
 
-        :param item : str
-            name of the item to which utility needs to be removed
-        :param: utility : int
-            Net utility of the item to be removed
+        :param item: name of the item to which utility needs to be removed
+
+        :type item: str
+
+        :param: utility: Net utility of the item to be removed
+
+        :type utility: int
         """
         self.table[item][0] -= utility
 
         if self.table[item][0] == 0:
             del self.table[item]
 
         self.itemOrdering()
@@ -259,19 +283,21 @@
         self.windowUtility = 0
         self.localTree = localTree
 
     def addTransaction(self, transaction, utility, itemUtility = None):
         """
         Adds transaction to the tree
 
-        :param transaction : list
-            list of items in the transaction
-        :param utility : int
-            Net utility of the transaction
+        :param transaction: list of items in the transaction
+
+        :type transaction: list
+
+        :param utility: Net utility of the transaction
 
+        :type utility: int
         """
         # print("Transaction", transaction, itemUtility, self.localTree)
         transaction.sort(key = lambda x: x[0])
         currentNode = self.root
         self.windowUtility += utility
 
         curUtility = 0
@@ -311,18 +337,21 @@
         currentNode.tail = [False for _ in range(self.batchSize)]
         currentNode.tail[self.batchIndex] = True
 
     def tailUtilities(self, root):
         """
         Calculates the utility of the tail of the node
 
-        :param root : _Node
-            pointer to the node in the tree
-        :return: netUtility : int
-            utility of the tail of the node
+        :param root: pointer to the node in the tree
+
+        :type root: _Node
+
+        :return: utility of the tail of the node
+
+        :rtype: int
         """
 
         if(root is None):
             return 0
 
         if(root.tail is not None):
             return root.utility[0]
@@ -352,17 +381,17 @@
 
 
     def removeBatchUtility(self, tempNode = None):
         
         """
         Removes the utility of the oldest batch from each subtree
 
-        :param tempNode : _Node
-                pointer to the node in the tree
+        :param tempNode: pointer to the node in the tree
 
+        :type tempNode: _Node
         """
         if tempNode is None:
             return
 
         for item in tempNode.children:
             self.removeBatchUtility(tempNode.children[item])
 
@@ -401,14 +430,24 @@
                     It stores the database of the current window in form of SHUTree and adjusts the tree based on upcoming
                     transactions removing the oldest batch. It is an optimized varaint of HUPMS algorithm.
 
     :References:   Chowdhury Farhan Ahmed and Syed Khairuzzaman Tanbeer and Byeong-Soo Jeong and Ho-Jin Choi : High utility
                    pattern mining over data streams with sliding window technique. Expert Systems with Applications Vol 57,
                    214 - 231, 2016. https://doi.org/10.1016/j.eswa.2016.03.001
 
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of High Utility patterns in Streams
+    :param  oFile: str :
+                   Name of the output file to store complete set of High Utility patterns in Streams
+    :param minUtil: int :
+                   Minimum utility threshold
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
 
         __startTime : float
             start time of the mining process
 
         __endTime : float
             end time of the mining process
@@ -493,27 +532,35 @@
         getPatternsAsDataFrame()
             Returns the final patterns generated in each window as a pandas dataframe
 
         getRuntime()
             Returns the runtime of the algorithm
 
         printResults()
-            Prints the statistics of the mining process
+            Prints the stats of the mining process
 
         save()
             Saves the patterns generated from each window in a file
 
 
     ** Executing the code on terminal:**
-    -------------------------------------
-            Format:
-                >>> python3 SHUGrowth.py <inputFile> <outputFile> <minUtil> <windowSize> <paneSize> <separator>
+    ---------------------------------------
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 SHUGrowth.py <inputFile> <outputFile> <minUtil> <windowSize> <paneSize> <separator>
+
+      Example Usage:
+
+      (.venv) $ python3 SHUGrowth.py retail.txt output.txt 107 100 1000 ','
+
+    .. note:: Here minimum utility is 107, Window size is 100 and pane size is 1000. The separator is comma for the input file
 
-            Example:
-                >>> python3 SHUGrowth.py retail.txt output.txt 107 100 1000 ','
 
     **Credits:**
     --------------
     The code is written by Vipul Chhabra under the supervision of Prof. Rage Uday Kiran.
     
     """
 
@@ -581,20 +628,21 @@
                     print("File Not Found")
                     quit()
 
     def minPathUtil(self, nodeIndex, stack):
         """
         Calculates the minimum utility of the path from the root to the ends of the tree
 
-        :param nodeIndex : int
-            index of the node in the stack
-        :param stack : list
-                list of the nodes in the prefix branch
-        :param minUtil : int
-                minimum utility of the path from the root to the ends of the tree
+        :param nodeIndex: index of the node in the stack
+
+        :type nodeIndex: int
+
+        :param stack: list of the nodes in the prefix branch
+
+        :type stack: list
         """
 
         minUtil = 0
         activeBatch = [i for i, e in enumerate(stack[0].utility) if e != 0]
 
         for batch in activeBatch:
             if(stack[nodeIndex + 1].tail == None or stack[nodeIndex + 1].tail[batch] == False):
@@ -603,20 +651,25 @@
         return minUtil
         
 
     def createPrefixBranch(self, root):
         """
         Creates the prefix branch of the node
 
-        :param root : _Node
-                pointer to the root node of the sub-tree
-        :return: stack : list
-                list of the nodes in prefix branch
-        :return: curUtil : int
-                utility of the prefix branch
+        :param root: pointer to the root node of the sub-tree
+
+        :type root: _Node
+
+        :return: list of the nodes in prefix branch
+
+        :rtype: list
+
+        :return: utility of the prefix branch
+
+        :rtype: int
         """
         stack = []
 
         while(root is not None):
             stack.append(root)
             root = root.parent
 
@@ -637,16 +690,17 @@
 
         return stack, lastUtil, otherUtilites
 
     def fixUtility(self, root):
         """
         Fixes the utility of the nodes in the tree by merging the utility values
 
-        :param root : _Node
-                pointer to the root node of the sub-tree
+        :param root: pointer to the root node of the sub-tree
+
+        :type root: _Node
         """
         
         if(root is None):
             return
         
         if(len(root.utility) > 1):
             root.utility = [sum(root.utility)]
@@ -654,22 +708,29 @@
         for child in root.children:
             self.fixUtility(root.children[child])
 
     def createConditionalTree(self, root, transactions, minUtil):
         """
         Creates the conditional tree for the given prefix tree
 
-        :param root : _Node
-                pointer to the root node of the prefix tree
-        :param transactions : list
-                list of transactions in prefix tree
-        :param minUtil : int
-                minimum utility threshold
-        :return: tempTree : _SHUTree
-                conditional tree for the given prefix tree
+        :param root: pointer to the root node of the prefix tree
+
+        :type root: _Node
+
+        :param transactions: list of transactions in prefix tree
+
+        :type transactions: list
+
+        :param minUtil: minimum utility threshold
+
+        :type minUtil: int
+
+        :return: tempTree: conditional tree for the given prefix tree
+
+        :rtype: _SHUTree
         """
         
         for transaction in transactions:
             for item in transaction["transaction"]:
                 if(root.headerTable.table[item][0] < minUtil):
                     itemIndex = transaction["transaction"].index(item)
                     transaction["transaction"].remove(item)
@@ -683,36 +744,48 @@
 
         return tempTree
     
     def contains(self, superset, subset):
         """
         Checks if the superset contains the subset
 
-        :param superset : list
-                list of items in the superset
-        :param subset : list
-                list of items in the subset
-        :return: bool
-                True if superset contains subset, False otherwise
+        :param superset: list of items in the superset
+
+        :type superset: list
+
+        :param subset: list of items in the subset
+
+        :type subset: list
+
+        :return: True if superset contains subset, False otherwise
+
+        :rtype: bool
         """
      
         return reduce(and_, [i in superset for i in subset])
 
     def treeGenerations(self, root, netUtil, candidatePattern, curItem = []):
         """
         Generates the tree of the high utility patterns
 
-        :param root : _Node
-            pointer to the root of the tree
-        :param netUtil : int
-            Net utility of the transaction
-        :param candidatePattern : list
-            Candidate patterns generated with utility
-        :param curItem : list
-            List of items in the current itemsets
+        :param root: pointer to the root of the tree
+
+        :type root: _Node
+
+        :param netUtil: Net utility of the transaction
+
+        :type netUtil: int
+
+        :param candidatePattern: Candidate patterns generated with utility
+
+        :type candidatePattern: list
+
+        :param curItem: List of items in the current itemsets
+
+        :type curItem: list
         """
 
         if(root is None):
             return
 
 
         for item in reversed(root.headerTable.orderedItems):
@@ -750,15 +823,15 @@
 
                 else:
                     candidatePattern[len(newItemset)].append(newItemset)
 
                 if(len(conditionalTree.headerTable.table) != 0):
                     self.treeGenerations(conditionalTree, netUtil, candidatePattern, newItemset)
 
-
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         This function will start the mining process
         """
         global _minUtil
         self.__startTime = _hus._time.time()
         if self._iFile is None:
@@ -827,20 +900,104 @@
         self.__endTime = _hus._time.time()
         self.__memoryUSS = float()
         self.__memoryRSS = float()
         process = _hus._psutil.Process(_hus._os.getpid())
         self.__memoryUSS = process.memory_full_info().uss
         self.__memoryRSS = process.memory_info().rss
 
+    def mine(self):
+        """
+        This function will start the mining process
+        """
+        global _minUtil
+        self.__startTime = _hus._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minUtil is None:
+            raise Exception("Please enter the Minimum Support")
+        if self._windowSize is None:
+            raise Exception("Please enter the Window Size")
+        if self._paneSize is None:
+            raise Exception("Please enter the Pane Size")
+        self.__windowSize = int(self._windowSize)
+        self.__paneSize = int(self._paneSize)
+
+        self._createItemsets()
+        self._minUtil = float(self._minUtil)
+        self.__tree = _SHUTree(self.__windowSize, self.__paneSize)
+
+        transactionwiseUtility = []
+
+        for i in range(len(self._transactions)):
+            curTrans = {}
+            for j in range(len(self._transactions[i])):
+                curTrans[self._transactions[i][j]] = self._utilities[i][j]
+            transactionwiseUtility.append(curTrans)
+
+        for i in range(0, self.__windowSize):
+            self.__tree.batchIndex = i
+            for j in range(0, self.__paneSize):
+                self.__tree.addTransaction(self._transactions[i * self.__paneSize + j],
+                                           self._utilitySum[i * self.__paneSize + j],
+                                           self._utilities[i * self.__paneSize + j])
+
+        startIndex = 0
+        endIndex = self.__windowSize * self.__paneSize
+
+        while (endIndex <= len(self._transactions)):
+
+            filteredItemsets = {}
+
+            self.treeGenerations(self.__tree, self._minUtil, filteredItemsets)
+
+            results = []
+
+            for itemSetLen in filteredItemsets:
+                for itemSet in filteredItemsets[itemSetLen]:
+                    itemSetUtility = 0
+                    for transId in range(startIndex, endIndex):
+                        if (self.contains(list(transactionwiseUtility[transId].keys()), itemSet)):
+                            for item in itemSet:
+                                itemSetUtility += transactionwiseUtility[transId][item]
+
+                    if (itemSetUtility >= self._minUtil):
+                        results.append([itemSet, itemSetUtility])
+
+            self.__finalPatterns[(startIndex, endIndex)] = results
+
+            if (endIndex >= len(self._transactions)):
+                break
+
+            self.__tree.removeBatch()
+
+            for i in range(0, self.__paneSize):
+                self.__tree.addTransaction(self._transactions[endIndex + i], self._utilitySum[endIndex + i],
+                                           self._utilities[endIndex + i])
+
+            startIndex += self.__paneSize
+            endIndex += self.__paneSize
+
+        self.__endTime = _hus._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _hus._psutil.Process(_hus._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
     def printTree(self, root, level = 0):
         """
         Prints the tree in a readable format.
 
         :param root: CPSTreeNode object for the root of the tree
+
+        :type root: CPSTreeNode
+
         :param level: Current level of the root node
+
+        :type level: int
         """
 
         print('  ' * level, level, root.itemName, root.utility, root.parent.itemName if root.parent else None )
         
         if(root.tail is not None):
             print('  ' * (level + 1), level + 1, root.tail)
 
@@ -929,14 +1086,15 @@
     _ap = str()
     if len(_hus._sys.argv) == 6 or len(_hus._sys.argv) == 7:
         if len(_hus._sys.argv) == 7:
             _ap = SHUGrowth(_hus._sys.argv[1], _hus._sys.argv[2], _hus._sys.argv[3], _hus._sys.argv[4], _hus._sys.argv[5], _hus._sys.argv[6])
         if len(_hus._sys.argv) == 6:
             _ap = SHUGrowth(_hus._sys.argv[1], _hus._sys.argv[2], _hus._sys.argv[3], _hus._sys.argv[4], _hus._sys.argv[5])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Windows Processes:", len( _ap.getPatterns()))
         _ap.getPatternsAsDataFrame().to_csv("result.csv", index = False, sep='\t')
         _ap.save()
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
```

### Comparing `pami-2024.3.9.2/PAMI/highUtilityPatternsInStreams/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/__init__.py` & `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py` & `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py`

 * *Files 12% similar despite different names*

```diff
@@ -2,41 +2,45 @@
 # mining with many real-world applications. It involves finding all spatially interesting itemSets having high value
 # in a quantitative spatio-temporal database.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.highUtilitySpatialPattern.basic import HDSHUIM as alg
+#             from PAMI.highUtilitySpatialPattern.basic import HDSHUIM as alg
 #
-#     obj=alg.HDSHUIM("input.txt","Neighbours.txt",35)
+#             obj=alg.HDSHUIM("input.txt","Neighbours.txt",35)
 #
-#     obj.startMine()
+#             obj.mine()
 #
-#     Patterns = obj.getPatterns()
+#             Patterns = obj.getPatterns()
 #
-#     print("Total number of Spatial High-Utility Patterns:", len(Patterns))
+#             print("Total number of Spatial High-Utility Patterns:", len(Patterns))
 #
-#     obj.save("output")
+#             obj.save("output")
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
 #
 
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -48,14 +52,15 @@
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.highUtilitySpatialPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 class _Element:
     """
     A class represents an Element of a utility list as used by the HDSHUIM algorithm.
 
     :Attributes:
 
@@ -118,14 +123,15 @@
 
     def addElements(self, element: _Element) -> None:
         """
         A method to add new element to CUList
 
         :param element: element to be added to CUList
         :type element: Element
+        :return: None
         """
         self.sumSnu += element.snu
         self.sumRemainingUtility += element.remainingUtility
         self.elements.append(element)
 
 
 class _Pair:
@@ -141,20 +147,37 @@
 class HDSHUIM(_ab._utilityPatterns):
     """
     :Description:
 
         Spatial High Utility ItemSet Mining (SHUIM) [3] is an important model in data
         mining with many real-world applications. It involves finding all spatially interesting itemSets having high value 
         in a quantitative spatio temporal database.
+
     :Reference:
 
         P. Pallikila et al., "Discovering Top-k Spatial High Utility Itemsets in Very Large Quantitative Spatiotemporal 
         databases," 2021 IEEE International Conference on Big Data (Big Data), Orlando, FL, USA, 2021, pp. 4925-4935,
         doi: 10.1109/BigData52589.2021.9671912.
 
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of High Utility Spatial patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of High Utility Spatial patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param minUtil: int :
+                   Minimum utility threshold given by User
+    :param nFile: str :
+                   Name of the input file to mine complete set of High Utility Spatial patterns
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
 
         iFile : str
             Name of the input file to mine complete set of frequent patterns
         oFile : str
             Name of the output file to store complete set of frequent patterns
         nFile: str
@@ -203,29 +226,37 @@
                A method to save itemSets
             updateElement(z, compactUList, st, exCul, newT, ex, duPrevPos, eyTs)
                A method to updates vales for duplicates
 
 
     **Executing the code on terminal:**
     -------------------------------------
-            Format:
-                    >>> python3 HDSHUIM.py <inputFile> <outputFile> <Neighbours> <minUtil> <separator>
-            Examples:
-                    >>> python3 HDSHUIM.py sampleTDB.txt output.txt sampleN.txt 35 ','
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 HDSHUIM.py <inputFile> <outputFile> <Neighbours> <minUtil> <separator>
+
+      Example Usage:
+
+      (.venv) $ python3 HDSHUIM.py sampleTDB.txt output.txt sampleN.txt 35 ','
+
+    .. note:: minSup will be considered in percentage of database transactions
 
 
     **Sample run of importing the code:**
     ---------------------------------------
     .. code-block:: python
         
             from PAMI.highUtilityGeoreferencedFrequentPattern.basic import HDSHUIM as alg
 
             obj=alg.HDSHUIM("input.txt","Neighbours.txt",35)
 
-            obj.startMine()
+            obj.mine()
 
             Patterns = obj.getPatterns()
 
             print("Total number of Spatial High-Utility Patterns:", len(Patterns))
 
             obj.save("output")
 
@@ -269,22 +300,35 @@
         self._mapOfPMU = {}
         self._mapFMAP = {}
         self._neighbors = {}
         self._finalPatterns = {}
 
     def _compareItems(self, o1: Any, o2: Any) -> int:
         """
-        A method to sort  list of huis in pmu ascending order
+        A Function that sort all FFI-list in ascending order of Support
+
+        :param o1: First FFI-list
+
+        :type o1: _FFList
+
+        :param o2: Second FFI-list
+
+        :type o1: _FFList
+
+        :return: Comparision Value
+
+        :rtype: int
         """
         compare = self._mapOfPMU[o1.item] - self._mapOfPMU[o2.item]
         if compare == 0:
             return int(o1.item) - int(o2.item)
         else:
             return compare
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         main program to start the operation
         """
         minUtil = self._minUtil
         self._startTime = _ab._time.time()
         with open(self._nFile, 'r') as file1:
@@ -397,28 +441,146 @@
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def _ExploreSearchTree(self, prefix: List[str], uList: List[_CUList], exNeighbours: set, minUtil: int) -> None:
+    def mine(self) -> None:
         """
-        A method to find all high utility itemSets
+        main program to start the operation
+        """
+        minUtil = self._minUtil
+        self._startTime = _ab._time.time()
+        with open(self._nFile, 'r') as file1:
+            for line in file1:
+                line = line.split("\n")[0]
+                parts = line.split(self._sep)
+                parts = [i.strip() for i in parts]
+                item = parts[0]
+                neigh1 = list()
+                for i in range(1, len(parts)):
+                    neigh1.append(parts[i])
+                self._neighbors[item] = set(neigh1)
+        with open(self._iFile, 'r') as file:
+            for line in file:
+                parts = line.split(":")
+                itemString = (parts[0].split("\n")[0]).split(self._sep)
+                utilityString = (parts[2].split("\n")[0]).split(self._sep)
+                transUtility = int(parts[1])
+                trans1 = set()
+                for i in range(0, len(itemString)):
+                    trans1.add(itemString[i])
+                for i in range(0, len(itemString)):
+                    item = itemString[i]
+                    twu = self._mapOfPMU.get(item)
+                    if twu is None:
+                        twu = int(utilityString[i])
+                    else:
+                        twu += int(utilityString[i])
+                    self._mapOfPMU[item] = twu
+                    if self._neighbors.get(item) is None:
+                        continue
+                    neighbours2 = trans1.intersection(self._neighbors.get(item))
+                    for item2 in neighbours2:
+                        if self._mapOfPMU.get(item2) is None:
+                            self._mapOfPMU[item2] = int(utilityString[i])
+                        else:
+                            self._mapOfPMU[item2] += int(utilityString[i])
 
-        :Attributes:
+        listOfCUList = []
+        hashTable = {}
+        mapItemsToCUList = {}
+        for item in self._mapOfPMU.keys():
+            if self._mapOfPMU.get(item) >= minUtil:
+                uList = _CUList(item)
+                mapItemsToCUList[item] = uList
+                listOfCUList.append(uList)
+        listOfCUList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        ts = 1
+        with open(self._iFile, 'r') as file:
+            for line in file:
+                parts = line.split(":")
+                items = (parts[0].split("\n")[0]).split(self._sep)
+                utilities = (parts[2].split("\n")[0]).split(self._sep)
+                ru = 0
+                newTwu = 0
+                txKey = []
+                revisedTrans = []
+                for i in range(0, len(items)):
+                    pair = _Pair()
+                    pair.item = items[i]
+                    pair.utility = int(utilities[i])
+                    if self._mapOfPMU.get(pair.item) >= minUtil:
+                        revisedTrans.append(pair)
+                        txKey.append(pair.item)
+                        newTwu += pair.utility
+                revisedTrans.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+                txKey1 = tuple(txKey)
+                if len(revisedTrans) > 0:
+                    if txKey1 not in hashTable.keys():
+                        hashTable[txKey1] = len(mapItemsToCUList[revisedTrans[len(revisedTrans) - 1].item].elements)
+                        for i in range(len(revisedTrans) - 1, -1, -1):
+                            pair = revisedTrans[i]
+                            cuListOfItems = mapItemsToCUList.get(pair.item)
+                            element = _Element(ts, pair.utility, ru, 0, 0)
+                            if i > 0:
+                                element.prevPos = len(mapItemsToCUList[revisedTrans[i - 1].item].elements)
+                            else:
+                                element.prevPos = -1
+                            cuListOfItems.addElements(element)
+                            ru += pair.utility
+                    else:
+                        pos = hashTable[txKey1]
+                        ru = 0
+                        for i in range(len(revisedTrans) - 1, -1, -1):
+                            cuListOfItems = mapItemsToCUList[revisedTrans[i].item]
+                            cuListOfItems.elements[pos].snu += revisedTrans[i].utility
+                            cuListOfItems.elements[pos].remainingUtility += ru
+                            cuListOfItems.sumSnu += revisedTrans[i].utility
+                            cuListOfItems.sumRemainingUtility += ru
+                            ru += revisedTrans[i].utility
+                            pos = cuListOfItems.elements[pos].prevPos
+                # EUCS
+                for i in range(len(revisedTrans) - 1, -1, -1):
+                    pair = revisedTrans[i]
+                    mapFMAPItem = self._mapFMAP.get(pair.item)
+                    if mapFMAPItem is None:
+                        mapFMAPItem = {}
+                        self._mapFMAP[pair.item] = mapFMAPItem
+                    for j in range(i + 1, len(revisedTrans)):
+                        pairAfter = revisedTrans[j]
+                        twuSUm = mapFMAPItem.get(pairAfter.item)
+                        if twuSUm is None:
+                            mapFMAPItem[pairAfter.item] = newTwu
+                        else:
+                            mapFMAPItem[pairAfter.item] = twuSUm + newTwu
+                ts += 1
+        exNeighbours = set(self._mapOfPMU.keys())
+        # print(self.Neighbours)
+        self._ExploreSearchTree([], listOfCUList, exNeighbours, minUtil)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
+    def _ExploreSearchTree(self, prefix: List[str], uList: List[_CUList], exNeighbours: set, minUtil: int) -> None:
+        """
+        A method to find all high utility itemSets
         :parm prefix: it represents all items in prefix
         :type prefix :list
         :parm uList:projected Utility list.
         :type uList: list
         :parm exNeighbours: keep track of common Neighbours
         :type exNeighbours: set
         :parm minUtil:user minUtil
         :type minUtil:int
+        :return: None
         """
         for i in range(0, len(uList)):
             x = uList[i]
             if x.item not in exNeighbours:
                 continue
             self._candidates += 1
             sortedPrefix = [0] * (len(prefix) + 1)
@@ -438,17 +600,14 @@
                     if exULs is None or set1 is None:
                         continue
                     self._ExploreSearchTree(sortedPrefix, exULs, set1, minUtil)
 
     def _constructCUL(self, x: _Element, compactUList: List[_CUList], st: int, minUtil: int, length: int, exNeighbours: set) -> List[_CUList]:
         """
         A method to construct CUL's database
-
-        :Attributes:
-
         :parm x: Compact utility list
         :type x: Node
         :parm compactUList:list of Compact utility lists.
         :type compactUList:list
         :parm st: starting pos of compactUList
         :type st:int
         :parm minUtil: user minUtil
@@ -536,47 +695,42 @@
                     exCul[j].sumCpu += x.sumCu
                 filter_compactUList.append(exCul[j])
         return filter_compactUList
 
     def _updateClosed(self, x: _Element, compactUList: List[_CUList], st: int, exCul: List[_CUList], newT: List[int], ex: _Element, eyTs: List[int], length: int) -> None:
         """
         A method to update closed values
-
-        :Attributes:
-
         :parm x: Compact utility list.
         :type x: list
         :parm compactUList:list of Compact utility lists.
         :type compactUList:list
         :parm st: starting pos of compactUList
         :type st:int
         :parm newT:transaction to be updated
         :type newT:list
         :parm ex: element ex
         :type ex:element
         :parm eyTs:list of tss
         :type eyTs:ts
         :parm length: length of x
         :type length:int
+        :return: None
         """
         remainingUtility = 0
         for j in range(len(newT) - 1, -1, -1):
             ey = compactUList[newT[j]]
             eyy = ey.elements[eyTs[newT[j]]]
             exCul[newT[j]].sumCu += ex.snu + eyy.snu - ex.pu
             exCul[newT[j]].sumCru += remainingUtility
             exCul[newT[j]].sumCpu += ex.snu
             remainingUtility = remainingUtility + eyy.snu - ex.pu
 
     def _updateElement(self, z: _Element, compactUList: List[_CUList], st: int, exCul: List[_CUList], newT: List[int], ex: _Element, duPrevPos: int, eyTs: List[int]) -> None:
         """
         A method to updates vales for duplicates
-
-        :Attributes:
-
         :parm z: Compact utility list
         :type z: list
         :parm compactUList:list of Compact utility lists
         :type compactUList:list
         :parm st: starting pos of compactUList
         :type st:int
         :parm exCul:list of compactUList
@@ -585,14 +739,15 @@
         :type newT:list
         :parm ex: element ex
         :type ex:element
         :parm duPrevPos: position of z in exCul
         :type duPrevPos:int
         :parm eyTs:list of tss
         :type eyTs:ts
+        :return: None
         """
         remainingUtility = 0
         pos = duPrevPos
         for j in range(len(newT) - 1, -1, -1):
             ey = compactUList[newT[j]]
             eyy = ey.elements[eyTs[newT[j]]]
             exCul[newT[j]].elements[pos].snu += ex.snu + eyy.snu - ex.pu
@@ -602,94 +757,99 @@
             exCul[newT[j]].elements[pos].pu += ex.snu
             remainingUtility = remainingUtility + eyy.snu - ex.pu
             pos = exCul[newT[j]].elements[pos].prevPos
 
     def _saveItemSet(self, prefix: List[str], prefixLen: int, item: str, utility: int) -> None:
         """
          A method to save itemSets
-
-         :Attributes:
-
         :parm prefix: it represents all items in prefix
         :type prefix :list
         :parm item:item
         :type item: int
         :parm utility:utility of itemSet
         :type utility:int
+        :return: None
         """
         self._huiCount += 1
         res = str()
         for i in range(0, prefixLen):
             res += str(prefix[i]) + "\t"
         res += str(item)
         res1 = str(utility)
         self._finalPatterns[res] = res1
 
     def getPatternsAsDataFrame(self) -> Dict[str, str]:
-        """Storing final frequent patterns in a dataframe
+        """
+        Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
     def getPatterns(self) -> Dict[str, str]:
-        """ Function to send the set of frequent patterns after completion of the mining process
+        """
+        Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def save(self, outFile: str) -> None:
-        """Complete set of frequent patterns will be loaded in to an output file
+        """
+        Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
     def getMemoryUSS(self) -> float:
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
-       """
+        """
         return self._memoryRSS
 
     def getRuntime(self) -> float:
-        """Calculating the total amount of runtime taken by the mining process
-
+        """
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
-       """
+        """
         return self._endTime - self._startTime
 
     def printResults(self) -> None:
-        """ This function is used to print the results
+        """
+        This function is used to print the results
         """
         print("Total number of Spatial High Utility Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
@@ -697,22 +857,24 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:  # to  include a user specified separator
             _ap = HDSHUIM(_ab._sys.argv[1], _ab._sys.argv[3], int(_ab._sys.argv[4]), _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:  # to consider "\t" as a separator
             _ap = HDSHUIM(_ab._sys.argv[1], _ab._sys.argv[3], int(_ab._sys.argv[4]))
         _ap.startMine()
+        _ap.mine()
         print("Total number of Spatial High-Utility Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         for i in [100000, 500000]:
             _ap = HDSHUIM('/Users/Likhitha/Downloads/mushroom_main_2000.txt',
                     '/Users/Likhitha/Downloads/mushroom_neighbors_2000.txt', i, ' ')
             _ap.startMine()
+            _ap.mine()
             print("Total number of Spatial High Utility Patterns:", len(_ap.getPatterns()))
             print("Total Memory in USS:", _ap.getMemoryUSS())
             print("Total Memory in RSS", _ap.getMemoryRSS())
             print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/basic/SHUIM.py` & `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/SHUIM.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,41 +1,44 @@
 # Spatial High Utility itemSet Mining (SHUIM) aims to discover all itemSets in a spatioTemporal database
 # that satisfy the user-specified minimum utility and maximum distance constraints
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     from PAMI.highUtilitySpatialPattern.basic import SHUIM as alg
+#             from PAMI.highUtilitySpatialPattern.basic import SHUIM as alg
 #
-#     obj=alg.SHUIM("input.txt","Neighbours.txt",35)
+#             obj=alg.SHUIM("input.txt","Neighbours.txt",35)
 #
-#     obj.startMine()
+#             obj.mine()
 #
-#     frequentPatterns = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
-#     print("Total number of Spatial high utility Patterns:", len(frequentPatterns))
+#             print("Total number of Spatial high utility Patterns:", len(frequentPatterns))
 #
-#     obj.save("output")
+#             obj.save("output")
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
-#
-#     print("Total ExecutionTime in seconds:", run)
+#             run = obj.getRuntime()
 #
+#             print("Total ExecutionTime in seconds:", run)
 #
 
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -49,14 +52,15 @@
 
 """
 
 from PAMI.highUtilitySpatialPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator, Optional, TypeVar
 from functools import cmp_to_key as _cmpToKey
 import pandas as pd
+from deprecated import deprecated
 
 
 class _Transaction:
     """
     A class to store Transaction of a database
 
     :Attributes:
@@ -101,15 +105,14 @@
         if pmus is not None:
             self.pmus = pmus
 
     def projectTransaction(self, offsetE: int) -> Self:
         """
         A method to create new Transaction from existing till offsetE
 
-
         :param offsetE: an offset over the original transaction for projecting the transaction
         :type offsetE: int
         """
         new_transaction = _Transaction(self.items, self.utilities, self.transactionUtility)
         utilityE = self.utilities[offsetE]
         new_transaction.prefixUtility = self.prefixUtility + utilityE
         new_transaction.transactionUtility = self.transactionUtility - utilityE
@@ -123,36 +126,45 @@
         A method to return items in transaction
         """
         return self.items
 
     def getPmus(self) -> List[int]:
         """
         A method to return pmus in transaction
+        :return: pmus in transaction
+        :rtype: list
         """
         return self.pmus
 
     def getUtilities(self) -> List[int]:
         """
         A method to return utilities in transaction
+        :return: utilities in transaction
+        :rtype: list
         """
         return self.utilities
 
     # get the last position in this transaction
     def getLastPosition(self) -> int:
         """
         A method to return last position in a transaction
+        :return: last position in a transaction
+        :rtype: int
         """
         return len(self.items) - 1
 
     def removeUnpromisingItems(self, oldNamesToNewNames: Dict[int, int]) -> None:
         """
         A method to remove items with low Utility than minUtil
 
         :param oldNamesToNewNames: A map represent old names to new names
+
         :type oldNamesToNewNames: map
+
+        :return: None
         """
         tempItems = []
         tempUtilities = []
         for idx, item in enumerate(self.items):
             if item in oldNamesToNewNames:
                 tempItems.append(oldNamesToNewNames[item])
                 tempUtilities.append(self.utilities[idx])
@@ -161,14 +173,15 @@
         self.items = tempItems
         self.utilities = tempUtilities
         self.insertionSort()
 
     def insertionSort(self) -> None:
         """
         A method to sort items in order
+        :return: None
         """
         for i in range(1, len(self.items)):
             key = self.items[i]
             utilityJ = self.utilities[i]
             j = i - 1
             while j >= 0 and key < self.items[j]:
                 self.items[j + 1] = self.items[j]
@@ -213,16 +226,14 @@
             for line in lines:
                 self.transactions.append(self.createTransaction(line))
         f.close()
 
     def createTransaction(self, line: str) -> _Transaction:
         """
         A method to create Transaction from dataset given
-            
-        :Attributes:
 
         :param line: represent a single line of database
         :type line: string
         :return : Transaction.
         :rtype: Transaction
         """
         trans_list = line.strip().split(':')
@@ -245,20 +256,28 @@
             utilities.append(int(utilityString[idx]))
             pmus.append(int(pmuString[idx]))
         return _Transaction(items, utilities, transactionUtility, pmus)
 
     def getMaxItem(self) -> int:
         """
         A method to return name of the largest item
+
+        :return: the largest item in the list
+
+        :rtype: int
         """
         return self.maxItem
 
     def getTransactions(self) -> List[_Transaction]:
         """
         A method to return transactions from database
+
+        :return: the list of transactions stored in the database
+
+        :rtype: list
         """
         return self.transactions
 
 
 class SHUIM(_ab._utilityPatterns):
     """
     :Description:
@@ -268,14 +287,35 @@
 
     :Reference:
 
         Rage, Uday & Veena, Pamalla & Penugonda, Ravikumar & Raj, Bathala & Dao, Minh & Zettsu, Koji & Bommisetti, Sai. 
         (2023). HDSHUI-miner: a novel algorithm for discovering spatial high-utility itemsets in high-dimensional 
         spatiotemporal databases. Applied Intelligence. 53. 1-26. 10.1007/s10489-022-04436-w.
 
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of High Utility Spatial patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of High Utility Spatial patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param minUtil: int :
+                   Minimum utility threshold given by User
+    :param maxMemory: int :
+                   Maximum memory used by this program for running
+    :param candidateCount: int :
+                   Number of candidates to consider when calculating a high utility spatial pattern
+    :param nFile: str :
+                   Name of the input file to mine complete set of High Utility Spatial patterns
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
+
     :Attributes:
 
         iFile : file
             Name of the input file to mine complete set of frequent patterns
         nFile : file
             Name of the Neighbours file that contain neighbours of items
         oFile : file
@@ -344,30 +384,38 @@
               A Method to sort transaction in the order of PMU
         sort_transaction(self, trans1, trans2)
               A Method to sort transaction in the order of PMU
         useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset)
              A method to scan the database using utility bin array to calculate the pmus                   
 
     **Executing the code on terminal:**
-    -------------------------------------
-            Format:
-                >>> python3 SHUIM.py <inputFile> <outputFile> <Neighbours> <minUtil> <sep>
+    ---------------------------------------
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 SHUIM.py <inputFile> <outputFile> <Neighbours> <minUtil> <sep>
+
+      Example Usage:
+
+      (.venv) $ python3 SHUIM.py sampleTDB.txt output.txt sampleN.txt 35
+
+    .. note:: minSup will be considered in percentage of database transactions
 
-            Examples:
-             >>> python3 SHUIM.py sampleTDB.txt output.txt sampleN.txt 35
 
     **Sample run of importing the code:**
     --------------------------------------
     .. code-block:: python
          
             from PAMI.highUtilitySpatialPattern.basic import SHUIM as alg
 
             obj=alg.SHUIM("input.txt","Neighbours.txt",35)
 
-            obj.startMine()
+            obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Spatial high utility Patterns:", len(frequentPatterns))
 
             obj.save("output")
 
@@ -410,14 +458,15 @@
     _minUtil = 0
     _memoryUSS = float()
     _memoryRSS = float()
     
     def __init__(self, iFile: str, nFile: str, minUtil: int, sep: str="\t") -> None:
         super().__init__(iFile, nFile, minUtil, sep)
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         main program to start the operation
         """
         self._startTime = _ab._time.time()
         self._patternCount = 0
         self._finalPatterns = {}
@@ -472,28 +521,89 @@
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
+    def mine(self) -> None:
+        """
+        main program to start the operation
+        """
+        self._startTime = _ab._time.time()
+        self._patternCount = 0
+        self._finalPatterns = {}
+        self._dataset = _Dataset(self._iFile, self._sep)
+        with open(self._nFile, 'r') as o:
+            lines = o.readlines()
+            for line in lines:
+                line = line.split("\n")[0]
+                line_split = line.split(self._sep)
+                line_split = [i.strip() for i in line_split]
+                item = self._dataset.strToInt.get(line_split[0])
+                lst = []
+                for i in range(1, len(line_split)):
+                    lst.append(self._dataset.strToInt.get(line_split[i]))
+                self._Neighbours[item] = lst
+        o.close()
+        #print(len(self._Neighbours))
+        InitialMemory = _ab._psutil.virtual_memory()[3]
+        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
+        itemsToKeep = []
+        for key in self._utilityBinArrayLU.keys():
+            if self._utilityBinArrayLU[key] >= self._minUtil:
+                itemsToKeep.append(key)
+        itemsToKeep = sorted(itemsToKeep, key=lambda x: self._utilityBinArrayLU[x])
+        currentName = 1
+        for idx, item in enumerate(itemsToKeep):
+            self._oldNamesToNewNames[item] = currentName
+            self._newNamesToOldNames[currentName] = item
+            itemsToKeep[idx] = currentName
+            currentName += 1
+        for transaction in self._dataset.getTransactions():
+            transaction.removeUnpromisingItems(self._oldNamesToNewNames)
+        self._sortDatabase(self._dataset.getTransactions())
+        emptyTransactionCount = 0
+        for transaction in self._dataset.getTransactions():
+            if len(transaction.getItems()) == 0:
+                emptyTransactionCount += 1
+        self._dataset.transactions = self._dataset.transactions[emptyTransactionCount:]
+        self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
+        itemsToExplore = []
+        for item in itemsToKeep:
+            if self._utilityBinArraySU[item] >= self._minUtil:
+                itemsToExplore.append(item)
+        commonitems = []
+        for i in range(self._dataset.maxItem):
+            commonitems.append(i)
+        self._backtrackingEFIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
+        finalMemory = _ab._psutil.virtual_memory()[3]
+        memory = (finalMemory - InitialMemory) / 10000
+        if memory > self._maxMemory:
+            self._maxMemory = memory
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+
     def _backtrackingEFIM(self, transactionsOfP: List[_Transaction], itemsToKeep: List[int], itemsToExplore: List[int], prefixLength: int) -> None:
         """
         A method to mine the SHUIs Recursively
 
-        :Attributes:
-
         :param transactionsOfP: the list of transactions containing the current prefix P
         :type transactionsOfP: list
         :param itemsToKeep: the list of secondary items in the p-projected database
         :type itemsToKeep: list
         :param itemsToExplore: the list of primary items in the p-projected database
         :type itemsToExplore: list
         :param prefixLength: current prefixLength
         :type prefixLength: int
+        :return: None
         """
         self._candidateCount += len(itemsToExplore)
         for idx, e in enumerate(itemsToExplore):
             initialMemory = _ab._psutil.virtual_memory()[3]
             transactionsPe = []
             utilityPe = 0
             previousTransaction = transactionsOfP[0]
@@ -565,23 +675,21 @@
             if self._maxMemory < memory:
                 self._maxMemory = memory
 
     def _useUtilityBinArraysToCalculateUpperBounds(self, transactionsPe: List[_Transaction], j: int, itemsToKeep: List[int], neighbourhoodList: List[int]) -> None:
         """
         A method to  calculate the subtree utility and local utility of all items that can extend itemSet P U {e}
 
-        :Attributes:
-
         :param transactionsPe: transactions the projected database for P U {e}
         :type transactionsPe: list
         :param j:the position of j in the list of promising items
         :type j:int
         :param itemsToKeep :the list of promising items
         :type itemsToKeep: list
-
+        :return: None
         """
         for i in range(j + 1, len(itemsToKeep)):
             item = itemsToKeep[i]
             self._utilityBinArrayLU[item] = 0
             self._utilityBinArraySU[item] = 0
         for transaction in transactionsPe:
             length = len(transaction.getItems())
@@ -603,55 +711,60 @@
                 i -= 1
 
     def _calculateNeighbourIntersection(self, prefixLength: int) -> List[int]:
         """
         A method to find common Neighbours
 
         :param prefixLength: the prefix itemSet
+
         :type prefixLength:int
 
+        :return: the common neighbours
+
+        :rtype: list
         """
         intersectionList = self._Neighbours.get(self._temp[0])
         for i in range(1, prefixLength+1):
             intersectionList = self._intersection(self._Neighbours[self._temp[i]], intersectionList)
         finalIntersectionList = []
         if intersectionList is None:
             return finalIntersectionList
         for item in intersectionList:
             if item in self._oldNamesToNewNames:
                 finalIntersectionList.append(self._oldNamesToNewNames[item])
         return finalIntersectionList
     
     def _output(self, tempPosition: int, utility: int) -> None:
         """
-         A method save all high-utility itemSet to file or memory depending on what the user chose
+        A method save all high-utility itemSet to file or memory depending on what the user chose
 
-         :param tempPosition: position of last item
-         :type tempPosition : int 
-         :param utility: total utility of itemSet
-         :type utility: int
+        :param tempPosition: position of last item
+        :type tempPosition : int
+        :param utility: total utility of itemSet
+        :type utility: int
+        :return: None
         """
         self._patternCount += 1
         s1 = str()
         for i in range(0, tempPosition+1):
             s1 += self._dataset.intToStr.get((self._temp[i]))
             if i != tempPosition:
                 s1 += "\t"
         self._finalPatterns[s1] = str(utility)
 
     def _isEqual(self, transaction1: _Transaction, transaction2: _Transaction) -> bool:
         """
-         A method to Check if two transaction are identical
+        A method to Check if two transaction are identical
 
-         :param  transaction1: the first transaction.
-         :type  transaction1: Transaction
-         :param  transaction2:   the second transaction.
-         :type  transaction2: Transaction
-         :return : whether both are identical or not
-         :rtype: bool
+        :param  transaction1: the first transaction.
+        :type  transaction1: Transaction
+        :param  transaction2:   the second transaction.
+        :type  transaction2: Transaction
+        :return : whether both are identical or not
+        :rtype: bool
         """
 
         length1 = len(transaction1.items) - transaction1.offset
         length2 = len(transaction2.items) - transaction2.offset
         if length1 != length2:
             return False
         position1 = transaction1.offset
@@ -679,15 +792,18 @@
         return lst3
 
     def _useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self, dataset: _Dataset) -> None:
         """
         Scan the initial database to calculate the subtree utility of each item using a utility-bin array
 
         :param dataset: the transaction database
+
         :type dataset: Dataset
+
+        :return: None
         """
         for transaction in dataset.getTransactions():
             items = transaction.getItems()
             utilities = transaction.getUtilities()
             for idx, item in enumerate(items):
                 if item not in self._utilityBinArraySU:
                     self._utilityBinArraySU[item] = 0
@@ -719,15 +835,15 @@
         A Method to sort transaction in the order of PMU
 
         :param trans1: the first transaction.
         :type trans1: Transaction
         :param trans2:the second transaction.
         :type trans2: Transaction
         :return: sorted transaction.
-        :rtype:    Transaction
+        :rtype: int
         """
         trans1_items = trans1.getItems()
         trans2_items = trans2.getItems()
         pos1 = len(trans1_items) - 1
         pos2 = len(trans2_items) - 1
         if len(trans1_items) < len(trans2_items):
             while pos1 >= 0:
@@ -755,86 +871,97 @@
             return 0
 
     def _useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset: _Dataset) -> None:
         """
         A method to scan the database using utility bin array to calculate the pmus
 
         :param dataset: the transaction database.
+
         :type dataset: database
 
+        :return: None
         """
         for transaction in dataset.getTransactions():
             for idx, item in enumerate(transaction.getItems()):
                 if item in self._utilityBinArrayLU:
                     self._utilityBinArrayLU[item] += transaction.getPmus()[idx]
                 else:
                     self._utilityBinArrayLU[item] = transaction.getPmus()[idx]
 
     def getPatternsAsDataFrame(self) -> pd.DataFrame:
-        """Storing final patterns in a dataframe
+        """
+        Storing final patterns in a dataframe
 
         :return: returning patterns in a dataframe
         :rtype: pd.DataFrame
         """
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Utility'])
 
         return dataFrame
     
     def getPatterns(self) -> Dict[str, str]:
-        """ Function to send the set of patterns after completion of the mining process
+        """
+        Function to send the set of patterns after completion of the mining process
 
         :return: returning patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def save(self, outFile: str) -> None:
-        """Complete set of patterns will be loaded in to an output file
+        """
+        Complete set of patterns will be loaded in to an output file
 
         :param outFile: name of the output file
+
         :type outFile: csv file
+
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
     def getMemoryUSS(self) -> float:
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
-       """
+        """
         return self._memoryRSS
 
     def getRuntime(self) -> float:
-        """Calculating the total amount of runtime taken by the mining process
-
+        """
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
-       """
+        """
         return self._endTime-self._startTime
 
     def printResults(self) -> None:
-        """ This function is used to print the results
+        """
+        This function is used to print the results
         """
         print("Total number of Spatial High Utility Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
@@ -842,22 +969,24 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = SHUIM(_ab._sys.argv[1], _ab._sys.argv[3], int(_ab._sys.argv[4]), _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = SHUIM(_ab._sys.argv[1], _ab._sys.argv[3], int(_ab._sys.argv[4]))
         _ap.startMine()
+        _ap.mine()
         print("Total number of Spatial High Utility Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
         for i in [100000, 500000]:
             _ap = SHUIM('/Users/Likhitha/Downloads/mushroom_main_2000.txt', '/Users/Likhitha/Downloads/mushroom_neighbors_2000.txt', i, ' ')
             _ap.startMine()
+            _ap.mine()
             print("Total number of Spatial High Utility Patterns:", len(_ap.getPatterns()))
             #_ap.save(_ab._sys.argv[2])
             print("Total Memory in USS:", _ap.getMemoryUSS())
             print("Total Memory in RSS", _ap.getMemoryRSS())
             print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py` & `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 # --------------------------------------------------------
 #
 #
 #             from PAMI.highUtilitySpatialPattern.topk import TKSHUIM as alg
 #
 #             obj=alg.TKSHUIM("input.txt","Neighbours.txt",35)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             Patterns = obj.getPatterns()
 #
 #             print("Total number of  Patterns:", len(Patterns))
 #
 #             obj.save("output")
 #
@@ -26,20 +26,22 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
+
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -52,14 +54,15 @@
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.highUtilitySpatialPattern.topk.abstract import *
 from functools import cmp_to_key
 import heapq
+from deprecated import deprecated
 
 class Transaction:
     """
     A class to store Transaction of a database
 
     :Attributes:
 
@@ -161,15 +164,15 @@
                 self.transactionUtility -= self.utilities[idx]
         self.items = tempItems
         self.utilities = tempUtilities
         self.insertionSort()
 
     def insertionSort(self):
         """
-            A method to sort items in order
+        A method to sort items in order
         """
         for i in range(1, len(self.items)):
             key = self.items[i]
             utilityJ = self.utilities[i]
             j = i - 1
             while j >= 0 and key < self.items[j]:
                 self.items[j + 1] = self.items[j]
@@ -213,21 +216,19 @@
             for line in lines:
                 self.transactions.append(self.createTransaction(line))
         f.close()
 
     def createTransaction(self, line):
         """
         A method to create Transaction from dataset given
-            
-        :Attributes:
 
         :param line: represent a single line of database
         :type line: string
         :return : Transaction.
-        :rtype: Transaction
+        :rtype: int
         """
         trans_list = line.strip().split(':')
         transactionUtility = int(trans_list[1])
         itemsString = trans_list[0].strip().split(self.sep)
         utilityString = trans_list[2].strip().split(self.sep)
         if (len(trans_list) == 4):
             pmuString = trans_list[3].strip().split(self.sep)
@@ -270,22 +271,29 @@
     :Reference:
 
        P. Pallikila et al., "Discovering Top-k Spatial High Utility Itemsets in Very Large Quantitative Spatiotemporal 
        databases," 2021 IEEE International Conference on Big Data (Big Data), Orlando, FL, USA, 2021, pp. 4925-4935, 
        doi: 10.1109/BigData52589.2021.9671912.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
+                   Name of the Input file to mine complete set of High Utility Spatial patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  k: int :
-                    User specified count of top frequent patterns
+                   Name of the output file to store complete set of High Utility Spatial patterns
+    :param minUtil: int :
+                   Minimum utility threshold given by User
+    :param maxMemory: int :
+                   Maximum memory used by this program for running
+    :param candidateCount: int :
+                   Number of candidates to consider when calculating a high utility spatial pattern
+    :param nFile: str :
+                   Name of the input file to mine complete set of High Utility Spatial patterns
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
+
     :Attributes:
 
         iFile : file
             Name of the input file to mine complete set of frequent patterns
         nFile : file
             Name of the Neighbours file that contain neighbours of items
         oFile : file
@@ -351,29 +359,38 @@
               A Method to sort transaction in the order of PMU
         sort_transaction(self, trans1, trans2)
               A Method to sort transaction in the order of PMU
         useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset)
              A method to scan the database using utility bin array to calculate the pmus                   
 
     **Executing the code on terminal:**
-    ------------------------------------
-            Format:
-                    >>> python3 TKSHUIM.py <inputFile> <outputFile> <Neighbours> <k> <sep>
-            Examples:
-                    >>> python3 TKSHUIM.py sampleTDB.txt output.txt sampleN.txt 35
+    -------------------------------------
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 TKSHUIM.py <inputFile> <outputFile> <Neighbours> <k> <sep>
+
+      Example Usage:
+
+      (.venv) $ python3 TKSHUIM.py sampleTDB.txt output.txt sampleN.txt 35
+
+    .. note:: maxMemory will be considered as Maximum memory used by this program for running
+
 
     **Sample run of importing the code:**
     ----------------------------------------
     .. code-block:: python
         
             from PAMI.highUtilitySpatialPattern.topk import TKSHUIM as alg
 
             obj=alg.TKSHUIM("input.txt","Neighbours.txt",35)
 
-            obj.startMine()
+            obj.mine()
 
             Patterns = obj.getPatterns()
 
             obj.save("output")
 
             memUSS = obj.getMemoryUSS()
 
@@ -412,14 +429,15 @@
     memoryUSS = float()
     memoryRSS = float()
     heapList = []
 
     def __init__(self, iFile, nFile, k, sep="\t"):
         super().__init__(iFile, nFile, k, sep)
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Main function of the program.
         """
         self.startTime = time.time()
         self.finalPatterns = {}
         self.dataset = Dataset(self.iFile, self.sep)
@@ -475,14 +493,77 @@
         self.memoryRSS = float()
         self.memoryUSS = process.memory_full_info().uss
         self.memoryRSS = process.memory_info().rss
         for item in self.heapList:
             self.finalPatterns[item[1]] = item[0]
         print('TOP-K mining process is completed by TKSHUIM')
 
+    def mine(self):
+        """
+        Main function of the program.
+        """
+        self.startTime = time.time()
+        self.finalPatterns = {}
+        self.dataset = Dataset(self.iFile, self.sep)
+        with open(self.nFile, 'r') as o:
+            lines = o.readlines()
+            for line in lines:
+                line = line.split("\n")[0]
+                line_split = line.split(self.sep)
+                item = self.dataset.strToint.get(line_split[0])
+                lst = []
+                for i in range(1, len(line_split)):
+                    lst.append(self.dataset.strToint.get(line_split[i]))
+                self.Neighbours[item] = lst
+        o.close()
+        InitialMemory = psutil.virtual_memory()[3]
+        self.useUtilityBinArrayToCalculateLocalUtilityFirstTime(self.dataset)
+        itemsToKeep = []
+        for key in self.utilityBinArrayLU.keys():
+            if self.utilityBinArrayLU[key] >= self.minUtil:
+                itemsToKeep.append(key)
+        itemsToKeep = sorted(itemsToKeep, key=lambda x: self.utilityBinArrayLU[x])
+        currentName = 1
+        for idx, item in enumerate(itemsToKeep):
+            self.oldNamesToNewNames[item] = currentName
+            self.newNamesToOldNames[currentName] = item
+            itemsToKeep[idx] = currentName
+            currentName += 1
+        for transaction in self.dataset.getTransactions():
+            transaction.removeUnpromisingItems(self.oldNamesToNewNames)
+        self.sortDatabase(self.dataset.getTransactions())
+        emptyTransactionCount = 0
+        for transaction in self.dataset.getTransactions():
+            if len(transaction.getItems()) == 0:
+                emptyTransactionCount += 1
+        self.dataset.transactions = self.dataset.transactions[emptyTransactionCount:]
+        self.useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self.dataset)
+        self.heapList = []
+        itemsToExplore = []
+        for item in itemsToKeep:
+            if self.utilityBinArraySU[item] >= self.minUtil:
+                itemsToExplore.append(item)
+        commonitems = []
+        for i in range(self.dataset.maxItem):
+            commonitems.append(i)
+        self.backtrackingEFIM(self.dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
+        finalMemory = psutil.virtual_memory()[3]
+        memory = (finalMemory - InitialMemory) / 10000
+        if memory > self.maxMemory:
+            self.maxMemory = memory
+        self.endTime = time.time()
+        process = psutil.Process(os.getpid())
+        self.memoryUSS = float()
+        self.memoryRSS = float()
+        self.memoryUSS = process.memory_full_info().uss
+        self.memoryRSS = process.memory_info().rss
+        for item in self.heapList:
+            self.finalPatterns[item[1]] = item[0]
+        print('TOP-K mining process is completed by TKSHUIM')
+
     def backtrackingEFIM(self, transactionsOfP, itemsToKeep, itemsToExplore, prefixLength):
         """
         A method to mine the TKSHUIs Recursively
 
         :param transactionsOfP: the list of transactions containing the current prefix P
         :type transactionsOfP: list
         :param itemsToKeep: the list of secondary items in the p-projected database
@@ -568,22 +649,22 @@
             if self.maxMemory < memory:
                 self.maxMemory = memory
 
     def useUtilityBinArraysToCalculateUpperBounds(self, transactionsPe, j, itemsToKeep, neighbourhoodList):
         """
         A method to  calculate the sub-tree utility and local utility of all items that can extend itemSet P U {e}
 
-
         :param transactionsPe: transactions the projected database for P U {e}
         :type transactionsPe: list
         :param j:the position of j in the list of promising items
         :type j:int
         :param itemsToKeep :the list of promising items
         :type itemsToKeep: list
-
+        :param neighbourhoodList: list of neighbourhood elements
+        :type neighbourhoodList: list
         """
         for i in range(j + 1, len(itemsToKeep)):
             item = itemsToKeep[i]
             self.utilityBinArrayLU[item] = 0
             self.utilityBinArraySU[item] = 0
         for transaction in transactionsPe:
             length = len(transaction.getItems())
@@ -606,53 +687,52 @@
 
     def calculateNeighbourIntersection(self, prefixLength):
         """
         A method to find common Neighbours
 
         :param prefixLength: the prefix itemSet
         :type prefixLength:int
-
         """
         intersectionList = self.Neighbours.get(self.temp[0])
         for i in range(1, prefixLength+1):
             intersectionList = self.intersection(self.Neighbours[self.temp[i]], intersectionList)
         finalIntersectionList = []
         if intersectionList is None:
             return finalIntersectionList
         for item in intersectionList:
             if item in self.oldNamesToNewNames:
                 finalIntersectionList.append(self.oldNamesToNewNames[item])
         return finalIntersectionList
     
     def output(self, tempPosition, utility):
         """
-         A method save all high-utility itemSet to file or memory depending on what the user chose
+        A method save all high-utility itemSet to file or memory depending on what the user chose
 
-         :param tempPosition: position of last item
-         :type tempPosition : int 
-         :param utility: total utility of itemSet
-         :type utility: int
+        :param tempPosition: position of last item
+        :type tempPosition : int
+        :param utility: total utility of itemSet
+        :type utility: int
         """
         s1 = str()
         for i in range(0, tempPosition+1):
             s1 += self.dataset.intTostr.get((self.temp[i]))
             if i != tempPosition:
                 s1 += "\t"
         self.additemset(s1, utility)
 
     def is_equal(self, transaction1, transaction2):
         """
-         A method to Check if two transaction are identical
+        A method to Check if two transaction are identical
 
-         :param  transaction1: the first transaction.
-         :type  transaction1: Transaction
-         :param  transaction2:   the second transaction.
-         :type  transaction2: Transaction
-         :return : whether both are identical or not
-         :rtype: bool
+        :param  transaction1: the first transaction.
+        :type  transaction1: Transaction
+        :param  transaction2:   the second transaction.
+        :type  transaction2: Transaction
+        :return : whether both are identical or not
+        :rtype: bool
         """
 
         length1 = len(transaction1.items) - transaction1.offset
         length2 = len(transaction2.items) - transaction2.offset
         if length1 != length2:
             return False
         position1 = transaction1.offset
@@ -720,15 +800,15 @@
         A Method to sort transaction in the order of PMU
 
         :param trans1: the first transaction.
         :type trans1: Transaction
         :param trans2:the second transaction.
         :type trans2: Transaction
         :return: sorted transaction.
-        :rtype:    Transaction
+        :rtype: int
         """
         trans1_items = trans1.getItems()
         trans2_items = trans2.getItems()
         pos1 = len(trans1_items) - 1
         pos2 = len(trans2_items) - 1
         if len(trans1_items) < len(trans2_items):
             while pos1 >= 0:
@@ -757,15 +837,14 @@
 
     def useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset):
         """
         A method to scan the database using utility bin array to calculate the pmus
 
         :param dataset: the transaction database.
         :type dataset: database
-
         """
         utilityMatrix = defaultdict(lambda: defaultdict(int))
         for transaction in dataset.getTransactions():
             for idx, item in enumerate(transaction.getItems()):
                 pmu = transaction.getUtilities()[idx]
                 if item in self.Neighbours:
                     neighbors = self.Neighbours[item]
@@ -796,118 +875,136 @@
                         else:
                             itemset = str(item) + str(itemj)
                         self.additemset(itemset, val)
 
     def additemset(self, itemset, utility):
         """
         adds the itemset to the priority queue
+
+        :param itemset: the itemset to be added
+
+        :type itemset: str
+
+        :param utility: utility matrix for the itemset to be added
+
+        :type utility: numpy.array
         """
         heapq.heappush(self.heapList, (utility, itemset))
         if len(self.heapList) > self.k:
             while len(self.heapList) > self.k:
                 heapq.heappop(self.heapList)
                 if len(self.heapList) == 0:
                     break
             self.minUtil = heapq.nsmallest(1, self.heapList)[0][0]
 
     def getPatternsAsDataFrame(self):
-        """Storing final patterns in a dataframe
+        """
+        Storing final patterns in a dataframe
 
         :return: returning patterns in a dataframe
         :rtype: pd.DataFrame
         """
         dataFrame = {}
         data = []
         for a, b in self.finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = pd.DataFrame(data, columns=['Patterns', 'Utility'])
 
         return dataFrame
     
     def getPatterns(self):
-        """ Function to send the set of patterns after completion of the mining process
+        """
+        Function to send the set of patterns after completion of the mining process
 
         :return: returning patterns
         :rtype: dict
         """
         return self.finalPatterns
 
     def save(self, outFile):
-        """Complete set of patterns will be loaded in to an output file
+        """
+        Complete set of patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self.finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
     def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self.memoryUSS
 
     def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
-       """
+        """
         return self.memoryRSS
 
     def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
-
+        """
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
-       """
+        """
         return self.endTime-self.startTime
 
     def printResults(self):
-        """ This function is used to print the results
+        """
+        This function is used to print the results
         """
         print("Top K Spatial  High Utility Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
 
 def main():
     inputFile = 'mushroom_utility_spmf.txt'
     neighborFile = 'mushroom_neighbourhood.txt' #Users can also specify this constraint between 0 to 1.
     k = 1000
     seperator = ' ' 
     obj = TKSHUIM(iFile=inputFile, nFile=neighborFile, k=k,  sep=seperator)    #initialize
-    obj.startMine()   
+    obj.startMine()
+    obj.mine()
     obj.printResults()
     print(obj.getPatterns())
 
 if __name__ == '__main__':
     main()
     # _ap = str()
     # if len(sys.argv) == 5 or len(sys.argv) == 6:
     #     if len(sys.argv) == 6:
     #         _ap = TKSHUIM(sys.argv[1], sys.argv[3], int(sys.argv[4]), sys.argv[5])
     #     if len(sys.argv) == 5:
     #         _ap = TKSHUIM(sys.argv[1], sys.argv[3], int(sys.argv[4]))
     #     _ap.startMine()
+    #     _ap.mine()
     #     print("Top K Spatial  High Utility Patterns:", len(_ap.getPatterns()))
     #     _ap.save(sys.argv[2])
     #     print("Total Memory in USS:", _ap.getMemoryUSS())
     #     print("Total Memory in RSS",  _ap.getMemoryRSS())
     #     print("Total ExecutionTime in seconds:", _ap.getRuntime())
     # else:
     #     for i in [1000, 5000]:
     #         _ap = TKSHUIM('/Users/Likhitha/Downloads/mushroom_main_2000.txt',
     #                 '/Users/Likhitha/Downloads/mushroom_neighbors_2000.txt', i, ' ')
     #         _ap.startMine()
+    #         _ap.mine()
     #         print("Total number of Spatial High Utility Patterns:", len(_ap.getPatterns()))
     #         print("Total Memory in USS:", _ap.getMemoryUSS())
     #         print("Total Memory in RSS", _ap.getMemoryRSS())
     #         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     #     print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

### Comparing `pami-2024.3.9.2/PAMI/highUtilitySpatialPattern/topk/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/localPeriodicPattern/basic/LPPGrowth.py` & `pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/localPeriodicPattern/basic/LPPMBreadth.py` & `pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPMBreadth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/localPeriodicPattern/basic/LPPMDepth.py` & `pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPMDepth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/localPeriodicPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py` & `pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,38 +1,40 @@
 # CFPGrowth is a basic code of the fundamental algorithm to discover frequent patterns based on multiple minimum support in a transactional database.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.multipleMinimumSupportBasedFrequentPattern.basic import basic as alg
+#             from PAMI.multipleMinimumSupportBasedFrequentPattern.basic import basic as alg
 #
-#     obj = alg.basic(iFile, mIS)
+#             obj = alg.basic(iFile, mIS)
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     frequentPatterns = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#     obj.save(oFile)
+#             obj.save(oFile)
 #
-#     Df = obj.getPatternInDataFrame()
+#             Df = obj.getPatternInDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#            print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#            run = obj.getRuntime()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#            print("Total ExecutionTime in seconds:", run)
+#
+
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
@@ -48,16 +50,17 @@
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 
 from PAMI.multipleMinimumSupportBasedFrequentPattern.basic import abstract as _fp
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from typing import List, Dict, Tuple, Generator
 import pandas as pd
+from deprecated import deprecated
 
 _fp._sys.setrecursionlimit(20000)
 _MIS = {}
 
 class _Node:
     """
         A class used to represent the node of frequentPatternTree
@@ -133,14 +136,15 @@
         """
         adding transaction into tree
 
         :param transaction: it represents the one transaction in database
         :type transaction: list
         :param count: frequency of item
         :type count: int
+        :return: None
         """
 
         # This method takes transaction as input and returns the tree
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
@@ -238,14 +242,25 @@
 class CFPGrowth(_fp._frequentPatterns):
     """
     :Description:   basic is one of the fundamental algorithm to discover frequent patterns based on multiple minimum support in a transactional database.
 
     :Reference:   Ya-Han Hu and Yen-Liang Chen. 2006. Mining association rules with multiple minimum supports: a new mining algorithm and a support tuning mechanism.
                   Decis. Support Syst. 42, 1 (October 2006), 1–24. https://doi.org/10.1016/j.dss.2004.09.007
 
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Uncertain Minimum Support Based Frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Uncertain Minimum Support Based Frequent patterns
+    :param  minSup: str:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
 
         iFile : file
             Input file name or path of the input file
         MIS: file or dictionary
             Multiple minimum supports of all items in the database
         sep : str
@@ -291,19 +306,27 @@
         creatingItemSets()
             Scans the dataset or dataframes and stores in list format
         frequentOneItem()
             Extracts the one-frequent patterns from transactions
 
     **Executing the code on terminal:**
     -------------------------------------
-        Format:
-            >>> python3 CFPGrowth.py <inputFile> <outputFile>
+    .. code-block:: console
+
+
+       Format:
+
+      (.venv) $ python3 CFPGrowth.py <inputFile> <outputFile>
+
+      Examples:
+
+      (.venv) $  python3 CFPGrowth.py sampleDB.txt patterns.txt MISFile.txt
+
 
-        Examples:
-            >>> python3 CFPGrowth.py sampleDB.txt patterns.txt MISFile.txt
+              .. note:: minSup  will be considered in support count or frequency
 
 
     **Sample run of the importing code:**
     ----------------------------------------
     .. code-block:: python
 
             from PAMI.multipleMinimumSupportBasedFrequentPattern.basic import basic as alg
@@ -356,14 +379,15 @@
 
     def __init__(self, iFile, MIS, sep='\t') -> None:
         super().__init__(iFile, MIS, sep)
 
     def __creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self.__Database = []
         if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
@@ -391,14 +415,15 @@
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _getMISValues(self) -> None:
         """
         Storing the Minimum supports given by the user for each item in the database
+        :reurtn: None
         """
         self._MISValues = {}
         if isinstance(self._MIS, _fp._pd.DataFrame):
             items, MIS = [], []
             if self._MIS.empty:
                 print("its empty..")
             i = self._MIS.columns.values.tolist()
@@ -514,17 +539,52 @@
         :return: patterns with original item names.
         """
         temp = str()
         for i in itemSet:
             temp = temp + self.__rankDup[i] + "\t"
         return temp
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         main program to start the operation
+        :return: none
+
+        """
+        global _MIS
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self.__creatingItemSets()
+        self._getMISValues()
+        #MIS = self._MISValues
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        for x, y in self.__rank.items():
+            _MIS[y] = self._MISValues[x]
+            self.__rankDup[y] = x
+        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Frequent patterns were generated successfully using basic algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
+    def Mine(self) -> None:
+        """
+        main program to start the operation
+        :return: none
 
         """
         global _MIS
         self.__startTime = _fp._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         self.__creatingItemSets()
@@ -594,14 +654,15 @@
         return dataframe
 
     def save(self, outFile: str) -> None:
         """Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self.__finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
@@ -612,14 +673,15 @@
         :rtype: dict
         """
         return self.__finalPatterns
 
     def printResults(self) -> None:
         """
         this function is used to print the results
+        :return: None
         """
         print("Total number of  Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
```

### Comparing `pami-2024.3.9.2/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPMC.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,601 +1,499 @@
-#  Copyright (C)  2021 Rage Uday Kiran
+# PFPMC is the fundamental approach to mine the periodic-frequent patterns.
 #
-#      This program is free software: you can redistribute it and/or modify
-#      it under the terms of the GNU General Public License as published by
-#      the Free Software Foundation, either version 3 of the License, or
-#      (at your option) any later version.
+# **Importing this algorithm into a python program**
+# --------------------------------------------------------
 #
-#      This program is distributed in the hope that it will be useful,
-#      but WITHOUT ANY WARRANTY; without even the implied warranty of
-#      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-#      GNU General Public License for more details.
 #
-#      You should have received a copy of the GNU General Public License
-#      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-
-from PAMI.multipleMinimumSupportBasedFrequentPattern.basic import abstract as _fp
-
-_fp._sys.setrecursionlimit(20000)
-MIS = {}
-
-class _Node:
-    """
-    A class used to represent the node of frequentPatternTree
-
-    :Attributes:
-
-        itemId: int
-            storing item of a node
-        counter: int
-            To maintain the support of node
-        parent: node
-            To maintain the parent of node
-        children: list
-            To maintain the children of node
-
-    :Methods:
-
-        addChild(node)
-            Updates the nodes children list and parent for the given node
-
-    """
-
-    def __init__(self, item, children):
-        self.itemId = item
-        self.counter = 1
-        self.parent = None
-        self.children = children
-
-    def addChild(self, node):
-        """
-        Retrieving the child from the tree
-
-        :param node: Children node.
-        :type node: Node
-        :return: Updates the children nodes and parent nodes
-        """
-        self.children[node.itemId] = node
-        node.parent = self
-
-
-class _Tree:
-    """
-    A class used to represent the frequentPatternGrowth tree structure
-
-    :Attributes:
-
-        root : Node
-            The first node of the tree set to Null.
-        summaries : dictionary
-            Stores the nodes itemId which shares same itemId
-        info : dictionary
-            frequency of items in the transactions
-
-    :Methods:
+#             from PAMI.periodicFrequentPattern.basic import PFPMC as alg
+#
+#             obj = alg.PFPMC("../basic/sampleTDB.txt", "2", "5")
+#
+#             obj.startMine()
+#
+#             periodicFrequentPatterns = obj.getPatterns()
+#
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#
+#             obj.save("patterns")
+#
+#             Df = obj.getPatternsAsDataFrame()
+#
+#             memUSS = obj.getMemoryUSS()
+#
+#             print("Total Memory in USS:", memUSS)
+#
+#             memRSS = obj.getMemoryRSS()
+#
+#             print("Total Memory in RSS", memRSS)
+#
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
+#
 
-        addTransaction(transaction, freq)
-            adding items of  transactions into the tree as nodes and freq is the count of nodes
-        getFinalConditionalPatterns(node)
-            getting the conditional patterns from fp-tree for a node
-        getConditionalPatterns(patterns, frequencies)
-            sort the patterns by removing the items with lower minSup
-        generatePatterns(prefix)
-            generating the patterns from fp-tree
-    """
 
-    def __init__(self):
-        self.root = _Node(None, {})
-        self.summaries = {}
-        self.info = {}
-
-    def addTransaction(self, transaction, count):
-        """
-        adding transaction into tree
-
-        :param transaction: it represents the one transactions in database
-        :type transaction: list
-        :param count: frequency of item
-        :type count: int
-        """
-
-        # This method takes transaction as input and returns the tree
-        currentNode = self.root
-        for i in range(len(transaction)):
-            if transaction[i] not in currentNode.children:
-                newNode = _Node(transaction[i], {})
-                newNode.freq = count
-                currentNode.addChild(newNode)
-                if transaction[i] in self.summaries:
-                    self.summaries[transaction[i]].append(newNode)
-                else:
-                    self.summaries[transaction[i]] = [newNode]
-                currentNode = newNode
-            else:
-                currentNode = currentNode.children[transaction[i]]
-                currentNode.freq += count
 
-    def getFinalConditionalPatterns(self, alpha, support):
-        """
-        generates the conditional patterns for a node
+__copyright__ = """
+ Copyright (C)  2021 Rage Uday Kiran
 
-        :param alpha: node to generate conditional patterns
-        :return: returns conditional patterns, frequency of each item in conditional patterns
-
-        """
-        finalPatterns = []
-        finalFreq = []
-        for i in self.summaries[alpha]:
-            set1 = i.freq
-            set2 = []
-            while i.parent.itemId is not None:
-                set2.append(i.parent.itemId)
-                i = i.parent
-            if len(set2) > 0:
-                set2.reverse()
-                finalPatterns.append(set2)
-                finalFreq.append(set1)
-        finalPatterns, finalFreq, info = self.getConditionalTransactions(finalPatterns, finalFreq, support)
-        return finalPatterns, finalFreq, info
-
-    @staticmethod
-    def getConditionalTransactions(ConditionalPatterns, conditionalFreq, support):
-        """
-        To calculate the frequency of items in conditional patterns and sorting the patterns
-
-        :param ConditionalPatterns: paths of a node
-        :param conditionalFreq: frequency of each item in the path
-        :return: conditional patterns and frequency of each item in transactions
-        """
-        #global _minSup
-        pat = []
-        freq = []
-        data1 = {}
-        for i in range(len(ConditionalPatterns)):
-            for j in ConditionalPatterns[i]:
-                if j in data1:
-                    data1[j] += conditionalFreq[i]
-                else:
-                    data1[j] = conditionalFreq[i]
-        up_dict = {k: v for k, v in data1.items() if v >= support}
-        #up_dict = data1.copy()
-        count = 0
-        for p in ConditionalPatterns:
-            p1 = [v for v in p if v in up_dict]
-            trans = sorted(p1, key=lambda x: (up_dict.get(x), -x), reverse=True)
-            if len(trans) > 0:
-                pat.append(trans)
-                freq.append(conditionalFreq[count])
-            count += 1
-        return pat, freq, up_dict
-
-    def generatePatterns(self, prefix):
-        """
-        To generate the frequent patterns
-
-        :param prefix: an empty list
-        :return: Frequent patterns that are extracted from fp-tree
-
-        """
-        global minMIS
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x), -x)):
-            pattern = prefix[:]
-            pattern.append(i)
-            if self.info[i] >= minMIS:
-              yield pattern, self.info[i]
-            patterns, freq, info = self.getFinalConditionalPatterns(i, self.info[i])
-            conditionalTree = _Tree()
-            conditionalTree.info = info.copy()
-            for pat in range(len(patterns)):
-                conditionalTree.addTransaction(patterns[pat], freq[pat])
-            if len(patterns) > 0:
-                for q in conditionalTree.generatePatterns(pattern):
-                    yield q
+     This program is free software: you can redistribute it and/or modify
+     it under the terms of the GNU General Public License as published by
+     the Free Software Foundation, either version 3 of the License, or
+     (at your option) any later version.
+
+     This program is distributed in the hope that it will be useful,
+     but WITHOUT ANY WARRANTY; without even the implied warranty of
+     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+     GNU General Public License for more details.
+
+     You should have received a copy of the GNU General Public License
+     along with this program.  If not, see <https://www.gnu.org/licenses/>.
+     Copyright (C)  2021 Rage Uday Kiran
+
+"""
+
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+from itertools import groupby as _groupby
+from operator import itemgetter as _itemgetter
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Tuple, Set, Union, Any, Generator
 
-minMIS = 0
 
-class CFPGrowthPlus(_fp._frequentPatterns):
+class PFPMC(_ab._periodicFrequentPatterns):
     """
+    :Description:   PFPMC is the fundamental approach to mine the periodic-frequent patterns.
 
-    :Description:
-
-    :Reference:   R. Uday Kiran P. Krishna Reddy Novel techniques to reduce search space in multiple minimum supports-based frequent
-                  pattern mining algorithms. 11-20 2011 EDBT https://doi.org/10.1145/1951365.1951370
+    :Reference: (has to be added)
 
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of periodic frequent pattern's
+    :param  oFile: str :
+                   Name of the output file to store complete set of periodic frequent pattern's
+    :param  minSup: str:
+                   Controls the minimum number of transactions in which every item must appear in a database.
+    :param  maxPer: str:
+                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         iFile : file
-            Input file name or path of the input file
-        MIS: file or dictionary
-            Multiple minimum supports of all items in the database
+            Name of the Input file or path of the input file
+        oFile : file
+            Name of the output file or path of the output file
+        minSup : int or float or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        maxPer : int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        oFile : file
-            Name of the output file or the path of the output file
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
         lno : int
             it represents the total no of transactions
         tree : class
             it represents the Tree class
+        itemSetCount : int
+            it represents the total no of patterns
         finalPatterns : dict
             it represents to store the patterns
+        tidList : dict
+            stores the timestamps of an item
+        hashing : dict
+            stores the patterns with their support to check for the closed property
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        savePatterns(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+        save(oFile)
+            Complete set of periodic-frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets()
-            Scans the dataset or dataframes and stores in list format
-        frequentOneItem()
-            Extracts the one-frequent patterns from transactions
-
-    **Executing the code on terminal:**
-    ------------------------------------
-        Format:
-            >>> python3 CFPGrowthPlus.py <inputFile> <outputFile>
+        creatingOneItemSets()
+            Scan the database and store the items with their timestamps which are periodic frequent
+        getPeriodAndSupport()
+            Calculates the support and period for a list of timestamps.
+        Generation()
+            Used to implement prefix class equivalence method to generate the periodic patterns recursively
+
+
+    **Methods to execute code on terminal**
+    ------------------------------------------
+    .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 PFPMC.py <inputFile> <outputFile> <minSup> <maxPer>
 
-        Examples:
-            >>> python3 CFPGrowthPlus.py sampleDB.txt patterns.txt MISFile.txt
+       Example usage:
 
+       (.venv) $ python3 PFPMC.py sampleDB.txt patterns.txt 10.0 4.0
 
-    **Sample run of the importing code:**
-    ----------------------------------------
+
+               .. note:: minSup and maxPer will be considered in percentage of database transactions
+
+    **Importing this algorithm into a python program**
+    ----------------------------------------------------
     .. code-block:: python
 
-            from PAMI.multipleMinimumSupportBasedFrequentPattern.basic import CFPGrowthPlus as alg
+                from PAMI.periodicFrequentPattern.basic import PFPMC as alg
 
-            obj = alg.CFPGrowthPlus(iFile, mIS)
+                obj = alg.PFPMC("../basic/sampleTDB.txt", "2", "5")
 
-            obj.startMine()
+                obj.startMine()
 
-            frequentPatterns = obj.getPatterns()
+                periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+                print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
-            obj.savePatterns(oFile)
+                obj.save("patterns")
 
-            Df = obj.getPatternInDataFrame()
+                Df = obj.getPatternsAsDataFrame()
 
-            memUSS = obj.getMemoryUSS()
+                memUSS = obj.getMemoryUSS()
 
-            print("Total Memory in USS:", memUSS)
+                print("Total Memory in USS:", memUSS)
 
-            memRSS = obj.getMemoryRSS()
+                memRSS = obj.getMemoryRSS()
 
-            print("Total Memory in RSS", memRSS)
+                print("Total Memory in RSS", memRSS)
 
-            run = obj.getRuntime()
+                run = obj.getRuntime()
 
-            print("Total ExecutionTime in seconds:", run)
+                print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    ---------------
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
+    ----------------
+
+    The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
-    __startTime = float()
-    __endTime = float()
-    _MIS = str
-    __finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    __memoryUSS = float()
-    __memoryRSS = float()
-    __Database = []
-    __mapSupport = {}
-    __lno = 0
-    __tree = _Tree()
-    __rank = {}
-    __rankDup = {}
+    _dbSize = None
+    _Database = None
+    _minSup = str()
+    _maxPer = str()
+    _tidSet = set()
+    _finalPatterns = {}
+    _startTime = None
+    _endTime = None
+    _lastTid = int()
+    _memoryUSS = float()
+    _memoryRSS = float()
+
+    def _getPeriodic(self, tids: set) -> int:
+        """
+        To get  Periodic frequent patterns
+
+        :param tids: represents the timestamp of a transaction
+        :type tids: set
+        :return: None
+        """
+        tids = list(tids)
+        tids.sort()
+        temp = self._maxPer + 1
+        diffs = []
+        if self._lastTid in tids:
+            tids.remove(self._lastTid)
+        for k, g in _groupby(enumerate(tids), lambda ix: ix[0] - ix[1]):
+            diffs.append(len(list(map(_itemgetter(1), g))))
+        if len(diffs) < 1:
+            return temp
+        return max(diffs) + 1
+
+    def _getPeriodic(self, tids: set):
+
+        tids = list(tids)
+        tids.sort()
+        temp = self._maxPer + 1
+        if self._lastTid in tids:
+            tids.remove(self._lastTid)
+        diffs = []
+        # find the longest consecutive period
+
+        count = 0
+        for i in range(len(tids) - 1):
+            if tids[i + 1] == tids[i] + 1:
+                count += 1
+            else:
+                diffs.append(count)
+                count = 0
+        if len(diffs) < 1:
+            return temp
+        return max(diffs) + 1
+
+    def _getPeriodic(self, tids: set):
+        tids = list(tids)
+        tids.sort()
+        temp = self._maxPer + 1
+        if self._lastTid in tids:
+            tids.remove(self._lastTid)
+        diffs = []
+        tempPer = 0
+        period = 0
+        for i in range(len(tids) - 1):
+            if tids[i+1] - tids[i] == 1:
+                tempPer += 1
+            else:
+                period = max(period, tempPer + 1)
+                if period > self._maxPer:
+                    return temp
+                tempPer = 0
 
-    def __init__(self, iFile, MIS, sep='\t'):
-        super().__init__(iFile, MIS, sep)
+        return period
 
-    def __creatingItemSets(self):
+    def _convert(self, value) -> float:
         """
-        Storing the complete transactions of the database/input file in a database variable
+        To convert the given user specified value
+
+        :param value: user specified value
+        :return: converted value
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (self._dbSize * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (self._dbSize * value)
+            else:
+                value = int(value)
+        return value
 
+    def _creatingOneItemSets(self) -> list:
         """
-        self.__Database = []
-        if isinstance(self._iFile, _fp._pd.DataFrame):
+        Storing the complete transactions of the database/input file in a database variable
+        :return: list
+        """
+        plist = []
+        Database = []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
+            ts, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self.__Database = self._iFile['Transactions'].tolist()
-
-            # print(self.Database)
+                data = self._iFile['Transactions'].tolist()
+            for i in range(len(data)):
+                tr = [ts[i][0]]
+                tr = tr + data[i]
+                Database.append(tr)
         if isinstance(self._iFile, str):
-            if _fp._validators.url(self._iFile):
-                data = _fp._urlopen(self._iFile)
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
                 for line in data:
-                    line = line.strip()
+                    line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self.__Database.append(temp)
+                    Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line = line.strip()
+                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self.__Database.append(temp)
+                            Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
+        tid = 0
+        itemsets = {}  # {key: item, value: list of tids}
+        periodicHelper = {}  # {key: item, value: [period, last_tid]}
+        for line in Database:
+            tid = int(line[0])
+            self._tidSet.add(tid)
+            for item in line[1:]:
+                if item in itemsets:
+                    itemsets[item].add(tid)
+                else:
+                    itemsets[item] = {tid}
 
-    def _getMISValues(self):
-        """
-        Storing the Minimum supports given by the user for each item in the database
-
-        """
-        self._MISValues = {}
-        if isinstance(self._MIS, _fp._pd.DataFrame):
-            items, MIS = [], []
-            if self._MIS.empty:
-                print("its empty..")
-            i = self._MIS.columns.values.tolist()
-            if 'items' in i:
-                items = self._MIS['items'].tolist()
-            if 'MIS' in i:
-                MIS = self._MIS['MIS'].tolist()
-            for i in range(len(items)):
-                self._MISValues[items[i]] = MIS[i]
-
-        if isinstance(self._MIS, str):
-            if _fp._validators.url(self._MIS):
-                data = _fp._urlopen(self._MIS)
-                for line in data:
-                    line = line.strip()
-                    line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    self._MISValues[temp[0]] = int(temp[1])
-            else:
-                try:
-                    with open(self._MIS, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line = line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self._MISValues[temp[0]] = int(temp[1])
-                except IOError:
-                    print("File Not Found")
-                    quit()
+        self._dbSize = len(Database)
+        self._lastTid = max(self._tidSet)
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        del Database
+        candidates = []
+        for item, tids in itemsets.items():
+            diff = self._tidSet.difference(tids)
+            per = self._getPeriodic(diff)
+            sup = len(tids)
+            if sup >= self._minSup and per <= self._maxPer:
+                candidates.append(item)
+                self._finalPatterns[item] = [sup, per, diff]
+        return candidates
+
+    def _generateDiffsetEclat(self, candidates: list) -> None:
+        new_freqList = []
+        for i in range(0, len(candidates)):
+            item1 = candidates[i]
+            i1_list = item1.split()
+            for j in range(i + 1, len(candidates)):
+                item2 = candidates[j]
+                i2_list = item2.split()
+                if i1_list[:-1] == i2_list[:-1]:
+                    union_DiffSet = self._finalPatterns[item2][2].union(self._finalPatterns[item1][2])
+                    sorted(union_DiffSet)
+                    union_supp = self._dbSize - len(union_DiffSet)
+                    period = self._getPeriodic(union_DiffSet)
+                    if union_supp >= self._minSup and period <= self._maxPer:
+                        newKey = item1 + "\t" + i2_list[-1]
+                        self._finalPatterns[newKey] = [union_supp, period, union_DiffSet]
+                        new_freqList.append(newKey)
+                else:
+                    break
 
-    def __convert(self, value):
-        """
-        to convert the type of user specified minSup value
+        if len(new_freqList) > 0:
+            self._generateDiffsetEclat(new_freqList)
 
-        :param value: user specified minSup value
-        :return: converted type
+    def startMine(self) -> None:
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self.__Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self.__Database) * value)
-            else:
-                value = int(value)
-        return value
+        Mining process will start from this function
+        :return: None
+        """
+        # print(f"Optimized {type(self).__name__}")
+        self._startTime = _ab._time.time()
+        self._finalPatterns = {}
+        frequentSets = self._creatingOneItemSets()
+        self._generateDiffsetEclat(frequentSets)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Periodic-Frequent patterns were generated successfully using PFPDiffset ECLAT algorithm ")
 
-    def __frequentOneItem(self):
-        """
-        Generating One frequent items sets
+    def getMemoryUSS(self) -> float:
         """
-        global minMIS
-        self.__mapSupport = {}
-        for tr in self.__Database:
-            for i in range(1, len(tr)):
-                if tr[i] not in self.__mapSupport:
-                    self.__mapSupport[tr[i]] = 1
-                else:
-                    self.__mapSupport[tr[i]] += 1
-        self.__mapSupport = {k: v for k, v in self.__mapSupport.items() if v >= min(self._MISValues.values())}
-        minMIS = min(self._MISValues.values())
-        genList = [k for k, v in sorted(self.__mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        self.__rank = dict([(index, item) for (item, index) in enumerate(genList)])
-        return genList
-
-    def __updateTransactions(self, itemSet):
-        """
-        Updates the items in transactions with rank of items according to their support
-
-        :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
-                    rank = {'a':0, 'b':1, 'c':2, 'd':3}
-
-        :param itemSet: list of one-frequent items
-
-        """
-        list1 = []
-        for tr in self.__Database:
-            list2 = []
-            for i in range(len(tr)):
-                if tr[i] in itemSet:
-                    list2.append(self.__rank[tr[i]])
-            if len(list2) >= 1:
-                list2.sort()
-                list1.append(list2)
-        return list1
-
-    @staticmethod
-    def __buildTree(transactions, info):
-        """
-        Builds the tree with updated transactions
-
-        :param transactions: updated transactions
-        :param info: support details of each item in transactions.
-        :return: transactions compressed in fp-tree
-        """
-        rootNode = _Tree()
-        rootNode.info = info.copy()
-        for i in range(len(transactions)):
-            rootNode.addTransaction(transactions[i], 1)
-        return rootNode
-
-    def __savePeriodic(self, itemSet):
-        """
-        The duplication items and their ranks
-
-        :param itemSet: frequent itemSet that generated
-        :return: patterns with original item names.
-
-        """
-        temp = str()
-        for i in itemSet:
-            temp = temp + self.__rankDup[i] + " "
-        return temp
-
-    def startMine(self):
-        """
-        main program to start the operation
-
-        """
-        global MIS
-        self.__startTime = _fp._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        self.__creatingItemSets()
-        self._getMISValues()
-        itemSet = self.__frequentOneItem()
-        updatedTransactions = self.__updateTransactions(itemSet)
-        for x, y in self.__rank.items():
-            MIS[y] = self._MISValues[x]
-            self.__rankDup[y] = x
-        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
-        __Tree = self.__buildTree(updatedTransactions, info)
-        patterns = __Tree.generatePatterns([])
-        self.__finalPatterns = {}
-        for k in patterns:
-            s = self.__savePeriodic(k[0])
-            self.__finalPatterns[str(s)] = k[1]
-        print("Frequent patterns were generated successfully using frequentPatternGrowth algorithm")
-        self.__endTime = _fp._time.time()
-        self.__memoryUSS = float()
-        self.__memoryRSS = float()
-        process = _fp._psutil.Process(_fp._os.getpid())
-        self.__memoryUSS = process.memory_full_info().uss
-        self.__memoryRSS = process.memory_info().rss
-
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self.__memoryUSS
+        return self._memoryUSS
 
-    def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+    def getMemoryRSS(self) -> float:
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self.__memoryRSS
-
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+        return self._memoryRSS
 
+    def getRuntime(self) -> float:
+        """
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
-        return self.__endTime - self.__startTime
+        return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+        """
+        Storing final periodic-frequent patterns in a dataframe
 
-        :return: returning frequent patterns in a dataframe
+        :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
-        for a, b in self.__finalPatterns.items():
-            data.append([a, b])
-            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        for a, b in self._finalPatterns.items():
+            data.append([a, b[0], b[1]])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataframe
 
-    def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to a output file
+    def save(self, outFile: str) -> None:
+        """
+        Complete set of periodic-frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
-        for x, y in self.__finalPatterns.items():
-            s1 = x + ":" + str(y)
+        for x, y in self._finalPatterns.items():
+            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
+            #s1 = x.replace(' ', '\t') + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
+    def getPatterns(self) -> dict:
+        """
+        Function to send the set of periodic-frequent patterns after completion of the mining process
 
-        :return: returning frequent patterns
+        :return: returning periodic-frequent patterns
         :rtype: dict
         """
-        return self.__finalPatterns
+        return self._finalPatterns
 
     def printResults(self) -> None:
         """
-        this function is used to print the results
+        This function is used to print the results
+        :return: None
         """
-        print("Total number of  Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_fp._sys.argv) == 4 or len(_fp._sys.argv) == 5:
-        if len(_fp._sys.argv) == 5:
-            _ap = CFPGrowthPlus(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4])
-        if len(_fp._sys.argv) == 4:
-            _ap = CFPGrowthPlus(_fp._sys.argv[1], _fp._sys.argv[3])
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = PFPMC(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:
+            _ap = PFPMC(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Frequent Patterns:", len(_Patterns))
-        _ap.savePatterns(_fp._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py` & `pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py` & `pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/__init__.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/basic/Gabstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/Gabstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/basic/PPPGrowth.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/PPPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,14 +27,16 @@
 #         print("Total Memory in RSS", memRSS)
 #
 #         run = obj.getRuntime()
 #
 #         print("Total ExecutionTime in seconds:", run)
 #
 
+
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -53,14 +55,18 @@
 
 from PAMI.partialPeriodicPattern.basic import abstract as _abstract
 from typing import List, Dict, Tuple, Set, Union, Any, Iterable, Generator
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 import sys as _sys
 
+from PAMI.partialPeriodicPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
 _minPS = float()
 _period = float()
 _lno = int()
 
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
@@ -153,14 +159,15 @@
 
     def _getConditionalPatterns(self, alpha: '_Node') -> Tuple[List, List, Dict]:
         """
         generates all the conditional patterns of respective node
 
         :param alpha : it represents the Node in tree
         :type alpha : Node
+        :return: tuple
         """
         finalPatterns = []
         finalSets = []
         for i in self.summaries[alpha]:
             set1 = i.timeStamps
             set2 = []
             while i.parent.item is not None:
@@ -185,14 +192,15 @@
 
     def _removeNode(self, nodeValue: int) -> None:
         """
         removing the node from tree
 
         :param nodeValue : it represents the node in tree
         :type nodeValue : node
+        :return: None
         """
         for i in self.summaries[nodeValue]:
             i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
             del i.parent.children[nodeValue]
 
     def _getTimeStamps(self, alpha: '_Node') -> List:
         """
@@ -208,15 +216,16 @@
         return temporary
 
     def _getPeriodicSupport(self, timeStamps: List) -> int:
         """
         calculates the support and periodicity with list of timestamps
 
         :param timeStamps : timestamps of a pattern
-        :type timeStamps : list
+        :type timeStamps : lis
+        :return: int
         """
         timeStamps.sort()
         per = 0
         sup = 0
         for i in range(len(timeStamps) - 1):
             j = i + 1
             if abs(timeStamps[j] - timeStamps[i]) <= _period:
@@ -229,14 +238,16 @@
         It generates the conditional patterns with periodic frequent items
 
         :param conditionalPatterns : conditional_patterns generated from condition_pattern method for
                                 respective node
         :type conditionalPatterns : list
         :param conditionalTimeStamps : represents the timestamps of conditional patterns of a node
         :type conditionalTimeStamps : list
+
+        :return: tuple
         """
         global _minPS, _period
         patterns = []
         timeStamps = []
         data1 = {}
         for i in range(len(conditionalPatterns)):
             for j in conditionalPatterns[i]:
@@ -260,14 +271,15 @@
 
     def _generatePatterns(self, prefix: List) -> Iterable[Tuple[List, int]]:
         """
         generates the patterns
 
         :param prefix : forms the combination of items
         :type prefix : list
+        :return : list
         """
         for i in sorted(self.summaries, key=lambda x: (self.info.get(x), -x)):
             pattern = prefix[:]
             pattern.append(i)
             yield pattern, self.info[i]
             patterns, timeStamps, info = self._getConditionalPatterns(i)
             conditionalTree = _Tree()
@@ -363,19 +375,24 @@
         buildTree()
             constrcuts the main tree by setting the root node as null
         startMine()
             main program to mine the partial periodic patterns
 
     **Executing the code on terminal:**
     --------------------------------------
-        Format:
-           >>> python3 PPPGrowth.py <inputFile> <outputFile> <minPS> <period>
+      .. code-block:: console
+
+
+       Format:
+
+       (.venv) $python3 PPPGrowth.py <inputFile> <outputFile> <minPS> <period>
     
-        Examples:
-           >>> python3 PPPGrowth.py sampleDB.txt patterns.txt 10.0 2.0
+       Examples:
+
+       (.venv) $ python3 PPPGrowth.py sampleDB.txt patterns.txt 10.0 2.0
 
 
     **Sample run of the importing code:**
     -----------------------------------------
     .. code-block:: python
 
             from PAMI.periodicFrequentPattern.basic import PPPGrowth as alg
@@ -424,14 +441,15 @@
     _rank = {}
     _rankdup = {}
     _lno = 0
 
     def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _abstract._pd.DataFrame):
             data, tids = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
@@ -465,14 +483,15 @@
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _partialPeriodicOneItem(self) -> Tuple[Dict, List]:
         """
         calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        :return: tuple
         """
         data = {}
         self._period = self._convert(self._period)
         self._minPS = self._convert(self._minPS)
         for tr in self._Database:
             for i in range(1, len(tr)):
                 if tr[i] not in data:
@@ -490,14 +509,15 @@
 
     def _updateTransactions(self, dict1: Dict) -> List[List]:
         """
         remove the items which are not frequent from transactions and updates the transactions with rank of items
 
         :param dict1 : frequent items with support
         :type dict1 : dictionary
+        :return: list
         """
         list1 = []
         for tr in self._Database:
             list2 = [int(tr[0])]
             for i in range(1, len(tr)):
                 if tr[i] in dict1:
                     list2.append(self._rank[tr[i]])
@@ -513,14 +533,15 @@
         it takes the transactions and support of each item and construct the main tree with setting root
                             node as null
 
         :param data : it represents the one transactions in database
         :type data : list
         :param info : it represents the support of each item
         :type info : dictionary
+        :return: tree
         """
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
             set1 = []
             set1.append(data[i][0])
             rootNode._addTransaction(data[i][1:], set1)
@@ -553,17 +574,20 @@
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
+
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree.
+        :return: None
 
         """
         global _minPS, _period, _lno
         self._startTime = _abstract._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._minPS is None:
@@ -585,14 +609,48 @@
         process = _abstract._psutil.Process(_abstract._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Partial Periodic Patterns were generated successfully using 3PGrowth algorithm ")
 
+    def Mine(self) -> None:
+        """
+        Main method where the patterns are mined by constructing tree.
+        :return: None
+
+        """
+        global _minPS, _period, _lno
+        self._startTime = _abstract._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minPS is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        generatedItems, pfList = self._partialPeriodicOneItem()
+        _minPS, _period, _lno = self._minPS, self._period, len(self._Database)
+        updatedTransactions = self._updateTransactions(generatedItems)
+        for x, y in self._rank.items():
+            self._rankdup[y] = x
+        info = {self._rank[k]: v for k, v in generatedItems.items()}
+        Tree = self._buildTree(updatedTransactions, info)
+        patterns = Tree._generatePatterns([])
+        self._finalPatterns = {}
+        for i in patterns:
+            s = self._savePeriodic(i[0])
+            self._finalPatterns[s] = i[1]
+        self._endTime = _abstract._time.time()
+        process = _abstract._psutil.Process(_abstract._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Partial Periodic Patterns were generated successfully using 3PGrowth algorithm ")
+
+
     def getMemoryUSS(self) -> float:
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -631,14 +689,15 @@
         return dataFrame
 
     def save(self, outFile: str) -> None:
         """Complete set of frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
@@ -649,14 +708,15 @@
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
         print("Total number of Partial Periodic Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
```

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,37 +1,38 @@
-# 3pEclat is the fundamental approach to mine the partial periodic frequent patterns.
-#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     from PAMI.periodicFrequentPattern.basic import PPP_ECLAT as alg
+
+#             import PAMI.periodicFrequentPattern.kPFPMiner as alg
+#
+#             obj = alg.kPFPMiner(iFile, k)
 #
-#     obj = alg.PPP_ECLAT(iFile, minPS, period)
+#             obj.startMine()
 #
-#     obj.startMine()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#     Patterns = obj.getPatterns()
+#             print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#     print("Total number of partial periodic patterns:", len(Patterns))
+#             obj.save(oFile)
 #
-#     obj.save(oFile)
+#             Df = obj.getPatternInDataFrame()
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             memUSS = obj.getMemoryUSS()
 #
-#     memUSS = obj.getMemoryUSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in USS:", memUSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in RSS", memRSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             run = obj.getRuntime()
 #
-#     run = obj.getRuntime()
+#             print("Total ExecutionTime in seconds:", run)
 #
-#     print("Total ExecutionTime in seconds:", run)
+
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
@@ -45,451 +46,439 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-
-
-from PAMI.partialPeriodicPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
 import pandas as pd
+from deprecated import deprecated
+
+from PAMI.periodicFrequentPattern.topk.kPFPMiner import abstract as _ab
 
 
-class PPP_ECLAT(_ab._partialPeriodicPatterns):
+class kPFPMiner(_ab._periodicFrequentPatterns):
     """
-    :Descripition:   3pEclat is the fundamental approach to mine the partial periodic frequent patterns.
+    :Description:   Top - K is and algorithm to discover top periodic-frequent patterns in a temporal database.
 
-    :Reference:   To be published
+    :Reference:   Likhitha, P., Ravikumar, P., Kiran, R.U., Watanobe, Y. (2022).
+                  Discovering Top-k Periodic-Frequent Patterns in Very Large Temporal Databases. Big Data Analytics.
+                 BDA 2022. Lecture Notes in Computer Science, vol 13773. Springer, Cham. https://doi.org/10.1007/978-3-031-24094-2_14
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minPS: float:
-                   Minimum partial periodic pattern...
-    :param  period: float:
-                   Minimum partial periodic...
+                   Name of the output file to store complete set of periodic frequent pattern's
 
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
-        self.iFile : file
-            Name of the Input file or path of the input file
-        self. oFile : file
-            Name of the output file or path of the output file
-        minPS: float or int or str
-            The user can specify minPS either in count or proportion of database size.
-            If the program detects the data type of minPS is integer, then it treats minPS is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minPS=10 will be treated as integer, while minPS=10.0 will be treated as float
-        period: float or int or str
-            The user can specify period either in count or proportion of database size.
-            If the program detects the data type of period is integer, then it treats period is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: period=10 will be treated as integer, while period=10.0 will be treated as float
+        iFile : str
+            Input file name or path of the input file
+        k: int
+            User specified counte of top-k periodic frequent patterns
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+        oFile : str
+            Name of the output file or the path of the output file
         startTime:float
             To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
-        Database : list
-            To store the transactions of a database in list
-        mapSupport : Dictionary
-            To maintain the information of item and their frequency
-        lno : int
-            it represents the total no of transactions
-        tree : class
-            it represents the Tree class
-        finalPatterns : dict
-            it represents to store the patterns
-        tidList : dict
-            stores the timestamps of an item
+        finalPatterns: dict
+            Storing the complete set of patterns in a dictionary variable
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
-            Complete set of frequent patterns will be loaded in to an  output file
+        savePatterns(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingOneitemSets()
-            Scan the database and store the items with their timestamps which are periodic frequent
-        getPeriodAndSupport()
-            Calculates the support and period for a list of timestamps.
-        Generation()
-            Used to implement prefix class equivalence method to generate the periodic patterns recursively
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
+        frequentOneItem()
+            Generates one frequent patterns
+        eclatGeneration(candidateList)
+            It will generate the combinations of frequent items
+        generateFrequentPatterns(tidList)
+            It will generate the combinations of frequent items from a list of items
 
     **Executing the code on terminal:**
-    ----------------------------------------
-        Format:
-           >>> python3 PPP_ECLAT.py <inputFile> <outputFile> <minPS> <period>
+    ------------------------------------------
+    .. code-block:: console
+
+
+       Format:
+
+
+       (.venv) $ python3 kPFPMiner.py <inputFile> <outputFile> <k>
 
-        Examples:
-           >>> python3 PPP_ECLAT.py sampleDB.txt patterns.txt 0.3 0.4
+       Examples :
 
+       (.venv) $  python3 kPFPMiner.py sampleDB.txt patterns.txt 10
 
-    **Sample run of importing the code:**
-    -----------------------------------------
-    ...     code-block:: python
 
-            from PAMI.periodicFrequentPattern.basic import PPP_ECLAT as alg
+    **Sample run of the importing code:
+    --------------------------------------
+    .. code-block:: python
 
-            obj = alg.PPP_ECLAT(iFile, minPS,period)
+            import PAMI.periodicFrequentPattern.kPFPMiner as alg
+
+            obj = alg.kPFPMiner(iFile, k)
 
             obj.startMine()
 
-            Patterns = obj.getPatterns()
+            periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of partial periodic patterns:", len(Patterns))
+            print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
             obj.save(oFile)
 
-            Df = obj.getPatternsAsDataFrame()
+            Df = obj.getPatternInDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    ------------------
-    The complete program was written by P.RaviKumar  under the supervision of Professor Rage Uday Kiran.\n
+    --------------
+            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
     _startTime = float()
     _endTime = float()
+    _k = int()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _mapSupport = {}
-    _itemsetCount = 0
-    _writer = None
-    _minPS = str()
-    _period = str()
-    _tidList = {}
-    _lno = 0
     _Database = []
+    _tidList = {}
+    lno = int()
+    _maximum = int()
 
-    def _convert(self, value) -> Union[int, float]:
-        """
-        To convert the given user specified value
-        :param value: user specified value
-
-        :return: converted value
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
-
-    def _getPeriodicSupport(self, timeStamps: list) -> int:
-        """
-        calculates the support and periodicity with list of timestamps.
-
-        :param timeStamps : timestamps of a pattern
-        :type timeStamps : list
-        """
-        timeStamps.sort()
-        per = 0
-        for i in range(len(timeStamps) - 1):
-            j = i + 1
-            if abs(timeStamps[j] - timeStamps[i]) <= self._period:
-                per += 1
-        return per
-
-    def _creatingItemSets(self) -> None:
+    def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
+
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            data, tids = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                tids = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            for i in range(len(data)):
-                tr = [tids[i][0]]
-                tr = tr + data[i]
-                self._Database.append(tr)
-            self._lno = len(self._Database)
+                self._Database = self._iFile['Transactions'].tolist()
 
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
-                    self._lno += 1
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
                     self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            self._lno += 1
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
+                    
+    def getPer_Sup(self, tids):
+        tids.sort()
+        cur=0
+        per=list()
+        sup=0
+        #print(tids)
+        for i in range(len(tids)-1):
+            j = i + 1
+            #if tids[j] - cur <= periodicity:
+                #return [0,0]
+            per.append(tids[j] - cur)
+            cur = tids[j]
+        per.append(self.lno - cur)
+        return max(per)
 
-    def _creatingOneitemSets(self) -> List[str]:
+    def _frequentOneItem(self):
         """
-        Scans the Temporal database / Input file and stores the 1-length partial-periodic patterns.
+        Generating one frequent patterns
         """
-        plist = []
-        self._tidList = {}
         self._mapSupport = {}
-        self._period = self._convert(self._period)
+        self._tidList = {}
+        n = 0
         for line in self._Database:
-            s = line
-            n = int(s[0])
-            for i in range(1, len(s)):
-                si = s[i]
+            self.lno += 1
+            n = int(line[0])
+            for i in range(1, len(line)):
+                si = line[i]
                 if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [0, n]
+                    self._mapSupport[si] = [1, abs(0 - n), n]
                     self._tidList[si] = [n]
                 else:
-                    lp = n - self._mapSupport[si][1]
-                    if lp <= self._period:
-                        self._mapSupport[si][0] += 1
-                    self._mapSupport[si][1] = n
+                    self._mapSupport[si][0] += 1
+                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
+                    self._mapSupport[si][2] = n
                     self._tidList[si].append(n)
-        self._minPS = self._convert(self._minPS)
-        self._mapSupport = {k: v[0] for k, v in self._mapSupport.items() if v[0] >= self._minPS}
+        for x, y in self._mapSupport.items():
+            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(n - self._mapSupport[x][2]))
         plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        for i in plist:
+            if len(self._finalPatterns) >= self._k:
+                break
+            else:
+                self._finalPatterns[i] = self._mapSupport[i][1]
+        self._maximum = max([self._finalPatterns[i] for i in self._finalPatterns.keys()])
+        plist = list(self._finalPatterns.keys())
         return plist
 
-    def _save(self, prefix: List[str], suffix: List[str], tidSetX: List[int]) -> None:
-        """
-        saves the patterns that satisfy the partial periodic property.
+
+    def _save(self, prefix, suffix, tidSetI):
+        """Saves the patterns that satisfy the periodic frequent property.
 
         :param prefix: the prefix of a pattern
         :type prefix: list
-        :param suffix : the suffix of a patterns
-        :type suffix : list
-        :param tidSetX : the timestamp of a patterns
-        :type tidSetX : list
+        :param suffix: the suffix of a patterns
+        :type suffix: list
+        :param tidSetI: the timestamp of a patterns
+        :type tidSetI: list
         """
 
         if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
-        val = self._getPeriodicSupport(tidSetX)
-        if val >= self._minPS:
-            sample = str()
-            for i in prefix:
-                sample = sample + i + "\t"
-            self._finalPatterns[sample] = val
-
-    def _Generation(self, prefix: List[str], itemSets: List[str], tidSets: List[list]) -> None:
-        """
-        Generates the patterns following Equivalence-class methods
-
-        :param prefix :  main equivalence prefix
-        :type prefix : partial-periodic item or pattern
-        :param itemSets : patterns which are items combined with prefix and satisfying the periodicity
-                        and partial property with their timestamps
-        :type itemSets : list
-        :param tidSets : timestamps of the items in the argument itemSets
-        :type tidSets : list
+        val = self.getPer_Sup(tidSetI)
+        sample = str()
+        for i in prefix:
+            sample = sample + i + " "
+        if len(self._finalPatterns) < self._k:
+            if val < self._maximum:
+                self._finalPatterns[sample] = val
+                self._finalPatterns = {k: v for k, v in sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
+                self._maximum = max([i for i in self._finalPatterns.values()])
+        else:
+            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1], reverse=True):
+                if val < y:
+                    del self._finalPatterns[x]
+                    self._finalPatterns[sample] = val
+                    self._finalPatterns = {k: v for k, v in
+                                              sorted(self._finalPatterns.items(), key=lambda item: item[1],
+                                                     reverse=True)}
+                    self._maximum = max([i for i in self._finalPatterns.values()])
+                    return
+
+    def _Generation(self, prefix, itemSets, tidSets):
+        """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+
+        :param prefix:  main equivalence prefix
+        :type prefix: periodic-frequent item or pattern
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
+        :type itemSets: list
+        :param tidSets: timestamps of the items in the argument itemSets
+        :type tidSets: list
+
         """
         if len(itemSets) == 1:
             i = itemSets[0]
-            tidi = tidSets[0]
-            self._save(prefix, [i], tidi)
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
             return
         for i in range(len(itemSets)):
             itemI = itemSets[i]
             if itemI is None:
                 continue
-            tidSetX = tidSets[i]
+            tidSetI = tidSets[i]
             classItemSets = []
             classTidSets = []
             itemSetX = [itemI]
             for j in range(i + 1, len(itemSets)):
                 itemJ = itemSets[j]
                 tidSetJ = tidSets[j]
-                y = list(set(tidSetX).intersection(tidSetJ))
-                val = self._getPeriodicSupport(y)
-                if val >= self._minPS:
+                y = list(set(tidSetI).intersection(tidSetJ))
+                if self.getPer_Sup(y) <= self._maximum:
                     classItemSets.append(itemJ)
                     classTidSets.append(y)
-            newprefix = list(set(itemSetX)) + prefix
-            self._Generation(newprefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetX)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetI)
 
-    def startMine(self) -> None:
+    def _convert(self, value):
         """
-        Main program start with extracting the periodic frequent items from the database and
-        performs prefix equivalence to form the combinations and generates partial-periodic patterns.
+        to convert the type of user specified minSup value
+
+        :param value: user specified minSup value
+        :return: converted type
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = ((len(self._Database)) * value)
+            else:
+                value = int(value)
+        return value
+
+    def startMine(self):
+        """
+        Main function of the program
 
         """
         self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._k is None:
+            raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
-        plist = self._creatingOneitemSets()
-        self._finalPatterns = {}
+        self._k = self._convert(self._k)
+        plist = self._frequentOneItem()
         for i in range(len(plist)):
             itemI = plist[i]
-            tidSetX = self._tidList[itemI]
+            tidSetI = self._tidList[itemI]
             itemSetX = [itemI]
             itemSets = []
             tidSets = []
             for j in range(i + 1, len(plist)):
                 itemJ = plist[j]
                 tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetX).intersection(tidSetJ))
-                val = self._getPeriodicSupport(y1)
-                if val >= self._minPS:
+                y1 = list(set(tidSetI).intersection(tidSetJ))
+                if self.getPer_Sup(y1) <= self._maximum:
                     itemSets.append(itemJ)
                     tidSets.append(y1)
             self._Generation(itemSetX, itemSets, tidSets)
-            self._save(None, itemSetX, tidSetX)
-        print("Partial Periodic Patterns were generated successfully using 3PEclat algorithm")
+        print("kPFPMiner has successfully generated top-k frequent patterns")
         self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
         self._memoryUSS = float()
+        self._memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self) -> float:
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+    def getMemoryUSS(self):
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
-        """
-        Calculating the total amount of runtime taken by the mining process
+    def getRuntime(self):
+        """Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataframe = {}
+        dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicSupport'])
-        return dataframe
+            data.append([a, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicity'])
+        return dataFrame
 
-    def save(self, outFile: str) -> None:
-        """Complete set of frequent patterns will be loaded in to an output file
+    def save(self, outFile):
+        """Complete set of frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
+
         :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
-            writer.write("%s \n" % s1)
+            patternsAndSupport = x + ":" + str(y)
+            writer.write("%s \n" % patternsAndSupport)
 
-    def getPatterns(self) -> Dict[str, int]:
+    def getPatterns(self):
         """ Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self) -> None:
-        """
-        This function is used to print the results
-        """
-        print("Total number of Partial Periodic Patterns:", len(self.getPatterns()))
+    def printResults(self):
+        print("Total number of  Top-k Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = PPP_ECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = PPP_ECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        print("Total number of Partial Periodic Patterns:", len(_ap.getPatterns()))
+        _Patterns = _ap.getPatterns()
+        print("Total number of top-k periodic frequent patterns:", len(_Patterns))
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
-        for i in [100, 200, 300, 400, 500]:
-            _ap = PPP_ECLAT('/Users/Likhitha/Downloads/temporal_T10I4D100K.csv', i, 5000, '\t')
-            _ap.startMine()
-            print("Total number of Maximal Partial Periodic Patterns:", len(_ap.getPatterns()))
-            _ap.save('/Users/Likhitha/Downloads/output.txt')
-            print("Total Memory in USS:", _ap.getMemoryUSS())
-            print("Total Memory in RSS", _ap.getMemoryRSS())
-            print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
+
```

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/basic/__init__.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/closed/PPPClose.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TubeP.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,563 +1,486 @@
-
-
+# TUFP is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#    from PAMI.partialPeriodicPattern.closed import PPPClose as alg
+#     from PAMI.uncertainFrequentPattern.basic import TUFP as alg
 #
-#     obj = alg.PPPClose("../basic/sampleTDB.txt", "2", "6")
+#     obj = alg.TUFP(iFile, minSup)
 #
 #     obj.startMine()
 #
-#     periodicFrequentPatterns = obj.getPatterns()
+#     frequentPatterns = obj.getPatterns()
 #
-#     print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+#     print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#     obj.save("patterns")
+#     obj.save(oFile)
 #
 #     Df = obj.getPatternsAsDataFrame()
 #
 #     memUSS = obj.getMemoryUSS()
 #
 #     print("Total Memory in USS:", memUSS)
 #
 #     memRSS = obj.getMemoryRSS()
 #
 #     print("Total Memory in RSS", memRSS)
 #
 #     run = obj.getRuntime()
 #
 #     print("Total ExecutionTime in seconds:", run)
-#
-#
-#
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
-
 """
 
 
+from PAMI.uncertainFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Union
+import pandas as pd
 
-import sys as _sys
-import validators as _validators
-from urllib.request import urlopen as _urlopen
-from PAMI.partialPeriodicPattern.closed import abstract as _abstract
+_minSup = float()
+_finalPatterns = {}
 
 
-class PPPClose(_abstract._partialPeriodicPatterns):
+class _Item:
+    """
+    A class used to represent the item with probability in transaction of dataset
+    :Attributes:
+        item : int or word
+            Represents the name of the item
+        probability : float
+            Represent the existential probability(likelihood presence) of an item
     """
-    :Description:
-
-    PPPClose algorithm is used to discover the closed partial periodic patterns in temporal databases.
-    It uses depth-first search.
-
-    :Reference:
 
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
-    :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  period: float:
-                   Minimum partial periodic...
-    :param  periodicSupport: float:
-                   Minimum partial periodic...
+    def __init__(self, item, probability) -> None:
+        self.item = item
+        self.probability = probability
 
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
+class TUFP(_ab._frequentPatterns):
+    """
+    :Description: It is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+    :Reference:
+        Tuong Le, Bay Vo, Van-Nam Huynh, Ngoc Thanh Nguyen, Sung Wook Baik 5, "Mining top-k frequent patterns from uncertain databases",
+        Springer Science+Business Media, LLC, part of Springer Nature 2020, https://doi.org/10.1007/s10489-019-01622-1
     :Attributes:
-
-        iFile : str
-            Input file name or path of the input file
-        oFile : str
-            Name of the output file or path of the input file
-        periodicSupport: int or float or str
-            The user can specify periodicSupport either in count or proportion of database size.
-            If the program detects the data type of periodicSupport is integer, then it treats periodicSupport is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: periodicSupport=10 will be treated as integer, while periodicSupport=10.0 will be treated as float
-        period: int or float or str
-            The user can specify period either in count or proportion of database size.
-            If the program detects the data type of period is integer, then it treats period is expressed in count.
+        iFile : file
+            Name of the Input file or path of the input file
+        oFile : file
+            Name of the output file or path of the output file
+        minSup : float or int or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
-            Example: period=10 will be treated as integer, while period=10.0 will be treated as float
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        finalPatterns: dict
-            Storing the complete set of patterns in a dictionary variable
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-
+        startTime : float
+            To record the start time of the mining process
+        endTime : float
+            To record the completion time of the mining process
+        Database : list
+            To store the transactions of a database in list
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+        lno : int
+            To represent the total no of transaction
+        tree : class
+            To represents the Tree class
+        itemSetCount : int
+            To represents the total no of patterns
+        finalPatterns : dict
+            To store the complete patterns
     :Methods:
-
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
+        storePatternsInFile(oFile)
             Complete set of frequent patterns will be loaded in to a output file
-        getPatternsAsDataFrame()
+        getPatternsInDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-
-    **Executing the code on terminal:**
-    -------------------------------------
-        Format:
-           >>> python3 PPPClose.py <inputFile> <outputFile> <periodicSupport> <period>
-
-        Examples:
-            >>> python3 PPPClose.py sampleTDB.txt patterns.txt 0.3 0.4
-
-    **Sample run of the imported code:**
-    --------------------------------------
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
+        frequentOneItem()
+            Extracts the one-length frequent patterns from database
+        updateTransactions()
+            Update the transactions by removing non-frequent items and sort the Database by item decreased support
+        buildTree()
+            After updating the Database, remaining items will be added into the tree by setting root node as null
+        convert()
+            to convert the user specified value
+        startMine()
+            Mining process will start from this function
+    **Methods to execute code on terminal**
+    -----------------------------------------
+            Format:
+                      >>> python3 TUFP.py <inputFile> <outputFile> <minSup>
+            Example:
+                      >>>  python3 TUFP.py sampleTDB.txt patterns.txt 0.6
+                      .. note:: minSup  will be considered in support count or frequency
+    **Importing this algorithm into a python program**
+    ------------------------------------------------------
     .. code-block:: python
-
-            from PAMI.partialPeriodicPattern.closed import PPPClose as alg
-
-            obj = alg.PPPClose("../basic/sampleTDB.txt", "2", "6")
-
+            from PAMI.uncertainFrequentPattern.basic import TUFP as alg
+            obj = alg.TUFP(iFile, minSup)
             obj.startMine()
-
-            periodicFrequentPatterns = obj.getPatterns()
-
-            print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
-
-            obj.save("patterns")
-
+            frequentPatterns = obj.getPatterns()
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            obj.save(oFile)
             Df = obj.getPatternsAsDataFrame()
-
-            memUSS = obj.getMemoryUSS()
-
+            memUSS = obj.getmemoryUSS()
             print("Total Memory in USS:", memUSS)
-
             memRSS = obj.getMemoryRSS()
-
             print("Total Memory in RSS", memRSS)
-
             run = obj.getRuntime()
-
             print("Total ExecutionTime in seconds:", run)
-
     **Credits:**
-    --------------
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
-
+    ---------------
+             The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
     """
 
-    _periodicSupport = float()
-    _period = float()
     _startTime = float()
     _endTime = float()
+    _minSup = str()
     _finalPatterns = {}
-    _Database = []
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _transaction = []
-    _hashing = {}
-    _mapSupport = {}
-    _itemSetCount = 0
-    _maxItemId = 0
-    _tableSize = 10000
-    _tidList = {}
-    _lno = 0
-
-    def _convert(self, value):
-        """
-        To convert the given user specified value
-
-        :param value: user specified value
-        :return: converted value
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (self._lno * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._lno * value)
-            else:
-                value = int(value)
-        return value
+    _Database = []
+    _cupList = {}
+    _topk = {}
+    _minimum = 9999
 
-    def _creatingItemSets(self):
+    def _creatingItemSets(self) -> None:
         """
-        Storing the complete transactions of the database/input file in a database variable
+        Scans the dataset
         """
         self._Database = []
-        if isinstance(self._iFile, _abstract._pd.DataFrame):
-            timeStamp, data = [], []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                timeStamp = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            for i in range(len(data)):
-                tr = [timeStamp[i]]
-                tr = tr + data[i]
+                self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
                 self._Database.append(tr)
-            self._lno = len(self._Database)
+
+            # print(self.Database)
         if isinstance(self._iFile, str):
-            if _validators.url(self._iFile):
-                data = _urlopen(self._iFile)
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
                 for line in data:
-                    self._lno += 1
+                    line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
+                    tr = []
+                    for i in temp:
+                        i1 = i.index('(')
+                        i2 = i.index(')')
+                        item = i[0:i1]
+                        probability = float(i[i1 + 1:i2])
+                        product = _Item(item, probability)
+                        tr.append(product)
                     self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            self._lno += 1
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            tr = []
+                            for i in temp:
+                                i1 = i.index('(')
+                                i2 = i.index(')')
+                                item = i[0:i1]
+                                probability = float(i[i1 + 1:i2])
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    quit()
 
-    def _OneLengthPartialItems(self):
+    def _frequentOneItem(self) -> List[str]:
         """
-        To scan the database and extracts the 1-length periodic-frequent items
-
-        :return: Returns the 1-length periodic-frequent items
+        takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        :param self.Database : it represents the one self.Database in database
+        :type self.Database : list
         """
-        self._mapSupport = {}
-        self._tidList = {}
-        self._period = self._convert(self._period)
-        for line in self._Database:
-            n = int(line[0])
-            for i in range(1, len(line)):
-                si = line[i]
-                if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [1, 0, n]
-                    self._tidList[si] = [n]
+
+        mapSupport = {}
+        k = 0
+        for i in self._Database:
+            k += 1
+            for j in i:
+                if j.item not in mapSupport:
+                    mapSupport[j.item] = j.probability
+                    self._cupList[j.item] = {k:j.probability}
                 else:
-                    self._mapSupport[si][0] += 1
-                    period = abs(n - self._mapSupport[si][2])
-                    if period <= self._period:
-                        self._mapSupport[si][1] += 1
-                    self._mapSupport[si][2] = n
-                    self._tidList[si].append(n)
-        for x, y in self._mapSupport.items():
-            period = abs(self._lno - self._mapSupport[x][2])
-            if period <= self._period:
-                self._mapSupport[x][1] += 1
-        self._periodicSupport = self._convert(self._periodicSupport)
-        self._mapSupport = {k: v[1] for k, v in self._mapSupport.items() if v[1] >= self._periodicSupport}
-        periodicFrequentItems = {}
-        self._tidList = {k: v for k, v in self._tidList.items() if k in self._mapSupport}
-        for x, y in self._tidList.items():
-            t1 = 0
-            for i in y:
-                t1 += i
-            periodicFrequentItems[x] = t1
-        periodicFrequentItems = [key for key, value in sorted(periodicFrequentItems.items(), key=lambda x: x[1])]
-        return periodicFrequentItems
-
-    def _calculate(self, tidSet):
-        """
-        To calculate the weight if pattern based on the respective timeStamps
-
-        :param tidSet: timeStamps of the pattern
-        :return: the calculated weight of the timeStamps
-        """
-        hashcode = 0
-        for i in tidSet:
-            hashcode += i
-        if hashcode < 0:
-            hashcode = abs(0 - hashcode)
-        return hashcode % self._tableSize
-
-    def _contains(self, itemSet, val, hashcode):
-        """
-        To check if the key(hashcode) is in dictionary(hashing) variable
-
-        :param itemSet: generated periodic-frequent itemSet
-        :param val: support and period of itemSet
-        :param hashcode: the key generated in calculate() method for every itemSet
-
-        :return: true if itemSet with same support present in dictionary(hashing) or else returns false
-        """
-        if self._hashing.get(hashcode) is None:
-            return False
-        for i in self._hashing[hashcode]:
-            itemSetX = i
-            if val == self._hashing[hashcode][itemSetX] and set(itemSetX).issuperset(itemSet):
-                return True
-        return False
-
-    def _getPeriodicSupport(self, timeStamps):
-        """
-        Calculates the period and support of timeStamps
-
-        :param: timeStamps: timeStamps of itemSet
-        :return: period and support
-        """
-        timeStamps.sort()
-        sup = 0
-        for j in range(len(timeStamps) - 1):
-            per = abs(timeStamps[j + 1] - timeStamps[j])
-            if per <= self._period:
-                sup += 1
-        return sup
-
-    def _save(self, prefix, suffix, tidSetX):
-        """
-        Saves the generated pattern which satisfies the closed property
-
-        :param prefix: the prefix part of itemSet
-        :param suffix: the suffix part of itemSet
-        :param tidSetX: the timeStamps of the generated itemSet
-        :return: saves the closed periodic-frequent pattern
+                    mapSupport[j.item] += j.probability
+                    self._cupList[j.item].update({k: j.probability})
+        plist = [k for k,v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        k = 0
+        for x, in plist:
+            k +=1
+            if k >= self._minSup:
+                break
+            self._finalPatterns[x] = mapSupport[x]
+        self._minimum = min(list(self._finalPatterns.values()))
+        return plist
+
+    @staticmethod
+    def _convert(value: Union[int, float, str]) -> Union[int, float]:
+        """
+        To convert the type of user specified minSup value
+        :param value: user specified minSup value
+        :return: converted type minSup value
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = float(value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+            else:
+                value = int(value)
+        return value
+
+    def _save(self, prefix: List[str], suffix: List[str], tidSetI: Dict[int, float]) -> None:
         """
+        Saves the patterns that satisfy the periodic frequent property.
+        :param prefix: the prefix of a pattern
+        :type prefix: list
+        :param suffix: the suffix of a patterns
+        :type suffix: list
+        :param tidSetI: the timestamp of a patterns
+        :type tidSetI: dict
+        """
+
         if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
-        prefix = list(set(prefix))
-        prefix.sort()
-        val = self._getPeriodicSupport(tidSetX)
-        if val >= self._periodicSupport:
-            hashcode = self._calculate(tidSetX)
-            if self._contains(prefix, val, hashcode) is False:
-                self._itemSetCount += 1
+        val = sum(tidSetI.values())
+        #print(prefix, val)
+        if len(self._finalPatterns) <= self._minSup:
+            sample = str()
+            for i in prefix:
+                sample = sample + i + " "
+            self._finalPatterns[sample] = val
+        if len(self._finalPatterns) == self._minSup:
+            if val > self._minimum:
                 sample = str()
                 for i in prefix:
-                    sample = sample + i + "\t"
+                    sample = sample + i + " "
+                index = list(self._finalPatterns.keys())[list(self._finalPatterns.values()).index(self._minimum)]
+                del self._finalPatterns[index]
                 self._finalPatterns[sample] = val
-            if hashcode not in self._hashing:
-                self._hashing[hashcode] = {tuple(prefix): val}
-            else:
-                self._hashing[hashcode][tuple(prefix)] = val
+                self._minimum = min(list(self._finalPatterns.values()))
+        #print(self.finalPatterns, self.minimum, self.minSup)
 
-    def _processEquivalenceClass(self, prefix, itemSets, tidSets):
-        """
 
-        :param prefix: Prefix class of an itemSet
-        :param itemSets: suffix items in periodicFrequentItems that satisfies the periodicSupport condition
-        :param tidSets: timeStamps of items in itemSets respectively
-        :return: closed periodic patterns with length more than 2
+    def _Generation(self, prefix: List[str], itemSets: List[str], tidSets: List[Dict[int, float]]) -> None:
+        """
+        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+        :param prefix:  main equivalence prefix
+        :type prefix: periodic-frequent item or pattern
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
+        :type itemSets: list
+        :param tidSets: timestamps of the items in the argument itemSets
+        :type tidSets: list
         """
         if len(itemSets) == 1:
             i = itemSets[0]
-            tidList = tidSets[0]
-            self._save(prefix, [i], tidList)
-            return
-        if len(itemSets) == 2:
-            itemI = itemSets[0]
-            tidSetI = tidSets[0]
-            itemJ = itemSets[1]
-            tidSetJ = tidSets[1]
-            y1 = list(set(tidSetI).intersection(tidSetJ))
-            if len(y1) >= self._periodicSupport:
-                suffix = []
-                suffix += [itemI, itemJ]
-                suffix = list(set(suffix))
-                self._save(prefix, suffix, y1)
-            if len(y1) != len(tidSetI):
-                self._save(prefix, [itemI], tidSetI)
-            if len(y1) != len(tidSetJ):
-                self._save(prefix, [itemJ], tidSetJ)
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
             return
-        for i in range(len(itemSets)):
-            itemX = itemSets[i]
-            if itemX is None:
+        for i in range(0, len(itemSets)):
+            itemI = itemSets[i]
+            if itemI is None:
                 continue
-            tidSetX = tidSets[i]
+            tidSetI = tidSets[i]
             classItemSets = []
             classTidSets = []
-            itemSetX = [itemX]
-            for j in range(i + 1, len(itemSets)):
+            itemSetX = [itemI]
+            for j in range(i+1, len(itemSets)):
                 itemJ = itemSets[j]
-                if itemJ is None:
-                    continue
                 tidSetJ = tidSets[j]
-                y = list(set(tidSetX).intersection(tidSetJ))
-                if len(y) < self._periodicSupport:
-                    continue
-                if len(tidSetX) == len(tidSetJ) and len(y) == len(tidSetX):
-                    itemSets.insert(j, None)
-                    tidSets.insert(j, None)
-                    itemSetX.append(itemJ)
-                elif len(tidSetX) < len(tidSetJ) and len(y) == len(tidSetX):
-                    itemSetX.append(itemJ)
-                elif len(tidSetX) > len(tidSetJ) and len(y) == len(tidSetJ):
-                    itemSets.insert(j, None)
-                    tidSets.insert(j, None)
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
-                else:
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
-            if len(classItemSets) > 0:
-                newPrefix = list(set(itemSetX)) + prefix
-                self._processEquivalenceClass(newPrefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetX)
+                y = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
+                sum2 = sum(list(y.values()))
+                #print(prefix, itemJ, y, sum2)
+                #if sum2 >= self.minimum:
+                self._save(prefix, [itemJ], y)
+                classItemSets.append(itemJ)
+                classTidSets.append(y)
+            #print(itemI, tidSetI, classItemSets)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            #self.save(prefix, list(set(itemSetX)), tidSetI)
 
-    def startMine(self):
+    def startMine(self) -> None:
         """
-        Mining process will start from here
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        self._startTime = _abstract._time.time()
+        global _minSup
+        self._startTime = _ab._time.time()
         self._creatingItemSets()
-        self._hashing = {}
-        self._finalPatterns = {}
-        periodicFrequentItems = self._OneLengthPartialItems()
-        for i in range(len(periodicFrequentItems)):
-            itemX = periodicFrequentItems[i]
-            if itemX is None:
-                continue
-            tidSetX = self._tidList[itemX]
-            itemSetX = [itemX]
+        self._minSup = self._convert(self._minSup)
+        _minSup = self._minSup
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetI = self._cupList[itemI]
+            itemSetX = [itemI]
             itemSets = []
             tidSets = []
-            for j in range(i + 1, len(periodicFrequentItems)):
-                itemJ = periodicFrequentItems[j]
-                if itemJ is None:
-                    continue
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetX).intersection(tidSetJ))
-                if len(y1) < self._periodicSupport:
-                    continue
-                if len(tidSetX) == len(tidSetJ) and len(y1) is len(tidSetX):
-                    periodicFrequentItems.insert(j, None)
-                    itemSetX.append(itemJ)
-                elif len(tidSetX) < len(tidSetJ) and len(y1) is len(tidSetX):
-                    itemSetX.append(itemJ)
-                elif len(tidSetX) > len(tidSetJ) and len(y1) is len(tidSetJ):
-                    periodicFrequentItems.insert(j, None)
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-                else:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            if len(itemSets) > 0:
-                self._processEquivalenceClass(itemSetX, itemSets, tidSets)
-            self._save([], itemSetX, tidSetX)
-        self._endTime = _abstract._time.time()
-        process = _abstract._psutil.Process(_abstract._os.getpid())
+            for j in range(i+1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._cupList[itemJ]
+                y1 = {key: tidSetJ[key] * tidSetI.get(key, 0)  for key in tidSetJ.keys()}
+                self._save(itemSetX, [itemJ], y1)
+                itemSets.append(itemJ)
+                tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print("Top-K Frequent patterns were generated from uncertain databases successfully using TUFP algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Closed periodic frequent patterns were generated successfully using PPPClose algorithm ")
-
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
+    def getMemoryUSS(self) -> float:
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
+    def getMemoryRSS(self) -> float:
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
-
+    def getRuntime(self) -> float:
+        """
+        Calculating the total amount of runtime taken by the mining process
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
-
+    def getPatternsAsDataFrame(self) -> pd.DataFrame:
+        """
+        Storing final frequent patterns in a dataframe
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataFrame = _abstract._pd.DataFrame(data, columns=['Patterns', 'periodicSupport'])
-        return dataFrame
-
-    def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to a output file
+            data.append([a, b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataframe
 
+    def save(self, outFile: str) -> None:
+        """
+        Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
         :type outFile: file
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
+        self.oFile = outFile
+        writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
+            s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
-
+    def getPatterns(self) -> Dict[str, float]:
+        """
+        Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self):
-        print("Total number of  Closed Partial Periodic Patterns:", len(self.getPatterns()))
+    def printResults(self) -> None:
+        """
+        This function is used to print the results
+        """
+        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
-
 if __name__ == "__main__":
     _ap = str()
-    if len(_sys.argv) == 5 or len(_sys.argv) == 6:
-        if len(_sys.argv) == 6:
-            _ap = PPPClose(_sys.argv[1], _sys.argv[3], _sys.argv[4], _sys.argv[5])
-        if len(_sys.argv) == 5:
-            _ap = PPPClose(_sys.argv[1], _sys.argv[3], _sys.argv[4])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+        if len(_ab._sys.argv) == 5:
+            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        print("Total number of  Patterns:", len(_ap.getPatterns()))
-        _ap.save(_sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _Patterns = _ap.getPatterns()
+        print("Total number of Patterns:", len(_Patterns))
+        _ap.save(_ab._sys.argv[2])
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
+        '''ap = TUFP("/home/apiiit-rkv/Desktop/uncertain/tubeSample", 10, ' ')
+        ap.startMine()
+        Patterns = ap.getPatterns()
+        print("Total number of Patterns:", len(Patterns))
+        ap.save("patterns.txt")
+        memUSS = ap.getMemoryUSS()
+        print("Total Memory in USS:", memUSS)
+        memRSS = ap.getMemoryRSS()
+        print("Total Memory in RSS", memRSS)
+        run = ap.getRuntime()
+        print("Total ExecutionTime in ms:", run)'''
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/closed/abstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/maximal/__init__.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/maximal/abstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/pyspark/__init__.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/pyspark/abstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,39 +1,40 @@
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#         from PAMI.partialPeriodicPattern.pyspark import 4PGrowth as alg
+#             from PAMI.partialPeriodicPattern.pyspark import 4PGrowth as alg
 #
-#         obj = alg.parallel3PGrowth(iFile, minPS, period,numWorkers)
+#             obj = alg.parallel3PGrowth(iFile, minPS, period,numWorkers)
 #
-#         obj.startMine()
+#             obj.startMine()
 #
-#         partialPeriodicPatterns = obj.getPatterns()
+#             partialPeriodicPatterns = obj.getPatterns()
 #
-#         print("Total number of partial periodic Patterns:", len(partialPeriodicPatterns))
+#             print("Total number of partial periodic Patterns:", len(partialPeriodicPatterns))
 #
-#         obj.save(oFile)
+#             obj.save(oFile)
 #
-#         Df = obj.getPatternInDf()
+#             Df = obj.getPatternInDf()
 #
-#         memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#         print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#         memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#         print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#         run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#         print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -51,14 +52,18 @@
 
 from PAMI.partialPeriodicPattern.pyspark import abstract as _ab
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 import sys as _sys
 from pyspark import SparkContext, SparkConf
 
+from PAMI.partialPeriodicPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
 _periodicSupport = float()
 _period = float()
 _lno = int()
 
 class Node(object):
     """
     A class to represent the node of a tree
@@ -404,19 +409,24 @@
         buildTree()
             constrcuts the main tree by setting the root node as null
         startMine()
             main program to mine the partial periodic patterns
 
     **Executing the code on terminal:**
     ---------------------------------------
-        Format:
-           >>> python3 parallel3PGrowth.py <inputFile> <outputFile> <periodicSupport> <period>
+    .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 parallel3PGrowth.py <inputFile> <outputFile> <periodicSupport> <period>
     
-        Examples:
-           >>> python3 parallel3PGrowth.py sampleDB.txt patterns.txt 10.0 2.0
+       Examples:
+
+       (.venv) $ python3 parallel3PGrowth.py sampleDB.txt patterns.txt 10.0 2.0
 
     **Sample run of the importing code:**
     -----------------------------------------
     .. code-block:: python
 
             from PAMI.partialPeriodicPattern.basic import 4PGrowth as alg
 
@@ -464,15 +474,15 @@
     _Database = []
     _rank = {}
     _rankdup = {}
     _lno = 0
 
     numPartitions = 5
 
-    
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Main method where the patterns are mined by constructing tree.
         """
         
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
@@ -515,14 +525,76 @@
         # self._creatingItemSets()
         # generatedItems, pfList = self._partialPeriodicOneItem()
         # _minPS, _period, _lno = self._minPS, self._period, len(self._Database)
         # updatedTransactions = self._updateTransactions(generatedItems)
         # for x, y in self._rank.items():
         #     self._rankdup[y] = x
         # info = {self._rank[k]: v for k, v in generatedItems.items()}
+        # Tree = self._buildTree(updatedTransactions, info)
+        # patterns = Tree._generatePatterns([])
+        # self._finalPatterns = {}
+        # for i in patterns:
+        #     s = self._savePeriodic(i[0])
+        #     self._finalPatterns[s] = i[1]
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Partial Periodic Patterns were generated successfully using 4PGrowth algorithm ")
+
+    def Mine(self):
+        """
+        Main method where the patterns are mined by constructing tree.
+        """
+
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minPS is None:
+            raise Exception("Please enter the Minimum Period-Support")
+
+        self._period = self._convert(self._period)
+        self._minPS = self._convert(self._minPS)
+        minPS = self._minPS
+        period = self._period
+
+        APP_NAME = "4PGrowth"
+        conf = SparkConf().setAppName(APP_NAME)
+        sc = SparkContext(conf=conf).getOrCreate()
+
+        self._startTime = _ab._time.time()
+
+        data = sc.textFile(self._iFile, self.numPartitions).map(lambda x: [y for y in x.strip().split(self._sep)])
+        # self.numPartitions = data.getNumPartitions()
+        # numPartitions = 50
+        freqItems, RecItems = self.getFrequentItems(data)
+        # print(RecItems)
+
+        trans = self.getFrequentItemsets(data, freqItems, self._period, self._minPS, dict(RecItems))
+        a = trans.collect()
+
+        # print(type(a))
+        for k, v in a:
+            string = "\t".join(k)
+            # print(string,":",v)
+            self._finalPatterns[string] = v
+
+        # print(self._finalPatterns)
+        #     print(k,":",v)
+        # trans.saveAsTextFile('temp')
+        self._endTime = _ab._time.time()
+        sc.stop()
+
+        # self._creatingItemSets()
+        # generatedItems, pfList = self._partialPeriodicOneItem()
+        # _minPS, _period, _lno = self._minPS, self._period, len(self._Database)
+        # updatedTransactions = self._updateTransactions(generatedItems)
+        # for x, y in self._rank.items():
+        #     self._rankdup[y] = x
+        # info = {self._rank[k]: v for k, v in generatedItems.items()}
         # Tree = self._buildTree(updatedTransactions, info)
         # patterns = Tree._generatePatterns([])
         # self._finalPatterns = {}
         # for i in patterns:
         #     s = self._savePeriodic(i[0])
         #     self._finalPatterns[s] = i[1]
         process = _ab._psutil.Process(_ab._os.getpid())
```

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/topk/abstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPattern/topk/k3PMiner.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/k3PMiner.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,39 +1,39 @@
 # k3PMiner is and algorithm to discover top - k partial periodic patterns in a temporal  database.
 #
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 
 #
-#     import PAMI.partialPeriodicPattern.topk.k3PMiner as alg
+#             import PAMI.partialPeriodicPattern.topk.k3PMiner as alg
 #
-#     obj = alg.k3PMiner(iFile, k, periodicity)
+#             obj = alg.k3PMiner(iFile, k, periodicity)
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     partialPeriodicPatterns = obj.getPatterns()
+#             partialPeriodicPatterns = obj.getPatterns()
 #
-#     print("Total number of top partial periodic Patterns:", len(partialPeriodicPatterns))
+#             print("Total number of top partial periodic Patterns:", len(partialPeriodicPatterns))
 #
-#     obj.save(oFile)
+#             obj.save(oFile)
 #
-#     Df = obj.getPatternInDataFrame()
+#             Df = obj.getPatternInDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
 
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
@@ -53,20 +53,31 @@
 """
 
 from PAMI.partialPeriodicPattern.topk import abstract as _abstract
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 import sys as _sys
 
+from PAMI.partialPeriodicPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
 
 class k3PMiner(_abstract.partialPeriodicPatterns):
     """
     :Description:   k3PMiner is and algorithm to discover top - k partial periodic patterns in a temporal  database.
 
-    :Reference:
+    :Reference:  Palla Likhitha,Rage Uday Kiran, Discovering Top-K Partial Periodic Patterns in Big Temporal Databases https://dl.acm.org/doi/10.1007/978-3-031-39847-6_28
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of periodic frequent pattern's
+    :param  oFile: str :
+                   Name of the output file to store complete set of periodic frequent pattern's
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent pattern's
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  period: str:
                    Minimum partial periodic...
@@ -119,19 +130,24 @@
             eclatGeneration(candidateList)
                 It will generate the combinations of frequent items
             generateFrequentPatterns(tidList)
                 It will generate the combinations of frequent items from a list of items
 
     **Executing the code on terminal:**
     -------------------------------------
-        Format:
-            >>> python3 k3PMiner.py <iFile> <oFile> <k> <period>
+     .. code-block:: console
 
-        Examples:
-            >>> python3 k3PMiner.py sampleDB.txt patterns.txt 10 3
+
+       Format:
+
+       python3 k3PMiner.py <iFile> <oFile> <k> <period>
+
+       Examples:
+
+       python3 k3PMiner.py sampleDB.txt patterns.txt 10 3
 
 
     **Sample run of the importing code:**
     --------------------------------------
     ...     code-block:: python
 
             import PAMI.partialPeriodicPattern.topk.k3PMiner as alg
@@ -376,14 +392,15 @@
                 if val > self._minimum:
                     classItemSets.append(itemJ)
                     classTidSets.append(y)
             newPrefix = list(set(itemSetX)) + prefix
             self._Generation(newPrefix, classItemSets, classTidSets)
             self._save(prefix, list(set(itemSetX)), tidSetI)
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Main function of the program
 
         """
         self._startTime = _abstract._time.time()
         if self._iFile is None:
@@ -411,14 +428,49 @@
         self._endTime = _abstract._time.time()
         process = _abstract._psutil.Process(_abstract._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
+    def Mine(self):
+            """
+            Main function of the program
+
+            """
+            self._startTime = _abstract._time.time()
+            if self._iFile is None:
+                raise Exception("Please enter the file path or file name:")
+            if self._k is None:
+                raise Exception("Please enter the Minimum Support")
+            self._creatingItemSets()
+            plist = self._frequentOneItem()
+            for i in range(len(plist)):
+                itemI = plist[i]
+                tidSetI = self._tidList[itemI]
+                itemSetX = [itemI]
+                itemSets = []
+                tidSets = []
+                for j in range(i + 1, len(plist)):
+                    itemJ = plist[j]
+                    tidSetJ = self._tidList[itemJ]
+                    y1 = list(set(tidSetI).intersection(tidSetJ))
+                    val = self._getSupportAndPeriod(y1)
+                    if val > self._minimum:
+                        itemSets.append(itemJ)
+                        tidSets.append(y1)
+                self._Generation(itemSetX, itemSets, tidSets)
+            print("TopK partial periodic patterns were generated successfully")
+            self._endTime = _abstract._time.time()
+            process = _abstract._psutil.Process(_abstract._os.getpid())
+            self._memoryUSS = float()
+            self._memoryRSS = float()
+            self._memoryUSS = process.memory_full_info().uss
+            self._memoryRSS = process.memory_info().rss
+
     def getMemoryUSS(self):
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
```

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py`

 * *Files 4% similar despite different names*

```diff
@@ -24,14 +24,15 @@
 #
 #     memRSS = obj.getMemoryRSS()
 #
 #     print("Total Memory in RSS", memRSS)
 #
 #     run = obj.getRuntime()
 
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -43,14 +44,16 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
+import pandas as pd
+from deprecated import deprecated
 from PAMI.partialPeriodicPatternInMultipleTimeSeries import abstract as _ab
 
 
 
 _lno = int()
 _periodicSupport = float()
 _period = float()
@@ -311,14 +314,22 @@
     """
     :Description:   PPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
 
     :Reference:   C. Saideep, R. Uday Kiran, K. Zettsu, P. Fournier-Viger, M. Kitsuregawa and P. Krishna Reddy,
                  "Discovering Periodic Patterns in Irregular Time Series," 2019 International Conference on Data Mining Workshops (ICDMW), 2019,
                   pp. 1020-1028, doi: 10.1109/ICDMW.2019.00147.
 
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of periodic frequent pattern's
+    :param  oFile: str :
+                   Name of the output file to store complete set of periodic frequent pattern's
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
         minSup: int or float or str
@@ -380,19 +391,24 @@
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
 
     **Executing the code on terminal:**
     -------------------------------------
-        Format:
-           >>> python3 PPGrowth.py <inputFile> <outputFile> <minSup> <maxPer>
+    .. code-block:: console
+
 
-        Examples:
-           >>> python3 PPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4
+       Format:
+
+       (.venv) $ python3 PPGrowth.py <inputFile> <outputFile> <minSup> <maxPer>
+
+       Examples:
+
+       (.venv) $  python3 PPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4
 
     **Sample run of importing the code:**
     ----------------------------------------
 
         from PAMI.periodicFrequentPattern.basic import PPGrowth as alg
 
         obj = alg.PPGrowth(iFile, minSup, maxPer)
@@ -594,23 +610,59 @@
                     count=count+1
                 else:
                     line.append(changeDic[j])
             newDatabase.append(line)
         self._Database=newDatabase
         return rechangeDic
 
-
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Mining process will start from this function
         """
 
         global _minSup, _maxPer, _lno,_period,_periodicSupport
         self._startTime = _ab._time.time()
         if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._periodicSupport is None:
+            raise Exception("Please enter the Periodic Support")
+        self._creatingItemSets()
+        changeDic = self._convertNumber()
+        self._periodicSupport = self._convert(self._periodicSupport)
+        self._period = self._convert(self._period)
+        _periodicSupport, _period, _lno = self._periodicSupport, self._period, len(self._Database)
+        if self._periodicSupport > len(self._Database):
+            raise Exception("Please enter the minSup in range between 0 to 1")
+
+        generatedItems, pfList = self._periodicFrequentOneItem()
+        updatedDatabases = self._updateDatabases(generatedItems)
+        self._rankedUp={y:x for x, y in self._rank.items()}
+        info = {self._rank[k]: v for k, v in generatedItems.items()}
+        Tree = self._buildTree(updatedDatabases, info)
+        patterns = Tree.generatePatterns([])
+
+        self._finalPatterns = {}
+        self._finalPatterns={self._savePeriodic(i[0],changeDic):i[1]for i in patterns}
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Periodic Frequent patterns were generated successfully using PPGrowth algorithm ")
+
+    def Mine(self):
+        """
+        Mining process will start from this function
+        """
+
+        global _minSup, _maxPer, _lno,_period,_periodicSupport
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._periodicSupport is None:
             raise Exception("Please enter the Periodic Support")
         self._creatingItemSets()
         changeDic = self._convertNumber()
         self._periodicSupport = self._convert(self._periodicSupport)
         self._period = self._convert(self._period)
```

### Comparing `pami-2024.3.9.2/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py` & `pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -328,30 +328,14 @@
 
 class EPCPGrowth(_ab._periodicCorrelatedPatterns):
     """
     :Description:   EPCPGrowth is an algorithm to discover periodic-Correlated patterns in a temporal database.
 
     :Reference:   http://www.tkl.iis.u-tokyo.ac.jp/new/uploads/publication_file/file/897/Venkatesh2018_Chapter_DiscoveringPeriodic-Correlated.pdf
 
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
-    :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: str:
-                   Minimum number of frequent patterns to be included in the output file.
-    :param  minAllCOnf: float:
-                   Minimum number of ...
-    :param  maxPer: float:
-                   Maximum number of frequent patterns to be included in the output file.
-    :param  maxPerAllConf: float:
-                   Maximum number of frequent patterns to be included in the output file.
-
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
         minSup : int or float or str
```

### Comparing `pami-2024.3.9.2/PAMI/periodicCorrelatedPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/PFECLAT.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFECLAT.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,39 +1,42 @@
 # PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
 #
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
 #
-#     obj = alg.PFECLAT("../basic/sampleTDB.txt", "2", "5")
+#             from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
 #
-#     obj.startMine()
+#             obj = alg.PFECLAT("../basic/sampleTDB.txt", "2", "5")
 #
-#     periodicFrequentPatterns = obj.getPatterns()
+#             obj.startMine()
 #
-#     print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#     obj.save("patterns")
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             obj.save("patterns")
 #
-#     memUSS = obj.getMemoryUSS()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     print("Total Memory in USS:", memUSS)
+#             memUSS = obj.getMemoryUSS()
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     run = obj.getRuntime()
+#             print("Total Memory in RSS", memRSS)
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             run = obj.getRuntime()
 #
+#             print("Total ExecutionTime in seconds:", run)
+#
+
+
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
@@ -48,32 +51,35 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
 
 
 class PFECLAT(_ab._periodicFrequentPatterns):
     """
     :Description:   PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
 
     :Reference:   P. Ravikumar, P.Likhitha, R. Uday kiran, Y. Watanobe, and Koji Zettsu, "Towards efficient discovery of
                   periodic-frequent patterns in columnar temporal databases", 2021 IEA/AIE.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
+                   Name of the output file to store complete set of periodic frequent pattern's
     :param  minSup: str:
                    Controls the minimum number of transactions in which every item must appear in a database.
     :param  maxPer: str:
                    Controls the maximum number of transactions in which any two items within a pattern can reappear.
-
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
@@ -138,22 +144,29 @@
         getPeriodAndSupport()
             Calculates the support and period for a list of timestamps.
         Generation()
             Used to implement prefix class equivalence method to generate the periodic patterns recursively
             
 
     **Methods to execute code on terminal**
-    ----------------------------------------
-            Format:
-                      >>>  python3 PFECLAT.py <inputFile> <outputFile> <minSup>
+    ------------------------------------------
+    .. code-block:: console
+
+
+       Format:
 
-            Example:
-                      >>>   python3 PFECLAT.py sampleDB.txt patterns.txt 10.0
+       (.venv) $ python3 PFECLAT.py <inputFile> <outputFile> <minSup>
 
-                      .. note:: minSup will be considered in percentage of database transactions
+       Example usage:
+
+       (.venv) $ python3 PFECLAT.py sampleDB.txt patterns.txt 10.0
+
+
+
+               .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
 
              from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
@@ -233,14 +246,15 @@
             else:
                 value = int(value)
         return value
 
     def _creatingOneItemSets(self) -> list:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :return: list
         """
         plist = []
         Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             ts, data = [], []
             if self._iFile.empty:
                 print("its empty..")
@@ -322,18 +336,37 @@
                     if sup >= self._minSup and per <= self._maxPer:
                         newItem = prefixItem + "\t" + itemSet[-1]
                         self._finalPatterns[newItem] = [sup, per, _value]
                         newCandidates.append(newItem)
 
         if len(newCandidates) > 0:
             self._generateEclat(newCandidates)
-    
+
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         Mining process will start from this function
+        :return: None
+        """
+        self._startTime = _ab._time.time()
+        self._finalPatterns = {}
+        frequentSets = self._creatingOneItemSets()
+        self._generateEclat(frequentSets)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
+
+    def Mine(self) -> None:
+        """
+        Mining process will start from this function
+        :return: None
         """
         self._startTime = _ab._time.time()
         self._finalPatterns = {}
         frequentSets = self._creatingOneItemSets()
         self._generateEclat(frequentSets)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
@@ -388,14 +421,15 @@
 
     def save(self, outFile: str) -> None:
         """
         Complete set of periodic-frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x + ":" + str(y[0]) + ":" + str(y[1])
             #s1 = x.replace(' ', '\t') + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
@@ -408,14 +442,15 @@
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
         print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
```

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/PFPGrowth.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,40 +1,39 @@
 # PFPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.periodicFrequentPattern.basic import PFPGrowth as alg
+#             from PAMI.periodicFrequentPattern.basic import PFPGrowth as alg
 #
-#     obj = alg.PFPGrowth(iFile, minSup, maxPer)
+#             obj = alg.PFPGrowth(iFile, minSup, maxPer)
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     periodicFrequentPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#     print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#     obj.save(oFile)
+#             obj.save(oFile)
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
 #
-#     print("Total ExecutionTime in seconds:", run)
-
-
 
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
@@ -52,14 +51,18 @@
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.periodicFrequentPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
 _maxPer = float()
 _minSup = float()
 _lno = int()
 
 
 class _Node(object):
     """
@@ -86,26 +89,28 @@
         """
         Initializing the Node class
 
         :param item: Storing the item of a node
         :type item: int or None
         :param children: To maintain the children of a node
         :type children: dict
+        :return: None
         """
 
         self.item = item
         self.children = children
         self.parent = None
         self.timeStamps = []
 
     def addChild(self, node) -> None:
         """
         To add the children to a node
 
         :param node: parent node in the tree
+        :return: None
         """
 
         self.children[node.item] = node
         node.parent = self
 
 
 class _Tree(object):
@@ -314,22 +319,21 @@
     """
     :Description:   PFPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
 
     :Reference:   Syed Khairuzzaman Tanbeer, Chowdhury Farhan, Byeong-Soo Jeong, and Young-Koo Lee, "Discovering Periodic-Frequent
                    Patterns in Transactional Databases", PAKDD 2009, https://doi.org/10.1007/978-3-642-01307-2_24
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
+                   Name of the output file to store complete set of periodic frequent pattern's
     :param  minSup: str:
                    Controls the minimum number of transactions in which every item must appear in a database.
     :param  maxPer: float:
                    Controls the maximum number of transactions in which any two items within a pattern can reappear.
-
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Attributes:
 
         iFile : file
@@ -417,14 +421,15 @@
     _rank = {}
     _rankedUp = {}
     _lno = 0
 
     def _creatingItemSets(self) -> None:
         """
             Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
@@ -548,17 +553,56 @@
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         Mining process will start from this function
+        :return: None
+        """
+
+        global _minSup, _maxPer, _lno
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        _minSup, _maxPer, _lno = self._minSup, self._maxPer, len(self._Database)
+        if self._minSup > len(self._Database):
+            raise Exception("Please enter the minSup in range between 0 to 1")
+        generatedItems, pfList = self._periodicFrequentOneItem()
+        updatedDatabases = self._updateDatabases(generatedItems)
+        for x, y in self._rank.items():
+            self._rankedUp[y] = x
+        info = {self._rank[k]: v for k, v in generatedItems.items()}
+        Tree = self._buildTree(updatedDatabases, info)
+        patterns = Tree.generatePatterns([])
+        self._finalPatterns = {}
+        for i in patterns:
+            sample = self._savePeriodic(i[0])
+            self._finalPatterns[sample] = i[1]
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Periodic Frequent patterns were generated successfully using PFPGrowth algorithm ")
+
+    def Mine(self) -> None:
+        """
+        Mining process will start from this function
+        :return: None
         """
 
         global _minSup, _maxPer, _lno
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._minSup is None:
@@ -633,14 +677,15 @@
 
     def save(self, outFile: str) -> None:
         """
         Complete set of periodic-frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x + ":" + str(y[0]) + ":" + str(y[1])
             #s1 = x.replace(' ', '\t').strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
@@ -653,14 +698,15 @@
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
         print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
@@ -680,21 +726,25 @@
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
 
     """
     **Methods to execute code on terminal**
     --------------------------------------------
-            Format:
-                      >>>  python3 PFPGrowth.py <inputFile> <outputFile> <minSup> <maxPer>
+    .. code-block:: console
+
+      Format:
+            
+      (.venv) $ python3 PFPGrowth.py <inputFile> <outputFile> <minSup> <maxPer>
 
-            Example:
-                      >>>  python3 PFPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4
+      Example:
+      
+      (.venv) $ python3 PFPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4
 
-                      .. note:: minSup will be considered in percentage of database transactions
+    .. note:: minSup will be considered in percentage of database transactions
 
     **Importing this algorithm into a python program**
     ---------------------------------------------------
     .. code-block:: python
 
                 from PAMI.periodicFrequentPattern.basic import PFPGrowth as alg
```

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,39 +1,40 @@
 # PFPGrowthPlus is fundamental and improved version of PFPGrowth algorithm to discover periodic-frequent patterns in temporal database.
 # It uses greedy approach to discover effectively
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.periodicFrequentPattern.basic import PFPGrowthPlus as alg
+#             from PAMI.periodicFrequentPattern.basic import PFPGrowthPlus as alg
 #
-#     obj = alg.PFPGrowthPlus("../basic/sampleTDB.txt", "2", "6")
+#             obj = alg.PFPGrowthPlus("../basic/sampleTDB.txt", "2", "6")
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     periodicFrequentPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#     print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#     obj.save("patterns")
+#             obj.save("patterns")
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
 #
-#     print("Total ExecutionTime in seconds:", run)
 
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
@@ -49,14 +50,18 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 
 _maxPer = float()
 _minSup = float()
 _lno = int()
 
 
@@ -129,14 +134,15 @@
         """
         adding transaction into tree
 
         :param transaction : it represents the one transaction in database
         :type transaction : list
         :param tid : represents the timestamp of transaction
         :type tid : list
+        :return: None
         """
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
@@ -280,22 +286,21 @@
     :Description:   PFPGrowthPlus is fundamental and improved version of PFPGrowth algorithm to discover periodic-frequent patterns in temporal database.
                     It uses greedy approach to discover effectively
 
     :Reference:   R. UdayKiran, MasaruKitsuregawa, and P. KrishnaReddyd, "Efficient discovery of periodic-frequent patterns in
                   very large databases," Journal of Systems and Software February 2016 https://doi.org/10.1016/j.jss.2015.10.035
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
+                   Name of the output file to store complete set of periodic frequent pattern's
     :param  minSup: str:
                    Controls the minimum number of transactions in which every item must appear in a database.
     :param  maxPer: str:
                    Controls the maximum number of transactions in which any two items within a pattern can reappear.
-
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Attributes:
 
         iFile : file
@@ -364,20 +369,27 @@
             after updating the Databases ar added into the tree by setting root node as null
         startMine()
             the main method to run the program
 
 
    **Methods to execute code on terminal**
    -------------------------------------------
-            Format:
-                      >>>  python3 PFPGrowthPlus.py <inputFile> <outputFile> <minSup> <maxPer>
-            Example:
-                      >>>  python3 PFPGrowthPlus.py sampleTDB.txt patterns.txt 0.3 0.4
+   .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 PFPGrowthPlus.py <inputFile> <outputFile> <minSup> <maxPer>
+
+       Example:
+
+       (.venv) $ python3 PFPGrowthPlus.py sampleTDB.txt patterns.txt 0.3 0.4
+
 
-                      .. note:: minSup will be considered in percentage of database transactions
+               .. note:: minSup will be considered in percentage of database transactions
 
    **Importing this algorithm into a python program**
    -----------------------------------------------------------
     .. code-block:: python
 
             from PAMI.periodicFrequentPattern.basic import PFPGorwthPlus as alg
 
@@ -424,14 +436,15 @@
     _rank = {}
     _rankedUp = {}
     _lno = 0
 
     def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
@@ -565,14 +578,15 @@
             else:
                 value = int(value)
         return value
 
     def startMine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree.
+        :return: None
         """
         global _minSup, _maxPer, _lno
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._minSup is None:
             raise Exception("Please enter the Minimum Support")
@@ -647,14 +661,15 @@
 
     def save(self, outFile: str) -> None:
         """
         Complete set of periodic-frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x + ":" + str(y[0]) + ":" + str(y[1])
             #s1 = x.replace(' ','\t').strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
@@ -667,14 +682,15 @@
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
         print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
```

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/PFPMC.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TUFP.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,24 +1,23 @@
-# PFPMC is the fundamental approach to mine the periodic-frequent patterns.
+# TUFP is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
+#     from PAMI.uncertainFrequentPattern.basic import TUFP as alg
 #
-#     from PAMI.periodicFrequentPattern.basic import PFPMC as alg
-#
-#     obj = alg.PFPMC("../basic/sampleTDB.txt", "2", "5")
+#     obj = alg.TUFP(iFile, minSup)
 #
 #     obj.startMine()
 #
-#     periodicFrequentPatterns = obj.getPatterns()
+#     frequentPatterns = obj.getPatterns()
 #
-#     print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#     print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#     obj.save("patterns")
+#     obj.save(oFile)
 #
 #     Df = obj.getPatternsAsDataFrame()
 #
 #     memUSS = obj.getMemoryUSS()
 #
 #     print("Total Memory in USS:", memUSS)
 #
@@ -26,459 +25,461 @@
 #
 #     print("Total Memory in RSS", memRSS)
 #
 #     run = obj.getRuntime()
 #
 #     print("Total ExecutionTime in seconds:", run)
 
-
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
-
 """
 
-from itertools import groupby as _groupby
-from operator import itemgetter as _itemgetter
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
 
+from PAMI.uncertainFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Union
+import pandas as pd
 
-class PFPMC(_ab._periodicFrequentPatterns):
-    """
-    :Description:   PFPMC is the fundamental approach to mine the periodic-frequent patterns.
+_minSup = float()
+_finalPatterns = {}
 
-    :Reference: (has to be added)
 
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
-    :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: str:
-                   Controls the minimum number of transactions in which every item must appear in a database.
-    :param  maxPer: str:
-                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
+class _Item:
+    """
+    A class used to represent the item with probability in transaction of dataset
+    :Attributes:
+        item : int or word
+            Represents the name of the item
+        probability : float
+            Represent the existential probability(likelihood presence) of an item
+    """
 
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    def __init__(self, item, probability) -> None:
+        self.item = item
+        self.probability = probability
 
-    :Attributes:
 
+class TUFP(_ab._frequentPatterns):
+    """
+    :Description: It is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+    :Reference:
+        Tuong Le, Bay Vo, Van-Nam Huynh, Ngoc Thanh Nguyen, Sung Wook Baik 5, "Mining top-k frequent patterns from uncertain databases",
+        Springer Science+Business Media, LLC, part of Springer Nature 2020, https://doi.org/10.1007/s10489-019-01622-1
+    :Attributes:
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
-        minSup : int or float or str
+        minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer : int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime:float
+        startTime : float
             To record the start time of the mining process
-        endTime:float
+        endTime : float
             To record the completion time of the mining process
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
         lno : int
-            it represents the total no of transactions
+            To represent the total no of transaction
         tree : class
-            it represents the Tree class
+            To represents the Tree class
         itemSetCount : int
-            it represents the total no of patterns
+            To represents the total no of patterns
         finalPatterns : dict
-            it represents to store the patterns
-        tidList : dict
-            stores the timestamps of an item
-        hashing : dict
-            stores the patterns with their support to check for the closed property
-
+            To store the complete patterns
     :Methods:
-
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to an output file
-        getPatternsAsDataFrame()
-            Complete set of periodic-frequent patterns will be loaded in to a dataframe
+        storePatternsInFile(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsInDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingOneItemSets()
-            Scan the database and store the items with their timestamps which are periodic frequent
-        getPeriodAndSupport()
-            Calculates the support and period for a list of timestamps.
-        Generation()
-            Used to implement prefix class equivalence method to generate the periodic patterns recursively
-
-
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
+        frequentOneItem()
+            Extracts the one-length frequent patterns from database
+        updateTransactions()
+            Update the transactions by removing non-frequent items and sort the Database by item decreased support
+        buildTree()
+            After updating the Database, remaining items will be added into the tree by setting root node as null
+        convert()
+            to convert the user specified value
+        startMine()
+            Mining process will start from this function
     **Methods to execute code on terminal**
-    ------------------------------------------
+    -----------------------------------------
             Format:
-                      >>>   python3 PFPMC.py <inputFile> <outputFile> <minSup> <maxPer>
-
+                      >>> python3 TUFP.py <inputFile> <outputFile> <minSup>
             Example:
-                      >>>   python3 PFPMC.py sampleDB.txt patterns.txt 10.0 4.0
-    
-                      .. note:: minSup and maxPer will be considered in percentage of database transactions
-
+                      >>>  python3 TUFP.py sampleTDB.txt patterns.txt 0.6
+                      .. note:: minSup  will be considered in support count or frequency
     **Importing this algorithm into a python program**
-    ----------------------------------------------------
+    ------------------------------------------------------
     .. code-block:: python
-
-                from PAMI.periodicFrequentPattern.basic import PFPMC as alg
-
-                obj = alg.PFPMC("../basic/sampleTDB.txt", "2", "5")
-
-                obj.startMine()
-
-                periodicFrequentPatterns = obj.getPatterns()
-
-                print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
-
-                obj.save("patterns")
-
-                Df = obj.getPatternsAsDataFrame()
-
-                memUSS = obj.getMemoryUSS()
-
-                print("Total Memory in USS:", memUSS)
-
-                memRSS = obj.getMemoryRSS()
-
-                print("Total Memory in RSS", memRSS)
-
-                run = obj.getRuntime()
-
-                print("Total ExecutionTime in seconds:", run)
-
+            from PAMI.uncertainFrequentPattern.basic import TUFP as alg
+            obj = alg.TUFP(iFile, minSup)
+            obj.startMine()
+            frequentPatterns = obj.getPatterns()
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            obj.save(oFile)
+            Df = obj.getPatternsAsDataFrame()
+            memUSS = obj.getmemoryUSS()
+            print("Total Memory in USS:", memUSS)
+            memRSS = obj.getMemoryRSS()
+            print("Total Memory in RSS", memRSS)
+            run = obj.getRuntime()
+            print("Total ExecutionTime in seconds:", run)
     **Credits:**
-    ----------------
-
-    The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
-
+    ---------------
+    The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
     """
 
+    _startTime = float()
+    _endTime = float()
+    _minSup = str()
+    _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    _dbSize = None
-    _Database = None
-    _minSup = str()
-    _maxPer = str()
-    _tidSet = set()
-    _finalPatterns = {}
-    _startTime = None
-    _endTime = None
-    _lastTid = int()
     _memoryUSS = float()
     _memoryRSS = float()
+    _Database = []
+    _cupList = {}
+    _topk = {}
+    _minimum = 9999
 
-    def _getPeriodic(self, tids: set) -> int:
+    def _creatingItemSets(self) -> None:
         """
-        To get  Periodic frequent patterns
-
-        :param tids: represents the timestamp of a transaction
-        :type tids: set
+        Scans the dataset
         """
-        tids = list(tids)
-        tids.sort()
-        temp = self._maxPer + 1
-        diffs = []
-        if self._lastTid in tids:
-            tids.remove(self._lastTid)
-        for k, g in _groupby(enumerate(tids), lambda ix: ix[0] - ix[1]):
-            diffs.append(len(list(map(_itemgetter(1), g))))
-        if len(diffs) < 1:
-            return temp
-        return max(diffs) + 1
-
-    def _getPeriodic(self, tids: set):
-
-        tids = list(tids)
-        tids.sort()
-        temp = self._maxPer + 1
-        if self._lastTid in tids:
-            tids.remove(self._lastTid)
-        diffs = []
-        # find the longest consecutive period
-
-        count = 0
-        for i in range(len(tids) - 1):
-            if tids[i + 1] == tids[i] + 1:
-                count += 1
-            else:
-                diffs.append(count)
-                count = 0
-        if len(diffs) < 1:
-            return temp
-        return max(diffs) + 1
-
-    def _getPeriodic(self, tids: set):
-        tids = list(tids)
-        tids.sort()
-        temp = self._maxPer + 1
-        if self._lastTid in tids:
-            tids.remove(self._lastTid)
-        diffs = []
-        tempPer = 0
-        period = 0
-        for i in range(len(tids) - 1):
-            if tids[i+1] - tids[i] == 1:
-                tempPer += 1
-            else:
-                period = max(period, tempPer + 1)
-                if period > self._maxPer:
-                    return temp
-                tempPer = 0
-
-        return period
-
-    def _convert(self, value) -> float:
-        """
-        To convert the given user specified value
-
-        :param value: user specified value
-        :return: converted value
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (self._dbSize * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._dbSize * value)
-            else:
-                value = int(value)
-        return value
-
-    def _creatingOneItemSets(self) -> list:
-        """
-        Storing the complete transactions of the database/input file in a database variable
-        """
-        plist = []
-        Database = []
+        self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            ts, data = [], []
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            for i in range(len(data)):
-                tr = [ts[i][0]]
-                tr = tr + data[i]
-                Database.append(tr)
+                self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
+                self._Database.append(tr)
+
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    Database.append(temp)
+                    tr = []
+                    for i in temp:
+                        i1 = i.index('(')
+                        i2 = i.index(')')
+                        item = i[0:i1]
+                        probability = float(i[i1 + 1:i2])
+                        product = _Item(item, probability)
+                        tr.append(product)
+                    self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            Database.append(temp)
+                            tr = []
+                            for i in temp:
+                                i1 = i.index('(')
+                                i2 = i.index(')')
+                                item = i[0:i1]
+                                probability = float(i[i1 + 1:i2])
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    quit()
-        tid = 0
-        itemsets = {}  # {key: item, value: list of tids}
-        periodicHelper = {}  # {key: item, value: [period, last_tid]}
-        for line in Database:
-            tid = int(line[0])
-            self._tidSet.add(tid)
-            for item in line[1:]:
-                if item in itemsets:
-                    itemsets[item].add(tid)
-                else:
-                    itemsets[item] = {tid}
 
-        self._dbSize = len(Database)
-        self._lastTid = max(self._tidSet)
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        del Database
-        candidates = []
-        for item, tids in itemsets.items():
-            diff = self._tidSet.difference(tids)
-            per = self._getPeriodic(diff)
-            sup = len(tids)
-            if sup >= self._minSup and per <= self._maxPer:
-                candidates.append(item)
-                self._finalPatterns[item] = [sup, per, diff]
-        return candidates
-
-    def _generateDiffsetEclat(self, candidates: list) -> None:
-        new_freqList = []
-        for i in range(0, len(candidates)):
-            item1 = candidates[i]
-            i1_list = item1.split()
-            for j in range(i + 1, len(candidates)):
-                item2 = candidates[j]
-                i2_list = item2.split()
-                if i1_list[:-1] == i2_list[:-1]:
-                    union_DiffSet = self._finalPatterns[item2][2].union(self._finalPatterns[item1][2])
-                    sorted(union_DiffSet)
-                    union_supp = self._dbSize - len(union_DiffSet)
-                    period = self._getPeriodic(union_DiffSet)
-                    if union_supp >= self._minSup and period <= self._maxPer:
-                        newKey = item1 + "\t" + i2_list[-1]
-                        self._finalPatterns[newKey] = [union_supp, period, union_DiffSet]
-                        new_freqList.append(newKey)
+    def _frequentOneItem(self) -> List[str]:
+        """
+        takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        :param self.Database : it represents the one self.Database in database
+        :type self.Database : list
+        """
+
+        mapSupport = {}
+        k = 0
+        for i in self._Database:
+            k += 1
+            for j in i:
+                if j.item not in mapSupport:
+                    mapSupport[j.item] = j.probability
+                    self._cupList[j.item] = {k:j.probability}
                 else:
-                    break
+                    mapSupport[j.item] += j.probability
+                    self._cupList[j.item].update({k: j.probability})
+        plist = [k for k,v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        k = 0
+        for x, in plist:
+            k +=1
+            if k >= self._minSup:
+                break
+            self._finalPatterns[x] = mapSupport[x]
+        self._minimum = min(list(self._finalPatterns.values()))
+        return plist
+
+    @staticmethod
+    def _convert(value: Union[int, float, str]) -> Union[int, float]:
+        """
+        To convert the type of user specified minSup value
+        :param value: user specified minSup value
+        :return: converted type minSup value
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = float(value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+            else:
+                value = int(value)
+        return value
 
-        if len(new_freqList) > 0:
-            self._generateDiffsetEclat(new_freqList)
+    def _save(self, prefix: List[str], suffix: List[str], tidSetI: Dict[int, float]) -> None:
+        """
+        Saves the patterns that satisfy the periodic frequent property.
+        :param prefix: the prefix of a pattern
+        :type prefix: list
+        :param suffix: the suffix of a patterns
+        :type suffix: list
+        :param tidSetI: the timestamp of a patterns
+        :type tidSetI: dict
+        """
+
+        if prefix is None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        val = sum(tidSetI.values())
+        # print(prefix, val)
+        if len(self._finalPatterns) <= self._minSup:
+            sample = str()
+            for i in prefix:
+                sample = sample + i + " "
+            self._finalPatterns[sample] = val
+        if len(self._finalPatterns) == self._minSup:
+            if val > self._minimum:
+                sample = str()
+                for i in prefix:
+                    sample = sample + i + " "
+                index = list(self._finalPatterns.keys())[list(self._finalPatterns.values()).index(self._minimum)]
+                del self._finalPatterns[index]
+                self._finalPatterns[sample] = val
+                self._minimum = min(list(self._finalPatterns.values()))
+        # print(self.finalPatterns, self.minimum, self.minSup)
+
+    def _Generation(self, prefix: List[str], itemSets: List[str], tidSets: List[Dict[int, float]]) -> None:
+        """
+        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+        :param prefix:  main equivalence prefix
+        :type prefix: periodic-frequent item or pattern
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
+        :type itemSets: list
+        :param tidSets: timestamps of the items in the argument itemSets
+        :type tidSets: list
+        """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
+            return
+        for i in range(0, len(itemSets)):
+            itemI = itemSets[i]
+            if itemI is None:
+                continue
+            tidSetI = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemI]
+            for j in range(i + 1, len(itemSets)):
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
+                sum2 = sum(list(y.values()))
+                # print(prefix, itemJ, y, sum2)
+                # if sum2 >= self.minimum:
+                self._save(prefix, [itemJ], y)
+                classItemSets.append(itemJ)
+                classTidSets.append(y)
+            # print(itemI, tidSetI, classItemSets)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            # self.save(prefix, list(set(itemSetX)), tidSetI)
 
     def startMine(self) -> None:
         """
-        Mining process will start from this function
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        # print(f"Optimized {type(self).__name__}")
+        global _minSup
         self._startTime = _ab._time.time()
-        self._finalPatterns = {}
-        frequentSets = self._creatingOneItemSets()
-        self._generateDiffsetEclat(frequentSets)
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        _minSup = self._minSup
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetI = self._cupList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._cupList[itemJ]
+                y1 = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
+                self._save(itemSetX, [itemJ], y1)
+                itemSets.append(itemJ)
+                tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print("Top-K Frequent patterns were generated from uncertain databases successfully using TUFP algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
         self._memoryUSS = float()
+        self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Periodic-Frequent patterns were generated successfully using PFPDiffset ECLAT algorithm ")
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self) -> pd.DataFrame:
         """
-        Storing final periodic-frequent patterns in a dataframe
-
-        :return: returning periodic-frequent patterns in a dataframe
+        Storing final frequent patterns in a dataframe
+        :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+            data.append([a, b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
-        Complete set of periodic-frequent patterns will be loaded in to an output file
-
+        Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
-        :type outFile: csv file
+        :type outFile: file
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
+        self.oFile = outFile
+        writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
-            #s1 = x.replace(' ', '\t') + ":" + str(y[0]) + ":" + str(y[1])
+            s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> dict:
+    def getPatterns(self) -> Dict[str, float]:
         """
-        Function to send the set of periodic-frequent patterns after completion of the mining process
-
-        :return: returning periodic-frequent patterns
+        Function to send the set of frequent patterns after completion of the mining process
+        :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
+    if __name__ == "__main__":
+        _ap = str()
+        if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+            if len(_ab._sys.argv) == 5:
+                _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            if len(_ab._sys.argv) == 4:
+                _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap.startMine()
+            _Patterns = _ap.getPatterns()
+            print("Total number of Patterns:", len(_Patterns))
+            _ap.save(_ab._sys.argv[2])
+            _memUSS = _ap.getMemoryUSS()
+            print("Total Memory in USS:", _memUSS)
+            _memRSS = _ap.getMemoryRSS()
+            print("Total Memory in RSS", _memRSS)
+            _run = _ap.getRuntime()
+            print("Total ExecutionTime in ms:", _run)
+        else:
+            '''ap = TUFP("/home/apiiit-rkv/Desktop/uncertain/tubeSample", 10, ' ')
+            ap.startMine()
+            Patterns = ap.getPatterns()
+            print("Total number of Patterns:", len(Patterns))
+            ap.save("patterns.txt")
+            memUSS = ap.getMemoryUSS()
+            print("Total Memory in USS:", memUSS)
+            memRSS = ap.getMemoryRSS()
+            print("Total Memory in RSS", memRSS)
+            run = ap.getRuntime()
+            print("Total ExecutionTime in ms:", run)'''
+            print("Error! The number of input parameters do not match the total number of parameters provided")
 
-if __name__ == "__main__":
-    _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = PFPMC(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = PFPMC(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        _ap.startMine()
-        print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
-    else:
-        print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/PSGrowth.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PSGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,38 +1,39 @@
 #  PS-Growth is one of the fundamental algorithm to discover periodic-frequent patterns in a temporal database.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.periodicFrequentPattern.basic import PSGrowth as alg
+#             from PAMI.periodicFrequentPattern.basic import PSGrowth as alg
 #
-#     obj = alg.PSGrowth("../basic/sampleTDB.txt", "2", "6")
+#             obj = alg.PSGrowth("../basic/sampleTDB.txt", "2", "6")
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     periodicFrequentPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#     print("Total number of  Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of  Patterns:", len(periodicFrequentPatterns))
 #
-#     obj.save("patterns")
+#             obj.save("patterns")
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
 #
-#     print("Total ExecutionTime in seconds:", run)
 
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
@@ -47,14 +48,17 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
 from itertools import combinations as _combinations
 from PAMI.periodicFrequentPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator  
 
 _pfList = []
 _minSup = int()
 _maxPer = int()
@@ -272,14 +276,15 @@
         """
         Initializing the Node class
 
         :param item: Storing the item of a node
         :type item: int
         :param children: To maintain the children of a node
         :type children: dict
+        :return: None
         """
         self.item = item
         self.children = children
         self.parent = None
         self.timeStamps = _NodeSummaries()
 
     def addChild(self, node) -> None:
@@ -334,14 +339,15 @@
         """
         Adding transaction into the tree
 
         :param transaction: it represents the one transaction in a database
         :type transaction: list
         :param tid: represents the timestamp of a transaction
         :type tid: list
+        :return: None
         """
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = Node(transaction[i], {})
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
@@ -431,14 +437,15 @@
         for i in self.summaries[alpha]:
             temp += i.timeStamps
         return temp
 
     def check(self) -> int:
         """
         To the total number of child and their summaries
+        :return: int
         """
         k = self.root
         while len(k.children) != 0:
             if len(k.children) > 1:
                 return 1
             if len(k.children) != 0 and len(k.timeStamps.totalSummaries) > 0:
                 return 1
@@ -565,22 +572,21 @@
     :Description:   PS-Growth is one of the fundamental algorithm to discover periodic-frequent patterns in a temporal database.
 
     :Reference :   A. Anirudh, R. U. Kiran, P. K. Reddy and M. Kitsuregaway, "Memory efficient mining of periodic-frequent
                    patterns in transactional databases," 2016 IEEE Symposium Series on Computational Intelligence (SSCI),
                    2016, pp. 1-8, https://doi.org/10.1109/SSCI.2016.7849926
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
+                   Name of the output file to store complete set of periodic frequent pattern's
     :param  minSup: str:
                    Controls the minimum number of transactions in which every item must appear in a database.
     :param  maxPer: str:
                    Controls the maximum number of transactions in which any two items within a pattern can reappear.
-
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
@@ -640,21 +646,28 @@
             Scans the dataset or dataframes and stores in list format
         buildTree()
             after updating the Databases ar added into the tree by setting root node as null
 
 
     **Methods to execute code on terminal**
     -----------------------------------------
-            Format:
-                      >>>  python3 PSGrowth.py <inputFile> <outputFile> <minSup> <maxPer>
+     .. code-block:: console
+
+
+       Format:
 
-            Example:
-                      >>>  python3 PSGrowth.py sampleTDB.txt patterns.txt 0.3 0.4
+       (.venv) $ python3 PSGrowth.py <inputFile> <outputFile> <minSup> <maxPer>
 
-                      .. note:: minSup will be considered in percentage of database transactions
+       Example:
+
+       (.venv) $ python3 PSGrowth.py sampleTDB.txt patterns.txt 0.3 0.4
+
+
+
+               .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
     .. code-block:: python
 
             from PAMI.periodicFrequentPattern.basic import PSGrowth as alg
@@ -721,14 +734,15 @@
             else:
                 value = int(value)
         return value
 
     def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             ts, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
@@ -810,17 +824,49 @@
             if len(list2) >= 2:
                 basket = list2[1:]
                 basket.sort()
                 list2[1:] = basket[0:]
                 rootNode.addTransaction(list2[1:], list2[0])
         return rootNode
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         Mining process will start from this function
+        :return: None
+        """
+        global _minSup, _maxPer, _lno, _pfList
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        OneLengthPeriodicItems, _pfList = self._OneLengthItems()
+        info = {self._rank[k]: v for k, v in OneLengthPeriodicItems.items()}
+        Tree = self._buildTree(info, OneLengthPeriodicItems)
+        patterns = Tree.generatePatterns([])
+        self._finalPatterns = {}
+        for i in patterns:
+            sample = str()
+            for k in i[0]:
+                sample = sample + k + "\t"
+            self._finalPatterns[sample] = i[1]
+        self._endTime = _ab._time.time()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Periodic-Frequent patterns were generated successfully using PS-Growth algorithm ")
+
+    def Mine(self) -> None:
+        """
+        Mining process will start from this function
+        :return: None
         """
         global _minSup, _maxPer, _lno, _pfList
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._minSup is None:
             raise Exception("Please enter the Minimum Support")
@@ -888,14 +934,15 @@
 
     def save(self, outFile: str) -> None:
         """
         Complete set of periodic-frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x + ":" + str(y[0]) + ":" + str(y[1])
             #s1 = x.replace(' ', '\t').strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
@@ -908,14 +955,15 @@
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self)-> None:
         """
         This function is used to print the results
+        :return: None
         """
         print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:", self.getRuntime())
```

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/__init__.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,37 +1,40 @@
 #  ParallelPFPGrowth is one of the fundamental distributed algorithm to discover periodic-frequent patterns in a transactional database. It is based PySpark framework.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     from PAMI.periodicFrequentPattern.basic import parallelPFPGrowth as alg
 #
-#     obj = alg.parallelPFPGrowth(iFile, minSup, maxPer, numWorkers, sep='\t')
+#             from PAMI.periodicFrequentPattern.basic import parallelPFPGrowth as alg
 #
-#     obj.startMine()
+#             obj = alg.parallelPFPGrowth(iFile, minSup, maxPer, numWorkers, sep='\t')
 #
-#     periodicFrequentPatterns = obj.getPatterns()
+#             obj.startMine()
 #
-#     print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#     obj.save(oFile)
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             obj.save(oFile)
 #
-#     memUSS = obj.getMemoryUSS()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     print("Total Memory in USS:", memUSS)
+#             memUSS = obj.getMemoryUSS()
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     run = obj.getRuntime()
+#             print("Total Memory in RSS", memRSS)
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
+#
+
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
@@ -45,14 +48,18 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
 # from PAMI.periodicFrequentPattern.basic
 import abstract as _ab
 
 _maxPer = float()
 _minSup = float()
 _lno = int()
 
@@ -314,22 +321,21 @@
 class parallelPFPGrowth(_ab._periodicFrequentPatterns):
     """
     :Description:   ParallelPFPGrowth is one of the fundamental distributed algorithm to discover periodic-frequent patterns in a transactional database. It is based PySpark framework.
 
     :Reference:   C. Saideep, R. Uday Kiran, Koji Zettsu, Cheng-Wei Wu, P. Krishna Reddy, Masashi Toyoda, Masaru Kitsuregawa: Parallel Mining of Partial Periodic Itemsets in Big Data. IEA/AIE 2020: 807-819
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
+                   Name of the output file to store complete set of periodic frequent pattern's
     :param  minSup: str:
                    Controls the minimum number of transactions in which every item must appear in a database.
     :param  maxPer: str:
                    Controls the maximum number of transactions in which any two items within a pattern can reappear.
-
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
@@ -397,21 +403,27 @@
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
 
 
     **Methods to execute code on terminal**
     ---------------------------------------------
-            Format:
-                      >>>  python3 parallelPFPGrowth.py <inputFile> <outputFile> <minSup> <maxPer> <noWorker>
+     .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 parallelPFPGrowth.py <inputFile> <outputFile> <minSup> <maxPer> <noWorker>
+
+       Example usage:
 
-            Example:
-                      >>>  python3 parallelPFPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4 5
+       (.venv) $ python3 parallelPFPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4 5
 
-                      .. note:: minSup will be considered in percentage of database transactions
+
+               .. note:: minSup will be considered in percentage of database transactions
 
     **Importing this algorithm into a python program**
     ---------------------------------------------------------
     .. code-block:: python
 
                 from PAMI.periodicFrequentPattern.basic import parallelPFPGrowth as alg
 
@@ -599,22 +611,54 @@
             if '.' in value:
                 value = float(value)
                 value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Start the mining process
 
         """
         self.__startTime = _ab._time.time()
         APP_NAME = "parallelPFPGrowth"
         conf = _ab.SparkConf().setAppName(APP_NAME)
+        # conf = conf.setMaster("local[*]")
+        sc = _ab.SparkContext(conf=conf).getOrCreate()
+        # sc = SparkContext.getOrCreate();
+        data = sc.textFile(self._iFile, minPartitions=self._numWorkers).map(
+            lambda x: [int(y) for y in x.strip().split(self._sep)])
+        # data = sc.textFile(finput).map(lambda x: [int(y) for y in x.strip().split(' ')])
+        data.cache()
+        # minSupport = data.count() * threshold/100
+        # maxPer = data.count() * periodicity_threshold/100
+        self._minSup = self.__convert(self._minSup)
+        self._maxPer = self.__convert(self._maxPer)
+        self._numTrans = sc.broadcast(data.count())
+        self._perFreqItems = self.getFrequentItems(data)
+        freqItemsets = self.getFrequentItemsets(data, self._perFreqItems)
+        self.__finalPatterns = freqItemsets.count()
+        sc.stop()
+        self.__endTime = _ab._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
+    def Mine(self):
+        """
+        Start the mining process
+
+        """
+        self.__startTime = _ab._time.time()
+        APP_NAME = "parallelPFPGrowth"
+        conf = _ab.SparkConf().setAppName(APP_NAME)
         # conf = conf.setMaster("local[*]")
         sc = _ab.SparkContext(conf=conf).getOrCreate()
         # sc = SparkContext.getOrCreate();
         data = sc.textFile(self._iFile, minPartitions=self._numWorkers).map(
             lambda x: [int(y) for y in x.strip().split(self._sep)])
         # data = sc.textFile(finput).map(lambda x: [int(y) for y in x.strip().split(' ')])
         data.cache()
```

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/closed/CPFPMiner.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/CPFPMiner.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,39 +1,42 @@
 #  CPFPMiner algorithm is used to discover the closed periodic frequent patterns in temporal databases.
 #  It uses depth-first search.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.periodicFrequentPattern.closed import CPFPMiner as alg
+#             from PAMI.periodicFrequentPattern.closed import CPFPMiner as alg
 #
-#     obj = alg.CPFPMiner("../basic/sampleTDB.txt", "2", "6")
+#             obj = alg.CPFPMiner("../basic/sampleTDB.txt", "2", "6")
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     periodicFrequentPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#     print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#     obj.save("patterns")
+#             obj.save("patterns")
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
+#
+
+
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -46,30 +49,33 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
 
 from PAMI.periodicFrequentPattern.closed import abstract as _ab
 
 
 class CPFPMiner(_ab._periodicFrequentPatterns):
     """ 
     :Description:   CPFPMiner algorithm is used to discover the closed periodic frequent patterns in temporal databases.
                     It uses depth-first search.
 
     :Reference:   P. Likhitha et al., "Discovering Closed Periodic-Frequent Patterns in Very Large Temporal Databases"
                   2020 IEEE International Conference on Big Data (Big Data), 2020, https://ieeexplore.ieee.org/document/9378215
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
+                   Name of the output file to store complete set of periodic frequent pattern's
     :param  minSup: float:
                    Controls the minimum number of transactions in which every item must appear in a database.
     :param  maxPer: float:
                    Controls the maximum number of transactions in which any two items within a pattern can reappear.
 
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
@@ -119,20 +125,27 @@
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
 
     **Methods to execute code on terminal**
     --------------------------------------------
-                Format:
-                          >>>  python3 CPFPMiner.py <inputFile> <outputFile> <minSup> <maxPer>
-                Example:
-                          >>>  python3 CPFPMiner.py sampleTDB.txt patterns.txt 0.3 0.4
+    .. code-block:: console
+
+
+       Format:
+
+       (.venv) $  python3 CPFPMiner.py <inputFile> <outputFile> <minSup> <maxPer>
+
+       Example:
+
+       (.venv) $ python3 CPFPMiner.py sampleTDB.txt patterns.txt 0.3 0.4
+
         
-                          .. note:: minSup will be considered in percentage of database transactions
+               .. note:: minSup will be considered in percentage of database transactions
         
         
     **Importing this algorithm into a python program**
     -------------------------------------------------------
     .. code-block:: python
         
                     from PAMI.periodicFrequentPattern.closed import CPFPMiner as alg
@@ -426,22 +439,71 @@
                     classItemSets.append(itemJ)
                     classTidSets.append(y)
             if len(classItemSets) > 0:
                 newPrefix = list(set(itemSetX)) + prefix
                 self._processEquivalenceClass(newPrefix, classItemSets, classTidSets)
             self._save(prefix, list(set(itemSetX)), tidSetX)
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Mining process will start from here
         """
         self._startTime = _ab._time.time()
         self._finalPatterns = {}
         self._hashing = {}
         periodicFrequentItems = self._scanDatabase()
+        for i in range(len(periodicFrequentItems)):
+            itemX = periodicFrequentItems[i]
+            if itemX is None:
+                continue
+            tidSetX = self._tidList[itemX]
+            itemSetX = [itemX]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(periodicFrequentItems)):
+                itemJ = periodicFrequentItems[j]
+                if itemJ is None:
+                    continue
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                if len(y1) < self._minSup:
+                    continue
+                if len(tidSetX) == len(tidSetJ) and len(y1) is len(tidSetX):
+                    periodicFrequentItems.insert(j, None)
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) < len(tidSetJ) and len(y1) is len(tidSetX):
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) > len(tidSetJ) and len(y1) is len(tidSetJ):
+                    periodicFrequentItems.insert(j, None)
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+                else:
+
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            if len(itemSets) > 0:
+                self._processEquivalenceClass(itemSetX, itemSets, tidSets)
+            self._save([], itemSetX, tidSetX)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Closed periodic frequent patterns were generated successfully using CPFPMiner algorithm ")
+
+    def Mine(self):
+        """
+        Mining process will start from here
+        """
+        self._startTime = _ab._time.time()
+        self._finalPatterns = {}
+        self._hashing = {}
+        periodicFrequentItems = self._scanDatabase()
         for i in range(len(periodicFrequentItems)):
             itemX = periodicFrequentItems[i]
             if itemX is None:
                 continue
             tidSetX = self._tidList[itemX]
             itemSetX = [itemX]
             itemSets = []
```

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/closed/__init__.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/closed/abstract.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/cuda/abstract.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py` & `pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/SPAM.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,565 +1,523 @@
-# cuGPFMiner is the fundamental approach to mine the periodic-frequent patterns using GPU.
-#
+# SPAM is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
+# This program employs SPAM property (or downward closure property) to  reduce the search space effectively.
+#  This algorithm employs breadth-first search technique  to find the complete set of frequent patterns in a sequential database.
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     from PAMI.periodicFrequentPattern.basic import cuGPFMiner as alg
 #
-#     obj = alg.cuGPFMiner("../basic/sampleTDB.txt", "2", "5")
+#             import PAMI.sequentialPatternMining.basic.SPAM as alg
+#
+#             obj = alg.SPAM(iFile, minSup)
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     periodicFrequentPatterns = obj.getPatterns()
+#             sequentialPatternMining = obj.getPatterns()
 #
-#     print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#     obj.save("patterns")
+#             obj.save(oFile)
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternInDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
-    This program is free software: you can redistribute it and/or modify
-    it under the terms of the GNU General Public License as published by
-    the Free Software Foundation, either version 3 of the License, or
-    (at your option) any later version.
-
-    This program is distributed in the hope that it will be useful,
-    but WITHOUT ANY WARRANTY; without even the implied warranty of
-    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-    GNU General Public License for more details.
-
-    You should have received a copy of the GNU General Public License
-    along with this program.  If not, see <https://www.gnu.org/licenses/>.
-    Copyright (C)  2021 Rage Uday Kiran
+     This program is free software: you can redistribute it and/or modify
+     it under the terms of the GNU General Public License as published by
+     the Free Software Foundation, either version 3 of the License, or
+     (at your option) any later version.
+
+     This program is distributed in the hope that it will be useful,
+     but WITHOUT ANY WARRANTY; without even the implied warranty of
+     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+     GNU General Public License for more details.
+
+     You should have received a copy of the GNU General Public License
+     along with this program.  If not, see <https://www.gnu.org/licenses/>.
+     Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-import abstract as _ab
 
-class cuGPFMiner(_ab._periodicFrequentPatterns):
+import pandas as pd
+from deprecated import deprecated
+
+from PAMI.sequentialPatternMining.basic import abstract as _ab
+_ab._sys.setrecursionlimit(10000)
+
+class SPAM(_ab._sequentialPatterns):
     """
-    :Description:   cuGPFMiner is the fundamental approach to mine the periodic-frequent patterns using GPU.
+    :Description:    SPAM is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
+                     This program employs SPAM property (or downward closure property) to  reduce the search space effectively.
+                     This algorithm employs breadth-first search technique  to find the complete set of frequent patterns in a sequential database.
 
-    :Reference:   Sreepada, Tarun, et al. "A Novel GPU-Accelerated Algorithm to Discover Periodic-Frequent Patterns in Temporal Databases."
-                  2022 IEEE International Conference on Big Data (Big Data). IEEE, 2022.
+    :Reference:   J. Ayres, J. Gehrke, T.Yiu, and J. Flannick. Sequential Pattern Mining Using Bitmaps. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Edmonton, Alberta, Canada, July 2002.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of  Sequential frequent patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: str:
-                   Controls the minimum number of transactions in which every item must appear in a database.
-    :param  maxPer: str:
-                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
-
+                   Name of the output file to store complete set of  Sequential frequent patterns
+    :param  minSup: float or int or str :
+                    minSup measure constraints the minimum number of transactions in a database where a pattern must appear
+                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
-        iFile : file
-            Name of the Input file or path of the input file
-        oFile : file
-            Name of the output file or path of the output file
-        minSup: int or float or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer: int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
-        sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
-            However, the users can override their default separator.
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        Database : list
-            To store the transactions of a database in list
-        mapSupport : Dictionary
-            To maintain the information of item and their frequency
-        lno : int
-            it represents the total no of transactions
-        tree : class
-            it represents the Tree class
-        itemSetCount : int
-            it represents the total no of patterns
-        finalPatterns : dict
-            it represents to store the patterns
-        tidList : dict
-            stores the timestamps of an item
-        hashing : dict
-            stores the patterns with their support to check for the closed property
+            iFile : str
+                Input file name or path of the input file
+            oFile : str
+                Name of the output file or the path of output file
+            minSup : float or int or str
+                The user can specify minSup either in count or proportion of database size.
+                If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+                Otherwise, it will be treated as float.
+                Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+            sep : str
+                This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+                However, the users can override their default separator.
+            startTime : float
+                To record the start time of the mining process
+            endTime : float
+                To record the completion time of the mining process
+            finalPatterns : dict
+                Storing the complete set of patterns in a dictionary variable
+            memoryUSS : float
+                To store the total amount of USS memory consumed by the program
+            memoryRSS : float
+                To store the total amount of RSS memory consumed by the program
+            Database : list
+                To store the sequences of a database in list
+            _idDatabase : dict
+                To store the sequences of a database by bit map
+            _maxSeqLen:
+                the maximum length of subsequence in sequence.
 
     :Methods:
 
-        startMine()
-            Mining process will start from here
-        getPatterns()
-            Complete set of patterns will be retrieved with this function
-        save(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to an output file
-        getPatternsAsDataFrame()
-            Complete set of periodic-frequent patterns will be loaded in to a dataframe
-        getMemoryUSS()
-            Total amount of USS memory consumed by the mining process will be retrieved from this function
-        getMemoryRSS()
-            Total amount of RSS memory consumed by the mining process will be retrieved from this function
-        getRuntime()
-            Total amount of runtime taken by the mining process will be retrieved from this function
+            _creatingItemSets():
+                Storing the complete sequences of the database/input file in a database variable
+            _convert(value):
+                To convert the user specified minSup value
+            make2BitDatabase():
+                To make 1 length frequent patterns by breadth-first search technique   and update Database to sequential database
+            DfsPruning(items,sStep,iStep):
+                the main algorithm of spam. This can search sstep and istep items and find next patterns, its sstep, and its istep. And call this function again by using them. Recursion until there are no more items available for exploration.
+            Sstep(s):
+                To convert bit to ssteo bit.The first time you get 1, you set it to 0 and subsequent ones to 1.(like 010101=>001111, 00001001=>00000111)
+            startMine()
+                Mining process will start from here
+            getPatterns()
+                Complete set of patterns will be retrieved with this function
+            savePatterns(oFile)
+                Complete set of frequent patterns will be loaded in to a output file
+            getPatternsAsDataFrame()
+                Complete set of frequent patterns will be loaded in to a dataframe
+            getMemoryUSS()
+                Total amount of USS memory consumed by the mining process will be retrieved from this function
+            getMemoryRSS()
+                Total amount of RSS memory consumed by the mining process will be retrieved from this function
+            getRuntime()
+                Total amount of runtime taken by the mining process will be retrieved from this function
+            candidateToFrequent(candidateList)
+                Generates frequent patterns from the candidate patterns
+            frequentToCandidate(frequentList, length)
+                Generates candidate patterns from the frequent patterns
+
+
+    **Executing the code on terminal**:
+    ----------------------------------------
+    .. code-block:: console
+
 
+       Format:
 
-    **Methods to execute code on terminal**
-    -----------------------------------------
-            Format:
-                        >>>  python3 PFECLAT.py <inputFile> <outputFile> <minSup>
+       (.venv) $ python3 SPAM.py <inputFile> <outputFile> <minSup> (<separator>)
 
-            Example:
-                        >>>   python3 PFECLAT.py sampleDB.txt patterns.txt 10.0
+       Examples usage:
 
-                       .. note:: minSup will be considered in percentage of database transactions
+       (.venv) $ python3 SPAM.py sampleDB.txt patterns.txt 10.0
 
-    **Importing this algorithm into a python program**
-    -----------------------------------------------------
-    .. code-block:: python
 
-                from PAMI.periodicFrequentPattern.basic import cuGPFMiner as alg
+               .. note:: minSup will be considered in times of minSup and count of database transactions
 
-                obj = alg.cuGPFMiner("../basic/sampleTDB.txt", "2", "5")
+    **Sample run of the importing code**:
+    -------------------------------------
+            import PAMI.sequentialPatternMining.basic.SPAM as alg
 
-                obj.startMine()
+            obj = alg.SPAM(iFile, minSup)
 
-                periodicFrequentPatterns = obj.getPatterns()
+            obj.startMine()
 
-                print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+            sequentialPatternMining = obj.getPatterns()
 
-                obj.save("patterns")
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-                Df = obj.getPatternsAsDataFrame()
+            obj.savePatterns(oFile)
 
-                memUSS = obj.getMemoryUSS()
+            Df = obj.getPatternInDataFrame()
 
-                print("Total Memory in USS:", memUSS)
+            memUSS = obj.getMemoryUSS()
 
-                memRSS = obj.getMemoryRSS()
+            print("Total Memory in USS:", memUSS)
 
-                print("Total Memory in RSS", memRSS)
+            memRSS = obj.getMemoryRSS()
 
-                run = obj.getRuntime()
+            print("Total Memory in RSS", memRSS)
 
-                print("Total ExecutionTime in seconds:", run)
+            run = obj.getRuntime()
 
-    **Credits:**
-    ---------------
-            The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+            print("Total ExecutionTime in seconds:", run)
 
+    **Credits**:
+    ------------
+            The complete program was written by Shota Suzuki  under the supervision of Professor Rage Uday Kiran.
     """
-    
+
+    _minSup = float()
+    _startTime = float()
+    _endTime = float()
+    _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    _dbSize = None
-    _Database = None
-    _minSup = str()
-    _maxPer = str()
-    _tidSet = set()
-    _finalPatterns = {}
-    _startTime = None
-    _endTime = None
     _memoryUSS = float()
     _memoryRSS = float()
-
-
-    supportAndPeriod = _ab._cp.RawKernel('''
-                  
-    #define uint32_t unsigned int
-    
-    extern "C" __global__
-    void supportAndPeriod(
-        uint32_t *bitValues, uint32_t arraySize,
-        uint32_t *candidates, uint32_t numberOfKeys, uint32_t keySize,
-        uint32_t *support, uint32_t *period,
-        uint32_t maxPeriod, uint32_t maxTimeStamp
-        )
-    {
-        uint32_t tid = threadIdx.x + blockIdx.x * blockDim.x;
-        if (tid >= numberOfKeys) return;
-
-        uint32_t intersection = 0;
-
-        uint32_t supportCount = 0;
-        uint32_t periodCount = 0;
-        uint32_t traversed = 0;
-
-        uint32_t bitRepr[32];
-        uint32_t bitRepIndex = 0;
-
-
-        for (uint32_t i = 0; i < arraySize; i++)
-        {
-            intersection = 0xFFFFFFFF;
-            for (uint32_t j = tid * keySize; j < (tid + 1) * keySize; j++)
-            { 
-                intersection = intersection & bitValues[candidates[j] * arraySize + i];
-            }
-
-            // reset bitRepr
-            for (uint32_t j = 0; j < 32; j++)
-            {
-                bitRepr[j] = 0;
-            }
-
-            // convert intersection to bitRepr
-            bitRepIndex = 31;
-            while (intersection > 0)
-            {
-                bitRepr[bitRepIndex] = intersection % 2;
-                intersection = intersection / 2;
-                bitRepIndex--;   
-            }
-
-            for (uint32_t j = 0; j < 32; j++)
-            {
-                periodCount++;
-                traversed++;
-                if (periodCount > maxPeriod)
-                {
-                    period[tid] = periodCount;
-                    support[tid] = supportCount;
-                    return;
-                }
-                if (bitRepr[j] == 1)
-                {
-                    supportCount++;
-                    if (periodCount > period[tid]) period[tid] = periodCount;
-                    periodCount = 0;
-                }
-                if (traversed == maxTimeStamp + 1)
-                {
-                    support[tid] = supportCount;
-                    if (periodCount > period[tid]) period[tid] = periodCount;
-                    return;
-                }
-            }
-        }
-
-    }
-    
-    ''', 'supportAndPeriod')
-
-    def _convert(self, value):
+    _Database = []
+    _idDatabase={}
+    _maxSeqLen=0
+    def _creatingItemSets(self):
         """
-        To convert the given user specified value
-
-        :param value: user specified value
-        :return: converted value
+        Storing the complete sequences of the database/input file in a database variable
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (self._dbSize * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._dbSize * value)
-            else:
-                value = int(value)
-        return value
+        self._Database = []
 
-    def _creatingOneItemSets(self):
-        """
-        Storing the complete transactions of the database/input file in a database variable
-        """
-        plist = []
-        Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            ts, data = [], []
+            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            for i in range(len(data)):
-                tr = [ts[i][0]]
-                tr = tr + data[i]
-                Database.append(tr)
+                temp = self._iFile['Transactions'].tolist()
+            if "tid" in i:
+                temp2=self._iFile[''].tolist()
+            addList=[]
+            addList.append(temp[0])
+            for k in range(len(temp)-1):
+                if temp2[k]==temp[k+1]:
+                    addList.append(temp[k+1])
+                else:
+                    self._Database.append(addList)
+                    addList=[]
+                    addList.append(temp[k+1])
+            self._Database.append(addList)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    Database.append(temp)
+                    temp.pop()
+                    self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            Database.append(temp)
+                            temp = [i.rstrip() for i in line.split('-1')]
+                            temp = [x for x in temp if x ]
+                            temp.pop()
+
+                            seq = []
+                            for i in temp:
+                                k = -2
+                                if len(i)>1:
+                                    seq.append(list(sorted(set(i.split()))))
+
+                                else:
+                                    seq.append(i)
+
+                            self._Database.append(seq)
+
                 except IOError:
                     print("File Not Found")
                     quit()
 
-        ArraysAndItems = {}
+    def _convert(self, value):
+        """
+        To convert the user specified minSup value
+
+        :param value: user specified minSup value
+        :return: converted type
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
 
-        maxTID = 0
-        for i in range(len(Database)):
-            tid = int(Database[i][0])
-            for j in Database[i][1:]:
-                j = tuple([j])
-                if j not in ArraysAndItems:
-                    ArraysAndItems[j] = [tid]
-                else:
-                    ArraysAndItems[j].append(tid)
-                maxTID = max(maxTID, tid)
-        
-        self._maxTS = maxTID
-
-        newArraysAndItems = {}
-
-        arraySize = maxTID // 32 + 1 if maxTID % 32 != 0 else maxTID // 32
-        self.arraySize = arraySize
-
-        self._rename = {}
-        number = 0
-
-
-        for k,v in ArraysAndItems.items():
-            if len(v) >= self._minSup:
-                nv = v.copy()
-                nv.append(maxTID)
-                nv.append(0)
-                nv = _ab._cp.array(nv, dtype=_ab._np.uint32)
-                nv = _ab._cp.sort(nv)
-                differences = _ab._cp.diff(nv)
-                maxDiff = _ab._cp.max(differences)
-                if maxDiff <= self._maxPer:
-                    # print(k, len(v), v, nv, differences, maxDiff)
-                    self._finalPatterns["\t".join(k)] = [len(v), maxDiff]
-                    # newArraysAndItems[k] = _ab._np.array(v, dtype=_ab._np.uint32)
-                    bitRep = _ab._np.zeros(arraySize, dtype=_ab._np.uint32)
-                    for i in range(len(v)):
-                        bitRep[v[i] // 32] |= 1 << 31 - (v[i] % 32)
-                    # print(k,v, end = " ")
-                    # for i in range(len(bitRep)):
-                    #     print(_ab._np.binary_repr(bitRep[i], width=32), end = " ")
-                    # print()
-                    newArraysAndItems[tuple([number])] = bitRep
-                    self._rename[number] = str(k[0])
-                    number += 1
-
-        return newArraysAndItems
-    
-    
 
-    def startMine(self):
+    def make2BitDatabase(self):
         """
-        Mining process will start from here
+        To make 1 length frequent patterns by breadth-first search technique   and update Database to sequential database
         """
+        self._maxSeqLen=max([len(i) for i in self._Database])
+        lineNumber=0
+        idDatabase={}
+        for line in self._Database:
+            seqNumber=1
+            for seq in line:
+
+                for data in seq:
+                    if data in idDatabase:
+                        while lineNumber+1!=len(idDatabase[data]):
+                            idDatabase[data].append(0)
+                        idDatabase[data][lineNumber]+=int(2**(self._maxSeqLen-seqNumber))
+
+                    else:
+                        idDatabase[data]=[]
+                        while lineNumber+1!=len(idDatabase[data]):
+                            idDatabase[data].append(0)
+                        idDatabase[data][lineNumber]+=(int(2 ** (self._maxSeqLen-seqNumber)))
+
+                seqNumber+=1
+            lineNumber+=1
+        for key,val in idDatabase.items():
+
+            sup=self.countSup(val)
+            while lineNumber+1!=len(idDatabase[key]):
+                            idDatabase[key].append(0)
+            if sup>=self._minSup:
+                self._finalPatterns[str(key)+self._sep+"-2"]=sup
+                self._idDatabase[str(key)]=val
+
+    def DfsPruning(self,items,sStep,iStep):
+        """
+        the main algorithm of spam. This can search sstep and istep items and find next patterns, its sstep, and its istep. And call this function again by using them. Recursion until there are no more items available for exploration.
+
+        :Attributes:
+
+        items : str
+            The pattrens I got before
+        sStep : list
+            Items presumed to have "sstep" relationship with "items".(sstep is What appears later like a-b and a-c)
+        iStep : list
+            Items presumed to have "istep" relationship with "items"(istep is What appears in same time like ab and ac)
+
+        """
+        Snext=[]
+        Inext=[]
+        ns = self.Sstep(self._idDatabase[items])
+        for i in sStep:
+            nnext=[]
+            for k in  range(len(self._idDatabase[items])):
+                nandi=ns[k] & self._idDatabase[i][k]
+                nnext.append(nandi)
+
+
+            sup=self.countSup(nnext)
+            if sup>=self._minSup:
+                key=items+self._sep+"-1"+self._sep+i
+                self._finalPatterns[key+self._sep+"-1"+self._sep+"-2"]=sup
+                self._idDatabase[key]=nnext
+                Snext.append(i)
+
+        for i in Snext:
+            key = items+self._sep+"-1"+self._sep+i
+            self.DfsPruning(key,Snext,[k for k in Snext if self._Database.index(i)<self._Database.index(k)])
+        for i in iStep:
+            nnext = []
+
+            for k in range(len(self._idDatabase[items])):
+                nandi = self._idDatabase[items][k] & self._idDatabase[i][k]
+                nnext.append(nandi)
+            sup=self.countSup(nnext)
+            if sup>=self._minSup:
+                key=items+self._sep+str(i)
+                self._finalPatterns[key+self._sep+"-1"+self._sep+"-2"]=sup
+                self._idDatabase[key]=nnext
+                Inext.append(i)
+        for i in Inext:
+            key = items +self._sep +str(i)
+            self.DfsPruning(key,Snext,[k for k in Inext if self._Database.index(i)<self._Database.index(k)])
+
+    def Sstep(self,s):
+        """
+        To convert bit to Sstep bit.The first time you get 1, you set it to 0 and subsequent ones to 1.(like 010101=>001111, 00001001=>00000111)
+
+
+        :param s:list
+            to store each bit sequence
+        :return:
+            nextS:list to store the bit sequence converted by sstep
+
+        """
+        nextS=[]
+        for bins in s:
+            binS=str(bin(bins))
+
+
+            LenNum=2
+            for i in range(len(binS)-2):
+                if binS[LenNum] == "1":
+
+                    binS = binS[:LenNum] + "0" + binS[LenNum + 1:]
+                    while len(binS)-1!=LenNum:
+                        LenNum += 1
+                        binS = binS[:LenNum] + "1" + binS[LenNum + 1:]
+                    break
+                LenNum+=1
+            nextS.append(int(binS, 0))
+
+
+        return nextS
+
+    def countSup(self,n):
+        """
+        count support
+
+        :param n:list
+                to store each bit sequence
+        :return:
+            count: int support of this list
+        """
+        count=0
+        for i in n:
+            if "1" in str(bin(i)):
+                count+=1
+        return count
 
+    def startMine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        self._Database = []
         self._startTime = _ab._time.time()
-        self._finalPatterns = {}
-        ArraysAndItems = self._creatingOneItemSets()
-        candidates = list(ArraysAndItems.keys())
-        candidates = [list(i) for i in candidates]
-        values = list(ArraysAndItems.values())
-
-        values = _ab._cp.array(values)
-        # print(values)
-
-        # print(type(candidates[0]))
-
-        while len(candidates) > 0:
-            newKeys = []
-            for i in range(len(candidates)):
-                for j in range(i+1, len(candidates)):
-                        if candidates[i][:-1] == candidates[j][:-1] and candidates[i][-1] != candidates[j][-1]:
-                            newKeys.append(candidates[i] + candidates[j][-1:])
-                        else:
-                            break
-
-            if len(newKeys) == 0:
-                break
-
-            # print(newKeys)
-
-            numberOfKeys = len(newKeys)
-            keySize = len(newKeys[0])
-
-            newKeys = _ab._cp.array(newKeys, dtype=_ab._cp.uint32)
-
-            # newKeys = _ab._cp.flatten(newKeys)
-            newKeys = _ab._cp.reshape(newKeys, (numberOfKeys * keySize,))
-
-            support = _ab._cp.zeros(numberOfKeys, dtype=_ab._cp.uint32)
-            period = _ab._cp.zeros(numberOfKeys, dtype=_ab._cp.uint32)
-
-            self.supportAndPeriod((numberOfKeys//32 + 1,), (32,),
-                                    (
-                                        values, self.arraySize,
-                                        newKeys, numberOfKeys, keySize,
-                                        support, period,
-                                        self._maxPer, self._maxTS
-                                    )
-            )
-
-            newKeys = _ab._cp.reshape(newKeys, (numberOfKeys, keySize))
-            newKeys = _ab._cp.asnumpy(newKeys)
-            support = support.get()
-            period = period.get()
-
-            newCandidates = []
-            for i in range(len(newKeys)):
-                # print(newKeys[i], support[i], period[i])
-                if support[i] >= self._minSup and period[i] <= self._maxPer:
-                    newCandidates.append(list(newKeys[i]))
-                    rename = [self._rename[j] for j in newKeys[i]]
-                    rename = "\t".join(rename)
-                    self._finalPatterns[rename] = [support[i], period[i]]
-
-            # print()
-
-            # print(newCandidates)
-
-            candidates = newCandidates
-            
-
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self.make2BitDatabase()
+        self._Database = [i for i in self._idDatabase.keys()]
+        for i in self._Database:
+            x=[]
+            for j in self._Database:
+                if self._Database.index(i)<self._Database.index(j):
+                    x.append(j)
 
+            self.DfsPruning(i,self._Database,x)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
         self._memoryUSS = float()
+        self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Periodic-Frequent patterns were generated successfully using cuGPFMiner algorithm ")
+        print("Frequent patterns were generated successfully using Apriori algorithm ")
 
     def getMemoryUSS(self):
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """Storing final periodic-frequent patterns in a dataframe
-
-        :return: returning periodic-frequent patterns in a dataframe
+        """Storing final frequent patterns in a dataframe
+        :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataframe = {}
+        dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
-        return dataframe
+            data.append([a, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataFrame
 
     def save(self, outFile):
-        """Complete set of periodic-frequent patterns will be loaded in to an output file
-
+        """Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
-        :type outFile: csv file
+        :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            # print(x,y)
-            # print(type(x), type(y))
-            s1 = x.replace(' ', '\t') + ":" + str(y[0]) + ":" + str(y[1])
+            s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
-        """ Function to send the set of periodic-frequent patterns after completion of the mining process
-
-        :return: returning periodic-frequent patterns
+        """ Function to send the set of frequent patterns after completion of the mining process
+        :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
-                    
+        print("Total ExecutionTime in ms:", self.getRuntime())
+
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = cuGPFMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = cuGPFMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = SPAM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = SPAM(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _Patterns = _ap.getPatterns()
+        print("Total number of Frequent Patterns:", len(_Patterns))
+        _ap.savePatterns(_ab._sys.argv[2])
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
-        print("Error! The number of input parameters do not match the total number of parameters provided")
-
-
-    _ap = cuGPFMiner("/home/tarun/Temporal_T10I4D100K.csv", 50, 10000, "\t")
-    # _ap = cuGPFMiner("/home/tarun/PAMI/PAMI/periodicFrequentPattern/cuda/test.txt", 1, 10, " ")
-
-    _ap.startMine()
-    print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
-    _ap.save("tarun.txt")
-    print("Total Memory in USS:", _ap.getMemoryUSS())
-    print("Total Memory in RSS", _ap.getMemoryRSS())
-    print("Total ExecutionTime in ms:", _ap.getRuntime())
 
+        print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from cudaAlgorithms import gPFMinerBit
+#             from cudaAlgorithms import gPFMinerBit
 #
-#     obj = gPFMinerBit.gPFMinerBit("data.txt", 2, 3)
+#             obj = gPFMinerBit.gPFMinerBit("data.txt", 2, 3)
 #
-#     obj.run()
+#             obj.run()
 #
-#     print(obj.getPatterns())
+#             print(obj.getPatterns())
 #
-#     print(obj.getRuntime())
+#             print(obj.getRuntime())
 #
-#     print(obj.getMemoryRSS())
+#             print(obj.getMemoryRSS())
 #
-#     print(obj.getMemoryUSS())
+#             print(obj.getMemoryUSS())
 #
-#     print(obj.getGPUMemory())
+#             print(obj.getGPUMemory())
 #
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
@@ -47,14 +47,19 @@
 import time
 import psutil
 import numpy as np
 import pycuda.autoinit
 import pycuda.driver as cuda
 from pycuda.compiler import SourceModule
 
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
+
 supportAndPeriod = SourceModule(r"""
 
 __global__ void supportAndPeriod(unsigned long long int *bitArray, // containing transactions
                                 unsigned long long int *support, // for support
                                 unsigned long long int *period, // for period
                                 unsigned long long int *thingsToCompare, // for things to compare
                                 unsigned long long int *thingsToCompareIndex, // for things to compare index
@@ -132,14 +137,29 @@
     """
     :Description:   ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
                     This algorithm applies ECLAT as well as calculates periodicity to find patterns in a temporal database.
                     This program employs downward closure property to  reduce the search space effectively.
                     This algorithm employs depth-first search technique to find the complete set of frequent patterns in a
                     temporal database.
 
+    :Reference:   Penugonda Ravikumar 1,2,† , Palla Likhitha 2,† , Bathala Venus Vikranth Raj 2,† , Rage Uday Kiran 1,3,* ,† , Yutaka Watanobe 1 and Koji Zettsu 3. "Efficient Discovery of Periodic-Frequent Patterns in Columnar Temporal Databases.https://www.mdpi.com/2079-9292/10/12/1478
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of periodic frequent pattern's
+    :param  oFile: str :
+                   Name of the output file to store complete set of periodic frequent pattern's
+    :param  minSup: str:
+                   Controls the minimum number of transactions in which every item must appear in a database.
+    :param  maxPer: str:
+                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
+
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
 
         filePath : str
              path of the file
 
         minSup : int
              minimum support
@@ -190,16 +210,23 @@
 
          print(obj.getMemoryUSS())
 
          print(obj.getGPUMemory())
 
     **Running from the command line:**
     ---------------------------------------
+   .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 gPFMinerBit.py data.txt 2 3 output.txt
+
 
-                >>> python3 gPFMinerBit.py data.txt 2 3 output.txt
+               .. note:: minSup will be considered in percentage of database transactions
 
     **Credits:**
     ---------------
         This program is created by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
         
     """
 
@@ -325,22 +352,43 @@
         bitValues = np.array(bitValues, dtype=np.uint64)
         # print(bitValues[0:10])
         gpuBitArray = cuda.mem_alloc(bitValues.nbytes)
         self.bvnb = bitValues.nbytes
         cuda.memcpy_htod(gpuBitArray, bitValues)
         return gpuBitArray, index2id
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Start the mining process
         """
         startTime = time.time()
         data = self.__readFile()
         bitValues, index2id = self.__generateBitArray(data)
 
+        keys = [[i] for i in range(len(index2id))]
+
+        if len(keys) > 1:
+            self.__eclat(bitValues, keys, index2id)
+
+        print(
+            "Periodic-Frequent patterns were generated successfully using gPFMinerBit"
+        )
+        self.__time = time.time() - startTime
+        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
+        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
+
+    def Mine(self):
+        """
+        Start the mining process
+        """
+        startTime = time.time()
+        data = self.__readFile()
+        bitValues, index2id = self.__generateBitArray(data)
+
         keys = [[i] for i in range(len(index2id))]
 
         if len(keys) > 1:
             self.__eclat(bitValues, keys, index2id)
 
         print(
             "Periodic-Frequent patterns were generated successfully using gPFMinerBit"
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,35 +1,36 @@
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     from PAMI.periodicFrequentPattern.maximal import MaxPFGrowth as alg
+#             from PAMI.periodicFrequentPattern.maximal import MaxPFGrowth as alg
 #
-#     obj = alg.MaxPFGrowth("../basic/sampleTDB.txt", "2", "6")
+#             obj = alg.MaxPFGrowth("../basic/sampleTDB.txt", "2", "6")
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     Patterns = obj.getPatterns()
+#             Patterns = obj.getPatterns()
 #
-#     print("Total number of Frequent Patterns:", len(Patterns))
+#             print("Total number of Frequent Patterns:", len(Patterns))
 #
-#     obj.save("patterns")
+#             obj.save("patterns")
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
 #
-#     print("Total ExecutionTime in seconds:", run)
 
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
@@ -48,14 +49,18 @@
 
 """
 
 
 from PAMI.periodicFrequentPattern.maximal import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
 #global maximalTree
 _minSup = float()
 _maxPer = float()
 _lno = int()
 
 
 class _Node(object):
@@ -174,14 +179,15 @@
         return finalPatterns, finalSets, info
 
     def removeNode(self, nodeValue: Any) -> None:
         """
         removes the leaf node by pushing its timestamps to parent node
 
         :param nodeValue: node of a tree
+        :return: None
         """
         for i in self.summaries[nodeValue]:
             i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
             del i.parent.children[nodeValue]
             i = None
 
     def getTimeStamps(self, alpha: Any) -> List[int]:
@@ -388,17 +394,17 @@
                     patterns in a temporal database.
 
     :Reference:   R. Uday Kiran, Yutaka Watanobe, Bhaskar Chaudhury, Koji Zettsu, Masashi Toyoda, Masaru Kitsuregawa,
                  "Discovering Maximal Periodic-Frequent Patterns in Very Large Temporal Databases",
                  IEEE 2020, https://ieeexplore.ieee.org/document/9260063
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
+                   Name of the output file to store complete set of periodic frequent pattern's
     :param  minSup: str:
                    Controls the minimum number of transactions in which every item must appear in a database.
     :param  maxPer: float:
                    Controls the maximum number of transactions in which any two items within a pattern can reappear.
 
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
@@ -468,19 +474,27 @@
         buildTree()
             after updating the Databases ar added into the tree by setting root node as null
         startMine()
             the main method to run the program
 
     **Executing the code on terminal:**
     -------------------------------------
-            Format:
-                    >>> python3 maxpfrowth.py <inputFile> <outputFile> <minSup> <maxPer>
+    .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 maxpfrowth.py <inputFile> <outputFile> <minSup> <maxPer>
+
+       Examples usage :
+
+       (.venv) $ python3 maxpfrowth.py sampleTDB.txt patterns.txt 0.3 0.4
 
-            Examples:
-                    >>> python3 maxpfrowth.py sampleTDB.txt patterns.txt 0.3 0.4
+
+               .. note:: minSup will be considered in percentage of database transactions
             
     **Sample run of the imported code:**
     ------------------------------------------
     .. code-block:: python
 
             from PAMI.periodicFrequentPattern.maximal import MaxPFGrowth as alg
 
@@ -533,14 +547,15 @@
     def __init__(self, iFile: Any, minSup: Union[int, float, str], maxPer: Union[int, float, str], sep: str='\t') -> None:
         super().__init__(iFile, minSup, maxPer, sep)
 
     def _creatingItemSets(self) -> None:
         """
         Storing the complete Databases of the database/input file in a database variable
         :rtype: storing transactions into Database variable
+        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
@@ -663,17 +678,19 @@
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         Mining process will start from this function
+        :return: None
         """
 
         global _minSup, _maxPer, _lno
         self._patterns = {}
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
@@ -704,14 +721,57 @@
         _process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = _process.memory_full_info().uss
         self._memoryRSS = _process.memory_info().rss
         print("Maximal Periodic Frequent patterns were generated successfully using MAX-PFPGrowth algorithm ")
 
+    def Mine(self) -> None:
+        """
+        Mining process will start from this function
+        :return: None
+        """
+
+        global _minSup, _maxPer, _lno
+        self._patterns = {}
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        _minSup, _maxPer, _lno = self._minSup, self._maxPer, len(self._Database)
+        if self._minSup > len(self._Database):
+            raise Exception("Please enter the minSup in range between 0 to 1")
+        _generatedItems = self._periodicFrequentOneItem()
+        _updatedDatabases = self._updateDatabases(_generatedItems)
+        for x, y in self._rank.items():
+            self._rankedUp[y] = x
+        _info = {self._rank[k]: v for k, v in _generatedItems.items()}
+        _Tree = self._buildTree(_updatedDatabases, _info)
+        self._finalPatterns = {}
+        self._maximalTree = _MPTree()
+        _Tree.generatePatterns([], self._patterns, self._maximalTree)
+        for x, y in self._patterns.items():
+            pattern = str()
+            x = self._savePeriodic(x)
+            for i in x:
+                pattern = pattern + i + " "
+            self._finalPatterns[pattern] = y
+        self._endTime = _ab._time.time()
+        _process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = _process.memory_full_info().uss
+        self._memoryRSS = _process.memory_info().rss
+        print("Maximal Periodic Frequent patterns were generated successfully using MAX-PFPGrowth algorithm ")
+
+
     def getMemoryUSS(self) -> float:
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -753,14 +813,15 @@
 
     def save(self, outFile: str) -> None:
         """
         Complete set of periodic-frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.replace(' ', '\t').strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
```

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/maximal/__init__.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/maximal/abstract.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/pyspark/abstract.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,36 +1,39 @@
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.periodicFrequentPattern.basic import parallelPFPGrowth as alg
+#             from PAMI.periodicFrequentPattern.basic import parallelPFPGrowth as alg
 #
-#     obj = alg.parallelPFPGrowth(iFile, minSup, maxPer, noWorkers)
+#             obj = alg.parallelPFPGrowth(iFile, minSup, maxPer, noWorkers)
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     periodicFrequentPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#     print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#     obj.savePatterns(oFile)
+#             obj.savePatterns(oFile)
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
+#
+
+
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -46,14 +49,18 @@
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.periodicFrequentPattern.pyspark import abstract as _ab
 from pyspark import SparkContext, SparkConf
 
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
 _maxPer = float()
 _minSup = float()
 _lno = int()
 
 
 class Node(object):
     """
@@ -308,17 +315,17 @@
 class parallelPFPGrowth(_ab._periodicFrequentPatterns):
     """
     :Description:   ParallelPFPGrowth is one of the fundamental distributed algorithm to discover periodic-frequent patterns in a transactional database. It is based PySpark framework.
 
     :Reference:   C. Saideep, R. Uday Kiran, Koji Zettsu, Cheng-Wei Wu, P. Krishna Reddy, Masashi Toyoda, Masaru Kitsuregawa: Parallel Mining of Partial Periodic Itemsets in Big Data. IEA/AIE 2020: 807-819
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
+                   Name of the output file to store complete set of periodic frequent pattern's
     :param  minSup: str:
                    Controls the minimum number of transactions in which every item must appear in a database.
     :param  maxPer: str:
                    Controls the maximum number of transactions in which any two items within a pattern can reappear.
 
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
@@ -391,21 +398,28 @@
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
 
 
     **Methods to execute code on terminal**
     --------------------------------------------
-            Format:
-                      >>>  python3 parallelPFPGrowth.py <inputFile> <outputFile> <minSup> <maxPer> <noWorker>
+   .. code-block:: console
+
+
+       Format:
 
-            Example:
-                      >>>  python3 parallelPFPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4 5
 
-                      .. note:: minSup will be considered in percentage of database transactions
+       (.venv) $ python3 parallelPFPGrowth.py <inputFile> <outputFile> <minSup> <maxPer> <noWorker>
+
+       Example usage :
+
+       (.venv) $ python3 parallelPFPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4 5
+
+
+               .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     -----------------------------------------------------
     .. code-block:: python
 
                 from PAMI.periodicFrequentPattern.basic import parallelPFPGrowth as alg
@@ -603,14 +617,15 @@
             if '.' in value:
                 value = float(value)
                 value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Start the mining process
 
         """
         self.__startTime = _ab._time.time()
 
@@ -635,14 +650,47 @@
         self.__endTime = _ab._time.time()
         self.__memoryUSS = float()
         self.__memoryRSS = float()
         process = _ab._psutil.Process(_ab._os.getpid())
         self.__memoryUSS = process.memory_full_info().uss
         self.__memoryRSS = process.memory_info().rss
 
+    def Mine(self):
+        """
+        Start the mining process
+
+        """
+        self.__startTime = _ab._time.time()
+
+        APP_NAME = "parallelPFPGrowth"
+        conf = _ab.SparkConf().setAppName(APP_NAME)
+        # conf = conf.setMaster("local[*]")
+        sc = _ab.SparkContext(conf=conf).getOrCreate()
+        # sc = SparkContext.getOrCreate();
+        data = sc.textFile(self._iFile, minPartitions=self._numWorkers).map(
+            lambda x: [int(y) for y in x.strip().split(self._sep)])
+        # data = sc.textFile(finput).map(lambda x: [int(y) for y in x.strip().split(' ')])
+        data.cache()
+        # minSupport = data.count() * threshold/100
+        # maxPer = data.count() * periodicity_threshold/100
+        self._minSup = self.__convert(self._minSup)
+        self._maxPer = self.__convert(self._maxPer)
+        self._numTrans = sc.broadcast(data.count())
+        self._perFreqItems = self.getFrequentItems(data)
+        freqItemsets = self.getFrequentItemsets(data, self._perFreqItems)
+        self.__finalPatterns = self.__tarunpat
+        sc.stop()
+        self.__endTime = _ab._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
+
     def getMemoryUSS(self):
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
```

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py` & `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,36 +1,43 @@
+# Stable periodic pattern mining aims to discover all interesting patterns in a temporal database using three constraints minimum support,
+# maximum period and maximum liability, that have support no less than the user-specified minimum support  constraint and liability no
+# greater than maximum liability.
+#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     import PAMI.periodicFrequentPattern.topk.TopkPFPGrowth as alg
+#             from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
+#
+#             obj = alg.SPPEclat("../basic/sampleTDB.txt", 5, 3, 3)
 #
-#     obj = alg.TopkPFPGrowth(iFile, k, maxPer)
+#             obj.startMine()
 #
-#     obj.startMine()
+#             Patterns = obj.getPatterns()
 #
-#     periodicFrequentPatterns = obj.getPatterns()
+#             print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
 #
-#     print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+#             obj.save("patterns")
 #
-#     obj.save(oFile)
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     Df = obj.getPatternInDataFrame()
+#             memUSS = obj.getMemoryUSS()
 #
-#     memUSS = obj.getMemoryUSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in USS:", memUSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in RSS", memRSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             run = obj.getRuntime()
 #
-#     run = obj.getRuntime()
+#             print("Total ExecutionTime in seconds:", run)
 #
-#     print("Total ExecutionTime in seconds:", run)
+
+
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -42,164 +49,205 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
+import pandas as pd
+from deprecated import deprecated
 
+from PAMI.stablePeriodicFrequentPattern.basic import abstract as _ab
 
-from PAMI.periodicFrequentPattern.topk import abstract as _ab
-
-
-class TopkPFPGrowth(_ab._periodicFrequentPatterns):
+class SPPEclat(_ab._stablePeriodicFrequentPatterns):
     """
-    :Description:   Top - K is and algorithm to discover top periodic frequent patterns in a temporal database.
+    :Description:   Stable periodic pattern mining aims to dicover all interesting patterns in a temporal database using three contraints minimum support,
+                    maximum period and maximum lability, that have support no less than the user-specified minimum support  constraint and lability no
+                    greater than maximum lability.
 
-    :Reference:   Komate Amphawan, Philippe Lenca, Athasit Surarerks: "Mining Top-K Periodic-Frequent Pattern from Transactional Databases without Support Threshold"
-                  International Conference on Advances in Information Technology: https://link.springer.com/chapter/10.1007/978-3-642-10392-6_3
+    :Reference:   Fournier-Viger, P., Yang, P., Lin, J. C.-W., Kiran, U. (2019). Discovering Stable Periodic-Frequent Patterns in Transactional Data. Proc.
+                  32nd Intern. Conf. on Industrial, Engineering and Other Applications of Applied Intelligent Systems (IEA AIE 2019), Springer LNAI, pp. 230-244
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of stable periodic Frequent Pattern.
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  maxPer: str:
-                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
-
+                   Name of the output file to store complete set of stable periodic Frequent Pattern.
+    :param  minSup: float or int or str :
+                    The user can specify minSup either in count or proportion of database size.
+                    If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+                    Otherwise, it will be treated as float.
+                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+    :param  itemSup: int or float :
+                    Frequency of an item
+    :param maxLa: float :
+                  minimum loss of a pattern
     :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+                 This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
 
     :Attributes:
 
-        iFile : str
-            Input file name or path of the input file
-        k: int
-            User specified counte of top frequent patterns
+        iFile : file
+            Name of the Input file or path of the input file
+        oFile : file
+            Name of the output file or path of the output file
+        minSup : int or float or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        maxPer : int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+        maxLa : int or float or str
+            The user can specify maxLa either in count or proportion of database size.
+            If the program detects the data type of maxLa is integer, then it treats maxLa is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxLa=10 will be treated as integer, while maxLa=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        oFile : str
-            Name of the output file or the path of the output file
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        finalPatterns: dict
-            Storing the complete set of patterns in a dictionary variable
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
+        Database : list
+            To store the transactions of a database in list
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+        lno : int
+            it represents the total no of transactions
+        tree : class
+            it represents the Tree class
+        itemSetCount : int
+            it represents the total no of patterns
+        finalPatterns : dict
+            it represents to store the patterns
+        tidList : dict
+            stores the timestamps of an item
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+            Complete set of periodic-frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         creatingItemSets()
-            Scans the dataset or dataframes and stores in list format
-        frequentOneItem()
-            Generates one frequent patterns
-        eclatGeneration(candidateList)
-            It will generate the combinations of frequent items
-        generateFrequentPatterns(tidList)
-            It will generate the combinations of frequent items from a list of items
+            Scan the database and store the items with their timestamps which are periodic frequent
+        calculateLa()
+            Calculates the support and period for a list of timestamps.
+        Generation()
+            Used to implement prefix class equivalence method to generate the periodic patterns recursively
+
+
+
+    **Methods to execute code on terminal**
+    -----------------------------------------
+    .. code-block:: console
 
-    **Executing the code on terminal:**
-    -------------------------------------
-            Format:
-                >>> python3 TopkPFP.py <inputFile> <outputFile> <k> <maxPer>
 
-            Examples:
-                >>> python3 TopkPFP.py sampleDB.txt patterns.txt 10 3
+       Format:
 
-    **Sample run of the importing code:**
-    ---------------------------------------
-    .. code-block:: python
+       (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup> <maxPer> <maxLa>
 
-            import PAMI.periodicFrequentPattern.topk.TopkPFPGrowth as alg
+       Example usage:
 
-            obj = alg.TopkPFPGrowth(iFile, k, maxPer)
+       (.venv) $ python3 basic.py sampleDB.txt patterns.txt 10.0 4.0 2.0
 
-            obj.startMine()
 
-            periodicFrequentPatterns = obj.getPatterns()
+               .. note:: constraints will be considered in percentage of database transactions
 
-            print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+    **Importing this algorithm into a python program**
+    ---------------------------------------------------
+    ... code-block:: python
 
-            obj.save(oFile)
+                    from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
 
-            Df = obj.getPatternInDataFrame()
+                    obj = alg.PFPECLAT("../basic/sampleTDB.txt", 5, 3, 3)
 
-            memUSS = obj.getMemoryUSS()
+                    obj.startMine()
 
-            print("Total Memory in USS:", memUSS)
+                    Patterns = obj.getPatterns()
 
-            memRSS = obj.getMemoryRSS()
+                    print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
 
-            print("Total Memory in RSS", memRSS)
+                    obj.save("patterns")
 
-            run = obj.getRuntime()
+                    Df = obj.getPatternsAsDataFrame()
 
-            print("Total ExecutionTime in seconds:", run)
+                    memUSS = obj.getMemoryUSS()
+
+                    print("Total Memory in USS:", memUSS)
+
+                    memRSS = obj.getMemoryRSS()
+
+                    print("Total Memory in RSS", memRSS)
+
+                    run = obj.getRuntime()
+
+                    print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
     --------------
-            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
-
-    """
+             The complete program was written by  P.Likhitha under the supervision of Professor Rage Uday Kiran.
 
-    _startTime = float()
-    _endTime = float()
-    _k = int()
-    _maxPer = " "
-    _finalPatterns = {}
+       """
     _iFile = " "
     _oFile = " "
+    _minSup = str()
+    _maxPer = str()
+    _maxLa = float()
     _sep = " "
+    _SPPList = {}
+    _itemList = []
+    _last = int()
+    _finalPatterns = {}
+    _tsList = {}
+    _startTime = float()
+    _endTime = float()
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _tidList = {}
-    _lno = int()
-    _minimum = int()
-    _mapSupport = {}
 
-    def _creatingItemSets(self):
+    def __init__(self, inputFile, minSup, maxPer, maxLa, sep='\t'):
+        self._iFile = inputFile
+        self._minSup = minSup
+        self._maxPer = maxPer
+        self._maxLa = maxLa
+        self._sep = sep
+
+    def _creatingItemsets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
-
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
+                self._Database = self._iFile['Transactions'].tolist()
             if 'Patterns' in i:
-                data = self._iFile['Patterns'].tolist()
-            for i in range(len(data)):
-                tr = [ts[i][0]]
-                tr = tr + data[i]
-                self._Database.append(tr)
+                self._Database = self._iFile['Patterns'].tolist()
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
@@ -215,274 +263,218 @@
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _convert(self, value):
         """
-        To convert the given user specified value
+        to convert the type of user specified minSup value
 
-        :param value: user specified value
-        :return: converted value
+        :param value: user specified minSup value
+        :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _frequentOneItem(self):
+    def _createSPPList(self):
         """
-        Generating one frequent patterns
+        to convert the single length stable periodic patterns
         """
-
-        self._mapSupport = {}
-        self._tidList = {}
-        n = 0
-        for line in self._Database:
-            self._lno += 1
-            n = int(line[0])
-            for i in range(1, len(line)):
-                si = line[i]
-                if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [1, abs(0 - n), n]
-                    self._tidList[si] = [n]
+        tidLast = {}
+        la = {}
+        self._SPPList = {}
+        self._tsList = {}
+        for transaction in self._Database:
+            ts = int(transaction[0])
+            for item in transaction[1:]:
+                if item not in self._SPPList:
+                    la[item] = max(0, ts - self._maxPer)
+                    self._SPPList[item] = [1, la[item]]
+                    self._tsList[item] = [ts]
                 else:
-                    self._mapSupport[si][0] += 1
-                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
-                    self._mapSupport[si][2] = n
-                    self._tidList[si].append(n)
-        for x, y in self._mapSupport.items():
-            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(n - self._mapSupport[x][2]))
-        self._maxPer = self._convert(self._maxPer)
-        self._k = self._convert(self._k)
-        self._mapSupport = {k: [v[0], v[1]] for k, v in self._mapSupport.items() if v[1] <= self._maxPer}
-        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
-        self._finalPatterns = {}
-        #print(len(plist))
-        for i in plist:
-            if len(self._finalPatterns) >= self._k:
-                break
-            else:
-                self._finalPatterns[i] = [self._mapSupport[i][0], self._mapSupport[i][1]]
-        self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
-        plist = list(self._finalPatterns.keys())
-        return plist
-
-    def _getSupportAndPeriod(self, timeStamps):
-        """To calculate the periodicity and support
-
-        :param timeStamps: Timestamps of an item set
-        :return: support, periodicity
-        """
-
-        global lno
-        timeStamps.sort()
-        cur = 0
-        per = list()
-        sup = 0
-        for j in range(len(timeStamps)):
-            per.append(timeStamps[j] - cur)
-            cur = timeStamps[j]
-            sup += 1
-        per.append(self._lno - cur)
-        if len(per) == 0:
-            return [0, 0]
-        return [sup, max(per)]
-
-    def _save(self, prefix, suffix, tidSetI):
-        """Saves the patterns that satisfy the periodic frequent property.
-
-        :param prefix: the prefix of a pattern
-        :type prefix: list
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetI: the timestamp of a patterns
-        :type tidSetI: list
-        """
-
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = self._getSupportAndPeriod(tidSetI)
-        sample = str()
-        for i in prefix:
-            sample = sample + i + " "
-        if len(self._finalPatterns) < self._k:
-            if val[0] >= self._minimum:
-                self._finalPatterns[sample] = val
-                self._finalPatterns = {k: v for k, v in
-                                  sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
-        else:
-            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1][0]):
-                if val[0] > y[0]:
-                    del self._finalPatterns[x]
-                    self._finalPatterns[x] = y
-                    self._finalPatterns = {k: v for k, v in
-                                          sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                    self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
-                    return
-
-    def _Generation(self, prefix, itemSets, tidSets):
-        """
-        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-
-        :param prefix:  main equivalence prefix
-        :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity
-                            and frequent with their timestamps
-        :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
-        :type tidSets: list
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
-                continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i + 1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = list(set(tidSetI).intersection(tidSetJ))
-                val = self._getSupportAndPeriod(y)
-                if val[0] >= self._minimum and val[1] <= self._maxPer:
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetI)
+                    s = self._SPPList[item][0] + 1
+                    la[item] = max(0, la[item] + ts - tidLast.get(item) - self._maxPer)
+                    self._SPPList[item] = [s, max(la[item], self._SPPList[item][1])]
+                    self._tsList[item].append(ts)
+                tidLast[item] = ts
+            self._last = ts
+        for item in self._SPPList:
+            la[item] = max(0, la[item] + self._last - tidLast[item] - self._maxPer)
+            self._SPPList[item][1] = max(la[item], self._SPPList[item][1])
+        self._SPPList = {k: v for k, v in self._SPPList.items() if v[0] >= self._minSup and v[1] <= self._maxLa}
+        self._SPPList = {k: v for k, v in sorted(self._SPPList.items(), key=lambda x: x[1][0], reverse=True)}
+        self._Generation(list(self._SPPList), set())
+
+    def _Generation(self, GPPFList, CP):
+        """
+        To generate the patterns using depth-first search
+        """
+        for i in range(len(GPPFList)):
+            item = GPPFList[i]
+            CP1 = CP | {item}
+            if CP != set():
+                self._tsList['\t'.join(CP1)] = list(set(self._tsList['\t'.join(CP)]) & set(self._tsList[item]))
+            la = self._calculateLa(self._tsList['\t'.join(CP1)])
+            support = len(self._tsList['\t'.join(CP1)])
+            if la <= self._maxLa and len(self._tsList['\t'.join(CP1)]) >= self._minSup:
+                #CP = CP1
+                self._finalPatterns['\t'.join(CP1)] = [support, la]
+                if i+1 < len(GPPFList):
+                    self._Generation(GPPFList[i+1:], CP1)
+
+    def _calculateLa(self, tsList):
+        """
+        To calculate the liability of a patterns based on its timestamps
+        """
+        previous = 0
+        la = 0
+        tsList = sorted(tsList)
+        laList = []
+        for ts in tsList:
+            la = max(0, la + ts - previous - self._maxPer)
+            laList.append(la)
+            previous = ts
+            
+        la = max(0, la + self._last - previous - self._maxPer)
+        laList.append(la)
+        maxla = max(laList)
+        return maxla
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
-        Main function of the program
-
+        Method to start the mining of patterns
         """
         self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._k is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        _plist = self._frequentOneItem()
-        for i in range(len(_plist)):
-            itemI = _plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(_plist)):
-                itemJ = _plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetI).intersection(tidSetJ))
-                val = self._getSupportAndPeriod(y1)
-                if val[0] >= self._minimum and val[1] <= self._maxPer:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print("TopK Periodic Frequent patterns were generated successfully")
+        self._creatingItemsets()
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        self._maxLa = self._convert(self._maxLa)
+        self._finalPatterns = {}
+        #print(self._minSup, self._maxPer, self._maxLa)
+        self._createSPPList()
         self._endTime = _ab._time.time()
-        _process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
         self._memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Stable Periodic Frequent patterns were generated successfully using basic algorithm ")
+
+    def Mine(self):
+        """
+        Method to start the mining of patterns
+        """
+        self._startTime = _ab._time.time()
+        self._creatingItemsets()
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        self._maxLa = self._convert(self._maxLa)
+        self._finalPatterns = {}
+        #print(self._minSup, self._maxPer, self._maxLa)
+        self._createSPPList()
+        self._endTime = _ab._time.time()
         self._memoryUSS = float()
-        self._memoryUSS = _process.memory_full_info().uss
-        self._memoryRSS = _process.memory_info().rss
+        self._memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Stable Periodic Frequent patterns were generated successfully using basic algorithm ")
 
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
-        :return: returning USS memory consumed by the mining process
+    def getRuntime(self):
+        """
+        Calculating the total amount of runtime taken by the mining process
+
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
+        return self._endTime - self._startTime
 
-        return self._memoryUSS
+    def getPatterns(self):
+        """
+        Function to return the set of stable periodic-frequent patterns after completion of the mining process
 
-    def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        :return: returning stable periodic-frequent patterns
+        :rtype: dict
+        """
+        return self._finalPatterns
 
-        :return: returning RSS memory consumed by the mining process
+    def getMemoryUSS(self):
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
-
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+        return self._memoryUSS
 
-        :return: returning total amount of runtime taken by the mining process
-        :rtype: float
+    def save(self, outFile):
         """
+        Complete set of periodic-frequent patterns will be loaded in to an output file
 
-        return self._endTime - self._startTime
+        :param outFile: name of the output file
+        :type outFile: csv file
+        """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
+            writer.write("%s \n" % s1)
 
     def getPatternsAsDataFrame(self):
         """
-        Storing final frequent patterns in a dataframe
+        Storing final periodic-frequent patterns in a dataframe
 
-        :return: returning frequent patterns in a dataframe
+        :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
+            data.append([a.replace('\t', ' '), b[0], b[1]])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
-    def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to a output file
-
-        :param outFile: name of the output file
-        :type outFile: file
+    def getMemoryRSS(self):
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.replace(' ', '\t') + ":" + f'{y[0]}:{y[1]}'
-            writer.write("%s \n" % patternsAndSupport)
-
-    def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
-
-        :return: returning frequent patterns
-        :rtype: dict
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        :return: returning RSS memory consumed by the mining process
+        :rtype: float
         """
-        return self._finalPatterns
+
+        return self._memoryRSS
 
     def printResults(self):
-        print("Top K Periodic Frequent Patterns:", len(self.getPatterns()))
+        """
+        This function is used to print the results
+        """
+        print("Total number of Stable Periodic  Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:", self.getRuntime())
 
-
-if __name__ == "__main__":
+if __name__ == '__main__':
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
+        if len(_ab._sys.argv) == 7:
+            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         if len(_ab._sys.argv) == 6:
-            _ap = TopkPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = TopkPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
-        print("Top K Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:",  _ap.getMemoryUSS())
+        print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,387 +1,562 @@
+# cuGPFMiner is the fundamental approach to mine the periodic-frequent patterns using GPU.
+#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     import PAMI.periodicFrequentPattern.kPFPMiner as alg
 #
-#     obj = alg.kPFPMiner(iFile, k)
+#             from PAMI.periodicFrequentPattern.basic import cuGPFMiner as alg
+#
+#             obj = alg.cuGPFMiner("../basic/sampleTDB.txt", "2", "5")
+#
+#             obj.startMine()
 #
-#     obj.startMine()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#     periodicFrequentPatterns = obj.getPatterns()
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#     print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             obj.save("patterns")
 #
-#     obj.save(oFile)
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     Df = obj.getPatternInDataFrame()
+#             memUSS = obj.getMemoryUSS()
 #
-#     memUSS = obj.getMemoryUSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in USS:", memUSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in RSS", memRSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             run = obj.getRuntime()
 #
-#     run = obj.getRuntime()
+#             print("Total ExecutionTime in seconds:", run)
 #
-#     print("Total ExecutionTime in seconds:", run)
+
+
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
-     This program is free software: you can redistribute it and/or modify
-     it under the terms of the GNU General Public License as published by
-     the Free Software Foundation, either version 3 of the License, or
-     (at your option) any later version.
-
-     This program is distributed in the hope that it will be useful,
-     but WITHOUT ANY WARRANTY; without even the implied warranty of
-     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-     GNU General Public License for more details.
-
-     You should have received a copy of the GNU General Public License
-     along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
+    This program is free software: you can redistribute it and/or modify
+    it under the terms of the GNU General Public License as published by
+    the Free Software Foundation, either version 3 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU General Public License for more details.
+
+    You should have received a copy of the GNU General Public License
+    along with this program.  If not, see <https://www.gnu.org/licenses/>.
+    Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 
-from PAMI.periodicFrequentPattern.topk.kPFPMiner import abstract as _ab
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
 
+import abstract as _ab
 
-class kPFPMiner(_ab._periodicFrequentPatterns):
+class cuGPFMiner(_ab._periodicFrequentPatterns):
     """
-    :Description:   Top - K is and algorithm to discover top periodic-frequent patterns in a temporal database.
+    :Description:   cuGPFMiner is the fundamental approach to mine the periodic-frequent patterns using GPU.
 
-    :Reference:   Likhitha, P., Ravikumar, P., Kiran, R.U., Watanobe, Y. (2022).
-                  Discovering Top-k Periodic-Frequent Patterns in Very Large Temporal Databases. Big Data Analytics.
-                 BDA 2022. Lecture Notes in Computer Science, vol 13773. Springer, Cham. https://doi.org/10.1007/978-3-031-24094-2_14
+    :Reference:   Sreepada, Tarun, et al. "A Novel GPU-Accelerated Algorithm to Discover Periodic-Frequent Patterns in Temporal Databases."
+                  2022 IEEE International Conference on Big Data (Big Data). IEEE, 2022.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
+                   Name of the output file to store complete set of periodic frequent pattern's
+    :param  minSup: str or int or float:
+                   Controls the minimum number of transactions in which every item must appear in a database.
+    :param  maxPer: str:
+                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
 
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
-        iFile : str
-            Input file name or path of the input file
-        k: int
-            User specified counte of top-k periodic frequent patterns
+        iFile : file
+            Name of the Input file or path of the input file
+        oFile : file
+            Name of the output file or path of the output file
+        minSup: int or float or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        maxPer: int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        oFile : str
-            Name of the output file or the path of the output file
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        finalPatterns: dict
-            Storing the complete set of patterns in a dictionary variable
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
+        Database : list
+            To store the transactions of a database in list
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+        lno : int
+            it represents the total no of transactions
+        tree : class
+            it represents the Tree class
+        itemSetCount : int
+            it represents the total no of patterns
+        finalPatterns : dict
+            it represents to store the patterns
+        tidList : dict
+            stores the timestamps of an item
+        hashing : dict
+            stores the patterns with their support to check for the closed property
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        savePatterns(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+        save(oFile)
+            Complete set of periodic-frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets()
-            Scans the dataset or dataframes and stores in list format
-        frequentOneItem()
-            Generates one frequent patterns
-        eclatGeneration(candidateList)
-            It will generate the combinations of frequent items
-        generateFrequentPatterns(tidList)
-            It will generate the combinations of frequent items from a list of items
-
-    **Executing the code on terminal:**
-    ------------------------------------------
-            Format:
-                    >>> python3 kPFPMiner.py <inputFile> <outputFile> <k>
 
-            Examples:
-                    >>> python3 kPFPMiner.py sampleDB.txt patterns.txt 10
 
+    **Methods to execute code on terminal**
+    -----------------------------------------
+    .. code-block:: console
+
+
+       Format:
+
+       (.venv) $  python3 PFECLAT.py <inputFile> <outputFile> <minSup>
+
+       Example usage:
 
-    **Sample run of the importing code:
-    --------------------------------------
+       (.venv) $ python3 PFECLAT.py sampleDB.txt patterns.txt 10.0
+
+
+               .. note:: minSup will be considered in percentage of database transactions
+
+    **Importing this algorithm into a python program**
+    -----------------------------------------------------
     .. code-block:: python
 
-            import PAMI.periodicFrequentPattern.kPFPMiner as alg
+                from PAMI.periodicFrequentPattern.basic import cuGPFMiner as alg
 
-            obj = alg.kPFPMiner(iFile, k)
+                obj = alg.cuGPFMiner("../basic/sampleTDB.txt", "2", "5")
 
-            obj.startMine()
+                obj.startMine()
 
-            periodicFrequentPatterns = obj.getPatterns()
+                periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+                print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
-            obj.save(oFile)
+                obj.save("patterns")
 
-            Df = obj.getPatternInDataFrame()
+                Df = obj.getPatternsAsDataFrame()
 
-            memUSS = obj.getMemoryUSS()
+                memUSS = obj.getMemoryUSS()
 
-            print("Total Memory in USS:", memUSS)
+                print("Total Memory in USS:", memUSS)
 
-            memRSS = obj.getMemoryRSS()
+                memRSS = obj.getMemoryRSS()
 
-            print("Total Memory in RSS", memRSS)
+                print("Total Memory in RSS", memRSS)
 
-            run = obj.getRuntime()
+                run = obj.getRuntime()
 
-            print("Total ExecutionTime in seconds:", run)
+                print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    --------------
-            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    ---------------
+            The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
 
     """
-
-    _startTime = float()
-    _endTime = float()
-    _k = int()
-    _finalPatterns = {}
+    
     _iFile = " "
     _oFile = " "
     _sep = " "
+    _dbSize = None
+    _Database = None
+    _minSup = str()
+    _maxPer = str()
+    _tidSet = set()
+    _finalPatterns = {}
+    _startTime = None
+    _endTime = None
     _memoryUSS = float()
     _memoryRSS = float()
-    _Database = []
-    _tidList = {}
-    lno = int()
-    _maximum = int()
 
-    def _creatingItemSets(self):
+
+    supportAndPeriod = _ab._cp.RawKernel('''
+                  
+    #define uint32_t unsigned int
+    
+    extern "C" __global__
+    void supportAndPeriod(
+        uint32_t *bitValues, uint32_t arraySize,
+        uint32_t *candidates, uint32_t numberOfKeys, uint32_t keySize,
+        uint32_t *support, uint32_t *period,
+        uint32_t maxPeriod, uint32_t maxTimeStamp
+        )
+    {
+        uint32_t tid = threadIdx.x + blockIdx.x * blockDim.x;
+        if (tid >= numberOfKeys) return;
+
+        uint32_t intersection = 0;
+
+        uint32_t supportCount = 0;
+        uint32_t periodCount = 0;
+        uint32_t traversed = 0;
+
+        uint32_t bitRepr[32];
+        uint32_t bitRepIndex = 0;
+
+
+        for (uint32_t i = 0; i < arraySize; i++)
+        {
+            intersection = 0xFFFFFFFF;
+            for (uint32_t j = tid * keySize; j < (tid + 1) * keySize; j++)
+            { 
+                intersection = intersection & bitValues[candidates[j] * arraySize + i];
+            }
+
+            // reset bitRepr
+            for (uint32_t j = 0; j < 32; j++)
+            {
+                bitRepr[j] = 0;
+            }
+
+            // convert intersection to bitRepr
+            bitRepIndex = 31;
+            while (intersection > 0)
+            {
+                bitRepr[bitRepIndex] = intersection % 2;
+                intersection = intersection / 2;
+                bitRepIndex--;   
+            }
+
+            for (uint32_t j = 0; j < 32; j++)
+            {
+                periodCount++;
+                traversed++;
+                if (periodCount > maxPeriod)
+                {
+                    period[tid] = periodCount;
+                    support[tid] = supportCount;
+                    return;
+                }
+                if (bitRepr[j] == 1)
+                {
+                    supportCount++;
+                    if (periodCount > period[tid]) period[tid] = periodCount;
+                    periodCount = 0;
+                }
+                if (traversed == maxTimeStamp + 1)
+                {
+                    support[tid] = supportCount;
+                    if (periodCount > period[tid]) period[tid] = periodCount;
+                    return;
+                }
+            }
+        }
+
+    }
+    
+    ''', 'supportAndPeriod')
+
+    def _convert(self, value):
         """
-        Storing the complete transactions of the database/input file in a database variable
+        To convert the given user specified value
+
+        :param value: user specified value
+        :return: converted value
         """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (self._dbSize * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (self._dbSize * value)
+            else:
+                value = int(value)
+        return value
 
-        self._Database = []
+    def _creatingOneItemSets(self):
+        """
+        Storing the complete transactions of the database/input file in a database variable
+        """
+        plist = []
+        Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
+            ts, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-
-            # print(self.Database)
+                data = self._iFile['Transactions'].tolist()
+            for i in range(len(data)):
+                tr = [ts[i][0]]
+                tr = tr + data[i]
+                Database.append(tr)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
-                    
-    def getPer_Sup(self, tids):
-        tids.sort()
-        cur=0
-        per=list()
-        sup=0
-        #print(tids)
-        for i in range(len(tids)-1):
-            j = i + 1
-            #if tids[j] - cur <= periodicity:
-                #return [0,0]
-            per.append(tids[j] - cur)
-            cur = tids[j]
-        per.append(self.lno - cur)
-        return max(per)
-
-    def _frequentOneItem(self):
-        """
-        Generating one frequent patterns
-        """
-        self._mapSupport = {}
-        self._tidList = {}
-        n = 0
-        for line in self._Database:
-            self.lno += 1
-            n = int(line[0])
-            for i in range(1, len(line)):
-                si = line[i]
-                if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [1, abs(0 - n), n]
-                    self._tidList[si] = [n]
-                else:
-                    self._mapSupport[si][0] += 1
-                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
-                    self._mapSupport[si][2] = n
-                    self._tidList[si].append(n)
-        for x, y in self._mapSupport.items():
-            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(n - self._mapSupport[x][2]))
-        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        for i in plist:
-            if len(self._finalPatterns) >= self._k:
-                break
-            else:
-                self._finalPatterns[i] = self._mapSupport[i][1]
-        self._maximum = max([self._finalPatterns[i] for i in self._finalPatterns.keys()])
-        plist = list(self._finalPatterns.keys())
-        return plist
-
-
-    def _save(self, prefix, suffix, tidSetI):
-        """Saves the patterns that satisfy the periodic frequent property.
-
-        :param prefix: the prefix of a pattern
-        :type prefix: list
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetI: the timestamp of a patterns
-        :type tidSetI: list
-        """
-
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = self.getPer_Sup(tidSetI)
-        sample = str()
-        for i in prefix:
-            sample = sample + i + " "
-        if len(self._finalPatterns) < self._k:
-            if val < self._maximum:
-                self._finalPatterns[sample] = val
-                self._finalPatterns = {k: v for k, v in sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                self._maximum = max([i for i in self._finalPatterns.values()])
-        else:
-            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1], reverse=True):
-                if val < y:
-                    del self._finalPatterns[x]
-                    self._finalPatterns[sample] = val
-                    self._finalPatterns = {k: v for k, v in
-                                              sorted(self._finalPatterns.items(), key=lambda item: item[1],
-                                                     reverse=True)}
-                    self._maximum = max([i for i in self._finalPatterns.values()])
-                    return
-
-    def _Generation(self, prefix, itemSets, tidSets):
-        """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-
-        :param prefix:  main equivalence prefix
-        :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
-        :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
-        :type tidSets: list
-
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
-                continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i + 1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = list(set(tidSetI).intersection(tidSetJ))
-                if self.getPer_Sup(y) <= self._maximum:
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetI)
 
-    def _convert(self, value):
-        """
-        to convert the type of user specified minSup value
+        ArraysAndItems = {}
 
-        :param value: user specified minSup value
-        :return: converted type
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = ((len(self._Database)) * value)
-            else:
-                value = int(value)
-        return value
+        maxTID = 0
+        for i in range(len(Database)):
+            tid = int(Database[i][0])
+            for j in Database[i][1:]:
+                j = tuple([j])
+                if j not in ArraysAndItems:
+                    ArraysAndItems[j] = [tid]
+                else:
+                    ArraysAndItems[j].append(tid)
+                maxTID = max(maxTID, tid)
+        
+        self._maxTS = maxTID
+
+        newArraysAndItems = {}
+
+        arraySize = maxTID // 32 + 1 if maxTID % 32 != 0 else maxTID // 32
+        self.arraySize = arraySize
+
+        self._rename = {}
+        number = 0
+
+
+        for k,v in ArraysAndItems.items():
+            if len(v) >= self._minSup:
+                nv = v.copy()
+                nv.append(maxTID)
+                nv.append(0)
+                nv = _ab._cp.array(nv, dtype=_ab._np.uint32)
+                nv = _ab._cp.sort(nv)
+                differences = _ab._cp.diff(nv)
+                maxDiff = _ab._cp.max(differences)
+                if maxDiff <= self._maxPer:
+                    # print(k, len(v), v, nv, differences, maxDiff)
+                    self._finalPatterns["\t".join(k)] = [len(v), maxDiff]
+                    # newArraysAndItems[k] = _ab._np.array(v, dtype=_ab._np.uint32)
+                    bitRep = _ab._np.zeros(arraySize, dtype=_ab._np.uint32)
+                    for i in range(len(v)):
+                        bitRep[v[i] // 32] |= 1 << 31 - (v[i] % 32)
+                    # print(k,v, end = " ")
+                    # for i in range(len(bitRep)):
+                    #     print(_ab._np.binary_repr(bitRep[i], width=32), end = " ")
+                    # print()
+                    newArraysAndItems[tuple([number])] = bitRep
+                    self._rename[number] = str(k[0])
+                    number += 1
+
+        return newArraysAndItems
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
-        Main function of the program
-
+        Mining process will start from here
         """
+
         self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._k is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        self._k = self._convert(self._k)
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetI).intersection(tidSetJ))
-                if self.getPer_Sup(y1) <= self._maximum:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print("kPFPMiner has successfully generated top-k frequent patterns")
+        self._finalPatterns = {}
+        ArraysAndItems = self._creatingOneItemSets()
+        candidates = list(ArraysAndItems.keys())
+        candidates = [list(i) for i in candidates]
+        values = list(ArraysAndItems.values())
+
+        values = _ab._cp.array(values)
+        # print(values)
+
+        # print(type(candidates[0]))
+
+        while len(candidates) > 0:
+            newKeys = []
+            for i in range(len(candidates)):
+                for j in range(i+1, len(candidates)):
+                        if candidates[i][:-1] == candidates[j][:-1] and candidates[i][-1] != candidates[j][-1]:
+                            newKeys.append(candidates[i] + candidates[j][-1:])
+                        else:
+                            break
+
+            if len(newKeys) == 0:
+                break
+
+            # print(newKeys)
+
+            numberOfKeys = len(newKeys)
+            keySize = len(newKeys[0])
+
+            newKeys = _ab._cp.array(newKeys, dtype=_ab._cp.uint32)
+
+            # newKeys = _ab._cp.flatten(newKeys)
+            newKeys = _ab._cp.reshape(newKeys, (numberOfKeys * keySize,))
+
+            support = _ab._cp.zeros(numberOfKeys, dtype=_ab._cp.uint32)
+            period = _ab._cp.zeros(numberOfKeys, dtype=_ab._cp.uint32)
+
+            self.supportAndPeriod((numberOfKeys//32 + 1,), (32,),
+                                    (
+                                        values, self.arraySize,
+                                        newKeys, numberOfKeys, keySize,
+                                        support, period,
+                                        self._maxPer, self._maxTS
+                                    )
+            )
+
+            newKeys = _ab._cp.reshape(newKeys, (numberOfKeys, keySize))
+            newKeys = _ab._cp.asnumpy(newKeys)
+            support = support.get()
+            period = period.get()
+
+            newCandidates = []
+            for i in range(len(newKeys)):
+                # print(newKeys[i], support[i], period[i])
+                if support[i] >= self._minSup and period[i] <= self._maxPer:
+                    newCandidates.append(list(newKeys[i]))
+                    rename = [self._rename[j] for j in newKeys[i]]
+                    rename = "\t".join(rename)
+                    self._finalPatterns[rename] = [support[i], period[i]]
+
+            # print()
+
+            # print(newCandidates)
+
+            candidates = newCandidates
+            
+
+
         self._endTime = _ab._time.time()
-        self._memoryUSS = float()
-        self._memoryRSS = float()
         process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
+        print("Periodic-Frequent patterns were generated successfully using cuGPFMiner algorithm ")
+
+    def Mine(self):
+            """
+            Mining process will start from here
+            """
+
+            self._startTime = _ab._time.time()
+            self._finalPatterns = {}
+            ArraysAndItems = self._creatingOneItemSets()
+            candidates = list(ArraysAndItems.keys())
+            candidates = [list(i) for i in candidates]
+            values = list(ArraysAndItems.values())
+
+            values = _ab._cp.array(values)
+            # print(values)
+
+            # print(type(candidates[0]))
+
+            while len(candidates) > 0:
+                newKeys = []
+                for i in range(len(candidates)):
+                    for j in range(i + 1, len(candidates)):
+                        if candidates[i][:-1] == candidates[j][:-1] and candidates[i][-1] != candidates[j][-1]:
+                            newKeys.append(candidates[i] + candidates[j][-1:])
+                        else:
+                            break
+
+                if len(newKeys) == 0:
+                    break
+
+                # print(newKeys)
+
+                numberOfKeys = len(newKeys)
+                keySize = len(newKeys[0])
+
+                newKeys = _ab._cp.array(newKeys, dtype=_ab._cp.uint32)
+
+                # newKeys = _ab._cp.flatten(newKeys)
+                newKeys = _ab._cp.reshape(newKeys, (numberOfKeys * keySize,))
+
+                support = _ab._cp.zeros(numberOfKeys, dtype=_ab._cp.uint32)
+                period = _ab._cp.zeros(numberOfKeys, dtype=_ab._cp.uint32)
+
+                self.supportAndPeriod((numberOfKeys // 32 + 1,), (32,),
+                                      (
+                                          values, self.arraySize,
+                                          newKeys, numberOfKeys, keySize,
+                                          support, period,
+                                          self._maxPer, self._maxTS
+                                      )
+                                      )
+
+                newKeys = _ab._cp.reshape(newKeys, (numberOfKeys, keySize))
+                newKeys = _ab._cp.asnumpy(newKeys)
+                support = support.get()
+                period = period.get()
+
+                newCandidates = []
+                for i in range(len(newKeys)):
+                    # print(newKeys[i], support[i], period[i])
+                    if support[i] >= self._minSup and period[i] <= self._maxPer:
+                        newCandidates.append(list(newKeys[i]))
+                        rename = [self._rename[j] for j in newKeys[i]]
+                        rename = "\t".join(rename)
+                        self._finalPatterns[rename] = [support[i], period[i]]
+
+                # print()
+
+                # print(newCandidates)
+
+                candidates = newCandidates
+
+            self._endTime = _ab._time.time()
+            process = _ab._psutil.Process(_ab._os.getpid())
+            self._memoryRSS = float()
+            self._memoryUSS = float()
+            self._memoryUSS = process.memory_full_info().uss
+            self._memoryRSS = process.memory_info().rss
+            print("Periodic-Frequent patterns were generated successfully using cuGPFMiner algorithm ")
 
     def getMemoryUSS(self):
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
@@ -403,69 +578,79 @@
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
+        """Storing final periodic-frequent patterns in a dataframe
 
-        :return: returning frequent patterns in a dataframe
+        :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicity'])
-        return dataFrame
+            data.append([a, b[0], b[1]])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+        return dataframe
 
     def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to a output file
+        """Complete set of periodic-frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-
-        :type outFile: file
+        :type outFile: csv file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x + ":" + str(y)
-            writer.write("%s \n" % patternsAndSupport)
+            # print(x,y)
+            # print(type(x), type(y))
+            s1 = x.replace(' ', '\t') + ":" + str(y[0]) + ":" + str(y[1])
+            writer.write("%s \n" % s1)
 
     def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
+        """ Function to send the set of periodic-frequent patterns after completion of the mining process
 
-        :return: returning frequent patterns
+        :return: returning periodic-frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
-        print("Total number of  Top-k Periodic Frequent Patterns:", len(self.getPatterns()))
+        """
+        This function is used to print the results
+        """
+        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
-
+                    
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = cuGPFMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cuGPFMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of top-k periodic frequent patterns:", len(_Patterns))
+        print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
 
+    _ap = cuGPFMiner("/home/tarun/Temporal_T10I4D100K.csv", 50, 10000, "\t")
+    # _ap = cuGPFMiner("/home/tarun/PAMI/PAMI/periodicFrequentPattern/cuda/test.txt", 1, 10, " ")
+
+    _ap.startMine()
+    print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+    _ap.save("tarun.txt")
+    print("Total Memory in USS:", _ap.getMemoryUSS())
+    print("Total Memory in RSS", _ap.getMemoryRSS())
+    print("Total ExecutionTime in ms:", _ap.getRuntime())
+
```

### Comparing `pami-2024.3.9.2/PAMI/recurringPattern/basic/RPGrowth.py` & `pami-2024.4.9.1/PAMI/recurringPattern/basic/RPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,37 +1,38 @@
 
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.periodicFrequentPattern.recurring import RPGrowth as alg
+#             from PAMI.periodicFrequentPattern.recurring import RPGrowth as alg
 #
-#     obj = alg.RPGrowth(iFile, maxPer, minPS, minRec)
+#             obj = alg.RPGrowth(iFile, maxPer, minPS, minRec)
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     periodicFrequentPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#     print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#     obj.savePatterns(oFile)
+#             obj.savePatterns(oFile)
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
 #
-#     print("Total ExecutionTime in seconds:", run)
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
@@ -46,14 +47,17 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.recurringPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+from PAMI.recurringPattern.basic import abstract as _ab
 
 _maxPer = float()
 _minPS = float()
 _minRec = float()
 _lno = int()
 
 
@@ -321,29 +325,29 @@
             self.removeNode(i)
 
 
 class RPGrowth(_ab._recurringPatterns):
     """
     :Description:   RPGrowth is one of the fundamental algorithm to discover recurring patterns in a transactional database.
 
-    :Reference:
+
+    :Reference:  R. Uday Kiran†, Haichuan Shang†, Masashi Toyoda† and Masaru Kitsuregawa† Discovering Recurring Patterns in Time Series,https://www.tkl.iis.u-tokyo.ac.jp/new/uploads/publication_file/file/693/Paper%2023.pdf
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of Recurring  patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minPS: str:
-                   Minimum number of frequent patterns to be included in the output file.
-    :param  minRec: str:
-                   Minimum number of ...
-    :param  maxPer: float:
-                   Maximum number of frequent patterns to be included in the output file.
-
+                   Name of the output file to store complete set of Recurring patterns
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  minPs: str :
+                   It could potentially represent a minimum parallelism percentage or some other value related to parallel processing.
+    :param  maxPer: float : minRec
+                   It represent a maximum percentage or some other numeric value.
+    :param  minRec: str :
+                   It could represent a minimum recommended value or some other string-based setting.
 
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
@@ -408,21 +412,27 @@
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
 
     **Methods to execute code on terminal**
     -------------------------------------------
-            Format:
-                      >>>  python3 RPGrowth.py <inputFile> <outputFile> <maxPer> <minPS> <minRec>
+    .. code-block:: console
+
 
-            Example:
-                      >>>  python3 RPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4 2
+       Format:
 
-                     .. note:: maxPer and minPS will be considered in percentage of database transactions
+       (.venv) $ python3 RPGrowth.py <inputFile> <outputFile> <maxPer> <minPS> <minRec>
+
+       Example usage:
+
+       (.venv) $ python3 RPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4 2
+
+
+               .. note:: maxPer and minPS will be considered in percentage of database transactions
 
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
 
                 from PAMI.periodicFrequentPattern.recurring import RPGrowth as alg
 
@@ -609,19 +619,51 @@
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Mining process will start from this function
         """
+        global _minPS, _minRec, _maxPer, _lno
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self._creatingItemSets()
+        self._minPS = self._convert(self._minPS)
+        self._maxPer = self._convert(self._maxPer)
+        self._minRec = int(self._minRec)
+        self._finalPatterns = {}
+        _maxPer, _minPS, _minRec, _lno = self._maxPer, self._minPS, self._minRec, len(self._Database)
+        generatedItems, pfList = self._OneItems()
+        updatedDatabases = self._updateDatabases(generatedItems)
+        for x, y in self._rank.items():
+            self._rankedUp[y] = x
+        info = {self._rank[k]: v for k, v in generatedItems.items()}
+        Tree = self._buildTree(updatedDatabases, info)
+        patterns = Tree.generatePatterns([])
+        for i in patterns:
+            sample = self._savePeriodic(i[0])
+            self._finalPatterns[sample] = i[1]
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Recurring patterns were generated successfully using RPGrowth algorithm ")
 
+    def Mine(self):
+        """
+        Mining process will start from this function
+        """
         global _minPS, _minRec, _maxPer, _lno
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         self._creatingItemSets()
         self._minPS = self._convert(self._minPS)
         self._maxPer = self._convert(self._maxPer)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.3.9.2/PAMI/recurringPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/recurringPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py` & `pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,38 +1,40 @@
 # RSFPGrowth algorithm is used to find all items with relative support from given dataset
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     from PAMI.relativeFrequentPattern import RSFPGrowth as alg
 #
-#     obj = alg.RSFPGrowth(iFile, minSup, __minRatio)
+#             from PAMI.relativeFrequentPattern import RSFPGrowth as alg
 #
-#     obj.startMine()
+#             obj = alg.RSFPGrowth(iFile, minSup, __minRatio)
 #
-#     frequentPatterns = obj.getPatterns()
+#             obj.startMine()
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             frequentPatterns = obj.getPatterns()
 #
-#     obj.save(oFile)
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             obj.save(oFile)
 #
-#     memUSS = obj.getMemoryUSS()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     print("Total Memory in USS:", memUSS)
+#             memUSS = obj.getMemoryUSS()
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     run = obj.getRuntime()
+#             print("Total Memory in RSS", memRSS)
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             run = obj.getRuntime()
 #
+#             print("Total ExecutionTime in seconds:", run)
+#
+
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
@@ -50,14 +52,19 @@
 
 """
 
 from PAMI.relativeFrequentPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 import pandas as pd
 
+from PAMI.relativeFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
+
 class _Node:
     """
     A class used to represent the node of frequent Pattern tree
 
     :Attributes:
 
         itemId: int
@@ -137,14 +144,15 @@
 
     def addTransaction(self, transaction: List[int]) -> None:
         """
         Adding transaction into tree
 
         :param transaction: it represents the one transaction in database
         :type transaction: list
+        :return: None
         """
 
         # This method taken a transaction as input and returns the tree
         current = self.root
         for i in transaction:
             child = current.getChild(i)
             if not child:
@@ -162,14 +170,15 @@
         """
         Fixing node link for the newNode that inserted into frequentPatternTree
 
         :param item: it represents the item of newNode
         :type item: int
         :param newNode: it represents the newNode that inserted in frequentPatternTree
         :type newNode: Node
+        :return: None
 
         """
         if item in self.mapItemLastNodes.keys():
             lastNode = self.mapItemLastNodes[item]
             lastNode.nodeLink = newNode
         self.mapItemLastNodes[item] = newNode
         if item not in self.mapItemNodes.keys():
@@ -177,14 +186,15 @@
 
     def printTree(self, root: '_Node') -> None:
         """
         Print the details of Node in frequentPatternTree
 
         :param root: it represents the Node in frequentPatternTree
         :type root: Node
+        :return: None
 
         """
 
         # this method is used print the details of tree
         if not root.child:
             return
         else:
@@ -196,14 +206,15 @@
         """
         To create the headerList
 
         :param __mapSupport: it represents the items with their supports
         :type __mapSupport: dictionary
         :param minSup: it represents the minSup
         :param minSup: float
+        :return: None
         """
         # the frequentPatternTree always maintains the header table to start the mining from leaf nodes
         t1 = []
         for x, y in __mapSupport.items():
             if y >= minSup:
                 t1.append(x)
         __itemSetBuffer = [k for k, v in sorted(__mapSupport.items(), key=lambda x: x[1], reverse=True)]
@@ -245,17 +256,17 @@
     """
     :Description:   Algorithm to find all items with relative support from given dataset
 
     :Reference:   'Towards Efficient Discovery of Frequent Patterns with Relative Support' R. Uday Kiran and
                    Masaru Kitsuregawa, http://comad.in/comad2012/pdf/kiran.pdf
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of Relative frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
+                   Name of the output file to store complete set of Relative frequent patterns
     :param  minSup: str:
                    Controls the minimum number of transactions in which every item must appear in a database.
     :param  minRS: float:
                    Controls the minimum number of transactions in which at least one time within a pattern must appear in a database.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
@@ -323,22 +334,28 @@
             Stores all the frequent patterns with their respective support
         frequentPatternGrowthGenerate(frequentPatternTree,prefix,port)
             Mining the frequent patterns by forming conditional frequentPatternTrees to particular prefix item.
             __mapSupport represents the 1-length items with their respective support
 
 
     **Methods to execute code on terminal**
-    -------------------------------------------
-            Format:
-                      >>>  python3 RSFPGrowth.py <inputFile> <outputFile> <minSup> <__minRatio>
+    ----------------------------------------------
+    .. code-block:: console
 
-            Example:
-                      >>>  python3 RSFPGrowth.py sampleDB.txt patterns.txt 0.23 0.2
 
-                     .. note:: maxPer and minPS will be considered in percentage of database transactions
+       Format:
+
+       (.venv) $python3 RSFPGrowth.py <inputFile> <outputFile> <minSup> <__minRatio>
+
+       Example Usage :
+
+       (.venv) $python3 python3 RSFPGrowth.py sampleDB.txt patterns.txt 0.23 0.2
+
+
+               .. note:: maxPer and minPS will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     -----------------------------------------------------
     .. code-block:: python
 
             from PAMI.relativeFrequentPattern import RSFPGrowth as alg
@@ -395,14 +412,15 @@
     def __init__(self, iFile: Union[str, pd.DataFrame], minSup: Union[int, float, str], minRS: float, sep: str='\t') -> None:
         super().__init__(iFile, minSup, minRS, sep)
         self.__finalPatterns = {}
 
     def __creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the __Database/input file in a __Database variable
+        :return: None
         """
         self.__Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
@@ -427,14 +445,15 @@
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def __frequentOneItem(self) -> None:
         """
         Generating One frequent items sets
+        :return: None
         """
         self.__mapSupport = {}
         for i in self.__Database:
             for j in i:
                 if j not in self.__mapSupport:
                     self.__mapSupport[j] = 1
                 else:
@@ -446,14 +465,15 @@
 
         :param prefix: the frequent pattern
         :type prefix: list
         :param prefixLength: the length of a frequent pattern
         :type prefixLength: int
         :param support: the support of a pattern
         :type support:  int
+        :return: None
         """
 
         sample = []
         for i in range(prefixLength):
             sample.append(prefix[i])
         self.__itemSetCount += 1
         self.__finalPatterns[tuple(sample)] = str(support) + " : " + str(ratio)
@@ -577,17 +597,19 @@
             if '.' in value:
                 value = float(value)
                 value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         Main program to start the operation
+        :return: None
         """
 
         self.__startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._minSup is None:
             raise Exception("Please enter the Minimum Support")
@@ -613,14 +635,52 @@
         self.__endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self.__memoryRSS = float()
         self.__memoryUSS = float()
         self.__memoryUSS = process.memory_full_info().uss
         self.__memoryRSS = process.memory_info().rss
 
+    def Mine(self) -> None:
+            """
+            Main program to start the operation
+            :return: None
+            """
+
+            self.__startTime = _ab._time.time()
+            if self._iFile is None:
+                raise Exception("Please enter the file path or file name:")
+            if self._minSup is None:
+                raise Exception("Please enter the Minimum Support")
+            self.__creatingItemSets()
+            self._minSup = self.__convert(self._minSup)
+            self._minRS = float(self._minRS)
+            self.__frequentOneItem()
+            self.__finalPatterns = {}
+            self.__mapSupport = {k: v for k, v in self.__mapSupport.items() if v >= self._minSup}
+            __itemSetBuffer = [k for k, v in sorted(self.__mapSupport.items(), key=lambda x: x[1], reverse=True)]
+            for i in self.__Database:
+                transaction = []
+                for j in i:
+                    if j in __itemSetBuffer:
+                        transaction.append(j)
+                transaction.sort(key=lambda val: self.__mapSupport[val], reverse=True)
+                self.__tree.addTransaction(transaction)
+            self.__tree.createHeaderList(self.__mapSupport, self._minSup)
+            if len(self.__tree.headerList) > 0:
+                self.__itemSetBuffer = []
+                self.__frequentPatternGrowthGenerate(self.__tree, self.__itemSetBuffer, 0, self.__mapSupport,
+                                                     self._minRS)
+            print("Relative support frequent patterns were generated successfully using RSFPGrowth algorithm")
+            self.__endTime = _ab._time.time()
+            process = _ab._psutil.Process(_ab._os.getpid())
+            self.__memoryRSS = float()
+            self.__memoryUSS = float()
+            self.__memoryUSS = process.memory_full_info().uss
+            self.__memoryRSS = process.memory_info().rss
+
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
@@ -678,14 +738,15 @@
 
     def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file.
         :type outFile: file
+        :return: None
         """
         self.__oFile = outFile
         writer = open(self.__oFile, 'w+')
         for x, y in self.__finalPatterns.items():
             pattern = str()
             for i in x:
                 pattern = pattern + i + "\t"
@@ -707,14 +768,15 @@
             s1 = str(y)
             res[pattern] = s1
         return res
 
     def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
         print("Total number of Relative Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
```

### Comparing `pami-2024.3.9.2/PAMI/relativeFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/relativeHighUtilityPattern/basic/RHUIM.py` & `pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,43 +1,45 @@
-# RHUIM algorithm helps us to mine Relative High Utility itemSets from transactional databases.
+# Spatial High Utility Frequent ItemSet Mining (SHUFIM) aims to discover all itemSets in a spatioTemporal database
+# that satisfy the user-specified minimum utility, minimum support and maximum distance constraints
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
+#             from PAMI.highUtilityGeoreferencedFrequentPattern.basic import SHUFIM as alg
 #
-#     from PAMI.relativeHighUtilityPattern.basic import RHUIM as alg
+#             obj=alg.SHUFIM("input.txt","Neighbours.txt",35,20)
 #
-#     obj = alg.RHUIM("input.txt", 35, 20)
+#             obj.mine()
 #
-#     obj.startMine()
+#             patterns = obj.getPatterns()
 #
-#     frequentPatterns = obj.getPatterns()
+#             print("Total number of Spatial high utility frequent Patterns:", len(patterns))
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             obj.save("output")
 #
-#     obj.savePatterns(oFile)
+#             memUSS = obj.getMemoryUSS()
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             print("Total Memory in USS:", memUSS)
 #
-#     memUSS = obj.getMemoryUSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             run = obj.getRuntime()
 #
-#     print("Total Memory in RSS", memRSS)
-#
-#     run = obj.getRuntime()
-#
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -46,99 +48,119 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
-
-
-from PAMI.relativeHighUtilityPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
-
+from PAMI.highUtilityGeoreferencedFrequentPattern.basic import abstract as _ab
+from functools import cmp_to_key as _comToKey
+from deprecated import deprecated
 
 class _Transaction:
     """
     A class to store Transaction of a database
 
     :Attributes:
 
         items: list
             A list of items in transaction 
         utilities: list
-            A list of utilities of items in transaction
+            A list of utilites of items in transaction
         transactionUtility: int
             represent total sum of all utilities in the database
-        prefixUtility:
+        pmus: list
+            represent the pmu (probable maximum utility) of each element in the transaction
+        prefixutility:
             prefix Utility values of item
         offset:
-            an offset pointer, used by projected transaction
-
+            an offset pointer, used by projected transactions
+        support:
+            maintains the support of the transaction
     :Methods:
 
         projectedTransaction(offsetE):
-            A method to create new Transaction from existing starting from offsetE until the end
+            A method to create new Transaction from existing till offsetE
         getItems():
             return items in transaction
         getUtilities():
             return utilities in transaction
+        getPmus():
+            return pmus in transaction
         getLastPosition():
             return last position in a transaction
         removeUnpromisingItems():
-            A method to remove items which are having low values when compared with minUtil
+            A method to remove items with low Utility than minUtil
         insertionSort():
             A method to sort all items in the transaction
+        getSupport():
+            returns the support of the transaction
     """
     offset = 0
     prefixUtility = 0
-
-    def __init__(self, items: list, utilities: list, transactionUtility: int) -> None:
+    support = 1
+    
+    def __init__(self, items, utilities, transactionUtility, pmus=None):
         self.items = items
         self.utilities = utilities
         self.transactionUtility = transactionUtility
+        if pmus is not None:
+            self.pmus = pmus
+        self.support = 1
 
-    def projectTransaction(self, offsetE: int) -> '_Transaction':
+    def projectTransaction(self, offsetE):
         """
-        A method to create new Transaction from existing transaction starting from offsetE until the end
-
+        A method to create new Transaction from existing till offsetE
         :param offsetE: an offset over the original transaction for projecting the transaction
         :type offsetE: int
         """
-        new_transaction = _Transaction(self.items, self.utilities, self.transactionUtility)
+        newTransaction = _Transaction(self.items, self.utilities, self.transactionUtility)
         utilityE = self.utilities[offsetE]
-        new_transaction.prefixUtility = self.prefixUtility + utilityE
-        new_transaction.transactionUtility = self.transactionUtility - utilityE
+        newTransaction.prefixUtility = self.prefixUtility + utilityE
+        newTransaction.transactionUtility = self.transactionUtility - utilityE
+        newTransaction.support = self.support
         for i in range(self.offset, offsetE):
-            new_transaction.transactionUtility -= self.utilities[i]
-        new_transaction.offset = offsetE + 1
-        return new_transaction
+            newTransaction.transactionUtility -= self.utilities[i]
+        newTransaction.offset = offsetE + 1
+        return newTransaction
 
-    def getItems(self) -> list:
+    def getItems(self):
         """
         A method to return items in transaction
         """
         return self.items
 
-    def getUtilities(self) -> list:
+    def getPmus(self):
+        """
+        A method to return pmus in transaction
+        """
+        return self.pmus
+
+    def getUtilities(self):
         """
         A method to return utilities in transaction
         """
         return self.utilities
 
-    def getLastPosition(self) -> int:
+    # get the last position in this transaction
+    def getLastPosition(self):
         """
         A method to return last position in a transaction
         """
-
         return len(self.items) - 1
 
-    def removeUnpromisingItems(self, oldNamesToNewNames: dict) -> None:
+    def getSupport(self):
         """
-        A method to remove items which are not present in the map passed to the function
+        A method to return support of a transaction (number of transactions in the original database having the items present in this transaction)
+        """
+        return self.support
 
+    def removeUnpromisingItems(self, oldNamesToNewNames):
+        """
+        A method to remove items with low Utility than minUtil
         :param oldNamesToNewNames: A map represent old names to new names
         :type oldNamesToNewNames: map
         """
         tempItems = []
         tempUtilities = []
         for idx, item in enumerate(self.items):
             if item in oldNamesToNewNames:
@@ -146,560 +168,779 @@
                 tempUtilities.append(self.utilities[idx])
             else:
                 self.transactionUtility -= self.utilities[idx]
         self.items = tempItems
         self.utilities = tempUtilities
         self.insertionSort()
 
-    def insertionSort(self) -> None:
+    def insertionSort(self):
         """
         A method to sort items in order
         """
         for i in range(1, len(self.items)):
             key = self.items[i]
             utilityJ = self.utilities[i]
             j = i - 1
             while j >= 0 and key < self.items[j]:
                 self.items[j + 1] = self.items[j]
                 self.utilities[j + 1] = self.utilities[j]
                 j -= 1
             self.items[j + 1] = key
             self.utilities[j + 1] = utilityJ
-        
+
 
 class _Dataset:
     """
     A class represent the list of transactions in this dataset
 
-   :Attributes:
+    :Attributes:
 
-        transactions:
+        transactions :
             the list of transactions in this dataset
         maxItem:
             the largest item name
         
-   :methods:
+    :methods:
 
         createTransaction(line):
             Create a transaction object from a line from the input file
         getMaxItem():
             return Maximum Item
         getTransactions():
             return transactions in database
 
     """
     transactions = []
     maxItem = 0
-    def __init__(self, datasetPath: str, sep: str) -> None:
+    
+    def __init__(self, datasetPath, sep):
         self.strToInt = {}
         self.intToStr = {}
         self.cnt = 1
         self.sep = sep
+        self.transactions = []
         self.createItemSets(datasetPath)
 
-    def createItemSets(self, datasetPath: str) -> None:
+    def createItemSets(self, datasetPath):
         """
         Storing the complete transactions of the database/input file in a database variable
+
+        :param datasetPath: Path to the input file
+
+        :type datasetPath: str
         """
-        self.transactions = []
-        itemsets, utilities, utilityValues = [], [], []
+        pmuString = None
         if isinstance(datasetPath, _ab._pd.DataFrame):
-            utilities, data, utilityValues = [], [], []
+            utilities, data, utilitySum, pmuString = [], [], [], []
             if datasetPath.empty:
                 print("its empty..")
             i = datasetPath.columns.values.tolist()
             if 'Transactions' in i:
-                itemsets = datasetPath['Transactions'].tolist()
+                data = datasetPath['Transactions'].tolist()
             if 'Utilities' in i:
-                utilities = datasetPath['Patterns'].tolist()
+                utilities = datasetPath['Utilities'].tolist()
             if 'UtilitySum' in i:
-                utilityValues = datasetPath['utilitySum'].tolist()
-            for k in range(len(itemsets)):
-                self.transactions.append(self.createTransaction(itemsets[k], utilities[k], utilityValues[k]))
+                utilitySum = datasetPath['UtilitySum'].tolist()
+            if 'pmuString' in i:
+                utilitySum = datasetPath['pmuString'].tolist()
+            for k in range(len(data)):
+                self.transactions.append(self.createTransaction(data[k], utilities[k], utilitySum[k], pmuString[k]))
         if isinstance(datasetPath, str):
             if _ab._validators.url(datasetPath):
                 data = _ab._urlopen(datasetPath)
                 for line in data:
                     line = line.decode("utf-8")
                     trans_list = line.strip().split(':')
                     transactionUtility = int(trans_list[1])
                     itemsString = trans_list[0].strip().split(self.sep)
                     itemsString = [x for x in itemsString if x]
                     utilityString = trans_list[2].strip().split(self.sep)
                     utilityString = [x for x in utilityString if x]
-                    self.transactions.append(self.createTransaction(itemsString, utilityString, transactionUtility))
+                    if len(trans_list) == 4:
+                        pmuString = trans_list[3].strip().split(self.sep)
+                        pmuString = [x for x in pmuString if x]
+                    self.transactions.append(self.createTransaction(itemsString, utilityString, transactionUtility, pmuString))
             else:
                 try:
                     with open(datasetPath, 'r', encoding='utf-8') as f:
                         for line in f:
                             trans_list = line.strip().split(':')
                             transactionUtility = int(trans_list[1])
                             itemsString = trans_list[0].strip().split(self.sep)
                             itemsString = [x for x in itemsString if x]
                             utilityString = trans_list[2].strip().split(self.sep)
                             utilityString = [x for x in utilityString if x]
-                            self.transactions.append(self.createTransaction(itemsString, utilityString, transactionUtility))
+                            if len(trans_list) == 4:
+                                pmuString = trans_list[3].strip().split(self.sep)
+                                pmuString = [x for x in pmuString if x]
+                            self.transactions.append(
+                                self.createTransaction(itemsString, utilityString, transactionUtility, pmuString))
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def createTransaction(self, itemSet: list, utilities: list, utilitySum: int) -> _Transaction:
+    def createTransaction(self, items, utilities, utilitySum, pmustring):
         """
         A method to create Transaction from dataset given
-            
-        :Attributes:
-
-        :param itemSet: represent a transactions itemset in database
-        :type itemSet: list
-        :param utilities: utility values of respective transaction itemSets
+        :param items: represent a utility items in a transaction
+        :type items: list
+        :param utilities: represent utility of an item in transaction
         :type utilities: list
-        :param utilitySum: represent the sum of utility Sum
+        :param utilitySum: represent utility sum of  transaction
         :type utilitySum: int
-        :return : Transaction.
-        :rtype: Transaction
+        :param pmustring: represent a pmustring in a given dataset
+        :type pmustring: str
         """
         transactionUtility = utilitySum
-        itemsString = itemSet
+        itemsString = items
         utilityString = utilities
+        pmuString = pmustring
         items = []
         utilities = []
+        pmus = []
         for idx, item in enumerate(itemsString):
-            if self.strToInt.get(item) is None:
+            if (self.strToInt).get(item) is None:
                 self.strToInt[item] = self.cnt
                 self.intToStr[self.cnt] = item
                 self.cnt += 1
-            item_int = self.strToInt.get(item)
-            if item_int > self.maxItem:
-                self.maxItem = item_int
-            items.append(item_int)
+            itemInt = self.strToInt.get(item)
+            if itemInt > self.maxItem:
+                self.maxItem = itemInt
+            items.append(itemInt)
             utilities.append(int(utilityString[idx]))
-        return _Transaction(items, utilities, transactionUtility)
+            if pmuString != None:
+                pmus.append(int(pmuString[idx]))
+        if pmuString == None:
+            pmus = None
+        return _Transaction(items, utilities, transactionUtility, pmus)
 
-    def getMaxItem(self) -> int:
+    def getMaxItem(self):
         """
         A method to return name of the largest item
         """
         return self.maxItem
 
-    def getTransactions(self) -> list:
+    def getTransactions(self):
         """
         A method to return transactions from database
         """
         return self.transactions
 
 
-class RHUIM(_ab._utilityPatterns):
+class SHUFIM(_ab._utilityPatterns):
     """
-    :Description:   RHUIM algorithm helps us to mine Relative High Utility itemSets from transactional databases.
-    
-    :Reference:   R. U. Kiran, P. Pallikila, J. M. Luna, P. Fournier-Viger, M. Toyoda and P. K. Reddy,
-                 "Discovering Relative High Utility Itemsets in Very Large Transactional Databases Using Null-Invariant Measure,"
-                  2021 IEEE International Conference on Big Data (Big Data), Orlando, FL, USA, 2021, pp. 252-262,
-                  doi: 10.1109/BigData52589.2021.9672064.
+    :Description:  Spatial High Utility Frequent ItemSet Mining (SHUFIM) aims to discover all itemSets in a spatioTemporal database
+                   that satisfy the user-specified minimum utility, minimum support and maximum distance constraints
+
+    :Reference:    10.1007/978-3-030-37188-3_17
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Geo-referenced frequent sequence patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Geo-referenced frequent sequence patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param minUtil: int :
+                   The user given minUtil value.
+    :param candidateCount: int
+                   Number of candidates
+    :param maxMemory: int
+                   Maximum memory used by this program for running
+    :param nFile: str :
+                   Name of the input file to mine complete set of Geo-referenced frequent sequence patterns
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
 
         iFile : file
-            Name of the input file to mine complete set of patterns
+            Name of the input file to mine complete set of frequent patterns
+        nFile : file
+            Name of the Neighbours file that contain neighbours of items
         oFile : file
-            Name of the output file to store complete set of patterns
+            Name of the output file to store complete set of frequent patterns
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime : float
+        startTime:float
             To record the start time of the mining process
-        endTime : float
+        endTime:float
             To record the completion time of the mining process
         minUtil : int
-            The user given minUtil value
-        minUR : float
-            The user given minUR value
-        relativeHighUtilityItemSets : map
-            set of relative high utility itemSets
-        candidateCount : int
+            The user given minUtil
+        minSup : float
+            The user given minSup value
+        highUtilityFrequentSpatialItemSets: map
+            set of high utility itemSets
+        candidateCount: int
              Number of candidates 
-        utilityBinArrayLU : list
-             A map to hold the local utility values of the items in database
-        utilityBinArraySU : list
+        utilityBinArrayLU: list
+             A map to hold the pmu values of the items in database
+        utilityBinArraySU: list
+            A map to hold the subtree utility values of the items is database
+        oldNamesToNewNames: list
             A map to hold the subtree utility values of the items is database
-        oldNamesToNewNames : list
-            A map which contains old names, new names of items as key value pairs
-        newNamesToOldNames : list
-            A map which contains new names, old names of items as key value pairs
-        maxMemory : float
+        newNamesToOldNames: list
+            A map to store the old name corresponding to new name
+        Neighbours : map
+            A dictionary to store the neighbours of a item
+        maxMemory: float
             Maximum memory used by this program for running
-        patternCount : int
-            Number of RHUI's
-        itemsToKeep : list
-            keep only the promising items i.e items that can extend other items to form RHUIs
-        itemsToExplore : list
-            list of items that needs to be explored
+        patternCount: int
+            Number of SHUFI's (Spatial High Utility Frequent Itemsets)
+        itemsToKeep: list
+            keep only the promising items ie items whose supersets can be required patterns
+        itemsToExplore: list
+            keep items that subtreeUtility grater than minUtil
 
-    :Methods:
+    :Methods :
 
         startMine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         save(oFile)
-                Complete set of patterns will be loaded in to a output file
+                Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
-                Complete set of patterns will be loaded in to a dataframe
+                Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
                 Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
                 Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
                Total amount of runtime taken by the mining process will be retrieved from this function
-        backTrackingRHUIM(transactionsOfP, itemsToKeep, itemsToExplore, prefixLength)
-               A method to mine the RHUIs Recursively
-        useUtilityBinArraysToCalculateUpperBounds(transactionsPe, j, itemsToKeep)
-               A method to calculate the sub-tree utility and local utility of all items that can extend itemSet P and e
+        calculateNeighbourIntersection(self, prefixLength)
+               A method to return common Neighbours of items
+        backtrackingEFIM(transactionsOfP, itemsToKeep, itemsToExplore, prefixLength)
+               A method to mine the SHUIs Recursively
+        useUtilityBinArraysToCalculateUpperBounds(transactionsPe, j, itemsToKeep, neighbourhoodList)
+               A method to  calculate the sub-tree utility and local utility of all items that can extend itemSet P and e
         output(tempPosition, utility)
-               A method to output a relative-high-utility itemSet to file or memory depending on what the user chose
-        is_equal(transaction1, transaction2)
+               A method ave a high-utility itemSet to file or memory depending on what the user chose
+        isEqual(transaction1, transaction2)
                A method to Check if two transaction are identical
+        intersection(lst1, lst2)
+               A method that return the intersection of 2 list
         useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(dataset)
-              A method to calculate the sub tree utility values for single items
+              Scan the initial database to calculate the subtree utility of each items using a utility-bin array
         sortDatabase(self, transactions)
-              A Method to sort transaction
-        sort_transaction(self, trans1, trans2)
-              A Method to sort transaction
+              A Method to sort transaction in the order of PMU
+        sortTransaction(self, trans1, trans2)
+              A Method to sort transaction in the order of PMU
         useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset)
-             A method to calculate local utility values for single itemSets
+             A method to scan the database using utility bin array to calculate the pmus
 
-    **Methods to execute code on terminal**
+    **Executing the code on terminal :**
     -----------------------------------------
-            Format:
-                      >>> python3 RHUIM.py <inputFile> <outputFile> <minUtil> <sep>
 
-            Example:
-                      >>>  python3 RHUIM.py sampleTDB.txt output.txt 35 20
+    .. code-block:: console
 
+      Format:
 
-    **Importing this algorithm into a python program**
-    -----------------------------------------------------
-    .. code-block:: python
+      (.venv) $ python3 SHUFIM.py <inputFile> <outputFile> <Neighbours> <minUtil> <minSup> <sep>
 
-            from PAMI.relativeHighUtilityPattern.basic import RHUIM as alg
+      Example Usage:
 
-            obj=alg.RHUIM("input.txt", 35, 20)
+      (.venv) $ python3 SHUFIM.py sampleTDB.txt output.txt sampleN.txt 35 20
 
-            obj.startMine()
+    .. note:: minSup will be considered in percentage of database transactions
 
-            frequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+    **Sample run of importing the code:**
+    -----------------------------------------
+    .. code-block:: python
+
+            from PAMI.highUtilityGeoreferencedFrequentPattern.basic import SHUFIM as alg
+
+            obj=alg.SHUFIM("input.txt","Neighbours.txt",35,20)
 
-            obj.savePatterns(oFile)
+            obj.mine()
 
-            Df = obj.getPatternsAsDataFrame()
+            patterns = obj.getPatterns()
 
-            memUSS = obj.getmemoryUSS()
+            print("Total number of Spatial high utility frequent Patterns:", len(patterns))
+
+            obj.save("output")
+
+            memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    -----------------
-             The complete program was written by  Pradeep Pallikila  under the supervision of Professor Rage Uday Kiran.
+    ---------------------
 
-    """
+            The complete program was written by Pradeep Pallikila under the supervision of Professor Rage Uday Kiran.
 
-    _relativeHighUtilityItemSets = []
+    """
     _candidateCount = 0
     _utilityBinArrayLU = {}
     _utilityBinArraySU = {}
     _oldNamesToNewNames = {}
     _newNamesToOldNames = {}
-    _singleItemSetsUtilities = {}
-    _strToInt = {}
-    _intToStr = {}
-    _temp = [0]*5000
-    _patternCount = int()
+    _singleItemSetsSupport = {}
+    _singleItemSetsUtility = {}
+    _strToint = {}
+    _intTostr = {}
+    _Neighbours = {}
+    _temp = [0] * 5000
     _maxMemory = 0
     _startTime = float()
     _endTime = float()
+    _minSup = str()
+    _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _nFile = " "
-    _lno = 0
     _sep = "\t"
     _minUtil = 0
-    _minUR = 0
     _memoryUSS = float()
     _memoryRSS = float()
+    
+    def __init__(self, iFile, nFile, minUtil, minSup, sep="\t"):
+        super().__init__(iFile, nFile, minUtil, minSup, sep)
+
+    def _convert(self, value):
+        """
+        To convert the type of user specified minSup value
 
-    def __init__(self, iFile: str, minUtil: int, minUR: float, sep: str="\t") -> None:
-        super().__init__(iFile, minUtil, minUR, sep)
+        :param value: user specified minSup value
+        :type value: int o float or str
+        :return: converted type
+        :rtype: float
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._dataset.getTransactions()) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._dataset.getTransactions()) * value)
+            else:
+                value = int(value)
+        return value
 
-    def startMine(self) -> None:
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
         """
-        Mining process will start from this function
+        High Utility Frequent Pattern mining start here
         """
         self._startTime = _ab._time.time()
-        self._dataset = _Dataset(self._iFile, self._sep)
+        self._patternCount = 0
         self._finalPatterns = {}
+        self._dataset = _Dataset(self._iFile, self._sep)
+        self._singleItemSetsSupport = _ab._defaultdict(int)
+        self._singleItemSetsUtility = _ab._defaultdict(int)
+        self._minUtil = int(self._minUtil)
+        self._minSup = self._convert(self._minSup)
+        with open(self._nFile, 'r') as o:
+            lines = o.readlines()
+            for line in lines:
+                line = line.split("\n")[0]
+                line_split = line.split(self._sep)
+                item = self._dataset.strToInt.get(line_split[0])
+                lst = []
+                for i in range(1, len(line_split)):
+                    lst.append(self._dataset.strToInt.get(line_split[i]))
+                self._Neighbours[item] = lst
+        o.close()
+        InitialMemory = _ab._psutil.virtual_memory()[3]
         self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
-        _minUtil = int(self._minUtil)
-        _minUR = float(self._minUR)
-        # print(minUR)
-        self._singleItemSetsUtilities = _ab._defaultdict(int)
-        itemsToKeep = []
+        _itemsToKeep = []
         for key in self._utilityBinArrayLU.keys():
-            if self._utilityBinArrayLU[key] >= _minUtil:
-                itemsToKeep.append(key)
-        itemsToKeep = sorted(itemsToKeep, key=lambda x: self._utilityBinArrayLU[x])
-        currentName = 1
-        for idx, item in enumerate(itemsToKeep):
-            self._oldNamesToNewNames[item] = currentName
-            self._newNamesToOldNames[currentName] = item
-            itemsToKeep[idx] = currentName
-            currentName += 1
+            if self._utilityBinArrayLU[key] >= self._minUtil and self._singleItemSetsSupport[key] >= self._minSup:
+                _itemsToKeep.append(key)
+        # sorting items in decreasing order of their utilities
+        _itemsToKeep = sorted(_itemsToKeep, key=lambda x: self._singleItemSetsUtility[x], reverse=True)
+        _currentName = 1
+        for idx, item in enumerate(_itemsToKeep):
+            self._oldNamesToNewNames[item] = _currentName
+            self._newNamesToOldNames[_currentName] = item
+            _itemsToKeep[idx] = _currentName
+            _currentName += 1
         for transaction in self._dataset.getTransactions():
             transaction.removeUnpromisingItems(self._oldNamesToNewNames)
-        self.sortDatabase(self._dataset.getTransactions())
-        emptyTransactionCount = 0
+        self._sortDatabase(self._dataset.getTransactions())
+        _emptyTransactionCount = 0
         for transaction in self._dataset.getTransactions():
             if len(transaction.getItems()) == 0:
-                emptyTransactionCount += 1
-        self._dataset.transactions = self._dataset.transactions[emptyTransactionCount:]
+                _emptyTransactionCount += 1
+        self._dataset.transactions = self._dataset.transactions[_emptyTransactionCount:]
+        # calculating neighborhood suffix utility values
+        _secondary = []
+        for idx, item in enumerate(_itemsToKeep):
+            _cumulativeUtility = self._singleItemSetsUtility[self._newNamesToOldNames[item]]
+            if self._newNamesToOldNames[item] in self._Neighbours:
+                neighbors = [self._oldNamesToNewNames[y] for y in self._Neighbours[self._newNamesToOldNames[item]] if y in self._oldNamesToNewNames]
+                for i in range(idx+1, len(_itemsToKeep)):
+                    _nextItem = _itemsToKeep[i]
+                    if _nextItem in neighbors:
+                        _cumulativeUtility += self._singleItemSetsUtility[self._newNamesToOldNames[_nextItem]]
+            if _cumulativeUtility >= self._minUtil:
+                _secondary.append(item)         
         self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
-        itemsToExplore = []
-        for item in itemsToKeep:
-            if self._utilityBinArraySU[item] >= _minUtil:
-                itemsToExplore.append(item)
-        utilitySum = 0
-        self._backTrackingRHUIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0, utilitySum)
+        _itemsToExplore = []
+        for item in _secondary:
+            if self._utilityBinArraySU[item] >= self._minUtil:
+                _itemsToExplore.append(item)
+        _commonitems = []
+        for i in range(self._dataset.maxItem):
+            _commonitems.append(i)
+        self._backtrackingEFIM(self._dataset.getTransactions(), _itemsToKeep, _itemsToExplore, 0)
+        _finalMemory = _ab._psutil.virtual_memory()[3]
+        memory = (_finalMemory - InitialMemory) / 10000
+        if memory > self._maxMemory:
+            self._maxMemory = memory
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Relative High Utility patterns were generated successfully using RHUIM algorithm")
+        print('Spatial High Utility Frequent Itemsets generated successfully using SHUFIM algorithm')
 
-    def _backTrackingRHUIM(self, transactionsOfP: list, itemsToKeep: list, itemsToExplore: list, prefixLength: int, utilitySumP: int) -> None:
+    def mine(self):
         """
-        A method to mine the RHUIs Recursively
-
-        :Attributes:
+        High Utility Frequent Pattern mining start here
+        """
+        self._startTime = _ab._time.time()
+        self._patternCount = 0
+        self._finalPatterns = {}
+        self._dataset = _Dataset(self._iFile, self._sep)
+        self._singleItemSetsSupport = _ab._defaultdict(int)
+        self._singleItemSetsUtility = _ab._defaultdict(int)
+        self._minUtil = int(self._minUtil)
+        self._minSup = self._convert(self._minSup)
+        with open(self._nFile, 'r') as o:
+            lines = o.readlines()
+            for line in lines:
+                line = line.split("\n")[0]
+                line_split = line.split(self._sep)
+                item = self._dataset.strToInt.get(line_split[0])
+                lst = []
+                for i in range(1, len(line_split)):
+                    lst.append(self._dataset.strToInt.get(line_split[i]))
+                self._Neighbours[item] = lst
+        o.close()
+        InitialMemory = _ab._psutil.virtual_memory()[3]
+        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
+        _itemsToKeep = []
+        for key in self._utilityBinArrayLU.keys():
+            if self._utilityBinArrayLU[key] >= self._minUtil and self._singleItemSetsSupport[key] >= self._minSup:
+                _itemsToKeep.append(key)
+        # sorting items in decreasing order of their utilities
+        _itemsToKeep = sorted(_itemsToKeep, key=lambda x: self._singleItemSetsUtility[x], reverse=True)
+        _currentName = 1
+        for idx, item in enumerate(_itemsToKeep):
+            self._oldNamesToNewNames[item] = _currentName
+            self._newNamesToOldNames[_currentName] = item
+            _itemsToKeep[idx] = _currentName
+            _currentName += 1
+        for transaction in self._dataset.getTransactions():
+            transaction.removeUnpromisingItems(self._oldNamesToNewNames)
+        self._sortDatabase(self._dataset.getTransactions())
+        _emptyTransactionCount = 0
+        for transaction in self._dataset.getTransactions():
+            if len(transaction.getItems()) == 0:
+                _emptyTransactionCount += 1
+        self._dataset.transactions = self._dataset.transactions[_emptyTransactionCount:]
+        # calculating neighborhood suffix utility values
+        _secondary = []
+        for idx, item in enumerate(_itemsToKeep):
+            _cumulativeUtility = self._singleItemSetsUtility[self._newNamesToOldNames[item]]
+            if self._newNamesToOldNames[item] in self._Neighbours:
+                neighbors = [self._oldNamesToNewNames[y] for y in self._Neighbours[self._newNamesToOldNames[item]] if y in self._oldNamesToNewNames]
+                for i in range(idx+1, len(_itemsToKeep)):
+                    _nextItem = _itemsToKeep[i]
+                    if _nextItem in neighbors:
+                        _cumulativeUtility += self._singleItemSetsUtility[self._newNamesToOldNames[_nextItem]]
+            if _cumulativeUtility >= self._minUtil:
+                _secondary.append(item)
+        self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
+        _itemsToExplore = []
+        for item in _secondary:
+            if self._utilityBinArraySU[item] >= self._minUtil:
+                _itemsToExplore.append(item)
+        _commonitems = []
+        for i in range(self._dataset.maxItem):
+            _commonitems.append(i)
+        self._backtrackingEFIM(self._dataset.getTransactions(), _itemsToKeep, _itemsToExplore, 0)
+        _finalMemory = _ab._psutil.virtual_memory()[3]
+        memory = (_finalMemory - InitialMemory) / 10000
+        if memory > self._maxMemory:
+            self._maxMemory = memory
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print('Spatial High Utility Frequent Itemsets generated successfully using SHUFIM algorithm')
 
+    def _backtrackingEFIM(self, transactionsOfP, itemsToKeep, itemsToExplore, prefixLength):
+        """
+        A method to mine the SHUFIs Recursively
         :param transactionsOfP: the list of transactions containing the current prefix P
         :type transactionsOfP: list
         :param itemsToKeep: the list of secondary items in the p-projected database
         :type itemsToKeep: list
         :param itemsToExplore: the list of primary items in the p-projected database
         :type itemsToExplore: list
         :param prefixLength: current prefixLength
         :type prefixLength: int
-        :param utilitySumP: a variable to hold sum of utilities of all items in P
-        :type utilitySumP int
         """
         self._candidateCount += len(itemsToExplore)
         for idx, e in enumerate(itemsToExplore):
+            initialMemory = _ab._psutil.virtual_memory()[3]
             transactionsPe = []
             utilityPe = 0
-            utilitySumPe = utilitySumP + self._singleItemSetsUtilities[e]
-            previousTransaction = transactionsOfP[0]
+            supportPe = 0
+            previousTransaction = []
             consecutiveMergeCount = 0
             for transaction in transactionsOfP:
                 items = transaction.getItems()
                 if e in items:
                     positionE = items.index(e)
                     if transaction.getLastPosition() == positionE:
                         utilityPe += transaction.getUtilities()[positionE] + transaction.prefixUtility
+                        supportPe += transaction.getSupport()
                     else:
                         projectedTransaction = transaction.projectTransaction(positionE)
                         utilityPe += projectedTransaction.prefixUtility
-                        if previousTransaction == transactionsOfP[0]:
+                        if previousTransaction == []:
                             previousTransaction = projectedTransaction
                         elif self._isEqual(projectedTransaction, previousTransaction):
                             if consecutiveMergeCount == 0:
                                 items = previousTransaction.items[previousTransaction.offset:]
                                 utilities = previousTransaction.utilities[previousTransaction.offset:]
+                                support = previousTransaction.getSupport()
                                 itemsCount = len(items)
                                 positionPrevious = 0
                                 positionProjection = projectedTransaction.offset
                                 while positionPrevious < itemsCount:
                                     utilities[positionPrevious] += projectedTransaction.utilities[positionProjection]
                                     positionPrevious += 1
                                     positionProjection += 1
                                 previousTransaction.prefixUtility += projectedTransaction.prefixUtility
                                 sumUtilities = previousTransaction.prefixUtility
                                 previousTransaction = _Transaction(items, utilities, previousTransaction.transactionUtility + projectedTransaction.transactionUtility)
                                 previousTransaction.prefixUtility = sumUtilities
+                                previousTransaction.support = support
+                                previousTransaction.support += projectedTransaction.getSupport()
                             else:
                                 positionPrevious = 0
                                 positionProjected = projectedTransaction.offset
                                 itemsCount = len(previousTransaction.items)
                                 while positionPrevious < itemsCount:
                                     previousTransaction.utilities[positionPrevious] += projectedTransaction.utilities[
                                         positionProjected]
                                     positionPrevious += 1
                                     positionProjected += 1
                                 previousTransaction.transactionUtility += projectedTransaction.transactionUtility
                                 previousTransaction.prefixUtility += projectedTransaction.prefixUtility
+                                previousTransaction.support += projectedTransaction.getSupport()
                             consecutiveMergeCount += 1
                         else:
                             transactionsPe.append(previousTransaction)
+                            supportPe += previousTransaction.getSupport()
                             previousTransaction = projectedTransaction
                             consecutiveMergeCount = 0
                     transaction.offset = positionE
-            if previousTransaction != transactionsOfP[0]:
+            if previousTransaction != []:
                 transactionsPe.append(previousTransaction)
+                supportPe += previousTransaction.getSupport()
             self._temp[prefixLength] = self._newNamesToOldNames[e]
-            utility_ratio_pe = float(utilityPe / utilitySumPe)
-            if (utilityPe >= self._minUtil) and (utility_ratio_pe * 100 >= self._minUR):
-                self._output(prefixLength, utilityPe, utility_ratio_pe)
-            self._useUtilityBinArraysToCalculateUpperBounds(transactionsPe, idx, itemsToKeep)
-            newItemsToKeep = []
-            newItemsToExplore = []
-            for l in range(idx + 1, len(itemsToKeep)):
-                itemK = itemsToKeep[l]
-                utility_sum_pek = utilitySumPe + self._singleItemSetsUtilities[itemK]
-                subtree_utility_ratio = float(self._utilityBinArraySU[itemK] / utility_sum_pek)
-                local_utility_ratio = float(self._utilityBinArrayLU[itemK] / utility_sum_pek)
-                if self._utilityBinArraySU[itemK] >= self._minUtil and subtree_utility_ratio * 100 >= self._minUR:
-                    newItemsToExplore.append(itemK)
-                    newItemsToKeep.append(itemK)
-                elif self._utilityBinArrayLU[itemK] >= self._minUtil and local_utility_ratio * 100 >= self._minUR:
-                    newItemsToKeep.append(itemK)
-            self._backTrackingRHUIM(transactionsPe, newItemsToKeep, newItemsToExplore, prefixLength + 1, utilitySumPe)
+            if utilityPe >= self._minUtil and supportPe >= self._minSup:
+                self._output(prefixLength, utilityPe, supportPe)
+            if supportPe >= self._minSup:
+                neighbourhoodList = self._calculateNeighbourIntersection(prefixLength)
+                #print(neighbourhoodList)
+                self._useUtilityBinArraysToCalculateUpperBounds(transactionsPe, idx, itemsToKeep, neighbourhoodList)
+                newItemsToKeep = []
+                newItemsToExplore = []
+                for l in range(idx + 1, len(itemsToKeep)):
+                    itemK = itemsToKeep[l]
+                    if self._utilityBinArraySU[itemK] >= self._minUtil:
+                        if itemK in neighbourhoodList:
+                            newItemsToExplore.append(itemK)
+                            newItemsToKeep.append(itemK)
+                    elif self._utilityBinArrayLU[itemK] >= self._minUtil:
+                        if itemK in neighbourhoodList:
+                            newItemsToKeep.append(itemK)
+                self._backtrackingEFIM(transactionsPe, newItemsToKeep, newItemsToExplore, prefixLength + 1)
+            finalMemory = _ab._psutil.virtual_memory()[3]
+            memory = (finalMemory - initialMemory) / 10000
+            if self._maxMemory < memory:
+                self._maxMemory = memory
 
-    def _useUtilityBinArraysToCalculateUpperBounds(self, transactionsPe: list, j: int, itemsToKeep: list) -> None:
+    def _useUtilityBinArraysToCalculateUpperBounds(self, transactionsPe, j, itemsToKeep, neighbourhoodList):
         """
         A method to  calculate the subtree utility and local utility of all items that can extend itemSet P U {e}
 
         :Attributes:
 
         :param transactionsPe: transactions the projected database for P U {e}
-        :type transactionsPe: list or Dataset
-        :param j: the position of j in the list of promising items
+        :type transactionsPe: list
+        :param j:the position of j in the list of promising items
         :type j:int
         :param itemsToKeep :the list of promising items
-        :type itemsToKeep: list or Dataset
+        :type itemsToKeep: list
+        :param neighbourhoodList : the list of promising items that can extend itemSet P U {e}
+        :type neighbourhoodList: list
+
         """
         for i in range(j + 1, len(itemsToKeep)):
             item = itemsToKeep[i]
             self._utilityBinArrayLU[item] = 0
             self._utilityBinArraySU[item] = 0
         for transaction in transactionsPe:
-            sumRemainingUtility = 0
-            i = len(transaction.getItems()) - 1
+            length = len(transaction.getItems())
+            i = length - 1
             while i >= transaction.offset:
                 item = transaction.getItems()[i]
                 if item in itemsToKeep:
-                    sumRemainingUtility += transaction.getUtilities()[i]
-                    self._utilityBinArraySU[item] += sumRemainingUtility + transaction.prefixUtility
+                    remainingUtility = 0
+                    if self._newNamesToOldNames[item] in self._Neighbours:
+                        itemNeighbours = self._Neighbours[self._newNamesToOldNames[item]]
+                        for k in range(i, length):
+                            transaction_item = transaction.getItems()[k]
+                            if self._newNamesToOldNames[transaction_item] in itemNeighbours and transaction_item in neighbourhoodList:
+                                remainingUtility += transaction.getUtilities()[k]
+
+                    remainingUtility += transaction.getUtilities()[i]
+                    self._utilityBinArraySU[item] += remainingUtility + transaction.prefixUtility
                     self._utilityBinArrayLU[item] += transaction.transactionUtility + transaction.prefixUtility
                 i -= 1
 
-    def _output(self, tempPosition: int, utility: int, utilityRatio: float) -> None:
+    def _calculateNeighbourIntersection(self, prefixLength):
         """
-         Method to print relative high utility itemSet
-
-         :Attributes:
-
-         :param tempPosition: position of last item 
+        A method to find common Neighbours
+        :param prefixLength: the prefix itemSet
+        :type prefixLength:int
+        """
+        intersectionList = self._Neighbours.get(self._temp[0])
+        for i in range(1, prefixLength+1):
+            intersectionList = self._intersection(self._Neighbours[self._temp[i]], intersectionList)
+        finalIntersectionList = []
+        if intersectionList is None:
+            return finalIntersectionList
+        for item in intersectionList:
+            if item in self._oldNamesToNewNames:
+                finalIntersectionList.append(self._oldNamesToNewNames[item])
+        return finalIntersectionList
+    
+    def _output(self, tempPosition, utility, support):
+        """
+         A method save all high-utility itemSet to file or memory depending on what the user chose
+         :param tempPosition: position of last item
          :type tempPosition : int 
          :param utility: total utility of itemSet
          :type utility: int
-         :param utilityRatio: utility ratio of an itemSet
-         :type utilityRatio: float
-        """
+         :param support: support of an itemSet
+         :type support: int
+         """
         self._patternCount += 1
         s1 = str()
         for i in range(0, tempPosition+1):
             s1 += self._dataset.intToStr.get((self._temp[i]))
             if i != tempPosition:
                 s1 += "\t"
-        self._finalPatterns[s1] = [utility, utilityRatio]
+        self._finalPatterns[s1] = [utility, support]
 
-    def _isEqual(self, transaction1: _Transaction, transaction2: _Transaction) -> bool:
+    def _isEqual(self, transaction1, transaction2):
         """
-         A method to Check if two transaction are identical
-
-         :Attributes:
-
-         :param  transaction1: the first transaction.
-         :type  transaction1: Transaction
-         :param  transaction2:   The second transaction.
-         :type  transaction2: Transaction
-         :return : whether both are identical or not
-         :rtype: bool
+        A method to Check if two transaction are identical
+        :param  transaction1: the first transaction
+        :type  transaction1: Trans
+        :param  transaction2:   the second transaction
+        :type  transaction2: Trans
+        :return : whether both are identical or not
+        :rtype: bool
         """
+
         length1 = len(transaction1.items) - transaction1.offset
         length2 = len(transaction2.items) - transaction2.offset
         if length1 != length2:
             return False
         position1 = transaction1.offset
         position2 = transaction2.offset
         while position1 < len(transaction1.items):
             if transaction1.items[position1] != transaction2.items[position2]:
                 return False
             position1 += 1
             position2 += 1
         return True
-
-    def _useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self, dataset: _Dataset) -> None:
+    
+    def _intersection(self, lst1, lst2):
         """
-        Scan the initial database to calculate the subtree utility of each item using a utility-bin array
+        A method that return the intersection of 2 list
 
-        :Attributes:
+        :param  lst1: items neighbour to item1
+        :type lst1: list
+        :param lst2: items neighbour to item2
+        :type lst2: list
+        :return :intersection of two lists
+        :rtype : list
+        """
+        temp = set(lst2)
+        lst3 = [value for value in lst1 if value in temp]
+        return lst3
 
+    def _useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self, dataset):
+        """
+        Scan the initial database to calculate the subtree utility of each item using a utility-bin array
         :param dataset: the transaction database
         :type dataset: Dataset
         """
         for transaction in dataset.getTransactions():
-            sumSU = 0
-            i = len(transaction.getItems()) - 1
-            while i >= 0:
-                item = transaction.getItems()[i]
-                currentUtility = transaction.getUtilities()[i]
-                sumSU += currentUtility
-                self._singleItemSetsUtilities[item] += currentUtility
-                if item in self._utilityBinArraySU.keys():
-                    self._utilityBinArraySU[item] += sumSU
-                else:
-                    self._utilityBinArraySU[item] = sumSU
-                i -= 1
+            items = transaction.getItems()
+            utilities = transaction.getUtilities()
+            for idx, item in enumerate(items):
+                if item not in self._utilityBinArraySU:
+                    self._utilityBinArraySU[item] = 0
+                if self._newNamesToOldNames[item] not in self._Neighbours:
+                    self._utilityBinArraySU[item] += utilities[idx]
+                    continue
+                i = idx + 1
+                sumSu = utilities[idx]
+                while i < len(items):
+                    if self._newNamesToOldNames[items[i]] in self._Neighbours[self._newNamesToOldNames[item]]:
+                        sumSu += utilities[i]
+                    i += 1
+                self._utilityBinArraySU[item] += sumSu
 
-    def sortDatabase(self, transactions: list) -> None:
+    def _sortDatabase(self, transactions):
         """
-        A Method to sort transaction
-
-        :Attributes:
-
+        A Method to sort transaction in the order of PMU
         :param transactions: transaction of items
-        :type transactions: list
-        :return: sorted transactions.
-        :rtype: Transactions or list
+        :type transactions: Transaction
+        :return: sorted transaction
+        :rtype: Trans
         """
-        cmp_items = _ab._functools.cmp_to_key(self.sort_transaction)
+        cmp_items = _comToKey(self._sortTransaction)
         transactions.sort(key=cmp_items)
 
-    def sort_transaction(self, trans1: _Transaction, trans2: _Transaction) -> int:
+    def _sortTransaction(self, trans1, trans2):
         """
-        A Method to sort transaction
-
-        :Attributes:
-
-        :param trans1: the first transaction .
-        :type trans1: Transaction
-        :param trans2:the second transaction.
-        :type trans2: Transaction
-        :return: sorted transaction.
-        :rtype:   Transaction
+        A Method to sort transaction in the order of PMU
+        :param trans1: the first transaction
+        :type trans1: Trans
+        :param trans2:the second transaction
+        :type trans2: Trans
+        :return: sorted transaction
+        :rtype: Trans
         """
         trans1_items = trans1.getItems()
         trans2_items = trans2.getItems()
         pos1 = len(trans1_items) - 1
         pos2 = len(trans2_items) - 1
         if len(trans1_items) < len(trans2_items):
             while pos1 >= 0:
@@ -722,118 +963,133 @@
                 sub = trans2_items[pos2] - trans1_items[pos1]
                 if sub != 0:
                     return sub
                 pos1 -= 1
                 pos2 -= 1
             return 0
 
-    def _useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset: _Dataset) -> None:
+    def _useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset):
         """
-        A method to calculate local utility of single itemSets
-
-        :Attributes:
-
-        :param dataset: the transaction database.
-        :type dataset: database
-
+        A method to scan the database using utility bin array to calculate the pmus
+        :param dataset: the transaction database
+        :type dataset: dataset
         """
         for transaction in dataset.getTransactions():
-            for item in transaction.getItems():
+            for idx, item in enumerate(transaction.getItems()):
+                self._singleItemSetsSupport[item] += 1
+                self._singleItemSetsUtility[item] += transaction.getUtilities()[idx]
+                pmu = transaction.getUtilities()[idx]
+                if item in self._Neighbours:
+                    neighbors = self._Neighbours[item]
+                    for idx, item in enumerate(transaction.getItems()):
+                        if item in neighbors:
+                            pmu += transaction.getUtilities()[idx]
                 if item in self._utilityBinArrayLU:
-                    self._utilityBinArrayLU[item] += transaction.transactionUtility
+                    # self._utilityBinArrayLU[item] += transaction.getPmus()[idx]
+                    self._utilityBinArrayLU[item] += pmu
                 else:
-                    self._utilityBinArrayLU[item] = transaction.transactionUtility
-
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
-        """Storing final patterns in a dataframe
+                    # self._utilityBinArrayLU[item] = transaction.getPmus()[idx]
+                    self._utilityBinArrayLU[item] = pmu
 
+    def getPatternsAsDataFrame(self):
+        """
+        Storing final patterns in a dataframe
         :return: returning patterns in a dataframe
         :rtype: pd.DataFrame
-            """
+        """
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Utility', 'UtilityRatio'])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Utility', 'Support'])
 
         return dataFrame
     
-    def getPatterns(self) -> dict:
-        """ Function to send the set of patterns after completion of the mining process
+    def getPatterns(self):
+        """
+        Function to send the set of patterns after completion of the mining process
 
         :return: returning patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def save(self, outFile: str) -> None:
+    def save(self, outFile):
         """
-        Complete set of frequent patterns will be loaded in to an output file
+        Complete set of patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % patternsAndSupport)
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+    def getMemoryRSS(self):
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
-       """
+        """
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
-        """Calculating the total amount of runtime taken by the mining process
-
+    def getRuntime(self):
+        """
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
-       """
+        """
         return self._endTime-self._startTime
-
-    def printResults(self) -> None:
+    
+    def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Relative Utility Patterns:", len(self.getPatterns()))
+        print("Total number of Spatial High Utility Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in seconds:", self.getRuntime())
+
+def main():
+    inputFile = '/home/nakamura/workspace/labwork/PAMI/PAMI/highUtilityGeoreferencedFrequentPattern/basic/mushroom_utility_spmf.txt'
+    neighborFile = '/home/nakamura/workspace/labwork/PAMI/PAMI/highUtilityGeoreferencedFrequentPattern/basic/mushroom_utility_spmf.txt'
+
+    minUtilCount = 10000
+    minSup = 100
+    seperator = ' '  
+    obj = SHUFIM(iFile=inputFile, nFile=neighborFile, minUtil=minUtilCount, minSup=minSup, sep=seperator)    #initialize
+    obj.startMine()   
+    obj.printResults()
+    print(obj.getPatterns())
 
 
 if __name__ == '__main__':
-    _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:    #includes separator
-            _ap = RHUIM(_ab._sys.argv[1], int(_ab._sys.argv[3]), float(_ab._sys.argv[4]), _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:    #takes "\t" as a separator
-            _ap = RHUIM(_ab._sys.argv[1], int(_ab._sys.argv[3]), float(_ab._sys.argv[4]))
-        _ap.startMine()
-        print("Total number of Relative High Utility Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
-    else:
-        _ap = RHUIM('/Users/likhitha/Downloads/utility_datasets/Utility_T10I4D100K.csv', 150000, 0.6, '\t')
-        _ap.startMine()
-        print("Total number of Relative High Utility Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
-        print("Error! The number of input parameters do not match the total number of parameters provided")
+    main()
+    # _ap = str()
+    # if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
+    #     if len(_ab._sys.argv) == 7:
+    #         _ap = SHUFIM(_ab._sys.argv[1], _ab._sys.argv[3], int(_ab._sys.argv[4]), _ab._sys.argv[5], _ab._sys.argv[6])
+    #     if len(_ab._sys.argv) == 6:
+    #         _ap = SHUFIM(_ab._sys.argv[1], _ab._sys.argv[3], int(_ab._sys.argv[4]), _ab._sys.argv[5])
+    #     _ap.startMine()
+    #     _ap.mine()
+    #     print("Total number of Spatial High Utility Frequent Patterns:", len(_ap.getPatterns()))
+    #     _ap.save(_ab._sys.argv[2])
+    #     print("Total Memory in USS:", _ap.getMemoryUSS())
+    #     print("Total Memory in RSS", _ap.getMemoryRSS())
+    #     print("Total ExecutionTime in seconds:", _ap.getRuntime())
+    # else:
+    #     print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/relativeHighUtilityPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/sequentialPatternMining/basic/SPADE.py` & `pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/SPADE.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,38 +2,42 @@
 # This program employs SPADE property (or downward closure property) to  reduce the search space effectively.
 # This algorithm employs breadth-first search technique when 1-2 length patterns and depth-first search when above 3 length patterns to find the complete set of frequent patterns in a transactional database.
 #
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     import PAMI.sequentialPatternMining.basic.SPADE as alg
 #
-#     obj = alg.SPADE(iFile, minSup)
+#             import PAMI.sequentialPatternMining.basic.SPADE as alg
 #
-#     obj.startMine()
+#             obj = alg.SPADE(iFile, minSup)
 #
-#     sequentialPatternMining = obj.getPatterns()
+#             obj.startMine()
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             sequentialPatternMining = obj.getPatterns()
 #
-#     obj.save(oFile)
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#     Df = obj.getPatternInDataFrame()
+#             obj.save(oFile)
 #
-#     memUSS = obj.getMemoryUSS()
+#             Df = obj.getPatternInDataFrame()
 #
-#     print("Total Memory in USS:", memUSS)
+#             memUSS = obj.getMemoryUSS()
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     run = obj.getRuntime()
+#             print("Total Memory in RSS", memRSS)
 #
+#             run = obj.getRuntime()
+#
+
+
+
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -47,28 +51,41 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 
+import pandas as pd
+from deprecated import deprecated
+
 from PAMI.sequentialPatternMining.basic import abstract as _ab
 
 _ab._sys.setrecursionlimit(10000)
 
 class SPADE(_ab._sequentialPatterns):
     """
     :Description:
 
         * SPADE is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
         * This program employs SPADE property (or downward closure property) to  reduce the search space effectively.
         * This algorithm employs breadth-first search technique when 1-2 length patterns and depth-first serch when above 3 length patterns to find the complete set of frequent patterns in a transactional database.
 
     :Reference:   Mohammed J. Zaki. 2001. SPADE: An Efficient Algorithm for Mining Frequent Sequences. Mach. Learn. 42, 1-2 (January 2001), 31-60. DOI=10.1023/A:1007652502315 http://dx.doi.org/10.1023/A:1007652502315
 
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of  Sequential frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of  Sequential frequent patterns
+    :param  minSup: float or int or str :
+                    minSup measure constraints the minimum number of transactions in a database where a pattern must appear
+                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
     :Attributes:
 
             iFile : str
                 Input file name or path of the input file
             oFile : str
                 Name of the output file or the path of output file
             minSup: float or int or str
@@ -115,19 +132,27 @@
             candidateToFrequent(candidateList)
                 Generates frequent patterns from the candidate patterns
             frequentToCandidate(frequentList, length)
                 Generates candidate patterns from the frequent patterns
 
     **Methods to execute code on terminal**
     -------------------------------------------
-            Format:
-                      >>>  python3 SPADE.py <inputFile> <outputFile> <minSup>
+    .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 SPADE.py <inputFile> <outputFile> <minSup>
+
+       Example usage:
+
+       (.venv) $ python3 SPADE.py sampleDB.txt patterns.txt 10.0
 
-            Example:
-                      >>>  python3 SPADE.py sampleDB.txt patterns.txt 10.0   (minSup will be considered in times of minSup and count of database transactions)
+
+               .. note:: minSup will be considered in times of minSup and count of database transactions
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
     .. code-block:: python
 
             import PAMI.sequentialPatternMining.basic.SPADE as alg
 
@@ -705,22 +730,43 @@
 
         x = list(sorted({latestWord, latestWord2}))
         x2 = x.pop()
         x3=x.pop()
         bs = bs + (-1,x3)
         bs2 = bs + (x2,)
         return  bs2,bs,x2
+
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
+        self.make1LenDatabase()
+        self.make2LenDatabase()
+        self.make3LenDatabase()
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Sequential Frequent patterns were generated successfully using SPADE algorithm ")
+
+    def Mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
         self.make1LenDatabase()
         self.make2LenDatabase()
         self.make3LenDatabase()
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
```

### Comparing `pami-2024.3.9.2/PAMI/sequentialPatternMining/basic/SPAM.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaAprioriTID.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,499 +1,601 @@
-# SPAM is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
-# This program employs SPAM property (or downward closure property) to  reduce the search space effectively.
-#  This algorithm employs breadth-first search technique  to find the complete set of frequent patterns in a sequential database.
-# **Importing this algorithm into a python program**
-# --------------------------------------------------------
+# cudaAprioriTID is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+#
 #
+# **Importing this algorithm into a python program**
+# ----------------------------------------------------
 #
-#     import PAMI.sequentialPatternMining.basic.SPAM as alg
+#             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
 #
-#     obj = alg.SPAM(iFile, minSup)
+#             obj = alg.cuAprioriBit(iFile, minSup)
 #
-#     obj.startMine()
+#             obj.mine()
 #
-#     sequentialPatternMining = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#     obj.save(oFile)
+#             obj.save(oFile)
 #
-#     Df = obj.getPatternInDataFrame()
+#             Df = obj.getPatternInDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
 #
 
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
+"""
+
+
+from deprecated import deprecated
+import abstract as _ab
+
+import os
+import csv
+import time
+import numpy as np
+import pycuda.gpuarray as _gpuarray
+import pycuda.autoinit
+import psutil
+import pycuda.driver as cuda
+from pycuda.compiler import SourceModule
+import pycuda
+
+deviceIntersection = SourceModule("""
+    __global__ void intersection(int *compareThis, int *compareThat, int *resultStart,
+                                 int *values, int *result, int resultX, int resultY){
+        const int tidX = blockIdx.x * blockDim.x + threadIdx.x;
+        const int tidY = blockIdx.y * blockDim.y + threadIdx.y;
+        int resultIndex = resultStart[tidX] + tidY;
+
+        // ignore if tidX or tidY is out of bounds or if the value comparing with is 0
+        if (tidX > resultX-1 || tidY > resultY-1 || values[compareThis[tidX] + tidY] == 0) return;
+
+        for (int i = 0; i < resultY; i++){
+            if ( values[compareThat[tidX] + i] == 0) return;
+            if ( values[compareThis[tidX] + tidY] == values[compareThat[tidX] + i]){
+                result[resultIndex] = values[compareThis[tidX] + tidY];
+                return;
+            }
+        }
+
+        //result[resultIndex] = values[compareThis[tidX] + tidY];
+
+    }
 
 """
+                                  )
 
-from PAMI.sequentialPatternMining.basic import abstract as _ab
-import sys
-sys.setrecursionlimit(10000)
 
-class SPAM(_ab._sequentialPatterns):
+class cudaAprioriTID:
     """
-    :Description:    SPAM is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
-                     This program employs SPAM property (or downward closure property) to  reduce the search space effectively.
-                     This algorithm employs breadth-first search technique  to find the complete set of frequent patterns in a sequential database.
-    :Reference:   J. Ayres, J. Gehrke, T.Yiu, and J. Flannick. Sequential Pattern Mining Using Bitmaps. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Edmonton, Alberta, Canada, July 2002.
+    :Description: Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+
+    :Reference:  Agrawal, R., Imieli ́nski, T., Swami, A.: Mining association rules between sets of items in large databases.
+            In: SIGMOD. pp. 207–216 (1993), https://doi.org/10.1145/170035.170072
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  minSup: int :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
-            iFile : str
-                Input file name or path of the input file
-            oFile : str
-                Name of the output file or the path of output file
-            minSup : float or int or str
-                The user can specify minSup either in count or proportion of database size.
-                If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-                Otherwise, it will be treated as float.
-                Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-            sep : str
-                This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
-                However, the users can override their default separator.
-            startTime : float
-                To record the start time of the mining process
-            endTime : float
-                To record the completion time of the mining process
-            finalPatterns : dict
-                Storing the complete set of patterns in a dictionary variable
-            memoryUSS : float
-                To store the total amount of USS memory consumed by the program
-            memoryRSS : float
-                To store the total amount of RSS memory consumed by the program
-            Database : list
-                To store the sequences of a database in list
-            _idDatabase : dict
-                To store the sequences of a database by bit map
-            _maxSeqLen:
-                the maximum length of subsequence in sequence.
-
-    :Methods:
-
-            _creatingItemSets():
-                Storing the complete sequences of the database/input file in a database variable
-            _convert(value):
-                To convert the user specified minSup value
-            make2BitDatabase():
-                To make 1 length frequent patterns by breadth-first search technique   and update Database to sequential database
-            DfsPruning(items,sStep,iStep):
-                the main algorithm of spam. This can search sstep and istep items and find next patterns, its sstep, and its istep. And call this function again by using them. Recursion until there are no more items available for exploration.
-            Sstep(s):
-                To convert bit to ssteo bit.The first time you get 1, you set it to 0 and subsequent ones to 1.(like 010101=>001111, 00001001=>00000111)
-            startMine()
-                Mining process will start from here
-            getPatterns()
-                Complete set of patterns will be retrieved with this function
-            savePatterns(oFile)
-                Complete set of frequent patterns will be loaded in to a output file
-            getPatternsAsDataFrame()
-                Complete set of frequent patterns will be loaded in to a dataframe
-            getMemoryUSS()
-                Total amount of USS memory consumed by the mining process will be retrieved from this function
-            getMemoryRSS()
-                Total amount of RSS memory consumed by the mining process will be retrieved from this function
-            getRuntime()
-                Total amount of runtime taken by the mining process will be retrieved from this function
-            candidateToFrequent(candidateList)
-                Generates frequent patterns from the candidate patterns
-            frequentToCandidate(frequentList, length)
-                Generates candidate patterns from the frequent patterns
-
-
-    **Executing the code on terminal**:
-    ----------------------------------------
-            Format:
-                      >>>  python3 SPAM.py <inputFile> <outputFile> <minSup> (<separator>)
-            Examples:
-                      >>> python3 SPAM.py sampleDB.txt patterns.txt 10.0
-
-    **Sample run of the importing code**:
-    -------------------------------------
-            import PAMI.sequentialPatternMining.basic.SPAM as alg
-
-            obj = alg.SPAM(iFile, minSup)
-
-            obj.startMine()
-
-            sequentialPatternMining = obj.getPatterns()
-
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
-
-            obj.savePatterns(oFile)
-
-            Df = obj.getPatternInDataFrame()
-
-            memUSS = obj.getMemoryUSS()
-
-            print("Total Memory in USS:", memUSS)
-
-            memRSS = obj.getMemoryRSS()
-
-            print("Total Memory in RSS", memRSS)
-
-            run = obj.getRuntime()
-
-            print("Total ExecutionTime in seconds:", run)
-
-    **Credits**:
-    ------------
-            The complete program was written by Shota Suzuki  under the supervision of Professor Rage Uday Kiran.
+        startTime : float
+          To record the start time of the mining process
+
+        endTime : float
+          To record the completion time of the mining process
+
+        finalPatterns : dict
+          Storing the complete set of patterns in a dictionary variable
+
+        memoryUSS : float
+          To store the total amount of USS memory consumed by the program
+
+        memoryRSS : float
+          To store the total amount of RSS memory consumed by the program
+
+        Database : list
+          To store the transactions of a database in list
+
+
+
+    **Methods to execute code on terminal**
+    ----------------------------------------------------
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 cudaAprioriTID.py <inputFile> <outputFile> <minSup>
+
+      Example Usage:
+
+      (.venv) $ python3 cudaAprioriTID.py sampleDB.txt patterns.txt 10.0
+
+    .. note:: minSup will be considered in percentage of database transactions
+
+
+    **Importing this algorithm into a python program**
+    ----------------------------------------------------
+
+    .. code-block:: python
+
+             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
+
+             obj = alg.cuAprioriBit(iFile, minSup)
+
+             obj.mine()
+
+             frequentPatterns = obj.getPatterns()
+
+             print("Total number of Frequent Patterns:", len(frequentPatterns))
+
+             obj.save(oFile)
+
+             Df = obj.getPatternInDataFrame()
+
+             memUSS = obj.getMemoryUSS()
+
+             print("Total Memory in USS:", memUSS)
+
+             memRSS = obj.getMemoryRSS()
+
+             print("Total Memory in RSS", memRSS)
+
+             run = obj.getRuntime()
+
+             print("Total ExecutionTime in seconds:", run)
+
+
+    **Credits:**
+    -------------
+
+             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+
     """
 
-    _minSup = float()
-    _startTime = float()
-    _endTime = float()
-    _finalPatterns = {}
+    __time = 0
+    __memRSS = 0
+    __memUSS = 0
+    __GPU_MEM = 0
+    filePath = ""
     _iFile = " "
-    _oFile = " "
-    _sep = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _Database = []
-    _idDatabase={}
-    _maxSeqLen=0
+    _sep = ""
+    _minSup = 0
+    Patterns = {}
+
+    def __init__(self, filePath, sep, minSup):
+        self.filePath = filePath
+        self.sep = sep
+        self.minSup = minSup
+        self.__time = 0
+        self.__memRSS = 0
+        self.__memUSS = 0
+
     def _creatingItemSets(self):
         """
-        Storing the complete sequences of the database/input file in a database variable
+        Storing the complete transactions of the database/input file in a database variable
         """
-        self._Database = []
-
+        self._Database = {}
+        lineNumber = 1
         if isinstance(self._iFile, _ab._pd.DataFrame):
             temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 temp = self._iFile['Transactions'].tolist()
-            if "tid" in i:
-                temp2=self._iFile[''].tolist()
-            addList=[]
-            addList.append(temp[0])
-            for k in range(len(temp)-1):
-                if temp2[k]==temp[k+1]:
-                    addList.append(temp[k+1])
-                else:
-                    self._Database.append(addList)
-                    addList=[]
-                    addList.append(temp[k+1])
-            self._Database.append(addList)
+
+            for k in temp:
+                self._Database.append(set(k))
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
+                    for i in range(len(line)):
+                        if line[i] in self._Database:
+                            self._Database[i].append(lineNumber)
+                        else:
+                            self._Database[i] = [lineNumber]
+                    lineNumber += 1
+
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    temp.pop()
-                    self._Database.append(temp)
+                    self._Database.append(set(temp))
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
-                            temp = [i.rstrip() for i in line.split('-1')]
-                            temp = [x for x in temp if x ]
-                            temp.pop()
-
-                            seq = []
-                            for i in temp:
-                                k = -2
-                                if len(i)>1:
-                                    seq.append(list(sorted(set(i.split()))))
-
-                                else:
-                                    seq.append(i)
-
-                            self._Database.append(seq)
-
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._Database.append(set(temp))
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _convert(self, value):
         """
-        To convert the user specified minSup value
+
+        To convert the type of user specified minSup value
 
         :param value: user specified minSup value
+
+        :type value: int or float or str
+
         :return: converted type
+
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
+    """def _readFile(self, fileName, separator):
+        
+        Reads a file and stores the data in a dictionary
+
+        Args:
+            fileName: string
+            separator: string
+
+        Returns:
+            dictionary: dictionary
+        
+        file = open(fileName, 'r')
+        dictionary = {}
+        lineNumber = 1
+        for line in file:
+            line = line.strip()
+            line = line.split(separator)
+            for i in range(len(line)):
+                if line[i] in dictionary:
+                    dictionary[line[i]].append(lineNumber)
+                else:
+                    dictionary[line[i]] = [lineNumber]
+            lineNumber += 1
 
-    def make2BitDatabase(self):
-        """
-        To make 1 length frequent patterns by breadth-first search technique   and update Database to sequential database
-        """
-        self._maxSeqLen=max([len(i) for i in self._Database])
-        lineNumber=0
-        idDatabase={}
-        for line in self._Database:
-            seqNumber=1
-            for seq in line:
-
-                for data in seq:
-                    if data in idDatabase:
-                        while lineNumber+1!=len(idDatabase[data]):
-                            idDatabase[data].append(0)
-                        idDatabase[data][lineNumber]+=int(2**(self._maxSeqLen-seqNumber))
-
-                    else:
-                        idDatabase[data]=[]
-                        while lineNumber+1!=len(idDatabase[data]):
-                            idDatabase[data].append(0)
-                        idDatabase[data][lineNumber]+=(int(2 ** (self._maxSeqLen-seqNumber)))
-
-                seqNumber+=1
-            lineNumber+=1
-        for key,val in idDatabase.items():
-
-            sup=self.countSup(val)
-            while lineNumber+1!=len(idDatabase[key]):
-                            idDatabase[key].append(0)
-            if sup>=self._minSup:
-                self._finalPatterns[str(key)+self._sep+"-2"]=sup
-                self._idDatabase[str(key)]=val
-
-    def DfsPruning(self,items,sStep,iStep):
-        """
-        the main algorithm of spam. This can search sstep and istep items and find next patterns, its sstep, and its istep. And call this function again by using them. Recursion until there are no more items available for exploration.
-
-        :Attributes:
-
-        items : str
-            The pattrens I got before
-        sStep : list
-            Items presumed to have "sstep" relationship with "items".(sstep is What appears later like a-b and a-c)
-        iStep : list
-            Items presumed to have "istep" relationship with "items"(istep is What appears in same time like ab and ac)
-
-        """
-        Snext=[]
-        Inext=[]
-        ns = self.Sstep(self._idDatabase[items])
-        for i in sStep:
-            nnext=[]
-            for k in  range(len(self._idDatabase[items])):
-                nandi=ns[k] & self._idDatabase[i][k]
-                nnext.append(nandi)
-
-
-            sup=self.countSup(nnext)
-            if sup>=self._minSup:
-                key=items+self._sep+"-1"+self._sep+i
-                self._finalPatterns[key+self._sep+"-1"+self._sep+"-2"]=sup
-                self._idDatabase[key]=nnext
-                Snext.append(i)
-
-        for i in Snext:
-            key = items+self._sep+"-1"+self._sep+i
-            self.DfsPruning(key,Snext,[k for k in Snext if self._Database.index(i)<self._Database.index(k)])
-        for i in iStep:
-            nnext = []
-
-            for k in range(len(self._idDatabase[items])):
-                nandi = self._idDatabase[items][k] & self._idDatabase[i][k]
-                nnext.append(nandi)
-            sup=self.countSup(nnext)
-            if sup>=self._minSup:
-                key=items+self._sep+str(i)
-                self._finalPatterns[key+self._sep+"-1"+self._sep+"-2"]=sup
-                self._idDatabase[key]=nnext
-                Inext.append(i)
-        for i in Inext:
-            key = items +self._sep +str(i)
-            self.DfsPruning(key,Snext,[k for k in Inext if self._Database.index(i)<self._Database.index(k)])
-
-    def Sstep(self,s):
-        """
-        To convert bit to Sstep bit.The first time you get 1, you set it to 0 and subsequent ones to 1.(like 010101=>001111, 00001001=>00000111)
-
-
-
-        :param s:list
-            to store each bit sequence
-        :return:
-            nextS:list to store the bit sequence converted by sstep
-
-        """
-        nextS=[]
-        for bins in s:
-            binS=str(bin(bins))
-
-
-            LenNum=2
-            for i in range(len(binS)-2):
-                if binS[LenNum] == "1":
-
-                    binS = binS[:LenNum] + "0" + binS[LenNum + 1:]
-                    while len(binS)-1!=LenNum:
-                        LenNum += 1
-                        binS = binS[:LenNum] + "1" + binS[LenNum + 1:]
-                    break
-                LenNum+=1
-            nextS.append(int(binS, 0))
-
-
-        return nextS
-
-    def countSup(self,n):
-        """
-        count support
-
-        :param n:list
-                to store each bit sequence
-        :return:
-            count: int support of this list
-        """
-        count=0
-        for i in n:
-            if "1" in str(bin(i)):
-                count+=1
-        return count
-
-    def startMine(self):
+        # sort dictionary by size of values
+        dictionary = dict(
+            sorted(dictionary.items(), key=lambda x: len(x[1]), reverse=True))
+        return dictionary, lineNumber
         """
-        Frequent pattern mining process will start from here
+    def getRuntime(self):
         """
-        self._Database = []
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        self.make2BitDatabase()
-        self._Database = [i for i in self._idDatabase.keys()]
-        for i in self._Database:
-            x=[]
-            for j in self._Database:
-                if self._Database.index(i)<self._Database.index(j):
-                    x.append(j)
-
-            self.DfsPruning(i,self._Database,x)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using Apriori algorithm ")
-
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
-        :return: returning USS memory consumed by the mining process
+        Calculating the total amount of time taken by the mining process
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
-        return self._memoryUSS
+        return self.__time
 
     def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
+        return self.__memRSS
 
-        return self._memoryRSS
-
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
-        :return: returning total amount of runtime taken by the mining process
+    def getMemoryUSS(self):
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
+        return self.__memUSS
 
-        return self._endTime - self._startTime
+    def getGPUMemory(self):
+        """
+        To calculate the total memory consumed by GPU
+        :return: return GPU memory
+        :rtype: int
+        """
 
-    def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
-        :return: returning frequent patterns in a dataframe
-        :rtype: pd.DataFrame
-        """
-
-        dataFrame = {}
-        data = []
-        for a, b in self._finalPatterns.items():
-            data.append([a, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataFrame
-
-    def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to an output file
-        :param outFile: name of the output file
-        :type outFile: file
-        """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y)
-            writer.write("%s \n" % s1)
+        return self.__GPU_MEM
 
     def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
+        """
+        Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self._finalPatterns
+        return self.Patterns
+
+    def get_numberOfPatterns(self):
+        return len(self.Patterns)
+
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        dev_Intersection = deviceIntersection.get_function("intersection")
+        startTime = time.time()
+        final = {}
+
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        minSup = self._minSup
+
+
+        data = dict(filter(lambda x: len(x[1]) >= self.minSup, self._Database()))
+        for key, value in data.items():
+            final[key] = len(value)
+
+        while len(data) > 1:
+            # sort data by size of values
+            data = dict(
+                sorted(data.items(), key=lambda x: len(x[1]), reverse=True))
+
+            values = list(data.values())
+            maxLength = values[0]
+            for i in range(1, len(values)):
+                while len(values[i]) != len(maxLength):
+                    values[i].append(0)
+
+            values = np.array(values)
+            resultSize = 0
+
+            compareThis = []
+            compareThat = []
+            resultStart = []
+            counter = 0
+
+            for i in range(len(values)):
+                for j in range(i+1, len(values)):
+                    resultSize += 1
+                    compareThis.append(i*len(maxLength))
+                    compareThat.append(j*len(maxLength))
+                    resultStart.append(counter)
+                    counter += len(maxLength)
+            result = np.zeros((resultSize, len(maxLength)), dtype=np.int32)
+
+            # convert all to uint32
+            compareThis = np.array(compareThis, dtype=np.uint32)
+            compareThat = np.array(compareThat, dtype=np.uint32)
+            resultStart = np.array(resultStart, dtype=np.uint32)
+            values = np.array(values, dtype=np.uint32)
+            result = np.array(result, dtype=np.uint32)
+
+            # allocate memory on GPU
+            compareThis_gpu = cuda.mem_alloc(compareThis.nbytes)
+            compareThat_gpu = cuda.mem_alloc(compareThat.nbytes)
+            resultStart_gpu = cuda.mem_alloc(resultStart.nbytes)
+            values_gpu = cuda.mem_alloc(values.nbytes)
+            result_gpu = cuda.mem_alloc(result.nbytes)
+
+            # add all nbytes to GPU_MEM
+            sumBytes = compareThis.nbytes + compareThat.nbytes + resultStart.nbytes + values.nbytes + result.nbytes
+            if sumBytes > self.__GPU_MEM:
+                self.__GPU_MEM = sumBytes
+
+            # copy data to GPU
+            cuda.memcpy_htod(compareThis_gpu, compareThis)
+            cuda.memcpy_htod(compareThat_gpu, compareThat)
+            cuda.memcpy_htod(resultStart_gpu, resultStart)
+            cuda.memcpy_htod(values_gpu, values)
+            cuda.memcpy_htod(result_gpu, result)
+
+            blockDim = (32, 32, 1)
+            gridDim = (resultSize//32 + 1, len(maxLength)//32 + 1, 1)
+
+            dev_Intersection(compareThis_gpu, compareThat_gpu,
+                             resultStart_gpu, values_gpu, result_gpu,
+                             np.uint32(resultSize), np.uint32(len(maxLength)),
+                             block=blockDim, grid=gridDim)
+
+            # copy data back to CPU
+            cuda.Context.synchronize()
+            cuda.memcpy_dtoh(result, result_gpu)
+
+            # free GPU memory
+            cuda.DeviceAllocation.free(compareThis_gpu)
+            cuda.DeviceAllocation.free(compareThat_gpu)
+            cuda.DeviceAllocation.free(resultStart_gpu)
+            cuda.DeviceAllocation.free(values_gpu)
+            cuda.DeviceAllocation.free(result_gpu)
+
+            keys = list(data.keys())
+            # convert all to string and add " "
+            for i in range(len(keys)):
+                keys[i] = str(keys[i]) + " "
+            data = {}
+            index = 0
+            for i in range(len(keys)):
+                for j in range(i+1, len(keys)):
+                    newResult = list(sorted(set(result[index])))
+                    newResult = list(filter(lambda x: x > 0, newResult))
+                    if len(newResult) >= self.minSup:
+                        keyI = keys[i].split()
+                        keyJ = keys[j].split()
+                        combinedKey = " ".join(list(str(x) for x in (
+                            sorted(int(x) for x in (set(keyI) | set(keyJ))))))
+                        if combinedKey not in final:
+                            data[combinedKey] = newResult
+                            final[combinedKey] = len(newResult)
+                    index += 1
+
+
+        self.__time = time.time() - startTime
+        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
+        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
+        self.Patterns = final
 
-    def printResults(self):
+    def mine(self):
         """
-        This function is used to print the results
+        Frequent pattern mining process will start from here
         """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        dev_Intersection = deviceIntersection.get_function("intersection")
+        startTime = time.time()
+        final = {}
+
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        minSup = self._minSup
+
+
+        data = dict(filter(lambda x: len(x[1]) >= self.minSup, self._Database()))
+        for key, value in data.items():
+            final[key] = len(value)
+
+        while len(data) > 1:
+            # sort data by size of values
+            data = dict(
+                sorted(data.items(), key=lambda x: len(x[1]), reverse=True))
+
+            values = list(data.values())
+            maxLength = values[0]
+            for i in range(1, len(values)):
+                while len(values[i]) != len(maxLength):
+                    values[i].append(0)
+
+            values = np.array(values)
+            resultSize = 0
+
+            compareThis = []
+            compareThat = []
+            resultStart = []
+            counter = 0
+
+            for i in range(len(values)):
+                for j in range(i+1, len(values)):
+                    resultSize += 1
+                    compareThis.append(i*len(maxLength))
+                    compareThat.append(j*len(maxLength))
+                    resultStart.append(counter)
+                    counter += len(maxLength)
+            result = np.zeros((resultSize, len(maxLength)), dtype=np.int32)
+
+            # convert all to uint32
+            compareThis = np.array(compareThis, dtype=np.uint32)
+            compareThat = np.array(compareThat, dtype=np.uint32)
+            resultStart = np.array(resultStart, dtype=np.uint32)
+            values = np.array(values, dtype=np.uint32)
+            result = np.array(result, dtype=np.uint32)
+
+            # allocate memory on GPU
+            compareThis_gpu = cuda.mem_alloc(compareThis.nbytes)
+            compareThat_gpu = cuda.mem_alloc(compareThat.nbytes)
+            resultStart_gpu = cuda.mem_alloc(resultStart.nbytes)
+            values_gpu = cuda.mem_alloc(values.nbytes)
+            result_gpu = cuda.mem_alloc(result.nbytes)
+
+            # add all nbytes to GPU_MEM
+            sumBytes = compareThis.nbytes + compareThat.nbytes + resultStart.nbytes + values.nbytes + result.nbytes
+            if sumBytes > self.__GPU_MEM:
+                self.__GPU_MEM = sumBytes
+
+            # copy data to GPU
+            cuda.memcpy_htod(compareThis_gpu, compareThis)
+            cuda.memcpy_htod(compareThat_gpu, compareThat)
+            cuda.memcpy_htod(resultStart_gpu, resultStart)
+            cuda.memcpy_htod(values_gpu, values)
+            cuda.memcpy_htod(result_gpu, result)
+
+            blockDim = (32, 32, 1)
+            gridDim = (resultSize//32 + 1, len(maxLength)//32 + 1, 1)
+
+            dev_Intersection(compareThis_gpu, compareThat_gpu,
+                             resultStart_gpu, values_gpu, result_gpu,
+                             np.uint32(resultSize), np.uint32(len(maxLength)),
+                             block=blockDim, grid=gridDim)
+
+            # copy data back to CPU
+            cuda.Context.synchronize()
+            cuda.memcpy_dtoh(result, result_gpu)
+
+            # free GPU memory
+            cuda.DeviceAllocation.free(compareThis_gpu)
+            cuda.DeviceAllocation.free(compareThat_gpu)
+            cuda.DeviceAllocation.free(resultStart_gpu)
+            cuda.DeviceAllocation.free(values_gpu)
+            cuda.DeviceAllocation.free(result_gpu)
+
+            keys = list(data.keys())
+            # convert all to string and add " "
+            for i in range(len(keys)):
+                keys[i] = str(keys[i]) + " "
+            data = {}
+            index = 0
+            for i in range(len(keys)):
+                for j in range(i+1, len(keys)):
+                    newResult = list(sorted(set(result[index])))
+                    newResult = list(filter(lambda x: x > 0, newResult))
+                    if len(newResult) >= self.minSup:
+                        keyI = keys[i].split()
+                        keyJ = keys[j].split()
+                        combinedKey = " ".join(list(str(x) for x in (
+                            sorted(int(x) for x in (set(keyI) | set(keyJ))))))
+                        if combinedKey not in final:
+                            data[combinedKey] = newResult
+                            final[combinedKey] = len(newResult)
+                    index += 1
+
+
+        self.__time = time.time() - startTime
+        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
+        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
+        self.Patterns = final
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = SPAM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cudaAprioriTID(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = SPAM(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cudaAprioriTID(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Frequent Patterns:", len(_Patterns))
-        _ap.savePatterns(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        _ap.mine()
+        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("GPU MEM: ", _ap.getGPUMemory())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
-
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
+
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.3.9.2/PAMI/sequentialPatternMining/basic/abstract.py` & `pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/sequentialPatternMining/basic/prefixSpan.py` & `pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/prefixSpan.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,40 +1,43 @@
 # Prefix Span is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
 # This program employs Prefix Span property (or downward closure property) to  reduce the search space effectively.
 # This algorithm employs depth-first search technique to find the complete set of frequent patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     import PAMI.frequentPattern.basic.prefixSpan as alg
 #
-#     obj = alg.prefixSpan(iFile, minSup)
+#             import PAMI.frequentPattern.basic.prefixSpan as alg
 #
-#     obj.startMine()
+#             obj = alg.prefixSpan(iFile, minSup,oFile,sep)
 #
-#     frequentPatterns = obj.getPatterns()
+#             obj.startMine()
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             frequentPatterns = obj.getPatterns()
 #
-#     obj.save(oFile)
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#     Df = obj.getPatternInDataFrame()
+#             obj.save(oFile)
 #
-#     memUSS = obj.getMemoryUSS()
+#             Df = obj.getPatternInDataFrame()
 #
-#     print("Total Memory in USS:", memUSS)
+#             memUSS = obj.getMemoryUSS()
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     run = obj.getRuntime()
+#             print("Total Memory in RSS", memRSS)
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             run = obj.getRuntime()
 #
+#             print("Total ExecutionTime in seconds:", run)
+#
+
+
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -46,28 +49,41 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 """
 
+import pandas as pd
+from deprecated import deprecated
+
 from PAMI.sequentialPatternMining.basic import abstract as _ab
 import copy
 import re
 _ab._sys.setrecursionlimit(10000)
 
 class prefixSpan(_ab._sequentialPatterns):
     """
     :Description:
         * Prefix Span is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
         * This program employs Prefix Span property (or downward closure property) to  reduce the search space effectively.
         * This algorithm employs depth-first search technique to find the complete set of frequent patterns in a transactional database.
 
     :Reference:   J. Pei, J. Han, B. Mortazavi-Asl, J. Wang, H. Pinto, Q. Chen, U. Dayal, M. Hsu: Mining Sequential Patterns by Pattern-Growth: The PrefixSpan Approach. IEEE Trans. Knowl. Data Eng. 16(11): 1424-1440 (2004)
 
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Sequential frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Sequential frequent patterns
+    :param  minSup: float or int or str :
+                    minSup measure constraints the minimum number of transactions in a database where a pattern must appear
+                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
         oFile : str
             Name of the output file or the path of output file
         minSup : float or int or str
@@ -110,19 +126,27 @@
         candidateToFrequent(candidateList)
             Generates frequent patterns from the candidate patterns
         frequentToCandidate(frequentList, length)
             Generates candidate patterns from the frequent patterns
 
     **Methods to execute code on terminal**
     ------------------------------------------
-            Format:
-                      >>>  python3 prefixSpan.py <inputFile> <outputFile> <minSup>
+    .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 prefixSpan.py <inputFile> <outputFile> <minSup>
+
+       Example usage:
+
+       (.venv) $ python3 prefixSpan.py sampleDB.txt patterns.txt 10
+
 
-            Example:
-                      >>>  python3 prefixSpan.py sampleDB.txt patterns.txt 10     (minSup will be considered in support count or frequency)
+               .. note:: minSup will be considered in support count or frequency
 
 
     **Importing this algorithm into a python program**
     -----------------------------------------------------
     .. code-block:: python
 
             import PAMI.frequentPattern.basic.prefixSpan as alg
@@ -476,24 +500,41 @@
 
 
         if len(seqDatabase)!=0:
             self.makeNext(seqDatabase,startrow)
         if len(seqDatabaseSame)!=0:
             self.makeNextSame(seqDatabaseSame,startrow)
 
-
-
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._Database=self.makeSupDatabase(self._Database,"")
+        self._minSup = self._convert(self._minSup)
+        self.makeSeqDatabaseFirst(self._Database)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using prefixSpan algorithm ")
+
+    def Mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._Database=self.makeSupDatabase(self._Database,"")
         self._minSup = self._convert(self._minSup)
         self.makeSeqDatabaseFirst(self._Database)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
```

### Comparing `pami-2024.3.9.2/PAMI/sequentialPatternMining/closed/abstract.py` & `pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py` & `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,43 +1,40 @@
-# Stable periodic pattern mining aims to discover all interesting patterns in a temporal database using three constraints minimum support,
-# maximum period and maximum liability, that have support no less than the user-specified minimum support  constraint and liability no
-# greater than maximum liability.
-#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
 #
-#     obj = alg.SPPEclat("../basic/sampleTDB.txt", 5, 3, 3)
+#             from PAMI.stablePeriodicFrequentPattern.basic import SPPGrowthDump as alg
 #
-#     obj.startMine()
+#             obj = alg.SPPGrowthDump(iFile, minSup, maxPer, maxLa)
 #
-#     Patterns = obj.getPatterns()
+#             obj.startMine()
 #
-#     print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
+#             Patterns = obj.getPatterns()
 #
-#     obj.save("patterns")
+#             print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             obj.save(oFile)
 #
-#     memUSS = obj.getMemoryUSS()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     print("Total Memory in USS:", memUSS)
+#             memUSS = obj.getMemoryUSS()
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     run = obj.getRuntime()
+#             print("Total Memory in RSS", memRSS)
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             run = obj.getRuntime()
 #
+#             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -49,183 +46,274 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
+from PAMI.stableperiodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
 
-from PAMI.stablePeriodicFrequentPattern.basic import abstract as _ab
+from urllib.request import urlopen
+import validators
+import pandas as pd
+import resource
+import time
+import sys
+import os
+import psutil
 
-class SPPEclat(_ab._stablePeriodicFrequentPatterns):
-    """
-    :Description:   Stable periodic pattern mining aims to dicover all interesting patterns in a temporal database using three contraints minimum support,
-                    maximum period and maximum lability, that have support no less than the user-specified minimum support  constraint and lability no
-                    greater than maximum lability.
-
-    :Reference:   Fournier-Viger, P., Yang, P., Lin, J. C.-W., Kiran, U. (2019). Discovering Stable Periodic-Frequent Patterns in Transactional Data. Proc.
-                  32nd Intern. Conf. on Industrial, Engineering and Other Applications of Applied Intelligent Systems (IEA AIE 2019), Springer LNAI, pp. 230-244
-
-    :Attributes:
-
-        iFile : file
-            Name of the Input file or path of the input file
-        oFile : file
-            Name of the output file or path of the output file
-        minSup : int or float or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer : int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
-        maxLa : int or float or str
-            The user can specify maxLa either in count or proportion of database size.
-            If the program detects the data type of maxLa is integer, then it treats maxLa is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxLa=10 will be treated as integer, while maxLa=10.0 will be treated as float
-        sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
-            However, the users can override their default separator.
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        Database : list
-            To store the transactions of a database in list
-        mapSupport : Dictionary
-            To maintain the information of item and their frequency
-        lno : int
-            it represents the total no of transactions
-        tree : class
-            it represents the Tree class
-        itemSetCount : int
-            it represents the total no of patterns
-        finalPatterns : dict
-            it represents to store the patterns
-        tidList : dict
-            stores the timestamps of an item
-
-    :Methods:
-
-        startMine()
-            Mining process will start from here
-        getPatterns()
-            Complete set of patterns will be retrieved with this function
-        save(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to an output file
-        getPatternsAsDataFrame()
-            Complete set of periodic-frequent patterns will be loaded in to a dataframe
-        getMemoryUSS()
-            Total amount of USS memory consumed by the mining process will be retrieved from this function
-        getMemoryRSS()
-            Total amount of RSS memory consumed by the mining process will be retrieved from this function
-        getRuntime()
-            Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets()
-            Scan the database and store the items with their timestamps which are periodic frequent
-        calculateLa()
-            Calculates the support and period for a list of timestamps.
-        Generation()
-            Used to implement prefix class equivalence method to generate the periodic patterns recursively
-
-
-
-    **Methods to execute code on terminal**
-    -----------------------------------------
-            Format:
-                      >>>   python3 basic.py <inputFile> <outputFile> <minSup> <maxPer> <maxLa>
-
-            Example:
-                      >>>    python3 basic.py sampleDB.txt patterns.txt 10.0 4.0 2.0
-
-                      .. note:: constraints will be considered in percentage of database transactions
-
-    **Importing this algorithm into a python program**
-    ---------------------------------------------------
-    ... code-block:: python
-
-                    from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
-
-                    obj = alg.PFPECLAT("../basic/sampleTDB.txt", 5, 3, 3)
-
-                    obj.startMine()
-
-                    Patterns = obj.getPatterns()
-
-                    print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
-
-                    obj.save("patterns")
-
-                    Df = obj.getPatternsAsDataFrame()
-
-                    memUSS = obj.getMemoryUSS()
-
-                    print("Total Memory in USS:", memUSS)
-
-                    memRSS = obj.getMemoryRSS()
-
-                    print("Total Memory in RSS", memRSS)
-
-                    run = obj.getRuntime()
-
-                    print("Total ExecutionTime in seconds:", run)
-
-    **Credits:**
-    --------------
-             The complete program was written by  P.Likhitha under the supervision of Professor Rage Uday Kiran.
+_minSup = int()
+_maxPer = int()
+_maxLa = int()
+_last = int()
 
-       """
-    _iFile = " "
-    _oFile = " "
+
+class _Node:
+
+    def __init__(self, item, children):
+        """
+        Initializing the Node class
+
+        :param item: Storing the item of a node
+        :type item: int or None
+        :param children: To maintain the children of a node
+        :type children: dict
+        """
+
+        self.item = item
+        self.children = children
+        self.parent = None
+        self.timeStamps = []
+
+    def addChild(self, node):
+        """
+        To add the children to a node
+
+        :param node: parent node in the tree
+        """
+
+        self.children[node.item] = node
+        node.parent = self
+
+class _Tree:
+    def __init__(self):
+        self.root = _Node(None, {})
+        self.summaries = {}
+        self.info = {}
+
+    def addTransaction(self, transaction, tid):
+        """
+        Adding a transaction into tree
+
+        :param transaction: To represent the complete database
+        :type transaction: list
+        :param tid: To represent the timestamp of a database
+        :type tid: list
+        :return: pfp-growth tree
+        """
+
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i] not in currentNode.children:
+                newNode = _Node(transaction[i], {})
+                currentNode.addChild(newNode)
+                if transaction[i] in self.summaries:
+                    self.summaries[transaction[i]].append(newNode)
+                else:
+                    self.summaries[transaction[i]] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i]]
+        currentNode.timeStamps = currentNode.timeStamps + tid
+
+    def getConditionalPatterns(self, alpha):
+        """
+        Generates all the conditional patterns of a respective node
+
+        :param alpha: To represent a Node in the tree
+        :type alpha: Node
+        :return: A tuple consisting of finalPatterns, conditional pattern base and information
+        """
+        finalPatterns = []
+        finalSets = []
+        for i in self.summaries[alpha]:
+            set1 = i.timeStamps
+            set2 = []
+            while i.parent.item is not None:
+                set2.append(i.parent.item)
+                i = i.parent
+            if len(set2) > 0:
+                set2.reverse()
+                finalPatterns.append(set2)
+                finalSets.append(set1)
+        finalPatterns, finalSets, info = self.conditionalDatabases(finalPatterns, finalSets)
+        return finalPatterns, finalSets, info
+
+    @staticmethod
+    def generateTimeStamps(node):
+        """
+        To get the timestamps of a node
+
+        :param node: A node in the tree
+        :return: Timestamps of a node
+        """
+
+        finalTimeStamps = node.timeStamps
+        return finalTimeStamps
+
+    def removeNode(self, nodeValue):
+        """
+        Removing the node from tree
+
+        :param nodeValue: To represent a node in the tree
+        :type nodeValue: node
+        :return: Tree with their nodes updated with timestamps
+        """
+
+        for i in self.summaries[nodeValue]:
+            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
+            del i.parent.children[nodeValue]
+
+    def getTimeStamps(self, alpha):
+        """
+        To get all the timestamps of the nodes which share same item name
+
+        :param alpha: Node in a tree
+        :return: Timestamps of a  node
+        """
+        temporary = []
+        for i in self.summaries[alpha]:
+            temporary += i.timeStamps
+        return temporary
+
+    @staticmethod
+    def getSupportAndPeriod(timeStamps):
+        """
+        To calculate the periodicity and support
+
+        :param timeStamps: Timestamps of an item set
+        :return: support, periodicity
+        """
+        global _maxPer, _last
+        previous = 0
+        la = 0
+        tsList = sorted(timeStamps)
+        for ts in tsList:
+            la = max(0, la + ts - previous - _maxPer)
+            previous = ts
+        la = max(0, la + _last - previous - _maxPer)
+        return len(timeStamps), la
+
+    def conditionalDatabases(self, conditionalPatterns, conditionalTimeStamps):
+        """
+        It generates the conditional patterns with periodic-frequent items
+
+        :param conditionalPatterns: conditionalPatterns generated from conditionPattern method of a respective node
+        :type conditionalPatterns: list
+        :param conditionalTimeStamps: Represents the timestamps of a conditional patterns of a node
+        :type conditionalTimeStamps: list
+        :returns: Returns conditional transactions by removing non-periodic and non-frequent items
+        """
+
+        global _maxPer, _minSup, _maxLa
+        pat = []
+        timeStamps = []
+        data1 = {}
+        for i in range(len(conditionalPatterns)):
+            for j in conditionalPatterns[i]:
+                if j in data1:
+                    data1[j] = data1[j] + conditionalTimeStamps[i]
+                else:
+                    data1[j] = conditionalTimeStamps[i]
+        updatedDictionary = {}
+        for m in data1:
+            updatedDictionary[m] = self.getSupportAndPeriod(data1[m])
+        updatedDictionary = {k: v for k, v in updatedDictionary.items() if v[0] >= _minSup and v[1] <= _maxLa}
+        count = 0
+        for p in conditionalPatterns:
+            p1 = [v for v in p if v in updatedDictionary]
+            trans = sorted(p1, key=lambda x: (updatedDictionary.get(x)[0], -x), reverse=True)
+            if len(trans) > 0:
+                pat.append(trans)
+                timeStamps.append(conditionalTimeStamps[count])
+            count += 1
+        return pat, timeStamps, updatedDictionary
+
+    def generatePatterns(self, prefix):
+        """
+        Generates the patterns
+
+        :param prefix: Forms the combination of items
+        :type prefix: list
+        :returns: yields patterns with their support and periodicity
+        """
+
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0], -x)):
+            pattern = prefix[:]
+            pattern.append(i)
+            yield pattern, self.info[i]
+            patterns, timeStamps, info = self.getConditionalPatterns(i)
+            conditionalTree = _Tree()
+            conditionalTree.info = info.copy()
+            for pat in range(len(patterns)):
+                conditionalTree.addTransaction(patterns[pat], timeStamps[pat])
+            if len(patterns) > 0:
+                for q in conditionalTree.generatePatterns(pattern):
+                    yield q
+            self.removeNode(i)
+
+class SPPGrowth():
+    _startTime = float()
+    _endTime = float()
     _minSup = str()
-    _maxPer = str()
+    _maxPer = float()
     _maxLa = float()
-    _sep = " "
-    _SPPList = {}
-    _itemList = []
-    _last = int()
     _finalPatterns = {}
-    _tsList = {}
-    _startTime = float()
-    _endTime = float()
+    _iFile = " "
+    _oFile = " "
+    _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
+    _rank = {}
+    _rankedUp = {}
+    _lno = 0
+    SPPList = {}
 
     def __init__(self, inputFile, minSup, maxPer, maxLa, sep='\t'):
         self._iFile = inputFile
         self._minSup = minSup
         self._maxPer = maxPer
         self._maxLa = maxLa
         self._sep = sep
 
-    def _creatingItemsets(self):
+    def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
         self._Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
+        if isinstance(self._iFile, pd.DataFrame):
+            data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-            if 'Patterns' in i:
-                self._Database = self._iFile['Patterns'].tolist()
+                data = self._iFile['Transactions'].tolist()
+            for i in range(len(data)):
+                tr = [ts[i][0]]
+                tr = tr + data[i]
+                self._Database.append(tr)
+
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if validators.url(self._iFile):
+                data = urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
                     self._Database.append(temp)
             else:
@@ -236,191 +324,283 @@
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
+    def _periodicFrequentOneItem(self):
+        """
+        Calculates the support of each item in the database and assign ranks to the items by decreasing support and returns the frequent items list
+
+        :returns: return the one-length periodic frequent patterns
+        """
+        global _last
+        tidLast = {}
+        la = {}
+        for transaction in self._Database:
+            ts = int(transaction[0])
+            for item in transaction[1:]:
+                if item not in self.SPPList:
+                    la[item] = max(0, ts - self._maxPer)
+                    self.SPPList[item] = [1, la[item]]
+                else:
+                    s = self.SPPList[item][0] + 1
+                    la[item] = max(0, la[item] + ts - tidLast.get(item) - self._maxPer)
+                    self.SPPList[item] = [s, max(la[item], self.SPPList[item][1])]
+                tidLast[item] = ts
+            _last = ts
+        for item in self.SPPList:
+            la[item] = max(0, la[item] + _last - tidLast[item] - self._maxPer)
+            self.SPPList[item][1] = max(la[item], self.SPPList[item][1])
+        self.SPPList = {k: v for k, v in self.SPPList.items() if v[0] >= self._minSup and v[1] <= self._maxLa}
+        self.SPPList = {k: v for k, v in sorted(self.SPPList.items(), key=lambda x: x[1][0], reverse=True)}
+        data = self.SPPList
+        pfList = [k for k, v in sorted(data.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
+        self._rank = dict([(index, item) for (item, index) in enumerate(pfList)])
+        #print(len(pfList))
+        return data, pfList
+
+    def _updateDatabases(self, dict1):
+        """
+        Remove the items which are not frequent from database and updates the database with rank of items
+
+        :param dict1: frequent items with support
+        :type dict1: dictionary
+        :return: Sorted and updated transactions
+        """
+        list1 = []
+        for tr in self._Database:
+            list2 = [int(tr[0])]
+            for i in range(1, len(tr)):
+                if tr[i] in dict1:
+                    list2.append(self._rank[tr[i]])
+            if len(list2) >= 2:
+                basket = list2[1:]
+                basket.sort()
+                list2[1:] = basket[0:]
+                list1.append(list2)
+        return list1
+
+    @staticmethod
+    def _buildTree(data, info):
+        """
+        It takes the database and support of each item and construct the main tree by setting root node as a null
+
+        :param data: it represents the one Database in database
+        :type data: list
+        :param info: it represents the support of each item
+        :type info: dictionary
+        :return: returns root node of tree
+        """
+
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(data)):
+            set1 = [data[i][0]]
+            rootNode.addTransaction(data[i][1:], set1)
+        return rootNode
+
+    def _savePeriodic(self, itemSet):
+        """
+        To convert the ranks of items in to their original item names
+
+        :param itemSet: frequent pattern.
+        :return: frequent pattern with original item names
+        """
+        t1 = str()
+        for i in itemSet:
+            t1 = t1 + self._rankedUp[i] + " "
+        return t1
+
     def _convert(self, value):
         """
-        to convert the type of user specified minSup value
+        To convert the given user specified value
 
-        :param value: user specified minSup value
-        :return: converted type
+        :param value: user specified value
+        :return: converted value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _createSPPList(self):
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self):
         """
-        to convert the single length stable periodic patterns
+        Mining process will start from this function
         """
-        tidLast = {}
-        la = {}
-        self._SPPList = {}
-        self._tsList = {}
-        for transaction in self._Database:
-            ts = int(transaction[0])
-            for item in transaction[1:]:
-                if item not in self._SPPList:
-                    la[item] = max(0, ts - self._maxPer)
-                    self._SPPList[item] = [1, la[item]]
-                    self._tsList[item] = [ts]
-                else:
-                    s = self._SPPList[item][0] + 1
-                    la[item] = max(0, la[item] + ts - tidLast.get(item) - self._maxPer)
-                    self._SPPList[item] = [s, max(la[item], self._SPPList[item][1])]
-                    self._tsList[item].append(ts)
-                tidLast[item] = ts
-            self._last = ts
-        for item in self._SPPList:
-            la[item] = max(0, la[item] + self._last - tidLast[item] - self._maxPer)
-            self._SPPList[item][1] = max(la[item], self._SPPList[item][1])
-        self._SPPList = {k: v for k, v in self._SPPList.items() if v[0] >= self._minSup and v[1] <= self._maxLa}
-        self._SPPList = {k: v for k, v in sorted(self._SPPList.items(), key=lambda x: x[1][0], reverse=True)}
-        self._Generation(list(self._SPPList), set())
-
-    def _Generation(self, GPPFList, CP):
-        """
-        To generate the patterns using depth-first search
-        """
-        for i in range(len(GPPFList)):
-            item = GPPFList[i]
-            CP1 = CP | {item}
-            if CP != set():
-                self._tsList['\t'.join(CP1)] = list(set(self._tsList['\t'.join(CP)]) & set(self._tsList[item]))
-            la = self._calculateLa(self._tsList['\t'.join(CP1)])
-            support = len(self._tsList['\t'.join(CP1)])
-            if la <= self._maxLa and len(self._tsList['\t'.join(CP1)]) >= self._minSup:
-                #CP = CP1
-                self._finalPatterns['\t'.join(CP1)] = [support, la]
-                if i+1 < len(GPPFList):
-                    self._Generation(GPPFList[i+1:], CP1)
-
-    def _calculateLa(self, tsList):
-        """ To calculate the liability of a patterns based on its timestamps"""
-        previous = 0
-        la = 0
-        tsList = sorted(tsList)
-        laList = []
-        for ts in tsList:
-            la = max(0, la + ts - previous - self._maxPer)
-            laList.append(la)
-            previous = ts
-            
-        la = max(0, la + self._last - previous - self._maxPer)
-        laList.append(la)
-        maxla = max(laList)
-        return maxla
 
-    def startMine(self):
-        """ Method to start the mining of patterns"""
-        self._startTime = _ab._time.time()
-        self._creatingItemsets()
+        global _minSup, _maxPer, _lno, _maxLa
+        self._startTime = time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
         self._maxPer = self._convert(self._maxPer)
         self._maxLa = self._convert(self._maxLa)
+        _minSup, _maxPer, _maxLa, _lno = self._minSup, self._maxPer, self._maxLa, len(self._Database)
+        print(_minSup, _maxPer, _maxLa)
+        if self._minSup > len(self._Database):
+            raise Exception("Please enter the minSup in range between 0 to 1")
+        generatedItems, pfList = self._periodicFrequentOneItem()
+        updatedDatabases = self._updateDatabases(generatedItems)
+        for x, y in self._rank.items():
+            self._rankedUp[y] = x
+        info = {self._rank[k]: v for k, v in generatedItems.items()}
+        Tree = self._buildTree(updatedDatabases, info)
+        patterns = Tree.generatePatterns([])
         self._finalPatterns = {}
-        #print(self._minSup, self._maxPer, self._maxLa)
-        self._createSPPList()
-        self._endTime = _ab._time.time()
+        for i in patterns:
+            sample = self._savePeriodic(i[0])
+            self._finalPatterns[sample] = i[1]
+        self._endTime = time.time()
+        process = psutil.Process(os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
-        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Stable Periodic Frequent patterns were generated successfully using basic algorithm ")
-
-
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+        print("Stable Periodic Frequent patterns were generated successfully using topk algorithm ")
 
-        :return: returning total amount of runtime taken by the mining process
-        :rtype: float
+    def Mine(self):
+        """
+        Mining process will start from this function
         """
-        return self._endTime - self._startTime
 
-    def getPatterns(self):
-        """ Function to return the set of stable periodic-frequent patterns after completion of the mining process
+        global _minSup, _maxPer, _lno, _maxLa
+        self._startTime = time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        self._maxLa = self._convert(self._maxLa)
+        _minSup, _maxPer, _maxLa, _lno = self._minSup, self._maxPer, self._maxLa, len(self._Database)
+        print(_minSup, _maxPer, _maxLa)
+        if self._minSup > len(self._Database):
+            raise Exception("Please enter the minSup in range between 0 to 1")
+        generatedItems, pfList = self._periodicFrequentOneItem()
+        updatedDatabases = self._updateDatabases(generatedItems)
+        for x, y in self._rank.items():
+            self._rankedUp[y] = x
+        info = {self._rank[k]: v for k, v in generatedItems.items()}
+        Tree = self._buildTree(updatedDatabases, info)
+        patterns = Tree.generatePatterns([])
+        self._finalPatterns = {}
+        for i in patterns:
+            sample = self._savePeriodic(i[0])
+            self._finalPatterns[sample] = i[1]
+        self._endTime = time.time()
+        process = psutil.Process(os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Stable Periodic Frequent patterns were generated successfully using topk algorithm ")
 
-        :return: returning stable periodic-frequent patterns
-        :rtype: dict
-        """
-        return self._finalPatterns
 
     def getMemoryUSS(self):
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def save(self, outFile):
+    def getMemoryRSS(self):
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
+        :return: returning RSS memory consumed by the mining process
+        :rtype: float
         """
-        Complete set of periodic-frequent patterns will be loaded in to an output file
 
-        :param outFile: name of the output file
-        :type outFile: csv file
+        return self._memoryRSS
+
+    def getRuntime(self):
+        """Calculating the total amount of runtime taken by the mining process
+
+        :return: returning total amount of runtime taken by the mining process
+        :rtype: float
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
-            writer.write("%s \n" % s1)
+
+        return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """
-        Storing final periodic-frequent patterns in a dataframe
+        """Storing final periodic-frequent patterns in a dataframe
 
         :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+            data.append([a, b[0], b[1]])
+            dataFrame = pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
-    def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
-        :return: returning RSS memory consumed by the mining process
-        :rtype: float
+    def save(self, outFile):
         """
+        Complete set of periodic-frequent patterns will be loaded in to an output file
 
-        return self._memoryRSS
-
-    def printResults(self):
+        :param outFile: name of the output file
+        :type outFile: csv file
         """
-        This function is used to print the results
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
+            writer.write("%s \n" % s1)
+
+    def getPatterns(self):
+        """ Function to send the set of periodic-frequent patterns after completion of the mining process
+
+        :return: returning periodic-frequent patterns
+        :rtype: dict
         """
-        print("Total number of Stable Periodic  Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        return self._finalPatterns
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
-        if len(_ab._sys.argv) == 7:
-            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
-        if len(_ab._sys.argv) == 6:
-            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+    if len(sys.argv) == 5 or len(sys.argv) == 6:
+        if len(sys.argv) == 6:
+            _ap = SPPGrowth(sys.argv[1], sys.argv[3], sys.argv[4], sys.argv[5])
+        if len(sys.argv) == 5:
+            _ap = SPPGrowth(sys.argv[1], sys.argv[3], sys.argv[4])
         _ap.startMine()
-        print("Total number of Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _Patterns = _ap.getPatterns()
+        print("Total number of Patterns:", len(_Patterns))
+        _ap.save(sys.argv[2])
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
+        '''ap = topk('https://www.u-aizu.ac.jp/~udayrage/datasets/temporalDatabases/temporal_retail.csv', 0.001, 0.005, 0.004)
+        #ap = topk('/Users/likhitha/Downloads/contextPrefixSpan.txt', 3, 6, 2, ' ')
+        ap.startMine()
+        Patterns = ap.getPatterns()
+        print("Total number of Frequent Patterns:", len(Patterns))
+        ap.save('/Users/Likhitha/Downloads/output')
+        memUSS = ap.getMemoryUSS()
+        print("Total Memory in USS:", memUSS)
+        memRSS = ap.getMemoryRSS()
+        print("Total Memory in RSS", memRSS)
+        run = ap.getRuntime()
+        print("Total ExecutionTime in ms:", run)'''
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

### Comparing `pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py` & `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,42 +2,44 @@
 # maximum period and maximum liability, that have support no less than the user-specified minimum support  constraint and liability no
 # greater than maximum liability.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.stablePeriodicFrequentPattern.basic import SPPGrowth as alg
+#             from PAMI.stablePeriodicFrequentPattern.basic import SPPGrowth as alg
 #
-#     obj = alg.SPPGrowth(iFile, minSup, maxPer, maxLa)
+#             obj = alg.SPPGrowth(iFile, minSup, maxPer, maxLa)
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     Patterns = obj.getPatterns()
+#             Patterns = obj.getPatterns()
 #
-#     print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
+#             print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
 #
-#     obj.save(oFile)
+#             obj.save(oFile)
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
-#
-#     print("Total ExecutionTime in seconds:", run)
+#             run = obj.getRuntime()
 #
+#             print("Total ExecutionTime in seconds:", run)
 #
 
+
+
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -269,27 +271,29 @@
 
     :Reference:   Dao, H.N. et al. (2022). Towards Efficient Discovery of Stable Periodic Patterns in Big Columnar Temporal Databases.
                   In: Fujita, H., Fournier-Viger, P., Ali, M., Wang, Y. (eds) Advances and Trends in Artificial Intelligence.
                   Theory and Practices in Artificial Intelligence. IEA/AIE 2022. Lecture Notes in Computer Science(), vol 13343. Springer, Cham.
                   https://doi.org/10.1007/978-3-031-08530-7_70
 
     :param  iFile: str :
+
                    Name of the Input file to mine complete set of frequent pattern's
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: str:
                    Minimum number of frequent patterns to be included in the output file.
     :param  maxLa: float:
                    Minimum number of ...
     :param  maxPer: float:
                    Maximum number of frequent patterns to be included in the output file.
 
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
+
     :Attributes:
 
             iFile : file
                 Name of the Input file or path of the input file
             oFile : file
                 Name of the output file or path of the output file
             minSup: int or float or str
@@ -356,21 +360,26 @@
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
 
     **Methods to execute code on terminal**
     -----------------------------------------
-            Format:
-                      >>>   python3 topk.py <inputFile> <outputFile> <minSup> <maxPer> <maxLa>
 
-            Example:
-                      >>>  python3 topk.py sampleTDB.txt patterns.txt 0.3 0.4 0.3
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 topk.py <inputFile> <outputFile> <minSup> <maxPer> <maxLa>
+
+      Example usage :
+
+      (.venv) $ python3 topk.py sampleTDB.txt patterns.txt 0.3 0.4 0.3
 
-                      .. note:: constraints will be considered in percentage of database transactions
+    .. note:: constraints will be considered in percentage of database transactions
 
     **Importing this algorithm into a python program**
     -----------------------------------------------------
     .. code-block:: python
 
             from PAMI.stablePeriodicFrequentPattern.basic import topk as alg
```

### Comparing `pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py` & `pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,40 +1,43 @@
-# **Importing this algorithm into a python program**
-# --------------------------------------------------------
-#
+# FSPGrowth is a transactional database and a spatial (or neighborhood) file, FSPM aims to discover all of those patterns
+# that satisfy the user-specified minimum support (minSup) and maximum distance (maxDist) constraints
 #
-#     from PAMI.stablePeriodicFrequentPattern.basic import SPPGrowthDump as alg
+# **Importing this algorithm into a python program**
+# -------------------------------------------------------
 #
-#     obj = alg.SPPGrowthDump(iFile, minSup, maxPer, maxLa)
+#             from PAMI.georeferencedFrequentPattern.basic import FSPGrowth as alg
 #
-#     obj.startMine()
+#             obj = alg.FSPGrowth("sampleTDB.txt", "sampleN.txt", 5)
 #
-#     Patterns = obj.getPatterns()
+#             obj.mine()
 #
-#     print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
+#             spatialFrequentPatterns = obj.getPatterns()
 #
-#     obj.save(oFile)
+#             print("Total number of Spatial Frequent Patterns:", len(spatialFrequentPatterns))
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             obj.save("outFile")
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
 #
 
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -43,517 +46,602 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
-from urllib.request import urlopen
-import validators
-import pandas as pd
-import resource
-import time
-import sys
-import os
-import psutil
-
-_minSup = int()
-_maxPer = int()
-_maxLa = int()
-_last = int()
 
 
+from PAMI.georeferencedFrequentPattern.basic import abstract as _ab
+from typing import List, Dict
+from deprecated import deprecated
+
 class _Node:
+    """
+    A class used to represent the node of frequentPatternTree
 
-    def __init__(self, item, children):
-        """
-        Initializing the Node class
+    :Attributes:
 
-        :param item: Storing the item of a node
-        :type item: int or None
-        :param children: To maintain the children of a node
-        :type children: dict
-        """
+        item : int
+            Storing item of a node
+        count : int
+            To maintain the support of node
+        children : dict
+            To maintain the children of node
+        prefix : list
+            To maintain the prefix of node
+    """
 
+    def __init__(self, item, count, children):
         self.item = item
+        self.count = count
         self.children = children
-        self.parent = None
-        self.timeStamps = []
+        self.prefix = []
 
-    def addChild(self, node):
-        """
-        To add the children to a node
 
-        :param node: parent node in the tree
-        """
+class _Tree:
+    """
+    A class used to represent the frequentPatternGrowth tree structure
 
-        self.children[node.item] = node
-        node.parent = self
+    :Attributes:
+
+        root : Node
+            The first node of the tree set to Null.
+        nodeLink : dict
+            Stores the nodes which shares same item
+
+    :Methods:
+
+        createTree(transaction,count)
+            Adding transaction into the tree
+        linkNode(node)
+            Adding node to nodeLink
+        createCPB(item,neighbour)
+            Create conditional pattern base based on item and neighbour
+        mergeTree(tree,fpList)
+            Merge tree into yourself
+        createTransactions(fpList)
+            Create transactions from yourself
+        getPattern(item,suffixItem,minSup,neighbour)
+            Get frequent patterns based on suffixItem
+        mining(minSup,isResponsible = lambda x:True,neighbourhood=None)
+            Mining yourself
+    """
 
-class _Tree:
     def __init__(self):
-        self.root = _Node(None, {})
-        self.summaries = {}
-        self.info = {}
+        self.root = _Node(None, 0, {})
+        self.nodeLink = _ab._OrderedDict()
 
-    def addTransaction(self, transaction, tid):
+    def createTree(self, transaction, count):
         """
-        Adding a transaction into tree
+        Create tree or add transaction into yourself.
 
-        :param transaction: To represent the complete database
+        :param transaction: Transactions list
         :type transaction: list
-        :param tid: To represent the timestamp of a database
-        :type tid: list
-        :return: pfp-growth tree
-        """
-
-        currentNode = self.root
-        for i in range(len(transaction)):
-            if transaction[i] not in currentNode.children:
-                newNode = _Node(transaction[i], {})
-                currentNode.addChild(newNode)
-                if transaction[i] in self.summaries:
-                    self.summaries[transaction[i]].append(newNode)
-                else:
-                    self.summaries[transaction[i]] = [newNode]
-                currentNode = newNode
+        :param count: Number of items in the transactions list
+        :type count: int
+        :return: Tree
+        """
+        current = self.root
+        for item in transaction:
+            if item not in current.children:
+                current.children[item] = _Node(item, count, {})
+                current.children[item].prefix = transaction[0:transaction.index(item)]
+                self.linkNode(current.children[item])
             else:
-                currentNode = currentNode.children[transaction[i]]
-        currentNode.timeStamps = currentNode.timeStamps + tid
+                current.children[item].count += count
+            current = current.children[item]
+        return self
+
+    def linkNode(self, node):
+        """
+        Maintain link of node by adding node to nodeLink
+
+        :param node: Node to link
+        :type node: Node
+        """
+        if node.item in self.nodeLink:
+            self.nodeLink[node.item].append(node)
+        else:
+            self.nodeLink[node.item] = []
+            self.nodeLink[node.item].append(node)
+
+    def createCPB(self, item, neighbour):
+        """
+        Create conditional pattern base based on item and neighbour
+
+        :param item: Item to check conditional pattern
+        :type item: str
+        :param neighbour: Neighbour to check conditional pattern
+        :type neighbour: dict
+        :return: Tree
+        """
+        pTree = _Tree()
+        for node in self.nodeLink[item]:
+            # print(node.item, neighbour[node.item])
+            if node.item in neighbour:
+                node.prefix = [item for item in node.prefix if item in neighbour.get(node.item)]
+            pTree.createTree(node.prefix, node.count)
+        return pTree
+
+    def mergeTree(self, tree, fpList):
+        """
+        Merge tree into yourself
+
+        :param tree: Tree to merge into yourself
+        :type tree: Tree
+        :param fpList: List of FPs to merge into yourself after merging into your tree and creating your own transactions
+        :type fpList: list
+        :return: Tree
+        """
+        transactions = tree.createTransactions(fpList)
+        for transaction in transactions:
+            self.createTree(transaction, 1)
+        return self
+
+    def createTransactions(self, fpList):
+        """
+        Create transactions that configure yourself
+
+        :param fpList: List of FPs to merge into yourself after merging into your tree and creating your own transactions
+        :type fpList: list
+        :return: transaction list
+        :rtype: list
+        """
+        transactions = []
+        flist = [x for x in fpList if x in self.nodeLink]
+        for item in reversed(flist):
+            for node in self.nodeLink[item]:
+                if node.count != 0:
+                    transaction = node.prefix
+                    transaction.append(node.item)
+                    transactions.extend([transaction for i in range(node.count)])
+                    current = self.root
+                    for i in transaction:
+                        current = current.children[i]
+                        current.count -= node.count
+        return transactions
+
+    def getPattern(self, item, suffixItem, minSup, neighbour):
+        """
+        Get frequent patterns based on suffixItem
+
+        :param item: Item to get patterns
+        :type item: int
+        :param suffixItem: Suffix item to get patterns
+        :type suffixItem: tuple
+        :param minSup: Minimum Support to get patterns
+        :type minSup: int
+        :param neighbour: Neighbour item to consider in the pattern
+        :type neighbour: dict
+        :return: Pattern list
+        :rtype: list
+        """
+        pTree = self.createCPB(item, neighbour)
+        frequentItems = {}
+        frequentPatterns = []
+        for i in pTree.nodeLink.keys():
+            frequentItems[i] = 0
+            for node in pTree.nodeLink[i]:
+                frequentItems[i] += node.count
+        frequentItems = {key: value for key, value in frequentItems.items() if value >= minSup}
+        for i in frequentItems:
+            pattern = suffixItem + "\t" + i
+            frequentPatterns.append((pattern, frequentItems[i]))
+            frequentPatterns.extend(pTree.getPattern(i, pattern, minSup, neighbour))
+        return frequentPatterns
+
+    def mining(self, minSup:callable[int, float], neighbourhood: [Dict[int, List[int]]] = None):
+        """
+        Pattern mining on your own
+
+        :param minSup: Minimum Support for your pattern for Mining
+        :type minSup: int or float
+        :param neighbourhood: function
+        :type neighbourhood: dict
+        :return: list
+        """
+        frequentPatterns = []
+        flist = sorted([item for item in self.nodeLink.keys()])
+        for item in reversed(flist):
+            frequentPatterns.extend(self.getPattern(item, item, minSup, neighbourhood))
+        return frequentPatterns
+
+
+class FSPGrowth(_ab._spatialFrequentPatterns):
+    """
+    :Description:   Given a transactional database and a spatial (or neighborhood) file, FSPM aims to discover all of those patterns
+                    that satisfy the user-specified minimum support (minSup) and maximum distance (maxDist) constraints
+
+    :Reference:   Rage, Uday & Fournier Viger, Philippe & Zettsu, Koji & Toyoda, Masashi & Kitsuregawa, Masaru. (2020).
+                  Discovering Frequent Spatial Patterns in Very Large Spatiotemporal Databases.
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Geo-referenced frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Geo-referenced frequent patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param nFile: str :
+                   Name of the input file to mine complete set of Geo-referenced frequent patterns
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+    :Attributes:
+
+        iFile : file
+            Input file name or path of the input file
+        nFile : file
+            Neighbourhood file name or path of the neighbourhood file
+        oFile : file
+            Name of the output file or the path of output file
+        minSup : float
+            The user can specify minSup either in count or proportion of database size.
+        finalPatterns : dict
+            Storing the complete set of patterns in a dictionary variable
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
+
+    :Methods:
+
+        startMine()
+            This function starts pattern mining.
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        save(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsInDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        getNeighbour(string)
+            This function changes string to tuple(neighbourhood).
+        getFrequentItems(database)
+            This function create frequent items from database.
+        genCondTransaction(transaction, rank)
+            This function generates conditional transaction for processing on each workers.
+        getPartitionId(item)
+            This function generates partition id
+        mapNeighbourToNumber(neighbour, rank)
+            This function maps neighbourhood to number.
+            Because in this program, each item is mapped to number based on fpList so that it can be distributed.
+            So the contents of neighbourhood must also be mapped to a number.
+        createFPTree()
+            This function creates FPTree.
+        getAllFrequentPatterns(data, fpList, ndata)
+            This function generates all frequent patterns
+
+    **Executing the code on terminal :**
+    ----------------------------------------
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 FSPGrowth.py <inputFile> <outputFile> <neighbourFile> <minSup>
+
+      Example Usage:
+
+      (.venv) $ python3 FSPGrowth.py sampleTDB.txt output.txt sampleN.txt 0.5
+
+    .. note:: minSup will be considered in percentage of database transactions
+
+
+
+    **Sample run of importing the code :**
+    ----------------------------------------
+    .. code-block:: python
+
+        from PAMI.georeferencedFrequentPattern.basic import FSPGrowth as alg
+
+        obj = alg.FSPGrowth("sampleTDB.txt", "sampleN.txt", 5)
+
+        obj.mine()
 
-    def getConditionalPatterns(self, alpha):
-        """
-        Generates all the conditional patterns of a respective node
+        spatialFrequentPatterns = obj.getPatterns()
+
+        print("Total number of Spatial Frequent Patterns:", len(spatialFrequentPatterns))
+
+        obj.save("outFile")
 
-        :param alpha: To represent a Node in the tree
-        :type alpha: Node
-        :return: A tuple consisting of finalPatterns, conditional pattern base and information
-        """
-        finalPatterns = []
-        finalSets = []
-        for i in self.summaries[alpha]:
-            set1 = i.timeStamps
-            set2 = []
-            while i.parent.item is not None:
-                set2.append(i.parent.item)
-                i = i.parent
-            if len(set2) > 0:
-                set2.reverse()
-                finalPatterns.append(set2)
-                finalSets.append(set1)
-        finalPatterns, finalSets, info = self.conditionalDatabases(finalPatterns, finalSets)
-        return finalPatterns, finalSets, info
-
-    @staticmethod
-    def generateTimeStamps(node):
-        """
-        To get the timestamps of a node
-
-        :param node: A node in the tree
-        :return: Timestamps of a node
-        """
-
-        finalTimeStamps = node.timeStamps
-        return finalTimeStamps
-
-    def removeNode(self, nodeValue):
-        """
-        Removing the node from tree
-
-        :param nodeValue: To represent a node in the tree
-        :type nodeValue: node
-        :return: Tree with their nodes updated with timestamps
-        """
-
-        for i in self.summaries[nodeValue]:
-            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
-            del i.parent.children[nodeValue]
-
-    def getTimeStamps(self, alpha):
-        """
-        To get all the timestamps of the nodes which share same item name
-
-        :param alpha: Node in a tree
-        :return: Timestamps of a  node
-        """
-        temporary = []
-        for i in self.summaries[alpha]:
-            temporary += i.timeStamps
-        return temporary
-
-    @staticmethod
-    def getSupportAndPeriod(timeStamps):
-        """
-        To calculate the periodicity and support
-
-        :param timeStamps: Timestamps of an item set
-        :return: support, periodicity
-        """
-        global _maxPer, _last
-        previous = 0
-        la = 0
-        tsList = sorted(timeStamps)
-        for ts in tsList:
-            la = max(0, la + ts - previous - _maxPer)
-            previous = ts
-        la = max(0, la + _last - previous - _maxPer)
-        return len(timeStamps), la
-
-    def conditionalDatabases(self, conditionalPatterns, conditionalTimeStamps):
-        """
-        It generates the conditional patterns with periodic-frequent items
-
-        :param conditionalPatterns: conditionalPatterns generated from conditionPattern method of a respective node
-        :type conditionalPatterns: list
-        :param conditionalTimeStamps: Represents the timestamps of a conditional patterns of a node
-        :type conditionalTimeStamps: list
-        :returns: Returns conditional transactions by removing non-periodic and non-frequent items
-        """
-
-        global _maxPer, _minSup, _maxLa
-        pat = []
-        timeStamps = []
-        data1 = {}
-        for i in range(len(conditionalPatterns)):
-            for j in conditionalPatterns[i]:
-                if j in data1:
-                    data1[j] = data1[j] + conditionalTimeStamps[i]
-                else:
-                    data1[j] = conditionalTimeStamps[i]
-        updatedDictionary = {}
-        for m in data1:
-            updatedDictionary[m] = self.getSupportAndPeriod(data1[m])
-        updatedDictionary = {k: v for k, v in updatedDictionary.items() if v[0] >= _minSup and v[1] <= _maxLa}
-        count = 0
-        for p in conditionalPatterns:
-            p1 = [v for v in p if v in updatedDictionary]
-            trans = sorted(p1, key=lambda x: (updatedDictionary.get(x)[0], -x), reverse=True)
-            if len(trans) > 0:
-                pat.append(trans)
-                timeStamps.append(conditionalTimeStamps[count])
-            count += 1
-        return pat, timeStamps, updatedDictionary
-
-    def generatePatterns(self, prefix):
-        """
-        Generates the patterns
-
-        :param prefix: Forms the combination of items
-        :type prefix: list
-        :returns: yields patterns with their support and periodicity
-        """
-
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0], -x)):
-            pattern = prefix[:]
-            pattern.append(i)
-            yield pattern, self.info[i]
-            patterns, timeStamps, info = self.getConditionalPatterns(i)
-            conditionalTree = _Tree()
-            conditionalTree.info = info.copy()
-            for pat in range(len(patterns)):
-                conditionalTree.addTransaction(patterns[pat], timeStamps[pat])
-            if len(patterns) > 0:
-                for q in conditionalTree.generatePatterns(pattern):
-                    yield q
-            self.removeNode(i)
+        memUSS = obj.getMemoryUSS()
 
-class SPPGrowth():
+        print("Total Memory in USS:", memUSS)
+
+        memRSS = obj.getMemoryRSS()
+
+        print("Total Memory in RSS", memRSS)
+
+        run = obj.getRuntime()
+
+        print("Total ExecutionTime in seconds:", run)
+
+    **Credits:**
+    --------------
+        The complete program was written by Yudai Masu under the supervision of Professor Rage Uday Kiran.
+    """
+
+    _minSup = float()
     _startTime = float()
     _endTime = float()
-    _minSup = str()
-    _maxPer = float()
-    _maxLa = float()
     _finalPatterns = {}
     _iFile = " "
+    _nFile = " "
     _oFile = " "
     _sep = " "
+    _lno = 0
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _rank = {}
-    _rankedUp = {}
-    _lno = 0
-    SPPList = {}
+    _neighbourList = {}
+    _fpList = []
 
-    def __init__(self, inputFile, minSup, maxPer, maxLa, sep='\t'):
-        self._iFile = inputFile
-        self._minSup = minSup
-        self._maxPer = maxPer
-        self._maxLa = maxLa
-        self._sep = sep
-
-    def _creatingItemSets(self):
+    def _readDatabase(self):
         """
-        Storing the complete transactions of the database/input file in a database variable
+        Read input file and neighborhood file
         """
+
         self._Database = []
-        if isinstance(self._iFile, pd.DataFrame):
-            data, ts = [], []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            for i in range(len(data)):
-                tr = [ts[i][0]]
-                tr = tr + data[i]
-                self._Database.append(tr)
-
+                self._Database = self._iFile['Transactions'].tolist()
+            self._lno = len(self._Database)
         if isinstance(self._iFile, str):
-            if validators.url(self._iFile):
-                data = urlopen(self._iFile)
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
+                    self._lno += 1
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
                     self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
+                            self._lno += 1
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
-                    print("File Not Found")
+                    print("File Not Found1")
                     quit()
 
-    def _periodicFrequentOneItem(self):
+        self._neighbourList = {}
+        if isinstance(self._nFile, _ab._pd.DataFrame):
+            data, items = [], []
+            if self._nFile.empty:
+                print("its empty..")
+            i = self._nFile.columns.values.tolist()
+            if 'item' in i:
+                items = self._nFile['items'].tolist()
+            if 'Neighbours' in i:
+                data = self._nFile['Neighbours'].tolist()
+            for k in range(len(items)):
+                self._neighbourList[items[k][0]] = data[k]
+            # print(self.Database)
+        if isinstance(self._nFile, str):
+            if _ab._validators.url(self._nFile):
+                data = _ab._urlopen(self._nFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._neighbourList[temp[0]] = temp[1:]
+            else:
+                try:
+                    with open(self._nFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._neighbourList[temp[0]] = temp[1:]
+                except IOError:
+                    print("File Not Found2")
+                    quit()
+
+    def _getFrequentItems(self):
+        """
+        Create frequent items and self.fpList from self.Database
         """
-        Calculates the support of each item in the database and assign ranks to the items by decreasing support and returns the frequent items list
+        oneFrequentItem = {}
+        for transaction in self._Database:
+            for item in transaction:
+                oneFrequentItem[item] = oneFrequentItem.get(item, 0) + 1
+        self._finalPatterns = {key: value for key, value in oneFrequentItem.items() if value >= self._minSup}
+        self._fpList = list(dict(sorted(oneFrequentItem.items(), key=lambda x: x[1], reverse=True)))
 
-        :returns: return the one-length periodic frequent patterns
+    def _createFPTree(self):
+        """
+        Create FP Tree and self.fpList from self.Database
         """
-        global _last
-        tidLast = {}
-        la = {}
+        FPTree = _Tree()
         for transaction in self._Database:
-            ts = int(transaction[0])
-            for item in transaction[1:]:
-                if item not in self.SPPList:
-                    la[item] = max(0, ts - self._maxPer)
-                    self.SPPList[item] = [1, la[item]]
-                else:
-                    s = self.SPPList[item][0] + 1
-                    la[item] = max(0, la[item] + ts - tidLast.get(item) - self._maxPer)
-                    self.SPPList[item] = [s, max(la[item], self.SPPList[item][1])]
-                tidLast[item] = ts
-            _last = ts
-        for item in self.SPPList:
-            la[item] = max(0, la[item] + _last - tidLast[item] - self._maxPer)
-            self.SPPList[item][1] = max(la[item], self.SPPList[item][1])
-        self.SPPList = {k: v for k, v in self.SPPList.items() if v[0] >= self._minSup and v[1] <= self._maxLa}
-        self.SPPList = {k: v for k, v in sorted(self.SPPList.items(), key=lambda x: x[1][0], reverse=True)}
-        data = self.SPPList
-        pfList = [k for k, v in sorted(data.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
-        self._rank = dict([(index, item) for (item, index) in enumerate(pfList)])
-        #print(len(pfList))
-        return data, pfList
-
-    def _updateDatabases(self, dict1):
-        """
-        Remove the items which are not frequent from database and updates the database with rank of items
-
-        :param dict1: frequent items with support
-        :type dict1: dictionary
-        :return: Sorted and updated transactions
-        """
-        list1 = []
-        for tr in self._Database:
-            list2 = [int(tr[0])]
-            for i in range(1, len(tr)):
-                if tr[i] in dict1:
-                    list2.append(self._rank[tr[i]])
-            if len(list2) >= 2:
-                basket = list2[1:]
-                basket.sort()
-                list2[1:] = basket[0:]
-                list1.append(list2)
-        return list1
-
-    @staticmethod
-    def _buildTree(data, info):
-        """
-        It takes the database and support of each item and construct the main tree by setting root node as a null
-
-        :param data: it represents the one Database in database
-        :type data: list
-        :param info: it represents the support of each item
-        :type info: dictionary
-        :return: returns root node of tree
-        """
-
-        rootNode = _Tree()
-        rootNode.info = info.copy()
-        for i in range(len(data)):
-            set1 = [data[i][0]]
-            rootNode.addTransaction(data[i][1:], set1)
-        return rootNode
-
-    def _savePeriodic(self, itemSet):
-        """
-        To convert the ranks of items in to their original item names
-
-        :param itemSet: frequent pattern.
-        :return: frequent pattern with original item names
-        """
-        t1 = str()
-        for i in itemSet:
-            t1 = t1 + self._rankedUp[i] + " "
-        return t1
+            FPTree.createTree(transaction, 1)
+        return FPTree
+
+    def _sortTransaction(self):
+        """
+        Sort each transaction of self.Database based on self.fpList
+        """
+        for i in range(len(self._Database)):
+            self._Database[i] = [item for item in self._Database[i] if item in self._fpList]
+            self._Database[i].sort(key=lambda value: self._fpList.index(value))
 
     def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
+        :type value: int or float or str
         :return: converted value
+        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = (self._lno * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self._Database) * value)
+                value = (self._lno * value)
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
-        Mining process will start from this function
+        Start pattern mining from here
         """
-
-        global _minSup, _maxPer, _lno, _maxLa
-        self._startTime = time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
+        self._startTime = _ab._time.time()
+        self._finalPatterns = {}
+        self._readDatabase()
+        print(len(self._Database), len(self._neighbourList))
         self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        self._maxLa = self._convert(self._maxLa)
-        _minSup, _maxPer, _maxLa, _lno = self._minSup, self._maxPer, self._maxLa, len(self._Database)
-        print(_minSup, _maxPer, _maxLa)
-        if self._minSup > len(self._Database):
-            raise Exception("Please enter the minSup in range between 0 to 1")
-        generatedItems, pfList = self._periodicFrequentOneItem()
-        updatedDatabases = self._updateDatabases(generatedItems)
-        for x, y in self._rank.items():
-            self._rankedUp[y] = x
-        info = {self._rank[k]: v for k, v in generatedItems.items()}
-        Tree = self._buildTree(updatedDatabases, info)
-        patterns = Tree.generatePatterns([])
+        self._getFrequentItems()
+        self._sortTransaction()
+        _FPTree = self._createFPTree()
+        self._finalPatterns.update(dict(_FPTree.mining(self._minSup, self._neighbourList)))
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent Spatial Patterns successfully generated using FSPGrowth")
+
+    def mine(self):
+        """
+        Start pattern mining from here
+        """
+        self._startTime = _ab._time.time()
         self._finalPatterns = {}
-        for i in patterns:
-            sample = self._savePeriodic(i[0])
-            self._finalPatterns[sample] = i[1]
-        self._endTime = time.time()
-        process = psutil.Process(os.getpid())
+        self._readDatabase()
+        print(len(self._Database), len(self._neighbourList))
+        self._minSup = self._convert(self._minSup)
+        self._getFrequentItems()
+        self._sortTransaction()
+        _FPTree = self._createFPTree()
+        self._finalPatterns.update(dict(_FPTree.mining(self._minSup, self._neighbourList)))
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Stable Periodic Frequent patterns were generated successfully using topk algorithm ")
+        print("Frequent Spatial Patterns successfully generated using FSPGrowth")
 
     def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+        """
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """Storing final periodic-frequent patterns in a dataframe
+        """
+        Storing final frequent patterns in a dataframe
 
-        :return: returning periodic-frequent patterns in a dataframe
+        :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataFrame = pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
-        return dataFrame
+            data.append([a.replace('\t', ' '), b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataframe
 
-    def save(self, outFile):
+    def save(self, oFile):
         """
-        Complete set of periodic-frequent patterns will be loaded in to an output file
+        Complete set of frequent patterns will be loaded in to a output file
 
-        :param outFile: name of the output file
-        :type outFile: csv file
+        :param oFile: name of the output file
+        :type oFile: csv file
         """
-        self._oFile = outFile
+        self._oFile = oFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
+            s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
-        """ Function to send the set of periodic-frequent patterns after completion of the mining process
+        """
+        Function to send the set of frequent patterns after completion of the mining process
 
-        :return: returning periodic-frequent patterns
+        :return: returning frequent patterns
         :rtype: dict
         """
+
         return self._finalPatterns
 
+    def printResults(self):
+        """
+        This function is used to print the results
+        """
+        print("Total number of Spatial Frequent Patterns:", len(self.getPatterns()))
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
+
 
 if __name__ == "__main__":
     _ap = str()
-    if len(sys.argv) == 5 or len(sys.argv) == 6:
-        if len(sys.argv) == 6:
-            _ap = SPPGrowth(sys.argv[1], sys.argv[3], sys.argv[4], sys.argv[5])
-        if len(sys.argv) == 5:
-            _ap = SPPGrowth(sys.argv[1], sys.argv[3], sys.argv[4])
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = FSPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:
+            _ap = FSPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Patterns:", len(_Patterns))
-        _ap.save(sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        _ap.mine()
+        print("Total number of Spatial Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
-        '''ap = topk('https://www.u-aizu.ac.jp/~udayrage/datasets/temporalDatabases/temporal_retail.csv', 0.001, 0.005, 0.004)
-        #ap = topk('/Users/likhitha/Downloads/contextPrefixSpan.txt', 3, 6, 2, ' ')
-        ap.startMine()
-        Patterns = ap.getPatterns()
-        print("Total number of Frequent Patterns:", len(Patterns))
-        ap.save('/Users/Likhitha/Downloads/output')
-        memUSS = ap.getMemoryUSS()
-        print("Total Memory in USS:", memUSS)
-        memRSS = ap.getMemoryRSS()
-        print("Total Memory in RSS", memRSS)
-        run = ap.getRuntime()
-        print("Total ExecutionTime in ms:", run)'''
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py` & `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/stablePeriodicFrequentPattern/topK/abstract.py` & `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/subgraphMining/basic/abstract.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/subgraphMining/basic/dfsCode.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/dfsCode.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/subgraphMining/basic/edge.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/edge.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/subgraphMining/basic/extendedEdge.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/extendedEdge.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/subgraphMining/basic/frequentSubgraph.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/frequentSubgraph.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/subgraphMining/basic/graph.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/graph.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/subgraphMining/basic/gspan.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/gspan.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,47 +1,48 @@
 # gSpan is a subgraph mining algorithm that uses DFS and DFS codes to mine subgraphs
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     from PAMI.subgraphMining.basic import gspan as alg
+#             from PAMI.subgraphMining.basic import gspan as alg
 #
-#     obj = alg.GSpan(iFile, minSupport)
+#             obj = alg.GSpan(iFile, minSupport)
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     frequentGraphs = obj.getFrequentSubgraphs()
+#             obj.run()
 #
-#     memUSS = obj.getMemoryUSS()
+#             frequentGraphs = obj.getFrequentSubgraphs()
 #
-#     obj.save(oFile)
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             obj.save(oFile)
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     run = obj.getRuntime()
+#             print("Total Memory in RSS", memRSS)
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             run = obj.getRuntime()
 #
+#             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 from PAMI.subgraphMining.basic import abstract as _ab
 
 class GSpan(_ab._gSpan):
 
     eliminate_infrequent_vertices = True
     eliminate_infrequent_vertex_pairs = True
     eliminate_infrequent_edge_labels = True
     edge_count_pruning = True
 
-    def __init__(self, iFile, minSupport, outputSingleVertices=True, maxNumberOfEdges=float('inf'), outputGraphIds=True) -> None:
+    def __init__(self, iFile, minSupport, outputSingleVertices=True, maxNumberOfEdges=float('inf'), outputGraphIds=False) -> None:
         """
         Initialize variables
         """
         
         self.minSup = minSupport
         self.frequentSubgraphs = []
         self._runtime = 0
```

### Comparing `pami-2024.3.9.2/PAMI/subgraphMining/basic/sparseTriangularMatrix.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/sparseTriangularMatrix.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/subgraphMining/basic/vertex.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/vertex.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py` & `pami-2024.4.9.1/PAMI/extras/stats/temporalDatabase.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,41 +1,28 @@
-# VBFTMine is one of the fundamental algorithm to discover fault-tolerant frequent patterns in an uncertain transactional database based on bitset representation.
+# TemporalDatabase is a class used to get stats of database.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#             import PAMI.uncertainFaultTolerantFrequentPattern.basic.VBFTMine as alg
+#             from PAMI.extras.stats import TemporalDatabase as db
 #
-#             obj = alg.VBFTMine(iFile, minSup, itemSup, minLength, faultTolerance)
-#
-#             obj.startMine()
-#
-#             faultTolerantFrequentPattern = obj.getPatterns()
-#
-#             print("Total number of Fault Tolerant Frequent Patterns:", len(faultTolerantFrequentPattern))
+#             obj = db.temporalDatabase(iFile, "\t")
 #
 #             obj.save(oFile)
 #
-#             Df = obj.getPatternInDataFrame()
-#
-#             memUSS = obj.getMemoryUSS()
-#
-#             print("Total Memory in USS:", memUSS)
+#             obj.run()
 #
-#             memRSS = obj.getMemoryRSS()
-#
-#             print("Total Memory in RSS", memRSS)
-#
-#             run = obj.getRuntime()
-#
-#             print("Total ExecutionTime in seconds:", run)
+#             obj.printStats()
 #
 
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -43,373 +30,428 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-import numpy as _np
-from PAMI.faultTolerantFrequentPattern.basic import abstract as _ab
+import sys
+import statistics
+import pandas as pd
+import validators
+import numpy as np
+from urllib.request import urlopen
+from typing import Dict, Union
 
-class VBFTMine(_ab._faultTolerantFrequentPatterns):
-    """
-    
-    :Description:  VBFTMine is one of the fundamental algorithm to discover fault tolerant frequent patterns in an uncertain transactional database based on
-                   bitset representation.
-                   This program employs apriori property (or downward closure property) to  reduce the search space effectively.
-
-    :Reference:   Koh, JL., Yo, PW. (2005). An Efficient Approach for Mining Fault-Tolerant Frequent Patterns Based on Bit Vector Representations.
-                  In: Zhou, L., Ooi, B.C., Meng, X. (eds) Database Systems for Advanced Applications. DASFAA 2005. Lecture Notes in Computer Science,
-                  vol 3453. Springer, Berlin, Heidelberg. https://doi.org/10.1007/11408079_51
-    :param  iFile: str :
-           Name of the Input file to mine complete set of frequent patterns
-    :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: float or int or str :
-                    The user can specify minSup either in count or proportion of database size.
-                    If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-                    Otherwise, it will be treated as float.
-                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-    :param  itemSup: int or float :
-                    Frequency of an item
-    :param minLength: int
-                    minimum length of a pattern
-    :param faultTolerance: int
 
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+class temporalDatabase:
+    """
+    :Description:   TemporalDatabase is class to get stats of database.
 
     :Attributes:
 
-        startTime : float
-          To record the start time of the mining process
-
-        endTime : float
-          To record the completion time of the mining process
-
-        finalPatterns : dict
-          Storing the complete set of patterns in a dictionary variable
-
-        memoryUSS : float
-          To store the total amount of USS memory consumed by the program
-
-        memoryRSS : float
-          To store the total amount of RSS memory consumed by the program
-
-        Database : list
-          To store the transactions of a database in list
-
+        :param inputFile : file
+            input file path
 
-    **Executing the code on terminal**:
-    ------------------------------------
-        Format:
-            >>> python3 VBFTMine.py <inputFile> <outputFile> <minSup> <itemSup> <minLength> <faultTolerance>
+        :param sep : str
+            separator in file. Default is tab space.
 
-        Examples:
-            >>> python3 VBFTMine.py sampleDB.txt patterns.txt 10.0 3.0 3 1  (minSup will be considered in times of minSup and count of database transactions)
+    :Methods:
 
+        run()
+            execute readDatabase function
+        readDatabase()
+            read database from input file
+        getDatabaseSize()
+            get the size of database
+        getMinimumTransactionLength()
+            get the minimum transaction length
+        getAverageTransactionLength()
+            get the average transaction length. It is sum of all transaction length divided by database length.
+        getMaximumTransactionLength()
+            get the maximum transaction length
+        getStandardDeviationTransactionLength()
+            get the standard deviation of transaction length
+        getSortedListOfItemFrequencies()
+            get sorted list of item frequencies
+        getSortedListOfTransactionLength()
+            get sorted list of transaction length
+        save(data, outputFile)
+            store data into outputFile
+        getMinimumPeriod()
+            get the minimum period
+        getAveragePeriod()
+            get the average period
+        getMaximumPeriod()
+            get the maximum period
+        getStandardDeviationPeriod()
+            get the standard deviation period
+        getNumberOfTransactionsPerTimestamp()
+            get number of transactions per time stamp. This time stamp range is 1 to max period.
 
-    **Sample run of the importing code**:
-    --------------------------------------------
+    **Importing this algorithm into a python program**
+    --------------------------------------------------------
     .. code-block:: python
-    
-            import PAMI.faultTolerantFrequentPattern.basic.VBFTMine as alg
 
-            obj = alg.VBFTMine(iFile, minSup, itemSup, minLength, faultTolerance)
+            from PAMI.extras.dbStats import TemporalDatabase as db
 
-            obj.startMine()
-
-            faultTolerantFrequentPattern = obj.getPatterns()
-
-            print("Total number of Fault Tolerant Frequent Patterns:", len(faultTolerantFrequentPattern))
+            obj = db.TemporalDatabase(iFile, "\t")
 
             obj.save(oFile)
 
-            Df = obj.getPatternInDataFrame()
-
-            print("Total Memory in USS:", obj.getMemoryUSS())
-
-            print("Total Memory in RSS", obj.getMemoryRSS())
-
-            print("Total ExecutionTime in seconds:", obj.getRuntime())
-
-    **Credits**:
-    ------------
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+            obj.run()
 
+            obj.printStats()
     """
 
-    _minSup = float()
-    _itemSup = float()
-    _minLength = int()
-    _faultTolerance = int()
-    _startTime = float()
-    _endTime = float()
-    _finalPatterns = {}
-    _iFile = " "
-    _oFile = " "
-    _sep = " "
-    _plist = []
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _Database = []
-    _mapSupport = {}
-
-    def _creatingItemSets(self):
-        """
-        Storing the complete transactions of the database/input file in a database variable
-        """
-        self._Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            temp = []
-            if self._iFile.empty:
+    def __init__(self, inputFile: Union[str, pd.DataFrame], sep: str = '\t') -> None:
+        """
+        :param inputFile: input file name or path
+        :type inputFile: str
+        :param sep: separator between column names and values in input file
+        :type sep: str
+        :return: None
+        """
+        self.inputFile = inputFile
+        self.database = {}
+        self.lengthList = []
+        self.timeStampCount = {}
+        self.periodList = []
+        self.sep = sep
+        self.periods = {}
+
+    def run(self) -> None:
+        self.readDatabase()
+
+    def readDatabase(self) -> None:
+        """
+        read database from input file and store into database and size of each transaction.
+        And store the period between transactions as list
+        """
+        numberOfTransaction = 0
+        if isinstance(self.inputFile, pd.DataFrame):
+            if self.inputFile.empty:
                 print("its empty..")
-            i = self._iFile.columns.values.tolist()
-            if 'Transactions' in i:
-                temp = self._iFile['Transactions'].tolist()
-
-            for k in temp:
-                self._Database.append(set(k))
-        if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            i = self.inputFile.columns.values.tolist()
+            if 'TS' in i and 'Transactions' in i:
+                self.database = self.inputFile.set_index('ts').T.to_dict(orient='records')[0]
+            if 'TS' in i and 'Patterns' in i:
+                self.database = self.inputFile.set_index('ts').T.to_dict(orient='records')[0]
+            self.timeStampCount = self.inputFile.groupby('ts').count().T.to_dict(orient='records')[0]
+
+        if isinstance(self.inputFile, str):
+            if validators.url(self.inputFile):
+                data = urlopen(self.inputFile)
                 for line in data:
+                    numberOfTransaction += 1
                     line.strip()
                     line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [i.rstrip() for i in line.split(self.sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(set(temp))
+                    self.database[numberOfTransaction] = temp[1:]
+                    self.timeStampCount[int(temp[0])] = self.timeStampCount.get(int(line[0]), 0)
+                    self.timeStampCount[int(temp[0])] += 1
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self.inputFile, 'r', encoding='utf-8') as f:
                         for line in f:
+                            numberOfTransaction += 1
                             line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [i.rstrip() for i in line.split(self.sep)]
                             temp = [x for x in temp if x]
-                            for i in temp:
-                                if i not in self._plist:
-                                    self._plist.append(i)
-                            self._Database.append(set(temp))
+                            if len(temp) > 0:
+                                self.database[numberOfTransaction] = temp[1:]
+                                self.timeStampCount[int(temp[0])] = self.timeStampCount.get(int(line[0]), 0)
+                                self.timeStampCount[int(temp[0])] += 1
                 except IOError:
                     print("File Not Found")
                     quit()
+        self.lengthList = [len(s) for s in self.database.values()]
+        timeStampList = sorted(list(self.database.keys()))
+        preTimeStamp = 0
+        for ts in timeStampList:
+            self.periodList.append(int(ts) - preTimeStamp)
+            preTimeStamp = ts
+
+        for x, y in self.database.items():
+            for i in y:
+                if i not in self.periods:
+                    self.periods[i] = [x, x]
+                else:
+                    self.periods[i][0] = max(self.periods[i][0], x - self.periods[i][1])
+                    self.periods[i][1] = x
+        for key in self.periods:
+            self.periods[key][0] = max(self.periods[key][0], abs(len(self.database) - self.periods[key][1]))
+        self.periods = {k: v[0] for k, v in self.periods.items()}
 
-    def _convert(self, value):
+    def getDatabaseSize(self) -> int:
         """
-        To convert the user specified minSup value
+        get the size of database
+        :return: dataset size
+        :rtype: int
+        """
+        return len(self.database)
 
-        :param value: user specified minSup value
-        :return: converted type
+    def getMinimumTransactionLength(self) -> int:
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
+        get the minimum transaction length
+        :return: minimum transaction length
+        :rtype: int
+        """
+        return min(self.lengthList)
 
-    def _Count(self, tids):
-        count = 0
-        for i in tids:
-            if i == 1:
-                count += 1
-        return count
-
-    def _save(self, prefix, suffix, tidsetx):
-        if (prefix == None):
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        prefix = list(set(prefix))
-        prefix.sort()
-        val = self._Count(tidsetx)
-        if len(prefix) > self._faultTolerance:
-            self._finalPatterns[tuple(prefix)] = val
-
-    def _processEquivalenceClass(self, prefix, itemsets, tidsets):
-        if (len(itemsets) == 1):
-            i = itemsets[0]
-            tidi = tidsets[0]
-            self._save(prefix, [i], tidi)
-            return
-        for i in range(len(itemsets)):
-            itemx = itemsets[i]
-            if (itemx == None):
-                continue
-            tidsetx = tidsets[i]
-            classItemsets = []
-            classtidsets = []
-            itemsetx = [itemx]
-            for j in range(i + 1, len(itemsets)):
-                itemj = itemsets[j]
-                tidsetj = tidsets[j]
-                y = list(_np.array(tidsetx) & _np.array(tidsetj))
-                total = self._Count(y)
-                if total >= self._minSup:
-                    classItemsets.append(itemj)
-                    classtidsets.append(y)
-            if (len(classItemsets) > 0):
-                newprefix = list(set(itemsetx)) + prefix
-                self._processEquivalenceClass(newprefix, classItemsets, classtidsets)
-            self._save(prefix, list(set(itemsetx)), tidsetx)
-
-    def _oneLengthFrequentItems(self):
-        """ To calculate the one Length items"""
-        Vector = {}
-        items = []
-        for i in self._Database:
-            for j in self._plist:
-                count = 0
-                if j in i:
-                    count = 1
-                if j in Vector:
-                    Vector[j].append(count)
-                else:
-                    Vector[j] = [count]
-        for x, y in Vector.items():
-            v = self._Count(y)
-            if v >= self._itemSup:
-                items.append(x)
-        return Vector, items
-
-    def startMine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-        self._Database = []
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        self._itemSup = self._convert(self._itemSup)
-        self._minLength = int(self._minLength)
-        self._faultTolerance = int(self._faultTolerance)
-        Vector, plist = self._oneLengthFrequentItems()
-        for i in range(len(plist)):
-            itemx = plist[i]
-            tidsetx = Vector[itemx]
-            itemsetx = [itemx]
-            itemsets = []
-            tidsets = []
-            for j in range(i + 1, len(plist)):
-                itemj = plist[j]
-                tidsetj = Vector[itemj]
-                y1 = list(_np.array(tidsetx) | _np.array(tidsetj))
-                total = self._Count(y1)
-                if total >= self._minSup:
-                    itemsets.append(itemj)
-                    tidsets.append(y1)
-            if (len(itemsets) > 0):
-                self._processEquivalenceClass(itemsetx, itemsets, tidsets)
-            self._save(None, itemsetx, tidsetx)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Fault-Tolerant Frequent patterns were generated successfully using VBFTMine algorithm ")
+    def getAverageTransactionLength(self) -> float:
+        """
+        get the average transaction length. It is sum of all transaction length divided by database length.
+        :return: average transaction length
+        :rtype: float
+        """
+        totalLength = sum(self.lengthList)
+        return totalLength / len(self.database)
+
+    def getMaximumTransactionLength(self) -> int:
+        """
+        get the maximum transaction length
+        :return: maximum transaction length
+        :rtype: int
+        """
+        return max(self.lengthList)
 
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+    def getStandardDeviationTransactionLength(self) -> float:
+        """
+        get the standard deviation transaction length
+        :return: standard deviation transaction length
+        :rtype: float
+        """
+        return statistics.pstdev(self.lengthList)
 
-        :return: returning USS memory consumed by the mining process
+    def getVarianceTransactionLength(self) -> float:
+        """
+        get the variance transaction length
+        :return: variance transaction length
         :rtype: float
         """
+        return statistics.variance(self.lengthList)
 
-        return self._memoryUSS
+    def convertDataIntoMatrix(self) -> np.ndarray:
+        singleItems = self.getSortedListOfItemFrequencies()
+        itemsets = {}
+        for tid in self.database:
+            for item in singleItems:
+                if item in itemsets:
+                    if item in self.database[tid]:
+                        itemsets[item].append(1)
+                    else:
+                        itemsets[item].append(0)
+                else:
+                    if item in self.database[tid]:
+                        itemsets[item] = [1]
+                    else:
+                        itemsets[item] = [0]
+        data = list(itemsets.values())
+        an_array = np.array(data)
+        return an_array
 
-    def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+    def getSparsity(self) -> float:
+        """
+        get the sparsity of database. sparsity is percentage of 0 of database.
+        :return: database sparsity
+        :rtype: float
+        """
+        big_array = self.convertDataIntoMatrix()
+        n_zeros = np.count_nonzero(big_array == 0)
+        return (n_zeros / big_array.size)
 
-        :return: returning RSS memory consumed by the mining process
+    def getDensity(self) -> float:
+        """
+        get the sparsity of database. sparsity is percentage of 0 of database.
+        :return: database sparsity
         :rtype: float
         """
+        big_array = self.convertDataIntoMatrix()
+        n_zeros = np.count_nonzero(big_array == 1)
+        return (1.0 - n_zeros / big_array.size)
 
-        return self._memoryRSS
+    def getTotalNumberOfItems(self) -> int:
+        """
+        get the number of items in database.
+        :return: number of items
+        :rtype: int
+        """
+        return len(self.getSortedListOfItemFrequencies())
 
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+    def getSortedListOfItemFrequencies(self) -> Dict[str, int]:
+        """
+        get sorted list of item frequencies
+        :return: item frequencies
+        :rtype: dict
+        """
+        itemFrequencies = {}
+        for tid in self.database:
+            for item in self.database[tid]:
+                itemFrequencies[item] = itemFrequencies.get(item, 0)
+                itemFrequencies[item] += 1
+        return {k: v for k, v in sorted(itemFrequencies.items(), key=lambda x: x[1], reverse=True)}
+
+    def getFrequenciesInRange(self) -> Dict[int, int]:
+        fre = self.getSortedListOfItemFrequencies()
+        rangeFrequencies = {}
+        maximum = max([i for i in fre.values()])
+        values = [int(i * maximum / 6) for i in range(1, 6)]
+        # print(maximum)
+        va = len({key: val for key, val in fre.items() if val > 0 and val < values[0]})
+        rangeFrequencies[va] = values[0]
+        for i in range(1, len(values)):
+            va = len({key: val for key, val in fre.items() if val < values[i] and val > values[i - 1]})
+            rangeFrequencies[va] = values[i]
+        return rangeFrequencies
+
+    def getPeriodsInRange(self) -> Dict[int, int]:
+        fre = {k: v for k, v in sorted(self.periods.items(), key=lambda x: x[1])}
+        rangePeriods = {}
+        maximum = max([i for i in fre.values()])
+        values = [int(i * maximum / 6) for i in range(1, 6)]
+        # print(maximum)
+        va = len({key: val for key, val in fre.items() if val > 0 and val < values[0]})
+        rangePeriods[va] = values[0]
+        for i in range(1, len(values)):
+            va = len({key: val for key, val in fre.items() if val < values[i] and val > values[i - 1]})
+            rangePeriods[va] = values[i]
+        return rangePeriods
 
-        :return: returning total amount of runtime taken by the mining process
-        :rtype: float
+    def getTransanctionalLengthDistribution(self) -> Dict[int, int]:
         """
+        get transaction length
+        :return: transactional length
+        :rtype: dict
+        """
+        transactionLength = {}
+        for length in self.lengthList:
+            transactionLength[length] = transactionLength.get(length, 0)
+            transactionLength[length] += 1
+        return {k: v for k, v in sorted(transactionLength.items(), key=lambda x: x[0])}
 
-        return self._endTime - self._startTime
+    def save(self, data: dict, outputFile: str) -> None:
+        """
+        store data into outputFile
+        :param data: input data
+        :type data: dict
+        :param outputFile: output file name or path to store
+        :type outputFile: str
+        :return: None
+        """
+        with open(outputFile, 'w') as f:
+            for key, value in data.items():
+                f.write(f'{key}\t{value}\n')
 
-    def getPatternsAsDataFrame(self):
+    def getMinimumInterArrivalPeriod(self) -> int:
         """
-        Storing final frequent patterns in a dataframe
+        get the minimum inter arrival period
+        :return: minimum inter arrival period
+        :rtype: int
+        """
+        return min(self.periodList)
 
-        :return: returning frequent patterns in a dataframe
-        :rtype: pd.DataFrame
+    def getAverageInterArrivalPeriod(self) -> float:
+        """
+        get the average inter arrival period. It is sum of all period divided by number of period.
+        :return: average inter arrival period
+        :rtype: float
         """
+        totalPeriod = sum(self.periodList)
+        return totalPeriod / len(self.periodList)
 
-        dataFrame = {}
-        data = []
-        for a, b in self._finalPatterns.items():
-            s = str()
-            for i in a:
-                s = s + i + ' '
-            data.append([s, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
-        return dataFrame
+    def getMaximumInterArrivalPeriod(self) -> int:
+        """
+        get the maximum inter arrival period
+        :return: maximum inter arrival period
+        :rtype: int
+        """
+        return max(self.periodList)
 
-    def save(self, outFile):
+    def getMinimumPeriodOfItem(self) -> int:
         """
-        Complete set of frequent patterns will be loaded in to an output file
+        get the minimum period of the item
+        :return: minimum period
+        :rtype: int
+        """
+        return min([i for i in self.periods.values()])
 
-        :param outFile: name of the output file
-        :type outFile: file
+    def getAveragePeriodOfItem(self) -> float:
+        """
+        get the average period of the item
+        :return: average period
+        :rtype: float
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s = str()
-            for i in x:
-                s = s + i + '\t'
-            s1 = s.strip() + ":" + str(y)
-            writer.write("%s \n" % s1)
+        return sum([i for i in self.periods.values()]) / len(self.periods)
 
-    def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
+    def getMaximumPeriodOfItem(self) -> int:
+        """
+        get the maximum period of the item
+        :return: maximum period
+        :rtype:int
+        """
+        return max([i for i in self.periods.values()])
 
-        :return: returning frequent patterns
-        :rtype: dict
+    def getStandardDeviationPeriod(self) -> float:
+        """
+        get the standard deviation period
+        :return: standard deviation period
+        :rtype: float
         """
-        return self._finalPatterns
+        return statistics.pstdev(self.periodList)
 
-    def printResults(self):
+    def getNumberOfTransactionsPerTimestamp(self) -> Dict[int, int]:
         """
-        This function is used to print the results
+        get number of transactions per time stamp
+        :return: number of transactions per time stamp as dict
+        :rtype: dict
         """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
-
-
-if __name__ == "__main__":
-    _ap = str()
-    if len(_ab._sys.argv) == 7 or len(_ab._sys.argv) == 8:
-        if len(_ab._sys.argv) == 8:
-            _ap = VBFTMine(_ab._sys.argv[1], _ab._sys.argv[3],  _ab._sys.argv[4],
-                            _ab._sys.argv[5], _ab._sys.argv[6], _ab._sys.argv[7],)
-        if len(_ab._sys.argv) == 7:
-            _ap = VBFTMine(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
-        _ap.startMine()
-        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        maxTS = max(list(self.timeStampCount.keys()))
+        return {ts: self.timeStampCount.get(ts, 0) for ts in range(1, maxTS + 1)}
+
+    def printStats(self) -> None:
+        print(f'Database size : {self.getDatabaseSize()}')
+        print(f'Number of items : {self.getTotalNumberOfItems()}')
+        print(f'Minimum Transaction Size : {self.getMinimumTransactionLength()}')
+        print(f'Average Transaction Size : {self.getAverageTransactionLength()}')
+        print(f'Maximum Transaction Size : {self.getMaximumTransactionLength()}')
+        print(f'Minimum Inter Arrival Period : {self.getMinimumInterArrivalPeriod()}')
+        print(f'Average Inter Arrival Period : {self.getAverageInterArrivalPeriod()}')
+        print(f'Maximum Inter Arrival Period : {self.getMaximumInterArrivalPeriod()}')
+        print(f'Minimum periodicity : {self.getMinimumPeriodOfItem()}')
+        print(f'Average periodicity : {self.getAveragePeriodOfItem()}')
+        print(f'Maximum periodicicty : {self.getMaximumPeriodOfItem()}')
+        print(f'Standard Deviation Transaction Size : {self.getStandardDeviationTransactionLength()}')
+        print(f'Variance : {self.getVarianceTransactionLength()}')
+        print(f'Sparsity : {self.getSparsity()}')
+
+    def plotGraphs(self) -> None:
+        itemFrequencies = self.getFrequenciesInRange()
+        transactionLength = self.getTransanctionalLengthDistribution()
+        plt.plotLineGraphFromDictionary(itemFrequencies, 100, 0, 'Frequency', 'no of items', 'frequency')
+        plt.plotLineGraphFromDictionary(transactionLength, 100, 0, 'transaction length', 'transaction length',
+                                        'frequency')
+
+
+if __name__ == '__main__':
+    data = {'tid': [1, 2, 3, 4, 5, 6, 7],
+
+            'Transactions': [['a', 'd', 'e'], ['b', 'a', 'f', 'g', 'h'], ['b', 'a', 'd', 'f'], ['b', 'a', 'c'],
+                             ['a', 'd', 'g', 'k'],
+
+                             ['b', 'd', 'g', 'c', 'i'], ['b', 'd', 'g', 'e', 'j']]}
+
+    # data = pd.DataFrame.from_dict('temporal_T10I4D100K.csv')
+    import PAMI.extras.graph.plotLineGraphFromDictionary as plt
+
+    if len(sys.argv) < 3:
+        print("Please provide two arguments.")
     else:
-        _ap = VBFTMine('/Users/Likhitha/Downloads/fault/sample4.txt', 5, 3, 2, 1, ' ')
-        _ap.startMine()
-        _ap.printResults()
-        print(_ap.getPatternsAsDataFrame())
-        print("Error! The number of input parameters do not match the total number of parameters provided")
+        obj = temporalDatabase(sys.argv[1], sys.argv[2])
+        obj1 = temporalDatabase(pd.DataFrame(data))
+        obj1.run()
+        if obj1.getDatabaseSize() > 0:
+            obj1.printStats()
+            obj1.plotGraphs()
+        else:
+            print("No data found in the database.")
```

### Comparing `pami-2024.3.9.2/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py` & `pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/uncertainFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/CUFPTree.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/CUFPTree.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
 #             from PAMI.uncertainFrequentPattern.basic import CUFPTree as alg
 #
-#             obj = alg.CUFPTree(iFile, minSup)
+#             obj = alg.CUFPTree(iFile, minSup,oFile,sep)
 #
 #             obj.startMine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -29,15 +29,14 @@
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
-
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -49,17 +48,19 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
+import pandas as pd
+from deprecated import deprecated
 
 from PAMI.uncertainFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from typing import List, Tuple
 
 
 _minSup = str()
 _ab._sys.setrecursionlimit(20000)
 _finalPatterns = {}
 
 
@@ -317,15 +318,15 @@
         Chun-Wei Lin Tzung-PeiHong, 'new mining approach for uncertain databases using CUFP trees',
         Expert Systems with Applications, Volume 39, Issue 4, March 2012, Pages 4084-4093, https://doi.org/10.1016/j.eswa.2011.09.087
     
     :param  iFile: str :
                    Name of the Input file to mine complete set of Uncertain Frequent Patterns
     :param  oFile: str :
                    Name of the output file to store complete set of Uncertain frequent patterns
-    :param  minSup: str:
+    :param  minSup: int or float or str :
                    minimum support thresholds were tuned to find the appropriate ranges in the limited memory
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Attributes:
 
@@ -392,23 +393,26 @@
             Mining process will start from this function
 
     **Methods to execute code on terminal**
     --------------------------------------------
 
     .. code-block:: console
 
-      Format:
 
-      (.venv) $ python3 CUFPTree.py <inputFile> <outputFile> <minSup>
+       Format:
+
+       (.venv) $ python3 CUFPTree.py <inputFile> <outputFile> <minSup>
+
+       Example Usage:
+
+       (.venv) $ python3 CUFPTree.py sampleTDB.txt patterns.txt 3
 
-      Example Usage:
 
-      (.venv) $ python3 CUFPTree.py sampleTDB.txt patterns.txt 3
 
-    .. note:: minSup  will be considered in support count or frequency
+               .. note:: minSup  will be considered in support count or frequency
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
     .. code-block:: python
 
             from PAMI.uncertainFrequentPattern.basic import CUFPTree as alg
 
@@ -455,14 +459,15 @@
 
     def __init__(self, iFile, minSup, sep='\t') -> None:
         super().__init__(iFile, minSup, sep)
 
     def _creatingItemSets(self) -> None:
         """
         Scans the uncertain transactional dataset
+        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
@@ -515,15 +520,15 @@
 
     def _frequentOneItem(self) -> Tuple[dict, List]:
         """
         Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
 
         :param self.Database : it represents the one self.Database in database
         :type self.Database : list
-        :return: return
+        :return: tuple
         """
 
         mapSupport = {}
         for i in self._Database:
             for j in i:
                 if j.item not in mapSupport:
                     mapSupport[j.item] = j.probability
@@ -641,17 +646,18 @@
         for x, y in periods.items():
             if y >= self._minSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns.
         :return: None
         """
         global minSup
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
         minSup = self._minSup
@@ -666,14 +672,40 @@
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self.memoryRSS = process.memory_info().rss
 
+    def Mine(self) -> None:
+        """
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns.
+        :return: None
+        """
+        global minSup
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        minSup = self._minSup
+        self._finalPatterns = {}
+        mapSupport, plist = self._frequentOneItem()
+        self.Database1 = self._updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        Tree1 = self._buildTree(self.Database1, info)
+        Tree1.generatePatterns([])
+        self._removeFalsePositives()
+        print("Uncertain Frequent patterns were successfully generated using CUFPTree algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self.memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self.memoryRSS = process.memory_info().rss
+
+
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
```

### Comparing `pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py`

 * *Files 1% similar despite different names*

```diff
@@ -28,74 +28,64 @@
 #     run = obj.getRuntime()
 #
 #     print("Total ExecutionTime in seconds:", run)
 #
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
-
 """
 
 
 from PAMI.uncertainFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from typing import List, Tuple
 
 _minSup = str()
 _ab._sys.setrecursionlimit(20000)
 _finalPatterns = {}
 
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
-
     :Attributes:
-
     item : int or word
         Represents the name of the item
     probability : float
         Represent the existential probability(likelihood presence) of an item
     """
 
     def __init__(self, item, probability) -> None:
         self.item = item
         self.probability = probability
 
 
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
-
     :Attributes:
-
     item : int
         storing item of a node
     probability : int
         To maintain the expected support of node
     parent : node
         To maintain the parent of every node
     children : list
         To maintain the children of node
-
     :Methods:
-
         addChild(itemName)
             storing the children to their respective parent nodes
     """
 
     def __init__(self, item, children) -> None:
         self.item = item
         self.probability = 1
@@ -109,26 +99,22 @@
         self.children[node.item] = node
         node.parent = self
 
 
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
-
     Attributes:
-
     root : Node
         Represents the root node of the tree
     summaries : dictionary
         storing the nodes with same item name
     info : dictionary
         stores the support of items
-
     :Methods:
-
         addTransaction(transaction)
             creating transaction as a branch in frequentPatternTree
         addConditionalPattern(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
         conditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
         conditionalTransactions(prefixPaths,Support)
@@ -144,15 +130,14 @@
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
     def addTransaction(self, transaction) -> None:
         """
         Adding transaction into tree
-
         :param transaction : it represents the one self.Database in database
         :type transaction : list
         """
 
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i].item not in currentNode.children:
@@ -183,15 +168,14 @@
                     currentNode.probability += transaction[i].probability
                 else:
                     currentNode.probability += max(lp) * transaction[i].probability
 
     def addConditionalPattern(self, transaction, sup) -> None:
         """
         Constructing conditional tree from prefixPaths
-
         :param transaction : it represents the one self.Database in database
         :type transaction : list
         :param sup : support of prefixPath taken at last child of the path
         :type sup : int
         """
 
         # This method takes transaction, support and constructs the conditional tree
@@ -209,15 +193,14 @@
             else:
                 currentNode = currentNode.children[transaction[i]]
                 currentNode.probability += sup
 
     def conditionalPatterns(self, alpha) -> Tuple[List, List, dict]:
         """
         Generates all the conditional patterns of respective node
-
         :param alpha : it represents the Node in tree
         :type alpha : _Node
         """
         # This method generates conditional patterns of node by traversing the tree
         finalPatterns = []
         sup = []
         for i in self.summaries[alpha]:
@@ -232,26 +215,24 @@
                 sup.append(s)
         finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
         return finalPatterns, support, info
 
     def removeNode(self, nodeValue) -> None:
         """
         Removing the node from tree
-
         :param nodeValue : it represents the node in tree
         :type nodeValue : node
         """
 
         for i in self.summaries[nodeValue]:
             del i.parent.children[nodeValue]
 
     def conditionalTransactions(self, condPatterns, support) -> Tuple[List, List, dict]:
         """
         It generates the conditional patterns with frequent items
-
         :param condPatterns : conditionalPatterns generated from conditionalPattern method for respective node
         :type condPatterns : list
         :support : the support of conditional pattern in tree
         :support : int
         """
         global minSup
         pat = []
@@ -274,15 +255,14 @@
                 sup.append(support[count])
                 count += 1
         return pat, sup, updatedDict
 
     def generatePatterns(self, prefix) -> None:
         """
         Generates the patterns
-
         :param prefix : forms the combination of items
         :type prefix : list
         """
 
         global _finalPatterns, minSup
         for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
             pattern = prefix[:]
@@ -301,22 +281,18 @@
                     conditionalTree.generatePatterns(pattern)
             self.removeNode(i)
 
 
 class PUFGrowth(_ab._frequentPatterns):
     """
     :Description: It is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using PUF-Tree.
-
     :Reference:
-
         Carson Kai-Sang Leung, Syed Khairuzzaman Tanbeer, "PUF-Tree: A Compact Tree Structure for Frequent Pattern Mining of Uncertain Data",
         Pacific-Asia Conference on Knowledge Discovery and Data Mining(PAKDD 2013), https://link.springer.com/chapter/10.1007/978-3-642-37453-1_2
-
     :Attributes:
-
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
         minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
@@ -342,15 +318,14 @@
         tree : class
             To represents the Tree class
         itemSetCount : int
             To represents the total no of patterns
         finalPatterns : dict
             To store the complete patterns
     :Methods:
-
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
@@ -369,57 +344,37 @@
             Update the transactions by removing non-frequent items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
         startMine()
             Mining process will start from this function
-
     **Methods to execute code on terminal**
     -----------------------------------------
-
             Format:
                     >>> python3 PUFGrowth.py <inputFile> <outputFile> <minSup>
-
             Example:
                      >>>  python3 PUFGrowth.py sampleTDB.txt patterns.txt 3
-
             .. note:: minSup  will be considered in support count or frequency
-
-
     **Importing this algorithm into a python program**
     -----------------------------------------------------
     .. code-block:: python
-
             from PAMI.uncertainFrequentPattern.basic import puf as alg
-
             obj = alg.PUFGrowth(iFile, minSup)
-
             obj.startMine()
-
             frequentPatterns = obj.getPatterns()
-
             print("Total number of Frequent Patterns:", len(frequentPatterns))
-
             obj.save(oFile)
-
             Df = obj.getPatternsAsDataFrame()
-
             memUSS = obj.getmemoryUSS()
-
             print("Total Memory in USS:", memUSS)
-
             memRSS = obj.getMemoryRSS()
-
             print("Total Memory in RSS", memRSS)
-
             run = obj.getRuntime()
-
             print("Total ExecutionTime in seconds:", run)
-
     **Credits:**
     --------------------
              The complete program was written by  P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 """
     _startTime = float()
     _endTime = float()
     _minSup = str()
@@ -492,15 +447,14 @@
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
 
     def _frequentOneItem(self) -> Tuple[dict, List]:
         """
         Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
-
         :param self.Database : it represents the one self.Database in database
         :type self.Database : list
         """
 
         mapSupport = {}
         for i in self._Database:
             for j in i:
@@ -528,15 +482,14 @@
         for i in range(len(data)):
             rootNode.addTransaction(data[i])
         return rootNode
 
     def _updateTransactions(self, dict1) -> List:
         """
         remove the items which are not frequent from self.Database and updates the self.Database with rank of items
-
         :param dict1 : frequent items with support
         :type dict1 : dictionary
         """
 
         list1 = []
         for tr in self._Database:
             list2 = []
@@ -550,15 +503,14 @@
                 list1.append(list2)
         return list1
 
     @staticmethod
     def _check(i, x) -> int:
         """
         To check the presence of item or pattern in transaction
-
         :param x: it represents the pattern
         :type x : list
         :param i : represents the uncertain self.Database
         :type i : list
         """
 
         # This method taken a transaction as input and returns the tree
@@ -570,15 +522,14 @@
             if k == 0:
                 return 0
         return 1
 
     def _convert(self, value) -> float:
         """
         To convert the type of user specified minSup value
-
         :param value: user specified minSup value
         :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
@@ -588,15 +539,14 @@
             else:
                 value = int(value)
         return value
 
     def _removeFalsePositives(self) -> None:
         """
         To remove the false positive patterns generated in frequent patterns
-
         :return: patterns with accurate probability
         """
         global _finalPatterns
         periods = {}
         for i in self._Database:
             for x, y in _finalPatterns.items():
                 if len(x) == 1:
@@ -642,73 +592,67 @@
         self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self.memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self.memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
-    def save(self, outFile: str) -> None:  
+    def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
-
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self) -> dict:
         """
         Function to send the set of frequent patterns after completion of the mining process
-
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
```

### Comparing `pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/TUFP.py` & `pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/WFIM.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,37 +1,41 @@
-# TUFP is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+# WFMiner is one of the fundamental algorithm to discover weighted frequent patterns in a transactional database.
+# It stores the database in compressed fp-tree decreasing the memory usage and extracts the
+# patterns from tree.It employs downward closure property to  reduce the search space effectively.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
+# ----------------------------------------------------------
 #
-#     from PAMI.uncertainFrequentPattern.basic import TUFP as alg
 #
-#     obj = alg.TUFP(iFile, minSup)
+#             from PAMI.weightFrequentPattern.basic import basic as alg
 #
-#     obj.startMine()
+#             obj = alg.basic(iFile, wFile, minSup, minWeight)
 #
-#     frequentPatterns = obj.getPatterns()
+#             obj.startMine()
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             frequentPatterns = obj.getPatterns()
 #
-#     obj.save(oFile)
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             obj.savePatterns(oFile)
 #
-#     memUSS = obj.getMemoryUSS()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     print("Total Memory in USS:", memUSS)
+#             memUSS = obj.getMemoryUSS()
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     run = obj.getRuntime()
+#             print("Total Memory in RSS", memRSS)
+#
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
 #
-#     print("Total ExecutionTime in seconds:", run)
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -44,137 +48,309 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-
-from PAMI.uncertainFrequentPattern.basic import abstract as _ab
+from PAMI.weightedFrequentPattern.basic import abstract as _fp
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 import pandas as pd
+from deprecated import deprecated
 
-_minSup = float()
-_finalPatterns = {}
 
+_minSup = str()
+_minWeight = int()
+_miniWeight = int()
+_maxWeight = int()
+_weights = {}
+_fp._sys.setrecursionlimit(20000)
 
-class _Item:
+
+class _Node:
     """
-    A class used to represent the item with probability in transaction of dataset
+    A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
-        item : int or word
-            Represents the name of the item
-        probability : float
-            Represent the existential probability(likelihood presence) of an item
+        itemId: int
+            storing item of a node
+        counter: int
+            To maintain the support of node
+        parent: node
+            To maintain the parent of node
+        children: list
+            To maintain the children of node
+
+    :Methods:
+
+        addChild(node)
+            Updates the nodes children list and parent for the given node
     """
 
-    def __init__(self, item, probability) -> None:
-        self.item = item
-        self.probability = probability
+    def __init__(self, item: str, children: list) -> None:
+        self.itemId = item
+        self.counter = 1
+        self.parent = None
+        self.children = children
 
+    def addChild(self, node: '_Node') -> None:
+        """
+        Retrieving the child from the tree
+
+        :param node: Children node
+        :type node: Node
+        :return: Updates the children nodes and parent nodes
+        """
+        self.children[node.itemId] = node
+        node.parent = self
 
-class TUFP(_ab._frequentPatterns):
-    """
-    :Description: It is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
 
-    :Reference:
-        Tuong Le, Bay Vo, Van-Nam Huynh, Ngoc Thanh Nguyen, Sung Wook Baik 5, "Mining top-k frequent patterns from uncertain databases",
-        Springer Science+Business Media, LLC, part of Springer Nature 2020, https://doi.org/10.1007/s10489-019-01622-1
+class _Tree:
+    """
+    A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
 
+        root : Node
+            The first node of the tree set to Null.
+        summaries : dictionary
+            Stores the nodes itemId which shares same itemId
+        info : dictionary
+            frequency of items in the transactions
+
+    :Methods:
+
+        addTransaction(transaction, freq)
+            adding items of  transactions into the tree as nodes and freq is the count of nodes
+        getFinalConditionalPatterns(node)
+            getting the conditional patterns from fp-tree for a node
+        getConditionalPatterns(patterns, frequencies)
+            sort the patterns by removing the items with lower minSup
+        generatePatterns(prefix)
+            generating the patterns from fp-tree
+    """
+
+    def __init__(self) -> None:
+        self.root = _Node(None, {})
+        self.summaries = {}
+        self.info = {}
+
+    def addTransaction(self, transaction: List[str], count: int) -> None:
+        """
+        Adding transaction into tree
+
+        :param transaction: it represents the one transaction in database
+        :type transaction: list
+        :param count: frequency of item
+        :type count: int
+        :return: None
+        """
+        # This method takes transaction as input and returns the tree
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i] not in currentNode.children:
+                newNode = _Node(transaction[i], {})
+                newNode.freq = count
+                currentNode.addChild(newNode)
+                if transaction[i] in self.summaries:
+                    self.summaries[transaction[i]].append(newNode)
+                else:
+                    self.summaries[transaction[i]] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i]]
+                currentNode.freq += count
+
+    def getFinalConditionalPatterns(self, alpha: str) -> Tuple[List[List[str]], List[int], Dict[str, int]]:
+        """
+        Generates the conditional patterns for a node
+
+        :param alpha: node to generate conditional patterns
+        :return: returns conditional patterns, frequency of each item in conditional patterns
+        """
+        finalPatterns = []
+        finalFreq = []
+        for i in self.summaries[alpha]:
+            set1 = i.freq
+            set2 = []
+            while i.parent.itemId is not None:
+                set2.append(i.parent.itemId)
+                i = i.parent
+            if len(set2) > 0:
+                set2.reverse()
+                finalPatterns.append(set2)
+                finalFreq.append(set1)
+        finalPatterns, finalFreq, info = self.getConditionalTransactions(finalPatterns, finalFreq)
+        return finalPatterns, finalFreq, info
+
+    @staticmethod
+    def getConditionalTransactions(ConditionalPatterns: List[List[str]], conditionalFreq: List[int]) -> Tuple[List[List[str]], List[int], Dict[str, int]]:
+        """
+        To calculate the frequency of items in conditional patterns and sorting the patterns
+
+        :param ConditionalPatterns: paths of a node
+        :param conditionalFreq: frequency of each item in the path
+        :return: conditional patterns and frequency of each item in transactions
+        """
+        global _minSup, _miniWeight
+        pat = []
+        freq = []
+        data1 = {}
+        for i in range(len(ConditionalPatterns)):
+            for j in ConditionalPatterns[i]:
+                if j in data1:
+                    data1[j] += conditionalFreq[i]
+                else:
+                    data1[j] = conditionalFreq[i]
+        up_dict = {k: v for k, v in data1.items() if v >= _minSup and v * _miniWeight > _minSup}
+        count = 0
+        for p in ConditionalPatterns:
+            p1 = [v for v in p if v in up_dict]
+            trans = sorted(p1, key=lambda x: (up_dict.get(x), -x), reverse=True)
+            if len(trans) > 0:
+                pat.append(trans)
+                freq.append(conditionalFreq[count])
+            count += 1
+        return pat, freq, up_dict
+
+    def generatePatterns(self, prefix: List[str]) -> Generator[Tuple[List[str], int], None, None]:
+        """
+        To generate the frequent patterns
+
+        :param prefix: an empty list
+        :return: Frequent patterns that are extracted from fp-tree
+        """
+        global _miniWeight, _maxWeight, _minWeight, _minSup
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x), -x)):
+            pattern = prefix[:]
+            pattern.append(i)
+            yield pattern, self.info[i]
+            patterns, freq, info = self.getFinalConditionalPatterns(i)
+            conditionalTree = _Tree()
+            conditionalTree.info = info.copy()
+            for pat in range(len(patterns)):
+                conditionalTree.addTransaction(patterns[pat], freq[pat])
+            if len(patterns) > 0:
+                for q in conditionalTree.generatePatterns(pattern):
+                    yield q
+
+
+class WFIM(_fp._weightedFrequentPatterns):
+    """
+    :Description:
+       * WFMiner is one of the fundamental algorithm to discover weighted frequent patterns in a transactional database.
+       * It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It employs employs downward closure property to  reduce the search space effectively.
+
+    :Reference :
+           U. Yun and J. J. Leggett, “Wfim: weighted frequent itemset mining with a weight range and a minimum weight,”
+           in Proceedings of the 2005 SIAM International Conference on Data Mining. SIAM, 2005, pp. 636–640.
+           https://epubs.siam.org/doi/pdf/10.1137/1.9781611972757.76
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of weighted Frequent Patterns.
+    :param  oFile: str :
+                   Name of the output file to store complete set of weighted Frequent Patterns.
+    :param  minSup: str or int or float:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
+    :Attributes :
+
         iFile : file
-            Name of the Input file or path of the input file
-        oFile : file
-            Name of the output file or path of the output file
-        minSup : float or int or str
+            Input file name or path of the input file
+        minSup: float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        minWeight: float or int or str
+            The user can specify minWeight either in count or proportion of database size.
+            If the program detects the data type of minWeight is integer, then it treats minWeight is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minWeight=10 will be treated as integer, while minWeight=10.0 will be treated as float
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator.
+        oFile : file
+            Name of the output file or the path of the output file
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime : float
-            To record the start time of the mining process
-        endTime : float
-            To record the completion time of the mining process
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
         lno : int
-            To represent the total no of transaction
+            it represents the total no of transactions
         tree : class
-            To represents the Tree class
-        itemSetCount : int
-            To represents the total no of patterns
+            it represents the Tree class
         finalPatterns : dict
-            To store the complete patterns
-    :Methods:
+            it represents to store the patterns
+
+    :Methods :
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        storePatternsInFile(oFile)
+        save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
-        getPatternsInDataFrame()
+        getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets(fileName)
-            Scans the dataset and stores in a list format
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
         frequentOneItem()
-            Extracts the one-length frequent patterns from database
-        updateTransactions()
-            Update the transactions by removing non-frequent items and sort the Database by item decreased support
-        buildTree()
-            After updating the Database, remaining items will be added into the tree by setting root node as null
-        convert()
-            to convert the user specified value
-        startMine()
-            Mining process will start from this function
-
+            Extracts the one-frequent patterns from transactions
 
     **Methods to execute code on terminal**
-    -----------------------------------------
-            Format:
-                      >>> python3 TUFP.py <inputFile> <outputFile> <minSup>
+    -------------------------------------------
+    .. code-block:: console
 
-            Example:
-                      >>>  python3 TUFP.py sampleTDB.txt patterns.txt 0.6
 
-                      .. note:: minSup  will be considered in support count or frequency
+       Format:
+
+       (.venv) $ python3 basic.py <inputFile> <weightFile> <outputFile> <minSup> <minWeight>
+
+       Example Usage:
+
+       (.venv) $ python3 basic.py sampleDB.txt weightSample.txt patterns.txt 10.0 3.4
+
+
+               .. note:: minSup and maxPer will be considered in support count or frequency
+
 
     **Importing this algorithm into a python program**
-    ------------------------------------------------------
+    -----------------------------------------------------
     .. code-block:: python
 
-            from PAMI.uncertainFrequentPattern.basic import TUFP as alg
+            from PAMI.weightFrequentPattern.basic import basic as alg
 
-            obj = alg.TUFP(iFile, minSup)
+            obj = alg.basic(iFile, wFile, minSup, minWeight)
 
             obj.startMine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            obj.save(oFile)
+            obj.savePatterns(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getmemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
@@ -183,344 +359,365 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    ---------------
-             The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
+    ----------------------
+             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
-    """
+        """
 
-    _startTime = float()
-    _endTime = float()
+    __startTime = float()
+    __endTime = float()
     _minSup = str()
-    _finalPatterns = {}
+    __finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _Database = []
-    _cupList = {}
-    _topk = {}
-    _minimum = 9999
-
-    def _creatingItemSets(self) -> None:
-        """
-        Scans the dataset
-        """
-        self._Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data = [], []
+    __memoryUSS = float()
+    __memoryRSS = float()
+    __Database = []
+    __mapSupport = {}
+    __lno = 0
+    __tree = _Tree()
+    __rank = {}
+    __rankDup = {}
+
+    def __init__(self, iFile: str, wFile: str, minSup: str, minWeight: int, sep: str='\t') -> None:
+        super().__init__(iFile, wFile, minSup, minWeight, sep)
+
+    def __creatingItemSets(self) -> None:
+        """
+        Storing the complete transactions of the database/input file in a database variable
+        :return: None
+        """
+        self.__Database = []
+        if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-            if 'uncertain' in i:
-                uncertain = self._iFile['uncertain'].tolist()
-            for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
-                    product = _Item(data[k][j], uncertain[k][j])
-                    tr.append(product)
-                self._Database.append(tr)
+                self.__Database = self._iFile['Transactions'].tolist()
 
             # print(self.Database)
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if _fp._validators.url(self._iFile):
+                data = _fp._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    tr = []
-                    for i in temp:
-                        i1 = i.index('(')
-                        i2 = i.index(')')
-                        item = i[0:i1]
-                        probability = float(i[i1 + 1:i2])
-                        product = _Item(item, probability)
-                        tr.append(product)
-                    self._Database.append(temp)
+                    self.__Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r') as f:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
+                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            tr = []
-                            for i in temp:
-                                i1 = i.index('(')
-                                i2 = i.index(')')
-                                item = i[0:i1]
-                                probability = float(i[i1 + 1:i2])
-                                product = _Item(item, probability)
-                                tr.append(product)
-                            self._Database.append(tr)
+                            # print(len(temp))
+                            self.__Database.append(temp)
                 except IOError:
                     print("File Not Found")
+                    quit()
 
-    def _frequentOneItem(self) -> List[str]:
+    def _scanningWeights(self) -> None:
         """
-        takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
-
-        :param self.Database : it represents the one self.Database in database
-        :type self.Database : list
+        Storing the weights of the variables in input file in a weights variable
+        :return: None
         """
+        global _weights
+        _weights = {}
+        if isinstance(self._wFile, _fp._pd.DataFrame):
+            items, weights = [], []
+            if self._wFile.empty:
+                print("its empty..")
+            i = self._wFile.columns.values.tolist()
+            if 'items' in i:
+                items = self._wFile['items'].tolist()
+            if 'weights' in i:
+                weights = self._wFile['weights'].tolist()
+            for i in range(len(weights)):
+                _weights[items[i]] = weights[i]
 
-        mapSupport = {}
-        k = 0
-        for i in self._Database:
-            k += 1
-            for j in i:
-                if j.item not in mapSupport:
-                    mapSupport[j.item] = j.probability
-                    self._cupList[j.item] = {k:j.probability}
-                else:
-                    mapSupport[j.item] += j.probability
-                    self._cupList[j.item].update({k: j.probability})
-        plist = [k for k,v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        k = 0
-        for x, in plist:
-            k +=1
-            if k >= self._minSup:
-                break
-            self._finalPatterns[x] = mapSupport[x]
-        self._minimum = min(list(self._finalPatterns.values()))
-        return plist
+            # print(self.Database)
+        if isinstance(self._wFile, str):
+            if _fp._validators.url(self._wFile):
+                data = _fp._urlopen(self._wFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    _weights[temp[0]] = temp[1]
+            else:
+                try:
+                    with open(self._wFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            s = int(float(temp[1]))
+                            _weights[temp[0]] = s
+                except IOError:
+                    print("File Not Found")
+                    quit()
 
-    @staticmethod
-    def _convert(value: Union[int, float, str]) -> Union[int, float]:
+    def __convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
-        To convert the type of user specified minSup value
+        To convert the type of user specified minSup value.
 
         :param value: user specified minSup value
-        :return: converted type minSup value
+        :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = float(value)
+            value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
+                value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _save(self, prefix: List[str], suffix: List[str], tidSetI: Dict[int, float]) -> None:
+    def __frequentOneItem(self) -> List[str]:
         """
-        Saves the patterns that satisfy the periodic frequent property.
+        Generating One frequent items sets
+        :return: list
+        """
+        global _maxWeight
+        self.__mapSupport = {}
+        for tr in self.__Database:
+            for i in range(0, len(tr)):
+                if tr[i] not in self.__mapSupport:
+                    self.__mapSupport[tr[i]] = 1
+                else:
+                    self.__mapSupport[tr[i]] += 1
+        self.__mapSupport = {k: v for k, v in self.__mapSupport.items() if v >= self._minSup and v * _maxWeight > self._minSup}
+        genList = [k for k, v in sorted(self.__mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self.__rank = dict([(index, item) for (item, index) in enumerate(genList)])
+        return genList
+
+    def __updateTransactions(self, itemSet: List[str]) -> List[List[int]]:
+        """
+        Updates the items in transactions with rank of items according to their support
+        :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
+                    rank = {'a':0, 'b':1, 'c':2, 'd':3}
+
+        :param itemSet: list of one-frequent items
+        :return: list
+        """
+        list1 = []
+        for tr in self.__Database:
+            list2 = []
+            for i in range(len(tr)):
+                if tr[i] in itemSet:
+                    list2.append(self.__rank[tr[i]])
+            if len(list2) >= 1:
+                list2.sort()
+                list1.append(list2)
+        return list1
 
-        :param prefix: the prefix of a pattern
-        :type prefix: list
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetI: the timestamp of a patterns
-        :type tidSetI: dict
-        """
-
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = sum(tidSetI.values())
-        #print(prefix, val)
-        if len(self._finalPatterns) <= self._minSup:
-            sample = str()
-            for i in prefix:
-                sample = sample + i + " "
-            self._finalPatterns[sample] = val
-        if len(self._finalPatterns) == self._minSup:
-            if val > self._minimum:
-                sample = str()
-                for i in prefix:
-                    sample = sample + i + " "
-                index = list(self._finalPatterns.keys())[list(self._finalPatterns.values()).index(self._minimum)]
-                del self._finalPatterns[index]
-                self._finalPatterns[sample] = val
-                self._minimum = min(list(self._finalPatterns.values()))
-        #print(self.finalPatterns, self.minimum, self.minSup)
-
-
-    def _Generation(self, prefix: List[str], itemSets: List[str], tidSets: List[Dict[int, float]]) -> None:
-        """
-        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-
-        :param prefix:  main equivalence prefix
-        :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
-        :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
-        :type tidSets: list
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(0, len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
-                continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i+1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
-                sum2 = sum(list(y.values()))
-                #print(prefix, itemJ, y, sum2)
-                #if sum2 >= self.minimum:
-                self._save(prefix, [itemJ], y)
-                classItemSets.append(itemJ)
-                classTidSets.append(y)
-            #print(itemI, tidSetI, classItemSets)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            #self.save(prefix, list(set(itemSetX)), tidSetI)
+    @staticmethod
+    def __buildTree(transactions: List[List[int]], info: Dict[int, int]) -> '_Tree':
+        """
+        Builds the tree with updated transactions
 
-    def startMine(self) -> None:
+        :param transactions: updated transactions
+        :param info: support details of each item in transactions.
+        :return: Transactions compressed in fp-tree
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(transactions)):
+            rootNode.addTransaction(transactions[i], 1)
+        return rootNode
 
+    def __savePeriodic(self, itemSet: List[int]) -> str:
         """
-        global _minSup
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
+        The duplication items and their ranks
+
+        :param itemSet: frequent itemSet that generated
+        :return: patterns with original item names.
+        """
+        temp = str()
+        for i in itemSet:
+            temp = temp + self.__rankDup[i] + "\t"
+        return temp
+
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self) -> None:
+        """
+        main program to start the operation
+        :return: None
+        """
+        global _minSup, _minWeight, _miniWeight, _maxWeight, _weights
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self.__creatingItemSets()
+        self._scanningWeights()
+        _weights = {k: v for k, v in _weights.items() if v >= _minWeight}
+        _maxWeight = max([s for s in _weights.values()])
+        _miniWeight = min([s for s in _weights.values()])
+        self._minSup = self.__convert(self._minSup)
+        _minSup = self._minSup
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        for x, y in self.__rank.items():
+            self.__rankDup[y] = x
+        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent patterns were generated successfully using basic algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
+    def Mine(self) -> None:
+        """
+        main program to start the operation
+        :return: None
+        """
+        global _minSup, _minWeight, _miniWeight, _maxWeight, _weights
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self.__creatingItemSets()
+        self._scanningWeights()
+        _weights = {k: v for k, v in _weights.items() if v >= _minWeight}
+        _maxWeight = max([s for s in _weights.values()])
+        _miniWeight = min([s for s in _weights.values()])
+        self._minSup = self.__convert(self._minSup)
         _minSup = self._minSup
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._cupList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i+1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._cupList[itemJ]
-                y1 = {key: tidSetJ[key] * tidSetI.get(key, 0)  for key in tidSetJ.keys()}
-                self._save(itemSetX, [itemJ], y1)
-                itemSets.append(itemJ)
-                tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print("Top-K Frequent patterns were generated from uncertain databases successfully using TUFP algorithm")
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        for x, y in self.__rank.items():
+            self.__rankDup[y] = x
+        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent patterns were generated successfully using basic algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryUSS
+        return self.__memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function.
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
+        return self.__memoryRSS
 
     def getRuntime(self) -> float:
         """
-        Calculating the total amount of runtime taken by the mining process
+        Calculating the total amount of runtime taken by the mining process.
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
-        return self._endTime - self._startTime
+        return self.__endTime - self.__startTime
 
     def getPatternsAsDataFrame(self) -> pd.DataFrame:
         """
-        Storing final frequent patterns in a dataframe
+        Storing final frequent patterns in a dataframe.
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
-        for a, b in self._finalPatterns.items():
-            data.append([a, b])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        for a, b in self.__finalPatterns.items():
+            data.append([a.replace('\t', ' '), b])
+            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
-        Complete set of frequent patterns will be loaded in to an output file
+        Complete set of frequent patterns will be loaded in to an output file.
 
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
+        :return: None
         """
-        self.oFile = outFile
-        writer = open(self.oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y)
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self.__finalPatterns.items():
+            s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, float]:
+    def getPatterns(self) -> Dict[str, int]:
         """
-        Function to send the set of frequent patterns after completion of the mining process
+        Function to send the set of frequent patterns after completion of the mining process.
 
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self._finalPatterns
+        return self.__finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
-        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Weighted Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
+        
+
+
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-        if len(_ab._sys.argv) == 5:
-            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3])
+    if len(_fp._sys.argv) == 6 or len(_fp._sys.argv) == 7:
+        if len(_fp._sys.argv) == 7:
+            _ap = WFIM(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
+        if len(_fp._sys.argv) == 6:
+            _ap = WFIM(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5])
         _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Patterns:", len(_Patterns))
-        _ap.save(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total number of Weighted Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_fp._sys.argv[2])
+        print("Total Memory in USS:",  _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
-        '''ap = TUFP("/home/apiiit-rkv/Desktop/uncertain/tubeSample", 10, ' ')
-        ap.startMine()
-        Patterns = ap.getPatterns()
-        print("Total number of Patterns:", len(Patterns))
-        ap.save("patterns.txt")
-        memUSS = ap.getMemoryUSS()
-        print("Total Memory in USS:", memUSS)
-        memRSS = ap.getMemoryRSS()
-        print("Total Memory in RSS", memRSS)
-        run = ap.getRuntime()
-        print("Total ExecutionTime in ms:", run)'''
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/TubeP.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TubeS.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-# TubeP is one of the fastest algorithm to discover frequent patterns in an uncertain transactional database.
+# TubeS is one of the fastest algorithm to discover frequent patterns in a uncertain transactional database.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     from PAMI.uncertainFrequentPattern.basic import TubeP as alg
+#     from PAMI.uncertainFrequentPattern.basic import TubeS as alg
 #
-#     obj = alg.TubeP(iFile, minSup)
+#     obj = alg.TubeS(iFile, minSup)
 #
 #     obj.startMine()
 #
 #     frequentPatterns = obj.getPatterns()
 #
 #     print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -25,160 +25,158 @@
 #
 #     print("Total Memory in RSS", memRSS)
 #
 #     run = obj.getRuntime()
 #
 #     print("Total ExecutionTime in seconds:", run)
 #
-#
+
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
-
 """
 
 from PAMI.uncertainFrequentPattern.basic import abstract as _fp
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
-import pandas as pd
 
 _minSup = float()
 _fp._sys.setrecursionlimit(20000)
 _finalPatterns = {}
 
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
-
-    :Attributes:
-
-    item : int or word
+    :Attributes
+        item : int or word
             Represents the name of the item
-    probability : float
+        probability : float
             Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self, item: int, probability: float) -> None:
+    def __init__(self, item, probability):
         self.item = item
         self.probability = probability
 
 
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
-
     :Attributes:
-
         item : int
             storing item of a node
         probability : int
             To maintain the expected support of node
         parent : node
             To maintain the parent of every node
         children : list
             To maintain the children of node
-
     :Methods:
-
         addChild(itemName)
-            storing the children to their respective parent nodes
+             storing the children to their respective parent nodes
     """
 
-    def __init__(self, item: int, children: list) -> None:
+    def __init__(self, item, children):
         self.item = item
         self.probability = 1
-        self.maxPrefixProbability = 1
-        self.p = 1
+        self.secondProbability = 1
         self.children = children
         self.parent = None
 
-    def addChild(self, node) -> None:
+    def addChild(self, node):
         """
-        This function is used to add a child
+        This function is used to add child
         """
         self.children[node.item] = node
         node.parent = self
 
 
-def printTree(root) -> None:
+def Second(transaction, i):
     """
-    To print the tree with root node through recursion
+    To calculate the second probability of a node in transaction
+    :param transaction: transaction in a database
+    :param i: index of item in transaction
+    :return: second probability of a node
+    """
+    temp = []
+    for j in range(0, i):
+        temp.append(transaction[j].probability)
+    l1 = max(temp)
+    temp.remove(l1)
+    l2 = max(temp)
+    return l2 * l2
 
+
+def printTree(root):
+    """
+    To print the tree with root node through recursion
     :param root: root node of  tree
     :return: details of tree
     """
     for x, y in root.children.items():
-        print(x, y.item, y.probability, y.parent.item, y.tids, y.maxPrefixProbability)
+        print(x, y.item, y.probability, y.parent.item, y.tids, y.secondProbability)
         printTree(y)
 
 
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
-
     :Attributes:
-
         root : Node
             Represents the root node of the tree
         summaries : dictionary
             storing the nodes with same item name
         info : dictionary
             stores the support of items
-
     :Methods:
-
         addTransaction(transaction)
             creating transaction as a branch in frequentPatternTree
         addConditionalTransaction(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
         conditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
         conditionalTransactions(prefixPaths,Support)
-                takes the prefixPath of a node and support at child of the path and extract the frequent items from
-                prefixPaths and generates prefixPaths with items which are frequent
-        remove(Node)
-                removes the node from tree once after generating all the patterns respective to the node
-        generatePatterns(Node)
+            takes the prefixPath of a node and support at child of the path and extract the frequent items from
+            prefixPaths and generates prefixPaths with items which are frequent
+        removeNode(Node)
+            removes the node from tree once after generating all the patterns respective to the node
+        generate_patterns(Node)
             starts from the root node of the tree and mines the frequent patterns
-    """
+            """
 
-    def __init__(self) -> None:
+    def __init__(self):
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransaction(self, transaction: list) -> None:
+    def addTransaction(self, transaction):
         """
-        Adding transaction into tree
-
-        :param transaction : it represents the one transaction in database
+        adding transaction into tree
+        :param transaction : it represents the one transactions in database
         :type transaction : list
         """
         currentNode = self.root
         k = 0
         for i in range(len(transaction)):
             k += 1
             if transaction[i].item not in currentNode.children:
                 newNode = _Node(transaction[i].item, {})
                 newNode.k = k
-                newNode.prefixProbability = transaction[i].probability
+                if k >= 3:
+                    newNode.secondProbability = Second(transaction, i)
                 l1 = i - 1
                 temp = []
                 while l1 >= 0:
                     temp.append(transaction[l1].probability)
                     l1 -= 1
                 if len(temp) == 0:
                     newNode.probability = round(transaction[i].probability, 2)
@@ -188,92 +186,90 @@
                 if transaction[i].item in self.summaries:
                     self.summaries[transaction[i].item].append(newNode)
                 else:
                     self.summaries[transaction[i].item] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i].item]
-                currentNode.prefixProbability = max(transaction[i].probability, currentNode.prefixProbability)
+                if k >= 3:
+                    currentNode.secondProbability = max(transaction[i].probability, currentNode.secondProbability)
                 currentNode.k = k
                 l1 = i - 1
                 temp = []
                 while l1 >= 0:
                     temp.append(transaction[l1].probability)
                     l1 -= 1
                 if len(temp) == 0:
                     currentNode.probability += round(transaction[i].probability, 2)
                 else:
                     nn = max(temp) * transaction[i].probability
                     currentNode.probability += round(nn, 2)
 
-    def addConditionalTransaction(self, transaction: list, sup: int, second: float) -> None:
+    def addConditionalTransaction(self, transaction, sup, second):
         """
-        Constructing conditional tree from prefixPaths
-
-        :param transaction: it represents the one transaction in database
-        :type transaction: list
-        :param sup: support of prefixPath taken at last child of the path
-        :type sup: int
-        :param second: the second probability of the node
+        constructing conditional tree from prefixPaths
+        :param transaction : it represents the one transactions in database
+        :type transaction : list
+        :param sup : support of prefixPath taken at last child of the path
+        :type sup : int
+        :param second: second probability of the leaf node
         :type second: float
         """
         currentNode = self.root
         k = 0
         for i in range(len(transaction)):
             k += 1
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
                 newNode.k = k
-                newNode.prefixProbability = second
+                newNode.secondProbability = second
                 newNode.probability = sup
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
                 currentNode.k = k
-                currentNode.prefixProbability = max(currentNode.prefixProbability, second)
+                currentNode.secondProbability = max(currentNode.secondProbability, second)
                 currentNode.probability += sup
 
-    def conditionalPatterns(self, alpha) -> Tuple:
+    def conditionalPatterns(self, alpha):
         """
-        Generates all the conditional patterns of respective node
-
+        generates all the conditional patterns of respective node
         :param alpha : it represents the Node in tree
         :type alpha : _Node
         """
         finalPatterns = []
         sup = []
         second = []
         for i in self.summaries[alpha]:
             s = i.probability
-            s1 = i.maxPrefixProbability
+            s1 = i.secondProbability
             set2 = []
             while i.parent.item is not None:
                 set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
                 second.append(s1)
                 sup.append(s)
         finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
         return finalPatterns, support, info, second
 
-    def conditionalTransactions(self, condPatterns: list, support: list) -> Tuple:
+    def conditionalTransactions(self, condPatterns, support):
         """
         It generates the conditional patterns with frequent items
-
-        :param condPatterns: condPatterns generated from condition pattern method for respective node
-        :type condPatterns: list
-        :param support: the support of conditional pattern in tree
-        :type support: list
+        :param condPatterns : conditional patterns generated from conditionalPatterns() method for respective node
+        :type condPatterns : list
+        :param support : the support of conditional pattern in tree
+        :type support : list
         """
         global _minSup
         pat = []
         sup = []
         data1 = {}
         for i in range(len(condPatterns)):
             for j in condPatterns[i]:
@@ -289,69 +285,64 @@
             trans = sorted(p1, key=lambda x: (updatedDict.get(x)), reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
                 sup.append(support[count])
             count += 1
         return pat, sup, updatedDict
 
-    def removeNode(self, nodeValue) -> None:
+    def removeNode(self, nodeValue):
         """
-        Removing the node from tree
-
+        removing the node from tree
         :param nodeValue : it represents the node in tree
         :type nodeValue : node
         """
         for i in self.summaries[nodeValue]:
             del i.parent.children[nodeValue]
 
-    def generatePatterns(self, prefix: list) -> None:
+    def generatePatterns(self, prefix):
         """
-        Generates the patterns
-
+        generates the patterns
         :param prefix : forms the combination of items
         :type prefix : list
         """
         global _finalPatterns, _minSup
         for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
             pattern = prefix[:]
             pattern.append(i)
             s = 0
             for x in self.summaries[i]:
                 #if x.k <= 2:
                     #s += x.probability
                 #elif x.k >= 3:
-                    #n = x.probability * pow(x.prefixProbability, (x.k - 2))
+                    #n = x.probability * pow(x.secondProbability, (x.k - 2))
                     #s += n
                 if len(pattern) <= 2:
                     s += x.probability
                 elif len(pattern) >= 3:
-                    n = x.probability * pow(x.prefixProbability, (x.k - 2))
+                    n = x.probability * pow(x.secondProbability, (x.k - 2))
                     s += n
             _finalPatterns[tuple(pattern)] = self.info[i]
             if s >= _minSup:
                 patterns, support, info, second = self.conditionalPatterns(i)
                 conditionalTree = _Tree()
                 conditionalTree.info = info.copy()
                 for pat in range(len(patterns)):
                     conditionalTree.addConditionalTransaction(patterns[pat], support[pat], second[pat])
                 if len(patterns) > 0:
                     conditionalTree.generatePatterns(pattern)
             self.removeNode(i)
 
 
-class TubeP(_fp._frequentPatterns):
+class TubeS(_fp._frequentPatterns):
     """
-    :Description: TubeP is one of the fastest algorithm to discover frequent patterns in a uncertain transactional database.
-
+    :Description: TubeS is one of the fastest algorithm to discover frequent patterns in a uncertain transactional database.
     :Reference:
         Carson Kai-Sang Leung and Richard Kyle MacKinnon. 2014. Fast Algorithms for Frequent Itemset Mining from Uncertain Data.
         In Proceedings of the 2014 IEEE International Conference on Data Mining (ICDM '14). IEEE Computer Society, USA, 893–898. https://doi.org/10.1109/ICDM.2014.146
-
     :Attributes:
-
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
         minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
@@ -377,15 +368,14 @@
         tree : class
             To represents the Tree class
         itemSetCount : int
             To represents the total no of patterns
         finalPatterns : dict
             To store the complete patterns
     :Methods:
-
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
@@ -402,77 +392,58 @@
             Extracts the one-length frequent patterns from database
         updateTransactions()
             Update the transactions by removing non-frequent items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
-
     **Methods to execute code on terminal**
     --------------------------------------------
             Format:
-                      >>> python3 TubeP.py <inputFile> <outputFile> <minSup>
-
+                      >>> python3 TubeS.py <inputFile> <outputFile> <minSup>
             Example:
-                      >>>  python3 TubeP.py sampleTDB.txt patterns.txt 3
-
+                      >>>  python3 TubeS.py sampleTDB.txt patterns.txt 3
                     .. note:: minSup  will be considered in support count or frequency
-
     **Importing this algorithm into a python program**
-    -----------------------------------------------------
+    ---------------------------------------------------
     .. code-block:: python
-
-            from PAMI.uncertainFrequentPattern.basic import TubeP as alg
-
-            obj = alg.TubeP(iFile, minSup)
-
+            from PAMI.uncertainFrequentPattern.basic import TubeS as alg
+            obj = alg.TubeS(iFile, minSup)
             obj.startMine()
-
             frequentPatterns = obj.getPatterns()
-
             print("Total number of Frequent Patterns:", len(frequentPatterns))
-
             obj.save(oFile)
-
             Df = obj.getPatternsAsDataFrame()
-
-            memUSS = obj.getmemoryUSS()
-
+            memUSS = obj.getMemoryUSS()
             print("Total Memory in USS:", memUSS)
-
             memRSS = obj.getMemoryRSS()
-
             print("Total Memory in RSS", memRSS)
-
             run = obj.getRuntime()
-
             print("Total ExecutionTime in seconds:", run)
-
     **Credits:**
-    --------------
-             The complete program was written by  P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    ---------------
+    The complete program was written by  P.Likhitha   under the supervision of Professor Rage Uday Kiran.
 """
     _startTime = float()
     _endTime = float()
-    _minSup = str()
+    _minSup = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
     _rank = {}
-
-    def __init__(self, iFile, minSup, sep='\t') -> None:
+    _lno = 0
+    def __init__(self, iFile, minSup, sep='\t'):
         super().__init__(iFile, minSup, sep)
-
-    def _creatingItemSets(self) -> None:
+    def _creatingItemSets(self):
         """
-        Scans the dataset and stores the transactions into Database variable
+        Scans the databases and stores the transactions into Database variable
         """
         self._Database = []
         if isinstance(self._iFile, _fp._pd.DataFrame):
             uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
@@ -482,14 +453,15 @@
                 uncertain = self._iFile['uncertain'].tolist()
             for k in range(len(data)):
                 tr = []
                 for j in range(len(data[k])):
                     product = _Item(data[k][j], uncertain[k][j])
                     tr.append(product)
                 self._Database.append(tr)
+                self._lno += 1
 
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _fp._validators.url(self._iFile):
                 data = _fp._urlopen(self._iFile)
                 for line in data:
                     line = line.strip()
@@ -499,14 +471,15 @@
                     uncertain = [float(i.rstrip()) for i in temp[1].split(self._sep)]
                     tr = []
                     for i in range(len(temp)):
                         item = temp[i]
                         probability = uncertain[i]
                         product = _Item(item, probability)
                         tr.append(product)
+                    self._lno += 1
                     self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r') as f:
                         for line in f:
                             temp1 = line.strip()
                             temp1 = temp1.split(':')
@@ -514,19 +487,20 @@
                             uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep)]
                             tr = []
                             for i in range(len(temp)):
                                 item = temp[i]
                                 probability = uncertain[i]
                                 product = _Item(item, probability)
                                 tr.append(product)
+                            self._lno += 1
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
 
-    def _frequentOneItem(self) -> Tuple:
+    def _frequentOneItem(self):
         """
         Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
         """
         global _minSup
         mapSupport = {}
         for i in self._Database:
             for j in i:
@@ -535,89 +509,84 @@
                 else:
                     mapSupport[j.item] += round(j.probability, 2)
         mapSupport = {k: round(v, 2) for k, v in mapSupport.items() if v >= self._minSup}
         plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
         self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
         return mapSupport, plist
 
-    def _buildTree(self, data: list, info) -> _Tree:
+    def _buildTree(self, data, info):
         """
-        It takes the transactions and support of each item and construct the main tree with setting root node as null
-
-        :param data : it represents the one transaction in database
+        it takes the transactions and support of each item and construct the main tree with setting root node as null
+        :param data : it represents the one transactions in database
         :type data : list
         :param info : it represents the support of each item
         :type info : dictionary
         """
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
             rootNode.addTransaction(data[i])
         return rootNode
 
-    def _updateTransactions(self, dict1) -> List:
+    def updateTransactions(self, dict1):
         """
-        Remove the items which are not frequent from transactions and updates the transactions with rank of items
-
+        remove the items which are not frequent from transactions and updates the transactions with rank of items
         :param dict1 : frequent items with support
         :type dict1 : dictionary
         """
         list1 = []
         for tr in self._Database:
             list2 = []
             for i in range(0, len(tr)):
                 if tr[i].item in dict1:
                     list2.append(tr[i])
-            if len(list2) >= 2:
+            if (len(list2) >= 2):
                 basket = list2
                 basket.sort(key=lambda val: self._rank[val.item])
                 list2 = basket
                 list1.append(list2)
         return list1
 
-    def _Check(self, i, x) -> int:
+    def _Check(self, i, x):
         """
         To check the presence of item or pattern in transaction
-
         :param x: it represents the pattern
         :type x : list
         :param i : represents the uncertain transactions
         :type i : list
         """
         for m in x:
             k = 0
             for n in i:
                 if m == n.item:
                     k += 1
             if k == 0:
                 return 0
         return 1
 
-    def _convert(self, value) -> Union[int, float]:
+    def _convert(self, value):
         """
         To convert the type of user specified minSup value
-
         :param value: user specified minSup value
         :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _removeFalsePositives(self) -> None:
+    def _removeFalsePositives(self):
         """
-        To remove the false positive patterns generated in frequent patterns.
-
+        To remove the false positive patterns generated in frequent patterns
         :return: patterns with accurate probability
         """
         global _finalPatterns
         periods = {}
         for i in self._Database:
             for x, y in _finalPatterns.items():
                 if len(x) == 1:
@@ -636,123 +605,117 @@
         for x, y in periods.items():
             if y >= self._minSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
-    def startMine(self) -> None:
+    def startMine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
         global _minSup
         self._startTime = _fp._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
         _minSup = self._minSup
         self._finalPatterns = {}
         mapSupport, plist = self._frequentOneItem()
-        transactions1 = self._updateTransactions(mapSupport)
+        transactions1 = self.updateTransactions(mapSupport)
         info = {k: v for k, v in mapSupport.items()}
         Tree1 = self._buildTree(transactions1, info)
         Tree1.generatePatterns([])
         self._removeFalsePositives()
-        print("Uncertain Frequent patterns were generated successfully using TubeP algorithm")
+        print("Uncertain Frequent patterns were generated successfully using TubeS algorithm")
         self._endTime = _fp._time.time()
         process = _fp._psutil.Process(_fp._os.getpid())
-        self._memoryRSS = float()
         self._memoryUSS = float()
+        self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
-    def save(self, outFile: str) -> None:
+    def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
-
         :param outFile: name of the output file
         :type outFile: file
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> int:
+    def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
-
         :return: returning frequent patterns
         :rtype: dict
         """
         return len(self._finalPatterns)
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
-        This Function is used to print the results
+        This function is used to print the results
         """
         print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_fp._sys.argv) == 4 or len(_fp._sys.argv) == 5:
         if len(_fp._sys.argv) == 5:
-            _ap = TubeP(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4])
+            _ap = TubeS(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4])
         if len(_fp._sys.argv) == 4:
-            _ap = TubeP(_fp._sys.argv[1], _fp._sys.argv[3])
+            _ap = TubeS(_fp._sys.argv[1], _fp._sys.argv[3])
         _ap.startMine()
-        print("Total number of  Uncertain Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Uncertain Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_fp._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/TubeS.py` & `pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,38 +1,40 @@
-# TubeS is one of the fastest algorithm to discover frequent patterns in a uncertain transactional database.
+# GFPGrowth algorithm is used to discover geo-referenced frequent patterns in a uncertain transactional database using GFP-Tree.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     from PAMI.uncertainFrequentPattern.basic import TubeS as alg
 #
-#     obj = alg.TubeS(iFile, minSup)
+#             from PAMI.uncertainGeoreferencedFrequentPattern.basic import GFPGrowth as alg
 #
-#     obj.startMine()
+#             obj = alg.GFPGrowth(iFile, nFile, minSup,sep, oFile)
 #
-#     frequentPatterns = obj.getPatterns()
+#             obj.startMine()
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             Patterns = obj.getPatterns()
 #
-#     obj.save(oFile)
+#             print("Total number of  Patterns:", len(Patterns))
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             obj.save(oFile)
 #
-#     memUSS = obj.getMemoryUSS()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     print("Total Memory in USS:", memUSS)
+#             memUSS = obj.getMemoryUSS()
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     run = obj.getRuntime()
+#             print("Total Memory in RSS", memRSS)
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             run = obj.getRuntime()
 #
+#             print("Total ExecutionTime in seconds:", run)
+#
+
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
@@ -45,27 +47,29 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
-
-from PAMI.uncertainFrequentPattern.basic import abstract as _fp
-
-_minSup = float()
-_fp._sys.setrecursionlimit(20000)
+from PAMI.uncertainGeoreferencedFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
+_minSup = str()
+_neighbourList = {}
+_ab._sys.setrecursionlimit(20000)
 _finalPatterns = {}
 
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
 
-    :Attributes
+    :Attributes:
 
         item : int or word
             Represents the name of the item
         probability : float
             Represent the existential probability(likelihood presence) of an item
     """
 
@@ -84,64 +88,32 @@
             storing item of a node
         probability : int
             To maintain the expected support of node
         parent : node
             To maintain the parent of every node
         children : list
             To maintain the children of node
+
     :Methods:
 
         addChild(itemName)
-             storing the children to their respective parent nodes
+            storing the children to their respective parent nodes
     """
 
     def __init__(self, item, children):
         self.item = item
         self.probability = 1
-        self.secondProbability = 1
         self.children = children
         self.parent = None
 
     def addChild(self, node):
-        """
-        This function is used to add child
-        """
         self.children[node.item] = node
         node.parent = self
 
 
-def Second(transaction, i):
-    """
-    To calculate the second probability of a node in transaction
-
-    :param transaction: transaction in a database
-    :param i: index of item in transaction
-    :return: second probability of a node
-    """
-    temp = []
-    for j in range(0, i):
-        temp.append(transaction[j].probability)
-    l1 = max(temp)
-    temp.remove(l1)
-    l2 = max(temp)
-    return l2 * l2
-
-
-def printTree(root):
-    """
-    To print the tree with root node through recursion
-
-    :param root: root node of  tree
-    :return: details of tree
-    """
-    for x, y in root.children.items():
-        print(x, y.item, y.probability, y.parent.item, y.tids, y.secondProbability)
-        printTree(y)
-
-
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
 
         root : Node
@@ -150,261 +122,261 @@
             storing the nodes with same item name
         info : dictionary
             stores the support of items
     :Methods:
 
         addTransaction(transaction)
             creating transaction as a branch in frequentPatternTree
-        addConditionalTransaction(prefixPaths, supportOfItems)
+        addConditionalPattern(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
         conditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
         conditionalTransactions(prefixPaths,Support)
             takes the prefixPath of a node and support at child of the path and extract the frequent items from
             prefixPaths and generates prefixPaths with items which are frequent
-        removeNode(Node)
+        remove(Node)
             removes the node from tree once after generating all the patterns respective to the node
-        generate_patterns(Node)
+        generatePatterns(Node)
             starts from the root node of the tree and mines the frequent patterns
-            """
+    """
 
     def __init__(self):
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
     def addTransaction(self, transaction):
         """
-        adding transaction into tree
+        Adding transaction into tree
 
-        :param transaction : it represents the one transactions in database
+        :param transaction : it represents the one self.Database in database
         :type transaction : list
         """
+        global _neighbourList
         currentNode = self.root
-        k = 0
         for i in range(len(transaction)):
-            k += 1
             if transaction[i].item not in currentNode.children:
                 newNode = _Node(transaction[i].item, {})
-                newNode.k = k
-                if k >= 3:
-                    newNode.secondProbability = Second(transaction, i)
+                nei = _neighbourList.get(transaction[i].item)
                 l1 = i - 1
-                temp = []
+                lp = []
                 while l1 >= 0:
-                    temp.append(transaction[l1].probability)
+                    if nei == None:
+                        break
+                    if transaction[l1].item in nei:
+                        lp.append(transaction[l1].probability)
                     l1 -= 1
-                if len(temp) == 0:
-                    newNode.probability = round(transaction[i].probability, 2)
+                if len(lp) == 0:
+                    newNode.probability = transaction[i].probability
                 else:
-                    newNode.probability = round(max(temp) * transaction[i].probability, 2)
+                    newNode.probability = max(lp) * transaction[i].probability
                 currentNode.addChild(newNode)
                 if transaction[i].item in self.summaries:
                     self.summaries[transaction[i].item].append(newNode)
                 else:
                     self.summaries[transaction[i].item] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i].item]
-                if k >= 3:
-                    currentNode.secondProbability = max(transaction[i].probability, currentNode.secondProbability)
-                currentNode.k = k
                 l1 = i - 1
-                temp = []
+                lp = []
                 while l1 >= 0:
-                    temp.append(transaction[l1].probability)
+                    lp.append(transaction[l1].probability)
                     l1 -= 1
-                if len(temp) == 0:
-                    currentNode.probability += round(transaction[i].probability, 2)
+                if len(lp) == 0:
+                    currentNode.probability += transaction[i].probability
                 else:
-                    nn = max(temp) * transaction[i].probability
-                    currentNode.probability += round(nn, 2)
+                    currentNode.probability += max(lp) * transaction[i].probability
 
-    def addConditionalTransaction(self, transaction, sup, second):
+    def addConditionalPattern(self, transaction, sup):
         """
         constructing conditional tree from prefixPaths
 
-        :param transaction : it represents the one transactions in database
+        :param transaction : it represents the one self.Database in database
         :type transaction : list
         :param sup : support of prefixPath taken at last child of the path
         :type sup : int
-        :param second: second probability of the leaf node
-        :type second: float
         """
+
+        # This method takes transaction, support and constructs the conditional tree
         currentNode = self.root
-        k = 0
         for i in range(len(transaction)):
-            k += 1
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
-                newNode.k = k
-                newNode.secondProbability = second
                 newNode.probability = sup
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
-                currentNode.k = k
-                currentNode.secondProbability = max(currentNode.secondProbability, second)
                 currentNode.probability += sup
 
     def conditionalPatterns(self, alpha):
         """
-        generates all the conditional patterns of respective node
+        Generates all the conditional patterns of respective node
 
         :param alpha : it represents the Node in tree
         :type alpha : _Node
         """
+
+        # This method generates conditional patterns of node by traversing the tree
+        global _neighbourList
         finalPatterns = []
         sup = []
-        second = []
         for i in self.summaries[alpha]:
+            j = i.item
             s = i.probability
-            s1 = i.secondProbability
             set2 = []
             while i.parent.item is not None:
-                set2.append(i.parent.item)
+                if _neighbourList.get(j) is not None:
+                    #print(_neighbourList.get(j))
+                    if i.parent.item in _neighbourList[j]:
+                        set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
-                second.append(s1)
                 sup.append(s)
         finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
-        return finalPatterns, support, info, second
+        return finalPatterns, support, info
+
+    def removeNode(self, nodeValue):
+        """
+        Removing the node from tree
+
+        :param nodeValue : it represents the node in tree
+        :type nodeValue : node
+        """
+
+        for i in self.summaries[nodeValue]:
+            del i.parent.children[nodeValue]
 
     def conditionalTransactions(self, condPatterns, support):
         """
         It generates the conditional patterns with frequent items
 
-        :param condPatterns : conditional patterns generated from conditionalPatterns() method for respective node
+        :param condPatterns : conditionalPatterns generated from conditionalPattern method for respective node
         :type condPatterns : list
-        :param support : the support of conditional pattern in tree
-        :type support : list
+        :support : the support of conditional pattern in tree
+        :support : int
         """
-        global _minSup
+
+        global minSup
         pat = []
         sup = []
-        data1 = {}
+        count = {}
         for i in range(len(condPatterns)):
             for j in condPatterns[i]:
-                if j in data1:
-                    data1[j] += support[i]
+                if j in count:
+                    count[j] += support[i]
                 else:
-                    data1[j] = support[i]
+                    count[j] = support[i]
         updatedDict = {}
-        updatedDict = {k: v for k, v in data1.items() if v >= _minSup}
+        updatedDict = {k: v for k, v in count.items() if v >= minSup}
         count = 0
         for p in condPatterns:
             p1 = [v for v in p if v in updatedDict]
-            trans = sorted(p1, key=lambda x: (updatedDict.get(x)), reverse=True)
+            trans = sorted(p1, key=lambda x: updatedDict[x], reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
                 sup.append(support[count])
-            count += 1
+                count += 1
         return pat, sup, updatedDict
 
-    def removeNode(self, nodeValue):
-        """
-        removing the node from tree
-
-        :param nodeValue : it represents the node in tree
-        :type nodeValue : node
-        """
-        for i in self.summaries[nodeValue]:
-            del i.parent.children[nodeValue]
-
     def generatePatterns(self, prefix):
         """
-        generates the patterns
+        Generates the patterns
 
         :param prefix : forms the combination of items
         :type prefix : list
         """
-        global _finalPatterns, _minSup
+
+        global _finalPatterns, minSup
         for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
             pattern = prefix[:]
             pattern.append(i)
             s = 0
             for x in self.summaries[i]:
-                #if x.k <= 2:
-                    #s += x.probability
-                #elif x.k >= 3:
-                    #n = x.probability * pow(x.secondProbability, (x.k - 2))
-                    #s += n
-                if len(pattern) <= 2:
-                    s += x.probability
-                elif len(pattern) >= 3:
-                    n = x.probability * pow(x.secondProbability, (x.k - 2))
-                    s += n
+                s += x.probability
             _finalPatterns[tuple(pattern)] = self.info[i]
-            if s >= _minSup:
-                patterns, support, info, second = self.conditionalPatterns(i)
+            if s >= minSup:
+                patterns, support, info = self.conditionalPatterns(i)
                 conditionalTree = _Tree()
                 conditionalTree.info = info.copy()
                 for pat in range(len(patterns)):
-                    conditionalTree.addConditionalTransaction(patterns[pat], support[pat], second[pat])
+                    conditionalTree.addConditionalPattern(patterns[pat], support[pat])
                 if len(patterns) > 0:
                     conditionalTree.generatePatterns(pattern)
             self.removeNode(i)
 
 
-class TubeS(_fp._frequentPatterns):
+class GFPGrowth(_ab._frequentPatterns):
     """
-    :Description: TubeS is one of the fastest algorithm to discover frequent patterns in a uncertain transactional database.
+    :Description: GFPGrowth algorithm is used to discover geo-referenced frequent patterns in a uncertain transactional database using GFP-Tree.
 
     :Reference:
-        Carson Kai-Sang Leung and Richard Kyle MacKinnon. 2014. Fast Algorithms for Frequent Itemset Mining from Uncertain Data.
-        In Proceedings of the 2014 IEEE International Conference on Data Mining (ICDM '14). IEEE Computer Society, USA, 893–898. https://doi.org/10.1109/ICDM.2014.146
+         Palla Likhitha,Pamalla Veena, Rage, Uday Kiran, Koji Zettsu (2023).
+         "Discovering Geo-referenced Frequent Patterns in Uncertain Geo-referenced
+         Transactional Databases".  PAKDD 2023.
+         https://doi.org/10.1007/978-3-031-33380-4_3
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of uncertain Geo referenced Frequent Patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Uncertain Geo referenced frequent patterns
+    :param  minSup: str:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
-        minSup : float or int or str
+        minSup: float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime : float
+        startTime:float
             To record the start time of the mining process
-        endTime : float
+        endTime:float
             To record the completion time of the mining process
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
         lno : int
             To represent the total no of transaction
         tree : class
             To represents the Tree class
         itemSetCount : int
             To represents the total no of patterns
         finalPatterns : dict
             To store the complete patterns
+
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
+        savePatterns(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
@@ -416,38 +388,47 @@
             Extracts the one-length frequent patterns from database
         updateTransactions()
             Update the transactions by removing non-frequent items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
+        startMine()
+            Mining process will start from this function
+
+    **Executing the code on terminal**:
+    ------------------------------------------
+
+    .. code-block:: console
+
 
-    **Methods to execute code on terminal**
-    --------------------------------------------
-            Format:
-                      >>> python3 TubeS.py <inputFile> <outputFile> <minSup>
+       Format:
 
-            Example:
-                      >>>  python3 TubeS.py sampleTDB.txt patterns.txt 3
+       (.venv) $ python3 GFPGrowth.py <inputFile> <neighborFile> <outputFile> <minSup>
 
-                    .. note:: minSup  will be considered in support count or frequency
+       Examples usage:
 
-    **Importing this algorithm into a python program**
-    ---------------------------------------------------
-    .. code-block:: python
+       (.venv) $ python3 GFPGrowth.py sampleTDB.txt sampleNeighbor.txt patterns.txt 3
 
-            from PAMI.uncertainFrequentPattern.basic import TubeS as alg
 
-            obj = alg.TubeS(iFile, minSup)
+               .. note:: minSup  will be considered in support count or frequency
+    
+    **Sample run of importing the code**:
+    ----------------------------------------
+     .. code-block:: python
+
+            from PAMI.uncertainGeoreferencedFrequentPattern.basic import GFPGrowth as alg
+
+            obj = alg.GFPGrowth(iFile, nFile, minSup)
 
             obj.startMine()
 
-            frequentPatterns = obj.getPatterns()
+            Patterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            print("Total number of  Patterns:", len(Patterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
@@ -456,154 +437,210 @@
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
-
-    **Credits:**
-    ---------------
-
-    The complete program was written by  P.Likhitha   under the supervision of Professor Rage Uday Kiran.
-"""
+        
+    **Credits**:
+    -------------
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
+        """
     _startTime = float()
     _endTime = float()
-    _minSup = float()
+    _minSup = str()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
     _rank = {}
-    _lno = 0
-    def __init__(self, iFile, minSup, sep='\t'):
-        super().__init__(iFile, minSup, sep)
+
+    def __init__(self, iFile, nFile, minSup, sep='\t'):
+        super().__init__(iFile, nFile, minSup, sep)
+
     def _creatingItemSets(self):
         """
-        Scans the databases and stores the transactions into Database variable
+        Scans the uncertain transactional dataset
         """
         self._Database = []
-        if isinstance(self._iFile, _fp._pd.DataFrame):
+        if isinstance(self._iFile, _ab._pd.DataFrame):
             uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 self._Database = self._iFile['Transactions'].tolist()
             if 'uncertain' in i:
                 uncertain = self._iFile['uncertain'].tolist()
             for k in range(len(data)):
                 tr = []
                 for j in range(len(data[k])):
                     product = _Item(data[k][j], uncertain[k][j])
                     tr.append(product)
                 self._Database.append(tr)
-                self._lno += 1
 
             # print(self.Database)
         if isinstance(self._iFile, str):
-            if _fp._validators.url(self._iFile):
-                data = _fp._urlopen(self._iFile)
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
                 for line in data:
-                    line = line.strip()
-                    line = line.decode("utf-8")
-                    temp1 = line.split(':')
-                    temp = [i.rstrip() for i in temp[0].split(self._sep)]
-                    uncertain = [float(i.rstrip()) for i in temp[1].split(self._sep)]
+                    temp1 = line.strip()
+                    temp1 = temp1.split(':')
+                    temp = [i.rstrip() for i in temp1[0].split(self._sep)]
+                    uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep)]
                     tr = []
                     for i in range(len(temp)):
                         item = temp[i]
                         probability = uncertain[i]
                         product = _Item(item, probability)
                         tr.append(product)
-                    self._lno += 1
-                    self._Database.append(temp)
+                    self._Database.append(tr)
             else:
                 try:
                     with open(self._iFile, 'r') as f:
                         for line in f:
                             temp1 = line.strip()
                             temp1 = temp1.split(':')
-                            temp = [i.rstrip() for i in temp1[0].split(self._sep)]
-                            uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep)]
+                            #temp1[0], temp1[1] = [i for i in temp1[0] if i], [i for i in temp1[1] if i]
+                            temp = [i.rstrip() for i in temp1[0].split(self._sep) if i]
+                            uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep) if i]
                             tr = []
                             for i in range(len(temp)):
                                 item = temp[i]
                                 probability = uncertain[i]
                                 product = _Item(item, probability)
                                 tr.append(product)
-                            self._lno += 1
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
+                    
+    def _creatingNeighbours(self):
+        """
+        Scans the uncertain transactional dataset
+        """
+        global _neighbourList
+        _neighbourList = {}
+        if isinstance(self._nFile, _ab._pd.DataFrame):
+            uncertain, data = [], []
+            if self._iFile.empty:
+                print("its empty..")
+            i = self._iFile.columns.values.tolist()
+            if 'Transactions' in i:
+                self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
+                self._Database.append(tr)
+
+            # print(self.Database)
+        if isinstance(self._nFile, str):
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    tr = []
+                    for i in temp:
+                        i1 = i.index('(')
+                        i2 = i.index(')')
+                        item = i[0:i1]
+                        probability = float(i[i1 + 1:i2])
+                        product = _Item(item, probability)
+                        tr.append(product)
+                    self._Database.append(temp)
+            else:
+                try:
+                    with open(self._nFile, 'r') as f:
+                        for line in f:
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            _neighbourList[temp[0]] = temp[1:]
+                except IOError:
+                    print("File Not Found")
 
     def _frequentOneItem(self):
         """
-        Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+
+        :param self.Database : it represents the one self.Database in database
+        :type self.Database : list
         """
-        global _minSup
+
         mapSupport = {}
         for i in self._Database:
             for j in i:
                 if j.item not in mapSupport:
-                    mapSupport[j.item] = round(j.probability, 2)
+                    mapSupport[j.item] = j.probability
                 else:
-                    mapSupport[j.item] += round(j.probability, 2)
-        mapSupport = {k: round(v, 2) for k, v in mapSupport.items() if v >= self._minSup}
+                    mapSupport[j.item] += j.probability
+        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
         plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
+        self.rank = dict([(index, item) for (item, index) in enumerate(plist)])
         return mapSupport, plist
 
-    def _buildTree(self, data, info):
+    @staticmethod
+    def _buildTree(data, info):
         """
-        it takes the transactions and support of each item and construct the main tree with setting root node as null
+        It takes the self.Database and support of each item and construct the main tree with setting root node as null
 
-        :param data : it represents the one transactions in database
+        :param data : it represents the one self.Database in database
         :type data : list
         :param info : it represents the support of each item
         :type info : dictionary
         """
+
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
             rootNode.addTransaction(data[i])
         return rootNode
 
-    def updateTransactions(self, dict1):
+    def _updateTransactions(self, dict1):
         """
-        remove the items which are not frequent from transactions and updates the transactions with rank of items
+        Remove the items which are not frequent from self.Database and updates the self.Database with rank of items
 
         :param dict1 : frequent items with support
         :type dict1 : dictionary
         """
+
         list1 = []
         for tr in self._Database:
             list2 = []
             for i in range(0, len(tr)):
                 if tr[i].item in dict1:
                     list2.append(tr[i])
-            if (len(list2) >= 2):
+            if len(list2) >= 2:
                 basket = list2
-                basket.sort(key=lambda val: self._rank[val.item])
+                basket.sort(key=lambda val: self.rank[val.item])
                 list2 = basket
                 list1.append(list2)
         return list1
 
-    def _Check(self, i, x):
+    @staticmethod
+    def _check(i, x):
         """
         To check the presence of item or pattern in transaction
 
         :param x: it represents the pattern
         :type x : list
-        :param i : represents the uncertain transactions
+        :param i : represents the uncertain self.Database
         :type i : list
         """
+
+        # This method taken a transaction as input and returns the tree
         for m in x:
             k = 0
             for n in i:
                 if m == n.item:
                     k += 1
             if k == 0:
                 return 0
@@ -625,27 +662,27 @@
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
     def _removeFalsePositives(self):
         """
-        To remove the false positive patterns generated in frequent patterns
+        To remove the false positive patterns generated in frequent patterns.
 
         :return: patterns with accurate probability
         """
         global _finalPatterns
         periods = {}
         for i in self._Database:
             for x, y in _finalPatterns.items():
                 if len(x) == 1:
                     periods[x] = y
                 else:
                     s = 1
-                    check = self._Check(i, x)
+                    check = self._check(i, x)
                     if check == 1:
                         for j in i:
                             if j.item in x:
                                 s *= j.probability
                         if x in periods:
                             periods[x] += s
                         else:
@@ -653,37 +690,65 @@
         for x, y in periods.items():
             if y >= self._minSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        global _minSup
-        self._startTime = _fp._time.time()
+        global minSup
+        global minSup
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._creatingNeighbours()
+        #self._minSup = self._convert(self._minSup)
+        minSup = self._minSup
+        self._finalPatterns = {}
+        mapSupport, plist = self._frequentOneItem()
+        self.Database1 = self._updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        Tree1 = self._buildTree(self.Database1, info)
+        Tree1.generatePatterns([])
+        self._removeFalsePositives()
+        print("Geo-Referenced Frequent patterns were generated from uncertain databases successfully using GFP algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self.memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self.memoryRSS = process.memory_info().rss
+
+    def Mine(self):
+        """
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        """
+        global minSup
+        self._startTime = _ab._time.time()
         self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        _minSup = self._minSup
+        self._creatingNeighbours()
+        # self._minSup = self._convert(self._minSup)
+        minSup = self._minSup
         self._finalPatterns = {}
         mapSupport, plist = self._frequentOneItem()
-        transactions1 = self.updateTransactions(mapSupport)
+        self.Database1 = self._updateTransactions(mapSupport)
         info = {k: v for k, v in mapSupport.items()}
-        Tree1 = self._buildTree(transactions1, info)
+        Tree1 = self._buildTree(self.Database1, info)
         Tree1.generatePatterns([])
         self._removeFalsePositives()
-        print("Uncertain Frequent patterns were generated successfully using TubeS algorithm")
-        self._endTime = _fp._time.time()
-        process = _fp._psutil.Process(_fp._os.getpid())
+        print("Geo-Referenced Frequent patterns were generated from uncertain databases successfully using GFP algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
-        self._memoryRSS = float()
+        self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
@@ -695,15 +760,15 @@
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
+        return self.memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
@@ -718,58 +783,63 @@
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
+            data.append([a, b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
+            s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
-        return len(self._finalPatterns)
-
+        return self._finalPatterns
+    
     def printResults(self):
-        """
-        This function is used to print the results
-        """
-        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total number of Patterns:", len(self.getPatterns()))
+        self.save("patterns.txt")
+        memUSS = self.getMemoryUSS()
+        print("Total Memory in USS:", memUSS)
+        memRSS = self.getMemoryRSS()
+        print("Total Memory in RSS", memRSS)
+        run = self.getRuntime()
+        print("Total ExecutionTime in ms:", run)
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_fp._sys.argv) == 4 or len(_fp._sys.argv) == 5:
-        if len(_fp._sys.argv) == 5:
-            _ap = TubeS(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4])
-        if len(_fp._sys.argv) == 4:
-            _ap = TubeS(_fp._sys.argv[1], _fp._sys.argv[3])
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = GFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:
+            _ap = GFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        print("Total number of Uncertain Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_fp._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _Patterns = _ap.getPatterns()
+        print("Total number of Patterns:", len(_Patterns))
+        _ap.save(_ab._sys.argv[2])
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/UFGrowth.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/UFGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -28,71 +28,62 @@
 #
 #     run = obj.getRuntime()
 #
 #     print("Total ExecutionTime in seconds:", run)
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
-
 """
 
 from PAMI.uncertainFrequentPattern.basic import abstract as _ab
 
 _minSup = str()
 _ab._sys.setrecursionlimit(20000)
 _finalPatterns = {}
 
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
-
     :Attributes:
-
         item : int or word
             Represents the name of the item
         probability : float
             Represent the existential probability(likelihood presence) of an item
     """
 
     def __init__(self, item, probability):
         self.item = item
         self.probability = probability
 
 
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
-
     :Attributes:
-
         item : int
             storing item of a node
         probability : int
             To maintain the expected support of node
         parent : node
             To maintain the parent of every node
         children : list
             To maintain the children of node
     :Methods:
-
         addChild(itemName)
             storing the children to their respective parent nodes
     """
 
     def __init__(self):
         self.itemId = -1
         self.counter = 0
@@ -108,25 +99,22 @@
                 return i
         return None
 
 
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
-
     :Attributes:
-
         root : Node
             Represents the root node of the tree
         summaries : dictionary
             storing the nodes with same item name
         info : dictionary
             stores the support of items
     :Methods:
-
         addTransaction(transaction)
             creating transaction as a branch in frequentPatternTree
         addConditionalPattern(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
         conditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
         conditionalTransactions(prefixPaths,Support)
@@ -247,21 +235,18 @@
                         self.fixNodeLinks(pathItem.itemid, newNode)
         return q
 
 
 class UFGrowth(_ab._frequentPatterns):
     """
     :Description: It is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using PUF-Tree.
-
     :Reference:
         Carson Kai-Sang Leung, Syed Khairuzzaman Tanbeer, "PUF-Tree: A Compact Tree Structure for Frequent Pattern Mining of Uncertain Data",
         Pacific-Asia Conference on Knowledge Discovery and Data Mining(PAKDD 2013), https://link.springer.com/chapter/10.1007/978-3-642-37453-1_2
-
     :Attributes:
-
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
         minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
@@ -287,15 +272,14 @@
         tree : class
             To represents the Tree class
         itemSetCount : int
             To represents the total no of patterns
         finalPatterns : dict
             To store the complete patterns
     :Methods:
-
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
@@ -314,59 +298,40 @@
             Update the transactions by removing non-frequent items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
         startMine()
             Mining process will start from this function
-
     **Methods to execute code on terminal**
     ----------------------------------------
             Format:
                       >>>  python3 PUFGrowth.py <inputFile> <outputFile> <minSup>
-
             Example:
                       >>>  python3 PUFGrowth.py sampleTDB.txt patterns.txt 3
-
                       .. note:: minSup  will be considered in support count or frequency
-
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
-
             from PAMI.uncertainFrequentPattern.basic import UFGrowth as alg
-
             obj = alg.UFGrowth(iFile, minSup)
-
             obj.startMine()
-
             frequentPatterns = obj.getPatterns()
-
             print("Total number of Frequent Patterns:", len(frequentPatterns))
-
             obj.save(oFile)
-
             Df = obj.getPatternsAsDataFrame()
-
             memUSS = obj.getmemoryUSS()
-
             print("Total Memory in USS:", memUSS)
-
             memRSS = obj.getMemoryRSS()
-
             print("Total Memory in RSS", memRSS)
-
             run = obj.getRuntime()
-
             print("Total ExecutionTime in seconds:", run)
-
     **Credits:**
     -----------------
              The complete program was written by P.Likhitha under the supervision of Professor Rage Uday Kiran.
-
     """
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
@@ -445,15 +410,14 @@
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
 
     def _frequentOneItem(self):
         """
         takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
-
         :param self.Database : it represents the one self.Database in database
         :type self.Database : list
         """
 
         mapSupport = {}
         for i in self._Database:
             for j in i:
@@ -545,15 +509,14 @@
                     newprefixLength += 1
                     support = TempBuffer[j].counter
             self._saveItemset(prefix, newprefixLength, s)
 
     def _convert(self, value):
         """
         To convert the type of user specified minSup value
-
         :param value: user specified minSup value
         :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
@@ -594,73 +557,67 @@
         self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self.memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self.memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
-
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
-
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
         """
```

### Comparing `pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/UVECLAT.py` & `pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/WFRIMiner.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,38 +1,40 @@
-# UVEclat is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
+# WFRIMiner is one of the fundamental algorithm to discover weighted frequent regular patterns in a transactional database.
+# It stores the database in compressed WFRI-tree decreasing the memory usage and extracts the patterns from tree.It employs downward closure property to  reduce the search space effectively.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
+#             from PAMI.weightedFrequentRegularPattern.basic import WFRIMiner as alg
 #
-#     obj = alg.UVEclat(iFile, minSup)
+#             obj = alg.WFRIMiner(iFile, WS, regularity)
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     frequentPatterns = obj.getPatterns()
+#             weightedFrequentRegularPatterns = obj.getPatterns()
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of Frequent Patterns:", len(weightedFrequentRegularPatterns))
 #
-#     obj.save(oFile)
+#             obj.save(oFile)
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternInDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
 #
-#     print("Total ExecutionTime in seconds:", run)
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
@@ -46,504 +48,772 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-import operator as _operator
-from PAMI.uncertainFrequentPattern.basic import abstract as _ab
+from PAMI.weightedFrequentRegularPattern.basic import abstract as _fp
+import pandas as pd
+from deprecated import deprecated
+from typing import List, Dict
 
 
-_minSup = float()
-_finalPatterns = {}
+_WS = str()
+_regularity = str()
+_lno = int()
+_weights = {}
+_wf = {}
+_fp._sys.setrecursionlimit(20000)
 
 
-class _Item:
+class _Node:
     """
-    A class used to represent the item with probability in transaction of dataset
+    A class used to represent the node of frequentPatternTree
 
     :Attributes:
+        itemId: int
+            storing item of a node
+        counter: int
+            To maintain the support of node
+        parent: node
+            To maintain the parent of node
+        children: list
+            To maintain the children of node
+
+    :Methods:
+        addChild(node)
+            Updates the nodes children list and parent for the given node
 
-        item : int or word
-            Represents the name of the item
-        probability : float
-            Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self, item, probability):
+    def __init__(self, item: int, children: dict) -> None:
+        """
+        Initializing the Node class
+
+        :param item: Storing the item of a node
+        :type item: int or None
+        :param children: To maintain the children of a node
+        :type children: dict
+        :return: None
+        """
+
         self.item = item
-        self.probability = probability
+        self.children = children
+        self.parent = None
+        self.timeStamps = []
+
+    def addChild(self, node) -> None:
+        """
+        To add the children to a node
+
+        :param node: parent node in the tree
+        :return: None
+        """
+
+        self.children[node.item] = node
+        node.parent = self
 
 
-class UVEclat(_ab._frequentPatterns):
+class _Tree:
     """
-    :Description: It is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
+    A class used to represent the frequentPatternGrowth tree structure
 
-    :Reference:
+    :Attributes:
+        root : Node
+            The first node of the tree set to Null.
+        summaries : dictionary
+            Stores the nodes itemId which shares same itemId
+        info : dictionary
+            frequency of items in the transactions
+
+    :Methods:
+        addTransaction(transaction, freq)
+            adding items of  transactions into the tree as nodes and freq is the count of nodes
+        getFinalConditionalPatterns(node)
+            getting the conditional patterns from fp-tree for a node
+        getConditionalPatterns(patterns, frequencies)
+            sort the patterns by removing the items with lower minSup
+        generatePatterns(prefix)
+            generating the patterns from fp-tree
+    """
+
+    def __init__(self) -> None:
+        self.root = _Node(None, {})
+        self.summaries = {}
+        self.info = {}
+
+    def addTransaction(self, transaction: list, tid: list) -> None:
+        """
+        Adding a transaction into tree
+
+        :param transaction: To represent the complete database
+        :type transaction: list
+        :param tid: To represent the timestamp of a database
+        :type tid: list
+        :return: pfp-growth tree
+        """
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i] not in currentNode.children:
+                newNode = _Node(transaction[i], {})
+                currentNode.addChild(newNode)
+                if transaction[i] in self.summaries:
+                    self.summaries[transaction[i]].append(newNode)
+                else:
+                    self.summaries[transaction[i]] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i]]
+        currentNode.timeStamps = currentNode.timeStamps + tid
+
+    def getConditionalPatterns(self, alpha, pattern) -> tuple:
+        """
+        Generates all the conditional patterns of a respective node
+
+        :param alpha: To represent a Node in the tree
+        :type alpha: Node
+        :param pattern: prefix of the pattern
+        :type alpha: list
+        :return: A tuple consisting of finalPatterns, conditional pattern base and information
+        """
+        finalPatterns = []
+        finalSets = []
+        for i in self.summaries[alpha]:
+            set1 = i.timeStamps
+            set2 = []
+            while i.parent.item is not None:
+                set2.append(i.parent.item)
+                i = i.parent
+            if len(set2) > 0:
+                set2.reverse()
+                finalPatterns.append(set2)
+                finalSets.append(set1)
+        finalPatterns, finalSets, info = self.conditionalDatabases(finalPatterns, finalSets, pattern)
+        return finalPatterns, finalSets, info
+
+    @staticmethod
+    def generateTimeStamps(node) -> list:
+        """
+        To get the timestamps of a node
+
+        :param node: A node in the tree
+        :return: Timestamps of a node
+        """
 
-    Carson Kai-Sang Leung, Lijing Sun: "Equivalence class transformation based mining of frequent itemsets from uncertain data",
-    SAC '11: Proceedings of the 2011 ACM Symposium on Applied ComputingMarch, 2011, Pages 983–984,
-    https://doi.org/10.1145/1982185.1982399
+        finalTimeStamps = node.timeStamps
+        return finalTimeStamps
+
+    def removeNode(self, nodeValue) -> None:
+        """
+        Removing the node from tree
+
+        :param nodeValue: To represent a node in the tree
+        :type nodeValue: node
+        :return: Tree with their nodes updated with timestamps
+        """
+
+        for i in self.summaries[nodeValue]:
+            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
+            del i.parent.children[nodeValue]
+
+    def getTimeStamps(self, alpha) -> list:
+        """
+        To get all the timestamps of the nodes which share same item name
+
+        :param alpha: Node in a tree
+        :return: Timestamps of a  node
+        """
+        temporary = []
+        for i in self.summaries[alpha]:
+            temporary += i.timeStamps
+        return temporary
+
+    @staticmethod
+    def getSupportAndPeriod(timeStamps: list, pattern: list) -> list:
+        """
+        To calculate the periodicity and support
+
+        :param timeStamps: Timestamps of an item set
+        :type timeStamps: list
+        :param pattern: pattern to evaluate the weighted frequent regular or not
+        :type pattern: list
+        :return: support, periodicity
+        """
+        global _WS, _regularity, _lno, _weights
+        timeStamps.sort()
+        cur = 0
+        per = list()
+        sup = 0
+        for j in range(len(timeStamps)):
+            per.append(timeStamps[j] - cur)
+            cur = timeStamps[j]
+            sup += 1
+        per.append(_lno - cur)
+        l = int()
+        for i in pattern:
+            l = l + _weights[i]
+        wf = (l / (len(pattern))) * sup
+        if len(per) == 0:
+            return [0, 0]
+        return [sup, max(per), wf]
+
+    def conditionalDatabases(self, conditionalPatterns: list, conditionalTimeStamps: list, pattern: list) -> tuple:
+        """
+        It generates the conditional patterns with periodic-frequent items
+
+        :param conditionalPatterns: conditionalPatterns generated from conditionPattern method of a respective node
+        :type conditionalPatterns: list
+        :param conditionalTimeStamps: Represents the timestamps of a conditional patterns of a node
+        :type conditionalTimeStamps: list
+        :param pattern: prefix of the pattern
+        :type pattern: list
+        :returns: Returns conditional transactions by removing non-periodic and non-frequent items
+        """
+        global _WS, _regularity
+        pat = []
+        timeStamps = []
+        data1 = {}
+        for i in range(len(conditionalPatterns)):
+            for j in conditionalPatterns[i]:
+                if j in data1:
+                    data1[j] = data1[j] + conditionalTimeStamps[i]
+                else:
+                    data1[j] = conditionalTimeStamps[i]
+        updatedDictionary = {}
+        for m in data1:
+            updatedDictionary[m] = self.getSupportAndPeriod(data1[m], pattern + [m])
+        updatedDictionary = {k: v for k, v in updatedDictionary.items() if v[0] >= _WS and v[1] <= _regularity}
+        count = 0
+        for p in conditionalPatterns:
+            p1 = [v for v in p if v in updatedDictionary]
+            trans = sorted(p1, key=lambda x: (updatedDictionary.get(x)[0], -x), reverse=True)
+            if len(trans) > 0:
+                pat.append(trans)
+                timeStamps.append(conditionalTimeStamps[count])
+            count += 1
+        return pat, timeStamps, updatedDictionary
+
+    def generatePatterns(self, prefix: list) -> None:
+        """
+        Generates the patterns
+
+        :param prefix: Forms the combination of items
+        :type prefix: list
+        :returns: yields patterns with their support and periodicity
+        """
+        global _WS
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0], -x)):
+            pattern = prefix[:]
+            pattern.append(i)
+            if self.info[i][2] >= _WS:
+                yield pattern, self.info[i]
+                patterns, timeStamps, info = self.getConditionalPatterns(i, pattern)
+                conditionalTree = _Tree()
+                conditionalTree.info = info.copy()
+                for pat in range(len(patterns)):
+                    conditionalTree.addTransaction(patterns[pat], timeStamps[pat])
+                if len(patterns) > 0:
+                    for q in conditionalTree.generatePatterns(pattern):
+                        yield q
+            self.removeNode(i)
+
+
+class WFRIMiner(_fp._weightedFrequentRegularPatterns):
+    """
+    :Description: WFRIMiner is one of the fundamental algorithm to discover weighted frequent regular patterns in a transactional database.
+       * It stores the database in compressed WFRI-tree decreasing the memory usage and extracts the patterns from tree.It employs downward closure property to  reduce the search space effectively.
+
+    :Reference:
+           K. Klangwisan and K. Amphawan, "Mining weighted-frequent-regular itemsets from transactional database,"
+           2017 9th International Conference on Knowledge and Smart Technology (KST), 2017, pp. 66-71,
+           doi: 10.1109/KST.2017.7886090.
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Weighted Frequent Regular Patterns.
+    :param  oFile: str :
+                   Name of the output file to store complete set of Weighted Frequent Regular Patterns.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  wFile: str :
+                This is a weighted file.
 
     :Attributes:
 
         iFile : file
-            Name of the Input file or path of the input file
-        oFile : file
-            Name of the output file or path of the output file
-        minSup : float or int or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Input file name or path of the input file
+        WS: float or int or str
+            The user can specify WS either in count or proportion of database size.
+            If the program detects the data type of WS is integer, then it treats WS is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: WS=10 will be treated as integer, while WS=10.0 will be treated as float
+        regularity: float or int or str
+            The user can specify regularity either in count or proportion of database size.
+            If the program detects the data type of regularity is integer, then it treats regularity is expressed in count.
             Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+            Example: regularity=10 will be treated as integer, while regularity=10.0 will be treated as float
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator.
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+        oFile : file
+            Name of the output file or the path of the output file
         startTime:float
             To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
         lno : int
-            To represent the total no of transaction
+            it represents the total no of transactions
         tree : class
-            To represents the Tree class
-        itemSetCount : int
-            To represents the total no of patterns
+            it represents the Tree class
         finalPatterns : dict
-            To store the complete patterns
+            it represents to store the patterns
+
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        storePatternsInFile(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
-        getPatternsInDataFrame()
+        save(oFile)
+            Complete set of frequent patterns will be loaded in to an output file
+        getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets(fileName)
-            Scans the dataset and stores in a list format
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
         frequentOneItem()
-            Extracts the one-length frequent patterns from database
-
+            Extracts the one-frequent patterns from transactions
 
     **Methods to execute code on terminal**
-    ------------------------------------------
-            Format:
-                      >>> python3 uveclat.py <inputFile> <outputFile> <minSup>
+    -------------------------------------------
+    .. code-block:: console
+
+
+      Format:
+
+      (.venv) $ python3 WFRIMiner.py <inputFile> <outputFile> <weightSupport> <regularity>
+
+      Example Usage:
+
+      (.venv) $ python3 WFRIMiner.py sampleDB.txt patterns.txt 10 5
+
 
-            Example:
-                      >>>  python3 uveclat.py sampleTDB.txt patterns.txt 3
+              .. note:: WS & regularity will be considered in support count or frequency
 
-                      .. note:: minSup  will be considered in support count or frequency
 
     **Importing this algorithm into a python program**
-    ---------------------------------------------------
+    ----------------------------------------------------
     .. code-block:: python
 
-            from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
-
-            obj = alg.UVEclat(iFile, minSup)
+            from PAMI.weightedFrequentRegularpattern.basic import WFRIMiner as alg
 
+            obj = alg.WFRIMiner(iFile, WS, regularity)
 
             obj.startMine()
 
-            frequentPatterns = obj.getPatterns()
+            weightedFrequentRegularPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            print("Total number of Frequent Patterns:", len(weightedFrequentRegularPatterns))
 
             obj.save(oFile)
 
-            Df = obj.getPatternsAsDataFrame()
+            Df = obj.getPatternInDataFrame()
 
-            memUSS = obj.getmemoryUSS()
+            memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    ---------------
-         The complete program was written by   P.Likhitha    under the supervision of Professor Rage Uday Kiran.
+    ----------------
+             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+
+        """
 
-    """
     _startTime = float()
     _endTime = float()
-    _minSup = str()
+    _WS = str()
+    _regularity = str()
+    _weight = {}
     _finalPatterns = {}
+    _wFile = " "
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _tidList = {}
+    _mapSupport = {}
+    _lno = 0
+    _tree = _Tree()
     _rank = {}
+    _rankDup = {}
 
-    def _creatingItemSets(self):
+    def __init__(self, iFile, _wFile, WS, regularity, sep='\t') -> None:
+        super().__init__(iFile, _wFile, WS, regularity, sep)
+
+    def _creatingItemSets(self) -> None:
         """
-        Scans the dataset
+        Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data = [], []
+        self._weight = {}
+        if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 self._Database = self._iFile['Transactions'].tolist()
-            if 'uncertain' in i:
-                uncertain = self._iFile['uncertain'].tolist()
-            for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
-                    product = _Item(data[k][j], uncertain[k][j])
-                    tr.append(product)
-                self._Database.append(tr)
+
+        if isinstance(self._wFile, _fp._pd.DataFrame):
+            _items, _weights = [], []
+            if self._wFile.empty:
+                print("its empty..")
+            i = self._wFile.columns.values.tolist()
+            if 'items' in i:
+                _items = self._wFile['items'].tolist()
+            if 'weight' in i:
+                _weights = self._wFile['weight'].tolist()
+            for i in range(len(_items)):
+                self._weight[_items[i]] = _weights[i]
 
             # print(self.Database)
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if _fp._validators.url(self._iFile):
+                data = _fp._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    tr = []
-                    for i in temp:
-                        i1 = i.index('(')
-                        i2 = i.index(')')
-                        item = i[0:i1]
-                        probability = float(i[i1 + 1:i2])
-                        product = _Item(item, probability)
-                        tr.append(product)
                     self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r') as f:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
+                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            tr = []
-                            for i in temp:
-                                i1 = i.index('(')
-                                i2 = i.index(')')
-                                item = i[0:i1]
-                                probability = float(i[i1 + 1:i2])
-                                product = _Item(item, probability)
-                                tr.append(product)
-                            self._Database.append(tr)
+                            self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
+                    quit()
 
-    def _frequentOneItem(self):
-        """
-        takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
-        """
-
-        mapSupport = {}
-        k = 0
-        for i in self._Database:
-            k += 1
-            for j in i:
-                if j.item not in mapSupport:
-                    mapSupport[str(j.item)] = j.probability
-                    self._tidList[str(j.item)] = {k: j.probability}
-                else:
-                    mapSupport[str(j.item)] += j.probability
-                    self._tidList[str(j.item)].update({k: j.probability})
-        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
-        plist = dict( sorted(mapSupport.items(), key=_operator.itemgetter(1),reverse=True))
-        return list(plist.keys())
-
-    @staticmethod
-    def _check(i, x):
-        """
-        To check the presence of item or pattern in transaction
-
-        :param x: it represents the pattern
-        :type x : list
-        :param i : represents the uncertain self.Database
-        :type i : list
-        """
-
-        # This method taken a transaction as input and returns the tree
-        for m in x:
-            k = 0
-            for n in i:
-                if m == n.item:
-                    k += 1
-            if k == 0:
-                return 0
-        return 1
+        if isinstance(self._wFile, str):
+            if _fp._validators.url(self._wFile):
+                data = _fp._urlopen(self._wFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._weight[temp[0]] = float(temp[1])
+            else:
+                try:
+                    with open(self._wFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._weight[temp[0]] = float(temp[1])
+                except IOError:
+                    print("File Not Found")
+                    quit()
 
-    @staticmethod
-    def _convert(value):
+    def _convert(self, value) -> float:
         """
         To convert the type of user specified minSup value
 
         :param value: user specified minSup value
-        :return: converted type minSup value
+        :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = float(value)
+            value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _removeFalsePositives(self):
+    def _frequentOneItem(self) -> List[str]:
         """
-        To remove the false positive patterns generated in frequent patterns
-
-        :return: patterns with accurate probability
+        Generating One frequent items sets
+        :return: list
         """
-        global _finalPatterns
-        periods = {}
-        for i in self._Database:
-            for x, y in _finalPatterns.items():
-                if len(x) == 1:
-                    periods[x] = y
+        global _lno, _wf, _weights
+        self._mapSupport = {}
+        _owf = {}
+        for tr in self._Database:
+            for i in range(1, len(tr)):
+                if tr[i] not in self._mapSupport:
+                    self._mapSupport[tr[i]] = [int(tr[0]), int(tr[0]), 1]
                 else:
-                    s = 1
-                    check = self._check(i, x)
-                    if check == 1:
-                        for j in i:
-                            if j.item in x:
-                                s *= j.probability
-                        if x in periods:
-                            periods[x] += s
-                        else:
-                            periods[x] = s
-        for x, y in periods.items():
-            if y >= self._minSup:
-                sample = str()
-                for i in x:
-                    sample = sample + i + "\t"
-                self._finalPatterns[sample] = y
+                    self._mapSupport[tr[i]][0] = max(self._mapSupport[tr[i]][0], (int(tr[0]) - self._mapSupport[tr[i]][1]))
+                    self._mapSupport[tr[i]][1] = int(tr[0])
+                    self._mapSupport[tr[i]][2] += 1
+        for key in self._mapSupport:
+            self._mapSupport[key][0] = max(self._mapSupport[key][0], abs(len(self._Database) - self._mapSupport[key][1]))
+        _lno = len(self._Database)
+        self._mapSupport = {k: [v[2], v[0]] for k, v in self._mapSupport.items() if v[0] <= self._regularity}
+        for x, y in self._mapSupport.items():
+            if self._weight.get(x) is None:
+                self._weight[x] = 0
+        gmax = max([self._weight[values] for values in self._mapSupport.keys()])
+        for x, y in self._mapSupport.items():
+            _owf[x] = y[0] * gmax
+        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v[0] * _owf[k] >= self._WS}
+        for x, y in self._mapSupport.items():
+            temp = self._weight[x] * y[0]
+            _wf[x] = temp
+            self._mapSupport[x].append(temp)
+        genList = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse= True)]
+        self._rank = dict([(index, item) for (item, index) in enumerate(genList)])
+        for x, y in self._rank.items():
+            _weights[y] = self._weight[x]
+        return genList
+
+    def _updateTransactions(self, itemSet) -> List[List[int]]:
+        """
+        Updates the items in transactions with rank of items according to their support
+
+        :Example:
+        oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
+        rank = {'a':0, 'b':1, 'c':2, 'd':3}
+
+        :param itemSet: list of one-frequent items
+        :return: None
+        """
+        list1 = []
+        for tr in self._Database:
+            list2 = [int(tr[0])]
+            for i in range(1, len(tr)):
+                if tr[i] in itemSet:
+                    list2.append(self._rank[tr[i]])
+            if len(list2) >= 2:
+                basket = list2[1:]
+                basket.sort()
+                list2[1:] = basket[0:]
+                list1.append(list2)
+        return list1
 
     @staticmethod
-    def _Intersection(tidSetx, tidSetY):
+    def _buildTree(transactions, info) -> _Tree:
         """
-        This function is used to find the intersection
+        Builds the tree with updated transactions
+
+        :param transactions: updated transactions
+        :param info: support details of each item in transactions
+        :return: transactions compressed in fp-tree
 
-        :param tidSetx: the timestamp of a patterns
-        :type tidSetx: dict
-        :param tidSetY: the timestamp of a patterns
-        :type tidSetY: dict
         """
-        tids = []
-        support = []
-        tidDict = {}
-        for x, y in tidSetx.items():
-            for x1, y1 in tidSetY.items():
-                if x == x1:
-                    tids.append(x)
-                    support.append(y * y1)
-                    tidDict.update({x: y * y1})
-        return tidDict
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(transactions)):
+            set1 = [transactions[i][0]]
+            rootNode.addTransaction(transactions[i][1:], set1)
+        return rootNode
 
-    def _calculateExpSup(self, tidList):
+    def _savePeriodic(self, itemSet) -> str:
         """
-        This function is used to calculate support of tidList
+        The duplication items and their ranks
 
-        :param tidList: timestamp of a list.
-        :type tidList: List
+        :param itemSet: frequent itemSet that generated
+        :return: patterns with original item names.
 
         """
-        return sum(tidList.values())
+        temp = str()
+        for i in itemSet:
+            temp = temp + self._rankDup[i] + "\t"
+        return temp
 
-    def _save(self, prefix, suffix, tidSetI):
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self) -> None:
         """
-        Saves the patterns that satisfy the periodic frequent property.
-
-        :param prefix: the prefix of a pattern
-        :type prefix: list
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetI: the timestamp of a patterns
-        :type tidSetI: dict
-        """
-
-        global _finalPatterns
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = self._calculateExpSup(tidSetI)
-        _finalPatterns[tuple(prefix)] = val
-
-    def _Generation(self, prefix, itemSets, tidSets):
-        """
-        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-
-        :param prefix:  main equivalence prefix
-        :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
-        :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
-        :type tidSets: list
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
-                continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i + 1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = self._Intersection(tidSetI, tidSetJ)
-                if self._calculateExpSup(y) >= self._minSup:
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetI)
+        main program to start the operation
+        :return: None
+        """
+        global _WS, _regularity, _weights
+        self._startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._WS is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._WS = self._convert(self._WS)
+        self._regularity = self._convert(self._regularity)
+        _WS, _regularity, _weights = self._WS, self._regularity, self._weight
+        itemSet = self._frequentOneItem()
+        updatedTransactions = self._updateTransactions(itemSet)
+        for x, y in self._rank.items():
+            self._rankDup[y] = x
+        info = {self._rank[k]: v for k, v in self._mapSupport.items()}
+        _Tree = self._buildTree(updatedTransactions, info)
+        patterns = _Tree.generatePatterns([])
+        self._finalPatterns = {}
+        for k in patterns:
+            s = self._savePeriodic(k[0])
+            self._finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent Regular patterns were generated successfully using WFRIM algorithm")
+        self._endTime = _fp._time.time()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
-    def startMine(self):
+    def Mine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        main program to start the operation
+        :return: None
         """
-        global _minSup
-        self._startTime = _ab._time.time()
+        global _WS, _regularity, _weights
+        self._startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._WS is None:
+            raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        _minSup = self._minSup
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i+1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = self._Intersection(tidSetI, tidSetJ)
-                if self._calculateExpSup(y1) >= self._minSup:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-            self._save(None, itemSetX, tidSetI)
-        self._removeFalsePositives()
-        print("Frequent patterns were generated from uncertain databases successfully using PUF algorithm")
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
+        self._WS = self._convert(self._WS)
+        self._regularity = self._convert(self._regularity)
+        _WS, _regularity, _weights = self._WS, self._regularity, self._weight
+        itemSet = self._frequentOneItem()
+        updatedTransactions = self._updateTransactions(itemSet)
+        for x, y in self._rank.items():
+            self._rankDup[y] = x
+        info = {self._rank[k]: v for k, v in self._mapSupport.items()}
+        _Tree = self._buildTree(updatedTransactions, info)
+        patterns = _Tree.generatePatterns([])
+        self._finalPatterns = {}
+        for k in patterns:
+            s = self._savePeriodic(k[0])
+            self._finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent Regular patterns were generated successfully using WFRIM algorithm")
+        self._endTime = _fp._time.time()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
         self._memoryRSS = float()
         self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+    def getMemoryUSS(self) -> float:
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+    def getMemoryRSS(self) -> float:
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+    def getRuntime(self) -> float:
+        """
+        Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
+    def getPatternsAsDataFrame(self) -> _fp._pd.DataFrame:
+        """
+        Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
-    def save(self, oFile):
-        """Complete set of frequent patterns will be loaded in to an output file
-        :param oFile: name of the output file
-        :type oFile: csv file
+    def save(self, outFile: str) -> None:
         """
-        self.oFile = oFile
-        writer = open(self.oFile, 'w+')
+        Complete set of frequent patterns will be loaded in to an output file
+
+        :param outFile: name of the output file
+        :type outFile: csv file
+        :return: None
+        """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
+    def getPatterns(self) -> Dict[str, float]:
+        """
+        Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self):
+    def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
-        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Weighted Frequent Regular Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-        if len(_ab._sys.argv) == 5:
-            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3])
+    if len(_fp._sys.argv) == 6 or len(_fp._sys.argv) == 7:
+        if len(_fp._sys.argv) == 7:
+            _ap = WFRIMiner(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
+        if len(_fp._sys.argv) == 5:
+            _ap = WFRIMiner(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5])
         _ap.startMine()
-        print("Total number of Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total number of Weighted Frequent Regular Patterns:", len(_ap.getPatterns()))
+        _ap.save(_fp._sys.argv[2])
+        print("Total Memory in USS:",  _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.3.9.2/PAMI/uncertainFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,67 +17,67 @@
     """
     :Description: This abstract base class defines the variables and methods that every frequent pattern mining algorithm must employ in PAMI
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        minSup : float or int or str
+        minSup: float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        startTime :float
+        startTime:float
             To record the start time of the algorithm
-        endTime : float
+        endTime:float
             To record the completion time of the algorithm
-        finalPatterns : dict
+        finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
+        savePatterns(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to data frame
         getMemoryUSS()
             Total amount of USS memory consumed by the program will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the program will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the program will be retrieved from this function
     """
 
-    def __init__(self, iFile, minSup, sep = '\t'):
+    def __init__(self, iFile, nFile, minSup, sep = '\t'):
         """
         :param iFile: Input file name or path of the input file
         :type iFile: str
         :param minSup: The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         :type minSup: int or float or str
         :param sep: separator used to distinguish items from each other. The default separator is tab space. However, users can override the default separator
         :type sep: str
         """
-
         self._iFile = iFile
+        self._nFile = nFile
         self._minSup = minSup
         self._sep = sep
         self._oFile = " "
         self._finalPatterns = {}
         self._startTime = float()
         self._endTime = float()
         self._memoryUSS = float()
@@ -93,15 +93,16 @@
     def getPatterns(self):
         """Complete set of patterns generated will be retrieved from this function"""
 
         pass
 
     @_abstractmethod
     def save(self, oFile):
-        """Complete set of frequent patterns will be saved in to an output file from this function
+        """
+        Complete set of frequent patterns will be saved in to an output file from this function
 
         :param oFile: Name of the output file
         :type oFile: csv file
         """
 
         pass
 
@@ -123,16 +124,13 @@
         pass
 
     @_abstractmethod
     def getRuntime(self):
         """Total amount of runtime taken by the program will be retrieved from this function"""
 
         pass
-
+      
     @_abstractmethod
     def printResults(self):
-        """ To print all the results of execution"""
+        """To print all the stats"""
 
         pass
-
-
-
```

### Comparing `pami-2024.3.9.2/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py` & `pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# GFPGrowth algorithm is used to discover geo-referenced frequent patterns in a uncertain transactional database using GFP-Tree.
+# UPFPGrowth is used to discover periodic-frequent patterns in an uncertain temporal database.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#             from PAMI.uncertainGeoreferencedFrequentPattern.basic import GFPGrowth as alg
 #
-#             obj = alg.GFPGrowth(iFile, nFile, minSup)
+#             from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowth as alg
 #
-#             obj.startMine()
+#             obj = alg.UPFPGrowth(iFile, minSup, maxPer)
 #
-#             Patterns = obj.getPatterns()
+#             obj.mine()
 #
-#             print("Total number of  Patterns:", len(Patterns))
+#             periodicFrequentPatterns = obj.getPatterns()
+#
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -27,14 +28,15 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -46,370 +48,441 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-# from geoReferencedFrequentPatterns import abstract as _ab
-# from PAMI.uncertainGeoreferencedFrequentPattern.basic import abstract as _ab
-import abstract as _ab
-
-_minSup = str()
-_neighbourList = {}
-_ab._sys.setrecursionlimit(20000)
-_finalPatterns = {}
 
+from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Tuple, Set, Union, Any, Generator
+
+_minSup = float()
+__maxPer = float()
+__first = int()
+_last = int()
+__lno = int()
+#rank = {}
+#periodic = {}
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
 
     :Attributes:
 
-        item : int or word
+        item: int or word
             Represents the name of the item
-        probability : float
+        probability: float
             Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self, item, probability):
+    def __init__(self, item: str, probability: float) -> None:
         self.item = item
         self.probability = probability
 
 
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
-        item : int
+        item: int
             storing item of a node
-        probability : int
+        probability: int
             To maintain the expected support of node
-        parent : node
+        parent: node
             To maintain the parent of every node
-        children : list
+        children: list
             To maintain the children of node
+        timeStamps: list
+            To maintain the timeStamps of node
 
     :Methods:
 
         addChild(itemName)
             storing the children to their respective parent nodes
     """
 
-    def __init__(self, item, children):
+    def __init__(self, item: str, children: Dict) -> None:
         self.item = item
         self.probability = 1
         self.children = children
         self.parent = None
+        self.timeStamps = []
+
+    def addChild(self, node: '_Node') -> None:
+        """
+        To add the children details to parent node
 
-    def addChild(self, node):
+        :param node: children node
+        :return: updated parent node children
+        """
         self.children[node.item] = node
         node.parent = self
 
 
+def _printTree(root) -> None:
+    """
+    To print the details of tree
+
+    :param root: root node of the tree
+    :return: details of tree
+    """
+    for x, y in root.children.items():
+        print(x, y.item, y.probability, y.parent.item, y.timeStamps)
+        _printTree(y)
+
+
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
-
         root : Node
             Represents the root node of the tree
         summaries : dictionary
             storing the nodes with same item name
         info : dictionary
             stores the support of items
-    :Methods:
 
-        addTransaction(transaction)
+    :Methods:
+        addTransactions(transaction)
             creating transaction as a branch in frequentPatternTree
-        addConditionalPattern(prefixPaths, supportOfItems)
+        addConditionalTransaction(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
         conditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
         conditionalTransactions(prefixPaths,Support)
             takes the prefixPath of a node and support at child of the path and extract the frequent items from
             prefixPaths and generates prefixPaths with items which are frequent
         remove(Node)
-            removes the node from tree once after generating all the patterns respective to the node
-        generatePatterns(Node)
-            starts from the root node of the tree and mines the frequent patterns
+            removes the node from tree once after generating all the patterns respective to the node generatePatterns(Node) starts from the root node of the tree and mines the frequent patterns
     """
 
-    def __init__(self):
+    def __init__(self) -> None:
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransaction(self, transaction):
+    def addTransactions(self, transaction: List['_Item'], tid: int) -> None:
         """
         Adding transaction into tree
 
-        :param transaction : it represents the one self.Database in database
-        :type transaction : list
+        :param transaction: it represents the one transaction in database
+        :type transaction: list
+        :param tid: the timestamp of transaction
+        :type tid: list
+        :return: None
         """
-        global _neighbourList
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i].item not in currentNode.children:
                 newNode = _Node(transaction[i].item, {})
-                nei = _neighbourList.get(transaction[i].item)
                 l1 = i - 1
-                lp = []
+                temp = []
                 while l1 >= 0:
-                    if nei == None:
-                        break
-                    if transaction[l1].item in nei:
-                        lp.append(transaction[l1].probability)
+                    temp.append(transaction[l1].probability)
                     l1 -= 1
-                if len(lp) == 0:
+                if len(temp) == 0:
                     newNode.probability = transaction[i].probability
                 else:
-                    newNode.probability = max(lp) * transaction[i].probability
+                    newNode.probability = max(temp) * transaction[i].probability
                 currentNode.addChild(newNode)
                 if transaction[i].item in self.summaries:
                     self.summaries[transaction[i].item].append(newNode)
                 else:
                     self.summaries[transaction[i].item] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i].item]
                 l1 = i - 1
-                lp = []
+                temp = []
                 while l1 >= 0:
-                    lp.append(transaction[l1].probability)
+                    temp.append(transaction[l1].probability)
                     l1 -= 1
-                if len(lp) == 0:
+                if len(temp) == 0:
                     currentNode.probability += transaction[i].probability
                 else:
-                    currentNode.probability += max(lp) * transaction[i].probability
+                    currentNode.probability += max(temp) * transaction[i].probability
+        currentNode.timeStamps = currentNode.timeStamps + tid
 
-    def addConditionalPattern(self, transaction, sup):
+    def addConditionalTransaction(self, transaction: List[str], ts: List[int], sup: float) -> None:
         """
-        constructing conditional tree from prefixPaths
+        Constructing conditional tree from prefixPaths
 
-        :param transaction : it represents the one self.Database in database
+        :param transaction : it represents the one transaction in database
         :type transaction : list
+        :param ts: timeStamp of a transaction
+        :type ts: list
         :param sup : support of prefixPath taken at last child of the path
         :type sup : int
+        :return: None
         """
-
-        # This method takes transaction, support and constructs the conditional tree
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
                 newNode.probability = sup
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
                 currentNode.probability += sup
+        currentNode.timeStamps = currentNode.timeStamps + ts
 
-    def conditionalPatterns(self, alpha):
+    def getConditionalPatterns(self, alpha: str) -> Tuple[List[List[str]], List[List[int]], List[float], Dict[str, List[float]]]:
         """
-        Generates all the conditional patterns of respective node
+        Generates all the conditional patterns of respective node.
 
         :param alpha : it represents the Node in tree
-        :type alpha : _Node
+        :type alpha : Node
+        :return: tuple
         """
 
-        # This method generates conditional patterns of node by traversing the tree
-        global _neighbourList
         finalPatterns = []
+        finalTimeStamps = []
         sup = []
         for i in self.summaries[alpha]:
-            j = i.item
+            set1 = i.timeStamps
             s = i.probability
             set2 = []
             while i.parent.item is not None:
-                if _neighbourList.get(j) is not None:
-                    #print(_neighbourList.get(j))
-                    if i.parent.item in _neighbourList[j]:
-                        set2.append(i.parent.item)
+                set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
+                finalTimeStamps.append(set1)
                 sup.append(s)
-        finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
-        return finalPatterns, support, info
+        finalPatterns, finalTimeStamps, support, info = self.conditionalTransactions(finalPatterns, finalTimeStamps,
+                                                                                     sup)
+        return finalPatterns, finalTimeStamps, support, info
 
-    def removeNode(self, nodeValue):
+    def removeNode(self, nodeValue: str) -> None:
         """
         Removing the node from tree
 
         :param nodeValue : it represents the node in tree
         :type nodeValue : node
+        :return: None
         """
-
         for i in self.summaries[nodeValue]:
+            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
             del i.parent.children[nodeValue]
 
-    def conditionalTransactions(self, condPatterns, support):
+    def getPeriodAndSupport(self, s: float, timeStamps: List[int]) -> List[float]:
+        global _lno, _maxPer
+        timeStamps.sort()
+        cur = 0
+        per = 0
+        sup = s
+        for j in range(len(timeStamps)):
+            per = max(per, timeStamps[j] - cur)
+            if per > _maxPer:
+                return [0, 0]
+            cur = timeStamps[j]
+        per = max(per, _lno - cur)
+        return [sup, per]
+
+    def conditionalTransactions(self, condPatterns: List[List[str]], condTimeStamps: List[List[int]], support: List[float]) -> Tuple[List[List[str]], List[List[int]], List[float], Dict[str, List[float]]]:
         """
         It generates the conditional patterns with frequent items
 
-        :param condPatterns : conditionalPatterns generated from conditionalPattern method for respective node
+        :param condPatterns : conditional patterns generated from getConditionalPatterns method for respective node
         :type condPatterns : list
-        :support : the support of conditional pattern in tree
-        :support : int
+        :param condTimeStamps: timeStamps of conditional transactions
+        :type condTimeStamps: list
+        :param support : the support of conditional pattern in tree
+        :type support : list
         """
-
-        global minSup
+        global _minSup, _maxPer
         pat = []
+        timeStamps = []
         sup = []
+        data1 = {}
         count = {}
         for i in range(len(condPatterns)):
             for j in condPatterns[i]:
-                if j in count:
+                if j in data1:
+                    data1[j] = data1[j] + condTimeStamps[i]
                     count[j] += support[i]
                 else:
+                    data1[j] = condTimeStamps[i]
                     count[j] = support[i]
         updatedDict = {}
-        updatedDict = {k: v for k, v in count.items() if v >= minSup}
+        for m in data1:
+            updatedDict[m] = self.getPeriodAndSupport(count[m], data1[m])
+        updatedDict = {k: v for k, v in updatedDict.items() if v[0] >= _minSup and v[1] <= _maxPer}
         count = 0
         for p in condPatterns:
             p1 = [v for v in p if v in updatedDict]
-            trans = sorted(p1, key=lambda x: updatedDict[x], reverse=True)
+            trans = sorted(p1, key=lambda x: (updatedDict.get(x)[0]), reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
+                timeStamps.append(condTimeStamps[count])
                 sup.append(support[count])
-                count += 1
-        return pat, sup, updatedDict
+            count += 1
+        return pat, timeStamps, sup, updatedDict
 
-    def generatePatterns(self, prefix):
+    def generatePatterns(self, prefix: List[str], periodic: Dict) -> None:
         """
         Generates the patterns
 
         :param prefix : forms the combination of items
         :type prefix : list
+        :return: None
         """
 
-        global _finalPatterns, minSup
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
+        global _minSup
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0])):
             pattern = prefix[:]
             pattern.append(i)
             s = 0
             for x in self.summaries[i]:
                 s += x.probability
-            _finalPatterns[tuple(pattern)] = self.info[i]
-            if s >= minSup:
-                patterns, support, info = self.conditionalPatterns(i)
+            periodic[tuple(pattern)] = self.info[i]
+            if s >= _minSup:
+                patterns, timeStamps, support, info = self.getConditionalPatterns(i)
                 conditionalTree = _Tree()
                 conditionalTree.info = info.copy()
                 for pat in range(len(patterns)):
-                    conditionalTree.addConditionalPattern(patterns[pat], support[pat])
+                    conditionalTree.addConditionalTransaction(patterns[pat], timeStamps[pat], support[pat])
                 if len(patterns) > 0:
-                    conditionalTree.generatePatterns(pattern)
+                    conditionalTree.generatePatterns(pattern, periodic)
             self.removeNode(i)
 
 
-class GFPGrowth(_ab._frequentPatterns):
+class UPFPGrowth(_ab._periodicFrequentPatterns):
     """
-    :Description: GFPGrowth algorithm is used to discover geo-referenced frequent patterns in a uncertain transactional database using GFP-Tree.
+    :Description: Basic is  to discover periodic-frequent patterns in a uncertain temporal database.
 
     :Reference:
-         Palla Likhitha,Pamalla Veena, Rage, Uday Kiran, Koji Zettsu (2023).
-         "Discovering Geo-referenced Frequent Patterns in Uncertain Geo-referenced
-         Transactional Databases".  PAKDD 2023.
-         https://doi.org/10.1007/978-3-031-33380-4_3
-        
-    :Attributes:
+            Uday Kiran, R., Likhitha, P., Dao, MS., Zettsu, K., Zhang, J. (2021).
+            Discovering Periodic-Frequent Patterns in Uncertain Temporal Databases. In:
+            Mantoro, T., Lee, M., Ayu, M.A., Wong, K.W., Hidayanto, A.N. (eds) Neural Information Processing.
+            ICONIP 2021. Communications in Computer and Information Science, vol 1516. Springer, Cham.
+            https://doi.org/10.1007/978-3-030-92307-5_83
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Uncertain Periodic Frequent Patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Uncertain Periodic Frequent patterns
+    :param  minSup: float:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  maxper: float :
+                   where maxPer represents the maximum periodicity threshold value specified by the user.
 
+
+    :Attributes:
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
-            Name of the output file or path of the output file
-        minSup: float or int or str
+            Name of the output file or path of output file
+        minSup: int or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        sep : str
+        maxPer: int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+        sep: str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        memoryUSS : float
+        memoryUSS: float
             To store the total amount of USS memory consumed by the program
-        memoryRSS : float
+        memoryRSS: float
             To store the total amount of RSS memory consumed by the program
-        startTime:float
+        startTime: float
             To record the start time of the mining process
-        endTime:float
+        endTime: float
             To record the completion time of the mining process
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
-        lno : int
+        _lno : int
             To represent the total no of transaction
         tree : class
             To represents the Tree class
-        itemSetCount : int
-            To represents the total no of patterns
         finalPatterns : dict
             To store the complete patterns
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        savePatterns(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+        save(oFile)
+            Complete set of periodic-frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets(fileName)
+        creatingItemSets()
             Scans the dataset and stores in a list format
-        frequentOneItem()
-            Extracts the one-length frequent patterns from database
-        updateTransactions()
-            Update the transactions by removing non-frequent items and sort the Database by item decreased support
+        PeriodicFrequentOneItem()
+            Extracts the one-periodic-frequent patterns from database
+        updateTransaction()
+            Update the database by removing aperiodic items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
-            to convert the user specified value
-        startMine()
-            Mining process will start from this function
+            To convert the user specified value
+        removeFalsePositives()
+            To remove the false positives in generated patterns
 
     **Executing the code on terminal**:
-    ------------------------------------
-        Format:
-          >>>  python3 GFPGrowth.py <inputFile> <neighborFile> <outputFile> <minSup>
-
-        Examples:
-          >>> python3 GFPGrowth.py sampleTDB.txt sampleNeighbor.txt patterns.txt 3    (minSup  will be considered in support count or frequency)
-    
-    **Sample run of importing the code**:
-    --------------------------------------
-     .. code-block:: python
+    --------------------------------------------
+    .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup> <maxPer>
+
+       Example Usage:
+
+       (.venv) $ python3 basic.py sampleTDB.txt patterns.txt 0.3 4
+
+
+               .. note:: minSup and maxPer will be considered in support count or frequency
 
-            from PAMI.uncertainGeoreferencedFrequentPattern.basic import GFPGrowth as alg
 
-            obj = alg.GFPGrowth(iFile, nFile, minSup)
+    **Importing this algorithm into a python program**
+    ----------------------------------------------------
+    .. code-block:: python
+
+            from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowth as alg
+
+            obj = alg.UPFPGrowth(iFile, minSup, maxPer)
 
             obj.startMine()
 
-            Patterns = obj.getPatterns()
+            periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of  Patterns:", len(Patterns))
+            print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
@@ -418,382 +491,396 @@
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
-        
+
     **Credits**:
     -------------
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
-        """
+
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+
+"""
+    _rank = {}
     _startTime = float()
     _endTime = float()
-    _minSup = str()
+    _minSup = float()
+    _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _rank = {}
-
-    def __init__(self, iFile, nFile, minSup, sep='\t'):
-        super().__init__(iFile, nFile, minSup, sep)
+    _lno = 0
+    _periodic = {}
 
-    def _creatingItemSets(self):
+    def _creatingItemSets(self) -> None:
         """
-        Scans the uncertain transactional dataset
+        Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data = [], []
+            uncertain, data, ts = [], [], []
             if self._iFile.empty:
                 print("its empty..")
-            i = self._iFile.columns.values.tolist()
+            i = self._iFile._columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                data = self._iFile['Transactions'].tolist()
             if 'uncertain' in i:
                 uncertain = self._iFile['uncertain'].tolist()
             for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
+                tr = [ts[k]]
+                for j in range(len(k)):
                     product = _Item(data[k][j], uncertain[k][j])
                     tr.append(product)
                 self._Database.append(tr)
+                self._lno += 1
 
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
-                    temp1 = line.strip()
-                    temp1 = temp1.split(':')
-                    temp = [i.rstrip() for i in temp1[0].split(self._sep)]
-                    uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep)]
-                    tr = []
-                    for i in range(len(temp)):
-                        item = temp[i]
-                        probability = uncertain[i]
+                    line = line.decode("utf-8")
+                    line = line.strip()
+                    line = [i for i in line.split(':')]
+                    temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                    temp2 = [i.rstrip() for i in line[1].split(self._sep)]
+                    temp1 = [x for x in temp1 if x]
+                    temp2 = [x for x in temp2 if x]
+                    tr = [int(temp1[0])]
+                    for i in range(len(temp1[1:])):
+                        item = temp1[i]
+                        probability = float(temp2[i])
                         product = _Item(item, probability)
                         tr.append(product)
+                    self._lno += 1
                     self._Database.append(tr)
             else:
                 try:
+                    count = 0
                     with open(self._iFile, 'r') as f:
                         for line in f:
-                            temp1 = line.strip()
-                            temp1 = temp1.split(':')
-                            #temp1[0], temp1[1] = [i for i in temp1[0] if i], [i for i in temp1[1] if i]
-                            temp = [i.rstrip() for i in temp1[0].split(self._sep) if i]
-                            uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep) if i]
-                            tr = []
-                            for i in range(len(temp)):
-                                item = temp[i]
-                                probability = uncertain[i]
+                            #count += 1
+                            line = line.strip()
+                            line = [i for i in line.split(':')]
+                            temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                            temp2 = [i.rstrip() for i in line[1].split(self._sep)]
+                            temp1 = [x for x in temp1 if x]
+                            temp2 = [x for x in temp2 if x]
+                            tr = [int(temp1[0])]
+                            for i in range(len(temp1[1:])):
+                                item = temp1[i]
+                                probability = float(temp2[i])
                                 product = _Item(item, probability)
                                 tr.append(product)
+                            self._lno += 1
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    
-    def _creatingNeighbours(self):
-        """
-        Scans the uncertain transactional dataset
-        """
-        global _neighbourList
-        _neighbourList = {}
-        if isinstance(self._nFile, _ab._pd.DataFrame):
-            uncertain, data = [], []
-            if self._iFile.empty:
-                print("its empty..")
-            i = self._iFile.columns.values.tolist()
-            if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-            if 'uncertain' in i:
-                uncertain = self._iFile['uncertain'].tolist()
-            for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
-                    product = _Item(data[k][j], uncertain[k][j])
-                    tr.append(product)
-                self._Database.append(tr)
-
-            # print(self.Database)
-        if isinstance(self._nFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
-                for line in data:
-                    line.strip()
-                    line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    tr = []
-                    for i in temp:
-                        i1 = i.index('(')
-                        i2 = i.index(')')
-                        item = i[0:i1]
-                        probability = float(i[i1 + 1:i2])
-                        product = _Item(item, probability)
-                        tr.append(product)
-                    self._Database.append(temp)
-            else:
-                try:
-                    with open(self._nFile, 'r') as f:
-                        for line in f:
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            _neighbourList[temp[0]] = temp[1:]
-                except IOError:
-                    print("File Not Found")
 
-    def _frequentOneItem(self):
+    def _periodicFrequentOneItem(self) -> Tuple[Dict, List]:
         """
-        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        :return: Tuple
 
-        :param self.Database : it represents the one self.Database in database
-        :type self.Database : list
         """
-
         mapSupport = {}
         for i in self._Database:
-            for j in i:
+            n = i[0]
+            for j in i[1:]:
                 if j.item not in mapSupport:
-                    mapSupport[j.item] = j.probability
+                    mapSupport[j.item] = [round(j.probability, 3), abs(0 - n), n]
                 else:
-                    mapSupport[j.item] += j.probability
-        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
-        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        self.rank = dict([(index, item) for (item, index) in enumerate(plist)])
+                    mapSupport[j.item][0] += round(j.probability, 3)
+                    mapSupport[j.item][1] = max(mapSupport[j.item][1], abs(n - mapSupport[j.item][2]))
+                    mapSupport[j.item][2] = n
+        for key in mapSupport:
+            mapSupport[key][1] = max(mapSupport[key][1], self._lno - mapSupport[key][2])
+        mapSupport = {k: [v[0], v[1]] for k, v in mapSupport.items() if v[1] <= self._maxPer and v[0] >= self._minSup}
+        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
+        self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
         return mapSupport, plist
 
-    @staticmethod
-    def _buildTree(data, info):
+    def _check(self, i: List, x: List) -> int:
         """
-        It takes the self.Database and support of each item and construct the main tree with setting root node as null
+        To check the presence of item or pattern in transaction
 
-        :param data : it represents the one self.Database in database
-        :type data : list
-        :param info : it represents the support of each item
-        :type info : dictionary
+        :param x: it represents the pattern
+        :type x : list
+        :param i : represents the uncertain transactions
+        :type i : list
+        :return: value
+        :rtype: int
+        """
+
+        for m in x:
+            k = 0
+            for n in i:
+                if m == n.item:
+                    k += 1
+            if k == 0:
+                return 0
+        return 1
+
+    def _getPeriodAndSupport(self, s: float, timeStamps: List[int]) -> List[float]:
         """
+        To calculate periodicity of timeStamps
 
+        :param s: support of a pattern
+        :param timeStamps: timeStamps of a pattern
+        :return: periodicity and Support
+        """
+        global __lno, _maxPer
+        timeStamps.sort()
+        cur = 0
+        per = 0
+        sup = s
+        for j in range(len(timeStamps)):
+            per = max(per, timeStamps[j] - cur)
+            if per > _maxPer:
+                return [0, 0]
+            cur = timeStamps[j]
+        per = max(per, _lno - cur)
+        return [sup, per]
+
+    def _buildTree(self, data: List[List], info: Dict) -> '_Tree':
+        """
+        It takes the transactions and support of each item and construct the main tree with setting root node as null
+
+        :param data: it represents the one transaction in database
+        :type data: list
+        :param info: it represents the support of each item
+        :type info : dictionary
+        """
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
-            rootNode.addTransaction(data[i])
+            set1 = [data[i][0]]
+            rootNode.addTransactions(data[i][1:], set1)
         return rootNode
 
-    def _updateTransactions(self, dict1):
+    def _updateTransactions(self, dict1: Dict) -> List[List]:
         """
-        Remove the items which are not frequent from self.Database and updates the self.Database with rank of items
+        Remove the items which are not frequent from transactions and updates the transactions with rank of items
 
         :param dict1 : frequent items with support
         :type dict1 : dictionary
+        :return: list
         """
-
         list1 = []
         for tr in self._Database:
-            list2 = []
-            for i in range(0, len(tr)):
+            list2 = [int(tr[0])]
+            for i in range(1, len(tr)):
                 if tr[i].item in dict1:
                     list2.append(tr[i])
             if len(list2) >= 2:
-                basket = list2
-                basket.sort(key=lambda val: self.rank[val.item])
-                list2 = basket
+                basket = list2[1:]
+                basket.sort(key=lambda val: self._rank[val.item])
+                list2[1:] = basket[0:]
                 list1.append(list2)
         return list1
 
-    @staticmethod
-    def _check(i, x):
+    def _convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
-        To check the presence of item or pattern in transaction
+        To convert the given user specified value
 
-        :param x: it represents the pattern
-        :type x : list
-        :param i : represents the uncertain self.Database
-        :type i : list
-        """
-
-        # This method taken a transaction as input and returns the tree
-        for m in x:
-            k = 0
-            for n in i:
-                if m == n.item:
-                    k += 1
-            if k == 0:
-                return 0
-        return 1
-
-    def _convert(self, value):
-        """
-        To convert the type of user specified minSup value
-
-        :param value: user specified minSup value
-        :return: converted type minSup value
+        :param value: user specified value
+        :return: converted value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = float(value)
         if type(value) is str:
             if '.' in value:
-                value = (len(self._Database) * value)
+                value = float(value)
             else:
                 value = int(value)
+
         return value
 
-    def _removeFalsePositives(self):
+    def _removeFalsePositives(self) -> None:
         """
-        To remove the false positive patterns generated in frequent patterns.
 
-        :return: patterns with accurate probability
+        :return: Removes the false positive patterns in generated patterns
         """
-        global _finalPatterns
         periods = {}
         for i in self._Database:
-            for x, y in _finalPatterns.items():
+            for x, y in self._periodic.items():
                 if len(x) == 1:
                     periods[x] = y
                 else:
                     s = 1
-                    check = self._check(i, x)
+                    check = self._check(i[1:], x)
                     if check == 1:
-                        for j in i:
+                        for j in i[1:]:
                             if j.item in x:
                                 s *= j.probability
                         if x in periods:
-                            periods[x] += s
+                            periods[x][0] += s
                         else:
-                            periods[x] = s
+                            periods[x] = [s, y[1]]
         for x, y in periods.items():
-            if y >= self._minSup:
+            if y[0] >= _minSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
-    def startMine(self):
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        Main method where the patterns are mined by constructing tree and remove the false patterns
+        by counting the original support of a patterns.
+        :return: None
         """
-        global minSup
+        global _lno, _maxPer, _minSup, _first, _last, periodic
         self._startTime = _ab._time.time()
         self._creatingItemSets()
-        self._creatingNeighbours()
-        #self._minSup = self._convert(self._minSup)
-        minSup = self._minSup
         self._finalPatterns = {}
-        mapSupport, plist = self._frequentOneItem()
-        self.Database1 = self._updateTransactions(mapSupport)
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        _minSup, _maxPer, _lno = self._minSup, self._maxPer, self._lno
+        mapSupport, plist = self._periodicFrequentOneItem()
+        updatedTrans = self._updateTransactions(mapSupport)
         info = {k: v for k, v in mapSupport.items()}
-        Tree1 = self._buildTree(self.Database1, info)
-        Tree1.generatePatterns([])
+        Tree1 = self._buildTree(updatedTrans, info)
+        self._periodic = {}
+        Tree1.generatePatterns([], self._periodic)
         self._removeFalsePositives()
-        print("Geo-Referenced Frequent patterns were generated from uncertain databases successfully using GFP algorithm")
+        print("Periodic frequent patterns were generated successfully using UPFP algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
-        self.memoryRSS = float()
+        self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
-        self.memoryRSS = process.memory_info().rss
+        self._memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self):
+    def Mine(self) -> None:
+        """
+        Main method where the patterns are mined by constructing tree and remove the false patterns
+        by counting the original support of a patterns.
+        :return: None
+        """
+        global _lno, _maxPer, _minSup, _first, _last, periodic
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        _minSup, _maxPer, _lno = self._minSup, self._maxPer, self._lno
+        mapSupport, plist = self._periodicFrequentOneItem()
+        updatedTrans = self._updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        Tree1 = self._buildTree(updatedTrans, info)
+        self._periodic = {}
+        Tree1.generatePatterns([], self._periodic)
+        self._removeFalsePositives()
+        print("Periodic frequent patterns were generated successfully using UPFP algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+
+    def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self.memoryRSS
+        return self._memoryRSS
 
-    def getRuntime(self):
+    def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
+    def getPatternsAsDataFrame(self) -> '_ab._pd.DataFrame':
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+            data.append([a.replace('\t', ' '), b[0], b[1]])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataframe
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y)
+            s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
+    def getPatterns(self) -> Dict[str, List[float]]:
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
-    
-    def printResults(self):
-        print("Total number of Patterns:", len(self.getPatterns()))
-        self.save("patterns.txt")
-        memUSS = self.getMemoryUSS()
-        print("Total Memory in USS:", memUSS)
-        memRSS = self.getMemoryRSS()
-        print("Total Memory in RSS", memRSS)
-        run = self.getRuntime()
-        print("Total ExecutionTime in ms:", run)
+
+    def printResults(self) -> None:
+        """
+        This function is used to print the results
+        """
+        print("Total number of  Uncertain Periodic-Frequent Patterns:", len(self.getPatterns()))
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
-            _ap = GFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+            _ap = UPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = GFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = UPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Patterns:", len(_Patterns))
+        print("Total number of Uncertain Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/abstract.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-
 from abc import ABC as _ABC, abstractmethod as _abstractmethod
 import time as _time
 import csv as _csv
 import pandas as _pd
 from collections import defaultdict as _defaultdict
 from itertools import combinations as _c
 import os as _os
@@ -12,72 +11,68 @@
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 
 
 class _frequentPatterns(_ABC):
     """
     :Description: This abstract base class defines the variables and methods that every frequent pattern mining algorithm must employ in PAMI
-
     :Attributes:
-
         iFile : str
             Input file name or path of the input file
-        minSup: float or int or str
+        minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        startTime:float
+        startTime :float
             To record the start time of the algorithm
-        endTime:float
+        endTime : float
             To record the completion time of the algorithm
-        finalPatterns: dict
+        finalPatterns : dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-
     :Methods:
-
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        savePatterns(oFile)
+        save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to data frame
         getMemoryUSS()
             Total amount of USS memory consumed by the program will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the program will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the program will be retrieved from this function
     """
 
-    def __init__(self, iFile, nFile, minSup, sep = '\t'):
+    def __init__(self, iFile, minSup, sep = '\t'):
         """
         :param iFile: Input file name or path of the input file
         :type iFile: str
         :param minSup: The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         :type minSup: int or float or str
         :param sep: separator used to distinguish items from each other. The default separator is tab space. However, users can override the default separator
         :type sep: str
         """
+
         self._iFile = iFile
-        self._nFile = nFile
         self._minSup = minSup
         self._sep = sep
         self._oFile = " "
         self._finalPatterns = {}
         self._startTime = float()
         self._endTime = float()
         self._memoryUSS = float()
@@ -93,17 +88,15 @@
     def getPatterns(self):
         """Complete set of patterns generated will be retrieved from this function"""
 
         pass
 
     @_abstractmethod
     def save(self, oFile):
-        """
-        Complete set of frequent patterns will be saved in to an output file from this function
-
+        """Complete set of frequent patterns will be saved in to an output file from this function
         :param oFile: Name of the output file
         :type oFile: csv file
         """
 
         pass
 
     @_abstractmethod
@@ -124,13 +117,14 @@
         pass
 
     @_abstractmethod
     def getRuntime(self):
         """Total amount of runtime taken by the program will be retrieved from this function"""
 
         pass
-      
+
     @_abstractmethod
     def printResults(self):
-        """To print all the statistics"""
+        """ To print all the results of execution"""
 
         pass
+
```

### Comparing `pami-2024.3.9.2/PAMI/uncertainPeriodicFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py` & `pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# UPFPGrowth is used to discover periodic-frequent patterns in an uncertain temporal database.
+# UPFPGrowthPlus is used to discover periodic-frequent patterns in an uncertain temporal database.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#             from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowth as alg
 #
-#             obj = alg.UPFPGrowth(iFile, minSup, maxPer)
+#             from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowthPlus as alg
+#
+#             obj = alg.UPFPGrowthPlus(iFile, minSup, maxPer)
 #
 #             obj.startMine()
 #
 #             periodicFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of uncertain Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -24,15 +25,15 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
-
+#
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
@@ -45,328 +46,388 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
+
+
+from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
 from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
 
 _minSup = float()
-__maxPer = float()
-__first = int()
+_maxPer = float()
+_lno = int()
+_first = int()
 _last = int()
-__lno = int()
-#rank = {}
-#periodic = {}
+
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
 
     :Attributes:
 
-        item: int or word
-            Represents the name of the item
-        probability: float
-            Represent the existential probability(likelihood presence) of an item
+    item : int or string
+        Represents the name of the item
+    probability : float
+        Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self, item: str, probability: float) -> None:
+    def __init__(self, item, probability):
         self.item = item
         self.probability = probability
 
 
+def printTree(root):
+    """
+    To print the tree with nodes with item name, probability, timestamps, and second probability respectively.
+
+    Attributes:
+
+    :param root: Node
+    :return: print all Tree with nodes with items, probability, parent item, timestamps, second probability respectively.
+    """
+    for x, y in root.children.items():
+        print(x, y.item, y.probability, y.parent.item, y.tids, y.secondProbability)
+        printTree(y)
+
+
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
-        item: int
+        item : int
             storing item of a node
-        probability: int
+        probability : int
             To maintain the expected support of node
-        parent: node
+        parent : node
             To maintain the parent of every node
-        children: list
+        children : list
             To maintain the children of node
-        timeStamps: list
-            To maintain the timeStamps of node
 
     :Methods:
 
         addChild(itemName)
             storing the children to their respective parent nodes
     """
 
-    def __init__(self, item: str, children: Dict) -> None:
+    def __init__(self, item, children):
         self.item = item
         self.probability = 1
+        self.secondProbability = 1
+        self.p = 1
         self.children = children
         self.parent = None
-        self.timeStamps = []
+        self.TimeStamps = []
 
-    def addChild(self, node: '_Node') -> None:
+    def addChild(self, node):
         """
-        To add the children details to parent node
+        To add children details to parent node
 
         :param node: children node
-        :return: updated parent node children
+        :return: update parent node children
         """
         self.children[node.item] = node
         node.parent = self
 
 
-def _printTree(root) -> None:
-    """
-    To print the details of tree
-
-    :param root: root node of the tree
-    :return: details of tree
-    """
-    for x, y in root.children.items():
-        print(x, y.item, y.probability, y.parent.item, y.timeStamps)
-        _printTree(y)
-
-
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
-    :Attributes:
-        root : Node
+    Attributes:
+
+        root: Node
             Represents the root node of the tree
-        summaries : dictionary
+        summaries: dictionary
             storing the nodes with same item name
-        info : dictionary
+        info: dictionary
             stores the support of items
 
+
     :Methods:
-        addTransactions(transaction)
-            creating transaction as a branch in frequentPatternTree
+
+        addTransaction(transaction)
+            creating transaction as a branch in Tree
         addConditionalTransaction(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
-        conditionalPatterns(Node)
+        getConditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
         conditionalTransactions(prefixPaths,Support)
-            takes the prefixPath of a node and support at child of the path and extract the frequent items from
-            prefixPaths and generates prefixPaths with items which are frequent
+            takes the prefixPath of a node and support at child of the path and extract the frequent items from prefixPaths and generates prefixPaths with items which are frequent
         remove(Node)
-            removes the node from tree once after generating all the patterns respective to the node generatePatterns(Node) starts from the root node of the tree and mines the frequent patterns
+            removes the node from tree once after generating all the patterns respective to the node
+        generatePatterns(Node)
+            starts from the root node of the tree and mines the frequent patterns
+
     """
 
-    def __init__(self) -> None:
+    def __init__(self):
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransactions(self, transaction: List['_Item'], tid: int) -> None:
+
+    def addTransaction(self, transaction, tid):
         """
         Adding transaction into tree
 
-        :param transaction: it represents the one transaction in database
-        :type transaction: list
-        :param tid: the timestamp of transaction
-        :type tid: list
+        :param transaction : it represents the one transaction in database
+        :type transaction : list
+        :param tid : the timestamp of transaction
+        :type tid : list
         """
         currentNode = self.root
+        k = 0
         for i in range(len(transaction)):
+            k += 1
             if transaction[i].item not in currentNode.children:
                 newNode = _Node(transaction[i].item, {})
+                newNode.k = k
+                newNode.secondProbability = transaction[i].probability
                 l1 = i - 1
                 temp = []
                 while l1 >= 0:
                     temp.append(transaction[l1].probability)
                     l1 -= 1
                 if len(temp) == 0:
-                    newNode.probability = transaction[i].probability
+                    newNode.probability = round(transaction[i].probability, 2)
                 else:
-                    newNode.probability = max(temp) * transaction[i].probability
+                    newNode.probability = round(max(temp) * transaction[i].probability, 2)
                 currentNode.addChild(newNode)
                 if transaction[i].item in self.summaries:
                     self.summaries[transaction[i].item].append(newNode)
                 else:
                     self.summaries[transaction[i].item] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i].item]
+                currentNode.secondProbability = max(transaction[i].probability, currentNode.secondProbability)
+                currentNode.k = k
                 l1 = i - 1
                 temp = []
                 while l1 >= 0:
                     temp.append(transaction[l1].probability)
                     l1 -= 1
                 if len(temp) == 0:
-                    currentNode.probability += transaction[i].probability
+                    currentNode.probability += round(transaction[i].probability, 2)
                 else:
-                    currentNode.probability += max(temp) * transaction[i].probability
-        currentNode.timeStamps = currentNode.timeStamps + tid
+                    nn = max(temp) * transaction[i].probability
+                    currentNode.probability += round(nn, 2)
+        currentNode.TimeStamps = currentNode.TimeStamps + tid
 
-    def addConditionalTransaction(self, transaction: List[str], ts: List[int], sup: float) -> None:
+    def addConditionalPatterns(self, transaction, tid, sup, probability):
         """
         Constructing conditional tree from prefixPaths
 
         :param transaction : it represents the one transaction in database
         :type transaction : list
-        :param ts: timeStamp of a transaction
-        :type ts: list
+        :param tid : timestamps of a pattern or transaction in tree
+        :param tid : list
         :param sup : support of prefixPath taken at last child of the path
         :type sup : int
+        :para probability : highest existential probability value among all periodic-frequent items
+        :type probability : list
         """
         currentNode = self.root
+        k = 0
         for i in range(len(transaction)):
+            k += 1
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
+                newNode.k = k
                 newNode.probability = sup
+                newNode.secondProbability = probability
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
+                currentNode.k = k
                 currentNode.probability += sup
-        currentNode.timeStamps = currentNode.timeStamps + ts
+                currentNode.secondProbability = max(probability, currentNode.secondProbability)
+        currentNode.TimeStamps = currentNode.TimeStamps + tid
 
-    def getConditionalPatterns(self, alpha: str) -> Tuple[List[List[str]], List[List[int]], List[float], Dict[str, List[float]]]:
+    def conditionalPatterns(self, alpha):
         """
-        Generates all the conditional patterns of respective node.
+        Generates all the conditional patterns of respective node
 
         :param alpha : it represents the Node in tree
         :type alpha : Node
         """
-
         finalPatterns = []
-        finalTimeStamps = []
+        finalSets = []
         sup = []
+        prob = []
         for i in self.summaries[alpha]:
-            set1 = i.timeStamps
+            set1 = i.TimeStamps
             s = i.probability
+            p = i.secondProbability
             set2 = []
             while i.parent.item is not None:
                 set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
-                finalTimeStamps.append(set1)
+                finalSets.append(set1)
                 sup.append(s)
-        finalPatterns, finalTimeStamps, support, info = self.conditionalTransactions(finalPatterns, finalTimeStamps,
-                                                                                     sup)
-        return finalPatterns, finalTimeStamps, support, info
+                prob.append(p)
+        finalPatterns, finalSets, support, prob, info = self.conditionalTransactions(finalPatterns, finalSets, sup, prob)
+        return finalPatterns, finalSets, support, prob, info
 
-    def removeNode(self, nodeValue: str) -> None:
+    def removeNode(self, nodeValue):
         """
         Removing the node from tree
 
         :param nodeValue : it represents the node in tree
         :type nodeValue : node
         """
         for i in self.summaries[nodeValue]:
-            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
+            i.parent.TimeStamps = i.parent.TimeStamps + i.TimeStamps
             del i.parent.children[nodeValue]
 
-    def getPeriodAndSupport(self, s: float, timeStamps: List[int]) -> List[float]:
-        global _lno, _maxPer
-        timeStamps.sort()
+    def getPeriodAndSupport(self, support, TimeStamps):
+        """
+        To calculate the periodicity of given timestamps
+
+        :param support: support of pattern
+        :param TimeStamps: timmeStamps of a pattern
+        :return: support and period
+        """
+        global _maxPer
+        global _lno
+        TimeStamps.sort()
         cur = 0
         per = 0
-        sup = s
-        for j in range(len(timeStamps)):
-            per = max(per, timeStamps[j] - cur)
+        sup = support
+        for j in range(len(TimeStamps)):
+            per = max(per, TimeStamps[j] - cur)
             if per > _maxPer:
                 return [0, 0]
-            cur = timeStamps[j]
+            cur = TimeStamps[j]
         per = max(per, _lno - cur)
         return [sup, per]
 
-    def conditionalTransactions(self, condPatterns: List[List[str]], condTimeStamps: List[List[int]], support: List[float]) -> Tuple[List[List[str]], List[List[int]], List[float], Dict[str, List[float]]]:
+    def conditionalTransactions(self, conditionalPatterns, conditionalTimeStamps, support, probability):
         """
         It generates the conditional patterns with frequent items
 
-        :param condPatterns : conditional patterns generated from getConditionalPatterns method for respective node
-        :type condPatterns : list
-        :param condTimeStamps: timeStamps of conditional transactions
-        :type condTimeStamps: list
+        :param conditionalPatterns : conditional patterns generated from conditionalPatterns() method for respective node
+        :type conditionalPatterns : list
+        :param conditionalTimeStamps : timestamps of respective conditional timestamps
+        :type conditionalTimeStamps : list
         :param support : the support of conditional pattern in tree
         :type support : list
+        :para probability : highest existential probability value among all periodic-frequent items
+        :type probability : list
         """
-        global _minSup, _maxPer
+        global _minSup, _maxPer, _lno
         pat = []
-        timeStamps = []
+        TimeStamps = []
         sup = []
+        prob = []
         data1 = {}
         count = {}
-        for i in range(len(condPatterns)):
-            for j in condPatterns[i]:
+        for i in range(len(conditionalPatterns)):
+            for j in conditionalPatterns[i]:
                 if j in data1:
-                    data1[j] = data1[j] + condTimeStamps[i]
+                    data1[j] = data1[j] + conditionalTimeStamps[i]
                     count[j] += support[i]
                 else:
-                    data1[j] = condTimeStamps[i]
+                    data1[j] = conditionalTimeStamps[i]
                     count[j] = support[i]
         updatedDict = {}
         for m in data1:
             updatedDict[m] = self.getPeriodAndSupport(count[m], data1[m])
         updatedDict = {k: v for k, v in updatedDict.items() if v[0] >= _minSup and v[1] <= _maxPer}
         count = 0
-        for p in condPatterns:
+        for p in conditionalPatterns:
             p1 = [v for v in p if v in updatedDict]
             trans = sorted(p1, key=lambda x: (updatedDict.get(x)[0]), reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
-                timeStamps.append(condTimeStamps[count])
+                TimeStamps.append(conditionalTimeStamps[count])
                 sup.append(support[count])
+                prob.append(probability[count])
             count += 1
-        return pat, timeStamps, sup, updatedDict
+        return pat, TimeStamps, sup, prob, updatedDict
 
-    def generatePatterns(self, prefix: List[str], periodic: Dict) -> None:
+    def generatePatterns(self, prefix, periodic):
         """
         Generates the patterns
 
         :param prefix : forms the combination of items
         :type prefix : list
+        :para periodic : occurring at intervals
+        :type periodic : list
         """
-
         global _minSup
         for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0])):
             pattern = prefix[:]
             pattern.append(i)
             s = 0
+            secProb = []
+            kk = int()
             for x in self.summaries[i]:
-                s += x.probability
+                if x.k <= 2:
+                    s += x.probability
+                elif x.k >= 3:
+                    n = x.probability * pow(x.secondProbability, (x.k - 2))
+                    s += n
+            periodic[tuple(pattern)] = self.info[i]
             periodic[tuple(pattern)] = self.info[i]
             if s >= _minSup:
-                patterns, timeStamps, support, info = self.getConditionalPatterns(i)
+                periodic[tuple(pattern)] = self.info[i]
+                patterns, TimeStamps, support, probability, info = self.conditionalPatterns(i)
                 conditionalTree = _Tree()
                 conditionalTree.info = info.copy()
                 for pat in range(len(patterns)):
-                    conditionalTree.addConditionalTransaction(patterns[pat], timeStamps[pat], support[pat])
+                    conditionalTree.addConditionalPatterns(patterns[pat], TimeStamps[pat], support[pat], probability[pat])
                 if len(patterns) > 0:
                     conditionalTree.generatePatterns(pattern, periodic)
             self.removeNode(i)
 
-
-class UPFPGrowth(_ab._periodicFrequentPatterns):
+class UPFPGrowthPlus(_ab._periodicFrequentPatterns):
     """
-    :Description: Basic is  to discover periodic-frequent patterns in a uncertain temporal database.
+    :Description: Basic Plus is  to discover periodic-frequent patterns in a uncertain temporal database.
 
     :Reference:
-            Uday Kiran, R., Likhitha, P., Dao, MS., Zettsu, K., Zhang, J. (2021).
-            Discovering Periodic-Frequent Patterns in Uncertain Temporal Databases. In:
-            Mantoro, T., Lee, M., Ayu, M.A., Wong, K.W., Hidayanto, A.N. (eds) Neural Information Processing.
-            ICONIP 2021. Communications in Computer and Information Science, vol 1516. Springer, Cham.
-            https://doi.org/10.1007/978-3-030-92307-5_83
+          Palla Likhitha, Rage Veena,Rage Uday Kiran, Koji Zettsu, Masashi Toyoda, Philippe Fournier-Viger, (2023). 
+          UPFP-growth++: An Efficient Algorithm to Find Periodic-Frequent Patterns in Uncertain Temporal Databases. 
+          ICONIP 2022. Communications in Computer and Information Science, vol 1792. Springer, Singapore.
+          https://doi.org/10.1007/978-981-99-1642-9_16
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Uncertain Periodic Frequent Patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Uncertain Periodic Frequent patterns
+    :param  minSup: str:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  maxper: floot :
+                   where maxPer represents the maximum periodicity threshold value specified by the user.
+
 
     :Attributes:
-        iFile : file
-            Name of the Input file or path of the input file
-        oFile : file
+
+        iFile: file
+            Name of the Input file or path of input file
+        oFile: file
             Name of the output file or path of output file
         minSup: int or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         maxPer: int or float or str
@@ -381,76 +442,85 @@
             To store the total amount of USS memory consumed by the program
         memoryRSS: float
             To store the total amount of RSS memory consumed by the program
         startTime: float
             To record the start time of the mining process
         endTime: float
             To record the completion time of the mining process
-        Database : list
+        Database: list
             To store the transactions of a database in list
-        mapSupport : Dictionary
+        mapSupport: Dictionary
             To maintain the information of item and their frequency
-        _lno : int
+        lno: int
             To represent the total no of transaction
-        tree : class
+        tree: class
             To represents the Tree class
-        finalPatterns : dict
+        itemSetCount: int
+            To represents the total no of patterns
+        finalPatterns: dict
             To store the complete patterns
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
+        savePatterns(oFile)
             Complete set of periodic-frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets()
+        creatingItemSets(fileName)
             Scans the dataset and stores in a list format
-        PeriodicFrequentOneItem()
-            Extracts the one-periodic-frequent patterns from database
-        updateTransaction()
+        updateDatabases()
             Update the database by removing aperiodic items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
-            To convert the user specified value
-        removeFalsePositives()
-            To remove the false positives in generated patterns
+            to convert the user specified value
+        PeriodicFrequentOneItems()
+            To extract the one-length periodic-frequent items
 
     **Executing the code on terminal**:
     --------------------------------------------
-            Format:
-                >>> python3 basic.py <inputFile> <outputFile> <minSup> <maxPer>
 
-            Examples:
-                >>> python3 basic.py sampleTDB.txt patterns.txt 0.3 4     (minSup and maxPer will be considered in support count or frequency)
+    .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 UPFPGrowthPlus.py <inputFile> <outputFile> <minSup> <maxPer>
+
+       Examples Usage:
+
+       (.venv) $ python3 UPFPGrowthPlus.py sampleTDB.txt patterns.txt 0.3 4
+
+
+               .. note:: minSup and maxPer will be considered in support count or frequency
 
 
     **Importing this algorithm into a python program**
-    ----------------------------------------------------
+    -----------------------------------------------------------------
     .. code-block:: python
 
-            from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowth as alg
+            from PAMI.uncertainPeriodicFrequentPattern import UPFPGrowthPlus as alg
 
-            obj = alg.UPFPGrowth(iFile, minSup, maxPer)
+            obj = alg.UPFPGrowthPlus(iFile, minSup, maxPer)
 
             obj.startMine()
 
             periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+            print("Total number of uncertain Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
@@ -460,44 +530,46 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
+
     **Credits**:
-    -------------
-    The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    --------------
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
 
-    """
-    _rank = {}
+        """
     _startTime = float()
     _endTime = float()
     _minSup = float()
     _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
+    _rank = {}
     _lno = 0
     _periodic = {}
 
-    def _creatingItemSets(self) -> None:
+    def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
+
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             uncertain, data, ts = [], [], []
             if self._iFile.empty:
                 print("its empty..")
-            i = self._iFile._columns.values.tolist()
+            i = self._iFile.columns.values.tolist()
             if 'TS' in i:
                 ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 data = self._iFile['Transactions'].tolist()
             if 'uncertain' in i:
                 uncertain = self._iFile['uncertain'].tolist()
             for k in range(len(data)):
@@ -529,15 +601,14 @@
                     self._lno += 1
                     self._Database.append(tr)
             else:
                 try:
                     count = 0
                     with open(self._iFile, 'r') as f:
                         for line in f:
-                            #count += 1
                             line = line.strip()
                             line = [i for i in line.split(':')]
                             temp1 = [i.rstrip() for i in line[0].split(self._sep)]
                             temp2 = [i.rstrip() for i in line[1].split(self._sep)]
                             temp1 = [x for x in temp1 if x]
                             temp2 = [x for x in temp2 if x]
                             tr = [int(temp1[0])]
@@ -547,93 +618,56 @@
                                 product = _Item(item, probability)
                                 tr.append(product)
                             self._lno += 1
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
 
-    def _periodicFrequentOneItem(self) -> Tuple[Dict, List]:
+    def _PeriodicFrequentOneItems(self):
         """
         Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
-
         """
+        global first, last
         mapSupport = {}
         for i in self._Database:
-            n = i[0]
+            n = int(i[0])
             for j in i[1:]:
                 if j.item not in mapSupport:
                     mapSupport[j.item] = [round(j.probability, 3), abs(0 - n), n]
                 else:
-                    mapSupport[j.item][0] += round(j.probability, 3)
+                    mapSupport[j.item][0] += round(j.probability, 2)
                     mapSupport[j.item][1] = max(mapSupport[j.item][1], abs(n - mapSupport[j.item][2]))
                     mapSupport[j.item][2] = n
         for key in mapSupport:
             mapSupport[key][1] = max(mapSupport[key][1], self._lno - mapSupport[key][2])
-        mapSupport = {k: [v[0], v[1]] for k, v in mapSupport.items() if v[1] <= self._maxPer and v[0] >= self._minSup}
+        mapSupport = {k: [round(v[0], 2), v[1]] for k, v in mapSupport.items() if
+                      v[1] <= self._maxPer and v[0] >= self._minSup}
         plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
         self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
         return mapSupport, plist
 
-    def _check(self, i: List, x: List) -> int:
-        """
-        To check the presence of item or pattern in transaction
-
-        :param x: it represents the pattern
-        :type x : list
-        :param i : represents the uncertain transactions
-        :type i : list
-        """
-
-        for m in x:
-            k = 0
-            for n in i:
-                if m == n.item:
-                    k += 1
-            if k == 0:
-                return 0
-        return 1
-
-    def _getPeriodAndSupport(self, s: float, timeStamps: List[int]) -> List[float]:
-        """
-        To calculate periodicity of timeStamps
-
-        :param s: support of a pattern
-        :param timeStamps: timeStamps of a pattern
-        :return: periodicity and Support
-        """
-        global __lno, _maxPer
-        timeStamps.sort()
-        cur = 0
-        per = 0
-        sup = s
-        for j in range(len(timeStamps)):
-            per = max(per, timeStamps[j] - cur)
-            if per > _maxPer:
-                return [0, 0]
-            cur = timeStamps[j]
-        per = max(per, _lno - cur)
-        return [sup, per]
-
-    def _buildTree(self, data: List[List], info: Dict) -> '_Tree':
+    def _buildTree(self, data, info):
         """
         It takes the transactions and support of each item and construct the main tree with setting root node as null
 
-        :param data: it represents the one transaction in database
-        :type data: list
-        :param info: it represents the support of each item
+        :param data : it represents the one transaction in database
+        :type data : list
+        :param info : it represents the support of each item
         :type info : dictionary
         """
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
             set1 = [data[i][0]]
-            rootNode.addTransactions(data[i][1:], set1)
+            rootNode.addTransaction(data[i][1:], set1)
+            #printTree(rootNode)
+            #print("....")
         return rootNode
 
-    def _updateTransactions(self, dict1: Dict) -> List[List]:
+    def _updateTransactions(self, dict1):
         """
         Remove the items which are not frequent from transactions and updates the transactions with rank of items
 
         :param dict1 : frequent items with support
         :type dict1 : dictionary
         """
         list1 = []
@@ -645,174 +679,223 @@
             if len(list2) >= 2:
                 basket = list2[1:]
                 basket.sort(key=lambda val: self._rank[val.item])
                 list2[1:] = basket[0:]
                 list1.append(list2)
         return list1
 
-    def _convert(self, value: Union[int, float, str]) -> Union[int, float]:
+    def _Check(self, i, x):
         """
+        To check the presence of item or pattern in transaction
+
+        :param x: it represents the pattern
+        :type x : list
+        :param i : represents the uncertain transactions
+        :type i : list
+        """
+        for m in x:
+            k = 0
+            for n in i:
+                if m == n.item:
+                    k += 1
+            if k == 0:
+                return 0
+        return 1
+
+    def _convert(self, value):
+        """
+
         To convert the given user specified value
 
         :param value: user specified value
         :return: converted value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = float(value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
             else:
                 value = int(value)
-
         return value
 
-    def _removeFalsePositives(self) -> None:
+    def _removeFalsePositives(self):
         """
-
-        :return: Removes the false positive patterns in generated patterns
+        To remove false positives in generated patterns
+        :return: original patterns
         """
         periods = {}
         for i in self._Database:
             for x, y in self._periodic.items():
                 if len(x) == 1:
                     periods[x] = y
                 else:
                     s = 1
-                    check = self._check(i[1:], x)
+                    check = self._Check(i[1:], x)
                     if check == 1:
                         for j in i[1:]:
                             if j.item in x:
                                 s *= j.probability
                         if x in periods:
                             periods[x][0] += s
                         else:
                             periods[x] = [s, y[1]]
+        count = 0
         for x, y in periods.items():
             if y[0] >= _minSup:
+                count += 1
                 sample = str()
                 for i in x:
-                    sample = sample + i + "\t"
+                    sample = sample + i + " "
                 self._finalPatterns[sample] = y
+        #print("Total false patterns generated:", len(self._periodic) - count)
 
-    def startMine(self) -> None:
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self):
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns
-                    by counting the original support of a patterns
-
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        global _lno, _maxPer, _minSup, _first, _last, periodic
+        global _minSup, _maxPer, _first, _last, _lno
         self._startTime = _ab._time.time()
         self._creatingItemSets()
-        self._finalPatterns = {}
         self._minSup = self._convert(self._minSup)
         self._maxPer = self._convert(self._maxPer)
-        _minSup, _maxPer, _lno = self._minSup, self._maxPer, self._lno
-        mapSupport, plist = self._periodicFrequentOneItem()
+        self._finalPatterns = {}
+        _minSup, _maxPer, _lno = self._minSup, self._maxPer, len(self._Database)
+        mapSupport, plist = self._PeriodicFrequentOneItems()
         updatedTrans = self._updateTransactions(mapSupport)
         info = {k: v for k, v in mapSupport.items()}
-        Tree1 = self._buildTree(updatedTrans, info)
+        root = self._buildTree(updatedTrans, info)
         self._periodic = {}
-        Tree1.generatePatterns([], self._periodic)
+        root.generatePatterns([], self._periodic)
         self._removeFalsePositives()
-        print("Periodic frequent patterns were generated successfully using UPFP algorithm")
+        print("Periodic Frequent patterns were generated successfully using UPFP-Growth++ algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self) -> float:
+    def Mine(self):
         """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        """
+        global _minSup, _maxPer, _first, _last, _lno
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        self._finalPatterns = {}
+        _minSup, _maxPer, _lno = self._minSup, self._maxPer, len(self._Database)
+        mapSupport, plist = self._PeriodicFrequentOneItems()
+        updatedTrans = self._updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        root = self._buildTree(updatedTrans, info)
+        self._periodic = {}
+        root.generatePatterns([], self._periodic)
+        self._removeFalsePositives()
+        print("Periodic Frequent patterns were generated successfully using UPFP-Growth++ algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
+    def getMemoryUSS(self):
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function.
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> '_ab._pd.DataFrame':
+    def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b[0], b[1]])
+            data.append([a, b[0], b[1]])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataframe
 
-    def save(self, outFile: str) -> None:
+    def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
-        self.oFile = outFile
-        writer = open(self.oFile, 'w+')
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
+            s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, List[float]]:
+    def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
         This function is used to print the results
         """
         print("Total number of  Uncertain Periodic-Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
-            _ap = UPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+            _ap = UPFPGrowthPlus(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = UPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = UPFPGrowthPlus(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        print("Total number of Uncertain Periodic-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _Patterns = _ap.getPatterns()
+        print("Total number of Patterns:", len(_Patterns))
+        _ap.savePatterns(_ab._sys.argv[2])
+        # print(ap.getPatternsAsDataFrame())
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py` & `pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/WUFIM.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# UPFPGrowthPlus is used to discover periodic-frequent patterns in an uncertain temporal database.
+# WUFIM is one of the algorithm to discover weighted frequent patterns in an uncertain transactional database using PUF-Tree.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#             from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowthPlus as alg
 #
-#             obj = alg.UPFPGrowthPlus(iFile, minSup, maxPer)
+#             from PAMI.weightedUncertainFrequentPattern.basic import basic as alg
+#
+#             obj = alg.basic(iFile, wFile, minSup, sep)
 #
 #             obj.startMine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             Patterns = obj.getPatterns()
 #
-#             print("Total number of uncertain Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of  Patterns:", len(Patterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -27,74 +28,60 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
-     along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
+     along with this program.  If not, see `<https://www.gnu.org/licenses/>`_.
+     
 """
 
-
-from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
-
-_minSup = float()
-_maxPer = float()
-_lno = int()
-_first = int()
-_last = int()
-
-
+from PAMI.weightedUncertainFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
+_expSup = str()
+_expWSup = str()
+_weights = {}
+_finalPatterns = {}
+_ab._sys.setrecursionlimit(20000)
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
 
     :Attributes:
 
-    item : int or string
-        Represents the name of the item
-    probability : float
-        Represent the existential probability(likelihood presence) of an item
+        item : int or word
+            Represents the name of the item
+
+        probability : float
+            Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self, item, probability):
+    def __init__(self, item: int, probability: float) -> None:
         self.item = item
         self.probability = probability
 
 
-def printTree(root):
-    """
-    To print the tree with nodes with item name, probability, timestamps, and second probability respectively.
-
-    Attributes:
-
-    :param root: Node
-    :return: print all Tree with nodes with items, probability, parent item, timestamps, second probability respectively.
-    """
-    for x, y in root.children.items():
-        print(x, y.item, y.probability, y.parent.item, y.tids, y.secondProbability)
-        printTree(y)
-
-
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
         item : int
@@ -108,386 +95,337 @@
 
     :Methods:
 
         addChild(itemName)
             storing the children to their respective parent nodes
     """
 
-    def __init__(self, item, children):
+    def __init__(self, item, children: list) -> None:
         self.item = item
         self.probability = 1
-        self.secondProbability = 1
-        self.p = 1
         self.children = children
         self.parent = None
-        self.TimeStamps = []
 
-    def addChild(self, node):
-        """
-        To add children details to parent node
-
-        :param node: children node
-        :return: update parent node children
-        """
+    def addChild(self, node) -> None:
         self.children[node.item] = node
         node.parent = self
 
 
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
-    Attributes:
+    :Attributes:
 
-        root: Node
+        root : Node
             Represents the root node of the tree
-        summaries: dictionary
+        summaries : dictionary
             storing the nodes with same item name
-        info: dictionary
+        info : dictionary
             stores the support of items
 
-
     :Methods:
 
         addTransaction(transaction)
-            creating transaction as a branch in Tree
-        addConditionalTransaction(prefixPaths, supportOfItems)
+            creating transaction as a branch in frequentPatternTree
+        addConditionalPattern(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
-        getConditionalPatterns(Node)
+        conditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
         conditionalTransactions(prefixPaths,Support)
             takes the prefixPath of a node and support at child of the path and extract the frequent items from prefixPaths and generates prefixPaths with items which are frequent
         remove(Node)
             removes the node from tree once after generating all the patterns respective to the node
         generatePatterns(Node)
             starts from the root node of the tree and mines the frequent patterns
 
     """
 
-    def __init__(self):
+    def __init__(self) -> None:
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-
-    def addTransaction(self, transaction, tid):
+    def addTransaction(self, transaction) -> None:
         """
         Adding transaction into tree
 
-        :param transaction : it represents the one transaction in database
+        :param transaction : it represents the one self.Database in database
         :type transaction : list
-        :param tid : the timestamp of transaction
-        :type tid : list
+        :return: None
         """
         currentNode = self.root
-        k = 0
         for i in range(len(transaction)):
-            k += 1
             if transaction[i].item not in currentNode.children:
                 newNode = _Node(transaction[i].item, {})
-                newNode.k = k
-                newNode.secondProbability = transaction[i].probability
                 l1 = i - 1
-                temp = []
+                lp = []
                 while l1 >= 0:
-                    temp.append(transaction[l1].probability)
+                    lp.append(transaction[l1].probability)
                     l1 -= 1
-                if len(temp) == 0:
-                    newNode.probability = round(transaction[i].probability, 2)
+                if len(lp) == 0:
+                    newNode.probability = transaction[i].probability
                 else:
-                    newNode.probability = round(max(temp) * transaction[i].probability, 2)
+                    newNode.probability = max(lp) * transaction[i].probability
                 currentNode.addChild(newNode)
                 if transaction[i].item in self.summaries:
                     self.summaries[transaction[i].item].append(newNode)
                 else:
                     self.summaries[transaction[i].item] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i].item]
-                currentNode.secondProbability = max(transaction[i].probability, currentNode.secondProbability)
-                currentNode.k = k
                 l1 = i - 1
-                temp = []
+                lp = []
                 while l1 >= 0:
-                    temp.append(transaction[l1].probability)
+                    lp.append(transaction[l1].probability)
                     l1 -= 1
-                if len(temp) == 0:
-                    currentNode.probability += round(transaction[i].probability, 2)
+                if len(lp) == 0:
+                    currentNode.probability += transaction[i].probability
                 else:
-                    nn = max(temp) * transaction[i].probability
-                    currentNode.probability += round(nn, 2)
-        currentNode.TimeStamps = currentNode.TimeStamps + tid
+                    currentNode.probability += max(lp) * transaction[i].probability
 
-    def addConditionalPatterns(self, transaction, tid, sup, probability):
+    def addConditionalPattern(self, transaction, sup) -> None:
         """
-        Constructing conditional tree from prefixPaths
+        constructing conditional tree from prefixPaths
 
-        :param transaction : it represents the one transaction in database
+        :param transaction : it represents the one self.Database in database
         :type transaction : list
-        :param tid : timestamps of a pattern or transaction in tree
-        :param tid : list
         :param sup : support of prefixPath taken at last child of the path
         :type sup : int
+        :return: None
+
         """
+        # This method takes transaction, support and constructs the conditional tree
         currentNode = self.root
-        k = 0
         for i in range(len(transaction)):
-            k += 1
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
-                newNode.k = k
                 newNode.probability = sup
-                newNode.secondProbability = probability
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
-                currentNode.k = k
                 currentNode.probability += sup
-                currentNode.secondProbability = max(probability, currentNode.secondProbability)
-        currentNode.TimeStamps = currentNode.TimeStamps + tid
 
-    def conditionalPatterns(self, alpha):
+    def conditionalPatterns(self, alpha) -> tuple:
         """
-        Generates all the conditional patterns of respective node
+        generates all the conditional patterns of respective node
 
         :param alpha : it represents the Node in tree
-        :type alpha : Node
+        :type alpha : _Node
+
         """
+        # This method generates conditional patterns of node by traversing the tree
         finalPatterns = []
-        finalSets = []
         sup = []
-        prob = []
         for i in self.summaries[alpha]:
-            set1 = i.TimeStamps
             s = i.probability
-            p = i.secondProbability
             set2 = []
             while i.parent.item is not None:
                 set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
-                finalSets.append(set1)
                 sup.append(s)
-                prob.append(p)
-        finalPatterns, finalSets, support, prob, info = self.conditionalTransactions(finalPatterns, finalSets, sup, prob)
-        return finalPatterns, finalSets, support, prob, info
+        finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
+        return finalPatterns, support, info
+
+    def removeNode(self, nodeValue) -> None:
 
-    def removeNode(self, nodeValue):
         """
         Removing the node from tree
 
         :param nodeValue : it represents the node in tree
         :type nodeValue : node
+        :return: None
         """
+
         for i in self.summaries[nodeValue]:
-            i.parent.TimeStamps = i.parent.TimeStamps + i.TimeStamps
             del i.parent.children[nodeValue]
 
-    def getPeriodAndSupport(self, support, TimeStamps):
-        """
-        To calculate the periodicity of given timestamps
-
-        :param support: support of pattern
-        :param TimeStamps: timmeStamps of a pattern
-        :return: support and period
-        """
-        global _maxPer
-        global _lno
-        TimeStamps.sort()
-        cur = 0
-        per = 0
-        sup = support
-        for j in range(len(TimeStamps)):
-            per = max(per, TimeStamps[j] - cur)
-            if per > _maxPer:
-                return [0, 0]
-            cur = TimeStamps[j]
-        per = max(per, _lno - cur)
-        return [sup, per]
-
-    def conditionalTransactions(self, conditionalPatterns, conditionalTimeStamps, support, probability):
+    def conditionalTransactions(self, condPatterns, support) -> tuple:
         """
         It generates the conditional patterns with frequent items
 
-        :param conditionalPatterns : conditional patterns generated from conditionalPatterns() method for respective node
-        :type conditionalPatterns : list
-        :param conditionalTimeStamps : timestamps of respective conditional timestamps
-        :type conditionalTimeStamps : list
-        :param support : the support of conditional pattern in tree
-        :type support : list
+        :param condPatterns : conditionalPatterns generated from conditionalPattern method for respective node
+        :type condPatterns : list
+        :support : the support of conditional pattern in tree
+        :support : int
+        :return: tuple
         """
-        global _minSup, _maxPer, _lno
+        global _expSup, _expWSup
         pat = []
-        TimeStamps = []
         sup = []
-        prob = []
-        data1 = {}
         count = {}
-        for i in range(len(conditionalPatterns)):
-            for j in conditionalPatterns[i]:
-                if j in data1:
-                    data1[j] = data1[j] + conditionalTimeStamps[i]
+        for i in range(len(condPatterns)):
+            for j in condPatterns[i]:
+                if j in count:
                     count[j] += support[i]
                 else:
-                    data1[j] = conditionalTimeStamps[i]
                     count[j] = support[i]
         updatedDict = {}
-        for m in data1:
-            updatedDict[m] = self.getPeriodAndSupport(count[m], data1[m])
-        updatedDict = {k: v for k, v in updatedDict.items() if v[0] >= _minSup and v[1] <= _maxPer}
+        updatedDict = {k: v for k, v in count.items() if v >= _expSup}
         count = 0
-        for p in conditionalPatterns:
+        for p in condPatterns:
             p1 = [v for v in p if v in updatedDict]
-            trans = sorted(p1, key=lambda x: (updatedDict.get(x)[0]), reverse=True)
+            trans = sorted(p1, key=lambda x: updatedDict[x], reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
-                TimeStamps.append(conditionalTimeStamps[count])
                 sup.append(support[count])
-                prob.append(probability[count])
-            count += 1
-        return pat, TimeStamps, sup, prob, updatedDict
+                count += 1
+        return pat, sup, updatedDict
 
-    def generatePatterns(self, prefix, periodic):
+    def generatePatterns(self, prefix) -> None:
         """
         Generates the patterns
 
         :param prefix : forms the combination of items
         :type prefix : list
+        :return: None
         """
-        global _minSup
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0])):
+
+        global _finalPatterns, _expSup, _expWSup, _weights
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
             pattern = prefix[:]
             pattern.append(i)
-            s = 0
-            secProb = []
-            kk = int()
-            for x in self.summaries[i]:
-                if x.k <= 2:
-                    s += x.probability
-                elif x.k >= 3:
-                    n = x.probability * pow(x.secondProbability, (x.k - 2))
-                    s += n
-            periodic[tuple(pattern)] = self.info[i]
-            periodic[tuple(pattern)] = self.info[i]
-            if s >= _minSup:
-                periodic[tuple(pattern)] = self.info[i]
-                patterns, TimeStamps, support, probability, info = self.conditionalPatterns(i)
+            weight = 0
+            for k in pattern:
+                weight = weight + _weights[k]
+            weight = weight/len(pattern)
+            if self.info.get(i) >= _expSup and self.info.get(i) * weight >= _expWSup:
+                _finalPatterns[tuple(pattern)] = self.info.get(i)
+                patterns, support, info = self.conditionalPatterns(i)
                 conditionalTree = _Tree()
                 conditionalTree.info = info.copy()
                 for pat in range(len(patterns)):
-                    conditionalTree.addConditionalPatterns(patterns[pat], TimeStamps[pat], support[pat], probability[pat])
+                    conditionalTree.addConditionalPattern(patterns[pat], support[pat])
                 if len(patterns) > 0:
-                    conditionalTree.generatePatterns(pattern, periodic)
+                    conditionalTree.generatePatterns(pattern)
             self.removeNode(i)
 
-class UPFPGrowthPlus(_ab._periodicFrequentPatterns):
+class WUFIM(_ab._weightedFrequentPatterns):
     """
-    :Description: Basic Plus is  to discover periodic-frequent patterns in a uncertain temporal database.
+    :Description: It is one of the algorithm to discover weighted frequent patterns in a uncertain transactional database using PUF-Tree.
+
+    :Reference: Efficient Mining of Weighted Frequent Itemsets in Uncertain Databases, In book: Machine Learning and Data Mining in Pattern Recognition Chun-Wei Jerry Lin, Wensheng Gan, Philippe Fournier Viger, Tzung-Pei Hong
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Weighted Uncertain Periodic Frequent Patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Weighted  Uncertain Periodic Frequent Patterns
+    :param  minSup: str:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  wFile: str :
+                    This is a weighted file.
 
-    :Reference:
-          Palla Likhitha, Rage Veena,Rage Uday Kiran, Koji Zettsu, Masashi Toyoda, Philippe Fournier-Viger, (2023). 
-          UPFP-growth++: An Efficient Algorithm to Find Periodic-Frequent Patterns in Uncertain Temporal Databases. 
-          ICONIP 2022. Communications in Computer and Information Science, vol 1792. Springer, Singapore.
-          https://doi.org/10.1007/978-981-99-1642-9_16
 
     :Attributes:
 
-        iFile: file
-            Name of the Input file or path of input file
-        oFile: file
-            Name of the output file or path of output file
-        minSup: int or float or str
+        iFile : file
+            Name of the Input file or path of the input file
+        wFile : file
+            Name of the Input file or path of the input file
+        oFile : file
+            Name of the output file or path of the output file
+        minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer: int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
-        sep: str
+        sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        memoryUSS: float
+        memoryUSS : float
             To store the total amount of USS memory consumed by the program
-        memoryRSS: float
+        memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime: float
+        startTime:float
             To record the start time of the mining process
-        endTime: float
+        endTime:float
             To record the completion time of the mining process
-        Database: list
+        Database : list
             To store the transactions of a database in list
-        mapSupport: Dictionary
+        mapSupport : Dictionary
             To maintain the information of item and their frequency
-        lno: int
+        lno : int
             To represent the total no of transaction
-        tree: class
+        tree : class
             To represents the Tree class
-        itemSetCount: int
+        itemSetCount : int
             To represents the total no of patterns
-        finalPatterns: dict
+        finalPatterns : dict
             To store the complete patterns
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        savePatterns(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to a output file
+        save(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
-            Complete set of periodic-frequent patterns will be loaded in to a dataframe
+            Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         creatingItemSets(fileName)
             Scans the dataset and stores in a list format
-        updateDatabases()
-            Update the database by removing aperiodic items and sort the Database by item decreased support
+        frequentOneItem()
+            Extracts the one-length frequent patterns from database
+        updateTransactions()
+            Update the transactions by removing non-frequent items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
-        PeriodicFrequentOneItems()
-            To extract the one-length periodic-frequent items
+        startMine()
+            Mining process will start from this function
 
-    **Executing the code on terminal**:
+    **Methods to execute code on terminal**
     --------------------------------------------
-            Format:
-                >>> python3 UPFPGrowthPlus.py <inputFile> <outputFile> <minSup> <maxPer>
+    .. code-block:: console
+
+
+      Format:
+
+      (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup>
+
+      Example Usage:
+
+      (.venv) $ python3 basic.py sampleTDB.txt patterns.txt 3
+
+
+              .. note:: minSup  will be considered in support count or frequency
 
-            Examples:
-                >>> python3 UPFPGrowthPlus.py sampleTDB.txt patterns.txt 0.3 4
 
     **Importing this algorithm into a python program**
-    -----------------------------------------------------------------
+    -----------------------------------------------------
     .. code-block:: python
 
-            from PAMI.uncertainPeriodicFrequentPattern import UPFPGrowthPlus as alg
+            from PAMI.weightedUncertainFrequentPattern.basic import basic as alg
 
-            obj = alg.UPFPGrowthPlus(iFile, minSup, maxPer)
+            obj = alg.basic(iFile, wFile, expSup, expWSup)
 
             obj.startMine()
 
-            periodicFrequentPatterns = obj.getPatterns()
+            Patterns = obj.getPatterns()
 
-            print("Total number of uncertain Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+            print("Total number of  Patterns:", len(Patterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
@@ -496,346 +434,419 @@
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
-
-    **Credits**:
-    --------------
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
-
-        """
+   """
     _startTime = float()
     _endTime = float()
-    _minSup = float()
-    _maxPer = float()
+    _minSup = str()
     _finalPatterns = {}
     _iFile = " "
+    _wFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
     _rank = {}
-    _lno = 0
-    _periodic = {}
+    _expSup = float()
+    _expWSup = float()
+
+    def __init__(self, iFile, wFile, expSup, expWSup, sep='\t') -> None:
+        super().__init__(iFile, wFile, expSup, expWSup, sep)
 
-    def _creatingItemSets(self):
+    def _creatingItemSets(self) -> None:
         """
-        Storing the complete transactions of the database/input file in a database variable
+        Scans the uncertain transactional dataset
+        :return: None
         """
-
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data, ts = [], [], []
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
+                self._Database = self._iFile['Transactions'].tolist()
             if 'uncertain' in i:
                 uncertain = self._iFile['uncertain'].tolist()
             for k in range(len(data)):
-                tr = [ts[k]]
-                for j in range(len(k)):
+                tr = []
+                for j in range(len(data[k])):
                     product = _Item(data[k][j], uncertain[k][j])
                     tr.append(product)
                 self._Database.append(tr)
-                self._lno += 1
 
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line = line.decode("utf-8")
                     line = line.strip()
                     line = [i for i in line.split(':')]
                     temp1 = [i.rstrip() for i in line[0].split(self._sep)]
                     temp2 = [i.rstrip() for i in line[1].split(self._sep)]
                     temp1 = [x for x in temp1 if x]
                     temp2 = [x for x in temp2 if x]
-                    tr = [int(temp1[0])]
-                    for i in range(len(temp1[1:])):
+                    tr = []
+                    for i in range(len(temp1)):
                         item = temp1[i]
                         probability = float(temp2[i])
                         product = _Item(item, probability)
                         tr.append(product)
-                    self._lno += 1
                     self._Database.append(tr)
             else:
                 try:
-                    count = 0
                     with open(self._iFile, 'r') as f:
                         for line in f:
                             line = line.strip()
                             line = [i for i in line.split(':')]
                             temp1 = [i.rstrip() for i in line[0].split(self._sep)]
                             temp2 = [i.rstrip() for i in line[1].split(self._sep)]
                             temp1 = [x for x in temp1 if x]
                             temp2 = [x for x in temp2 if x]
-                            tr = [int(temp1[0])]
-                            for i in range(len(temp1[1:])):
+                            tr = []
+                            for i in range(len(temp1)):
                                 item = temp1[i]
                                 probability = float(temp2[i])
                                 product = _Item(item, probability)
                                 tr.append(product)
-                            self._lno += 1
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
 
-    def _PeriodicFrequentOneItems(self):
+    def _scanningWeights(self) -> None:
+        """
+        Scans the uncertain transactional dataset
+        :return: None
+        """
+        self._weights = {}
+        if isinstance(self._wFile, _ab._pd.DataFrame):
+            weights, data = [], []
+            if self._wFile.empty:
+                print("its empty..")
+            i = self._wFile.columns.values.tolist()
+            if 'items' in i:
+                data = self._wFile['items'].tolist()
+            if 'weights' in i:
+                weights = self._wFile['weights'].tolist()
+            for k in range(len(data)):
+                self._weights[data[k]] = int(float(weights[k]))
+
+            # print(self.Database)
+        if isinstance(self._wFile, str):
+            if _ab._validators.url(self._wFile):
+                data = _ab._urlopen(self._wFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._weights[temp[0]] = int(float(temp[1]))
+            else:
+                try:
+                    with open(self._wFile, 'r') as f:
+                        for line in f:
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._weights[temp[0]] = float(temp[1])
+                except IOError:
+                    print("File Not Found")
+
+    def _frequentOneItem(self) -> tuple:
         """
-        Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+
+        :param self.Database : it represents the one self.Database in database
+        :type self.Database : list
+        :return: tuple
         """
-        global first, last
+
         mapSupport = {}
         for i in self._Database:
-            n = int(i[0])
-            for j in i[1:]:
+            for j in i:
                 if j.item not in mapSupport:
-                    mapSupport[j.item] = [round(j.probability, 3), abs(0 - n), n]
+                    if self._weights.get(j.item) is not None:
+                        mapSupport[j.item] = [j.probability, self._weights[j.item]]
                 else:
-                    mapSupport[j.item][0] += round(j.probability, 2)
-                    mapSupport[j.item][1] = max(mapSupport[j.item][1], abs(n - mapSupport[j.item][2]))
-                    mapSupport[j.item][2] = n
-        for key in mapSupport:
-            mapSupport[key][1] = max(mapSupport[key][1], self._lno - mapSupport[key][2])
-        mapSupport = {k: [round(v[0], 2), v[1]] for k, v in mapSupport.items() if
-                      v[1] <= self._maxPer and v[0] >= self._minSup}
-        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
-        self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
+                    mapSupport[j.item][0] += j.probability
+        mapSupport = {k: v[0] for k, v in mapSupport.items() if v[0] >= self._expSup and v[0] * v[1] >= self._expWSup}
+        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self.rank = dict([(index, item) for (item, index) in enumerate(plist)])
         return mapSupport, plist
 
-    def _buildTree(self, data, info):
+    @staticmethod
+    def _buildTree(data, info) -> _Tree:
         """
-        It takes the transactions and support of each item and construct the main tree with setting root node as null
-
-        :param data : it represents the one transaction in database
+        It takes the self.Database and support of each item and construct the main tree with setting root node as null
+        :param data : it represents the one self.Database in database
         :type data : list
         :param info : it represents the support of each item
         :type info : dictionary
+        :return: tree
         """
+
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
-            set1 = [data[i][0]]
-            rootNode.addTransaction(data[i][1:], set1)
-            #printTree(rootNode)
-            #print("....")
+            rootNode.addTransaction(data[i])
         return rootNode
 
-    def _updateTransactions(self, dict1):
+    def _updateTransactions(self, dict1) -> list:
         """
-        Remove the items which are not frequent from transactions and updates the transactions with rank of items
+        Remove the items which are not frequent from self.Database and updates the self.Database with rank of items
 
         :param dict1 : frequent items with support
         :type dict1 : dictionary
+        :return: list
         """
         list1 = []
         for tr in self._Database:
-            list2 = [int(tr[0])]
-            for i in range(1, len(tr)):
+            list2 = []
+            for i in range(0, len(tr)):
                 if tr[i].item in dict1:
                     list2.append(tr[i])
             if len(list2) >= 2:
-                basket = list2[1:]
-                basket.sort(key=lambda val: self._rank[val.item])
-                list2[1:] = basket[0:]
+                basket = list2
+                basket.sort(key=lambda val: self.rank[val.item])
+                list2 = basket
                 list1.append(list2)
         return list1
 
-    def _Check(self, i, x):
+    @staticmethod
+    def _check(i, x) -> int:
         """
         To check the presence of item or pattern in transaction
 
         :param x: it represents the pattern
         :type x : list
-        :param i : represents the uncertain transactions
+        :param i : represents the uncertain self.Database
         :type i : list
+        :return: integer number
         """
+
+        # This method taken a transaction as input and returns the tree
         for m in x:
             k = 0
             for n in i:
                 if m == n.item:
                     k += 1
             if k == 0:
                 return 0
         return 1
 
-    def _convert(self, value):
+    def _convert(self, value) -> float:
         """
+        To convert the type of user specified minSup value
 
-        To convert the given user specified value
-
-        :param value: user specified value
-        :return: converted value
+        :param value: user specified minSup value
+        :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = float(value)
+            value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
-                value = float(value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _removeFalsePositives(self):
+    def _removeFalsePositives(self) -> None:
         """
-        To remove false positives in generated patterns
-        :return: original patterns
+        To remove the false positive patterns generated in frequent patterns.
+        :return: patterns with accurate probability
         """
+        global _finalPatterns
         periods = {}
         for i in self._Database:
-            for x, y in self._periodic.items():
+            for x, y in _finalPatterns.items():
                 if len(x) == 1:
                     periods[x] = y
                 else:
                     s = 1
-                    check = self._Check(i[1:], x)
+                    check = self._check(i, x)
                     if check == 1:
-                        for j in i[1:]:
+                        for j in i:
                             if j.item in x:
                                 s *= j.probability
                         if x in periods:
-                            periods[x][0] += s
+                            periods[x] += s
                         else:
-                            periods[x] = [s, y[1]]
-        count = 0
+                            periods[x] = s
         for x, y in periods.items():
-            if y[0] >= _minSup:
-                count += 1
+            weight = 0
+            for i in x:
+                weight += self._weights[i]
+            weight = weight / len(x)
+            if weight * y >= self._expWSup:
                 sample = str()
                 for i in x:
-                    sample = sample + i + " "
+                    sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
-        #print("Total false patterns generated:", len(self._periodic) - count)
 
-    def startMine(self):
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        startMine() method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns.
         """
-        global _minSup, _maxPer, _first, _last, _lno
+        global _expSup, _expWSup, _weights, _finalPatterns
         self._startTime = _ab._time.time()
+        self._Database, self._weights = [], {}
         self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
+        self._scanningWeights()
+        _weights = self._weights
+        self._expSup = float(self._expSup)
+        self._expWSup = float(self._expWSup)
+        _expSup = self._expSup
+        _expWSup = self._expWSup
         self._finalPatterns = {}
-        _minSup, _maxPer, _lno = self._minSup, self._maxPer, len(self._Database)
-        mapSupport, plist = self._PeriodicFrequentOneItems()
-        updatedTrans = self._updateTransactions(mapSupport)
+        mapSupport, plist = self._frequentOneItem()
+        self.Database1 = self._updateTransactions(mapSupport)
         info = {k: v for k, v in mapSupport.items()}
-        root = self._buildTree(updatedTrans, info)
-        self._periodic = {}
-        root.generatePatterns([], self._periodic)
+        Tree1 = self._buildTree(self.Database1, info)
+        Tree1.generatePatterns([])
         self._removeFalsePositives()
-        print("Periodic Frequent patterns were generated successfully using UPFP-Growth++ algorithm")
+        print("Weighted Frequent patterns were generated  successfully using basic algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
-        self._memoryRSS = float()
+        self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self):
+    def mine(self) -> None:
         """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        mine() method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patternS
+        """
+        global _expSup, _expWSup, _weights, _finalPatterns
+        self._startTime = _ab._time.time()
+        self._Database, self._weights = [], {}
+        self._creatingItemSets()
+        self._scanningWeights()
+        _weights = self._weights
+        self._expSup = float(self._expSup)
+        self._expWSup = float(self._expWSup)
+        _expSup = self._expSup
+        _expWSup = self._expWSup
+        self._finalPatterns = {}
+        mapSupport, plist = self._frequentOneItem()
+        self.Database1 = self._updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        Tree1 = self._buildTree(self.Database1, info)
+        Tree1.generatePatterns([])
+        self._removeFalsePositives()
+        print("Weighted Frequent patterns were generated  successfully using basic algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self.memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self.memoryRSS = process.memory_info().rss
 
+    def getMemoryUSS(self) -> float:
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
+        return self.memoryRSS
 
-        return self._memoryRSS
-
-    def getRuntime(self):
+    def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
+
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
+    def getPatternsAsDataFrame(self) -> pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
-
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+            s = str()
+            for i in a:
+                s = s + i + " "
+            data.append([s, b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
 
-        :param outFile: name of the output file
+        :param outFile: Specify name of the output file
         :type outFile: csv file
+        :return: None
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
+        self.oFile = outFile
+        writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y)
+            s = str()
+            for i in x:
+                s = s + i + "\t"
+            s1 = s.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
+    def getPatterns(self) -> dict:
         """
         Function to send the set of frequent patterns after completion of the mining process
-
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self):
+    def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
-        print("Total number of  Uncertain Periodic-Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Weighted Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
+        if len(_ab._sys.argv) == 7:
+            _ap = WUFIM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         if len(_ab._sys.argv) == 6:
-            _ap = UPFPGrowthPlus(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = UPFPGrowthPlus(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = WUFIM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Patterns:", len(_Patterns))
-        _ap.savePatterns(_ab._sys.argv[2])
-        # print(ap.getPatternsAsDataFrame())
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total number of Weighted Uncertain Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
+        for k in [120, 140, 160, 180, 200]:
+            _ap = WUFIM('/Users/likhitha/Downloads/uncertainTransaction_T10I4D200K.csv', '/Users/likhitha/Downloads/T10_weights.txt',
+                        k, 500, '\t')
+            _ap.startMine()
+            print("Total number of Weighted Uncertain Frequent Patterns:", len(_ap.getPatterns()))
+            _ap.save('/Users/likhitha/Downloads/WUFIM_output.txt')
+            print("Total Memory in USS:", _ap.getMemoryUSS())
+            print("Total Memory in RSS", _ap.getMemoryRSS())
+            print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.3.9.2/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py` & `pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # SWFPGrowth is an algorithm to mine the weighted spatial frequent patterns in spatiotemporal databases.
 #
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------
+# -------------------------------------------------------
 #
 #             from PAMI.weightFrequentNeighbourhoodPattern.basic import SWFPGrowth as alg
 #
-#             obj = alg.SWFPGrowth(iFile, wFile, nFile, minSup, minWeight, seperator)
+#             obj = alg.SWFPGrowth(iFile, wFile, nFile, minSup, minWeight, sep)
 #
 #             obj.startMine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -27,14 +27,16 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -47,15 +49,17 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.weightedFrequentNeighbourhoodPattern.basic import abstract as _fp
-from typing import List, Dict, Tuple, Set, Union, Any, Generator, Iterable
+import pandas as pd
+from deprecated import deprecated
+from typing import List, Dict, Tuple, Union, Iterable
 
 _minWS = str()
 _weights = {}
 _rank = {}
 _neighbourList = {}
 
 _fp._sys.setrecursionlimit(20000)
@@ -110,14 +114,15 @@
     def addChild(self, node: '_Node') -> None:
         """
         Retrieving the child from the tree
 
         :param node: Children node.
         :type node: Node
         :return: Updates the children nodes and parent nodes
+        :return: None
 
         """
         self.children[node.itemId] = node
         node.parent = self
 
 
 class _Tree:
@@ -154,14 +159,15 @@
         """
         Adding transaction into tree
 
         :param transaction: it represents the one transaction in database
         :type transaction: list
         :param count: frequency of item
         :type count: int
+        :return: None
         """
 
         # This method takes transaction as input and returns the tree
         global _neighbourList, _rank
         currentNode = self.root
         for i in range(len(transaction)):
             wei = 0
@@ -188,14 +194,15 @@
         """
         Adding transaction into tree
 
         :param transaction: it represents the one transaction in database
         :type transaction: list
         :param count: frequency of item
         :type count: int
+        :return : None
         """
         # This method takes transaction as input and returns the tree
         global _neighbourList, _rank
         currentNode = self.root
         for i in range(len(transaction)):
             wei = 0
             l1 = i
@@ -316,14 +323,26 @@
     :Description: SWFPGrowth is an algorithm to mine the weighted spatial frequent patterns in spatiotemporal databases.
 
     :Reference:
         R. Uday Kiran, P. P. C. Reddy, K. Zettsu, M. Toyoda, M. Kitsuregawa and P. Krishna Reddy,
         "Discovering Spatial Weighted Frequent Itemsets in Spatiotemporal Databases," 2019 International
         Conference on Data Mining Workshops (ICDMW), 2019, pp. 987-996, doi: 10.1109/ICDMW.2019.00143.
 
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of weighted Frequent Neighbourhood Patterns.
+    :param  oFile: str :
+                   Name of the output file to store complete set of weighted Frequent Neighbourhood Patterns.
+    :param  minSup: int or str or float:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  maxper: floot :
+                   where maxPer represents the maximum periodicity threshold value specified by the user.
+
+
     :Attributes:
 
         iFile : file
             Input file name or path of the input file
         minWS: float or int or str
             The user can specify minWS either in count or proportion of database size.
             If the program detects the data type of minWS is integer, then it treats minWS is expressed in count.
@@ -377,21 +396,28 @@
         creatingItemSets()
             Scans the dataset or dataframes and stores in list format
         frequentOneItem()
             Extracts the one-frequent patterns from transactions
 
     **Methods to execute code on terminal**
     -------------------------------------------
-            Format:
-                      >>>  python3 SWFPGrowth.py <inputFile> <weightFile> <outputFile> <minSup> <minWeight>
+    .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 SWFPGrowth.py <inputFile> <weightFile> <outputFile> <minSup> <minWeight>
 
-            Example:
-                      >>>  python3 SWFPGrowth.py sampleDB.txt weightFile.txt patterns.txt 10  2
+       Example usage :
+
+       (.venv) $ python3 SWFPGrowth.py sampleDB.txt weightFile.txt patterns.txt 10  2
+
+
+               .. note:: minSup will be considered in support count or frequency
 
-                     .. note:: minSup will be considered in support count or frequency
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
     .. code-block:: python
 
             from PAMI.weightFrequentNeighbourhoodPattern.basic import SWFPGrowth as alg
 
@@ -445,14 +471,15 @@
 
     def __init__(self, iFile: Union[str, _fp._pd.DataFrame], nFile: Union[str, _fp._pd.DataFrame], minWS: Union[int, float, str], sep='\t') -> None:
         super().__init__(iFile, nFile, minWS, sep)
 
     def __creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
@@ -537,14 +564,15 @@
             else:
                 value = int(value)
         return value
 
     def __frequentOneItem(self) -> List[str]:
         """
         Generating One frequent items sets
+        :return: None
         """
         global _maxWeight
         self._mapSupport = {}
         for tr in self._Database:
             for i in tr:
                 nn = [j for j in tr if j.item in self._neighbourList[i.item]]
                 if i.item not in self._mapSupport:
@@ -560,14 +588,15 @@
 
     def __updateTransactions(self, itemSet: List[str]) -> List[List[_WeightedItem]]:
         """
         Updates the items in transactions with rank of items according to their support
         :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
                   rank = {'a':0, 'b':1, 'c':2, 'd':3}
         :param itemSet: list of one-frequent items
+        :return: list
         """
         list1 = []
         for tr in self._Database:
             list2 = []
             for i in range(len(tr)):
                 if tr[i].item in itemSet:
                     list2.append(tr[i])
@@ -601,17 +630,61 @@
 
         """
         temp = str()
         for i in itemSet:
             temp = temp + self.__rankDup[i] + "\t"
         return temp
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         main program to start the operation
+        :return : None
+
+        """
+        global _minWS, _neighbourList, _rank
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minWS is None:
+            raise Exception("Please enter the Minimum Support")
+        self.__creatingItemSets()
+        self._scanNeighbours()
+        self._minWS = self.__convert(self._minWS)
+        _minWS = self._minWS
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        info = {self.__rank[k]: v for k, v in self._mapSupport.items()}
+        _rank = self.__rank
+        for x, y in self.__rank.items():
+            self.__rankDup[y] = x
+        _neighbourList = self._neighbourList
+        #self._neighbourList = {k:v for k, v in self._neighbourList.items() if k in self._mapSupport.keys()}
+        # for x, y in self._neighbourList.items():
+        #     xx = [self.__rank[i] for i in y if i in self._mapSupport.keys()]
+        #     _neighbourList[self.__rank[x]] = xx
+        # print(_neighbourList)
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent patterns were generated successfully using SWFPGrowth algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
+    def Mine(self) -> None:
+        """
+        main program to start the operation
+        :return : None
 
         """
         global _minWS, _neighbourList, _rank
         self.__startTime = _fp._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._minWS is None:
@@ -694,14 +767,15 @@
 
     def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self.__finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
@@ -713,14 +787,15 @@
         :rtype: dict
         """
         return self.__finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
         print("Total number of  Weighted Spatial Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
```

### Comparing `pami-2024.3.9.2/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/weightedFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/weightedFrequentRegularPattern/basic/WFRIMiner.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/PPPClose.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,38 +1,39 @@
-# WFRIMiner is one of the fundamental algorithm to discover weighted frequent regular patterns in a transactional database.
-# It stores the database in compressed WFRI-tree decreasing the memory usage and extracts the patterns from tree.It employs downward closure property to  reduce the search space effectively.
-#
+
+
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#             from PAMI.weightedFrequentRegularPattern.basic import WFRIMiner as alg
+#             from PAMI.partialPeriodicPattern.closed import PPPClose as alg
 #
-#             obj = alg.WFRIMiner(iFile, WS, regularity)
+#             obj = alg.PPPClose("../basic/sampleTDB.txt", "2", "6")
 #
 #             obj.startMine()
 #
-#             weightedFrequentRegularPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
+#
+#             print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+#
+#             obj.save("patterns")
 #
-#              print("Total number of Frequent Patterns:", len(weightedFrequentRegularPatterns))
+#             Df = obj.getPatternsAsDataFrame()
 #
-#              obj.save(oFile)
+#             memUSS = obj.getMemoryUSS()
 #
-#              Df = obj.getPatternInDataFrame()
+#             print("Total Memory in USS:", memUSS)
 #
-#              memUSS = obj.getMemoryUSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#              print("Total Memory in USS:", memUSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#              memRSS = obj.getMemoryRSS()
+#             run = obj.getRuntime()
 #
-#              print("Total Memory in RSS", memRSS)
+#             print("Total ExecutionTime in seconds:", run)
 #
-#              run = obj.getRuntime()
 #
-#              print("Total ExecutionTime in seconds:", run)
 #
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
@@ -46,709 +47,585 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from PAMI.weightedFrequentRegularPattern.basic import abstract as _fp
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
-
-_WS = str()
-_regularity = str()
-_lno = int()
-_weights = {}
-_wf = {}
-_fp._sys.setrecursionlimit(20000)
-
-
-class _Node:
-    """
-    A class used to represent the node of frequentPatternTree
-
-    :Attributes:
-        itemId: int
-            storing item of a node
-        counter: int
-            To maintain the support of node
-        parent: node
-            To maintain the parent of node
-        children: list
-            To maintain the children of node
-
-    :Methods:
-        addChild(node)
-            Updates the nodes children list and parent for the given node
-
-    """
-
-    def __init__(self, item: int, children: dict) -> None:
-        """
-        Initializing the Node class
-
-        :param item: Storing the item of a node
-        :type item: int or None
-        :param children: To maintain the children of a node
-        :type children: dict
-        """
 
-        self.item = item
-        self.children = children
-        self.parent = None
-        self.timeStamps = []
 
-    def addChild(self, node) -> None:
-        """
-        To add the children to a node
-
-        :param node: parent node in the tree
-        """
+import sys as _sys
+import validators as _validators
+from urllib.request import urlopen as _urlopen
+from PAMI.partialPeriodicPattern.closed import abstract as _abstract
 
-        self.children[node.item] = node
-        node.parent = self
 
+from PAMI.partialPeriodicPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
 
-class _Tree:
+class PPPClose(_abstract._partialPeriodicPatterns):
     """
-    A class used to represent the frequentPatternGrowth tree structure
-
-    :Attributes:
-        root : Node
-            The first node of the tree set to Null.
-        summaries : dictionary
-            Stores the nodes itemId which shares same itemId
-        info : dictionary
-            frequency of items in the transactions
-
-    :Methods:
-        addTransaction(transaction, freq)
-            adding items of  transactions into the tree as nodes and freq is the count of nodes
-        getFinalConditionalPatterns(node)
-            getting the conditional patterns from fp-tree for a node
-        getConditionalPatterns(patterns, frequencies)
-            sort the patterns by removing the items with lower minSup
-        generatePatterns(prefix)
-            generating the patterns from fp-tree
-    """
-
-    def __init__(self) -> None:
-        self.root = _Node(None, {})
-        self.summaries = {}
-        self.info = {}
-
-    def addTransaction(self, transaction: list, tid: list) -> None:
-        """
-        Adding a transaction into tree
-
-        :param transaction: To represent the complete database
-        :type transaction: list
-        :param tid: To represent the timestamp of a database
-        :type tid: list
-        :return: pfp-growth tree
-        """
-        currentNode = self.root
-        for i in range(len(transaction)):
-            if transaction[i] not in currentNode.children:
-                newNode = _Node(transaction[i], {})
-                currentNode.addChild(newNode)
-                if transaction[i] in self.summaries:
-                    self.summaries[transaction[i]].append(newNode)
-                else:
-                    self.summaries[transaction[i]] = [newNode]
-                currentNode = newNode
-            else:
-                currentNode = currentNode.children[transaction[i]]
-        currentNode.timeStamps = currentNode.timeStamps + tid
-
-    def getConditionalPatterns(self, alpha, pattern) -> tuple:
-        """
-        Generates all the conditional patterns of a respective node
-
-        :param alpha: To represent a Node in the tree
-        :type alpha: Node
-        :param pattern: prefix of the pattern
-        :type alpha: list
-        :return: A tuple consisting of finalPatterns, conditional pattern base and information
-        """
-        finalPatterns = []
-        finalSets = []
-        for i in self.summaries[alpha]:
-            set1 = i.timeStamps
-            set2 = []
-            while i.parent.item is not None:
-                set2.append(i.parent.item)
-                i = i.parent
-            if len(set2) > 0:
-                set2.reverse()
-                finalPatterns.append(set2)
-                finalSets.append(set1)
-        finalPatterns, finalSets, info = self.conditionalDatabases(finalPatterns, finalSets, pattern)
-        return finalPatterns, finalSets, info
+    :Description:
 
-    @staticmethod
-    def generateTimeStamps(node) -> list:
-        """
-        To get the timestamps of a node
-
-        :param node: A node in the tree
-        :return: Timestamps of a node
-        """
-
-        finalTimeStamps = node.timeStamps
-        return finalTimeStamps
-
-    def removeNode(self, nodeValue) -> None:
-        """
-        Removing the node from tree
-
-        :param nodeValue: To represent a node in the tree
-        :type nodeValue: node
-        :return: Tree with their nodes updated with timestamps
-        """
-
-        for i in self.summaries[nodeValue]:
-            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
-            del i.parent.children[nodeValue]
-
-    def getTimeStamps(self, alpha) -> list:
-        """
-        To get all the timestamps of the nodes which share same item name
-
-        :param alpha: Node in a tree
-        :return: Timestamps of a  node
-        """
-        temporary = []
-        for i in self.summaries[alpha]:
-            temporary += i.timeStamps
-        return temporary
-
-    @staticmethod
-    def getSupportAndPeriod(timeStamps: list, pattern: list) -> list:
-        """
-        To calculate the periodicity and support
-
-        :param timeStamps: Timestamps of an item set
-        :type timeStamps: list
-        :param pattern: pattern to evaluate the weighted frequent regular or not
-        :type pattern: list
-        :return: support, periodicity
-        """
-        global _WS, _regularity, _lno, _weights
-        timeStamps.sort()
-        cur = 0
-        per = list()
-        sup = 0
-        for j in range(len(timeStamps)):
-            per.append(timeStamps[j] - cur)
-            cur = timeStamps[j]
-            sup += 1
-        per.append(_lno - cur)
-        l = int()
-        for i in pattern:
-            l = l + _weights[i]
-        wf = (l / (len(pattern))) * sup
-        if len(per) == 0:
-            return [0, 0]
-        return [sup, max(per), wf]
-
-    def conditionalDatabases(self, conditionalPatterns: list, conditionalTimeStamps: list, pattern: list) -> tuple:
-        """
-        It generates the conditional patterns with periodic-frequent items
-
-        :param conditionalPatterns: conditionalPatterns generated from conditionPattern method of a respective node
-        :type conditionalPatterns: list
-        :param conditionalTimeStamps: Represents the timestamps of a conditional patterns of a node
-        :type conditionalTimeStamps: list
-        :param pattern: prefix of the pattern
-        :type pattern: list
-        :returns: Returns conditional transactions by removing non-periodic and non-frequent items
-        """
-        global _WS, _regularity
-        pat = []
-        timeStamps = []
-        data1 = {}
-        for i in range(len(conditionalPatterns)):
-            for j in conditionalPatterns[i]:
-                if j in data1:
-                    data1[j] = data1[j] + conditionalTimeStamps[i]
-                else:
-                    data1[j] = conditionalTimeStamps[i]
-        updatedDictionary = {}
-        for m in data1:
-            updatedDictionary[m] = self.getSupportAndPeriod(data1[m], pattern + [m])
-        updatedDictionary = {k: v for k, v in updatedDictionary.items() if v[0] >= _WS and v[1] <= _regularity}
-        count = 0
-        for p in conditionalPatterns:
-            p1 = [v for v in p if v in updatedDictionary]
-            trans = sorted(p1, key=lambda x: (updatedDictionary.get(x)[0], -x), reverse=True)
-            if len(trans) > 0:
-                pat.append(trans)
-                timeStamps.append(conditionalTimeStamps[count])
-            count += 1
-        return pat, timeStamps, updatedDictionary
-
-    def generatePatterns(self, prefix: list) -> None:
-        """
-        Generates the patterns
-
-        :param prefix: Forms the combination of items
-        :type prefix: list
-        :returns: yields patterns with their support and periodicity
-        """
-        global _WS
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0], -x)):
-            pattern = prefix[:]
-            pattern.append(i)
-            if self.info[i][2] >= _WS:
-                yield pattern, self.info[i]
-                patterns, timeStamps, info = self.getConditionalPatterns(i, pattern)
-                conditionalTree = _Tree()
-                conditionalTree.info = info.copy()
-                for pat in range(len(patterns)):
-                    conditionalTree.addTransaction(patterns[pat], timeStamps[pat])
-                if len(patterns) > 0:
-                    for q in conditionalTree.generatePatterns(pattern):
-                        yield q
-            self.removeNode(i)
+    PPPClose algorithm is used to discover the closed partial periodic patterns in temporal databases.
+    It uses depth-first search.
 
+    :Reference: R. Uday Kiran1 , J. N. Venkatesh2 , Philippe Fournier-Viger3 , Masashi Toyoda1 , P. Krishna Reddy2 and Masaru Kitsuregawa
+                 https://www.tkl.iis.u-tokyo.ac.jp/new/uploads/publication_file/file/799/PAKDD.pdf
 
-class WFRIMiner(_fp._weightedFrequentRegularPatterns):
-    """
-    :Description: WFRIMiner is one of the fundamental algorithm to discover weighted frequent regular patterns in a transactional database.
-       * It stores the database in compressed WFRI-tree decreasing the memory usage and extracts the patterns from tree.It employs downward closure property to  reduce the search space effectively.
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of periodic frequent pattern's
+    :param  oFile: str :
+                   Name of the output file to store complete set of periodic frequent pattern's
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent pattern's
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  period: float:
+                   Minimum partial periodic...
+    :param  periodicSupport: float:
+                   Minimum partial periodic...
 
-    :Reference:
-           K. Klangwisan and K. Amphawan, "Mining weighted-frequent-regular itemsets from transactional database,"
-           2017 9th International Conference on Knowledge and Smart Technology (KST), 2017, pp. 66-71,
-           doi: 10.1109/KST.2017.7886090.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
-        iFile : file
+        iFile : str
             Input file name or path of the input file
-        WS: float or int or str
-            The user can specify WS either in count or proportion of database size.
-            If the program detects the data type of WS is integer, then it treats WS is expressed in count.
+        oFile : str
+            Name of the output file or path of the input file
+        periodicSupport: int or float or str
+            The user can specify periodicSupport either in count or proportion of database size.
+            If the program detects the data type of periodicSupport is integer, then it treats periodicSupport is expressed in count.
             Otherwise, it will be treated as float.
-            Example: WS=10 will be treated as integer, while WS=10.0 will be treated as float
-        regularity: float or int or str
-            The user can specify regularity either in count or proportion of database size.
-            If the program detects the data type of regularity is integer, then it treats regularity is expressed in count.
+            Example: periodicSupport=10 will be treated as integer, while periodicSupport=10.0 will be treated as float
+        period: int or float or str
+            The user can specify period either in count or proportion of database size.
+            If the program detects the data type of period is integer, then it treats period is expressed in count.
             Otherwise, it will be treated as float.
-            Example: regularity=10 will be treated as integer, while regularity=10.0 will be treated as float
+            Example: period=10 will be treated as integer, while period=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator.
-        oFile : file
-            Name of the output file or the path of the output file
         startTime:float
             To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
+        finalPatterns: dict
+            Storing the complete set of patterns in a dictionary variable
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        Database : list
-            To store the transactions of a database in list
-        mapSupport : Dictionary
-            To maintain the information of item and their frequency
-        lno : int
-            it represents the total no of transactions
-        tree : class
-            it represents the Tree class
-        finalPatterns : dict
-            it represents to store the patterns
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of frequent patterns will be loaded in to an output file
+            Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets()
-            Scans the dataset or dataframes and stores in list format
-        frequentOneItem()
-            Extracts the one-frequent patterns from transactions
-
-    **Methods to execute code on terminal**
-    -----------------------------------------
-            Format:
-                      >>> python3 WFRIMiner.py <inputFile> <outputFile> <weightSupport> <regularity>
 
-            Example:
-                      >>>  python3 WFRIMiner.py sampleDB.txt patterns.txt 10 5
+    **Executing the code on terminal:**
+    -------------------------------------
+    .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 PPPClose.py <inputFile> <outputFile> <periodicSupport> <period>
+
+       Examples:
+
+       (.venv) $ python3 PPPClose.py sampleTDB.txt patterns.txt 0.3 0.4
 
-                     .. note:: WS & regularity will be considered in support count or frequency
 
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
+    **Sample run of the imported code:**
+    --------------------------------------
     .. code-block:: python
 
-            from PAMI.weightedFrequentRegularpattern.basic import WFRIMiner as alg
+            from PAMI.partialPeriodicPattern.closed import PPPClose as alg
 
-            obj = alg.WFRIMiner(iFile, WS, regularity)
+            obj = alg.PPPClose("../basic/sampleTDB.txt", "2", "6")
 
             obj.startMine()
 
-            weightedFrequentRegularPatterns = obj.getPatterns()
+            periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(weightedFrequentRegularPatterns))
+            print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
 
-            obj.save(oFile)
+            obj.save("patterns")
 
-            Df = obj.getPatternInDataFrame()
+            Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    ----------------
-             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    --------------
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
 
-        """
+    """
 
+    _periodicSupport = float()
+    _period = float()
     _startTime = float()
     _endTime = float()
-    _WS = str()
-    _regularity = str()
-    _weight = {}
     _finalPatterns = {}
-    _wFile = " "
+    _Database = []
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _Database = []
+    _transaction = []
+    _hashing = {}
     _mapSupport = {}
+    _itemSetCount = 0
+    _maxItemId = 0
+    _tableSize = 10000
+    _tidList = {}
     _lno = 0
-    _tree = _Tree()
-    _rank = {}
-    _rankDup = {}
 
-    def __init__(self, iFile, _wFile, WS, regularity, sep='\t') -> None:
-        super().__init__(iFile, _wFile, WS, regularity, sep)
+    def _convert(self, value):
+        """
+        To convert the given user specified value
 
-    def _creatingItemSets(self) -> None:
+        :param value: user specified value
+        :return: converted value
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (self._lno * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (self._lno * value)
+            else:
+                value = int(value)
+        return value
+
+    def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
         self._Database = []
-        self._weight = {}
-        if isinstance(self._iFile, _fp._pd.DataFrame):
+        if isinstance(self._iFile, _abstract._pd.DataFrame):
+            timeStamp, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                timeStamp = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-
-        if isinstance(self._wFile, _fp._pd.DataFrame):
-            _items, _weights = [], []
-            if self._wFile.empty:
-                print("its empty..")
-            i = self._wFile.columns.values.tolist()
-            if 'items' in i:
-                _items = self._wFile['items'].tolist()
-            if 'weight' in i:
-                _weights = self._wFile['weight'].tolist()
-            for i in range(len(_items)):
-                self._weight[_items[i]] = _weights[i]
-
-            # print(self.Database)
+                data = self._iFile['Transactions'].tolist()
+            for i in range(len(data)):
+                tr = [timeStamp[i]]
+                tr = tr + data[i]
+                self._Database.append(tr)
+            self._lno = len(self._Database)
         if isinstance(self._iFile, str):
-            if _fp._validators.url(self._iFile):
-                data = _fp._urlopen(self._iFile)
+            if _validators.url(self._iFile):
+                data = _urlopen(self._iFile)
                 for line in data:
-                    line.strip()
+                    self._lno += 1
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
                     self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line.strip()
+                            self._lno += 1
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-        if isinstance(self._wFile, str):
-            if _fp._validators.url(self._wFile):
-                data = _fp._urlopen(self._wFile)
-                for line in data:
-                    line.strip()
-                    line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    self._weight[temp[0]] = float(temp[1])
-            else:
-                try:
-                    with open(self._wFile, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self._weight[temp[0]] = float(temp[1])
-                except IOError:
-                    print("File Not Found")
-                    quit()
-
-    def _convert(self, value) -> float:
+    def _OneLengthPartialItems(self):
         """
-        To convert the type of user specified minSup value
+        To scan the database and extracts the 1-length periodic-frequent items
 
-        :param value: user specified minSup value
-        :return: converted type
+        :return: Returns the 1-length periodic-frequent items
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
-
-    def _frequentOneItem(self) -> List[str]:
-        """
-        Generating One frequent items sets
-        """
-        global _lno, _wf, _weights
         self._mapSupport = {}
-        _owf = {}
-        for tr in self._Database:
-            for i in range(1, len(tr)):
-                if tr[i] not in self._mapSupport:
-                    self._mapSupport[tr[i]] = [int(tr[0]), int(tr[0]), 1]
+        self._tidList = {}
+        self._period = self._convert(self._period)
+        for line in self._Database:
+            n = int(line[0])
+            for i in range(1, len(line)):
+                si = line[i]
+                if self._mapSupport.get(si) is None:
+                    self._mapSupport[si] = [1, 0, n]
+                    self._tidList[si] = [n]
                 else:
-                    self._mapSupport[tr[i]][0] = max(self._mapSupport[tr[i]][0], (int(tr[0]) - self._mapSupport[tr[i]][1]))
-                    self._mapSupport[tr[i]][1] = int(tr[0])
-                    self._mapSupport[tr[i]][2] += 1
-        for key in self._mapSupport:
-            self._mapSupport[key][0] = max(self._mapSupport[key][0], abs(len(self._Database) - self._mapSupport[key][1]))
-        _lno = len(self._Database)
-        self._mapSupport = {k: [v[2], v[0]] for k, v in self._mapSupport.items() if v[0] <= self._regularity}
-        for x, y in self._mapSupport.items():
-            if self._weight.get(x) is None:
-                self._weight[x] = 0
-        gmax = max([self._weight[values] for values in self._mapSupport.keys()])
+                    self._mapSupport[si][0] += 1
+                    period = abs(n - self._mapSupport[si][2])
+                    if period <= self._period:
+                        self._mapSupport[si][1] += 1
+                    self._mapSupport[si][2] = n
+                    self._tidList[si].append(n)
         for x, y in self._mapSupport.items():
-            _owf[x] = y[0] * gmax
-        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v[0] * _owf[k] >= self._WS}
-        for x, y in self._mapSupport.items():
-            temp = self._weight[x] * y[0]
-            _wf[x] = temp
-            self._mapSupport[x].append(temp)
-        genList = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse= True)]
-        self._rank = dict([(index, item) for (item, index) in enumerate(genList)])
-        for x, y in self._rank.items():
-            _weights[y] = self._weight[x]
-        return genList
+            period = abs(self._lno - self._mapSupport[x][2])
+            if period <= self._period:
+                self._mapSupport[x][1] += 1
+        self._periodicSupport = self._convert(self._periodicSupport)
+        self._mapSupport = {k: v[1] for k, v in self._mapSupport.items() if v[1] >= self._periodicSupport}
+        periodicFrequentItems = {}
+        self._tidList = {k: v for k, v in self._tidList.items() if k in self._mapSupport}
+        for x, y in self._tidList.items():
+            t1 = 0
+            for i in y:
+                t1 += i
+            periodicFrequentItems[x] = t1
+        periodicFrequentItems = [key for key, value in sorted(periodicFrequentItems.items(), key=lambda x: x[1])]
+        return periodicFrequentItems
+
+    def _calculate(self, tidSet):
+        """
+        To calculate the weight if pattern based on the respective timeStamps
+
+        :param tidSet: timeStamps of the pattern
+        :return: the calculated weight of the timeStamps
+        """
+        hashcode = 0
+        for i in tidSet:
+            hashcode += i
+        if hashcode < 0:
+            hashcode = abs(0 - hashcode)
+        return hashcode % self._tableSize
+
+    def _contains(self, itemSet, val, hashcode):
+        """
+        To check if the key(hashcode) is in dictionary(hashing) variable
+
+        :param itemSet: generated periodic-frequent itemSet
+        :param val: support and period of itemSet
+        :param hashcode: the key generated in calculate() method for every itemSet
+
+        :return: true if itemSet with same support present in dictionary(hashing) or else returns false
+        """
+        if self._hashing.get(hashcode) is None:
+            return False
+        for i in self._hashing[hashcode]:
+            itemSetX = i
+            if val == self._hashing[hashcode][itemSetX] and set(itemSetX).issuperset(itemSet):
+                return True
+        return False
 
-    def _updateTransactions(self, itemSet) -> List[List[int]]:
+    def _getPeriodicSupport(self, timeStamps):
         """
-        Updates the items in transactions with rank of items according to their support
-
-        :Example:
-        oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
-        rank = {'a':0, 'b':1, 'c':2, 'd':3}
+        Calculates the period and support of timeStamps
 
-        :param itemSet: list of one-frequent items
+        :param: timeStamps: timeStamps of itemSet
+        :return: period and support
         """
-        list1 = []
-        for tr in self._Database:
-            list2 = [int(tr[0])]
-            for i in range(1, len(tr)):
-                if tr[i] in itemSet:
-                    list2.append(self._rank[tr[i]])
-            if len(list2) >= 2:
-                basket = list2[1:]
-                basket.sort()
-                list2[1:] = basket[0:]
-                list1.append(list2)
-        return list1
+        timeStamps.sort()
+        sup = 0
+        for j in range(len(timeStamps) - 1):
+            per = abs(timeStamps[j + 1] - timeStamps[j])
+            if per <= self._period:
+                sup += 1
+        return sup
+
+    def _save(self, prefix, suffix, tidSetX):
+        """
+        Saves the generated pattern which satisfies the closed property
+
+        :param prefix: the prefix part of itemSet
+        :param suffix: the suffix part of itemSet
+        :param tidSetX: the timeStamps of the generated itemSet
+        :return: saves the closed periodic-frequent pattern
+        """
+        if prefix is None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        prefix = list(set(prefix))
+        prefix.sort()
+        val = self._getPeriodicSupport(tidSetX)
+        if val >= self._periodicSupport:
+            hashcode = self._calculate(tidSetX)
+            if self._contains(prefix, val, hashcode) is False:
+                self._itemSetCount += 1
+                sample = str()
+                for i in prefix:
+                    sample = sample + i + "\t"
+                self._finalPatterns[sample] = val
+            if hashcode not in self._hashing:
+                self._hashing[hashcode] = {tuple(prefix): val}
+            else:
+                self._hashing[hashcode][tuple(prefix)] = val
 
-    @staticmethod
-    def _buildTree(transactions, info) -> _Tree:
+    def _processEquivalenceClass(self, prefix, itemSets, tidSets):
         """
-        Builds the tree with updated transactions
-
-        :param transactions: updated transactions
-        :param info: support details of each item in transactions
-        :return: transactions compressed in fp-tree
 
-        """
-        rootNode = _Tree()
-        rootNode.info = info.copy()
-        for i in range(len(transactions)):
-            set1 = [transactions[i][0]]
-            rootNode.addTransaction(transactions[i][1:], set1)
-        return rootNode
+        :param prefix: Prefix class of an itemSet
+        :param itemSets: suffix items in periodicFrequentItems that satisfies the periodicSupport condition
+        :param tidSets: timeStamps of items in itemSets respectively
+        :return: closed periodic patterns with length more than 2
+        """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidList = tidSets[0]
+            self._save(prefix, [i], tidList)
+            return
+        if len(itemSets) == 2:
+            itemI = itemSets[0]
+            tidSetI = tidSets[0]
+            itemJ = itemSets[1]
+            tidSetJ = tidSets[1]
+            y1 = list(set(tidSetI).intersection(tidSetJ))
+            if len(y1) >= self._periodicSupport:
+                suffix = []
+                suffix += [itemI, itemJ]
+                suffix = list(set(suffix))
+                self._save(prefix, suffix, y1)
+            if len(y1) != len(tidSetI):
+                self._save(prefix, [itemI], tidSetI)
+            if len(y1) != len(tidSetJ):
+                self._save(prefix, [itemJ], tidSetJ)
+            return
+        for i in range(len(itemSets)):
+            itemX = itemSets[i]
+            if itemX is None:
+                continue
+            tidSetX = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemX]
+            for j in range(i + 1, len(itemSets)):
+                itemJ = itemSets[j]
+                if itemJ is None:
+                    continue
+                tidSetJ = tidSets[j]
+                y = list(set(tidSetX).intersection(tidSetJ))
+                if len(y) < self._periodicSupport:
+                    continue
+                if len(tidSetX) == len(tidSetJ) and len(y) == len(tidSetX):
+                    itemSets.insert(j, None)
+                    tidSets.insert(j, None)
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) < len(tidSetJ) and len(y) == len(tidSetX):
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) > len(tidSetJ) and len(y) == len(tidSetJ):
+                    itemSets.insert(j, None)
+                    tidSets.insert(j, None)
+                    classItemSets.append(itemJ)
+                    classTidSets.append(y)
+                else:
+                    classItemSets.append(itemJ)
+                    classTidSets.append(y)
+            if len(classItemSets) > 0:
+                newPrefix = list(set(itemSetX)) + prefix
+                self._processEquivalenceClass(newPrefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetX)
 
-    def _savePeriodic(self, itemSet) -> str:
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self):
         """
-        The duplication items and their ranks
-
-        :param itemSet: frequent itemSet that generated
-        :return: patterns with original item names.
-
+        Mining process will start from here
         """
-        temp = str()
-        for i in itemSet:
-            temp = temp + self._rankDup[i] + "\t"
-        return temp
+        self._startTime = _abstract._time.time()
+        self._creatingItemSets()
+        self._hashing = {}
+        self._finalPatterns = {}
+        periodicFrequentItems = self._OneLengthPartialItems()
+        for i in range(len(periodicFrequentItems)):
+            itemX = periodicFrequentItems[i]
+            if itemX is None:
+                continue
+            tidSetX = self._tidList[itemX]
+            itemSetX = [itemX]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(periodicFrequentItems)):
+                itemJ = periodicFrequentItems[j]
+                if itemJ is None:
+                    continue
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                if len(y1) < self._periodicSupport:
+                    continue
+                if len(tidSetX) == len(tidSetJ) and len(y1) is len(tidSetX):
+                    periodicFrequentItems.insert(j, None)
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) < len(tidSetJ) and len(y1) is len(tidSetX):
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) > len(tidSetJ) and len(y1) is len(tidSetJ):
+                    periodicFrequentItems.insert(j, None)
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+                else:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            if len(itemSets) > 0:
+                self._processEquivalenceClass(itemSetX, itemSets, tidSets)
+            self._save([], itemSetX, tidSetX)
+        self._endTime = _abstract._time.time()
+        process = _abstract._psutil.Process(_abstract._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Closed periodic frequent patterns were generated successfully using PPPClose algorithm ")
 
-    def startMine(self) -> None:
+    def Mine(self):
         """
-            main program to start the operation
-
+        Mining process will start from here
         """
-        global _WS, _regularity, _weights
-        self._startTime = _fp._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._WS is None:
-            raise Exception("Please enter the Minimum Support")
+        self._startTime = _abstract._time.time()
         self._creatingItemSets()
-        self._WS = self._convert(self._WS)
-        self._regularity = self._convert(self._regularity)
-        _WS, _regularity, _weights = self._WS, self._regularity, self._weight
-        itemSet = self._frequentOneItem()
-        updatedTransactions = self._updateTransactions(itemSet)
-        for x, y in self._rank.items():
-            self._rankDup[y] = x
-        info = {self._rank[k]: v for k, v in self._mapSupport.items()}
-        _Tree = self._buildTree(updatedTransactions, info)
-        patterns = _Tree.generatePatterns([])
+        self._hashing = {}
         self._finalPatterns = {}
-        for k in patterns:
-            s = self._savePeriodic(k[0])
-            self._finalPatterns[str(s)] = k[1]
-        print("Weighted Frequent Regular patterns were generated successfully using WFRIM algorithm")
-        self._endTime = _fp._time.time()
+        periodicFrequentItems = self._OneLengthPartialItems()
+        for i in range(len(periodicFrequentItems)):
+            itemX = periodicFrequentItems[i]
+            if itemX is None:
+                continue
+            tidSetX = self._tidList[itemX]
+            itemSetX = [itemX]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(periodicFrequentItems)):
+                itemJ = periodicFrequentItems[j]
+                if itemJ is None:
+                    continue
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                if len(y1) < self._periodicSupport:
+                    continue
+                if len(tidSetX) == len(tidSetJ) and len(y1) is len(tidSetX):
+                    periodicFrequentItems.insert(j, None)
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) < len(tidSetJ) and len(y1) is len(tidSetX):
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) > len(tidSetJ) and len(y1) is len(tidSetJ):
+                    periodicFrequentItems.insert(j, None)
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+                else:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            if len(itemSets) > 0:
+                self._processEquivalenceClass(itemSetX, itemSets, tidSets)
+            self._save([], itemSetX, tidSetX)
+        self._endTime = _abstract._time.time()
+        process = _abstract._psutil.Process(_abstract._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
-        process = _fp._psutil.Process(_fp._os.getpid())
-        self._memoryRSS = float()
-        self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
+        print("Closed periodic frequent patterns were generated successfully using PPPClose algorithm ")
 
-    def getMemoryUSS(self) -> float:
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+    def getMemoryUSS(self):
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+    def getMemoryRSS(self):
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
-        """
-        Calculating the total amount of runtime taken by the mining process
+    def getRuntime(self):
+        """Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _fp._pd.DataFrame:
-        """
-        Storing final frequent patterns in a dataframe
+    def getPatternsAsDataFrame(self):
+        """Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataframe = {}
+        dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
-            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataframe
+            dataFrame = _abstract._pd.DataFrame(data, columns=['Patterns', 'periodicSupport'])
+        return dataFrame
 
-    def save(self, outFile: str) -> None:
-        """
-        Complete set of frequent patterns will be loaded in to an output file
+    def save(self, outFile):
+        """Complete set of frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
-        :type outFile: csv file
+        :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, float]:
-        """
-        Function to send the set of frequent patterns after completion of the mining process
+    def getPatterns(self):
+        """ Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self) -> None:
-        """
-        This function is used to print the results
-        """
-        print("Total number of  Weighted Frequent Regular Patterns:", len(self.getPatterns()))
+    def printResults(self):
+        print("Total number of  Closed Partial Periodic Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_fp._sys.argv) == 6 or len(_fp._sys.argv) == 7:
-        if len(_fp._sys.argv) == 7:
-            _ap = WFRIMiner(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
-        if len(_fp._sys.argv) == 5:
-            _ap = WFRIMiner(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5])
+    if len(_sys.argv) == 5 or len(_sys.argv) == 6:
+        if len(_sys.argv) == 6:
+            _ap = PPPClose(_sys.argv[1], _sys.argv[3], _sys.argv[4], _sys.argv[5])
+        if len(_sys.argv) == 5:
+            _ap = PPPClose(_sys.argv[1], _sys.argv[3], _sys.argv[4])
         _ap.startMine()
-        print("Total number of Weighted Frequent Regular Patterns:", len(_ap.getPatterns()))
-        _ap.save(_fp._sys.argv[2])
-        print("Total Memory in USS:",  _ap.getMemoryUSS())
+        print("Total number of  Patterns:", len(_ap.getPatterns()))
+        _ap.save(_sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

### Comparing `pami-2024.3.9.2/PAMI/weightedFrequentRegularPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PAMI/weightedUncertainFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.3.9.2/PKG-INFO` & `pami-2024.4.9.1/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -1,51 +1,7 @@
-Metadata-Version: 2.1
-Name: pami
-Version: 2024.3.9.2
-Summary: This software is being developed at the University of Aizu, Aizu-Wakamatsu, Fukushima, Japan
-Home-page: https://github.com/udayLab/PAMI
-Author: Rage Uday Kiran
-Author-email: uday.rage@gmail.com
-License: GPLv3
-Classifier: Development Status :: 5 - Production/Stable
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: GNU General Public License v3 (GPLv3)
-Classifier: Operating System :: OS Independent
-Requires-Python: >=3.5
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: psutil
-Requires-Dist: pandas
-Requires-Dist: plotly
-Requires-Dist: matplotlib
-Requires-Dist: resource
-Requires-Dist: validators
-Requires-Dist: urllib3
-Requires-Dist: Pillow
-Requires-Dist: numpy
-Requires-Dist: sphinx-rtd-theme
-Requires-Dist: validators
-Requires-Dist: discord.py
-Provides-Extra: gpu
-Requires-Dist: cupy; extra == "gpu"
-Requires-Dist: pycuda; extra == "gpu"
-Provides-Extra: spark
-Requires-Dist: pyspark; extra == "spark"
-Provides-Extra: dev
-Requires-Dist: twine; extra == "dev"
-Requires-Dist: setuptools; extra == "dev"
-Requires-Dist: build; extra == "dev"
-Provides-Extra: all
-Requires-Dist: cupy; extra == "all"
-Requires-Dist: pycuda; extra == "all"
-Requires-Dist: pyspark; extra == "all"
-Requires-Dist: twine; extra == "all"
-Requires-Dist: setuptools; extra == "all"
-Requires-Dist: build; extra == "all"
-
 ![PyPI](https://img.shields.io/pypi/v/PAMI)
 ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/PAMI)
 [![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
 ![PyPI - Implementation](https://img.shields.io/pypi/implementation/PAMI)
 [![Documentation Status](https://readthedocs.org/projects/pami-1/badge/?version=latest)](https://pami-1.readthedocs.io/en/latest/?badge=latest)
 ![PyPI - Wheel](https://img.shields.io/pypi/wheel/PAMI)
 ![PyPI - Status](https://img.shields.io/pypi/status/PAMI)
@@ -75,34 +31,47 @@
 
 6. Datasets   https://u-aizu.ac.jp/~udayrage/datasets.html
 
 7. Discussions on PAMI usage https://github.com/UdayLab/PAMI/discussions
 
 8. Report issues https://github.com/UdayLab/PAMI/issues
 
+# Recent Updates  
+
+- Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
+- Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
+- Version 2023.03.01: prefixSpan and SPADE   
+
+Total number of algorithms: 83
+
 # Features
 
 - ✅ Well-tested and production-ready
 - 🔋 Highly optimized to our best effort, light-weight, and energy-efficient
 - 👀 Proper code documentation
 - 🍼 Ample examples of using various algorithms at [./notebooks](https://github.com/UdayLab/PAMI/tree/main/notebooks) folder
 - 🤖 Works with AI libraries such as TensorFlow, PyTorch, and sklearn. 
 - ⚡️ Supports Cuda and PySpark 
 - 🖥️ Operating System Independence
 - 🔬 Knowledge discovery in static data and streams
 - 🐎 Snappy
 - 🐻 Ease of use
 
-# Recent versions  
+# Table of Content
 
-- Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
-- Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
-- Version 2023.03.01: prefixSpan and SPADE   
-
-Total number of algorithms: 83
+- [Maintenance](#Maintenance)
+- [Try your first PAMI program](#try-your-first-PAMI-program)
+- [Reading Material](#Reading-Material)
+- [Tutorials](#Tutorials)
+- [License](#License)
+- [Documentation](#Documentation)
+- [Background](#Background)
+- [Getting Help](#Getting-Help)
+- [Discussion and Development](#Discussion-and-Development)
+- [Contribution to PAMI](#Contribution-to-PAMI)
 
 # Maintenance
 
   __Installation__
   
   1. Installing basic pami package (recommended)
 
@@ -116,15 +85,27 @@
          pip install 'pami[gpu]'
 
 
   3. Installing pami package in a distributed network environment supporting Spark
 
 
          pip install 'pami[spark]'
-  
+
+
+  4. Installing pami package for developing purpose
+          
+
+         pip install 'pami[dev]'
+
+
+  5. Installing complete Library of pami 
+
+
+         pip install 'pami[all]'
+
 
   __Upgradation__
 
   
         pip install --upgrade pami
   
 
@@ -135,15 +116,47 @@
        
 
   __Information__ 
 
 
         pip show pami
 
+# *Try your first PAMI program*
 
+```shell
+$ python
+```
+
+```python
+# first import pami 
+from PAMI.frequentPattern.basic import FPGrowth as alg
+fileURL = "https://u-aizu.ac.jp/~udayrage/datasets/transactionalDatabases/Transactional_T10I4D100K.csv"
+minSup=300
+obj = alg.FPGrowth(iFile=fileURL, minSup=minSup, sep='\t')
+obj.startMine()
+obj.save('frequentPatternsAtMinSupCount300.txt')
+frequentPatternsDF= obj.getPatternsAsDataFrame()
+print('Total No of patterns: ' + str(len(frequentPatternsDF))) #print the total number of patterns
+print('Runtime: ' + str(obj.getRuntime())) #measure the runtime
+print('Memory (RSS): ' + str(obj.getMemoryRSS()))
+print('Memory (USS): ' + str(obj.getMemoryUSS()))
+```
+
+```
+Output:
+Frequent patterns were generated successfully using frequentPatternGrowth algorithm
+Total No of patterns: 4540
+Runtime: 8.749667644500732
+Memory (RSS): 522911744
+Memory (USS): 475353088
+```
+
+# Reading Material
+For more examples, refer this YouTube link [YouTube](https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ)
+ 
 ***
 
 # Tutorials 
 
 ### 1. Pattern mining in binary transactional databases
 
 #### 1.1. Frequent pattern mining: [Sample](https://udaylab.github.io/PAMI/frequentPatternMining.html)
@@ -436,12 +449,35 @@
 
 #### 10.1. Contiguous Frequent Patterns
 
 | Basic                                                                                                                                                                                                                                                                   |
 |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | PositionMining <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/contiguousFrequentPattern/basic/PositionMining.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
-## 11. Mining pattrens from Graphs
-__coming soon__
-  
-  
-     
+## 11. Mining patterns from Graphs
+
+#### 11.1. Frequent sub-graph mining
+| Basic                                                                                                                                                                                                                                      | topk                                                                                                                                                                                                                                  |
+|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
+ | Gspan <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/basic/gspan.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | TKG <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/topk/tkg.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
+
+# License
+
+[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
+
+# Documentation
+The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
+
+# Background
+The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
+has been under active development since then.
+
+# Getting Help
+For any queries, the best place to go to is github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
+
+# Discussion and Development
+In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
+
+# Contribution to PAMI
+We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
+
+[Go to Top](#table-of-contents)
```

### Comparing `pami-2024.3.9.2/README.md` & `pami-2024.4.9.1/PKG-INFO`

 * *Files 9% similar despite different names*

```diff
@@ -1,7 +1,52 @@
+Metadata-Version: 2.1
+Name: pami
+Version: 2024.4.9.1
+Summary: This software is being developed at the University of Aizu, Aizu-Wakamatsu, Fukushima, Japan
+Home-page: https://github.com/udayLab/PAMI
+Author: Rage Uday Kiran
+Author-email: uday.rage@gmail.com
+License: GPLv3
+Classifier: Development Status :: 5 - Production/Stable
+Classifier: Programming Language :: Python :: 3
+Classifier: License :: OSI Approved :: GNU General Public License v3 (GPLv3)
+Classifier: Operating System :: OS Independent
+Requires-Python: >=3.5
+Description-Content-Type: text/markdown
+License-File: LICENSE
+Requires-Dist: psutil
+Requires-Dist: pandas
+Requires-Dist: plotly
+Requires-Dist: matplotlib
+Requires-Dist: resource
+Requires-Dist: validators
+Requires-Dist: urllib3
+Requires-Dist: Pillow
+Requires-Dist: numpy
+Requires-Dist: sphinx-rtd-theme
+Requires-Dist: validators
+Requires-Dist: discord.py
+Requires-Dist: networkx
+Provides-Extra: gpu
+Requires-Dist: cupy; extra == "gpu"
+Requires-Dist: pycuda; extra == "gpu"
+Provides-Extra: spark
+Requires-Dist: pyspark; extra == "spark"
+Provides-Extra: dev
+Requires-Dist: twine; extra == "dev"
+Requires-Dist: setuptools; extra == "dev"
+Requires-Dist: build; extra == "dev"
+Provides-Extra: all
+Requires-Dist: cupy; extra == "all"
+Requires-Dist: pycuda; extra == "all"
+Requires-Dist: pyspark; extra == "all"
+Requires-Dist: twine; extra == "all"
+Requires-Dist: setuptools; extra == "all"
+Requires-Dist: build; extra == "all"
+
 ![PyPI](https://img.shields.io/pypi/v/PAMI)
 ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/PAMI)
 [![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
 ![PyPI - Implementation](https://img.shields.io/pypi/implementation/PAMI)
 [![Documentation Status](https://readthedocs.org/projects/pami-1/badge/?version=latest)](https://pami-1.readthedocs.io/en/latest/?badge=latest)
 ![PyPI - Wheel](https://img.shields.io/pypi/wheel/PAMI)
 ![PyPI - Status](https://img.shields.io/pypi/status/PAMI)
@@ -31,34 +76,47 @@
 
 6. Datasets   https://u-aizu.ac.jp/~udayrage/datasets.html
 
 7. Discussions on PAMI usage https://github.com/UdayLab/PAMI/discussions
 
 8. Report issues https://github.com/UdayLab/PAMI/issues
 
+# Recent Updates  
+
+- Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
+- Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
+- Version 2023.03.01: prefixSpan and SPADE   
+
+Total number of algorithms: 83
+
 # Features
 
 - ✅ Well-tested and production-ready
 - 🔋 Highly optimized to our best effort, light-weight, and energy-efficient
 - 👀 Proper code documentation
 - 🍼 Ample examples of using various algorithms at [./notebooks](https://github.com/UdayLab/PAMI/tree/main/notebooks) folder
 - 🤖 Works with AI libraries such as TensorFlow, PyTorch, and sklearn. 
 - ⚡️ Supports Cuda and PySpark 
 - 🖥️ Operating System Independence
 - 🔬 Knowledge discovery in static data and streams
 - 🐎 Snappy
 - 🐻 Ease of use
 
-# Recent versions  
+# Table of Content
 
-- Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
-- Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
-- Version 2023.03.01: prefixSpan and SPADE   
-
-Total number of algorithms: 83
+- [Maintenance](#Maintenance)
+- [Try your first PAMI program](#try-your-first-PAMI-program)
+- [Reading Material](#Reading-Material)
+- [Tutorials](#Tutorials)
+- [License](#License)
+- [Documentation](#Documentation)
+- [Background](#Background)
+- [Getting Help](#Getting-Help)
+- [Discussion and Development](#Discussion-and-Development)
+- [Contribution to PAMI](#Contribution-to-PAMI)
 
 # Maintenance
 
   __Installation__
   
   1. Installing basic pami package (recommended)
 
@@ -72,15 +130,27 @@
          pip install 'pami[gpu]'
 
 
   3. Installing pami package in a distributed network environment supporting Spark
 
 
          pip install 'pami[spark]'
-  
+
+
+  4. Installing pami package for developing purpose
+          
+
+         pip install 'pami[dev]'
+
+
+  5. Installing complete Library of pami 
+
+
+         pip install 'pami[all]'
+
 
   __Upgradation__
 
   
         pip install --upgrade pami
   
 
@@ -91,15 +161,47 @@
        
 
   __Information__ 
 
 
         pip show pami
 
+# *Try your first PAMI program*
 
+```shell
+$ python
+```
+
+```python
+# first import pami 
+from PAMI.frequentPattern.basic import FPGrowth as alg
+fileURL = "https://u-aizu.ac.jp/~udayrage/datasets/transactionalDatabases/Transactional_T10I4D100K.csv"
+minSup=300
+obj = alg.FPGrowth(iFile=fileURL, minSup=minSup, sep='\t')
+obj.startMine()
+obj.save('frequentPatternsAtMinSupCount300.txt')
+frequentPatternsDF= obj.getPatternsAsDataFrame()
+print('Total No of patterns: ' + str(len(frequentPatternsDF))) #print the total number of patterns
+print('Runtime: ' + str(obj.getRuntime())) #measure the runtime
+print('Memory (RSS): ' + str(obj.getMemoryRSS()))
+print('Memory (USS): ' + str(obj.getMemoryUSS()))
+```
+
+```
+Output:
+Frequent patterns were generated successfully using frequentPatternGrowth algorithm
+Total No of patterns: 4540
+Runtime: 8.749667644500732
+Memory (RSS): 522911744
+Memory (USS): 475353088
+```
+
+# Reading Material
+For more examples, refer this YouTube link [YouTube](https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ)
+ 
 ***
 
 # Tutorials 
 
 ### 1. Pattern mining in binary transactional databases
 
 #### 1.1. Frequent pattern mining: [Sample](https://udaylab.github.io/PAMI/frequentPatternMining.html)
@@ -392,12 +494,35 @@
 
 #### 10.1. Contiguous Frequent Patterns
 
 | Basic                                                                                                                                                                                                                                                                   |
 |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | PositionMining <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/contiguousFrequentPattern/basic/PositionMining.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
-## 11. Mining pattrens from Graphs
-__coming soon__
-  
-  
-     
+## 11. Mining patterns from Graphs
+
+#### 11.1. Frequent sub-graph mining
+| Basic                                                                                                                                                                                                                                      | topk                                                                                                                                                                                                                                  |
+|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
+ | Gspan <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/basic/gspan.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | TKG <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/topk/tkg.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
+
+# License
+
+[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
+
+# Documentation
+The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
+
+# Background
+The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
+has been under active development since then.
+
+# Getting Help
+For any queries, the best place to go to is github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
+
+# Discussion and Development
+In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
+
+# Contribution to PAMI
+We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
+
+[Go to Top](#table-of-contents)
```

### Comparing `pami-2024.3.9.2/pami.egg-info/PKG-INFO` & `pami-2024.4.9.1/pami.egg-info/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pami
-Version: 2024.3.9.2
+Version: 2024.4.9.1
 Summary: This software is being developed at the University of Aizu, Aizu-Wakamatsu, Fukushima, Japan
 Home-page: https://github.com/udayLab/PAMI
 Author: Rage Uday Kiran
 Author-email: uday.rage@gmail.com
 License: GPLv3
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Programming Language :: Python :: 3
@@ -21,14 +21,15 @@
 Requires-Dist: validators
 Requires-Dist: urllib3
 Requires-Dist: Pillow
 Requires-Dist: numpy
 Requires-Dist: sphinx-rtd-theme
 Requires-Dist: validators
 Requires-Dist: discord.py
+Requires-Dist: networkx
 Provides-Extra: gpu
 Requires-Dist: cupy; extra == "gpu"
 Requires-Dist: pycuda; extra == "gpu"
 Provides-Extra: spark
 Requires-Dist: pyspark; extra == "spark"
 Provides-Extra: dev
 Requires-Dist: twine; extra == "dev"
@@ -75,34 +76,47 @@
 
 6. Datasets   https://u-aizu.ac.jp/~udayrage/datasets.html
 
 7. Discussions on PAMI usage https://github.com/UdayLab/PAMI/discussions
 
 8. Report issues https://github.com/UdayLab/PAMI/issues
 
+# Recent Updates  
+
+- Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
+- Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
+- Version 2023.03.01: prefixSpan and SPADE   
+
+Total number of algorithms: 83
+
 # Features
 
 - ✅ Well-tested and production-ready
 - 🔋 Highly optimized to our best effort, light-weight, and energy-efficient
 - 👀 Proper code documentation
 - 🍼 Ample examples of using various algorithms at [./notebooks](https://github.com/UdayLab/PAMI/tree/main/notebooks) folder
 - 🤖 Works with AI libraries such as TensorFlow, PyTorch, and sklearn. 
 - ⚡️ Supports Cuda and PySpark 
 - 🖥️ Operating System Independence
 - 🔬 Knowledge discovery in static data and streams
 - 🐎 Snappy
 - 🐻 Ease of use
 
-# Recent versions  
+# Table of Content
 
-- Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
-- Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
-- Version 2023.03.01: prefixSpan and SPADE   
-
-Total number of algorithms: 83
+- [Maintenance](#Maintenance)
+- [Try your first PAMI program](#try-your-first-PAMI-program)
+- [Reading Material](#Reading-Material)
+- [Tutorials](#Tutorials)
+- [License](#License)
+- [Documentation](#Documentation)
+- [Background](#Background)
+- [Getting Help](#Getting-Help)
+- [Discussion and Development](#Discussion-and-Development)
+- [Contribution to PAMI](#Contribution-to-PAMI)
 
 # Maintenance
 
   __Installation__
   
   1. Installing basic pami package (recommended)
 
@@ -116,15 +130,27 @@
          pip install 'pami[gpu]'
 
 
   3. Installing pami package in a distributed network environment supporting Spark
 
 
          pip install 'pami[spark]'
-  
+
+
+  4. Installing pami package for developing purpose
+          
+
+         pip install 'pami[dev]'
+
+
+  5. Installing complete Library of pami 
+
+
+         pip install 'pami[all]'
+
 
   __Upgradation__
 
   
         pip install --upgrade pami
   
 
@@ -135,15 +161,47 @@
        
 
   __Information__ 
 
 
         pip show pami
 
+# *Try your first PAMI program*
 
+```shell
+$ python
+```
+
+```python
+# first import pami 
+from PAMI.frequentPattern.basic import FPGrowth as alg
+fileURL = "https://u-aizu.ac.jp/~udayrage/datasets/transactionalDatabases/Transactional_T10I4D100K.csv"
+minSup=300
+obj = alg.FPGrowth(iFile=fileURL, minSup=minSup, sep='\t')
+obj.startMine()
+obj.save('frequentPatternsAtMinSupCount300.txt')
+frequentPatternsDF= obj.getPatternsAsDataFrame()
+print('Total No of patterns: ' + str(len(frequentPatternsDF))) #print the total number of patterns
+print('Runtime: ' + str(obj.getRuntime())) #measure the runtime
+print('Memory (RSS): ' + str(obj.getMemoryRSS()))
+print('Memory (USS): ' + str(obj.getMemoryUSS()))
+```
+
+```
+Output:
+Frequent patterns were generated successfully using frequentPatternGrowth algorithm
+Total No of patterns: 4540
+Runtime: 8.749667644500732
+Memory (RSS): 522911744
+Memory (USS): 475353088
+```
+
+# Reading Material
+For more examples, refer this YouTube link [YouTube](https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ)
+ 
 ***
 
 # Tutorials 
 
 ### 1. Pattern mining in binary transactional databases
 
 #### 1.1. Frequent pattern mining: [Sample](https://udaylab.github.io/PAMI/frequentPatternMining.html)
@@ -436,12 +494,35 @@
 
 #### 10.1. Contiguous Frequent Patterns
 
 | Basic                                                                                                                                                                                                                                                                   |
 |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | PositionMining <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/contiguousFrequentPattern/basic/PositionMining.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
-## 11. Mining pattrens from Graphs
-__coming soon__
-  
-  
-     
+## 11. Mining patterns from Graphs
+
+#### 11.1. Frequent sub-graph mining
+| Basic                                                                                                                                                                                                                                      | topk                                                                                                                                                                                                                                  |
+|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
+ | Gspan <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/basic/gspan.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | TKG <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/topk/tkg.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
+
+# License
+
+[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
+
+# Documentation
+The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
+
+# Background
+The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
+has been under active development since then.
+
+# Getting Help
+For any queries, the best place to go to is github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
+
+# Discussion and Development
+In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
+
+# Contribution to PAMI
+We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
+
+[Go to Top](#table-of-contents)
```

### Comparing `pami-2024.3.9.2/pami.egg-info/SOURCES.txt` & `pami-2024.4.9.1/pami.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -71,14 +71,22 @@
 PAMI/extras/messaging/discord.py
 PAMI/extras/messaging/gmail.py
 PAMI/extras/neighbours/__init__.py
 PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py
 PAMI/extras/neighbours/findNeighboursUsingEuclidean.py
 PAMI/extras/neighbours/findNeighboursUsingGeodesic.py
 PAMI/extras/sampleDatasets/__init__.py
+PAMI/extras/stats/TransactionalDatabase.py
+PAMI/extras/stats/__init__.py
+PAMI/extras/stats/graphDatabase.py
+PAMI/extras/stats/sequentialDatabase.py
+PAMI/extras/stats/temporalDatabase.py
+PAMI/extras/stats/utilityDatabase.py
+PAMI/extras/syntheticDataGenerator/TemporalDatabase.py
+PAMI/extras/syntheticDataGenerator/TransactionalDatabase.py
 PAMI/extras/syntheticDataGenerator/__init__.py
 PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py
 PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py
 PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py
 PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py
 PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py
 PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py
@@ -89,17 +97,19 @@
 PAMI/extras/syntheticDataGenerator/generateTransactional.py
 PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py
 PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py
 PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py
 PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py
 PAMI/extras/syntheticDataGenerator/georeferencedTemporalDatabase.py
 PAMI/extras/syntheticDataGenerator/georeferencedTransactionalDatabase.py
-PAMI/extras/syntheticDataGenerator/temporalDatabase.py
-PAMI/extras/syntheticDataGenerator/transactionalDatabase.py
+PAMI/extras/syntheticDataGenerator/syntheticUtilityDatabase.py
+PAMI/extras/syntheticDataGenerator/temporalDatabaseGen.py
 PAMI/extras/syntheticDataGenerator/utilityDatabase.py
+PAMI/extras/visualize/__init__.py
+PAMI/extras/visualize/graphs.py
 PAMI/faultTolerantFrequentPattern/__init__.py
 PAMI/faultTolerantFrequentPattern/basic/FTApriori.py
 PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py
 PAMI/faultTolerantFrequentPattern/basic/__init__.py
 PAMI/faultTolerantFrequentPattern/basic/abstract.py
 PAMI/frequentPattern/__init__.py
 PAMI/frequentPattern/basic/Apriori.py
@@ -315,14 +325,25 @@
 PAMI/subgraphMining/basic/edge.py
 PAMI/subgraphMining/basic/extendedEdge.py
 PAMI/subgraphMining/basic/frequentSubgraph.py
 PAMI/subgraphMining/basic/graph.py
 PAMI/subgraphMining/basic/gspan.py
 PAMI/subgraphMining/basic/sparseTriangularMatrix.py
 PAMI/subgraphMining/basic/vertex.py
+PAMI/subgraphMining/topK/DFSCode.py
+PAMI/subgraphMining/topK/DFSThread.py
+PAMI/subgraphMining/topK/__init__.py
+PAMI/subgraphMining/topK/abstract.py
+PAMI/subgraphMining/topK/edge.py
+PAMI/subgraphMining/topK/extendedEdge.py
+PAMI/subgraphMining/topK/frequentSubgraph.py
+PAMI/subgraphMining/topK/graph.py
+PAMI/subgraphMining/topK/sparseTriangularMatrix.py
+PAMI/subgraphMining/topK/tkg.py
+PAMI/subgraphMining/topK/vertex.py
 PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py
 PAMI/uncertainFaultTolerantFrequentPattern/__init__.py
 PAMI/uncertainFaultTolerantFrequentPattern/abstract.py
 PAMI/uncertainFrequentPattern/__init__.py
 PAMI/uncertainFrequentPattern/basic/CUFPTree.py
 PAMI/uncertainFrequentPattern/basic/PUFGrowth.py
 PAMI/uncertainFrequentPattern/basic/TUFP.py
```

### Comparing `pami-2024.3.9.2/setup.py` & `pami-2024.4.9.1/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import setuptools
 
 with open('README.md', 'r') as fh:
     long_description = fh.read()
 
 setuptools.setup(
     name='pami',
-    version='2024.3.9.2',
+    version='2024.4.9.1',
     author='Rage Uday Kiran',
     author_email='uday.rage@gmail.com',
     description='This software is being developed at the University of Aizu, Aizu-Wakamatsu, Fukushima, Japan',
     long_description=long_description,
     long_description_content_type='text/markdown',
     packages=setuptools.find_packages(),
     url='https://github.com/udayLab/PAMI',
@@ -22,15 +22,16 @@
         'resource',
         'validators',
         'urllib3',
         'Pillow',
         'numpy',
         'sphinx-rtd-theme',
         'validators',
-        ' discord.py',
+        'discord.py',
+        'networkx',
     ],
     extras_require={
         'gpu':  ['cupy', 'pycuda'],
         'spark': ['pyspark'],
         'dev': ['twine', 'setuptools', 'build'],
         'all': ['cupy', 'pycuda', 'pyspark', 'twine', 'setuptools', 'build']
     },
```

